/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-10 15:37:15,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-10 15:37:15,376] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f0217c9fe50>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-10 15:37:18,542] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-10 15:37:18,542] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-10 15:37:18,561] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-10 15:37:18,561] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-10 15:37:18,597] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.03989124298095703 seconds
[2025-01-10 15:37:18,808] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-10 15:37:18,808] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-10 15:37:18,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-10 15:37:18,810] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-10 15:37:18,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-10 15:37:18,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-10 15:37:18,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-10 15:37:18,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 15:37:18,817] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f02174cee30>
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-10 15:37:18,817] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-10 15:37:18,818] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-10 15:37:18,818] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1430639 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1430640 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1430636 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-10 15:37:25,111] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-10 15:37:25,171] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f89b959fe50>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-10 15:37:28,025] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-10 15:37:28,025] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-10 15:37:28,033] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-10 15:37:28,033] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-10 15:37:28,068] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.03939986228942871 seconds
[2025-01-10 15:37:28,274] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-10 15:37:28,274] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-10 15:37:28,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-10 15:37:28,276] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-10 15:37:28,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-10 15:37:28,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-10 15:37:28,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-10 15:37:28,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 15:37:28,283] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f89b5fbae30>
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-10 15:37:28,283] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-10 15:37:28,284] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-10 15:37:28,284] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
Epoch: [0]  [   0/1404]  eta: 6:32:35  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 16.7776  data: 5.9414  max mem: 15572
Epoch: [0]  [  10/1404]  eta: 0:44:29  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.9149  data: 0.5404  max mem: 15572
Epoch: [0]  [  20/1404]  eta: 0:29:09  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4885  data: 0.0006  max mem: 15572
Epoch: [0]  [  30/1404]  eta: 0:23:31  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0012  max mem: 15572
Epoch: [0]  [  40/1404]  eta: 0:20:38  lr: 0.000001  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0014  max mem: 15572
Epoch: [0]  [  50/1404]  eta: 0:18:34  lr: 0.000001  min_lr: 0.000000  loss: 5.1593 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0010  max mem: 15572
Epoch: [0]  [  60/1404]  eta: 0:17:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1588 (5.1596)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4857  data: 0.0006  max mem: 15572
Epoch: [0]  [  70/1404]  eta: 0:16:35  lr: 0.000001  min_lr: 0.000000  loss: 5.1585 (5.1594)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0207  max mem: 15572
Epoch: [0]  [  80/1404]  eta: 0:15:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0598  max mem: 15572
Epoch: [0]  [  90/1404]  eta: 0:15:35  lr: 0.000001  min_lr: 0.000000  loss: 5.1579 (5.1591)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1044  max mem: 15572
Epoch: [0]  [ 100/1404]  eta: 0:15:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1591)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1187  max mem: 15572
Epoch: [0]  [ 110/1404]  eta: 0:15:10  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1589)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0901  max mem: 15572
Epoch: [0]  [ 120/1404]  eta: 0:14:47  lr: 0.000002  min_lr: 0.000000  loss: 5.1574 (5.1588)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0613  max mem: 15572
[2025-01-10 15:38:57,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:38:57,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
[2025-01-10 15:38:57,290] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:38:57,291] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/1404]  eta: 0:14:31  lr: 0.000002  min_lr: 0.000000  loss: 5.1565 (5.1586)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0252  max mem: 15572
Epoch: [0]  [ 140/1404]  eta: 0:14:27  lr: 0.000002  min_lr: 0.000000  loss: 5.1563 (5.1584)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.0770  max mem: 15572
Epoch: [0]  [ 150/1404]  eta: 0:14:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1554 (5.1582)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6977  data: 0.1453  max mem: 15572
Epoch: [0]  [ 160/1404]  eta: 0:14:09  lr: 0.000002  min_lr: 0.000000  loss: 5.1550 (5.1580)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.1339  max mem: 15572
Epoch: [0]  [ 170/1404]  eta: 0:14:03  lr: 0.000002  min_lr: 0.000000  loss: 5.1546 (5.1577)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1527  max mem: 15572
Epoch: [0]  [ 180/1404]  eta: 0:13:54  lr: 0.000002  min_lr: 0.000000  loss: 5.1536 (5.1575)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.1766  max mem: 15572
Epoch: [0]  [ 190/1404]  eta: 0:13:40  lr: 0.000003  min_lr: 0.000000  loss: 5.1531 (5.1572)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1201  max mem: 15572
Epoch: [0]  [ 200/1404]  eta: 0:13:33  lr: 0.000003  min_lr: 0.000000  loss: 5.1512 (5.1568)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1215  max mem: 15572
Epoch: [0]  [ 210/1404]  eta: 0:13:16  lr: 0.000003  min_lr: 0.000000  loss: 5.1500 (5.1565)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0909  max mem: 15572
Epoch: [0]  [ 220/1404]  eta: 0:13:02  lr: 0.000003  min_lr: 0.000000  loss: 5.1483 (5.1562)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0010  max mem: 15572
Epoch: [0]  [ 230/1404]  eta: 0:12:51  lr: 0.000003  min_lr: 0.000000  loss: 5.1476 (5.1558)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0278  max mem: 15572
Epoch: [0]  [ 240/1404]  eta: 0:12:45  lr: 0.000003  min_lr: 0.000000  loss: 5.1428 (5.1552)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1026  max mem: 15572
Epoch: [0]  [ 250/1404]  eta: 0:12:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1423 (5.1547)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1693  max mem: 15572
[2025-01-10 15:40:17,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:40:17,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
[2025-01-10 15:40:17,242] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:40:17,243] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/1404]  eta: 0:12:38  lr: 0.000003  min_lr: 0.000000  loss: 5.1404 (5.1541)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.2152  max mem: 15572
Epoch: [0]  [ 270/1404]  eta: 0:12:29  lr: 0.000004  min_lr: 0.000000  loss: 5.1373 (5.1535)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.1900  max mem: 15572
Epoch: [0]  [ 280/1404]  eta: 0:12:19  lr: 0.000004  min_lr: 0.000000  loss: 5.1362 (5.1531)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0994  max mem: 15572
Epoch: [0]  [ 290/1404]  eta: 0:12:10  lr: 0.000004  min_lr: 0.000000  loss: 5.1402 (5.1526)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0848  max mem: 15572
Epoch: [0]  [ 300/1404]  eta: 0:12:04  lr: 0.000004  min_lr: 0.000000  loss: 5.1323 (5.1518)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1501  max mem: 15572
Epoch: [0]  [ 310/1404]  eta: 0:11:56  lr: 0.000004  min_lr: 0.000000  loss: 5.1250 (5.1511)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1609  max mem: 15572
Epoch: [0]  [ 320/1404]  eta: 0:11:47  lr: 0.000004  min_lr: 0.000000  loss: 5.1309 (5.1504)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0997  max mem: 15572
Epoch: [0]  [ 330/1404]  eta: 0:11:41  lr: 0.000004  min_lr: 0.000000  loss: 5.1295 (5.1496)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1264  max mem: 15572
Epoch: [0]  [ 340/1404]  eta: 0:11:30  lr: 0.000005  min_lr: 0.000000  loss: 5.1108 (5.1484)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0920  max mem: 15572
Epoch: [0]  [ 350/1404]  eta: 0:11:22  lr: 0.000005  min_lr: 0.000000  loss: 5.1158 (5.1477)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0589  max mem: 15572
Epoch: [0]  [ 360/1404]  eta: 0:11:13  lr: 0.000005  min_lr: 0.000000  loss: 5.1197 (5.1468)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0776  max mem: 15572
Epoch: [0]  [ 370/1404]  eta: 0:11:07  lr: 0.000005  min_lr: 0.000000  loss: 5.1111 (5.1458)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0917  max mem: 15572
Epoch: [0]  [ 380/1404]  eta: 0:10:58  lr: 0.000005  min_lr: 0.000000  loss: 5.1024 (5.1447)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0832  max mem: 15572
[2025-01-10 15:41:35,566] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:41:35,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2025-01-10 15:41:35,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:41:35,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/1404]  eta: 0:10:52  lr: 0.000005  min_lr: 0.000000  loss: 5.1010 (5.1436)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0890  max mem: 15572
Epoch: [0]  [ 400/1404]  eta: 0:10:45  lr: 0.000005  min_lr: 0.000000  loss: 5.1010 (5.1426)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1140  max mem: 15572
Epoch: [0]  [ 410/1404]  eta: 0:10:37  lr: 0.000005  min_lr: 0.000000  loss: 5.1009 (5.1419)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0647  max mem: 15572
Epoch: [0]  [ 420/1404]  eta: 0:10:30  lr: 0.000006  min_lr: 0.000000  loss: 5.1009 (5.1407)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0813  max mem: 15572
Epoch: [0]  [ 430/1404]  eta: 0:10:22  lr: 0.000006  min_lr: 0.000000  loss: 5.0888 (5.1395)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0917  max mem: 15572
Epoch: [0]  [ 440/1404]  eta: 0:10:17  lr: 0.000006  min_lr: 0.000000  loss: 5.0852 (5.1385)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1091  max mem: 15572
Epoch: [0]  [ 450/1404]  eta: 0:10:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0882 (5.1376)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0961  max mem: 15572
Epoch: [0]  [ 460/1404]  eta: 0:10:02  lr: 0.000006  min_lr: 0.000000  loss: 5.0989 (5.1369)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0510  max mem: 15572
Epoch: [0]  [ 470/1404]  eta: 0:09:56  lr: 0.000006  min_lr: 0.000000  loss: 5.0989 (5.1359)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0871  max mem: 15572
Epoch: [0]  [ 480/1404]  eta: 0:09:47  lr: 0.000006  min_lr: 0.000000  loss: 5.0973 (5.1350)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0630  max mem: 15572
Epoch: [0]  [ 490/1404]  eta: 0:09:41  lr: 0.000007  min_lr: 0.000000  loss: 5.0796 (5.1340)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0007  max mem: 15572
Epoch: [0]  [ 500/1404]  eta: 0:09:34  lr: 0.000007  min_lr: 0.000000  loss: 5.0696 (5.1327)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0045  max mem: 15572
Epoch: [0]  [ 510/1404]  eta: 0:09:28  lr: 0.000007  min_lr: 0.000000  loss: 5.0646 (5.1316)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0130  max mem: 15572
[2025-01-10 15:42:54,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:42:54,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2025-01-10 15:42:54,592] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:42:54,592] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/1404]  eta: 0:09:22  lr: 0.000007  min_lr: 0.000000  loss: 5.0646 (5.1304)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0937  max mem: 15572
Epoch: [0]  [ 530/1404]  eta: 0:09:15  lr: 0.000007  min_lr: 0.000000  loss: 5.0805 (5.1298)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1018  max mem: 15572
Epoch: [0]  [ 540/1404]  eta: 0:09:10  lr: 0.000007  min_lr: 0.000000  loss: 5.0629 (5.1283)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0831  max mem: 15572
Epoch: [0]  [ 550/1404]  eta: 0:09:02  lr: 0.000007  min_lr: 0.000000  loss: 5.0627 (5.1274)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1079  max mem: 15572
Epoch: [0]  [ 560/1404]  eta: 0:08:57  lr: 0.000007  min_lr: 0.000000  loss: 5.0749 (5.1264)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1354  max mem: 15572
Epoch: [0]  [ 570/1404]  eta: 0:08:50  lr: 0.000008  min_lr: 0.000000  loss: 5.0551 (5.1254)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.1403  max mem: 15572
Epoch: [0]  [ 580/1404]  eta: 0:08:45  lr: 0.000008  min_lr: 0.000000  loss: 5.0525 (5.1244)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1180  max mem: 15572
Epoch: [0]  [ 590/1404]  eta: 0:08:37  lr: 0.000008  min_lr: 0.000000  loss: 5.0561 (5.1232)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0837  max mem: 15572
Epoch: [0]  [ 600/1404]  eta: 0:08:32  lr: 0.000008  min_lr: 0.000000  loss: 5.0506 (5.1219)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1078  max mem: 15572
Epoch: [0]  [ 610/1404]  eta: 0:08:25  lr: 0.000008  min_lr: 0.000000  loss: 5.0544 (5.1212)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1352  max mem: 15572
Epoch: [0]  [ 620/1404]  eta: 0:08:20  lr: 0.000008  min_lr: 0.000000  loss: 5.0772 (5.1204)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.1699  max mem: 15572
Epoch: [0]  [ 630/1404]  eta: 0:08:13  lr: 0.000008  min_lr: 0.000000  loss: 5.0539 (5.1193)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.1889  max mem: 15572
[2025-01-10 15:44:16,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:44:16,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/1404]  eta: 0:08:06  lr: 0.000009  min_lr: 0.000000  loss: 5.0800 (5.1187)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0794  max mem: 15572
[2025-01-10 15:44:16,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:44:16,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 650/1404]  eta: 0:07:59  lr: 0.000009  min_lr: 0.000000  loss: 5.0812 (5.1178)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0421  max mem: 15572
Epoch: [0]  [ 660/1404]  eta: 0:07:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0291 (5.1162)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0580  max mem: 15572
Epoch: [0]  [ 670/1404]  eta: 0:07:46  lr: 0.000009  min_lr: 0.000000  loss: 5.0094 (5.1152)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1048  max mem: 15572
Epoch: [0]  [ 680/1404]  eta: 0:07:40  lr: 0.000009  min_lr: 0.000000  loss: 5.0184 (5.1140)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.1554  max mem: 15572
Epoch: [0]  [ 690/1404]  eta: 0:07:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0184 (5.1130)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0876  max mem: 15572
Epoch: [0]  [ 700/1404]  eta: 0:07:26  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.1121)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0504  max mem: 15572
Epoch: [0]  [ 710/1404]  eta: 0:07:20  lr: 0.000009  min_lr: 0.000000  loss: 5.0339 (5.1110)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1475  max mem: 15572
Epoch: [0]  [ 720/1404]  eta: 0:07:14  lr: 0.000010  min_lr: 0.000000  loss: 5.0251 (5.1100)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.1728  max mem: 15572
Epoch: [0]  [ 730/1404]  eta: 0:07:06  lr: 0.000010  min_lr: 0.000000  loss: 5.0358 (5.1094)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0757  max mem: 15572
Epoch: [0]  [ 740/1404]  eta: 0:06:59  lr: 0.000010  min_lr: 0.000000  loss: 5.0506 (5.1088)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0008  max mem: 15572
Epoch: [0]  [ 750/1404]  eta: 0:06:52  lr: 0.000010  min_lr: 0.000000  loss: 5.0349 (5.1073)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0335  max mem: 15572
Epoch: [0]  [ 760/1404]  eta: 0:06:47  lr: 0.000010  min_lr: 0.000000  loss: 5.0390 (5.1068)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1331  max mem: 15572
[2025-01-10 15:45:33,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:45:33,717] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2025-01-10 15:45:33,722] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:45:33,723] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/1404]  eta: 0:06:40  lr: 0.000010  min_lr: 0.000000  loss: 5.0466 (5.1059)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.1568  max mem: 15572
Epoch: [0]  [ 780/1404]  eta: 0:06:33  lr: 0.000010  min_lr: 0.000000  loss: 5.0185 (5.1047)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0575  max mem: 15572
Epoch: [0]  [ 790/1404]  eta: 0:06:26  lr: 0.000011  min_lr: 0.000000  loss: 5.0088 (5.1036)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0247  max mem: 15572
Epoch: [0]  [ 800/1404]  eta: 0:06:20  lr: 0.000011  min_lr: 0.000000  loss: 4.9854 (5.1023)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0917  max mem: 15572
Epoch: [0]  [ 810/1404]  eta: 0:06:14  lr: 0.000011  min_lr: 0.000000  loss: 5.0443 (5.1017)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1348  max mem: 15572
Epoch: [0]  [ 820/1404]  eta: 0:06:08  lr: 0.000011  min_lr: 0.000000  loss: 5.0387 (5.1006)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.1517  max mem: 15572
Epoch: [0]  [ 830/1404]  eta: 0:06:01  lr: 0.000011  min_lr: 0.000000  loss: 4.9705 (5.0993)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1285  max mem: 15572
Epoch: [0]  [ 840/1404]  eta: 0:05:55  lr: 0.000011  min_lr: 0.000000  loss: 5.0180 (5.0986)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0760  max mem: 15572
Epoch: [0]  [ 850/1404]  eta: 0:05:49  lr: 0.000011  min_lr: 0.000000  loss: 5.0180 (5.0978)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0820  max mem: 15572
Epoch: [0]  [ 860/1404]  eta: 0:05:42  lr: 0.000011  min_lr: 0.000000  loss: 5.0143 (5.0966)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0915  max mem: 15572
Epoch: [0]  [ 870/1404]  eta: 0:05:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0040 (5.0955)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0931  max mem: 15572
Epoch: [0]  [ 880/1404]  eta: 0:05:29  lr: 0.000012  min_lr: 0.000000  loss: 4.9828 (5.0947)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0995  max mem: 15572
Epoch: [0]  [ 890/1404]  eta: 0:05:23  lr: 0.000012  min_lr: 0.000000  loss: 4.9824 (5.0936)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1138  max mem: 15572
[2025-01-10 15:46:53,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:46:53,459] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2025-01-10 15:46:53,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:46:53,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/1404]  eta: 0:05:18  lr: 0.000012  min_lr: 0.000000  loss: 5.0013 (5.0932)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.1573  max mem: 15572
Epoch: [0]  [ 910/1404]  eta: 0:05:11  lr: 0.000012  min_lr: 0.000000  loss: 5.0432 (5.0926)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1044  max mem: 15572
Epoch: [0]  [ 920/1404]  eta: 0:05:04  lr: 0.000012  min_lr: 0.000000  loss: 5.0424 (5.0924)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0497  max mem: 15572
Epoch: [0]  [ 930/1404]  eta: 0:04:58  lr: 0.000012  min_lr: 0.000000  loss: 5.0337 (5.0920)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1194  max mem: 15572
Epoch: [0]  [ 940/1404]  eta: 0:04:52  lr: 0.000013  min_lr: 0.000000  loss: 5.0094 (5.0910)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1375  max mem: 15572
Epoch: [0]  [ 950/1404]  eta: 0:04:45  lr: 0.000013  min_lr: 0.000000  loss: 5.0094 (5.0905)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0554  max mem: 15572
Epoch: [0]  [ 960/1404]  eta: 0:04:38  lr: 0.000013  min_lr: 0.000000  loss: 5.0361 (5.0899)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0281  max mem: 15572
Epoch: [0]  [ 970/1404]  eta: 0:04:33  lr: 0.000013  min_lr: 0.000000  loss: 5.0396 (5.0890)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.1783  max mem: 15572
Epoch: [0]  [ 980/1404]  eta: 0:04:26  lr: 0.000013  min_lr: 0.000000  loss: 5.0270 (5.0883)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1827  max mem: 15572
Epoch: [0]  [ 990/1404]  eta: 0:04:20  lr: 0.000013  min_lr: 0.000000  loss: 5.0270 (5.0879)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0991  max mem: 15572
[2025-01-10 15:47:57,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.2928139878801235e-07, 1.2928139878801235e-07, 1.8468771255430337e-07, 1.8468771255430337e-07, 2.638395893632906e-07, 2.638395893632906e-07, 3.7691369909041513e-07, 3.7691369909041513e-07, 5.384481415577359e-07, 5.384481415577359e-07, 7.692116307967656e-07, 7.692116307967656e-07, 1.0988737582810937e-06, 1.0988737582810937e-06, 1.569819654687277e-06, 1.569819654687277e-06, 2.24259950669611e-06, 2.24259950669611e-06, 3.2037135809944434e-06, 3.2037135809944434e-06, 4.5767336871349184e-06, 4.5767336871349184e-06, 6.538190981621313e-06, 6.538190981621313e-06, 9.34027283088759e-06, 9.34027283088759e-06, 1.3343246901267988e-05, 1.3343246901267988e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 15:47:57,307] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=48.92386038776496, CurrSamplesPerSec=43.36206352898607, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/1404]  eta: 0:04:13  lr: 0.000013  min_lr: 0.000000  loss: 5.0556 (5.0875)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1198  max mem: 15572
Epoch: [0]  [1010/1404]  eta: 0:04:07  lr: 0.000013  min_lr: 0.000000  loss: 5.0640 (5.0872)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0823  max mem: 15572
Epoch: [0]  [1020/1404]  eta: 0:04:01  lr: 0.000014  min_lr: 0.000000  loss: 5.0430 (5.0863)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0890  max mem: 15572
[2025-01-10 15:48:11,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:48:11,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2025-01-10 15:48:12,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:48:12,025] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/1404]  eta: 0:03:54  lr: 0.000014  min_lr: 0.000000  loss: 4.9590 (5.0852)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1052  max mem: 15572
Epoch: [0]  [1040/1404]  eta: 0:03:48  lr: 0.000014  min_lr: 0.000000  loss: 4.9700 (5.0848)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1195  max mem: 15572
Epoch: [0]  [1050/1404]  eta: 0:03:42  lr: 0.000014  min_lr: 0.000000  loss: 5.0628 (5.0842)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1396  max mem: 15572
Epoch: [0]  [1060/1404]  eta: 0:03:35  lr: 0.000014  min_lr: 0.000000  loss: 5.0158 (5.0834)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0895  max mem: 15572
Epoch: [0]  [1070/1404]  eta: 0:03:29  lr: 0.000014  min_lr: 0.000000  loss: 4.9793 (5.0826)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0836  max mem: 15572
Epoch: [0]  [1080/1404]  eta: 0:03:22  lr: 0.000014  min_lr: 0.000000  loss: 5.0309 (5.0819)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0661  max mem: 15572
Epoch: [0]  [1090/1404]  eta: 0:03:16  lr: 0.000015  min_lr: 0.000000  loss: 5.0742 (5.0823)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0484  max mem: 15572
Epoch: [0]  [1100/1404]  eta: 0:03:10  lr: 0.000015  min_lr: 0.000000  loss: 5.0284 (5.0813)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0727  max mem: 15572
Epoch: [0]  [1110/1404]  eta: 0:03:04  lr: 0.000015  min_lr: 0.000000  loss: 4.9885 (5.0801)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1283  max mem: 15572
Epoch: [0]  [1120/1404]  eta: 0:02:57  lr: 0.000015  min_lr: 0.000000  loss: 4.9556 (5.0791)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1296  max mem: 15572
Epoch: [0]  [1130/1404]  eta: 0:02:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9694 (5.0784)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0766  max mem: 15572
Epoch: [0]  [1140/1404]  eta: 0:02:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9774 (5.0776)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0857  max mem: 15572
Epoch: [0]  [1150/1404]  eta: 0:02:39  lr: 0.000015  min_lr: 0.000000  loss: 4.9774 (5.0766)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.1380  max mem: 15572
[2025-01-10 15:49:31,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:49:31,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2025-01-10 15:49:31,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:49:31,297] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/1404]  eta: 0:02:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9714 (5.0759)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0975  max mem: 15572
Epoch: [0]  [1170/1404]  eta: 0:02:26  lr: 0.000016  min_lr: 0.000000  loss: 5.0212 (5.0756)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0820  max mem: 15572
Epoch: [0]  [1180/1404]  eta: 0:02:20  lr: 0.000016  min_lr: 0.000000  loss: 5.0263 (5.0750)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1153  max mem: 15572
Epoch: [0]  [1190/1404]  eta: 0:02:13  lr: 0.000016  min_lr: 0.000000  loss: 5.0165 (5.0745)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0876  max mem: 15572
Epoch: [0]  [1200/1404]  eta: 0:02:07  lr: 0.000016  min_lr: 0.000000  loss: 4.9971 (5.0739)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1026  max mem: 15572
Epoch: [0]  [1210/1404]  eta: 0:02:01  lr: 0.000016  min_lr: 0.000000  loss: 4.9716 (5.0730)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1077  max mem: 15572
Epoch: [0]  [1220/1404]  eta: 0:01:55  lr: 0.000016  min_lr: 0.000000  loss: 4.9630 (5.0724)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0829  max mem: 15572
Epoch: [0]  [1230/1404]  eta: 0:01:48  lr: 0.000016  min_lr: 0.000000  loss: 4.9570 (5.0718)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0484  max mem: 15572
Epoch: [0]  [1240/1404]  eta: 0:01:42  lr: 0.000017  min_lr: 0.000000  loss: 4.9424 (5.0710)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0799  max mem: 15572
Epoch: [0]  [1250/1404]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9991 (5.0704)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1116  max mem: 15572
Epoch: [0]  [1260/1404]  eta: 0:01:29  lr: 0.000017  min_lr: 0.000000  loss: 4.9991 (5.0696)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1001  max mem: 15572
Epoch: [0]  [1270/1404]  eta: 0:01:23  lr: 0.000017  min_lr: 0.000000  loss: 4.9727 (5.0689)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1028  max mem: 15572
[2025-01-10 15:50:49,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:50:49,416] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2025-01-10 15:50:49,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:50:49,520] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/1404]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 4.9929 (5.0689)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1103  max mem: 15572
Epoch: [0]  [1290/1404]  eta: 0:01:11  lr: 0.000017  min_lr: 0.000000  loss: 5.0127 (5.0682)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0981  max mem: 15572
Epoch: [0]  [1300/1404]  eta: 0:01:04  lr: 0.000017  min_lr: 0.000000  loss: 5.0070 (5.0680)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1021  max mem: 15572
Epoch: [0]  [1310/1404]  eta: 0:00:58  lr: 0.000017  min_lr: 0.000000  loss: 5.0401 (5.0674)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0999  max mem: 15572
Epoch: [0]  [1320/1404]  eta: 0:00:52  lr: 0.000018  min_lr: 0.000000  loss: 5.0500 (5.0672)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0749  max mem: 15572
Epoch: [0]  [1330/1404]  eta: 0:00:46  lr: 0.000018  min_lr: 0.000000  loss: 5.0718 (5.0670)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0920  max mem: 15572
Epoch: [0]  [1340/1404]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 5.0077 (5.0666)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1075  max mem: 15572
Epoch: [0]  [1350/1404]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 5.0077 (5.0663)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.1462  max mem: 15572
Epoch: [0]  [1360/1404]  eta: 0:00:27  lr: 0.000018  min_lr: 0.000000  loss: 5.0087 (5.0660)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1149  max mem: 15572
Epoch: [0]  [1370/1404]  eta: 0:00:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9893 (5.0653)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1005  max mem: 15572
Epoch: [0]  [1380/1404]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9963 (5.0649)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.1742  max mem: 15572
Epoch: [0]  [1390/1404]  eta: 0:00:08  lr: 0.000019  min_lr: 0.000000  loss: 5.0110 (5.0645)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0888  max mem: 15572
Epoch: [0]  [1400/1404]  eta: 0:00:02  lr: 0.000019  min_lr: 0.000000  loss: 4.9659 (5.0638)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.4341  data: 0.0004  max mem: 15572
Epoch: [0]  [1403/1404]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 5.0053 (5.0638)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)  time: 0.4030  data: 0.0003  max mem: 15572
Epoch: [0] Total time: 0:14:33 (0.6222 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 5.0053 (5.0628)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:02  loss: 5.1042 (5.1042)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.7534  data: 5.4531  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 5.1506 (5.0705)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8112  data: 0.6027  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 5.0167 (5.0084)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3083  data: 0.1112  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 4.8438 (5.0142)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2986  data: 0.1040  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 4.9481 (4.9528)  acc1: 0.0000 (4.7425)  acc5: 0.0000 (8.9431)  time: 0.3428  data: 0.1431  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 5.0473 (5.0219)  acc1: 0.0000 (3.8126)  acc5: 0.0000 (7.1895)  time: 0.3655  data: 0.1631  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 5.2331 (5.0668)  acc1: 0.0000 (3.1876)  acc5: 0.0000 (6.0109)  time: 0.3758  data: 0.1678  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 5.0510 (5.0418)  acc1: 0.0000 (2.7387)  acc5: 0.0000 (8.9984)  time: 0.3909  data: 0.1751  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 4.7027 (5.0005)  acc1: 0.0000 (2.4005)  acc5: 0.0000 (9.9451)  time: 0.4257  data: 0.2099  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 4.9924 (5.0136)  acc1: 0.0000 (2.1368)  acc5: 0.0000 (8.8523)  time: 0.3911  data: 0.1768  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 5.0938 (5.0343)  acc1: 0.0000 (1.9252)  acc5: 0.0000 (7.9758)  time: 0.3004  data: 0.0764  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 5.0315 (5.0209)  acc1: 0.0000 (1.7518)  acc5: 0.0000 (7.2573)  time: 0.3862  data: 0.1592  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7658 (5.0096)  acc1: 0.0000 (1.6070)  acc5: 0.0000 (6.6575)  time: 0.4072  data: 0.1900  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.9219 (5.0257)  acc1: 0.0000 (1.4843)  acc5: 0.0000 (6.1493)  time: 0.2554  data: 0.0671  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 5.2168 (5.0267)  acc1: 0.0000 (1.4333)  acc5: 0.0000 (5.9378)  time: 0.2343  data: 0.0667  max mem: 15572
Val: Total time: 0:00:51 (0.3791 s / it)
* Acc@1 1.454 Acc@5 5.938 loss 5.026
Accuracy of the network on the 4883 val videos: 1.5%
[2025-01-10 15:52:53,978] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-10 15:52:53,980] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 15:52:53,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 15:52:53,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 15:52:56,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 15:52:56,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.45%
Epoch: [1]  [   0/1404]  eta: 3:35:59  lr: 0.000019  min_lr: 0.000000  loss: 5.0225 (5.0225)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2301  data: 8.7981  max mem: 15572
[2025-01-10 15:53:07,764] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:53:07,764] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
[2025-01-10 15:53:07,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:53:07,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [1]  [  10/1404]  eta: 0:31:20  lr: 0.000019  min_lr: 0.000000  loss: 5.0225 (5.0036)  loss_scale: 262144.0000 (214481.4545)  weight_decay: 0.0500 (0.0500)  time: 1.3488  data: 0.8006  max mem: 15572
[2025-01-10 15:53:15,424] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-10 15:53:15,424] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-10 15:53:15,424] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-10 15:53:15,425] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-10 15:53:15,425] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [1]  [  20/1404]  eta: 0:21:57  lr: 0.000019  min_lr: 0.000000  loss: 4.9886 (4.9903)  loss_scale: 262144.0000 (218453.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0008  max mem: 15572
Epoch: [1]  [  30/1404]  eta: 0:19:18  lr: 0.000019  min_lr: 0.000000  loss: 4.9750 (4.9995)  loss_scale: 131072.0000 (190265.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0008  max mem: 15572
Epoch: [1]  [  40/1404]  eta: 0:17:40  lr: 0.000019  min_lr: 0.000000  loss: 4.9750 (5.0030)  loss_scale: 131072.0000 (175828.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0449  max mem: 15572
Epoch: [1]  [  50/1404]  eta: 0:17:02  lr: 0.000019  min_lr: 0.000000  loss: 4.9686 (4.9894)  loss_scale: 131072.0000 (167052.5490)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1043  max mem: 15572
Epoch: [1]  [  60/1404]  eta: 0:16:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9665 (4.9843)  loss_scale: 131072.0000 (161154.0984)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.1740  max mem: 15572
Epoch: [1]  [  70/1404]  eta: 0:16:00  lr: 0.000020  min_lr: 0.000000  loss: 4.9987 (4.9955)  loss_scale: 131072.0000 (156917.1831)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1398  max mem: 15572
Epoch: [1]  [  80/1404]  eta: 0:15:44  lr: 0.000020  min_lr: 0.000000  loss: 5.0141 (4.9980)  loss_scale: 131072.0000 (153726.4198)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1055  max mem: 15572
Epoch: [1]  [  90/1404]  eta: 0:15:09  lr: 0.000020  min_lr: 0.000000  loss: 4.9973 (5.0018)  loss_scale: 131072.0000 (151236.9231)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0914  max mem: 15572
Epoch: [1]  [ 100/1404]  eta: 0:14:53  lr: 0.000020  min_lr: 0.000000  loss: 5.0166 (5.0014)  loss_scale: 131072.0000 (149240.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0560  max mem: 15572
Epoch: [1]  [ 110/1404]  eta: 0:14:38  lr: 0.000020  min_lr: 0.000000  loss: 4.9969 (4.9965)  loss_scale: 131072.0000 (147603.6036)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0810  max mem: 15572
Epoch: [1]  [ 120/1404]  eta: 0:14:24  lr: 0.000020  min_lr: 0.000000  loss: 5.0226 (5.0042)  loss_scale: 131072.0000 (146237.3554)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0731  max mem: 15572
Epoch: [1]  [ 130/1404]  eta: 0:14:10  lr: 0.000020  min_lr: 0.000000  loss: 5.0782 (5.0070)  loss_scale: 131072.0000 (145079.6947)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0790  max mem: 15572
Epoch: [1]  [ 140/1404]  eta: 0:13:52  lr: 0.000021  min_lr: 0.000000  loss: 5.0786 (5.0104)  loss_scale: 131072.0000 (144086.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0559  max mem: 15572
[2025-01-10 15:54:33,056] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:54:33,056] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-10 15:54:33,085] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:54:33,086] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-10 15:54:33,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-10 15:54:33,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-10 15:54:33,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-10 15:54:33,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-10 15:54:33,717] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [1]  [ 150/1404]  eta: 0:13:39  lr: 0.000021  min_lr: 0.000000  loss: 5.0652 (5.0137)  loss_scale: 131072.0000 (144092.3974)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0454  max mem: 15572
Epoch: [1]  [ 160/1404]  eta: 0:13:35  lr: 0.000021  min_lr: 0.000000  loss: 4.9734 (5.0100)  loss_scale: 131072.0000 (143283.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1030  max mem: 15572
Epoch: [1]  [ 170/1404]  eta: 0:13:26  lr: 0.000021  min_lr: 0.000000  loss: 4.9148 (5.0034)  loss_scale: 131072.0000 (142569.5439)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.1310  max mem: 15572
Epoch: [1]  [ 180/1404]  eta: 0:13:18  lr: 0.000021  min_lr: 0.000000  loss: 4.9361 (5.0030)  loss_scale: 131072.0000 (141934.3204)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1125  max mem: 15572
Epoch: [1]  [ 190/1404]  eta: 0:13:11  lr: 0.000021  min_lr: 0.000000  loss: 5.0448 (5.0038)  loss_scale: 131072.0000 (141365.6126)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1236  max mem: 15572
[2025-01-10 15:55:02,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1597
[2025-01-10 15:55:02,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:55:02,623] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1597
[2025-01-10 15:55:02,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:55:02,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 200/1404]  eta: 0:12:59  lr: 0.000021  min_lr: 0.000000  loss: 5.0127 (5.0029)  loss_scale: 131072.0000 (138245.0945)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1045  max mem: 15572
Epoch: [1]  [ 210/1404]  eta: 0:12:50  lr: 0.000022  min_lr: 0.000000  loss: 5.0090 (5.0026)  loss_scale: 65536.0000 (134799.1659)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0737  max mem: 15572
Epoch: [1]  [ 220/1404]  eta: 0:12:41  lr: 0.000022  min_lr: 0.000000  loss: 5.0197 (5.0030)  loss_scale: 65536.0000 (131665.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0734  max mem: 15572
Epoch: [1]  [ 230/1404]  eta: 0:12:34  lr: 0.000022  min_lr: 0.000000  loss: 5.0061 (5.0020)  loss_scale: 65536.0000 (128802.3550)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0939  max mem: 15572
Epoch: [1]  [ 240/1404]  eta: 0:12:29  lr: 0.000022  min_lr: 0.000000  loss: 5.0031 (5.0032)  loss_scale: 65536.0000 (126177.1950)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.1212  max mem: 15572
Epoch: [1]  [ 250/1404]  eta: 0:12:22  lr: 0.000022  min_lr: 0.000000  loss: 4.9444 (5.0010)  loss_scale: 65536.0000 (123761.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.1237  max mem: 15572
Epoch: [1]  [ 260/1404]  eta: 0:12:20  lr: 0.000022  min_lr: 0.000000  loss: 4.9386 (4.9999)  loss_scale: 65536.0000 (121530.3602)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1799  max mem: 15572
Epoch: [1]  [ 270/1404]  eta: 0:12:10  lr: 0.000022  min_lr: 0.000000  loss: 5.0329 (5.0011)  loss_scale: 65536.0000 (119464.1476)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.1488  max mem: 15572
Epoch: [1]  [ 280/1404]  eta: 0:12:08  lr: 0.000022  min_lr: 0.000000  loss: 5.0566 (5.0048)  loss_scale: 65536.0000 (117544.9964)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.1479  max mem: 15572
Epoch: [1]  [ 290/1404]  eta: 0:11:56  lr: 0.000023  min_lr: 0.000000  loss: 5.0581 (5.0036)  loss_scale: 65536.0000 (115757.7457)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1163  max mem: 15572
Epoch: [1]  [ 300/1404]  eta: 0:11:47  lr: 0.000023  min_lr: 0.000000  loss: 5.0011 (5.0031)  loss_scale: 65536.0000 (114089.2492)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0371  max mem: 15572
Epoch: [1]  [ 310/1404]  eta: 0:11:37  lr: 0.000023  min_lr: 0.000000  loss: 4.9791 (5.0022)  loss_scale: 65536.0000 (112528.0514)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0445  max mem: 15572
Epoch: [1]  [ 320/1404]  eta: 0:11:30  lr: 0.000023  min_lr: 0.000000  loss: 5.0287 (5.0052)  loss_scale: 65536.0000 (111064.1246)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0586  max mem: 15572
[2025-01-10 15:56:22,095] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:56:22,096] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 15:56:22,128] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:56:22,128] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 330/1404]  eta: 0:11:22  lr: 0.000023  min_lr: 0.000000  loss: 5.0273 (5.0052)  loss_scale: 65536.0000 (111470.5982)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0928  max mem: 15572
[2025-01-10 15:56:30,089] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1740
[2025-01-10 15:56:30,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:56:30,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 15:56:30,095] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1740
[2025-01-10 15:56:30,096] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [1]  [ 340/1404]  eta: 0:11:17  lr: 0.000023  min_lr: 0.000000  loss: 5.0005 (5.0073)  loss_scale: 131072.0000 (111084.4809)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1248  max mem: 15572
Epoch: [1]  [ 350/1404]  eta: 0:11:12  lr: 0.000023  min_lr: 0.000000  loss: 5.0611 (5.0087)  loss_scale: 65536.0000 (109786.8034)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.1236  max mem: 15572
Epoch: [1]  [ 360/1404]  eta: 0:11:02  lr: 0.000024  min_lr: 0.000000  loss: 4.9738 (5.0072)  loss_scale: 65536.0000 (108561.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0495  max mem: 15572
Epoch: [1]  [ 370/1404]  eta: 0:10:54  lr: 0.000024  min_lr: 0.000000  loss: 4.9379 (5.0053)  loss_scale: 65536.0000 (107401.3154)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0094  max mem: 15572
Epoch: [1]  [ 380/1404]  eta: 0:10:48  lr: 0.000024  min_lr: 0.000000  loss: 4.9764 (5.0055)  loss_scale: 65536.0000 (106302.4882)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0155  max mem: 15572
Epoch: [1]  [ 390/1404]  eta: 0:10:41  lr: 0.000024  min_lr: 0.000000  loss: 5.0080 (5.0041)  loss_scale: 65536.0000 (105259.8670)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0154  max mem: 15572
Epoch: [1]  [ 400/1404]  eta: 0:10:34  lr: 0.000024  min_lr: 0.000000  loss: 4.9609 (5.0031)  loss_scale: 65536.0000 (104269.2469)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0008  max mem: 15572
Epoch: [1]  [ 410/1404]  eta: 0:10:27  lr: 0.000024  min_lr: 0.000000  loss: 4.9932 (5.0039)  loss_scale: 65536.0000 (103326.8321)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0007  max mem: 15572
Epoch: [1]  [ 420/1404]  eta: 0:10:21  lr: 0.000024  min_lr: 0.000000  loss: 5.0247 (5.0049)  loss_scale: 65536.0000 (102429.1876)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.0009  max mem: 15572
Epoch: [1]  [ 430/1404]  eta: 0:10:13  lr: 0.000024  min_lr: 0.000000  loss: 5.0231 (5.0044)  loss_scale: 65536.0000 (101573.1972)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0009  max mem: 15572
Epoch: [1]  [ 440/1404]  eta: 0:10:05  lr: 0.000025  min_lr: 0.000000  loss: 5.0231 (5.0042)  loss_scale: 65536.0000 (100756.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0007  max mem: 15572
Epoch: [1]  [ 450/1404]  eta: 0:09:58  lr: 0.000025  min_lr: 0.000000  loss: 5.0180 (5.0043)  loss_scale: 65536.0000 (99975.0953)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0348  max mem: 15572
Epoch: [1]  [ 460/1404]  eta: 0:09:52  lr: 0.000025  min_lr: 0.000000  loss: 4.9908 (5.0038)  loss_scale: 65536.0000 (99228.0434)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0461  max mem: 15572
[2025-01-10 15:57:49,001] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:57:49,002] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 15:57:49,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:57:49,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 15:57:49,491] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1870
[2025-01-10 15:57:49,491] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:57:49,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1870
[2025-01-10 15:57:49,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:57:49,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 470/1404]  eta: 0:09:47  lr: 0.000025  min_lr: 0.000000  loss: 4.9798 (5.0036)  loss_scale: 65536.0000 (98651.8556)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.0324  max mem: 15572
Epoch: [1]  [ 480/1404]  eta: 0:09:40  lr: 0.000025  min_lr: 0.000000  loss: 4.9741 (5.0023)  loss_scale: 65536.0000 (97963.3763)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.0212  max mem: 15572
Epoch: [1]  [ 490/1404]  eta: 0:09:33  lr: 0.000025  min_lr: 0.000000  loss: 4.9147 (5.0017)  loss_scale: 65536.0000 (97302.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0007  max mem: 15572
Epoch: [1]  [ 500/1404]  eta: 0:09:27  lr: 0.000025  min_lr: 0.000000  loss: 5.0143 (5.0010)  loss_scale: 65536.0000 (96668.8703)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0006  max mem: 15572
Epoch: [1]  [ 510/1404]  eta: 0:09:21  lr: 0.000026  min_lr: 0.000000  loss: 5.0493 (5.0019)  loss_scale: 65536.0000 (96059.6164)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0009  max mem: 15572
Epoch: [1]  [ 520/1404]  eta: 0:09:15  lr: 0.000026  min_lr: 0.000000  loss: 4.9826 (5.0006)  loss_scale: 65536.0000 (95473.7505)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0009  max mem: 15572
Epoch: [1]  [ 530/1404]  eta: 0:09:09  lr: 0.000026  min_lr: 0.000000  loss: 4.9491 (4.9996)  loss_scale: 65536.0000 (94909.9510)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.0006  max mem: 15572
Epoch: [1]  [ 540/1404]  eta: 0:09:01  lr: 0.000026  min_lr: 0.000000  loss: 4.9554 (4.9989)  loss_scale: 65536.0000 (94366.9945)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0010  max mem: 15572
Epoch: [1]  [ 550/1404]  eta: 0:08:56  lr: 0.000026  min_lr: 0.000000  loss: 4.9579 (4.9982)  loss_scale: 65536.0000 (93843.7459)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0012  max mem: 15572
Epoch: [1]  [ 560/1404]  eta: 0:08:49  lr: 0.000026  min_lr: 0.000000  loss: 4.9918 (4.9992)  loss_scale: 65536.0000 (93339.1515)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0008  max mem: 15572
Epoch: [1]  [ 570/1404]  eta: 0:08:42  lr: 0.000026  min_lr: 0.000000  loss: 4.9462 (4.9978)  loss_scale: 65536.0000 (92852.2312)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0007  max mem: 15572
Epoch: [1]  [ 580/1404]  eta: 0:08:35  lr: 0.000026  min_lr: 0.000000  loss: 4.9224 (4.9979)  loss_scale: 65536.0000 (92382.0723)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0008  max mem: 15572
Epoch: [1]  [ 590/1404]  eta: 0:08:29  lr: 0.000027  min_lr: 0.000000  loss: 4.9867 (4.9973)  loss_scale: 65536.0000 (91927.8240)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0009  max mem: 15572
[2025-01-10 15:59:09,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:59:09,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 15:59:09,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 15:59:09,144] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 15:59:09,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[2.586922083856223e-07, 2.586922083856223e-07, 3.695602976937462e-07, 3.695602976937462e-07, 5.279432824196375e-07, 5.279432824196375e-07, 7.542046891709107e-07, 7.542046891709107e-07, 1.0774352702441582e-06, 1.0774352702441582e-06, 1.5391932432059404e-06, 1.5391932432059404e-06, 2.1988474902942005e-06, 2.1988474902942005e-06, 3.141210700420287e-06, 3.141210700420287e-06, 4.487443857743267e-06, 4.487443857743267e-06, 6.410634082490383e-06, 6.410634082490383e-06, 9.158048689271974e-06, 9.158048689271974e-06, 1.3082926698959966e-05, 1.3082926698959966e-05, 1.8689895284228524e-05, 1.8689895284228524e-05, 2.669985040604075e-05, 2.669985040604075e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 15:59:09,157] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=48.08998134483233, CurrSamplesPerSec=51.87503401192272, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 15:59:10,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2000
[2025-01-10 15:59:10,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:59:10,026] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 15:59:10,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2000
[2025-01-10 15:59:10,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 15:59:11,636] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2003
[2025-01-10 15:59:11,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 15:59:11,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2003
[2025-01-10 15:59:11,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 15:59:11,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [ 600/1404]  eta: 0:08:22  lr: 0.000027  min_lr: 0.000000  loss: 4.9442 (4.9963)  loss_scale: 65536.0000 (91488.6922)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0009  max mem: 15572
Epoch: [1]  [ 610/1404]  eta: 0:08:15  lr: 0.000027  min_lr: 0.000000  loss: 4.9446 (4.9956)  loss_scale: 32768.0000 (90527.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0012  max mem: 15572
Epoch: [1]  [ 620/1404]  eta: 0:08:10  lr: 0.000027  min_lr: 0.000000  loss: 4.9940 (4.9962)  loss_scale: 32768.0000 (89597.5266)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.0011  max mem: 15572
Epoch: [1]  [ 630/1404]  eta: 0:08:03  lr: 0.000027  min_lr: 0.000000  loss: 4.9839 (4.9956)  loss_scale: 32768.0000 (88696.9002)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.0007  max mem: 15572
Epoch: [1]  [ 640/1404]  eta: 0:07:56  lr: 0.000027  min_lr: 0.000000  loss: 4.9653 (4.9950)  loss_scale: 32768.0000 (87824.3744)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0008  max mem: 15572
Epoch: [1]  [ 650/1404]  eta: 0:07:50  lr: 0.000027  min_lr: 0.000000  loss: 4.9859 (4.9963)  loss_scale: 32768.0000 (86978.6544)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0011  max mem: 15572
Epoch: [1]  [ 660/1404]  eta: 0:07:43  lr: 0.000028  min_lr: 0.000000  loss: 4.9821 (4.9952)  loss_scale: 32768.0000 (86158.5234)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0010  max mem: 15572
Epoch: [1]  [ 670/1404]  eta: 0:07:36  lr: 0.000028  min_lr: 0.000000  loss: 4.9637 (4.9950)  loss_scale: 32768.0000 (85362.8376)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0009  max mem: 15572
Epoch: [1]  [ 680/1404]  eta: 0:07:30  lr: 0.000028  min_lr: 0.000000  loss: 4.9965 (4.9945)  loss_scale: 32768.0000 (84590.5198)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0013  max mem: 15572
Epoch: [1]  [ 690/1404]  eta: 0:07:23  lr: 0.000028  min_lr: 0.000000  loss: 4.9848 (4.9932)  loss_scale: 32768.0000 (83840.5557)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0012  max mem: 15572
Epoch: [1]  [ 700/1404]  eta: 0:07:16  lr: 0.000028  min_lr: 0.000000  loss: 4.9248 (4.9924)  loss_scale: 32768.0000 (83111.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0009  max mem: 15572
Epoch: [1]  [ 710/1404]  eta: 0:07:11  lr: 0.000028  min_lr: 0.000000  loss: 4.9636 (4.9914)  loss_scale: 32768.0000 (82403.9156)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0012  max mem: 15572
Epoch: [1]  [ 720/1404]  eta: 0:07:05  lr: 0.000028  min_lr: 0.000000  loss: 5.0284 (4.9930)  loss_scale: 32768.0000 (81715.4840)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.0013  max mem: 15572
[2025-01-10 16:00:30,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:00:30,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:00:30,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:00:30,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 730/1404]  eta: 0:06:58  lr: 0.000029  min_lr: 0.000000  loss: 5.0067 (4.9921)  loss_scale: 32768.0000 (81180.3666)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0032  max mem: 15572
Epoch: [1]  [ 740/1404]  eta: 0:06:52  lr: 0.000029  min_lr: 0.000000  loss: 4.8805 (4.9907)  loss_scale: 65536.0000 (80969.2416)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0032  max mem: 15572
Epoch: [1]  [ 750/1404]  eta: 0:06:46  lr: 0.000029  min_lr: 0.000000  loss: 4.9335 (4.9903)  loss_scale: 65536.0000 (80763.7390)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0301  max mem: 15572
Epoch: [1]  [ 760/1404]  eta: 0:06:40  lr: 0.000029  min_lr: 0.000000  loss: 4.9335 (4.9891)  loss_scale: 65536.0000 (80563.6373)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0300  max mem: 15572
Epoch: [1]  [ 770/1404]  eta: 0:06:34  lr: 0.000029  min_lr: 0.000000  loss: 4.9113 (4.9884)  loss_scale: 65536.0000 (80368.7263)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.0010  max mem: 15572
Epoch: [1]  [ 780/1404]  eta: 0:06:27  lr: 0.000029  min_lr: 0.000000  loss: 4.9550 (4.9886)  loss_scale: 65536.0000 (80178.8067)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0009  max mem: 15572
Epoch: [1]  [ 790/1404]  eta: 0:06:21  lr: 0.000029  min_lr: 0.000000  loss: 4.9889 (4.9881)  loss_scale: 65536.0000 (79993.6890)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0243  max mem: 15572
Epoch: [1]  [ 800/1404]  eta: 0:06:15  lr: 0.000029  min_lr: 0.000000  loss: 4.9889 (4.9884)  loss_scale: 65536.0000 (79813.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0767  max mem: 15572
Epoch: [1]  [ 810/1404]  eta: 0:06:09  lr: 0.000030  min_lr: 0.000000  loss: 4.9248 (4.9874)  loss_scale: 65536.0000 (79637.1492)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.0532  max mem: 15572
Epoch: [1]  [ 820/1404]  eta: 0:06:04  lr: 0.000030  min_lr: 0.000000  loss: 4.8920 (4.9865)  loss_scale: 65536.0000 (79465.3934)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.0008  max mem: 15572
Epoch: [1]  [ 830/1404]  eta: 0:05:57  lr: 0.000030  min_lr: 0.000000  loss: 4.9106 (4.9865)  loss_scale: 65536.0000 (79297.7714)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0008  max mem: 15572
Epoch: [1]  [ 840/1404]  eta: 0:05:50  lr: 0.000030  min_lr: 0.000000  loss: 4.9252 (4.9855)  loss_scale: 65536.0000 (79134.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0008  max mem: 15572
Epoch: [1]  [ 850/1404]  eta: 0:05:44  lr: 0.000030  min_lr: 0.000000  loss: 4.8942 (4.9845)  loss_scale: 65536.0000 (78974.3455)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0007  max mem: 15572
[2025-01-10 16:01:50,183] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:01:50,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:01:50,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:01:50,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:01:50,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2261
[2025-01-10 16:01:50,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:01:50,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 16:01:50,775] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2261
[2025-01-10 16:01:50,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:01:52,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2264
[2025-01-10 16:01:52,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:01:52,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2264
[2025-01-10 16:01:52,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:01:52,438] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [ 860/1404]  eta: 0:05:38  lr: 0.000030  min_lr: 0.000000  loss: 4.9041 (4.9837)  loss_scale: 65536.0000 (78856.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0009  max mem: 15572
Epoch: [1]  [ 870/1404]  eta: 0:05:32  lr: 0.000030  min_lr: 0.000000  loss: 4.9266 (4.9826)  loss_scale: 32768.0000 (78327.1825)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.0010  max mem: 15572
Epoch: [1]  [ 880/1404]  eta: 0:05:25  lr: 0.000031  min_lr: 0.000000  loss: 4.9500 (4.9828)  loss_scale: 32768.0000 (77810.0522)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0009  max mem: 15572
Epoch: [1]  [ 890/1404]  eta: 0:05:19  lr: 0.000031  min_lr: 0.000000  loss: 4.9628 (4.9829)  loss_scale: 32768.0000 (77304.5297)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0010  max mem: 15572
Epoch: [1]  [ 900/1404]  eta: 0:05:12  lr: 0.000031  min_lr: 0.000000  loss: 4.9979 (4.9826)  loss_scale: 32768.0000 (76810.2286)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0010  max mem: 15572
Epoch: [1]  [ 910/1404]  eta: 0:05:06  lr: 0.000031  min_lr: 0.000000  loss: 5.0011 (4.9830)  loss_scale: 32768.0000 (76326.7794)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0010  max mem: 15572
Epoch: [1]  [ 920/1404]  eta: 0:04:59  lr: 0.000031  min_lr: 0.000000  loss: 5.0011 (4.9829)  loss_scale: 32768.0000 (75853.8284)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0454  max mem: 15572
Epoch: [1]  [ 930/1404]  eta: 0:04:53  lr: 0.000031  min_lr: 0.000000  loss: 4.9032 (4.9827)  loss_scale: 32768.0000 (75391.0376)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1033  max mem: 15572
Epoch: [1]  [ 940/1404]  eta: 0:04:47  lr: 0.000031  min_lr: 0.000000  loss: 4.9016 (4.9823)  loss_scale: 32768.0000 (74938.0829)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1358  max mem: 15572
Epoch: [1]  [ 950/1404]  eta: 0:04:41  lr: 0.000031  min_lr: 0.000000  loss: 4.9108 (4.9817)  loss_scale: 32768.0000 (74494.6540)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1000  max mem: 15572
Epoch: [1]  [ 960/1404]  eta: 0:04:34  lr: 0.000032  min_lr: 0.000000  loss: 4.9069 (4.9814)  loss_scale: 32768.0000 (74060.4537)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0232  max mem: 15572
Epoch: [1]  [ 970/1404]  eta: 0:04:28  lr: 0.000032  min_lr: 0.000000  loss: 4.9519 (4.9809)  loss_scale: 32768.0000 (73635.1967)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0009  max mem: 15572
Epoch: [1]  [ 980/1404]  eta: 0:04:22  lr: 0.000032  min_lr: 0.000000  loss: 4.9595 (4.9805)  loss_scale: 32768.0000 (73218.6096)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0203  max mem: 15572
[2025-01-10 16:03:10,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:03:10,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:03:10,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:03:10,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 990/1404]  eta: 0:04:16  lr: 0.000032  min_lr: 0.000000  loss: 4.9043 (4.9797)  loss_scale: 32768.0000 (72876.5610)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0205  max mem: 15572
Epoch: [1]  [1000/1404]  eta: 0:04:10  lr: 0.000032  min_lr: 0.000000  loss: 4.9309 (4.9796)  loss_scale: 65536.0000 (72803.2288)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0015  max mem: 15572
Epoch: [1]  [1010/1404]  eta: 0:04:04  lr: 0.000032  min_lr: 0.000000  loss: 4.9565 (4.9793)  loss_scale: 65536.0000 (72731.3472)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0015  max mem: 15572
[2025-01-10 16:03:26,218] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-10 16:03:26,219] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:03:26,219] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:03:26,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-10 16:03:26,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [1020/1404]  eta: 0:03:57  lr: 0.000032  min_lr: 0.000000  loss: 4.9159 (4.9786)  loss_scale: 65536.0000 (72468.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0011  max mem: 15572
Epoch: [1]  [1030/1404]  eta: 0:03:51  lr: 0.000033  min_lr: 0.000000  loss: 4.8926 (4.9782)  loss_scale: 32768.0000 (72083.2435)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0019  max mem: 15572
Epoch: [1]  [1040/1404]  eta: 0:03:45  lr: 0.000033  min_lr: 0.000000  loss: 4.9772 (4.9784)  loss_scale: 32768.0000 (71705.5754)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.0018  max mem: 15572
Epoch: [1]  [1050/1404]  eta: 0:03:39  lr: 0.000033  min_lr: 0.000000  loss: 4.9853 (4.9782)  loss_scale: 32768.0000 (71335.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0009  max mem: 15572
Epoch: [1]  [1060/1404]  eta: 0:03:32  lr: 0.000033  min_lr: 0.000000  loss: 4.9294 (4.9776)  loss_scale: 32768.0000 (70971.5966)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0009  max mem: 15572
Epoch: [1]  [1070/1404]  eta: 0:03:26  lr: 0.000033  min_lr: 0.000000  loss: 4.8840 (4.9770)  loss_scale: 32768.0000 (70614.8870)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0008  max mem: 15572
Epoch: [1]  [1080/1404]  eta: 0:03:20  lr: 0.000033  min_lr: 0.000000  loss: 4.9072 (4.9769)  loss_scale: 32768.0000 (70264.7771)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0009  max mem: 15572
Epoch: [1]  [1090/1404]  eta: 0:03:14  lr: 0.000033  min_lr: 0.000000  loss: 4.9539 (4.9766)  loss_scale: 32768.0000 (69921.0852)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.0009  max mem: 15572
Epoch: [1]  [1100/1404]  eta: 0:03:08  lr: 0.000033  min_lr: 0.000000  loss: 4.9854 (4.9761)  loss_scale: 32768.0000 (69583.6367)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0011  max mem: 15572
Epoch: [1]  [1110/1404]  eta: 0:03:01  lr: 0.000034  min_lr: 0.000000  loss: 4.9048 (4.9751)  loss_scale: 32768.0000 (69252.2628)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0010  max mem: 15572
Epoch: [1]  [1120/1404]  eta: 0:02:56  lr: 0.000034  min_lr: 0.000000  loss: 4.9036 (4.9750)  loss_scale: 32768.0000 (68926.8011)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.0008  max mem: 15572
Epoch: [1]  [1130/1404]  eta: 0:02:49  lr: 0.000034  min_lr: 0.000000  loss: 4.9462 (4.9746)  loss_scale: 32768.0000 (68607.0946)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0007  max mem: 15572
Epoch: [1]  [1140/1404]  eta: 0:02:43  lr: 0.000034  min_lr: 0.000000  loss: 4.8847 (4.9732)  loss_scale: 32768.0000 (68292.9921)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0007  max mem: 15572
[2025-01-10 16:04:46,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:04:46,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:04:46,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:04:46,578] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1150/1404]  eta: 0:02:37  lr: 0.000034  min_lr: 0.000000  loss: 4.8367 (4.9727)  loss_scale: 32768.0000 (68183.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0009  max mem: 15572
Epoch: [1]  [1160/1404]  eta: 0:02:31  lr: 0.000034  min_lr: 0.000000  loss: 4.9682 (4.9723)  loss_scale: 65536.0000 (68160.8269)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0008  max mem: 15572
Epoch: [1]  [1170/1404]  eta: 0:02:25  lr: 0.000034  min_lr: 0.000000  loss: 4.9357 (4.9719)  loss_scale: 65536.0000 (68138.4116)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0009  max mem: 15572
Epoch: [1]  [1180/1404]  eta: 0:02:18  lr: 0.000035  min_lr: 0.000000  loss: 4.8740 (4.9718)  loss_scale: 65536.0000 (68116.3760)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.0010  max mem: 15572
Epoch: [1]  [1190/1404]  eta: 0:02:12  lr: 0.000035  min_lr: 0.000000  loss: 4.9118 (4.9715)  loss_scale: 65536.0000 (68094.7103)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0009  max mem: 15572
Epoch: [1]  [1200/1404]  eta: 0:02:06  lr: 0.000035  min_lr: 0.000000  loss: 4.8331 (4.9700)  loss_scale: 65536.0000 (68073.4055)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0008  max mem: 15572
Epoch: [1]  [1210/1404]  eta: 0:02:00  lr: 0.000035  min_lr: 0.000000  loss: 4.8933 (4.9697)  loss_scale: 65536.0000 (68052.4525)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.0009  max mem: 15572
Epoch: [1]  [1220/1404]  eta: 0:01:54  lr: 0.000035  min_lr: 0.000000  loss: 4.9164 (4.9692)  loss_scale: 65536.0000 (68031.8428)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0010  max mem: 15572
Epoch: [1]  [1230/1404]  eta: 0:01:47  lr: 0.000035  min_lr: 0.000000  loss: 4.9164 (4.9692)  loss_scale: 65536.0000 (68011.5678)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0011  max mem: 15572
[2025-01-10 16:05:45,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2643
[2025-01-10 16:05:45,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:05:45,391] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2643
[2025-01-10 16:05:45,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:05:45,391] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1240/1404]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000000  loss: 4.9218 (4.9683)  loss_scale: 65536.0000 (67938.8106)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0012  max mem: 15572
Epoch: [1]  [1250/1404]  eta: 0:01:35  lr: 0.000035  min_lr: 0.000000  loss: 4.8714 (4.9673)  loss_scale: 32768.0000 (67657.6691)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0010  max mem: 15572
Epoch: [1]  [1260/1404]  eta: 0:01:29  lr: 0.000036  min_lr: 0.000000  loss: 4.8714 (4.9664)  loss_scale: 32768.0000 (67380.9865)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0008  max mem: 15572
Epoch: [1]  [1270/1404]  eta: 0:01:23  lr: 0.000036  min_lr: 0.000000  loss: 4.8892 (4.9663)  loss_scale: 32768.0000 (67108.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0008  max mem: 15572
Epoch: [1]  [1280/1404]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000000  loss: 4.9313 (4.9660)  loss_scale: 32768.0000 (66840.5808)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.0007  max mem: 15572
Epoch: [1]  [1290/1404]  eta: 0:01:10  lr: 0.000036  min_lr: 0.000000  loss: 4.8861 (4.9651)  loss_scale: 32768.0000 (66576.6569)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.0008  max mem: 15572
Epoch: [1]  [1300/1404]  eta: 0:01:04  lr: 0.000036  min_lr: 0.000000  loss: 4.9085 (4.9652)  loss_scale: 32768.0000 (66316.7902)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0009  max mem: 15572
Epoch: [1]  [1310/1404]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000000  loss: 4.9142 (4.9645)  loss_scale: 32768.0000 (66060.8879)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0009  max mem: 15572
Epoch: [1]  [1320/1404]  eta: 0:00:51  lr: 0.000036  min_lr: 0.000000  loss: 4.8394 (4.9633)  loss_scale: 32768.0000 (65808.8600)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0012  max mem: 15572
Epoch: [1]  [1330/1404]  eta: 0:00:45  lr: 0.000037  min_lr: 0.000000  loss: 4.8050 (4.9623)  loss_scale: 32768.0000 (65560.6191)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0011  max mem: 15572
Epoch: [1]  [1340/1404]  eta: 0:00:39  lr: 0.000037  min_lr: 0.000000  loss: 4.8234 (4.9620)  loss_scale: 32768.0000 (65316.0805)  weight_decay: 0.0500 (0.0500)  time: 0.6795  data: 0.0008  max mem: 15572
Epoch: [1]  [1350/1404]  eta: 0:00:33  lr: 0.000037  min_lr: 0.000000  loss: 4.8992 (4.9608)  loss_scale: 32768.0000 (65075.1621)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0008  max mem: 15572
Epoch: [1]  [1360/1404]  eta: 0:00:27  lr: 0.000037  min_lr: 0.000000  loss: 4.8998 (4.9606)  loss_scale: 32768.0000 (64837.7840)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0008  max mem: 15572
[2025-01-10 16:07:04,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:07:04,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:07:04,036] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:07:04,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1370/1404]  eta: 0:00:21  lr: 0.000037  min_lr: 0.000000  loss: 4.9323 (4.9606)  loss_scale: 32768.0000 (64675.5711)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0007  max mem: 15572
Epoch: [1]  [1380/1404]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000000  loss: 4.8968 (4.9596)  loss_scale: 65536.0000 (64681.8016)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.0011  max mem: 15572
Epoch: [1]  [1390/1404]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 4.8490 (4.9584)  loss_scale: 65536.0000 (64687.9425)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.0070  max mem: 15572
Epoch: [1]  [1400/1404]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000000  loss: 4.8172 (4.9576)  loss_scale: 65536.0000 (64693.9957)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0065  max mem: 15572
Epoch: [1]  [1403/1404]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.8172 (4.9576)  loss_scale: 65536.0000 (64695.7949)  weight_decay: 0.0500 (0.0500)  time: 0.4914  data: 0.0064  max mem: 15572
Epoch: [1] Total time: 0:14:27 (0.6175 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.8172 (4.9560)  loss_scale: 65536.0000 (64695.7949)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:47  loss: 5.0638 (5.0638)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.6407  data: 5.3891  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 4.9768 (4.8298)  acc1: 0.0000 (9.0909)  acc5: 0.0000 (16.6667)  time: 0.7951  data: 0.5841  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 4.6567 (4.6682)  acc1: 0.0000 (10.5820)  acc5: 5.5556 (28.3069)  time: 0.3223  data: 0.1254  max mem: 15572
Val:  [ 30/136]  eta: 0:00:56  loss: 4.2989 (4.6589)  acc1: 0.0000 (7.1685)  acc5: 16.6667 (26.8817)  time: 0.3818  data: 0.1728  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 4.4277 (4.6164)  acc1: 0.0000 (7.8591)  acc5: 11.1111 (27.2358)  time: 0.4132  data: 0.2029  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 4.8407 (4.6958)  acc1: 0.0000 (6.4270)  acc5: 0.0000 (23.3115)  time: 0.3940  data: 0.1943  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 4.9178 (4.7504)  acc1: 0.0000 (5.4645)  acc5: 0.0000 (19.9454)  time: 0.3582  data: 0.1538  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 4.7984 (4.6985)  acc1: 0.0000 (7.4335)  acc5: 0.0000 (23.0829)  time: 0.3876  data: 0.1910  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 4.5770 (4.6903)  acc1: 0.0000 (7.7503)  acc5: 0.0000 (22.7709)  time: 0.4124  data: 0.2105  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 4.7387 (4.7109)  acc1: 0.0000 (6.8987)  acc5: 0.0000 (20.3907)  time: 0.3008  data: 0.0996  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.9164 (4.7456)  acc1: 0.0000 (6.2156)  acc5: 0.0000 (18.3718)  time: 0.2975  data: 0.0918  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.8819 (4.7522)  acc1: 0.0000 (5.6557)  acc5: 0.0000 (17.5175)  time: 0.3790  data: 0.1705  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7196 (4.7511)  acc1: 0.0000 (5.1882)  acc5: 5.5556 (16.8503)  time: 0.4099  data: 0.2140  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.6973 (4.7574)  acc1: 0.0000 (4.8346)  acc5: 11.1111 (17.0059)  time: 0.3407  data: 0.1750  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.8780 (4.7578)  acc1: 0.0000 (4.6683)  acc5: 5.5556 (17.3219)  time: 0.2053  data: 0.0566  max mem: 15572
Val: Total time: 0:00:53 (0.3916 s / it)
* Acc@1 4.566 Acc@5 16.790 loss 4.767
Accuracy of the network on the 4883 val videos: 4.6%
[2025-01-10 16:08:16,949] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 16:08:16,951] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 16:08:16,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 16:08:16,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 16:08:19,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 16:08:19,304] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 4.57%
Epoch: [2]  [   0/1404]  eta: 3:41:45  lr: 0.000038  min_lr: 0.000000  loss: 4.8352 (4.8352)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4771  data: 4.8726  max mem: 15572
[2025-01-10 16:08:32,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2816
[2025-01-10 16:08:32,827] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:08:32,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:08:32,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2816
[2025-01-10 16:08:32,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [  10/1404]  eta: 0:31:51  lr: 0.000038  min_lr: 0.000000  loss: 4.8884 (4.9028)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 1.3711  data: 0.4437  max mem: 15572
Epoch: [2]  [  20/1404]  eta: 0:23:51  lr: 0.000038  min_lr: 0.000000  loss: 4.8884 (4.9045)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0008  max mem: 15572
Epoch: [2]  [  30/1404]  eta: 0:20:08  lr: 0.000038  min_lr: 0.000000  loss: 4.8780 (4.9020)  loss_scale: 32768.0000 (41224.2581)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0007  max mem: 15572
Epoch: [2]  [  40/1404]  eta: 0:17:55  lr: 0.000038  min_lr: 0.000000  loss: 4.9608 (4.9086)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0009  max mem: 15572
Epoch: [2]  [  50/1404]  eta: 0:16:53  lr: 0.000038  min_lr: 0.000000  loss: 4.8111 (4.8824)  loss_scale: 32768.0000 (37908.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0011  max mem: 15572
Epoch: [2]  [  60/1404]  eta: 0:16:38  lr: 0.000038  min_lr: 0.000000  loss: 4.7798 (4.8780)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0011  max mem: 15572
Epoch: [2]  [  70/1404]  eta: 0:16:09  lr: 0.000038  min_lr: 0.000000  loss: 4.9314 (4.8801)  loss_scale: 32768.0000 (36460.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.0007  max mem: 15572
Epoch: [2]  [  80/1404]  eta: 0:15:44  lr: 0.000039  min_lr: 0.000000  loss: 4.8551 (4.8675)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0009  max mem: 15572
Epoch: [2]  [  90/1404]  eta: 0:15:15  lr: 0.000039  min_lr: 0.000000  loss: 4.8318 (4.8627)  loss_scale: 32768.0000 (35648.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0009  max mem: 15572
Epoch: [2]  [ 100/1404]  eta: 0:15:00  lr: 0.000039  min_lr: 0.000000  loss: 4.8327 (4.8528)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0006  max mem: 15572
Epoch: [2]  [ 110/1404]  eta: 0:14:32  lr: 0.000039  min_lr: 0.000000  loss: 4.8688 (4.8554)  loss_scale: 32768.0000 (35129.6577)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0006  max mem: 15572
Epoch: [2]  [ 120/1404]  eta: 0:14:25  lr: 0.000039  min_lr: 0.000000  loss: 4.8018 (4.8493)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0141  max mem: 15572
Epoch: [2]  [ 130/1404]  eta: 0:14:12  lr: 0.000039  min_lr: 0.000000  loss: 4.8520 (4.8576)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.0142  max mem: 15572
[2025-01-10 16:09:50,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:09:50,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:09:50,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:09:50,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 140/1404]  eta: 0:13:53  lr: 0.000039  min_lr: 0.000000  loss: 4.9152 (4.8557)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0010  max mem: 15572
Epoch: [2]  [ 150/1404]  eta: 0:13:48  lr: 0.000040  min_lr: 0.000000  loss: 4.8324 (4.8536)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0009  max mem: 15572
Epoch: [2]  [ 160/1404]  eta: 0:13:47  lr: 0.000040  min_lr: 0.000000  loss: 4.8324 (4.8563)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.7024  data: 0.0009  max mem: 15572
Epoch: [2]  [ 170/1404]  eta: 0:13:28  lr: 0.000040  min_lr: 0.000000  loss: 4.8180 (4.8552)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0009  max mem: 15572
Epoch: [2]  [ 180/1404]  eta: 0:13:13  lr: 0.000040  min_lr: 0.000000  loss: 4.8009 (4.8537)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0008  max mem: 15572
Epoch: [2]  [ 190/1404]  eta: 0:13:11  lr: 0.000040  min_lr: 0.000000  loss: 4.7993 (4.8510)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0009  max mem: 15572
[2025-01-10 16:10:24,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[3.8810301798323227e-07, 3.8810301798323227e-07, 5.54432882833189e-07, 5.54432882833189e-07, 7.920469754759844e-07, 7.920469754759844e-07, 1.1314956792514063e-06, 1.1314956792514063e-06, 1.6164223989305805e-06, 1.6164223989305805e-06, 2.309174855615115e-06, 2.309174855615115e-06, 3.2988212223073076e-06, 3.2988212223073076e-06, 4.712601746153297e-06, 4.712601746153297e-06, 6.732288208790424e-06, 6.732288208790424e-06, 9.617554583986322e-06, 9.617554583986322e-06, 1.373936369140903e-05, 1.373936369140903e-05, 1.9627662416298616e-05, 1.9627662416298616e-05, 2.8039517737569454e-05, 2.8039517737569454e-05, 4.005645391081351e-05, 4.005645391081351e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 16:10:24,394] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=45.68307859082331, CurrSamplesPerSec=47.32323872913105, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 200/1404]  eta: 0:12:57  lr: 0.000040  min_lr: 0.000000  loss: 4.8526 (4.8552)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0008  max mem: 15572
[2025-01-10 16:10:31,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-10 16:10:31,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:10:31,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:10:31,760] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-10 16:10:31,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 210/1404]  eta: 0:12:46  lr: 0.000040  min_lr: 0.000000  loss: 4.9054 (4.8546)  loss_scale: 65536.0000 (44570.6919)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0176  max mem: 15572
Epoch: [2]  [ 220/1404]  eta: 0:12:36  lr: 0.000040  min_lr: 0.000000  loss: 4.7852 (4.8504)  loss_scale: 32768.0000 (44036.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0481  max mem: 15572
Epoch: [2]  [ 230/1404]  eta: 0:12:30  lr: 0.000041  min_lr: 0.000000  loss: 4.8055 (4.8511)  loss_scale: 32768.0000 (43548.8139)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0953  max mem: 15572
Epoch: [2]  [ 240/1404]  eta: 0:12:25  lr: 0.000041  min_lr: 0.000000  loss: 4.8274 (4.8502)  loss_scale: 32768.0000 (43101.4772)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.1352  max mem: 15572
Epoch: [2]  [ 250/1404]  eta: 0:12:18  lr: 0.000041  min_lr: 0.000000  loss: 4.8749 (4.8526)  loss_scale: 32768.0000 (42689.7849)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1367  max mem: 15572
Epoch: [2]  [ 260/1404]  eta: 0:12:14  lr: 0.000041  min_lr: 0.000000  loss: 4.8227 (4.8491)  loss_scale: 32768.0000 (42309.6398)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1451  max mem: 15572
Epoch: [2]  [ 270/1404]  eta: 0:12:05  lr: 0.000041  min_lr: 0.000000  loss: 4.7860 (4.8503)  loss_scale: 32768.0000 (41957.5498)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1211  max mem: 15572
Epoch: [2]  [ 280/1404]  eta: 0:11:59  lr: 0.000041  min_lr: 0.000000  loss: 4.8631 (4.8495)  loss_scale: 32768.0000 (41630.5196)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1069  max mem: 15572
Epoch: [2]  [ 290/1404]  eta: 0:11:50  lr: 0.000041  min_lr: 0.000000  loss: 4.8397 (4.8478)  loss_scale: 32768.0000 (41325.9656)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0969  max mem: 15572
Epoch: [2]  [ 300/1404]  eta: 0:11:44  lr: 0.000042  min_lr: 0.000000  loss: 4.8580 (4.8492)  loss_scale: 32768.0000 (41041.6478)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0971  max mem: 15572
Epoch: [2]  [ 310/1404]  eta: 0:11:38  lr: 0.000042  min_lr: 0.000000  loss: 4.8775 (4.8510)  loss_scale: 32768.0000 (40775.6141)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.1414  max mem: 15572
Epoch: [2]  [ 320/1404]  eta: 0:11:31  lr: 0.000042  min_lr: 0.000000  loss: 4.8047 (4.8506)  loss_scale: 32768.0000 (40526.1558)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1250  max mem: 15572
Epoch: [2]  [ 330/1404]  eta: 0:11:22  lr: 0.000042  min_lr: 0.000000  loss: 4.8211 (4.8523)  loss_scale: 32768.0000 (40291.7704)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0719  max mem: 15572
[2025-01-10 16:11:51,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:11:51,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:11:51,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:11:51,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 340/1404]  eta: 0:11:17  lr: 0.000042  min_lr: 0.000000  loss: 4.9004 (4.8536)  loss_scale: 32768.0000 (40743.7889)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0911  max mem: 15572
Epoch: [2]  [ 350/1404]  eta: 0:11:10  lr: 0.000042  min_lr: 0.000000  loss: 4.7793 (4.8515)  loss_scale: 65536.0000 (41450.1197)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1323  max mem: 15572
Epoch: [2]  [ 360/1404]  eta: 0:11:01  lr: 0.000042  min_lr: 0.000000  loss: 4.8097 (4.8504)  loss_scale: 65536.0000 (42117.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0804  max mem: 15572
Epoch: [2]  [ 370/1404]  eta: 0:10:55  lr: 0.000042  min_lr: 0.000000  loss: 4.8372 (4.8519)  loss_scale: 65536.0000 (42748.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0763  max mem: 15572
Epoch: [2]  [ 380/1404]  eta: 0:10:46  lr: 0.000043  min_lr: 0.000000  loss: 4.8087 (4.8489)  loss_scale: 65536.0000 (43346.6457)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0932  max mem: 15572
Epoch: [2]  [ 390/1404]  eta: 0:10:42  lr: 0.000043  min_lr: 0.000000  loss: 4.6845 (4.8463)  loss_scale: 65536.0000 (43914.1483)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1239  max mem: 15572
Epoch: [2]  [ 400/1404]  eta: 0:10:35  lr: 0.000043  min_lr: 0.000000  loss: 4.6870 (4.8453)  loss_scale: 65536.0000 (44453.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.1295  max mem: 15572
Epoch: [2]  [ 410/1404]  eta: 0:10:28  lr: 0.000043  min_lr: 0.000000  loss: 4.7220 (4.8439)  loss_scale: 65536.0000 (44966.3066)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0809  max mem: 15572
[2025-01-10 16:12:39,794] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3219
[2025-01-10 16:12:39,794] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:12:39,794] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:12:39,849] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3219
[2025-01-10 16:12:39,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 420/1404]  eta: 0:10:22  lr: 0.000043  min_lr: 0.000000  loss: 4.7481 (4.8424)  loss_scale: 32768.0000 (44676.5606)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0994  max mem: 15572
Epoch: [2]  [ 430/1404]  eta: 0:10:15  lr: 0.000043  min_lr: 0.000000  loss: 4.7688 (4.8420)  loss_scale: 32768.0000 (44400.2599)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1089  max mem: 15572
Epoch: [2]  [ 440/1404]  eta: 0:10:11  lr: 0.000043  min_lr: 0.000000  loss: 4.8432 (4.8421)  loss_scale: 32768.0000 (44136.4898)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.1710  max mem: 15572
Epoch: [2]  [ 450/1404]  eta: 0:10:01  lr: 0.000044  min_lr: 0.000000  loss: 4.8192 (4.8420)  loss_scale: 32768.0000 (43884.4169)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1196  max mem: 15572
Epoch: [2]  [ 460/1404]  eta: 0:09:54  lr: 0.000044  min_lr: 0.000000  loss: 4.8654 (4.8435)  loss_scale: 32768.0000 (43643.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0241  max mem: 15572
Epoch: [2]  [ 470/1404]  eta: 0:09:47  lr: 0.000044  min_lr: 0.000000  loss: 4.8259 (4.8419)  loss_scale: 32768.0000 (43412.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0668  max mem: 15572
Epoch: [2]  [ 480/1404]  eta: 0:09:40  lr: 0.000044  min_lr: 0.000000  loss: 4.7421 (4.8398)  loss_scale: 32768.0000 (43191.0852)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0860  max mem: 15572
Epoch: [2]  [ 490/1404]  eta: 0:09:35  lr: 0.000044  min_lr: 0.000000  loss: 4.7798 (4.8391)  loss_scale: 32768.0000 (42978.8024)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1377  max mem: 15572
Epoch: [2]  [ 500/1404]  eta: 0:09:29  lr: 0.000044  min_lr: 0.000000  loss: 4.8482 (4.8393)  loss_scale: 32768.0000 (42774.9940)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1413  max mem: 15572
Epoch: [2]  [ 510/1404]  eta: 0:09:23  lr: 0.000044  min_lr: 0.000000  loss: 4.7887 (4.8378)  loss_scale: 32768.0000 (42579.1624)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1149  max mem: 15572
Epoch: [2]  [ 520/1404]  eta: 0:09:17  lr: 0.000044  min_lr: 0.000000  loss: 4.7421 (4.8368)  loss_scale: 32768.0000 (42390.8484)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1196  max mem: 15572
Epoch: [2]  [ 530/1404]  eta: 0:09:09  lr: 0.000045  min_lr: 0.000000  loss: 4.7503 (4.8355)  loss_scale: 32768.0000 (42209.6271)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0807  max mem: 15572
[2025-01-10 16:13:59,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:13:59,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 540/1404]  eta: 0:09:03  lr: 0.000045  min_lr: 0.000000  loss: 4.7812 (4.8354)  loss_scale: 32768.0000 (42095.6747)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0951  max mem: 15572
[2025-01-10 16:13:59,934] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:13:59,935] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 550/1404]  eta: 0:08:55  lr: 0.000045  min_lr: 0.000000  loss: 4.8266 (4.8343)  loss_scale: 65536.0000 (42521.0889)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0656  max mem: 15572
Epoch: [2]  [ 560/1404]  eta: 0:08:49  lr: 0.000045  min_lr: 0.000000  loss: 4.7489 (4.8331)  loss_scale: 65536.0000 (42931.3369)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0616  max mem: 15572
[2025-01-10 16:14:17,778] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3378
[2025-01-10 16:14:17,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:14:17,779] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:14:17,780] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3378
[2025-01-10 16:14:17,780] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 570/1404]  eta: 0:08:43  lr: 0.000045  min_lr: 0.000000  loss: 4.7981 (4.8333)  loss_scale: 65536.0000 (43269.8284)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1326  max mem: 15572
Epoch: [2]  [ 580/1404]  eta: 0:08:36  lr: 0.000045  min_lr: 0.000000  loss: 4.8445 (4.8337)  loss_scale: 32768.0000 (43089.0740)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1191  max mem: 15572
Epoch: [2]  [ 590/1404]  eta: 0:08:30  lr: 0.000045  min_lr: 0.000000  loss: 4.8293 (4.8323)  loss_scale: 32768.0000 (42914.4365)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1251  max mem: 15572
Epoch: [2]  [ 600/1404]  eta: 0:08:25  lr: 0.000046  min_lr: 0.000000  loss: 4.7639 (4.8311)  loss_scale: 32768.0000 (42745.6106)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.1718  max mem: 15572
Epoch: [2]  [ 610/1404]  eta: 0:08:18  lr: 0.000046  min_lr: 0.000000  loss: 4.7915 (4.8313)  loss_scale: 32768.0000 (42582.3110)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1590  max mem: 15572
Epoch: [2]  [ 620/1404]  eta: 0:08:11  lr: 0.000046  min_lr: 0.000000  loss: 4.7925 (4.8297)  loss_scale: 32768.0000 (42424.2705)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1003  max mem: 15572
Epoch: [2]  [ 630/1404]  eta: 0:08:04  lr: 0.000046  min_lr: 0.000000  loss: 4.7771 (4.8296)  loss_scale: 32768.0000 (42271.2393)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0694  max mem: 15572
Epoch: [2]  [ 640/1404]  eta: 0:07:59  lr: 0.000046  min_lr: 0.000000  loss: 4.8401 (4.8289)  loss_scale: 32768.0000 (42122.9828)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1148  max mem: 15572
Epoch: [2]  [ 650/1404]  eta: 0:07:52  lr: 0.000046  min_lr: 0.000000  loss: 4.7724 (4.8262)  loss_scale: 32768.0000 (41979.2811)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.1329  max mem: 15572
Epoch: [2]  [ 660/1404]  eta: 0:07:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6880 (4.8243)  loss_scale: 32768.0000 (41839.9274)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0595  max mem: 15572
Epoch: [2]  [ 670/1404]  eta: 0:07:38  lr: 0.000046  min_lr: 0.000000  loss: 4.7455 (4.8235)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0788  max mem: 15572
Epoch: [2]  [ 680/1404]  eta: 0:07:32  lr: 0.000047  min_lr: 0.000000  loss: 4.7625 (4.8221)  loss_scale: 32768.0000 (41573.4978)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1231  max mem: 15572
Epoch: [2]  [ 690/1404]  eta: 0:07:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7588 (4.8208)  loss_scale: 32768.0000 (41446.0666)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0837  max mem: 15572
[2025-01-10 16:15:36,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:15:36,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:15:36,884] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:15:36,885] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 700/1404]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 4.7976 (4.8215)  loss_scale: 32768.0000 (41415.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0843  max mem: 15572
Epoch: [2]  [ 710/1404]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 4.7345 (4.8191)  loss_scale: 65536.0000 (41755.0042)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1036  max mem: 15572
Epoch: [2]  [ 720/1404]  eta: 0:07:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6835 (4.8174)  loss_scale: 65536.0000 (42084.8377)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1145  max mem: 15572
Epoch: [2]  [ 730/1404]  eta: 0:07:00  lr: 0.000047  min_lr: 0.000000  loss: 4.7423 (4.8156)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0979  max mem: 15572
Epoch: [2]  [ 740/1404]  eta: 0:06:54  lr: 0.000047  min_lr: 0.000000  loss: 4.8322 (4.8162)  loss_scale: 65536.0000 (42717.7976)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0983  max mem: 15572
Epoch: [2]  [ 750/1404]  eta: 0:06:47  lr: 0.000048  min_lr: 0.000000  loss: 4.8384 (4.8136)  loss_scale: 65536.0000 (43021.6352)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0649  max mem: 15572
Epoch: [2]  [ 760/1404]  eta: 0:06:41  lr: 0.000048  min_lr: 0.000000  loss: 4.7330 (4.8144)  loss_scale: 65536.0000 (43317.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0626  max mem: 15572
Epoch: [2]  [ 770/1404]  eta: 0:06:34  lr: 0.000048  min_lr: 0.000000  loss: 4.7372 (4.8141)  loss_scale: 65536.0000 (43605.6654)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0751  max mem: 15572
Epoch: [2]  [ 780/1404]  eta: 0:06:28  lr: 0.000048  min_lr: 0.000000  loss: 4.8165 (4.8144)  loss_scale: 65536.0000 (43886.4635)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0701  max mem: 15572
Epoch: [2]  [ 790/1404]  eta: 0:06:21  lr: 0.000048  min_lr: 0.000000  loss: 4.8165 (4.8139)  loss_scale: 65536.0000 (44160.1618)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0897  max mem: 15572
Epoch: [2]  [ 800/1404]  eta: 0:06:15  lr: 0.000048  min_lr: 0.000000  loss: 4.8146 (4.8141)  loss_scale: 65536.0000 (44427.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0652  max mem: 15572
Epoch: [2]  [ 810/1404]  eta: 0:06:08  lr: 0.000048  min_lr: 0.000000  loss: 4.8146 (4.8134)  loss_scale: 65536.0000 (44687.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0997  max mem: 15572
Epoch: [2]  [ 820/1404]  eta: 0:06:02  lr: 0.000048  min_lr: 0.000000  loss: 4.7512 (4.8126)  loss_scale: 65536.0000 (44941.2521)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1210  max mem: 15572
[2025-01-10 16:16:54,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:16:54,374] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:16:54,403] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:16:54,403] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:16:54,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3636
[2025-01-10 16:16:54,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:16:54,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3636
[2025-01-10 16:16:54,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:16:54,818] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 830/1404]  eta: 0:05:56  lr: 0.000049  min_lr: 0.000000  loss: 4.7501 (4.8117)  loss_scale: 65536.0000 (45267.9471)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1300  max mem: 15572
Epoch: [2]  [ 840/1404]  eta: 0:05:49  lr: 0.000049  min_lr: 0.000000  loss: 4.7665 (4.8111)  loss_scale: 65536.0000 (45508.9465)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1047  max mem: 15572
Epoch: [2]  [ 850/1404]  eta: 0:05:43  lr: 0.000049  min_lr: 0.000000  loss: 4.7412 (4.8110)  loss_scale: 65536.0000 (45744.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0728  max mem: 15572
Epoch: [2]  [ 860/1404]  eta: 0:05:37  lr: 0.000049  min_lr: 0.000000  loss: 4.7543 (4.8112)  loss_scale: 65536.0000 (45974.1510)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1122  max mem: 15572
Epoch: [2]  [ 870/1404]  eta: 0:05:31  lr: 0.000049  min_lr: 0.000000  loss: 4.8115 (4.8108)  loss_scale: 65536.0000 (46198.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0825  max mem: 15572
Epoch: [2]  [ 880/1404]  eta: 0:05:25  lr: 0.000049  min_lr: 0.000000  loss: 4.8115 (4.8111)  loss_scale: 65536.0000 (46418.2338)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0904  max mem: 15572
Epoch: [2]  [ 890/1404]  eta: 0:05:18  lr: 0.000049  min_lr: 0.000000  loss: 4.8139 (4.8108)  loss_scale: 65536.0000 (46632.7991)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1062  max mem: 15572
Epoch: [2]  [ 900/1404]  eta: 0:05:12  lr: 0.000050  min_lr: 0.000000  loss: 4.8133 (4.8103)  loss_scale: 65536.0000 (46842.6016)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1077  max mem: 15572
Epoch: [2]  [ 910/1404]  eta: 0:05:06  lr: 0.000050  min_lr: 0.000000  loss: 4.8265 (4.8107)  loss_scale: 65536.0000 (47047.7980)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1602  max mem: 15572
[2025-01-10 16:17:46,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-10 16:17:46,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:17:46,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:17:46,637] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-10 16:17:46,638] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 920/1404]  eta: 0:05:00  lr: 0.000050  min_lr: 0.000000  loss: 4.7631 (4.8089)  loss_scale: 65536.0000 (46963.9088)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1418  max mem: 15572
Epoch: [2]  [ 930/1404]  eta: 0:04:54  lr: 0.000050  min_lr: 0.000000  loss: 4.6052 (4.8074)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1030  max mem: 15572
Epoch: [2]  [ 940/1404]  eta: 0:04:47  lr: 0.000050  min_lr: 0.000000  loss: 4.7201 (4.8070)  loss_scale: 32768.0000 (46662.1892)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0812  max mem: 15572
Epoch: [2]  [ 950/1404]  eta: 0:04:41  lr: 0.000050  min_lr: 0.000000  loss: 4.7560 (4.8058)  loss_scale: 32768.0000 (46516.0883)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0776  max mem: 15572
Epoch: [2]  [ 960/1404]  eta: 0:04:34  lr: 0.000050  min_lr: 0.000000  loss: 4.7712 (4.8064)  loss_scale: 32768.0000 (46373.0281)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0788  max mem: 15572
Epoch: [2]  [ 970/1404]  eta: 0:04:28  lr: 0.000050  min_lr: 0.000000  loss: 4.8500 (4.8055)  loss_scale: 32768.0000 (46232.9145)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1303  max mem: 15572
Epoch: [2]  [ 980/1404]  eta: 0:04:22  lr: 0.000051  min_lr: 0.000000  loss: 4.7422 (4.8050)  loss_scale: 32768.0000 (46095.6575)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1256  max mem: 15572
Epoch: [2]  [ 990/1404]  eta: 0:04:15  lr: 0.000051  min_lr: 0.000000  loss: 4.7422 (4.8045)  loss_scale: 32768.0000 (45961.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0626  max mem: 15572
Epoch: [2]  [1000/1404]  eta: 0:04:09  lr: 0.000051  min_lr: 0.000000  loss: 4.7558 (4.8042)  loss_scale: 32768.0000 (45829.3706)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0538  max mem: 15572
Epoch: [2]  [1010/1404]  eta: 0:04:03  lr: 0.000051  min_lr: 0.000000  loss: 4.7390 (4.8036)  loss_scale: 32768.0000 (45700.1780)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1059  max mem: 15572
Epoch: [2]  [1020/1404]  eta: 0:03:57  lr: 0.000051  min_lr: 0.000000  loss: 4.7390 (4.8034)  loss_scale: 32768.0000 (45573.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1171  max mem: 15572
Epoch: [2]  [1030/1404]  eta: 0:03:51  lr: 0.000051  min_lr: 0.000000  loss: 4.7517 (4.8023)  loss_scale: 32768.0000 (45449.3113)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0652  max mem: 15572
Epoch: [2]  [1040/1404]  eta: 0:03:44  lr: 0.000051  min_lr: 0.000000  loss: 4.5826 (4.8005)  loss_scale: 32768.0000 (45327.4928)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0294  max mem: 15572
[2025-01-10 16:19:03,576] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:19:03,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:19:03,581] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:19:03,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1050/1404]  eta: 0:03:38  lr: 0.000052  min_lr: 0.000000  loss: 4.6860 (4.8002)  loss_scale: 32768.0000 (45488.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0194  max mem: 15572
Epoch: [2]  [1060/1404]  eta: 0:03:32  lr: 0.000052  min_lr: 0.000001  loss: 4.7500 (4.7985)  loss_scale: 65536.0000 (45677.5419)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0564  max mem: 15572
Epoch: [2]  [1070/1404]  eta: 0:03:26  lr: 0.000052  min_lr: 0.000001  loss: 4.5943 (4.7977)  loss_scale: 65536.0000 (45862.9617)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0642  max mem: 15572
Epoch: [2]  [1080/1404]  eta: 0:03:20  lr: 0.000052  min_lr: 0.000001  loss: 4.7803 (4.7973)  loss_scale: 65536.0000 (46044.9510)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1003  max mem: 15572
Epoch: [2]  [1090/1404]  eta: 0:03:14  lr: 0.000052  min_lr: 0.000001  loss: 4.6959 (4.7962)  loss_scale: 65536.0000 (46223.6040)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1033  max mem: 15572
Epoch: [2]  [1100/1404]  eta: 0:03:08  lr: 0.000052  min_lr: 0.000001  loss: 4.6561 (4.7955)  loss_scale: 65536.0000 (46399.0118)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.0972  max mem: 15572
[2025-01-10 16:19:46,472] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-10 16:19:46,473] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:19:46,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-10 16:19:46,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:19:46,526] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1110/1404]  eta: 0:03:01  lr: 0.000052  min_lr: 0.000001  loss: 4.6825 (4.7947)  loss_scale: 65536.0000 (46512.2736)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1078  max mem: 15572
Epoch: [2]  [1120/1404]  eta: 0:02:55  lr: 0.000052  min_lr: 0.000001  loss: 4.7437 (4.7934)  loss_scale: 32768.0000 (46389.6664)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0903  max mem: 15572
Epoch: [2]  [1130/1404]  eta: 0:02:49  lr: 0.000053  min_lr: 0.000001  loss: 4.7566 (4.7927)  loss_scale: 32768.0000 (46269.2272)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1075  max mem: 15572
Epoch: [2]  [1140/1404]  eta: 0:02:43  lr: 0.000053  min_lr: 0.000001  loss: 4.6825 (4.7910)  loss_scale: 32768.0000 (46150.8992)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1185  max mem: 15572
Epoch: [2]  [1150/1404]  eta: 0:02:36  lr: 0.000053  min_lr: 0.000001  loss: 4.7049 (4.7908)  loss_scale: 32768.0000 (46034.6273)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0740  max mem: 15572
Epoch: [2]  [1160/1404]  eta: 0:02:30  lr: 0.000053  min_lr: 0.000001  loss: 4.7538 (4.7902)  loss_scale: 32768.0000 (45920.3583)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1097  max mem: 15572
Epoch: [2]  [1170/1404]  eta: 0:02:24  lr: 0.000053  min_lr: 0.000001  loss: 4.7487 (4.7906)  loss_scale: 32768.0000 (45808.0410)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1642  max mem: 15572
Epoch: [2]  [1180/1404]  eta: 0:02:18  lr: 0.000053  min_lr: 0.000001  loss: 4.7423 (4.7906)  loss_scale: 32768.0000 (45697.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1041  max mem: 15572
Epoch: [2]  [1190/1404]  eta: 0:02:12  lr: 0.000053  min_lr: 0.000001  loss: 4.7423 (4.7904)  loss_scale: 32768.0000 (45589.0647)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1172  max mem: 15572
[2025-01-10 16:20:36,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[5.175138275808422e-07, 5.175138275808422e-07, 7.393054679726318e-07, 7.393054679726318e-07, 1.0561506685323313e-06, 1.0561506685323313e-06, 1.508786669331902e-06, 1.508786669331902e-06, 2.1554095276170026e-06, 2.1554095276170026e-06, 3.0791564680242897e-06, 3.0791564680242897e-06, 4.398794954320414e-06, 4.398794954320414e-06, 6.283992791886307e-06, 6.283992791886307e-06, 8.97713255983758e-06, 8.97713255983758e-06, 1.282447508548226e-05, 1.282447508548226e-05, 1.8320678693546087e-05, 1.8320678693546087e-05, 2.6172398133637267e-05, 2.6172398133637267e-05, 3.7389140190910385e-05, 3.7389140190910385e-05, 5.341305741558627e-05, 5.341305741558627e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 16:20:36,409] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=46.594387430952324, CurrSamplesPerSec=48.0677300177586, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1200/1404]  eta: 0:02:06  lr: 0.000054  min_lr: 0.000001  loss: 4.7033 (4.7891)  loss_scale: 32768.0000 (45482.3114)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1256  max mem: 15572
Epoch: [2]  [1210/1404]  eta: 0:01:59  lr: 0.000054  min_lr: 0.000001  loss: 4.6244 (4.7878)  loss_scale: 32768.0000 (45377.3212)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1391  max mem: 15572
Epoch: [2]  [1220/1404]  eta: 0:01:53  lr: 0.000054  min_lr: 0.000001  loss: 4.7072 (4.7873)  loss_scale: 32768.0000 (45274.0508)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1342  max mem: 15572
Epoch: [2]  [1230/1404]  eta: 0:01:47  lr: 0.000054  min_lr: 0.000001  loss: 4.7398 (4.7870)  loss_scale: 32768.0000 (45172.4582)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0403  max mem: 15572
[2025-01-10 16:21:04,034] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:21:04,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:21:04,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:21:04,086] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1240/1404]  eta: 0:01:41  lr: 0.000054  min_lr: 0.000001  loss: 4.7669 (4.7868)  loss_scale: 32768.0000 (45151.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0146  max mem: 15572
Epoch: [2]  [1250/1404]  eta: 0:01:35  lr: 0.000054  min_lr: 0.000001  loss: 4.7645 (4.7867)  loss_scale: 65536.0000 (45314.6603)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0947  max mem: 15572
Epoch: [2]  [1260/1404]  eta: 0:01:28  lr: 0.000054  min_lr: 0.000001  loss: 4.6925 (4.7855)  loss_scale: 65536.0000 (45475.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1471  max mem: 15572
Epoch: [2]  [1270/1404]  eta: 0:01:22  lr: 0.000054  min_lr: 0.000001  loss: 4.6622 (4.7847)  loss_scale: 65536.0000 (45632.8560)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1147  max mem: 15572
Epoch: [2]  [1280/1404]  eta: 0:01:16  lr: 0.000055  min_lr: 0.000001  loss: 4.6245 (4.7833)  loss_scale: 65536.0000 (45788.2279)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0722  max mem: 15572
Epoch: [2]  [1290/1404]  eta: 0:01:10  lr: 0.000055  min_lr: 0.000001  loss: 4.6654 (4.7829)  loss_scale: 65536.0000 (45941.1929)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1244  max mem: 15572
Epoch: [2]  [1300/1404]  eta: 0:01:04  lr: 0.000055  min_lr: 0.000001  loss: 4.6654 (4.7813)  loss_scale: 65536.0000 (46091.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1159  max mem: 15572
Epoch: [2]  [1310/1404]  eta: 0:00:58  lr: 0.000055  min_lr: 0.000001  loss: 4.7255 (4.7812)  loss_scale: 65536.0000 (46240.1220)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0829  max mem: 15572
Epoch: [2]  [1320/1404]  eta: 0:00:51  lr: 0.000055  min_lr: 0.000001  loss: 4.6900 (4.7805)  loss_scale: 65536.0000 (46386.1923)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0925  max mem: 15572
Epoch: [2]  [1330/1404]  eta: 0:00:45  lr: 0.000055  min_lr: 0.000001  loss: 4.6542 (4.7803)  loss_scale: 65536.0000 (46530.0676)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1113  max mem: 15572
Epoch: [2]  [1340/1404]  eta: 0:00:39  lr: 0.000055  min_lr: 0.000001  loss: 4.7015 (4.7801)  loss_scale: 65536.0000 (46671.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0939  max mem: 15572
Epoch: [2]  [1350/1404]  eta: 0:00:33  lr: 0.000056  min_lr: 0.000001  loss: 4.5642 (4.7783)  loss_scale: 65536.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0817  max mem: 15572
Epoch: [2]  [1360/1404]  eta: 0:00:27  lr: 0.000056  min_lr: 0.000001  loss: 4.5781 (4.7773)  loss_scale: 65536.0000 (46949.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1205  max mem: 15572
[2025-01-10 16:22:22,582] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:22:22,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:22:22,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:22:22,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:22:23,053] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4175
[2025-01-10 16:22:23,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:22:23,054] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4175
[2025-01-10 16:22:23,055] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:22:23,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 16:22:24,029] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4177
[2025-01-10 16:22:24,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:22:24,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:22:24,032] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4177
[2025-01-10 16:22:24,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [1370/1404]  eta: 0:00:20  lr: 0.000056  min_lr: 0.000001  loss: 4.7778 (4.7774)  loss_scale: 65536.0000 (47084.5806)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1273  max mem: 15572
Epoch: [2]  [1380/1404]  eta: 0:00:14  lr: 0.000056  min_lr: 0.000001  loss: 4.8010 (4.7778)  loss_scale: 32768.0000 (46980.9124)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1366  max mem: 15572
Epoch: [2]  [1390/1404]  eta: 0:00:08  lr: 0.000056  min_lr: 0.000001  loss: 4.6074 (4.7766)  loss_scale: 32768.0000 (46878.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0843  max mem: 15572
Epoch: [2]  [1400/1404]  eta: 0:00:02  lr: 0.000056  min_lr: 0.000001  loss: 4.6074 (4.7764)  loss_scale: 32768.0000 (46778.0157)  weight_decay: 0.0500 (0.0500)  time: 0.4638  data: 0.0282  max mem: 15572
Epoch: [2]  [1403/1404]  eta: 0:00:00  lr: 0.000056  min_lr: 0.000001  loss: 4.6324 (4.7759)  loss_scale: 32768.0000 (46748.0798)  weight_decay: 0.0500 (0.0500)  time: 0.4378  data: 0.0281  max mem: 15572
Epoch: [2] Total time: 0:14:22 (0.6142 s / it)
Averaged stats: lr: 0.000056  min_lr: 0.000001  loss: 4.6324 (4.7790)  loss_scale: 32768.0000 (46748.0798)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:16:28  loss: 3.9295 (3.9295)  acc1: 0.0000 (0.0000)  acc5: 55.5556 (55.5556)  time: 7.2719  data: 6.9928  max mem: 15572
Val:  [ 10/136]  eta: 0:01:45  loss: 4.6447 (4.5215)  acc1: 0.0000 (6.0606)  acc5: 0.0000 (20.2020)  time: 0.8403  data: 0.6365  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 4.4401 (4.3727)  acc1: 0.0000 (7.9365)  acc5: 11.1111 (26.7196)  time: 0.2575  data: 0.0203  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 3.8193 (4.2508)  acc1: 0.0000 (7.8853)  acc5: 44.4444 (36.0215)  time: 0.2795  data: 0.0372  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 3.6299 (4.1989)  acc1: 0.0000 (11.2466)  acc5: 55.5556 (34.5528)  time: 0.3165  data: 0.1079  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 4.5136 (4.3170)  acc1: 0.0000 (9.5861)  acc5: 0.0000 (29.0850)  time: 0.3945  data: 0.1904  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 4.7378 (4.3987)  acc1: 0.0000 (8.0146)  acc5: 0.0000 (25.3188)  time: 0.3957  data: 0.1862  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 4.5996 (4.3081)  acc1: 0.0000 (10.4851)  acc5: 5.5556 (28.1690)  time: 0.4091  data: 0.1941  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 4.1800 (4.3109)  acc1: 0.0000 (10.8368)  acc5: 27.7778 (29.0809)  time: 0.3921  data: 0.1885  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 4.3128 (4.3345)  acc1: 0.0000 (9.6459)  acc5: 11.1111 (27.2894)  time: 0.3696  data: 0.1461  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 4.6582 (4.3890)  acc1: 0.0000 (8.6909)  acc5: 5.5556 (24.9725)  time: 0.3997  data: 0.1666  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.5649 (4.3986)  acc1: 0.0000 (8.3083)  acc5: 5.5556 (24.9249)  time: 0.4096  data: 0.1867  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.3286 (4.3871)  acc1: 5.5556 (9.0909)  acc5: 16.6667 (26.0331)  time: 0.3694  data: 0.1453  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.2174 (4.3757)  acc1: 11.1111 (9.4148)  acc5: 33.3333 (27.0568)  time: 0.2777  data: 0.0876  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.4577 (4.3794)  acc1: 5.5556 (9.5823)  acc5: 33.3333 (27.3956)  time: 0.1889  data: 0.0278  max mem: 15572
Val: Total time: 0:00:52 (0.3894 s / it)
* Acc@1 9.214 Acc@5 26.679 loss 4.392
Accuracy of the network on the 4883 val videos: 9.2%
[2025-01-10 16:23:34,574] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 16:23:34,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 16:23:34,577] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 16:23:34,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 16:23:37,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 16:23:37,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 9.21%
Epoch: [3]  [   0/1404]  eta: 3:26:46  lr: 0.000056  min_lr: 0.000001  loss: 4.4240 (4.4240)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8366  data: 8.3655  max mem: 15572
Epoch: [3]  [  10/1404]  eta: 0:29:48  lr: 0.000056  min_lr: 0.000001  loss: 4.6630 (4.6980)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2827  data: 0.7961  max mem: 15572
Epoch: [3]  [  20/1404]  eta: 0:21:20  lr: 0.000057  min_lr: 0.000001  loss: 4.6502 (4.6122)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0199  max mem: 15572
Epoch: [3]  [  30/1404]  eta: 0:18:15  lr: 0.000057  min_lr: 0.000001  loss: 4.5596 (4.6019)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0007  max mem: 15572
Epoch: [3]  [  40/1404]  eta: 0:17:12  lr: 0.000057  min_lr: 0.000001  loss: 4.5601 (4.6251)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0789  max mem: 15572
Epoch: [3]  [  50/1404]  eta: 0:16:42  lr: 0.000057  min_lr: 0.000001  loss: 4.5755 (4.6241)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0792  max mem: 15572
Epoch: [3]  [  60/1404]  eta: 0:16:14  lr: 0.000057  min_lr: 0.000001  loss: 4.6039 (4.6379)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.0289  max mem: 15572
Epoch: [3]  [  70/1404]  eta: 0:15:46  lr: 0.000057  min_lr: 0.000001  loss: 4.7578 (4.6568)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.0738  max mem: 15572
Epoch: [3]  [  80/1404]  eta: 0:15:21  lr: 0.000057  min_lr: 0.000001  loss: 4.7578 (4.6546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0764  max mem: 15572
Epoch: [3]  [  90/1404]  eta: 0:15:14  lr: 0.000057  min_lr: 0.000001  loss: 4.6720 (4.6650)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.0880  max mem: 15572
[2025-01-10 16:24:42,502] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:24:42,503] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:24:42,504] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:24:42,504] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 100/1404]  eta: 0:14:39  lr: 0.000058  min_lr: 0.000001  loss: 4.7336 (4.6725)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0573  max mem: 15572
Epoch: [3]  [ 110/1404]  eta: 0:14:36  lr: 0.000058  min_lr: 0.000001  loss: 4.7312 (4.6757)  loss_scale: 65536.0000 (37786.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0966  max mem: 15572
Epoch: [3]  [ 120/1404]  eta: 0:14:16  lr: 0.000058  min_lr: 0.000001  loss: 4.7239 (4.6811)  loss_scale: 65536.0000 (40079.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1134  max mem: 15572
[2025-01-10 16:24:59,325] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4333
[2025-01-10 16:24:59,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:24:59,326] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:24:59,382] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4333
[2025-01-10 16:24:59,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 130/1404]  eta: 0:14:06  lr: 0.000058  min_lr: 0.000001  loss: 4.6890 (4.6783)  loss_scale: 32768.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0176  max mem: 15572
Epoch: [3]  [ 140/1404]  eta: 0:13:46  lr: 0.000058  min_lr: 0.000001  loss: 4.6795 (4.6793)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0006  max mem: 15572
Epoch: [3]  [ 150/1404]  eta: 0:13:37  lr: 0.000058  min_lr: 0.000001  loss: 4.7563 (4.6802)  loss_scale: 32768.0000 (38627.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0009  max mem: 15572
Epoch: [3]  [ 160/1404]  eta: 0:13:31  lr: 0.000058  min_lr: 0.000001  loss: 4.7306 (4.6808)  loss_scale: 32768.0000 (38263.2547)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0010  max mem: 15572
Epoch: [3]  [ 170/1404]  eta: 0:13:32  lr: 0.000059  min_lr: 0.000001  loss: 4.5444 (4.6759)  loss_scale: 32768.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.7111  data: 0.0007  max mem: 15572
Epoch: [3]  [ 180/1404]  eta: 0:13:16  lr: 0.000059  min_lr: 0.000001  loss: 4.6586 (4.6768)  loss_scale: 32768.0000 (37656.0442)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0008  max mem: 15572
Epoch: [3]  [ 190/1404]  eta: 0:13:01  lr: 0.000059  min_lr: 0.000001  loss: 4.6921 (4.6790)  loss_scale: 32768.0000 (37400.1257)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0009  max mem: 15572
Epoch: [3]  [ 200/1404]  eta: 0:12:45  lr: 0.000059  min_lr: 0.000001  loss: 4.6921 (4.6790)  loss_scale: 32768.0000 (37169.6716)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0008  max mem: 15572
Epoch: [3]  [ 210/1404]  eta: 0:12:39  lr: 0.000059  min_lr: 0.000001  loss: 4.7859 (4.6850)  loss_scale: 32768.0000 (36961.0616)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0007  max mem: 15572
Epoch: [3]  [ 220/1404]  eta: 0:12:32  lr: 0.000059  min_lr: 0.000001  loss: 4.7875 (4.6836)  loss_scale: 32768.0000 (36771.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0006  max mem: 15572
Epoch: [3]  [ 230/1404]  eta: 0:12:28  lr: 0.000059  min_lr: 0.000001  loss: 4.6836 (4.6844)  loss_scale: 32768.0000 (36598.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0008  max mem: 15572
Epoch: [3]  [ 240/1404]  eta: 0:12:23  lr: 0.000059  min_lr: 0.000001  loss: 4.6827 (4.6820)  loss_scale: 32768.0000 (36439.1037)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0010  max mem: 15572
[2025-01-10 16:26:16,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:26:16,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:26:16,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:26:16,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 250/1404]  eta: 0:12:12  lr: 0.000060  min_lr: 0.000001  loss: 4.5765 (4.6807)  loss_scale: 32768.0000 (36423.3944)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0010  max mem: 15572
Epoch: [3]  [ 260/1404]  eta: 0:12:06  lr: 0.000060  min_lr: 0.000001  loss: 4.6569 (4.6798)  loss_scale: 65536.0000 (37538.8199)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0009  max mem: 15572
Epoch: [3]  [ 270/1404]  eta: 0:11:58  lr: 0.000060  min_lr: 0.000001  loss: 4.7179 (4.6794)  loss_scale: 65536.0000 (38571.9262)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0008  max mem: 15572
Epoch: [3]  [ 280/1404]  eta: 0:11:49  lr: 0.000060  min_lr: 0.000001  loss: 4.5548 (4.6736)  loss_scale: 65536.0000 (39531.5018)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
Epoch: [3]  [ 290/1404]  eta: 0:11:45  lr: 0.000060  min_lr: 0.000001  loss: 4.5411 (4.6744)  loss_scale: 65536.0000 (40425.1271)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0011  max mem: 15572
Epoch: [3]  [ 300/1404]  eta: 0:11:39  lr: 0.000060  min_lr: 0.000001  loss: 4.6792 (4.6759)  loss_scale: 65536.0000 (41259.3754)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.0015  max mem: 15572
Epoch: [3]  [ 310/1404]  eta: 0:11:28  lr: 0.000060  min_lr: 0.000001  loss: 4.6803 (4.6761)  loss_scale: 65536.0000 (42039.9743)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0012  max mem: 15572
Epoch: [3]  [ 320/1404]  eta: 0:11:22  lr: 0.000061  min_lr: 0.000001  loss: 4.6654 (4.6753)  loss_scale: 65536.0000 (42771.9377)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0303  max mem: 15572
Epoch: [3]  [ 330/1404]  eta: 0:11:19  lr: 0.000061  min_lr: 0.000001  loss: 4.6520 (4.6747)  loss_scale: 65536.0000 (43459.6737)  weight_decay: 0.0500 (0.0500)  time: 0.6859  data: 0.1023  max mem: 15572
Epoch: [3]  [ 340/1404]  eta: 0:11:13  lr: 0.000061  min_lr: 0.000001  loss: 4.6190 (4.6744)  loss_scale: 65536.0000 (44107.0733)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.1284  max mem: 15572
[2025-01-10 16:27:18,649] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-10 16:27:18,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:27:18,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:27:18,657] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-10 16:27:18,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 350/1404]  eta: 0:11:08  lr: 0.000061  min_lr: 0.000001  loss: 4.5948 (4.6734)  loss_scale: 65536.0000 (44437.5157)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1247  max mem: 15572
Epoch: [3]  [ 360/1404]  eta: 0:10:58  lr: 0.000061  min_lr: 0.000001  loss: 4.6803 (4.6756)  loss_scale: 32768.0000 (44114.2604)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0690  max mem: 15572
Epoch: [3]  [ 370/1404]  eta: 0:10:54  lr: 0.000061  min_lr: 0.000001  loss: 4.7357 (4.6789)  loss_scale: 32768.0000 (43808.4313)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0560  max mem: 15572
Epoch: [3]  [ 380/1404]  eta: 0:10:46  lr: 0.000061  min_lr: 0.000001  loss: 4.6617 (4.6774)  loss_scale: 32768.0000 (43518.6562)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.0990  max mem: 15572
Epoch: [3]  [ 390/1404]  eta: 0:10:39  lr: 0.000061  min_lr: 0.000001  loss: 4.5512 (4.6743)  loss_scale: 32768.0000 (43243.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0746  max mem: 15572
Epoch: [3]  [ 400/1404]  eta: 0:10:30  lr: 0.000062  min_lr: 0.000001  loss: 4.6113 (4.6736)  loss_scale: 32768.0000 (42982.4638)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0316  max mem: 15572
Epoch: [3]  [ 410/1404]  eta: 0:10:23  lr: 0.000062  min_lr: 0.000001  loss: 4.7173 (4.6736)  loss_scale: 32768.0000 (42733.9367)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0008  max mem: 15572
Epoch: [3]  [ 420/1404]  eta: 0:10:17  lr: 0.000062  min_lr: 0.000001  loss: 4.6887 (4.6733)  loss_scale: 32768.0000 (42497.2162)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0008  max mem: 15572
Epoch: [3]  [ 430/1404]  eta: 0:10:12  lr: 0.000062  min_lr: 0.000001  loss: 4.6623 (4.6734)  loss_scale: 32768.0000 (42271.4803)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.0008  max mem: 15572
Epoch: [3]  [ 440/1404]  eta: 0:10:03  lr: 0.000062  min_lr: 0.000001  loss: 4.6333 (4.6731)  loss_scale: 32768.0000 (42055.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0009  max mem: 15572
Epoch: [3]  [ 450/1404]  eta: 0:09:56  lr: 0.000062  min_lr: 0.000001  loss: 4.6874 (4.6746)  loss_scale: 32768.0000 (41850.0399)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0008  max mem: 15572
Epoch: [3]  [ 460/1404]  eta: 0:09:51  lr: 0.000062  min_lr: 0.000001  loss: 4.6781 (4.6725)  loss_scale: 32768.0000 (41653.0325)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0350  max mem: 15572
Epoch: [3]  [ 470/1404]  eta: 0:09:44  lr: 0.000063  min_lr: 0.000001  loss: 4.6443 (4.6735)  loss_scale: 32768.0000 (41464.3907)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.0349  max mem: 15572
[2025-01-10 16:28:35,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:28:35,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:28:35,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:28:35,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 480/1404]  eta: 0:09:36  lr: 0.000063  min_lr: 0.000001  loss: 4.6729 (4.6739)  loss_scale: 32768.0000 (41556.0915)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0267  max mem: 15572
Epoch: [3]  [ 490/1404]  eta: 0:09:30  lr: 0.000063  min_lr: 0.000001  loss: 4.6949 (4.6755)  loss_scale: 65536.0000 (42044.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0817  max mem: 15572
Epoch: [3]  [ 500/1404]  eta: 0:09:25  lr: 0.000063  min_lr: 0.000001  loss: 4.6619 (4.6736)  loss_scale: 65536.0000 (42513.3733)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.1369  max mem: 15572
Epoch: [3]  [ 510/1404]  eta: 0:09:17  lr: 0.000063  min_lr: 0.000001  loss: 4.6824 (4.6737)  loss_scale: 65536.0000 (42963.9139)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0896  max mem: 15572
Epoch: [3]  [ 520/1404]  eta: 0:09:10  lr: 0.000063  min_lr: 0.000001  loss: 4.6795 (4.6727)  loss_scale: 65536.0000 (43397.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0550  max mem: 15572
Epoch: [3]  [ 530/1404]  eta: 0:09:04  lr: 0.000063  min_lr: 0.000001  loss: 4.5683 (4.6716)  loss_scale: 65536.0000 (43814.0866)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0975  max mem: 15572
Epoch: [3]  [ 540/1404]  eta: 0:08:58  lr: 0.000063  min_lr: 0.000001  loss: 4.6018 (4.6711)  loss_scale: 65536.0000 (44215.6007)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0998  max mem: 15572
Epoch: [3]  [ 550/1404]  eta: 0:08:50  lr: 0.000064  min_lr: 0.000001  loss: 4.6733 (4.6727)  loss_scale: 65536.0000 (44602.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0663  max mem: 15572
Epoch: [3]  [ 560/1404]  eta: 0:08:45  lr: 0.000064  min_lr: 0.000001  loss: 4.6706 (4.6722)  loss_scale: 65536.0000 (44975.6863)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0173  max mem: 15572
Epoch: [3]  [ 570/1404]  eta: 0:08:42  lr: 0.000064  min_lr: 0.000001  loss: 4.6470 (4.6706)  loss_scale: 65536.0000 (45335.7618)  weight_decay: 0.0500 (0.0500)  time: 0.7516  data: 0.0006  max mem: 15572
Epoch: [3]  [ 580/1404]  eta: 0:08:34  lr: 0.000064  min_lr: 0.000001  loss: 4.6470 (4.6685)  loss_scale: 65536.0000 (45683.4423)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.0006  max mem: 15572
[2025-01-10 16:29:41,838] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4796
[2025-01-10 16:29:41,838] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:29:41,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:29:41,839] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4796
[2025-01-10 16:29:41,840] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 590/1404]  eta: 0:08:29  lr: 0.000064  min_lr: 0.000001  loss: 4.6407 (4.6681)  loss_scale: 65536.0000 (45631.2420)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0006  max mem: 15572
Epoch: [3]  [ 600/1404]  eta: 0:08:21  lr: 0.000064  min_lr: 0.000001  loss: 4.6351 (4.6675)  loss_scale: 32768.0000 (45417.2113)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0006  max mem: 15572
Epoch: [3]  [ 610/1404]  eta: 0:08:15  lr: 0.000064  min_lr: 0.000001  loss: 4.6340 (4.6667)  loss_scale: 32768.0000 (45210.1866)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0006  max mem: 15572
Epoch: [3]  [ 620/1404]  eta: 0:08:07  lr: 0.000065  min_lr: 0.000001  loss: 4.7512 (4.6684)  loss_scale: 32768.0000 (45009.8293)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0006  max mem: 15572
Epoch: [3]  [ 630/1404]  eta: 0:08:01  lr: 0.000065  min_lr: 0.000001  loss: 4.7030 (4.6683)  loss_scale: 32768.0000 (44815.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0008  max mem: 15572
Epoch: [3]  [ 640/1404]  eta: 0:07:54  lr: 0.000065  min_lr: 0.000001  loss: 4.6696 (4.6675)  loss_scale: 32768.0000 (44627.8690)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0008  max mem: 15572
Epoch: [3]  [ 650/1404]  eta: 0:07:48  lr: 0.000065  min_lr: 0.000001  loss: 4.6085 (4.6671)  loss_scale: 32768.0000 (44445.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0008  max mem: 15572
Epoch: [3]  [ 660/1404]  eta: 0:07:41  lr: 0.000065  min_lr: 0.000001  loss: 4.5991 (4.6659)  loss_scale: 32768.0000 (44269.0227)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0012  max mem: 15572
Epoch: [3]  [ 670/1404]  eta: 0:07:35  lr: 0.000065  min_lr: 0.000001  loss: 4.6034 (4.6671)  loss_scale: 32768.0000 (44097.6215)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0010  max mem: 15572
Epoch: [3]  [ 680/1404]  eta: 0:07:29  lr: 0.000065  min_lr: 0.000001  loss: 4.7022 (4.6664)  loss_scale: 32768.0000 (43931.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0006  max mem: 15572
Epoch: [3]  [ 690/1404]  eta: 0:07:23  lr: 0.000065  min_lr: 0.000001  loss: 4.6638 (4.6673)  loss_scale: 32768.0000 (43769.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.0005  max mem: 15572
Epoch: [3]  [ 700/1404]  eta: 0:07:16  lr: 0.000066  min_lr: 0.000001  loss: 4.6340 (4.6655)  loss_scale: 32768.0000 (43612.7589)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0007  max mem: 15572
Epoch: [3]  [ 710/1404]  eta: 0:07:10  lr: 0.000066  min_lr: 0.000001  loss: 4.6183 (4.6657)  loss_scale: 32768.0000 (43460.2307)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0009  max mem: 15572
[2025-01-10 16:31:00,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:31:00,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:31:00,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:31:00,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 720/1404]  eta: 0:07:03  lr: 0.000066  min_lr: 0.000001  loss: 4.7125 (4.6670)  loss_scale: 32768.0000 (43675.5173)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0012  max mem: 15572
Epoch: [3]  [ 730/1404]  eta: 0:06:56  lr: 0.000066  min_lr: 0.000001  loss: 4.6791 (4.6674)  loss_scale: 65536.0000 (43974.5663)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0011  max mem: 15572
Epoch: [3]  [ 740/1404]  eta: 0:06:50  lr: 0.000066  min_lr: 0.000001  loss: 4.5830 (4.6657)  loss_scale: 65536.0000 (44265.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0009  max mem: 15572
[2025-01-10 16:31:21,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-10 16:31:21,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:31:21,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:31:21,473] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-10 16:31:21,474] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 750/1404]  eta: 0:06:45  lr: 0.000066  min_lr: 0.000001  loss: 4.4714 (4.6633)  loss_scale: 65536.0000 (44374.2423)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0010  max mem: 15572
Epoch: [3]  [ 760/1404]  eta: 0:06:39  lr: 0.000066  min_lr: 0.000001  loss: 4.5805 (4.6628)  loss_scale: 32768.0000 (44221.7293)  weight_decay: 0.0500 (0.0500)  time: 0.6945  data: 0.0014  max mem: 15572
Epoch: [3]  [ 770/1404]  eta: 0:06:32  lr: 0.000067  min_lr: 0.000001  loss: 4.6197 (4.6628)  loss_scale: 32768.0000 (44073.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0011  max mem: 15572
Epoch: [3]  [ 780/1404]  eta: 0:06:27  lr: 0.000067  min_lr: 0.000001  loss: 4.6197 (4.6627)  loss_scale: 32768.0000 (43928.4200)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.0009  max mem: 15572
[2025-01-10 16:31:46,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[6.469246371784522e-07, 6.469246371784522e-07, 9.241780531120747e-07, 9.241780531120747e-07, 1.3202543615886782e-06, 1.3202543615886782e-06, 1.8860776594123977e-06, 1.8860776594123977e-06, 2.694396656303425e-06, 2.694396656303425e-06, 3.849138080433465e-06, 3.849138080433465e-06, 5.498768686333522e-06, 5.498768686333522e-06, 7.855383837619317e-06, 7.855383837619317e-06, 1.1221976910884738e-05, 1.1221976910884738e-05, 1.60313955869782e-05, 1.60313955869782e-05, 2.290199369568314e-05, 2.290199369568314e-05, 3.271713385097592e-05, 3.271713385097592e-05, 4.673876264425132e-05, 4.673876264425132e-05, 6.676966092035903e-05, 6.676966092035903e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 16:31:46,228] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=46.25228655824029, CurrSamplesPerSec=46.04533391151132, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 790/1404]  eta: 0:06:20  lr: 0.000067  min_lr: 0.000001  loss: 4.6399 (4.6623)  loss_scale: 32768.0000 (43787.3274)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0009  max mem: 15572
Epoch: [3]  [ 800/1404]  eta: 0:06:14  lr: 0.000067  min_lr: 0.000001  loss: 4.6135 (4.6618)  loss_scale: 32768.0000 (43649.7578)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0006  max mem: 15572
Epoch: [3]  [ 810/1404]  eta: 0:06:07  lr: 0.000067  min_lr: 0.000001  loss: 4.6101 (4.6611)  loss_scale: 32768.0000 (43515.5808)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0007  max mem: 15572
Epoch: [3]  [ 820/1404]  eta: 0:06:01  lr: 0.000067  min_lr: 0.000001  loss: 4.6194 (4.6591)  loss_scale: 32768.0000 (43384.6724)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0007  max mem: 15572
Epoch: [3]  [ 830/1404]  eta: 0:05:54  lr: 0.000067  min_lr: 0.000001  loss: 4.6194 (4.6596)  loss_scale: 32768.0000 (43256.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0006  max mem: 15572
Epoch: [3]  [ 840/1404]  eta: 0:05:48  lr: 0.000067  min_lr: 0.000001  loss: 4.6180 (4.6595)  loss_scale: 32768.0000 (43132.1950)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0006  max mem: 15572
Epoch: [3]  [ 850/1404]  eta: 0:05:42  lr: 0.000068  min_lr: 0.000001  loss: 4.6641 (4.6604)  loss_scale: 32768.0000 (43010.4066)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0005  max mem: 15572
Epoch: [3]  [ 860/1404]  eta: 0:05:36  lr: 0.000068  min_lr: 0.000001  loss: 4.6430 (4.6596)  loss_scale: 32768.0000 (42891.4472)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0008  max mem: 15572
Epoch: [3]  [ 870/1404]  eta: 0:05:30  lr: 0.000068  min_lr: 0.000001  loss: 4.5391 (4.6598)  loss_scale: 32768.0000 (42775.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.0008  max mem: 15572
[2025-01-10 16:32:40,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:32:40,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:32:40,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:32:40,207] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 880/1404]  eta: 0:05:24  lr: 0.000068  min_lr: 0.000001  loss: 4.4690 (4.6570)  loss_scale: 32768.0000 (42847.6005)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0008  max mem: 15572
Epoch: [3]  [ 890/1404]  eta: 0:05:17  lr: 0.000068  min_lr: 0.000001  loss: 4.5836 (4.6572)  loss_scale: 65536.0000 (43102.2402)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0009  max mem: 15572
Epoch: [3]  [ 900/1404]  eta: 0:05:11  lr: 0.000068  min_lr: 0.000001  loss: 4.6841 (4.6581)  loss_scale: 65536.0000 (43351.2275)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0010  max mem: 15572
Epoch: [3]  [ 910/1404]  eta: 0:05:05  lr: 0.000068  min_lr: 0.000001  loss: 4.6950 (4.6570)  loss_scale: 65536.0000 (43594.7486)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0009  max mem: 15572
Epoch: [3]  [ 920/1404]  eta: 0:05:04  lr: 0.000069  min_lr: 0.000001  loss: 4.6539 (4.6575)  loss_scale: 65536.0000 (43832.9815)  weight_decay: 0.0500 (0.0500)  time: 1.1791  data: 0.0008  max mem: 15572
Epoch: [3]  [ 930/1404]  eta: 0:04:57  lr: 0.000069  min_lr: 0.000001  loss: 4.4697 (4.6558)  loss_scale: 65536.0000 (44066.0967)  weight_decay: 0.0500 (0.0500)  time: 1.0911  data: 0.0007  max mem: 15572
Epoch: [3]  [ 940/1404]  eta: 0:04:50  lr: 0.000069  min_lr: 0.000001  loss: 4.6257 (4.6558)  loss_scale: 65536.0000 (44294.2572)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0007  max mem: 15572
Epoch: [3]  [ 950/1404]  eta: 0:04:45  lr: 0.000069  min_lr: 0.000001  loss: 4.6791 (4.6553)  loss_scale: 65536.0000 (44517.6193)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0010  max mem: 15572
Epoch: [3]  [ 960/1404]  eta: 0:04:38  lr: 0.000069  min_lr: 0.000001  loss: 4.6129 (4.6542)  loss_scale: 65536.0000 (44736.3330)  weight_decay: 0.0500 (0.0500)  time: 0.6752  data: 0.0009  max mem: 15572
Epoch: [3]  [ 970/1404]  eta: 0:04:32  lr: 0.000069  min_lr: 0.000001  loss: 4.7214 (4.6553)  loss_scale: 65536.0000 (44950.5417)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0006  max mem: 15572
Epoch: [3]  [ 980/1404]  eta: 0:04:25  lr: 0.000069  min_lr: 0.000001  loss: 4.7358 (4.6553)  loss_scale: 65536.0000 (45160.3833)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0008  max mem: 15572
Epoch: [3]  [ 990/1404]  eta: 0:04:19  lr: 0.000069  min_lr: 0.000001  loss: 4.7291 (4.6559)  loss_scale: 65536.0000 (45365.9899)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0008  max mem: 15572
[2025-01-10 16:34:04,972] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5212
[2025-01-10 16:34:04,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:34:04,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5212
[2025-01-10 16:34:04,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:34:04,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1000/1404]  eta: 0:04:13  lr: 0.000070  min_lr: 0.000001  loss: 4.6474 (4.6546)  loss_scale: 65536.0000 (45534.7532)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0009  max mem: 15572
Epoch: [3]  [1010/1404]  eta: 0:04:06  lr: 0.000070  min_lr: 0.000001  loss: 4.5745 (4.6537)  loss_scale: 32768.0000 (45408.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0009  max mem: 15572
Epoch: [3]  [1020/1404]  eta: 0:04:00  lr: 0.000070  min_lr: 0.000001  loss: 4.5433 (4.6525)  loss_scale: 32768.0000 (45284.6699)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0008  max mem: 15572
Epoch: [3]  [1030/1404]  eta: 0:03:53  lr: 0.000070  min_lr: 0.000001  loss: 4.5452 (4.6527)  loss_scale: 32768.0000 (45163.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0007  max mem: 15572
Epoch: [3]  [1040/1404]  eta: 0:03:47  lr: 0.000070  min_lr: 0.000001  loss: 4.5452 (4.6516)  loss_scale: 32768.0000 (45044.1960)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0008  max mem: 15572
Epoch: [3]  [1050/1404]  eta: 0:03:41  lr: 0.000070  min_lr: 0.000001  loss: 4.5364 (4.6515)  loss_scale: 32768.0000 (44927.3911)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0011  max mem: 15572
Epoch: [3]  [1060/1404]  eta: 0:03:34  lr: 0.000070  min_lr: 0.000001  loss: 4.5364 (4.6505)  loss_scale: 32768.0000 (44812.7879)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0009  max mem: 15572
Epoch: [3]  [1070/1404]  eta: 0:03:28  lr: 0.000071  min_lr: 0.000001  loss: 4.4658 (4.6492)  loss_scale: 32768.0000 (44700.3249)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0010  max mem: 15572
Epoch: [3]  [1080/1404]  eta: 0:03:22  lr: 0.000071  min_lr: 0.000001  loss: 4.5874 (4.6503)  loss_scale: 32768.0000 (44589.9426)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0016  max mem: 15572
Epoch: [3]  [1090/1404]  eta: 0:03:16  lr: 0.000071  min_lr: 0.000001  loss: 4.7800 (4.6515)  loss_scale: 32768.0000 (44481.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0014  max mem: 15572
Epoch: [3]  [1100/1404]  eta: 0:03:09  lr: 0.000071  min_lr: 0.000001  loss: 4.6735 (4.6501)  loss_scale: 32768.0000 (44375.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0007  max mem: 15572
Epoch: [3]  [1110/1404]  eta: 0:03:03  lr: 0.000071  min_lr: 0.000001  loss: 4.4429 (4.6487)  loss_scale: 32768.0000 (44270.7183)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0007  max mem: 15572
Epoch: [3]  [1120/1404]  eta: 0:02:57  lr: 0.000071  min_lr: 0.000001  loss: 4.4766 (4.6488)  loss_scale: 32768.0000 (44168.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0007  max mem: 15572
[2025-01-10 16:35:22,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:35:22,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:35:22,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:35:22,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1130/1404]  eta: 0:02:50  lr: 0.000071  min_lr: 0.000001  loss: 4.6857 (4.6488)  loss_scale: 32768.0000 (44125.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0007  max mem: 15572
Epoch: [3]  [1140/1404]  eta: 0:02:44  lr: 0.000071  min_lr: 0.000001  loss: 4.6857 (4.6492)  loss_scale: 65536.0000 (44312.9045)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0007  max mem: 15572
[2025-01-10 16:35:34,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5361
[2025-01-10 16:35:34,958] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:35:34,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:35:34,959] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5361
[2025-01-10 16:35:34,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [1150/1404]  eta: 0:02:38  lr: 0.000072  min_lr: 0.000001  loss: 4.6856 (4.6482)  loss_scale: 65536.0000 (44440.3545)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0008  max mem: 15572
Epoch: [3]  [1160/1404]  eta: 0:02:32  lr: 0.000072  min_lr: 0.000001  loss: 4.5118 (4.6458)  loss_scale: 32768.0000 (44339.8174)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0010  max mem: 15572
Epoch: [3]  [1170/1404]  eta: 0:02:25  lr: 0.000072  min_lr: 0.000001  loss: 4.5170 (4.6445)  loss_scale: 32768.0000 (44240.9974)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0008  max mem: 15572
Epoch: [3]  [1180/1404]  eta: 0:02:19  lr: 0.000072  min_lr: 0.000001  loss: 4.5170 (4.6438)  loss_scale: 32768.0000 (44143.8510)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0009  max mem: 15572
Epoch: [3]  [1190/1404]  eta: 0:02:13  lr: 0.000072  min_lr: 0.000001  loss: 4.7208 (4.6451)  loss_scale: 32768.0000 (44048.3359)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0010  max mem: 15572
Epoch: [3]  [1200/1404]  eta: 0:02:07  lr: 0.000072  min_lr: 0.000001  loss: 4.7327 (4.6449)  loss_scale: 32768.0000 (43954.4113)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0008  max mem: 15572
Epoch: [3]  [1210/1404]  eta: 0:02:00  lr: 0.000072  min_lr: 0.000001  loss: 4.5624 (4.6435)  loss_scale: 32768.0000 (43862.0380)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
Epoch: [3]  [1220/1404]  eta: 0:01:54  lr: 0.000073  min_lr: 0.000001  loss: 4.5428 (4.6432)  loss_scale: 32768.0000 (43771.1777)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0007  max mem: 15572
Epoch: [3]  [1230/1404]  eta: 0:01:48  lr: 0.000073  min_lr: 0.000001  loss: 4.5428 (4.6425)  loss_scale: 32768.0000 (43681.7937)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0009  max mem: 15572
Epoch: [3]  [1240/1404]  eta: 0:01:42  lr: 0.000073  min_lr: 0.000001  loss: 4.5750 (4.6426)  loss_scale: 32768.0000 (43593.8501)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0014  max mem: 15572
Epoch: [3]  [1250/1404]  eta: 0:01:35  lr: 0.000073  min_lr: 0.000001  loss: 4.6157 (4.6418)  loss_scale: 32768.0000 (43507.3125)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0013  max mem: 15572
Epoch: [3]  [1260/1404]  eta: 0:01:29  lr: 0.000073  min_lr: 0.000001  loss: 4.6397 (4.6414)  loss_scale: 32768.0000 (43422.1475)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0009  max mem: 15572
Epoch: [3]  [1270/1404]  eta: 0:01:23  lr: 0.000073  min_lr: 0.000001  loss: 4.6576 (4.6420)  loss_scale: 32768.0000 (43338.3226)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0011  max mem: 15572
[2025-01-10 16:36:54,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:36:54,419] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:36:54,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:36:54,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1280/1404]  eta: 0:01:17  lr: 0.000073  min_lr: 0.000001  loss: 4.4998 (4.6401)  loss_scale: 32768.0000 (43332.5464)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0010  max mem: 15572
Epoch: [3]  [1290/1404]  eta: 0:01:10  lr: 0.000073  min_lr: 0.000001  loss: 4.4457 (4.6387)  loss_scale: 65536.0000 (43504.5329)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.0006  max mem: 15572
Epoch: [3]  [1300/1404]  eta: 0:01:04  lr: 0.000074  min_lr: 0.000001  loss: 4.5056 (4.6376)  loss_scale: 65536.0000 (43673.8755)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0004  max mem: 15572
Epoch: [3]  [1310/1404]  eta: 0:00:58  lr: 0.000074  min_lr: 0.000001  loss: 4.5318 (4.6378)  loss_scale: 65536.0000 (43840.6346)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0005  max mem: 15572
[2025-01-10 16:37:17,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5528
[2025-01-10 16:37:17,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:37:17,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:37:17,834] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5528
[2025-01-10 16:37:17,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [1320/1404]  eta: 0:00:52  lr: 0.000074  min_lr: 0.000001  loss: 4.5641 (4.6371)  loss_scale: 65536.0000 (43880.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0008  max mem: 15572
Epoch: [3]  [1330/1404]  eta: 0:00:46  lr: 0.000074  min_lr: 0.000001  loss: 4.5219 (4.6356)  loss_scale: 32768.0000 (43797.3494)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0009  max mem: 15572
Epoch: [3]  [1340/1404]  eta: 0:00:39  lr: 0.000074  min_lr: 0.000001  loss: 4.5219 (4.6359)  loss_scale: 32768.0000 (43715.1022)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0006  max mem: 15572
Epoch: [3]  [1350/1404]  eta: 0:00:33  lr: 0.000074  min_lr: 0.000001  loss: 4.6702 (4.6360)  loss_scale: 32768.0000 (43634.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0007  max mem: 15572
Epoch: [3]  [1360/1404]  eta: 0:00:27  lr: 0.000074  min_lr: 0.000001  loss: 4.6108 (4.6352)  loss_scale: 32768.0000 (43554.2337)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0011  max mem: 15572
Epoch: [3]  [1370/1404]  eta: 0:00:21  lr: 0.000075  min_lr: 0.000001  loss: 4.5283 (4.6341)  loss_scale: 32768.0000 (43475.5594)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0010  max mem: 15572
Epoch: [3]  [1380/1404]  eta: 0:00:14  lr: 0.000075  min_lr: 0.000001  loss: 4.5669 (4.6335)  loss_scale: 32768.0000 (43398.0246)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.0007  max mem: 15572
Epoch: [3]  [1390/1404]  eta: 0:00:08  lr: 0.000075  min_lr: 0.000001  loss: 4.5217 (4.6327)  loss_scale: 32768.0000 (43321.6046)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0008  max mem: 15572
Epoch: [3]  [1400/1404]  eta: 0:00:02  lr: 0.000075  min_lr: 0.000001  loss: 4.5070 (4.6325)  loss_scale: 32768.0000 (43246.2755)  weight_decay: 0.0500 (0.0500)  time: 0.4705  data: 0.0005  max mem: 15572
Epoch: [3]  [1403/1404]  eta: 0:00:00  lr: 0.000075  min_lr: 0.000001  loss: 4.5123 (4.6322)  loss_scale: 32768.0000 (43223.8860)  weight_decay: 0.0500 (0.0500)  time: 0.4397  data: 0.0005  max mem: 15572
Epoch: [3] Total time: 0:14:30 (0.6199 s / it)
Averaged stats: lr: 0.000075  min_lr: 0.000001  loss: 4.5123 (4.6237)  loss_scale: 32768.0000 (43223.8860)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:39  loss: 2.6372 (2.6372)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.7049  data: 4.4819  max mem: 15572
Val:  [ 10/136]  eta: 0:01:46  loss: 4.1425 (4.0922)  acc1: 0.0000 (13.6364)  acc5: 16.6667 (27.2727)  time: 0.8452  data: 0.6336  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 4.1425 (4.0425)  acc1: 0.0000 (10.8466)  acc5: 16.6667 (32.0106)  time: 0.3911  data: 0.1745  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 3.4759 (3.8597)  acc1: 5.5556 (17.0251)  acc5: 55.5556 (40.8602)  time: 0.2660  data: 0.0506  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 3.1431 (3.8109)  acc1: 5.5556 (18.6992)  acc5: 72.2222 (41.7344)  time: 0.3076  data: 0.1047  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 4.3950 (3.9633)  acc1: 0.0000 (15.6863)  acc5: 0.0000 (34.9673)  time: 0.4038  data: 0.1979  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 4.5664 (4.0681)  acc1: 0.0000 (13.2058)  acc5: 0.0000 (31.5118)  time: 0.3603  data: 0.1546  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 4.4171 (4.0074)  acc1: 0.0000 (15.1017)  acc5: 22.2222 (33.8811)  time: 0.3429  data: 0.1441  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 4.1356 (4.0295)  acc1: 0.0000 (15.0206)  acc5: 33.3333 (33.8820)  time: 0.3779  data: 0.1794  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.1895 (4.0439)  acc1: 0.0000 (14.2247)  acc5: 33.3333 (34.1270)  time: 0.3683  data: 0.1765  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.1974 (4.0647)  acc1: 0.0000 (13.3113)  acc5: 27.7778 (33.9384)  time: 0.3665  data: 0.1839  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.3653 (4.0931)  acc1: 0.0000 (12.5125)  acc5: 11.1111 (32.4324)  time: 0.4019  data: 0.2152  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.1304 (4.0557)  acc1: 5.5556 (14.0496)  acc5: 22.2222 (35.0781)  time: 0.3987  data: 0.1982  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.6260 (4.0317)  acc1: 16.6667 (15.0551)  acc5: 44.4444 (35.5810)  time: 0.3062  data: 0.1277  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.8538 (4.0334)  acc1: 16.6667 (14.9468)  acc5: 44.4444 (36.1179)  time: 0.2568  data: 0.1020  max mem: 15572
Val: Total time: 0:00:51 (0.3822 s / it)
* Acc@1 15.049 Acc@5 35.811 loss 4.057
Accuracy of the network on the 4883 val videos: 15.0%
[2025-01-10 16:38:59,531] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 16:38:59,534] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 16:38:59,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 16:38:59,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 16:39:01,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 16:39:01,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 15.05%
Epoch: [4]  [   0/1404]  eta: 2:40:32  lr: 0.000075  min_lr: 0.000001  loss: 4.0628 (4.0628)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.8609  data: 6.3987  max mem: 15572
Epoch: [4]  [  10/1404]  eta: 0:27:50  lr: 0.000075  min_lr: 0.000001  loss: 4.6361 (4.5338)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1985  data: 0.7109  max mem: 15572
Epoch: [4]  [  20/1404]  eta: 0:22:44  lr: 0.000075  min_lr: 0.000001  loss: 4.6309 (4.5576)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.2032  max mem: 15572
Epoch: [4]  [  30/1404]  eta: 0:19:29  lr: 0.000075  min_lr: 0.000001  loss: 4.6237 (4.5773)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.1563  max mem: 15572
Epoch: [4]  [  40/1404]  eta: 0:17:34  lr: 0.000076  min_lr: 0.000001  loss: 4.5500 (4.5638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0539  max mem: 15572
[2025-01-10 16:39:34,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:39:34,194] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:39:34,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:39:34,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [  50/1404]  eta: 0:16:24  lr: 0.000076  min_lr: 0.000001  loss: 4.5441 (4.5512)  loss_scale: 32768.0000 (39193.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0467  max mem: 15572
Epoch: [4]  [  60/1404]  eta: 0:16:01  lr: 0.000076  min_lr: 0.000001  loss: 4.5372 (4.5313)  loss_scale: 65536.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0717  max mem: 15572
Epoch: [4]  [  70/1404]  eta: 0:15:22  lr: 0.000076  min_lr: 0.000001  loss: 4.5372 (4.5321)  loss_scale: 65536.0000 (46613.6338)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0683  max mem: 15572
Epoch: [4]  [  80/1404]  eta: 0:15:10  lr: 0.000076  min_lr: 0.000001  loss: 4.5383 (4.5347)  loss_scale: 65536.0000 (48949.7284)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0174  max mem: 15572
Epoch: [4]  [  90/1404]  eta: 0:14:56  lr: 0.000076  min_lr: 0.000001  loss: 4.5383 (4.5420)  loss_scale: 65536.0000 (50772.3956)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.0825  max mem: 15572
Epoch: [4]  [ 100/1404]  eta: 0:14:52  lr: 0.000076  min_lr: 0.000001  loss: 4.5964 (4.5453)  loss_scale: 65536.0000 (52234.1386)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.1726  max mem: 15572
Epoch: [4]  [ 110/1404]  eta: 0:14:39  lr: 0.000076  min_lr: 0.000001  loss: 4.5432 (4.5393)  loss_scale: 65536.0000 (53432.5045)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.1460  max mem: 15572
Epoch: [4]  [ 120/1404]  eta: 0:14:17  lr: 0.000077  min_lr: 0.000001  loss: 4.4632 (4.5415)  loss_scale: 65536.0000 (54432.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0759  max mem: 15572
Epoch: [4]  [ 130/1404]  eta: 0:14:05  lr: 0.000077  min_lr: 0.000001  loss: 4.6335 (4.5471)  loss_scale: 65536.0000 (55280.3664)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0831  max mem: 15572
Epoch: [4]  [ 140/1404]  eta: 0:13:51  lr: 0.000077  min_lr: 0.000001  loss: 4.6540 (4.5522)  loss_scale: 65536.0000 (56007.7163)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0909  max mem: 15572
Epoch: [4]  [ 150/1404]  eta: 0:13:38  lr: 0.000077  min_lr: 0.000001  loss: 4.5602 (4.5509)  loss_scale: 65536.0000 (56638.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0587  max mem: 15572
Epoch: [4]  [ 160/1404]  eta: 0:13:36  lr: 0.000077  min_lr: 0.000001  loss: 4.5152 (4.5450)  loss_scale: 65536.0000 (57191.3540)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1191  max mem: 15572
[2025-01-10 16:40:52,944] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:40:52,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:40:52,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:40:52,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 170/1404]  eta: 0:13:31  lr: 0.000077  min_lr: 0.000001  loss: 4.4598 (4.5463)  loss_scale: 65536.0000 (58445.8480)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1687  max mem: 15572
[2025-01-10 16:40:55,749] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5788
[2025-01-10 16:40:55,749] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:40:55,750] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 16:40:55,750] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5788
[2025-01-10 16:40:55,750] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [4]  [ 180/1404]  eta: 0:13:17  lr: 0.000077  min_lr: 0.000001  loss: 4.4810 (4.5438)  loss_scale: 65536.0000 (59199.6464)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0987  max mem: 15572
Epoch: [4]  [ 190/1404]  eta: 0:13:08  lr: 0.000078  min_lr: 0.000001  loss: 4.5847 (4.5465)  loss_scale: 65536.0000 (59531.3927)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0778  max mem: 15572
Epoch: [4]  [ 200/1404]  eta: 0:13:01  lr: 0.000078  min_lr: 0.000001  loss: 4.4562 (4.5418)  loss_scale: 65536.0000 (59830.1294)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1204  max mem: 15572
Epoch: [4]  [ 210/1404]  eta: 0:12:50  lr: 0.000078  min_lr: 0.000001  loss: 4.5262 (4.5490)  loss_scale: 65536.0000 (60100.5498)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0827  max mem: 15572
Epoch: [4]  [ 220/1404]  eta: 0:12:39  lr: 0.000078  min_lr: 0.000001  loss: 4.6529 (4.5506)  loss_scale: 65536.0000 (60346.4977)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0436  max mem: 15572
Epoch: [4]  [ 230/1404]  eta: 0:12:35  lr: 0.000078  min_lr: 0.000001  loss: 4.6263 (4.5542)  loss_scale: 65536.0000 (60571.1515)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1211  max mem: 15572
[2025-01-10 16:41:33,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5851
[2025-01-10 16:41:33,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:41:33,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5851
[2025-01-10 16:41:33,938] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:41:33,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 240/1404]  eta: 0:12:25  lr: 0.000078  min_lr: 0.000001  loss: 4.5493 (4.5506)  loss_scale: 65536.0000 (59961.3610)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1454  max mem: 15572
Epoch: [4]  [ 250/1404]  eta: 0:12:17  lr: 0.000078  min_lr: 0.000001  loss: 4.5818 (4.5577)  loss_scale: 32768.0000 (58877.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1084  max mem: 15572
Epoch: [4]  [ 260/1404]  eta: 0:12:10  lr: 0.000078  min_lr: 0.000001  loss: 4.6529 (4.5613)  loss_scale: 32768.0000 (57877.5785)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0971  max mem: 15572
Epoch: [4]  [ 270/1404]  eta: 0:11:59  lr: 0.000079  min_lr: 0.000001  loss: 4.6026 (4.5606)  loss_scale: 32768.0000 (56951.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0578  max mem: 15572
Epoch: [4]  [ 280/1404]  eta: 0:11:54  lr: 0.000079  min_lr: 0.000001  loss: 4.5134 (4.5532)  loss_scale: 32768.0000 (56090.4199)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1009  max mem: 15572
Epoch: [4]  [ 290/1404]  eta: 0:11:42  lr: 0.000079  min_lr: 0.000001  loss: 4.3796 (4.5461)  loss_scale: 32768.0000 (55288.9622)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0820  max mem: 15572
Epoch: [4]  [ 300/1404]  eta: 0:11:38  lr: 0.000079  min_lr: 0.000001  loss: 4.4246 (4.5461)  loss_scale: 32768.0000 (54540.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0772  max mem: 15572
Epoch: [4]  [ 310/1404]  eta: 0:11:29  lr: 0.000079  min_lr: 0.000001  loss: 4.4340 (4.5438)  loss_scale: 32768.0000 (53840.6688)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0859  max mem: 15572
Epoch: [4]  [ 320/1404]  eta: 0:11:25  lr: 0.000079  min_lr: 0.000001  loss: 4.4708 (4.5457)  loss_scale: 32768.0000 (53184.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0921  max mem: 15572
Epoch: [4]  [ 330/1404]  eta: 0:11:16  lr: 0.000079  min_lr: 0.000001  loss: 4.4977 (4.5430)  loss_scale: 32768.0000 (52567.3958)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1127  max mem: 15572
Epoch: [4]  [ 340/1404]  eta: 0:11:08  lr: 0.000080  min_lr: 0.000001  loss: 4.4977 (4.5446)  loss_scale: 32768.0000 (51986.7683)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0542  max mem: 15572
Epoch: [4]  [ 350/1404]  eta: 0:11:02  lr: 0.000080  min_lr: 0.000001  loss: 4.5318 (4.5439)  loss_scale: 32768.0000 (51439.2251)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0867  max mem: 15572
Epoch: [4]  [ 360/1404]  eta: 0:10:55  lr: 0.000080  min_lr: 0.000001  loss: 4.4680 (4.5404)  loss_scale: 32768.0000 (50922.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1001  max mem: 15572
[2025-01-10 16:42:51,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:42:51,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:42:51,255] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:42:51,256] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 370/1404]  eta: 0:10:47  lr: 0.000080  min_lr: 0.000001  loss: 4.4680 (4.5397)  loss_scale: 32768.0000 (51050.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0733  max mem: 15572
Epoch: [4]  [ 380/1404]  eta: 0:10:41  lr: 0.000080  min_lr: 0.000001  loss: 4.5693 (4.5395)  loss_scale: 65536.0000 (51431.1391)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0881  max mem: 15572
[2025-01-10 16:43:02,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=29, lr=[7.763354467760621e-07, 7.763354467760621e-07, 1.1090506382515173e-06, 1.1090506382515173e-06, 1.584358054645025e-06, 1.584358054645025e-06, 2.263368649492893e-06, 2.263368649492893e-06, 3.2333837849898472e-06, 3.2333837849898472e-06, 4.619119692842639e-06, 4.619119692842639e-06, 6.5987424183466274e-06, 6.5987424183466274e-06, 9.426774883352326e-06, 9.426774883352326e-06, 1.3466821261931895e-05, 1.3466821261931895e-05, 1.923831608847414e-05, 1.923831608847414e-05, 2.7483308697820196e-05, 2.7483308697820196e-05, 3.926186956831457e-05, 3.926186956831457e-05, 5.6088385097592245e-05, 5.6088385097592245e-05, 8.012626442513179e-05, 8.012626442513179e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 16:43:02,404] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=45.79331943516955, CurrSamplesPerSec=52.750163889067935, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 390/1404]  eta: 0:10:35  lr: 0.000080  min_lr: 0.000001  loss: 4.3954 (4.5358)  loss_scale: 65536.0000 (51791.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0533  max mem: 15572
Epoch: [4]  [ 400/1404]  eta: 0:10:27  lr: 0.000080  min_lr: 0.000001  loss: 4.4375 (4.5362)  loss_scale: 65536.0000 (52134.6234)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0182  max mem: 15572
Epoch: [4]  [ 410/1404]  eta: 0:10:19  lr: 0.000080  min_lr: 0.000001  loss: 4.5206 (4.5329)  loss_scale: 65536.0000 (52460.6910)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0431  max mem: 15572
Epoch: [4]  [ 420/1404]  eta: 0:10:14  lr: 0.000081  min_lr: 0.000001  loss: 4.4302 (4.5291)  loss_scale: 65536.0000 (52771.2684)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0891  max mem: 15572
Epoch: [4]  [ 430/1404]  eta: 0:10:08  lr: 0.000081  min_lr: 0.000001  loss: 4.4302 (4.5288)  loss_scale: 65536.0000 (53067.4339)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.0640  max mem: 15572
Epoch: [4]  [ 440/1404]  eta: 0:10:02  lr: 0.000081  min_lr: 0.000001  loss: 4.5415 (4.5266)  loss_scale: 65536.0000 (53350.1678)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.0008  max mem: 15572
Epoch: [4]  [ 450/1404]  eta: 0:09:54  lr: 0.000081  min_lr: 0.000001  loss: 4.5381 (4.5287)  loss_scale: 65536.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0006  max mem: 15572
Epoch: [4]  [ 460/1404]  eta: 0:09:49  lr: 0.000081  min_lr: 0.000001  loss: 4.5313 (4.5288)  loss_scale: 65536.0000 (53878.8373)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0006  max mem: 15572
Epoch: [4]  [ 470/1404]  eta: 0:09:46  lr: 0.000081  min_lr: 0.000001  loss: 4.5121 (4.5292)  loss_scale: 65536.0000 (54126.3355)  weight_decay: 0.0500 (0.0500)  time: 0.7303  data: 0.0007  max mem: 15572
Epoch: [4]  [ 480/1404]  eta: 0:09:38  lr: 0.000081  min_lr: 0.000001  loss: 4.5041 (4.5288)  loss_scale: 65536.0000 (54363.5426)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.0008  max mem: 15572
Epoch: [4]  [ 490/1404]  eta: 0:09:33  lr: 0.000082  min_lr: 0.000001  loss: 4.4970 (4.5268)  loss_scale: 65536.0000 (54591.0876)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0008  max mem: 15572
[2025-01-10 16:44:11,368] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:44:11,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:44:11,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:44:11,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:44:11,886] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6109
[2025-01-10 16:44:11,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:44:11,906] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6109
[2025-01-10 16:44:11,907] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:44:11,907] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 16:44:12,441] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6110
[2025-01-10 16:44:12,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:44:12,442] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:44:12,443] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6110
[2025-01-10 16:44:12,443] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [ 500/1404]  eta: 0:09:26  lr: 0.000082  min_lr: 0.000001  loss: 4.4922 (4.5247)  loss_scale: 65536.0000 (54482.5230)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.0007  max mem: 15572
Epoch: [4]  [ 510/1404]  eta: 0:09:20  lr: 0.000082  min_lr: 0.000001  loss: 4.4076 (4.5230)  loss_scale: 32768.0000 (54057.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0008  max mem: 15572
Epoch: [4]  [ 520/1404]  eta: 0:09:14  lr: 0.000082  min_lr: 0.000001  loss: 4.3819 (4.5211)  loss_scale: 32768.0000 (53648.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.0011  max mem: 15572
Epoch: [4]  [ 530/1404]  eta: 0:09:08  lr: 0.000082  min_lr: 0.000001  loss: 4.5011 (4.5218)  loss_scale: 32768.0000 (53255.7137)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.0011  max mem: 15572
Epoch: [4]  [ 540/1404]  eta: 0:09:00  lr: 0.000082  min_lr: 0.000001  loss: 4.5419 (4.5242)  loss_scale: 32768.0000 (52877.0129)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0009  max mem: 15572
Epoch: [4]  [ 550/1404]  eta: 0:08:54  lr: 0.000082  min_lr: 0.000001  loss: 4.5153 (4.5236)  loss_scale: 32768.0000 (52512.0581)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0013  max mem: 15572
Epoch: [4]  [ 560/1404]  eta: 0:08:49  lr: 0.000082  min_lr: 0.000001  loss: 4.6425 (4.5271)  loss_scale: 32768.0000 (52160.1141)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.0012  max mem: 15572
Epoch: [4]  [ 570/1404]  eta: 0:08:41  lr: 0.000083  min_lr: 0.000001  loss: 4.6425 (4.5280)  loss_scale: 32768.0000 (51820.4974)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0009  max mem: 15572
Epoch: [4]  [ 580/1404]  eta: 0:08:35  lr: 0.000083  min_lr: 0.000001  loss: 4.6178 (4.5311)  loss_scale: 32768.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0008  max mem: 15572
Epoch: [4]  [ 590/1404]  eta: 0:08:30  lr: 0.000083  min_lr: 0.000001  loss: 4.6269 (4.5313)  loss_scale: 32768.0000 (51175.7428)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.0007  max mem: 15572
Epoch: [4]  [ 600/1404]  eta: 0:08:22  lr: 0.000083  min_lr: 0.000001  loss: 4.4939 (4.5273)  loss_scale: 32768.0000 (50869.4576)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0005  max mem: 15572
Epoch: [4]  [ 610/1404]  eta: 0:08:15  lr: 0.000083  min_lr: 0.000001  loss: 4.4939 (4.5278)  loss_scale: 32768.0000 (50573.1980)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0006  max mem: 15572
Epoch: [4]  [ 620/1404]  eta: 0:08:09  lr: 0.000083  min_lr: 0.000001  loss: 4.4500 (4.5246)  loss_scale: 32768.0000 (50286.4799)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0008  max mem: 15572
[2025-01-10 16:45:31,330] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:45:31,331] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:45:31,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:45:31,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:45:33,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6243
[2025-01-10 16:45:33,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6243
[2025-01-10 16:45:33,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:45:33,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:45:33,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 630/1404]  eta: 0:08:03  lr: 0.000083  min_lr: 0.000001  loss: 4.4500 (4.5259)  loss_scale: 32768.0000 (50216.5705)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0008  max mem: 15572
Epoch: [4]  [ 640/1404]  eta: 0:07:55  lr: 0.000084  min_lr: 0.000001  loss: 4.5390 (4.5271)  loss_scale: 32768.0000 (49944.3619)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0009  max mem: 15572
Epoch: [4]  [ 650/1404]  eta: 0:07:49  lr: 0.000084  min_lr: 0.000001  loss: 4.5545 (4.5281)  loss_scale: 32768.0000 (49680.5161)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0008  max mem: 15572
Epoch: [4]  [ 660/1404]  eta: 0:07:41  lr: 0.000084  min_lr: 0.000001  loss: 4.4605 (4.5267)  loss_scale: 32768.0000 (49424.6536)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0007  max mem: 15572
Epoch: [4]  [ 670/1404]  eta: 0:07:37  lr: 0.000084  min_lr: 0.000001  loss: 4.4118 (4.5254)  loss_scale: 32768.0000 (49176.4173)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0007  max mem: 15572
Epoch: [4]  [ 680/1404]  eta: 0:07:29  lr: 0.000084  min_lr: 0.000001  loss: 4.4528 (4.5249)  loss_scale: 32768.0000 (48935.4714)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.0009  max mem: 15572
Epoch: [4]  [ 690/1404]  eta: 0:07:22  lr: 0.000084  min_lr: 0.000001  loss: 4.5765 (4.5277)  loss_scale: 32768.0000 (48701.4993)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0009  max mem: 15572
Epoch: [4]  [ 700/1404]  eta: 0:07:17  lr: 0.000084  min_lr: 0.000001  loss: 4.5765 (4.5269)  loss_scale: 32768.0000 (48474.2026)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0008  max mem: 15572
Epoch: [4]  [ 710/1404]  eta: 0:07:10  lr: 0.000084  min_lr: 0.000001  loss: 4.4606 (4.5292)  loss_scale: 32768.0000 (48253.2996)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.0008  max mem: 15572
Epoch: [4]  [ 720/1404]  eta: 0:07:04  lr: 0.000085  min_lr: 0.000001  loss: 4.4513 (4.5286)  loss_scale: 32768.0000 (48038.5243)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0008  max mem: 15572
Epoch: [4]  [ 730/1404]  eta: 0:06:58  lr: 0.000085  min_lr: 0.000001  loss: 4.5596 (4.5302)  loss_scale: 32768.0000 (47829.6252)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0013  max mem: 15572
Epoch: [4]  [ 740/1404]  eta: 0:06:51  lr: 0.000085  min_lr: 0.000001  loss: 4.5686 (4.5286)  loss_scale: 32768.0000 (47626.3644)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0014  max mem: 15572
Epoch: [4]  [ 750/1404]  eta: 0:06:45  lr: 0.000085  min_lr: 0.000001  loss: 4.5052 (4.5267)  loss_scale: 32768.0000 (47428.5166)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0011  max mem: 15572
[2025-01-10 16:46:50,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:46:50,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:46:50,772] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:46:50,772] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 760/1404]  eta: 0:06:39  lr: 0.000085  min_lr: 0.000001  loss: 4.4643 (4.5269)  loss_scale: 32768.0000 (47451.1643)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0009  max mem: 15572
Epoch: [4]  [ 770/1404]  eta: 0:06:33  lr: 0.000085  min_lr: 0.000001  loss: 4.4227 (4.5264)  loss_scale: 65536.0000 (47685.7276)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0006  max mem: 15572
Epoch: [4]  [ 780/1404]  eta: 0:06:27  lr: 0.000085  min_lr: 0.000001  loss: 4.6067 (4.5273)  loss_scale: 65536.0000 (47914.2843)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0005  max mem: 15572
Epoch: [4]  [ 790/1404]  eta: 0:06:20  lr: 0.000086  min_lr: 0.000001  loss: 4.6067 (4.5274)  loss_scale: 65536.0000 (48137.0619)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0006  max mem: 15572
Epoch: [4]  [ 800/1404]  eta: 0:06:14  lr: 0.000086  min_lr: 0.000001  loss: 4.6076 (4.5283)  loss_scale: 65536.0000 (48354.2772)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0006  max mem: 15572
Epoch: [4]  [ 810/1404]  eta: 0:06:07  lr: 0.000086  min_lr: 0.000001  loss: 4.6276 (4.5273)  loss_scale: 65536.0000 (48566.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0007  max mem: 15572
Epoch: [4]  [ 820/1404]  eta: 0:06:00  lr: 0.000086  min_lr: 0.000001  loss: 4.4580 (4.5271)  loss_scale: 65536.0000 (48772.8331)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0010  max mem: 15572
Epoch: [4]  [ 830/1404]  eta: 0:05:54  lr: 0.000086  min_lr: 0.000001  loss: 4.4682 (4.5265)  loss_scale: 65536.0000 (48974.5560)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0010  max mem: 15572
Epoch: [4]  [ 840/1404]  eta: 0:05:48  lr: 0.000086  min_lr: 0.000001  loss: 4.3899 (4.5248)  loss_scale: 65536.0000 (49171.4816)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.0470  max mem: 15572
Epoch: [4]  [ 850/1404]  eta: 0:05:42  lr: 0.000086  min_lr: 0.000001  loss: 4.4380 (4.5236)  loss_scale: 65536.0000 (49363.7791)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0657  max mem: 15572
Epoch: [4]  [ 860/1404]  eta: 0:05:36  lr: 0.000086  min_lr: 0.000001  loss: 4.3056 (4.5212)  loss_scale: 65536.0000 (49551.6098)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.0962  max mem: 15572
Epoch: [4]  [ 870/1404]  eta: 0:05:30  lr: 0.000087  min_lr: 0.000001  loss: 4.4384 (4.5219)  loss_scale: 65536.0000 (49735.1274)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0777  max mem: 15572
Epoch: [4]  [ 880/1404]  eta: 0:05:24  lr: 0.000087  min_lr: 0.000001  loss: 4.4401 (4.5211)  loss_scale: 65536.0000 (49914.4790)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0519  max mem: 15572
[2025-01-10 16:48:10,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6500
[2025-01-10 16:48:10,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:48:10,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:48:10,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6500
[2025-01-10 16:48:10,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [ 890/1404]  eta: 0:05:18  lr: 0.000087  min_lr: 0.000001  loss: 4.4178 (4.5203)  loss_scale: 65536.0000 (49832.3681)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1108  max mem: 15572
Epoch: [4]  [ 900/1404]  eta: 0:05:11  lr: 0.000087  min_lr: 0.000001  loss: 4.5597 (4.5207)  loss_scale: 32768.0000 (49642.9745)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0599  max mem: 15572
Epoch: [4]  [ 910/1404]  eta: 0:05:05  lr: 0.000087  min_lr: 0.000001  loss: 4.5377 (4.5190)  loss_scale: 32768.0000 (49457.7387)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0012  max mem: 15572
Epoch: [4]  [ 920/1404]  eta: 0:04:59  lr: 0.000087  min_lr: 0.000001  loss: 4.4300 (4.5191)  loss_scale: 32768.0000 (49276.5255)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0344  max mem: 15572
Epoch: [4]  [ 930/1404]  eta: 0:04:52  lr: 0.000087  min_lr: 0.000001  loss: 4.5181 (4.5184)  loss_scale: 32768.0000 (49099.2052)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0341  max mem: 15572
Epoch: [4]  [ 940/1404]  eta: 0:04:46  lr: 0.000088  min_lr: 0.000001  loss: 4.4590 (4.5177)  loss_scale: 32768.0000 (48925.6536)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0531  max mem: 15572
Epoch: [4]  [ 950/1404]  eta: 0:04:40  lr: 0.000088  min_lr: 0.000001  loss: 4.4713 (4.5179)  loss_scale: 32768.0000 (48755.7518)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1174  max mem: 15572
Epoch: [4]  [ 960/1404]  eta: 0:04:34  lr: 0.000088  min_lr: 0.000001  loss: 4.4713 (4.5171)  loss_scale: 32768.0000 (48589.3861)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.1068  max mem: 15572
Epoch: [4]  [ 970/1404]  eta: 0:04:28  lr: 0.000088  min_lr: 0.000001  loss: 4.4800 (4.5166)  loss_scale: 32768.0000 (48426.4470)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.0640  max mem: 15572
Epoch: [4]  [ 980/1404]  eta: 0:04:22  lr: 0.000088  min_lr: 0.000001  loss: 4.4577 (4.5155)  loss_scale: 32768.0000 (48266.8298)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0222  max mem: 15572
Epoch: [4]  [ 990/1404]  eta: 0:04:16  lr: 0.000088  min_lr: 0.000001  loss: 4.3919 (4.5144)  loss_scale: 32768.0000 (48110.4339)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0007  max mem: 15572
Epoch: [4]  [1000/1404]  eta: 0:04:10  lr: 0.000088  min_lr: 0.000001  loss: 4.4316 (4.5142)  loss_scale: 32768.0000 (47957.1628)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.0007  max mem: 15572
Epoch: [4]  [1010/1404]  eta: 0:04:03  lr: 0.000089  min_lr: 0.000001  loss: 4.4310 (4.5144)  loss_scale: 32768.0000 (47806.9238)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0007  max mem: 15572
[2025-01-10 16:49:29,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:49:29,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:49:29,854] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:49:29,855] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1020/1404]  eta: 0:03:57  lr: 0.000089  min_lr: 0.000001  loss: 4.4310 (4.5129)  loss_scale: 32768.0000 (47916.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0005  max mem: 15572
Epoch: [4]  [1030/1404]  eta: 0:03:51  lr: 0.000089  min_lr: 0.000001  loss: 4.4968 (4.5132)  loss_scale: 65536.0000 (48087.2784)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0005  max mem: 15572
Epoch: [4]  [1040/1404]  eta: 0:03:44  lr: 0.000089  min_lr: 0.000001  loss: 4.5258 (4.5138)  loss_scale: 65536.0000 (48254.8934)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0007  max mem: 15572
Epoch: [4]  [1050/1404]  eta: 0:03:38  lr: 0.000089  min_lr: 0.000001  loss: 4.4153 (4.5123)  loss_scale: 65536.0000 (48419.3187)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0010  max mem: 15572
Epoch: [4]  [1060/1404]  eta: 0:03:32  lr: 0.000089  min_lr: 0.000001  loss: 4.3891 (4.5105)  loss_scale: 65536.0000 (48580.6447)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0011  max mem: 15572
Epoch: [4]  [1070/1404]  eta: 0:03:25  lr: 0.000089  min_lr: 0.000001  loss: 4.4189 (4.5095)  loss_scale: 65536.0000 (48738.9580)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0010  max mem: 15572
Epoch: [4]  [1080/1404]  eta: 0:03:19  lr: 0.000089  min_lr: 0.000001  loss: 4.4279 (4.5090)  loss_scale: 65536.0000 (48894.3423)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0009  max mem: 15572
Epoch: [4]  [1090/1404]  eta: 0:03:13  lr: 0.000090  min_lr: 0.000001  loss: 4.5634 (4.5100)  loss_scale: 65536.0000 (49046.8781)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0009  max mem: 15572
Epoch: [4]  [1100/1404]  eta: 0:03:07  lr: 0.000090  min_lr: 0.000001  loss: 4.5634 (4.5103)  loss_scale: 65536.0000 (49196.6431)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0009  max mem: 15572
[2025-01-10 16:50:25,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6723
[2025-01-10 16:50:25,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:50:25,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:50:25,520] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6723
[2025-01-10 16:50:25,521] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [1110/1404]  eta: 0:03:01  lr: 0.000090  min_lr: 0.000001  loss: 4.5000 (4.5111)  loss_scale: 65536.0000 (49225.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0008  max mem: 15572
Epoch: [4]  [1120/1404]  eta: 0:02:54  lr: 0.000090  min_lr: 0.000001  loss: 4.4893 (4.5108)  loss_scale: 32768.0000 (49078.9224)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0010  max mem: 15572
Epoch: [4]  [1130/1404]  eta: 0:02:48  lr: 0.000090  min_lr: 0.000001  loss: 4.4660 (4.5101)  loss_scale: 32768.0000 (48934.7056)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0009  max mem: 15572
Epoch: [4]  [1140/1404]  eta: 0:02:42  lr: 0.000090  min_lr: 0.000001  loss: 4.4095 (4.5101)  loss_scale: 32768.0000 (48793.0167)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.0006  max mem: 15572
Epoch: [4]  [1150/1404]  eta: 0:02:36  lr: 0.000090  min_lr: 0.000001  loss: 4.3903 (4.5094)  loss_scale: 32768.0000 (48653.7897)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0009  max mem: 15572
Epoch: [4]  [1160/1404]  eta: 0:02:30  lr: 0.000091  min_lr: 0.000001  loss: 4.3068 (4.5067)  loss_scale: 32768.0000 (48516.9612)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0010  max mem: 15572
Epoch: [4]  [1170/1404]  eta: 0:02:24  lr: 0.000091  min_lr: 0.000001  loss: 4.3068 (4.5058)  loss_scale: 32768.0000 (48382.4697)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0009  max mem: 15572
Epoch: [4]  [1180/1404]  eta: 0:02:17  lr: 0.000091  min_lr: 0.000001  loss: 4.5803 (4.5074)  loss_scale: 32768.0000 (48250.2557)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0010  max mem: 15572
Epoch: [4]  [1190/1404]  eta: 0:02:11  lr: 0.000091  min_lr: 0.000001  loss: 4.5682 (4.5064)  loss_scale: 32768.0000 (48120.2620)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0012  max mem: 15572
Epoch: [4]  [1200/1404]  eta: 0:02:05  lr: 0.000091  min_lr: 0.000001  loss: 4.4337 (4.5062)  loss_scale: 32768.0000 (47992.4330)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.0011  max mem: 15572
Epoch: [4]  [1210/1404]  eta: 0:01:59  lr: 0.000091  min_lr: 0.000001  loss: 4.5063 (4.5058)  loss_scale: 32768.0000 (47866.7151)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.0008  max mem: 15572
Epoch: [4]  [1220/1404]  eta: 0:01:53  lr: 0.000091  min_lr: 0.000001  loss: 4.4787 (4.5054)  loss_scale: 32768.0000 (47743.0565)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0010  max mem: 15572
Epoch: [4]  [1230/1404]  eta: 0:01:47  lr: 0.000091  min_lr: 0.000001  loss: 4.4315 (4.5042)  loss_scale: 32768.0000 (47621.4070)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0011  max mem: 15572
[2025-01-10 16:51:44,885] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:51:44,885] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:51:44,922] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:51:44,923] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1240/1404]  eta: 0:01:41  lr: 0.000092  min_lr: 0.000001  loss: 4.3964 (4.5020)  loss_scale: 32768.0000 (47633.7405)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0008  max mem: 15572
Epoch: [4]  [1250/1404]  eta: 0:01:34  lr: 0.000092  min_lr: 0.000001  loss: 4.5146 (4.5021)  loss_scale: 65536.0000 (47776.8441)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0041  max mem: 15572
Epoch: [4]  [1260/1404]  eta: 0:01:28  lr: 0.000092  min_lr: 0.000001  loss: 4.5146 (4.5018)  loss_scale: 65536.0000 (47917.6780)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0040  max mem: 15572
Epoch: [4]  [1270/1404]  eta: 0:01:22  lr: 0.000092  min_lr: 0.000001  loss: 4.4623 (4.5020)  loss_scale: 65536.0000 (48056.2958)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0008  max mem: 15572
Epoch: [4]  [1280/1404]  eta: 0:01:16  lr: 0.000092  min_lr: 0.000001  loss: 4.4945 (4.5017)  loss_scale: 65536.0000 (48192.7494)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0008  max mem: 15572
Epoch: [4]  [1290/1404]  eta: 0:01:10  lr: 0.000092  min_lr: 0.000001  loss: 4.3551 (4.5001)  loss_scale: 65536.0000 (48327.0891)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0008  max mem: 15572
Epoch: [4]  [1300/1404]  eta: 0:01:03  lr: 0.000092  min_lr: 0.000001  loss: 4.2540 (4.4994)  loss_scale: 65536.0000 (48459.3636)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0009  max mem: 15572
Epoch: [4]  [1310/1404]  eta: 0:00:57  lr: 0.000093  min_lr: 0.000001  loss: 4.4424 (4.4987)  loss_scale: 65536.0000 (48589.6201)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0008  max mem: 15572
Epoch: [4]  [1320/1404]  eta: 0:00:51  lr: 0.000093  min_lr: 0.000001  loss: 4.3707 (4.4977)  loss_scale: 65536.0000 (48717.9046)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0009  max mem: 15572
Epoch: [4]  [1330/1404]  eta: 0:00:45  lr: 0.000093  min_lr: 0.000001  loss: 4.3707 (4.4965)  loss_scale: 65536.0000 (48844.2615)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.0008  max mem: 15572
Epoch: [4]  [1340/1404]  eta: 0:00:39  lr: 0.000093  min_lr: 0.000001  loss: 4.4347 (4.4963)  loss_scale: 65536.0000 (48968.7338)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0009  max mem: 15572
Epoch: [4]  [1350/1404]  eta: 0:00:33  lr: 0.000093  min_lr: 0.000001  loss: 4.4118 (4.4951)  loss_scale: 65536.0000 (49091.3634)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.0010  max mem: 15572
Epoch: [4]  [1360/1404]  eta: 0:00:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4077 (4.4943)  loss_scale: 65536.0000 (49212.1910)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.0011  max mem: 15572
[2025-01-10 16:53:03,369] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:53:03,370] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:53:03,377] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:53:03,378] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:53:05,128] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6983
[2025-01-10 16:53:05,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:53:05,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6983
[2025-01-10 16:53:05,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:53:05,131] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1370/1404]  eta: 0:00:20  lr: 0.000093  min_lr: 0.000001  loss: 4.3752 (4.4927)  loss_scale: 65536.0000 (49474.6608)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0009  max mem: 15572
Epoch: [4]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.3965 (4.4929)  loss_scale: 65536.0000 (49590.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0008  max mem: 15572
[2025-01-10 16:53:13,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=35, lr=[9.057462563736721e-07, 9.057462563736721e-07, 1.2939232233909602e-06, 1.2939232233909602e-06, 1.848461747701372e-06, 1.848461747701372e-06, 2.6406596395733886e-06, 2.6406596395733886e-06, 3.7723709136762698e-06, 3.7723709136762698e-06, 5.389101305251814e-06, 5.389101305251814e-06, 7.698716150359736e-06, 7.698716150359736e-06, 1.0998165929085337e-05, 1.0998165929085337e-05, 1.5711665612979053e-05, 1.5711665612979053e-05, 2.2445236589970078e-05, 2.2445236589970078e-05, 3.206462369995725e-05, 3.206462369995725e-05, 4.5806605285653225e-05, 4.5806605285653225e-05, 6.543800755093318e-05, 6.543800755093318e-05, 9.348286792990455e-05, 9.348286792990455e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 16:53:13,873] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=45.22470478503705, CurrSamplesPerSec=38.99611949792494, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.2926 (4.4905)  loss_scale: 65536.0000 (49705.5931)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0011  max mem: 15572
Epoch: [4]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2450 (4.4903)  loss_scale: 65536.0000 (49818.5867)  weight_decay: 0.0500 (0.0500)  time: 0.4891  data: 0.0007  max mem: 15572
Epoch: [4]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.2655 (4.4900)  loss_scale: 65536.0000 (49852.1709)  weight_decay: 0.0500 (0.0500)  time: 0.4551  data: 0.0006  max mem: 15572
Epoch: [4] Total time: 0:14:21 (0.6135 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.2655 (4.4898)  loss_scale: 65536.0000 (49852.1709)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:40  loss: 2.1358 (2.1358)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.7059  data: 4.4308  max mem: 15572
Val:  [ 10/136]  eta: 0:01:36  loss: 3.8812 (3.7410)  acc1: 5.5556 (16.6667)  acc5: 27.7778 (32.3232)  time: 0.7638  data: 0.5742  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 3.8812 (3.8156)  acc1: 0.0000 (13.7566)  acc5: 27.7778 (34.6561)  time: 0.3402  data: 0.1531  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 3.4328 (3.5628)  acc1: 11.1111 (23.1183)  acc5: 50.0000 (43.3692)  time: 0.3741  data: 0.1542  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 2.7221 (3.5201)  acc1: 33.3333 (23.0352)  acc5: 66.6667 (44.9865)  time: 0.4400  data: 0.2183  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 4.0731 (3.6761)  acc1: 0.0000 (19.3900)  acc5: 16.6667 (39.4336)  time: 0.3797  data: 0.1824  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 4.2599 (3.7816)  acc1: 0.0000 (16.6667)  acc5: 11.1111 (36.4299)  time: 0.3287  data: 0.1313  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 4.1510 (3.6950)  acc1: 0.0000 (19.3271)  acc5: 22.2222 (38.5759)  time: 0.3547  data: 0.1556  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 3.6251 (3.6862)  acc1: 11.1111 (19.3416)  acc5: 50.0000 (39.9177)  time: 0.3549  data: 0.1560  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 3.6641 (3.6986)  acc1: 5.5556 (19.1697)  acc5: 44.4444 (39.9267)  time: 0.3513  data: 0.1452  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.7964 (3.7369)  acc1: 0.0000 (18.2618)  acc5: 44.4444 (39.6590)  time: 0.3536  data: 0.1313  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.7738 (3.7384)  acc1: 0.0000 (19.1692)  acc5: 33.3333 (40.4905)  time: 0.3658  data: 0.1460  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.4463 (3.6885)  acc1: 27.7778 (20.3857)  acc5: 61.1111 (43.6180)  time: 0.3616  data: 0.1372  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.0595 (3.6493)  acc1: 33.3333 (22.1798)  acc5: 72.2222 (44.6141)  time: 0.2705  data: 0.0704  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.4463 (3.6587)  acc1: 22.2222 (21.9492)  acc5: 61.1111 (44.8403)  time: 0.1990  data: 0.0210  max mem: 15572
Val: Total time: 0:00:51 (0.3762 s / it)
* Acc@1 21.478 Acc@5 44.451 loss 3.678
Accuracy of the network on the 4883 val videos: 21.5%
[2025-01-10 16:54:15,149] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 16:54:15,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 16:54:15,152] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 16:54:15,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 16:54:17,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 16:54:17,488] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.48%
Epoch: [5]  [   0/1404]  eta: 3:19:27  lr: 0.000094  min_lr: 0.000001  loss: 4.5968 (4.5968)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5239  data: 7.9354  max mem: 15572
[2025-01-10 16:54:30,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7028
[2025-01-10 16:54:30,062] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:54:30,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 16:54:30,062] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7028
[2025-01-10 16:54:30,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [  10/1404]  eta: 0:32:42  lr: 0.000094  min_lr: 0.000001  loss: 4.4513 (4.4184)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 1.4075  data: 0.9007  max mem: 15572
Epoch: [5]  [  20/1404]  eta: 0:23:17  lr: 0.000094  min_lr: 0.000001  loss: 4.4004 (4.3723)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1133  max mem: 15572
Epoch: [5]  [  30/1404]  eta: 0:19:47  lr: 0.000094  min_lr: 0.000001  loss: 4.4177 (4.3396)  loss_scale: 32768.0000 (41224.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0151  max mem: 15572
Epoch: [5]  [  40/1404]  eta: 0:18:07  lr: 0.000094  min_lr: 0.000001  loss: 4.4652 (4.3793)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0010  max mem: 15572
Epoch: [5]  [  50/1404]  eta: 0:17:19  lr: 0.000094  min_lr: 0.000001  loss: 4.4705 (4.3944)  loss_scale: 32768.0000 (37908.0784)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0010  max mem: 15572
Epoch: [5]  [  60/1404]  eta: 0:16:36  lr: 0.000094  min_lr: 0.000001  loss: 4.4128 (4.3812)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0008  max mem: 15572
Epoch: [5]  [  70/1404]  eta: 0:16:05  lr: 0.000094  min_lr: 0.000001  loss: 4.4120 (4.3981)  loss_scale: 32768.0000 (36460.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0008  max mem: 15572
Epoch: [5]  [  80/1404]  eta: 0:15:44  lr: 0.000094  min_lr: 0.000001  loss: 4.4682 (4.4066)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0350  max mem: 15572
Epoch: [5]  [  90/1404]  eta: 0:15:26  lr: 0.000094  min_lr: 0.000001  loss: 4.3652 (4.4012)  loss_scale: 32768.0000 (35648.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0348  max mem: 15572
Epoch: [5]  [ 100/1404]  eta: 0:14:58  lr: 0.000094  min_lr: 0.000001  loss: 4.3736 (4.4152)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0028  max mem: 15572
Epoch: [5]  [ 110/1404]  eta: 0:14:42  lr: 0.000094  min_lr: 0.000001  loss: 4.4461 (4.4109)  loss_scale: 32768.0000 (35129.6577)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0027  max mem: 15572
Epoch: [5]  [ 120/1404]  eta: 0:14:29  lr: 0.000094  min_lr: 0.000001  loss: 4.4799 (4.4200)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0007  max mem: 15572
Epoch: [5]  [ 130/1404]  eta: 0:14:19  lr: 0.000094  min_lr: 0.000001  loss: 4.5480 (4.4190)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0007  max mem: 15572
[2025-01-10 16:55:49,339] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:55:49,340] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:55:49,373] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:55:49,374] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 140/1404]  eta: 0:13:58  lr: 0.000094  min_lr: 0.000001  loss: 4.4665 (4.4311)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0012  max mem: 15572
Epoch: [5]  [ 150/1404]  eta: 0:13:41  lr: 0.000094  min_lr: 0.000001  loss: 4.5859 (4.4432)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0013  max mem: 15572
Epoch: [5]  [ 160/1404]  eta: 0:13:35  lr: 0.000094  min_lr: 0.000001  loss: 4.4420 (4.4230)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0012  max mem: 15572
Epoch: [5]  [ 170/1404]  eta: 0:13:27  lr: 0.000094  min_lr: 0.000001  loss: 4.2970 (4.4209)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0010  max mem: 15572
Epoch: [5]  [ 180/1404]  eta: 0:13:20  lr: 0.000094  min_lr: 0.000001  loss: 4.4464 (4.4238)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0051  max mem: 15572
Epoch: [5]  [ 190/1404]  eta: 0:13:09  lr: 0.000094  min_lr: 0.000001  loss: 4.4082 (4.4164)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0496  max mem: 15572
Epoch: [5]  [ 200/1404]  eta: 0:13:02  lr: 0.000094  min_lr: 0.000001  loss: 4.3654 (4.4105)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0454  max mem: 15572
Epoch: [5]  [ 210/1404]  eta: 0:12:53  lr: 0.000094  min_lr: 0.000001  loss: 4.3654 (4.4122)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0011  max mem: 15572
Epoch: [5]  [ 220/1404]  eta: 0:12:45  lr: 0.000094  min_lr: 0.000001  loss: 4.3935 (4.4104)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0012  max mem: 15572
Epoch: [5]  [ 230/1404]  eta: 0:12:34  lr: 0.000094  min_lr: 0.000001  loss: 4.4078 (4.4138)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0011  max mem: 15572
Epoch: [5]  [ 240/1404]  eta: 0:12:27  lr: 0.000094  min_lr: 0.000001  loss: 4.3464 (4.4112)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0008  max mem: 15572
Epoch: [5]  [ 250/1404]  eta: 0:12:17  lr: 0.000094  min_lr: 0.000001  loss: 4.2229 (4.4099)  loss_scale: 65536.0000 (48695.0757)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0008  max mem: 15572
Epoch: [5]  [ 260/1404]  eta: 0:12:13  lr: 0.000094  min_lr: 0.000001  loss: 4.2182 (4.4062)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0009  max mem: 15572
[2025-01-10 16:57:08,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:57:08,400] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:57:08,444] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:57:08,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 16:57:08,955] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7286
[2025-01-10 16:57:08,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 16:57:08,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 16:57:08,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7286
[2025-01-10 16:57:08,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [5]  [ 270/1404]  eta: 0:12:06  lr: 0.000094  min_lr: 0.000001  loss: 4.3180 (4.4030)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.0008  max mem: 15572
Epoch: [5]  [ 280/1404]  eta: 0:11:57  lr: 0.000094  min_lr: 0.000001  loss: 4.4117 (4.4060)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0009  max mem: 15572
Epoch: [5]  [ 290/1404]  eta: 0:11:51  lr: 0.000094  min_lr: 0.000001  loss: 4.4083 (4.4044)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0011  max mem: 15572
Epoch: [5]  [ 300/1404]  eta: 0:11:43  lr: 0.000094  min_lr: 0.000001  loss: 4.2692 (4.4004)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.0011  max mem: 15572
Epoch: [5]  [ 310/1404]  eta: 0:11:35  lr: 0.000094  min_lr: 0.000001  loss: 4.2692 (4.3990)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0059  max mem: 15572
[2025-01-10 16:57:41,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7339
[2025-01-10 16:57:41,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:57:41,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7339
[2025-01-10 16:57:41,852] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 16:57:41,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 320/1404]  eta: 0:11:31  lr: 0.000094  min_lr: 0.000001  loss: 4.3190 (4.3955)  loss_scale: 65536.0000 (52367.5514)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0173  max mem: 15572
Epoch: [5]  [ 330/1404]  eta: 0:11:22  lr: 0.000094  min_lr: 0.000001  loss: 4.3683 (4.3985)  loss_scale: 32768.0000 (51775.4199)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0122  max mem: 15572
Epoch: [5]  [ 340/1404]  eta: 0:11:12  lr: 0.000094  min_lr: 0.000001  loss: 4.4205 (4.3962)  loss_scale: 32768.0000 (51218.0176)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0008  max mem: 15572
Epoch: [5]  [ 350/1404]  eta: 0:11:04  lr: 0.000094  min_lr: 0.000001  loss: 4.4205 (4.3978)  loss_scale: 32768.0000 (50692.3761)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0136  max mem: 15572
Epoch: [5]  [ 360/1404]  eta: 0:10:55  lr: 0.000094  min_lr: 0.000001  loss: 4.2539 (4.3925)  loss_scale: 32768.0000 (50195.8560)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0136  max mem: 15572
Epoch: [5]  [ 370/1404]  eta: 0:10:50  lr: 0.000094  min_lr: 0.000001  loss: 4.3657 (4.3946)  loss_scale: 32768.0000 (49726.1024)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0650  max mem: 15572
Epoch: [5]  [ 380/1404]  eta: 0:10:43  lr: 0.000094  min_lr: 0.000001  loss: 4.4436 (4.3959)  loss_scale: 32768.0000 (49281.0079)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1164  max mem: 15572
Epoch: [5]  [ 390/1404]  eta: 0:10:35  lr: 0.000094  min_lr: 0.000001  loss: 4.3687 (4.3943)  loss_scale: 32768.0000 (48858.6803)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0789  max mem: 15572
Epoch: [5]  [ 400/1404]  eta: 0:10:30  lr: 0.000094  min_lr: 0.000001  loss: 4.4119 (4.3954)  loss_scale: 32768.0000 (48457.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0795  max mem: 15572
Epoch: [5]  [ 410/1404]  eta: 0:10:22  lr: 0.000094  min_lr: 0.000001  loss: 4.3726 (4.3948)  loss_scale: 32768.0000 (48075.6788)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0527  max mem: 15572
Epoch: [5]  [ 420/1404]  eta: 0:10:15  lr: 0.000094  min_lr: 0.000001  loss: 4.3618 (4.3938)  loss_scale: 32768.0000 (47712.0760)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0045  max mem: 15572
Epoch: [5]  [ 430/1404]  eta: 0:10:09  lr: 0.000094  min_lr: 0.000001  loss: 4.2775 (4.3924)  loss_scale: 32768.0000 (47365.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0857  max mem: 15572
Epoch: [5]  [ 440/1404]  eta: 0:10:03  lr: 0.000094  min_lr: 0.000001  loss: 4.3552 (4.3910)  loss_scale: 32768.0000 (47034.3401)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1441  max mem: 15572
[2025-01-10 16:58:59,485] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:58:59,485] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 16:58:59,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 16:58:59,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 450/1404]  eta: 0:09:58  lr: 0.000094  min_lr: 0.000001  loss: 4.4692 (4.3922)  loss_scale: 32768.0000 (46935.9823)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1456  max mem: 15572
Epoch: [5]  [ 460/1404]  eta: 0:09:50  lr: 0.000094  min_lr: 0.000001  loss: 4.4669 (4.3951)  loss_scale: 65536.0000 (47339.4534)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1258  max mem: 15572
Epoch: [5]  [ 470/1404]  eta: 0:09:44  lr: 0.000094  min_lr: 0.000001  loss: 4.4692 (4.3965)  loss_scale: 65536.0000 (47725.7919)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0908  max mem: 15572
Epoch: [5]  [ 480/1404]  eta: 0:09:39  lr: 0.000094  min_lr: 0.000001  loss: 4.4242 (4.3934)  loss_scale: 65536.0000 (48096.0665)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1187  max mem: 15572
Epoch: [5]  [ 490/1404]  eta: 0:09:31  lr: 0.000094  min_lr: 0.000001  loss: 4.1831 (4.3905)  loss_scale: 65536.0000 (48451.2587)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0710  max mem: 15572
Epoch: [5]  [ 500/1404]  eta: 0:09:23  lr: 0.000094  min_lr: 0.000001  loss: 4.3717 (4.3901)  loss_scale: 65536.0000 (48792.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0097  max mem: 15572
Epoch: [5]  [ 510/1404]  eta: 0:09:17  lr: 0.000094  min_lr: 0.000001  loss: 4.3857 (4.3929)  loss_scale: 65536.0000 (49119.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0131  max mem: 15572
Epoch: [5]  [ 520/1404]  eta: 0:09:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3658 (4.3912)  loss_scale: 65536.0000 (49435.0250)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0042  max mem: 15572
Epoch: [5]  [ 530/1404]  eta: 0:09:04  lr: 0.000094  min_lr: 0.000001  loss: 4.4425 (4.3922)  loss_scale: 65536.0000 (49738.2448)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0007  max mem: 15572
Epoch: [5]  [ 540/1404]  eta: 0:08:57  lr: 0.000094  min_lr: 0.000001  loss: 4.4763 (4.3897)  loss_scale: 65536.0000 (50030.2551)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0007  max mem: 15572
Epoch: [5]  [ 550/1404]  eta: 0:08:50  lr: 0.000094  min_lr: 0.000001  loss: 4.3318 (4.3888)  loss_scale: 65536.0000 (50311.6661)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0182  max mem: 15572
Epoch: [5]  [ 560/1404]  eta: 0:08:44  lr: 0.000094  min_lr: 0.000001  loss: 4.4301 (4.3892)  loss_scale: 65536.0000 (50583.0446)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0305  max mem: 15572
Epoch: [5]  [ 570/1404]  eta: 0:08:38  lr: 0.000094  min_lr: 0.000001  loss: 4.4882 (4.3887)  loss_scale: 65536.0000 (50844.9177)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.0288  max mem: 15572
[2025-01-10 17:00:16,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:00:16,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:00:16,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:00:16,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:00:16,652] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7597
[2025-01-10 17:00:16,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:00:16,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7597
[2025-01-10 17:00:16,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:00:16,755] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 580/1404]  eta: 0:08:32  lr: 0.000094  min_lr: 0.000001  loss: 4.3277 (4.3855)  loss_scale: 65536.0000 (51210.5749)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.0169  max mem: 15572
[2025-01-10 17:00:22,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7605
[2025-01-10 17:00:22,300] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:00:22,339] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7605
[2025-01-10 17:00:22,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:00:22,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 590/1404]  eta: 0:08:26  lr: 0.000094  min_lr: 0.000001  loss: 4.2946 (4.3848)  loss_scale: 65536.0000 (51120.2978)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0014  max mem: 15572
Epoch: [5]  [ 600/1404]  eta: 0:08:20  lr: 0.000094  min_lr: 0.000001  loss: 4.3536 (4.3842)  loss_scale: 32768.0000 (50814.9351)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0014  max mem: 15572
Epoch: [5]  [ 610/1404]  eta: 0:08:13  lr: 0.000094  min_lr: 0.000001  loss: 4.2418 (4.3812)  loss_scale: 32768.0000 (50519.5679)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0011  max mem: 15572
Epoch: [5]  [ 620/1404]  eta: 0:08:07  lr: 0.000094  min_lr: 0.000001  loss: 4.1599 (4.3789)  loss_scale: 32768.0000 (50233.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0006  max mem: 15572
Epoch: [5]  [ 630/1404]  eta: 0:08:01  lr: 0.000094  min_lr: 0.000001  loss: 4.3212 (4.3801)  loss_scale: 32768.0000 (49956.9192)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0009  max mem: 15572
Epoch: [5]  [ 640/1404]  eta: 0:07:54  lr: 0.000094  min_lr: 0.000001  loss: 4.3713 (4.3793)  loss_scale: 32768.0000 (49688.7613)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0009  max mem: 15572
Epoch: [5]  [ 650/1404]  eta: 0:07:47  lr: 0.000094  min_lr: 0.000001  loss: 4.4069 (4.3819)  loss_scale: 32768.0000 (49428.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0008  max mem: 15572
Epoch: [5]  [ 660/1404]  eta: 0:07:41  lr: 0.000094  min_lr: 0.000001  loss: 4.2841 (4.3795)  loss_scale: 32768.0000 (49176.7867)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0010  max mem: 15572
Epoch: [5]  [ 670/1404]  eta: 0:07:35  lr: 0.000094  min_lr: 0.000001  loss: 4.2841 (4.3798)  loss_scale: 32768.0000 (48932.2444)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0009  max mem: 15572
Epoch: [5]  [ 680/1404]  eta: 0:07:29  lr: 0.000094  min_lr: 0.000001  loss: 4.4486 (4.3793)  loss_scale: 32768.0000 (48694.8840)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.0009  max mem: 15572
Epoch: [5]  [ 690/1404]  eta: 0:07:23  lr: 0.000094  min_lr: 0.000001  loss: 4.4486 (4.3799)  loss_scale: 32768.0000 (48464.3936)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0010  max mem: 15572
Epoch: [5]  [ 700/1404]  eta: 0:07:16  lr: 0.000094  min_lr: 0.000001  loss: 4.2931 (4.3740)  loss_scale: 32768.0000 (48240.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0010  max mem: 15572
Epoch: [5]  [ 710/1404]  eta: 0:07:10  lr: 0.000094  min_lr: 0.000001  loss: 4.1807 (4.3758)  loss_scale: 32768.0000 (48022.8636)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0011  max mem: 15572
[2025-01-10 17:01:41,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:01:41,958] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:01:41,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:01:41,958] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 720/1404]  eta: 0:07:04  lr: 0.000094  min_lr: 0.000001  loss: 4.3962 (4.3734)  loss_scale: 32768.0000 (48129.4202)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0007  max mem: 15572
Epoch: [5]  [ 730/1404]  eta: 0:06:58  lr: 0.000094  min_lr: 0.000001  loss: 4.2546 (4.3729)  loss_scale: 65536.0000 (48367.5404)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0006  max mem: 15572
Epoch: [5]  [ 740/1404]  eta: 0:06:52  lr: 0.000094  min_lr: 0.000001  loss: 4.3649 (4.3743)  loss_scale: 65536.0000 (48599.2335)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0331  max mem: 15572
Epoch: [5]  [ 750/1404]  eta: 0:06:45  lr: 0.000094  min_lr: 0.000001  loss: 4.4739 (4.3748)  loss_scale: 65536.0000 (48824.7563)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0330  max mem: 15572
Epoch: [5]  [ 760/1404]  eta: 0:06:39  lr: 0.000094  min_lr: 0.000001  loss: 4.4739 (4.3744)  loss_scale: 65536.0000 (49044.3522)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0088  max mem: 15572
Epoch: [5]  [ 770/1404]  eta: 0:06:32  lr: 0.000094  min_lr: 0.000001  loss: 4.4023 (4.3741)  loss_scale: 65536.0000 (49258.2516)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0088  max mem: 15572
Epoch: [5]  [ 780/1404]  eta: 0:06:26  lr: 0.000094  min_lr: 0.000001  loss: 4.2520 (4.3718)  loss_scale: 65536.0000 (49466.6735)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0005  max mem: 15572
Epoch: [5]  [ 790/1404]  eta: 0:06:20  lr: 0.000094  min_lr: 0.000001  loss: 4.2194 (4.3706)  loss_scale: 65536.0000 (49669.8255)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0005  max mem: 15572
Epoch: [5]  [ 800/1404]  eta: 0:06:13  lr: 0.000094  min_lr: 0.000001  loss: 4.3636 (4.3697)  loss_scale: 65536.0000 (49867.9051)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0005  max mem: 15572
Epoch: [5]  [ 810/1404]  eta: 0:06:08  lr: 0.000094  min_lr: 0.000001  loss: 4.4044 (4.3689)  loss_scale: 65536.0000 (50061.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0005  max mem: 15572
Epoch: [5]  [ 820/1404]  eta: 0:06:01  lr: 0.000094  min_lr: 0.000001  loss: 4.2537 (4.3682)  loss_scale: 65536.0000 (50249.5883)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0007  max mem: 15572
Epoch: [5]  [ 830/1404]  eta: 0:05:55  lr: 0.000094  min_lr: 0.000001  loss: 4.2784 (4.3681)  loss_scale: 65536.0000 (50433.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0008  max mem: 15572
Epoch: [5]  [ 840/1404]  eta: 0:05:49  lr: 0.000094  min_lr: 0.000001  loss: 4.2784 (4.3666)  loss_scale: 65536.0000 (50613.1177)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0009  max mem: 15572
[2025-01-10 17:02:59,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:02:59,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:02:59,634] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:02:59,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:03:00,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7863
[2025-01-10 17:03:00,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:03:00,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7863
[2025-01-10 17:03:00,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:03:00,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 850/1404]  eta: 0:05:42  lr: 0.000094  min_lr: 0.000001  loss: 4.3633 (4.3672)  loss_scale: 65536.0000 (50865.4853)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0009  max mem: 15572
Epoch: [5]  [ 860/1404]  eta: 0:05:36  lr: 0.000094  min_lr: 0.000001  loss: 4.2197 (4.3659)  loss_scale: 65536.0000 (51035.8746)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0011  max mem: 15572
Epoch: [5]  [ 870/1404]  eta: 0:05:30  lr: 0.000094  min_lr: 0.000001  loss: 4.2736 (4.3664)  loss_scale: 65536.0000 (51202.3513)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0014  max mem: 15572
Epoch: [5]  [ 880/1404]  eta: 0:05:24  lr: 0.000094  min_lr: 0.000001  loss: 4.4296 (4.3669)  loss_scale: 65536.0000 (51365.0488)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.0013  max mem: 15572
Epoch: [5]  [ 890/1404]  eta: 0:05:17  lr: 0.000094  min_lr: 0.000001  loss: 4.4296 (4.3685)  loss_scale: 65536.0000 (51524.0943)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0011  max mem: 15572
[2025-01-10 17:03:33,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7919
[2025-01-10 17:03:33,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:03:33,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:03:33,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7919
[2025-01-10 17:03:33,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [ 900/1404]  eta: 0:05:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3976 (4.3681)  loss_scale: 65536.0000 (51606.8724)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0009  max mem: 15572
Epoch: [5]  [ 910/1404]  eta: 0:05:04  lr: 0.000094  min_lr: 0.000001  loss: 4.3357 (4.3672)  loss_scale: 32768.0000 (51400.0790)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0008  max mem: 15572
Epoch: [5]  [ 920/1404]  eta: 0:04:58  lr: 0.000094  min_lr: 0.000001  loss: 4.3742 (4.3683)  loss_scale: 32768.0000 (51197.7763)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0011  max mem: 15572
Epoch: [5]  [ 930/1404]  eta: 0:04:52  lr: 0.000094  min_lr: 0.000001  loss: 4.3521 (4.3681)  loss_scale: 32768.0000 (50999.8195)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0011  max mem: 15572
Epoch: [5]  [ 940/1404]  eta: 0:04:46  lr: 0.000094  min_lr: 0.000001  loss: 4.2990 (4.3670)  loss_scale: 32768.0000 (50806.0701)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0007  max mem: 15572
Epoch: [5]  [ 950/1404]  eta: 0:04:39  lr: 0.000094  min_lr: 0.000001  loss: 4.2972 (4.3666)  loss_scale: 32768.0000 (50616.3954)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0006  max mem: 15572
Epoch: [5]  [ 960/1404]  eta: 0:04:33  lr: 0.000094  min_lr: 0.000001  loss: 4.4505 (4.3669)  loss_scale: 32768.0000 (50430.6681)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0133  max mem: 15572
Epoch: [5]  [ 970/1404]  eta: 0:04:26  lr: 0.000094  min_lr: 0.000001  loss: 4.5074 (4.3677)  loss_scale: 32768.0000 (50248.7662)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0674  max mem: 15572
[2025-01-10 17:04:21,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=42, lr=[9.074460811791398e-07, 9.074460811791398e-07, 1.2963515445416284e-06, 1.2963515445416284e-06, 1.8519307779166122e-06, 1.8519307779166122e-06, 2.6456153970237317e-06, 2.6456153970237317e-06, 3.77945056717676e-06, 3.77945056717676e-06, 5.3992150959668e-06, 5.3992150959668e-06, 7.713164422809715e-06, 7.713164422809715e-06, 1.1018806318299594e-05, 1.1018806318299594e-05, 1.5741151883285134e-05, 1.5741151883285134e-05, 2.248735983326448e-05, 2.248735983326448e-05, 3.21247997618064e-05, 3.21247997618064e-05, 4.589257108829486e-05, 4.589257108829486e-05, 6.556081584042124e-05, 6.556081584042124e-05, 9.365830834345891e-05, 9.365830834345891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:04:21,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=44.971499772176564, CurrSamplesPerSec=42.369542406246886, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 980/1404]  eta: 0:04:21  lr: 0.000094  min_lr: 0.000001  loss: 4.4063 (4.3669)  loss_scale: 32768.0000 (50070.5729)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1396  max mem: 15572
Epoch: [5]  [ 990/1404]  eta: 0:04:14  lr: 0.000094  min_lr: 0.000001  loss: 4.1979 (4.3651)  loss_scale: 32768.0000 (49895.9758)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1461  max mem: 15572
Epoch: [5]  [1000/1404]  eta: 0:04:08  lr: 0.000094  min_lr: 0.000001  loss: 4.2257 (4.3645)  loss_scale: 32768.0000 (49724.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1093  max mem: 15572
Epoch: [5]  [1010/1404]  eta: 0:04:02  lr: 0.000094  min_lr: 0.000001  loss: 4.3289 (4.3634)  loss_scale: 32768.0000 (49557.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0522  max mem: 15572
Epoch: [5]  [1020/1404]  eta: 0:03:56  lr: 0.000094  min_lr: 0.000001  loss: 4.3754 (4.3643)  loss_scale: 32768.0000 (49392.7052)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0764  max mem: 15572
[2025-01-10 17:04:51,154] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:04:51,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:04:51,168] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:04:51,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1030/1404]  eta: 0:03:50  lr: 0.000094  min_lr: 0.000001  loss: 4.2516 (4.3611)  loss_scale: 32768.0000 (49326.8050)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1109  max mem: 15572
Epoch: [5]  [1040/1404]  eta: 0:03:43  lr: 0.000094  min_lr: 0.000001  loss: 4.0732 (4.3610)  loss_scale: 65536.0000 (49482.5130)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0781  max mem: 15572
Epoch: [5]  [1050/1404]  eta: 0:03:37  lr: 0.000094  min_lr: 0.000001  loss: 4.4179 (4.3617)  loss_scale: 65536.0000 (49635.2578)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1322  max mem: 15572
Epoch: [5]  [1060/1404]  eta: 0:03:32  lr: 0.000094  min_lr: 0.000001  loss: 4.4039 (4.3609)  loss_scale: 65536.0000 (49785.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.2107  max mem: 15572
Epoch: [5]  [1070/1404]  eta: 0:03:25  lr: 0.000094  min_lr: 0.000001  loss: 4.2878 (4.3605)  loss_scale: 65536.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.1842  max mem: 15572
Epoch: [5]  [1080/1404]  eta: 0:03:19  lr: 0.000094  min_lr: 0.000001  loss: 4.3308 (4.3607)  loss_scale: 65536.0000 (50076.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1445  max mem: 15572
Epoch: [5]  [1090/1404]  eta: 0:03:13  lr: 0.000094  min_lr: 0.000001  loss: 4.3308 (4.3611)  loss_scale: 65536.0000 (50218.2365)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1441  max mem: 15572
Epoch: [5]  [1100/1404]  eta: 0:03:07  lr: 0.000094  min_lr: 0.000001  loss: 4.3152 (4.3620)  loss_scale: 65536.0000 (50357.3624)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1300  max mem: 15572
Epoch: [5]  [1110/1404]  eta: 0:03:01  lr: 0.000094  min_lr: 0.000001  loss: 4.3382 (4.3619)  loss_scale: 65536.0000 (50493.9838)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1233  max mem: 15572
Epoch: [5]  [1120/1404]  eta: 0:02:55  lr: 0.000094  min_lr: 0.000001  loss: 4.2840 (4.3603)  loss_scale: 65536.0000 (50628.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1026  max mem: 15572
Epoch: [5]  [1130/1404]  eta: 0:02:49  lr: 0.000094  min_lr: 0.000001  loss: 4.2056 (4.3598)  loss_scale: 65536.0000 (50759.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1219  max mem: 15572
[2025-01-10 17:05:59,002] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8156
[2025-01-10 17:05:59,002] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:05:59,002] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:05:59,034] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8156
[2025-01-10 17:05:59,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [1140/1404]  eta: 0:02:42  lr: 0.000094  min_lr: 0.000001  loss: 4.2814 (4.3586)  loss_scale: 65536.0000 (50745.8861)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1041  max mem: 15572
Epoch: [5]  [1150/1404]  eta: 0:02:36  lr: 0.000094  min_lr: 0.000001  loss: 4.3738 (4.3587)  loss_scale: 32768.0000 (50589.6924)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0379  max mem: 15572
Epoch: [5]  [1160/1404]  eta: 0:02:30  lr: 0.000094  min_lr: 0.000001  loss: 4.3012 (4.3578)  loss_scale: 32768.0000 (50436.1895)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1095  max mem: 15572
Epoch: [5]  [1170/1404]  eta: 0:02:24  lr: 0.000094  min_lr: 0.000001  loss: 4.2942 (4.3575)  loss_scale: 32768.0000 (50285.3083)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.1721  max mem: 15572
Epoch: [5]  [1180/1404]  eta: 0:02:17  lr: 0.000094  min_lr: 0.000001  loss: 4.2942 (4.3576)  loss_scale: 32768.0000 (50136.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0746  max mem: 15572
Epoch: [5]  [1190/1404]  eta: 0:02:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3165 (4.3574)  loss_scale: 32768.0000 (49991.1469)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0320  max mem: 15572
Epoch: [5]  [1200/1404]  eta: 0:02:05  lr: 0.000094  min_lr: 0.000001  loss: 4.4539 (4.3560)  loss_scale: 32768.0000 (49847.7402)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0496  max mem: 15572
Epoch: [5]  [1210/1404]  eta: 0:01:59  lr: 0.000094  min_lr: 0.000001  loss: 4.2003 (4.3549)  loss_scale: 32768.0000 (49706.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0865  max mem: 15572
Epoch: [5]  [1220/1404]  eta: 0:01:53  lr: 0.000094  min_lr: 0.000001  loss: 4.3359 (4.3543)  loss_scale: 32768.0000 (49567.9738)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.1464  max mem: 15572
Epoch: [5]  [1230/1404]  eta: 0:01:47  lr: 0.000094  min_lr: 0.000001  loss: 4.3359 (4.3536)  loss_scale: 32768.0000 (49431.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0911  max mem: 15572
Epoch: [5]  [1240/1404]  eta: 0:01:40  lr: 0.000094  min_lr: 0.000001  loss: 4.3227 (4.3541)  loss_scale: 32768.0000 (49297.2248)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0765  max mem: 15572
Epoch: [5]  [1250/1404]  eta: 0:01:34  lr: 0.000094  min_lr: 0.000001  loss: 4.2906 (4.3545)  loss_scale: 32768.0000 (49165.0967)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0927  max mem: 15572
Epoch: [5]  [1260/1404]  eta: 0:01:28  lr: 0.000094  min_lr: 0.000001  loss: 4.2458 (4.3537)  loss_scale: 32768.0000 (49035.0642)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1029  max mem: 15572
[2025-01-10 17:07:16,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:07:16,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:07:16,359] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:07:16,359] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1270/1404]  eta: 0:01:22  lr: 0.000094  min_lr: 0.000001  loss: 4.2861 (4.3544)  loss_scale: 32768.0000 (49061.7655)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1073  max mem: 15572
Epoch: [5]  [1280/1404]  eta: 0:01:16  lr: 0.000094  min_lr: 0.000001  loss: 4.2337 (4.3535)  loss_scale: 65536.0000 (49190.3700)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0550  max mem: 15572
Epoch: [5]  [1290/1404]  eta: 0:01:10  lr: 0.000094  min_lr: 0.000001  loss: 4.3053 (4.3541)  loss_scale: 65536.0000 (49316.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0392  max mem: 15572
Epoch: [5]  [1300/1404]  eta: 0:01:03  lr: 0.000094  min_lr: 0.000001  loss: 4.3705 (4.3541)  loss_scale: 65536.0000 (49441.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1053  max mem: 15572
Epoch: [5]  [1310/1404]  eta: 0:00:57  lr: 0.000094  min_lr: 0.000001  loss: 4.3705 (4.3547)  loss_scale: 65536.0000 (49564.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1577  max mem: 15572
Epoch: [5]  [1320/1404]  eta: 0:00:51  lr: 0.000094  min_lr: 0.000001  loss: 4.2818 (4.3544)  loss_scale: 65536.0000 (49685.3172)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1434  max mem: 15572
Epoch: [5]  [1330/1404]  eta: 0:00:45  lr: 0.000094  min_lr: 0.000001  loss: 4.2257 (4.3530)  loss_scale: 65536.0000 (49804.4057)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.2046  max mem: 15572
Epoch: [5]  [1340/1404]  eta: 0:00:39  lr: 0.000094  min_lr: 0.000001  loss: 4.1324 (4.3533)  loss_scale: 65536.0000 (49921.7181)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1323  max mem: 15572
Epoch: [5]  [1350/1404]  eta: 0:00:33  lr: 0.000094  min_lr: 0.000001  loss: 4.2912 (4.3526)  loss_scale: 65536.0000 (50037.2939)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0519  max mem: 15572
Epoch: [5]  [1360/1404]  eta: 0:00:27  lr: 0.000094  min_lr: 0.000001  loss: 4.2767 (4.3527)  loss_scale: 65536.0000 (50151.1712)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0855  max mem: 15572
Epoch: [5]  [1370/1404]  eta: 0:00:20  lr: 0.000094  min_lr: 0.000001  loss: 4.3670 (4.3525)  loss_scale: 65536.0000 (50263.3873)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0988  max mem: 15572
Epoch: [5]  [1380/1404]  eta: 0:00:14  lr: 0.000094  min_lr: 0.000001  loss: 4.3771 (4.3527)  loss_scale: 65536.0000 (50373.9783)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0926  max mem: 15572
Epoch: [5]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.3771 (4.3531)  loss_scale: 65536.0000 (50482.9792)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0727  max mem: 15572
[2025-01-10 17:08:34,477] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:08:34,477] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:08:34,478] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:08:34,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:08:34,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8414
[2025-01-10 17:08:34,852] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:08:34,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-10 17:08:34,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8414
[2025-01-10 17:08:34,852] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:08:37,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8420
[2025-01-10 17:08:37,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:08:37,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2794 (4.3513)  loss_scale: 65536.0000 (50613.8130)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0447  max mem: 15572
[2025-01-10 17:08:37,103] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8420
[2025-01-10 17:08:37,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [5]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.2950 (4.3522)  loss_scale: 65536.0000 (50575.6809)  weight_decay: 0.0500 (0.0500)  time: 0.4596  data: 0.0447  max mem: 15572
Epoch: [5] Total time: 0:14:20 (0.6132 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.2950 (4.3564)  loss_scale: 65536.0000 (50575.6809)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:07:47  loss: 1.9824 (1.9824)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 3.4346  data: 3.2346  max mem: 15572
Val:  [ 10/136]  eta: 0:01:36  loss: 3.7965 (3.6031)  acc1: 5.5556 (21.7172)  acc5: 44.4444 (37.8788)  time: 0.7687  data: 0.5773  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 3.6765 (3.6714)  acc1: 5.5556 (16.9312)  acc5: 44.4444 (40.7407)  time: 0.4195  data: 0.2223  max mem: 15572
Val:  [ 30/136]  eta: 0:00:48  loss: 3.4187 (3.3665)  acc1: 11.1111 (25.8065)  acc5: 55.5556 (48.2079)  time: 0.2889  data: 0.0749  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.3108 (3.3051)  acc1: 38.8889 (25.7453)  acc5: 77.7778 (51.7615)  time: 0.3131  data: 0.1000  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 3.7724 (3.4619)  acc1: 0.0000 (22.0044)  acc5: 44.4444 (46.1874)  time: 0.4171  data: 0.2005  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 3.9599 (3.5664)  acc1: 0.0000 (19.3989)  acc5: 22.2222 (43.7158)  time: 0.4184  data: 0.1904  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 3.9085 (3.4756)  acc1: 5.5556 (22.7700)  acc5: 44.4444 (45.8529)  time: 0.3294  data: 0.1182  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 3.2668 (3.4634)  acc1: 22.2222 (22.3594)  acc5: 55.5556 (46.5706)  time: 0.3255  data: 0.1229  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.6134 (3.4619)  acc1: 16.6667 (21.9780)  acc5: 44.4444 (47.5580)  time: 0.3708  data: 0.1683  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.5016 (3.5031)  acc1: 5.5556 (20.9021)  acc5: 50.0000 (47.0297)  time: 0.3510  data: 0.1558  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.5528 (3.5157)  acc1: 5.5556 (21.4214)  acc5: 38.8889 (46.8468)  time: 0.3726  data: 0.1712  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.2648 (3.4600)  acc1: 33.3333 (23.2782)  acc5: 61.1111 (49.4490)  time: 0.4394  data: 0.2323  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.8660 (3.4241)  acc1: 38.8889 (24.8516)  acc5: 72.2222 (50.2545)  time: 0.3717  data: 0.1887  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.0582 (3.4277)  acc1: 38.8889 (24.9386)  acc5: 61.1111 (50.4095)  time: 0.2153  data: 0.0540  max mem: 15572
Val: Total time: 0:00:52 (0.3849 s / it)
* Acc@1 24.304 Acc@5 49.161 loss 3.462
Accuracy of the network on the 4883 val videos: 24.3%
[2025-01-10 17:09:30,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 17:09:30,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 17:09:30,819] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 17:09:30,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 17:09:33,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 17:09:33,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.30%
Epoch: [6]  [   0/1404]  eta: 3:47:47  lr: 0.000094  min_lr: 0.000001  loss: 4.2785 (4.2785)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.7349  data: 7.0866  max mem: 15572
Epoch: [6]  [  10/1404]  eta: 0:33:00  lr: 0.000094  min_lr: 0.000001  loss: 4.2785 (4.2876)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4205  data: 0.6453  max mem: 15572
Epoch: [6]  [  20/1404]  eta: 0:23:52  lr: 0.000094  min_lr: 0.000001  loss: 4.2721 (4.3479)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0568  max mem: 15572
Epoch: [6]  [  30/1404]  eta: 0:19:49  lr: 0.000094  min_lr: 0.000001  loss: 4.3003 (4.3814)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0664  max mem: 15572
Epoch: [6]  [  40/1404]  eta: 0:18:50  lr: 0.000094  min_lr: 0.000001  loss: 4.4531 (4.3924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1125  max mem: 15572
Epoch: [6]  [  50/1404]  eta: 0:17:48  lr: 0.000094  min_lr: 0.000001  loss: 4.4031 (4.3671)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.1554  max mem: 15572
Epoch: [6]  [  60/1404]  eta: 0:17:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2051 (4.3330)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1262  max mem: 15572
Epoch: [6]  [  70/1404]  eta: 0:16:26  lr: 0.000094  min_lr: 0.000001  loss: 4.2087 (4.3281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1397  max mem: 15572
Epoch: [6]  [  80/1404]  eta: 0:16:03  lr: 0.000094  min_lr: 0.000001  loss: 4.2795 (4.3258)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1314  max mem: 15572
Epoch: [6]  [  90/1404]  eta: 0:15:24  lr: 0.000094  min_lr: 0.000001  loss: 4.3411 (4.3321)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0651  max mem: 15572
Epoch: [6]  [ 100/1404]  eta: 0:15:12  lr: 0.000094  min_lr: 0.000001  loss: 4.4442 (4.3322)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0700  max mem: 15572
Epoch: [6]  [ 110/1404]  eta: 0:15:04  lr: 0.000094  min_lr: 0.000001  loss: 4.3149 (4.3303)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.1620  max mem: 15572
Epoch: [6]  [ 120/1404]  eta: 0:14:43  lr: 0.000094  min_lr: 0.000001  loss: 4.2732 (4.3295)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1242  max mem: 15572
[2025-01-10 17:10:59,323] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:10:59,324] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:10:59,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:10:59,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 130/1404]  eta: 0:14:31  lr: 0.000094  min_lr: 0.000001  loss: 4.3944 (4.3335)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0914  max mem: 15572
Epoch: [6]  [ 140/1404]  eta: 0:14:12  lr: 0.000094  min_lr: 0.000001  loss: 4.4138 (4.3355)  loss_scale: 65536.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0760  max mem: 15572
Epoch: [6]  [ 150/1404]  eta: 0:13:56  lr: 0.000094  min_lr: 0.000001  loss: 4.3650 (4.3317)  loss_scale: 65536.0000 (38410.1722)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0453  max mem: 15572
Epoch: [6]  [ 160/1404]  eta: 0:13:45  lr: 0.000094  min_lr: 0.000001  loss: 4.2636 (4.3222)  loss_scale: 65536.0000 (40095.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0568  max mem: 15572
[2025-01-10 17:11:26,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8594
[2025-01-10 17:11:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:11:26,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8594
[2025-01-10 17:11:26,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:11:26,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 170/1404]  eta: 0:13:34  lr: 0.000094  min_lr: 0.000001  loss: 4.2869 (4.3272)  loss_scale: 65536.0000 (41391.1579)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0621  max mem: 15572
Epoch: [6]  [ 180/1404]  eta: 0:13:29  lr: 0.000094  min_lr: 0.000001  loss: 4.4559 (4.3345)  loss_scale: 32768.0000 (40914.7403)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1119  max mem: 15572
Epoch: [6]  [ 190/1404]  eta: 0:13:15  lr: 0.000094  min_lr: 0.000001  loss: 4.4223 (4.3279)  loss_scale: 32768.0000 (40488.2094)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1015  max mem: 15572
Epoch: [6]  [ 200/1404]  eta: 0:13:07  lr: 0.000094  min_lr: 0.000001  loss: 4.3863 (4.3260)  loss_scale: 32768.0000 (40104.1194)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0510  max mem: 15572
Epoch: [6]  [ 210/1404]  eta: 0:13:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4033 (4.3228)  loss_scale: 32768.0000 (39756.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0771  max mem: 15572
Epoch: [6]  [ 220/1404]  eta: 0:12:51  lr: 0.000093  min_lr: 0.000001  loss: 4.4033 (4.3248)  loss_scale: 32768.0000 (39440.2172)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0923  max mem: 15572
Epoch: [6]  [ 230/1404]  eta: 0:12:40  lr: 0.000093  min_lr: 0.000001  loss: 4.2026 (4.3181)  loss_scale: 32768.0000 (39151.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0651  max mem: 15572
Epoch: [6]  [ 240/1404]  eta: 0:12:33  lr: 0.000093  min_lr: 0.000001  loss: 4.2026 (4.3175)  loss_scale: 32768.0000 (38886.5062)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0510  max mem: 15572
Epoch: [6]  [ 250/1404]  eta: 0:12:26  lr: 0.000093  min_lr: 0.000001  loss: 4.3027 (4.3172)  loss_scale: 32768.0000 (38642.7410)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.0649  max mem: 15572
Epoch: [6]  [ 260/1404]  eta: 0:12:20  lr: 0.000093  min_lr: 0.000001  loss: 4.0977 (4.3041)  loss_scale: 32768.0000 (38417.6552)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.0993  max mem: 15572
Epoch: [6]  [ 270/1404]  eta: 0:12:08  lr: 0.000093  min_lr: 0.000001  loss: 4.0977 (4.3024)  loss_scale: 32768.0000 (38209.1808)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0757  max mem: 15572
Epoch: [6]  [ 280/1404]  eta: 0:11:58  lr: 0.000093  min_lr: 0.000001  loss: 4.3629 (4.3012)  loss_scale: 32768.0000 (38015.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0441  max mem: 15572
Epoch: [6]  [ 290/1404]  eta: 0:11:53  lr: 0.000093  min_lr: 0.000001  loss: 4.2444 (4.2986)  loss_scale: 32768.0000 (37835.2165)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0537  max mem: 15572
[2025-01-10 17:12:44,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:12:44,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:12:44,967] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:12:44,967] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 300/1404]  eta: 0:11:44  lr: 0.000093  min_lr: 0.000001  loss: 4.2444 (4.3029)  loss_scale: 32768.0000 (37884.5980)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0227  max mem: 15572
Epoch: [6]  [ 310/1404]  eta: 0:11:35  lr: 0.000093  min_lr: 0.000001  loss: 4.3344 (4.3046)  loss_scale: 65536.0000 (38773.7106)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0008  max mem: 15572
Epoch: [6]  [ 320/1404]  eta: 0:11:28  lr: 0.000093  min_lr: 0.000001  loss: 4.4387 (4.3078)  loss_scale: 65536.0000 (39607.4268)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0558  max mem: 15572
Epoch: [6]  [ 330/1404]  eta: 0:11:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3395 (4.3050)  loss_scale: 65536.0000 (40390.7674)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1237  max mem: 15572
[2025-01-10 17:13:05,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8757
[2025-01-10 17:13:05,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:13:05,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8757
[2025-01-10 17:13:05,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:13:05,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 340/1404]  eta: 0:11:14  lr: 0.000093  min_lr: 0.000001  loss: 4.2193 (4.3064)  loss_scale: 65536.0000 (40359.4135)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1163  max mem: 15572
Epoch: [6]  [ 350/1404]  eta: 0:11:08  lr: 0.000093  min_lr: 0.000001  loss: 4.1776 (4.3018)  loss_scale: 32768.0000 (40143.1339)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1235  max mem: 15572
Epoch: [6]  [ 360/1404]  eta: 0:11:01  lr: 0.000093  min_lr: 0.000001  loss: 4.2882 (4.3011)  loss_scale: 32768.0000 (39938.8366)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1282  max mem: 15572
Epoch: [6]  [ 370/1404]  eta: 0:10:55  lr: 0.000093  min_lr: 0.000001  loss: 4.3249 (4.3020)  loss_scale: 32768.0000 (39745.5526)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1438  max mem: 15572
Epoch: [6]  [ 380/1404]  eta: 0:10:47  lr: 0.000093  min_lr: 0.000001  loss: 4.3249 (4.3003)  loss_scale: 32768.0000 (39562.4147)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1174  max mem: 15572
Epoch: [6]  [ 390/1404]  eta: 0:10:38  lr: 0.000093  min_lr: 0.000001  loss: 4.1951 (4.3001)  loss_scale: 32768.0000 (39388.6445)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0349  max mem: 15572
Epoch: [6]  [ 400/1404]  eta: 0:10:31  lr: 0.000093  min_lr: 0.000001  loss: 4.1908 (4.2989)  loss_scale: 32768.0000 (39223.5411)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0542  max mem: 15572
Epoch: [6]  [ 410/1404]  eta: 0:10:24  lr: 0.000093  min_lr: 0.000001  loss: 4.3190 (4.3003)  loss_scale: 32768.0000 (39066.4720)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0835  max mem: 15572
Epoch: [6]  [ 420/1404]  eta: 0:10:17  lr: 0.000093  min_lr: 0.000001  loss: 4.3470 (4.3012)  loss_scale: 32768.0000 (38916.8646)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0383  max mem: 15572
Epoch: [6]  [ 430/1404]  eta: 0:10:11  lr: 0.000093  min_lr: 0.000001  loss: 4.3221 (4.3000)  loss_scale: 32768.0000 (38774.1995)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0230  max mem: 15572
Epoch: [6]  [ 440/1404]  eta: 0:10:05  lr: 0.000093  min_lr: 0.000001  loss: 4.3277 (4.3013)  loss_scale: 32768.0000 (38638.0045)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0233  max mem: 15572
Epoch: [6]  [ 450/1404]  eta: 0:09:58  lr: 0.000093  min_lr: 0.000001  loss: 4.3205 (4.3017)  loss_scale: 32768.0000 (38507.8492)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0010  max mem: 15572
Epoch: [6]  [ 460/1404]  eta: 0:09:53  lr: 0.000093  min_lr: 0.000001  loss: 4.3632 (4.3047)  loss_scale: 32768.0000 (38383.3406)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0783  max mem: 15572
[2025-01-10 17:14:24,128] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:14:24,128] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:14:24,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:14:24,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 470/1404]  eta: 0:09:47  lr: 0.000093  min_lr: 0.000001  loss: 4.2855 (4.2989)  loss_scale: 32768.0000 (38890.2590)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.1615  max mem: 15572
Epoch: [6]  [ 480/1404]  eta: 0:09:41  lr: 0.000093  min_lr: 0.000001  loss: 4.2058 (4.2950)  loss_scale: 65536.0000 (39444.2245)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1369  max mem: 15572
Epoch: [6]  [ 490/1404]  eta: 0:09:35  lr: 0.000093  min_lr: 0.000001  loss: 4.2809 (4.2964)  loss_scale: 65536.0000 (39975.6253)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.1228  max mem: 15572
Epoch: [6]  [ 500/1404]  eta: 0:09:27  lr: 0.000093  min_lr: 0.000001  loss: 4.3163 (4.2965)  loss_scale: 65536.0000 (40485.8124)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0698  max mem: 15572
Epoch: [6]  [ 510/1404]  eta: 0:09:20  lr: 0.000093  min_lr: 0.000001  loss: 4.3615 (4.2994)  loss_scale: 65536.0000 (40976.0313)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0236  max mem: 15572
[2025-01-10 17:15:01,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-10 17:15:01,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:15:01,283] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-10 17:15:01,283] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:15:01,284] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 520/1404]  eta: 0:09:16  lr: 0.000093  min_lr: 0.000001  loss: 4.3615 (4.2981)  loss_scale: 65536.0000 (41384.5374)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.1577  max mem: 15572
Epoch: [6]  [ 530/1404]  eta: 0:09:07  lr: 0.000093  min_lr: 0.000001  loss: 4.1872 (4.2967)  loss_scale: 32768.0000 (41222.2674)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1475  max mem: 15572
Epoch: [6]  [ 540/1404]  eta: 0:09:00  lr: 0.000093  min_lr: 0.000001  loss: 4.3260 (4.2995)  loss_scale: 32768.0000 (41065.9963)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0302  max mem: 15572
Epoch: [6]  [ 550/1404]  eta: 0:08:53  lr: 0.000093  min_lr: 0.000001  loss: 4.4512 (4.2984)  loss_scale: 32768.0000 (40915.3975)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0718  max mem: 15572
Epoch: [6]  [ 560/1404]  eta: 0:08:48  lr: 0.000093  min_lr: 0.000001  loss: 4.4512 (4.2975)  loss_scale: 32768.0000 (40770.1676)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.1518  max mem: 15572
Epoch: [6]  [ 570/1404]  eta: 0:08:40  lr: 0.000093  min_lr: 0.000001  loss: 4.0602 (4.2947)  loss_scale: 32768.0000 (40630.0245)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1174  max mem: 15572
[2025-01-10 17:15:32,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=48, lr=[9.047079288391729e-07, 9.047079288391729e-07, 1.2924398983416757e-06, 1.2924398983416757e-06, 1.8463427119166798e-06, 1.8463427119166798e-06, 2.637632445595257e-06, 2.637632445595257e-06, 3.7680463508503673e-06, 3.7680463508503673e-06, 5.382923358357668e-06, 5.382923358357668e-06, 7.689890511939525e-06, 7.689890511939525e-06, 1.0985557874199324e-05, 1.0985557874199324e-05, 1.5693654105999034e-05, 1.5693654105999034e-05, 2.241950586571291e-05, 2.241950586571291e-05, 3.202786552244701e-05, 3.202786552244701e-05, 4.575409360349573e-05, 4.575409360349573e-05, 6.536299086213677e-05, 6.536299086213677e-05, 9.337570123162396e-05, 9.337570123162396e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:15:32,565] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=45.398361887766335, CurrSamplesPerSec=42.2500570814586, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 580/1404]  eta: 0:08:33  lr: 0.000093  min_lr: 0.000001  loss: 4.0138 (4.2917)  loss_scale: 32768.0000 (40494.7057)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0525  max mem: 15572
Epoch: [6]  [ 590/1404]  eta: 0:08:26  lr: 0.000093  min_lr: 0.000001  loss: 4.2045 (4.2918)  loss_scale: 32768.0000 (40363.9662)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0820  max mem: 15572
Epoch: [6]  [ 600/1404]  eta: 0:08:20  lr: 0.000093  min_lr: 0.000001  loss: 4.2755 (4.2924)  loss_scale: 32768.0000 (40237.5774)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0994  max mem: 15572
Epoch: [6]  [ 610/1404]  eta: 0:08:13  lr: 0.000093  min_lr: 0.000001  loss: 4.1850 (4.2911)  loss_scale: 32768.0000 (40115.3257)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0498  max mem: 15572
Epoch: [6]  [ 620/1404]  eta: 0:08:07  lr: 0.000093  min_lr: 0.000001  loss: 4.1741 (4.2888)  loss_scale: 32768.0000 (39997.0113)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0007  max mem: 15572
Epoch: [6]  [ 630/1404]  eta: 0:08:01  lr: 0.000093  min_lr: 0.000001  loss: 4.2408 (4.2876)  loss_scale: 32768.0000 (39882.4469)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.0009  max mem: 15572
Epoch: [6]  [ 640/1404]  eta: 0:07:55  lr: 0.000093  min_lr: 0.000001  loss: 4.2408 (4.2872)  loss_scale: 32768.0000 (39771.4571)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0009  max mem: 15572
[2025-01-10 17:16:18,510] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:16:18,510] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:16:18,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:16:18,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 650/1404]  eta: 0:07:49  lr: 0.000093  min_lr: 0.000001  loss: 4.1952 (4.2857)  loss_scale: 32768.0000 (39764.5469)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0009  max mem: 15572
Epoch: [6]  [ 660/1404]  eta: 0:07:43  lr: 0.000093  min_lr: 0.000001  loss: 4.1762 (4.2847)  loss_scale: 65536.0000 (40154.4327)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.0011  max mem: 15572
Epoch: [6]  [ 670/1404]  eta: 0:07:36  lr: 0.000093  min_lr: 0.000001  loss: 4.2994 (4.2867)  loss_scale: 65536.0000 (40532.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0014  max mem: 15572
Epoch: [6]  [ 680/1404]  eta: 0:07:30  lr: 0.000093  min_lr: 0.000001  loss: 4.3636 (4.2855)  loss_scale: 65536.0000 (40899.8532)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0012  max mem: 15572
Epoch: [6]  [ 690/1404]  eta: 0:07:23  lr: 0.000093  min_lr: 0.000001  loss: 4.3636 (4.2873)  loss_scale: 65536.0000 (41256.3821)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0008  max mem: 15572
Epoch: [6]  [ 700/1404]  eta: 0:07:17  lr: 0.000093  min_lr: 0.000001  loss: 4.3422 (4.2874)  loss_scale: 65536.0000 (41602.7389)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0275  max mem: 15572
[2025-01-10 17:16:50,191] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9127
[2025-01-10 17:16:50,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:16:50,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:16:50,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9127
[2025-01-10 17:16:50,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 710/1404]  eta: 0:07:10  lr: 0.000093  min_lr: 0.000001  loss: 4.3080 (4.2870)  loss_scale: 65536.0000 (41570.6554)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0415  max mem: 15572
Epoch: [6]  [ 720/1404]  eta: 0:07:03  lr: 0.000093  min_lr: 0.000001  loss: 4.1708 (4.2827)  loss_scale: 32768.0000 (41448.5659)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0151  max mem: 15572
Epoch: [6]  [ 730/1404]  eta: 0:06:57  lr: 0.000093  min_lr: 0.000001  loss: 4.1708 (4.2830)  loss_scale: 32768.0000 (41329.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0010  max mem: 15572
Epoch: [6]  [ 740/1404]  eta: 0:06:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2689 (4.2818)  loss_scale: 32768.0000 (41214.2726)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.0328  max mem: 15572
Epoch: [6]  [ 750/1404]  eta: 0:06:45  lr: 0.000093  min_lr: 0.000001  loss: 4.1696 (4.2809)  loss_scale: 32768.0000 (41101.8056)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0328  max mem: 15572
Epoch: [6]  [ 760/1404]  eta: 0:06:38  lr: 0.000093  min_lr: 0.000001  loss: 4.2164 (4.2810)  loss_scale: 32768.0000 (40992.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0006  max mem: 15572
Epoch: [6]  [ 770/1404]  eta: 0:06:32  lr: 0.000093  min_lr: 0.000001  loss: 4.3767 (4.2812)  loss_scale: 32768.0000 (40885.6239)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0008  max mem: 15572
Epoch: [6]  [ 780/1404]  eta: 0:06:26  lr: 0.000093  min_lr: 0.000001  loss: 4.3809 (4.2816)  loss_scale: 32768.0000 (40781.6850)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0010  max mem: 15572
Epoch: [6]  [ 790/1404]  eta: 0:06:19  lr: 0.000093  min_lr: 0.000001  loss: 4.2731 (4.2798)  loss_scale: 32768.0000 (40680.3742)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.0009  max mem: 15572
Epoch: [6]  [ 800/1404]  eta: 0:06:13  lr: 0.000093  min_lr: 0.000001  loss: 4.3956 (4.2841)  loss_scale: 32768.0000 (40581.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0011  max mem: 15572
Epoch: [6]  [ 810/1404]  eta: 0:06:07  lr: 0.000093  min_lr: 0.000001  loss: 4.3470 (4.2833)  loss_scale: 32768.0000 (40485.2478)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0413  max mem: 15572
Epoch: [6]  [ 820/1404]  eta: 0:06:00  lr: 0.000093  min_lr: 0.000001  loss: 4.2285 (4.2827)  loss_scale: 32768.0000 (40391.2497)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0410  max mem: 15572
Epoch: [6]  [ 830/1404]  eta: 0:05:55  lr: 0.000093  min_lr: 0.000001  loss: 4.2604 (4.2833)  loss_scale: 32768.0000 (40299.5138)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0717  max mem: 15572
[2025-01-10 17:18:08,986] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:18:08,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:18:09,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:18:09,049] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:18:11,699] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9261
[2025-01-10 17:18:11,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:18:11,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:18:11,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9261
[2025-01-10 17:18:11,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 840/1404]  eta: 0:05:49  lr: 0.000093  min_lr: 0.000001  loss: 4.2926 (4.2844)  loss_scale: 32768.0000 (40404.7753)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.1054  max mem: 15572
Epoch: [6]  [ 850/1404]  eta: 0:05:43  lr: 0.000093  min_lr: 0.000001  loss: 4.3014 (4.2832)  loss_scale: 32768.0000 (40315.0364)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0857  max mem: 15572
Epoch: [6]  [ 860/1404]  eta: 0:05:37  lr: 0.000093  min_lr: 0.000001  loss: 4.3142 (4.2841)  loss_scale: 32768.0000 (40227.3821)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1222  max mem: 15572
Epoch: [6]  [ 870/1404]  eta: 0:05:31  lr: 0.000093  min_lr: 0.000001  loss: 4.1728 (4.2829)  loss_scale: 32768.0000 (40141.7405)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.1291  max mem: 15572
Epoch: [6]  [ 880/1404]  eta: 0:05:24  lr: 0.000093  min_lr: 0.000001  loss: 4.0725 (4.2817)  loss_scale: 32768.0000 (40058.0431)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1178  max mem: 15572
Epoch: [6]  [ 890/1404]  eta: 0:05:18  lr: 0.000093  min_lr: 0.000001  loss: 4.1072 (4.2812)  loss_scale: 32768.0000 (39976.2245)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0875  max mem: 15572
Epoch: [6]  [ 900/1404]  eta: 0:05:12  lr: 0.000093  min_lr: 0.000001  loss: 4.2806 (4.2797)  loss_scale: 32768.0000 (39896.2220)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0954  max mem: 15572
Epoch: [6]  [ 910/1404]  eta: 0:05:05  lr: 0.000093  min_lr: 0.000001  loss: 4.0824 (4.2768)  loss_scale: 32768.0000 (39817.9759)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0940  max mem: 15572
Epoch: [6]  [ 920/1404]  eta: 0:04:59  lr: 0.000093  min_lr: 0.000001  loss: 4.1234 (4.2771)  loss_scale: 32768.0000 (39741.4289)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0942  max mem: 15572
Epoch: [6]  [ 930/1404]  eta: 0:04:52  lr: 0.000093  min_lr: 0.000001  loss: 4.3765 (4.2785)  loss_scale: 32768.0000 (39666.5263)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1074  max mem: 15572
Epoch: [6]  [ 940/1404]  eta: 0:04:47  lr: 0.000093  min_lr: 0.000001  loss: 4.4214 (4.2799)  loss_scale: 32768.0000 (39593.2157)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1066  max mem: 15572
Epoch: [6]  [ 950/1404]  eta: 0:04:40  lr: 0.000093  min_lr: 0.000001  loss: 4.2897 (4.2777)  loss_scale: 32768.0000 (39521.4469)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1187  max mem: 15572
Epoch: [6]  [ 960/1404]  eta: 0:04:34  lr: 0.000093  min_lr: 0.000001  loss: 4.2710 (4.2781)  loss_scale: 32768.0000 (39451.1717)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1123  max mem: 15572
[2025-01-10 17:19:31,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:19:31,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:19:31,648] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:19:31,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 970/1404]  eta: 0:04:28  lr: 0.000093  min_lr: 0.000001  loss: 4.3755 (4.2786)  loss_scale: 32768.0000 (39551.0772)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0748  max mem: 15572
Epoch: [6]  [ 980/1404]  eta: 0:04:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3529 (4.2790)  loss_scale: 65536.0000 (39815.9592)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0536  max mem: 15572
Epoch: [6]  [ 990/1404]  eta: 0:04:15  lr: 0.000093  min_lr: 0.000001  loss: 4.3529 (4.2804)  loss_scale: 65536.0000 (40075.4955)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0506  max mem: 15572
[2025-01-10 17:19:48,092] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9418
[2025-01-10 17:19:48,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:19:48,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:19:48,156] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9418
[2025-01-10 17:19:48,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [1000/1404]  eta: 0:04:09  lr: 0.000093  min_lr: 0.000001  loss: 4.2280 (4.2801)  loss_scale: 65536.0000 (40100.6993)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0458  max mem: 15572
Epoch: [6]  [1010/1404]  eta: 0:04:03  lr: 0.000093  min_lr: 0.000001  loss: 4.3319 (4.2801)  loss_scale: 32768.0000 (40028.1701)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0570  max mem: 15572
Epoch: [6]  [1020/1404]  eta: 0:03:57  lr: 0.000093  min_lr: 0.000001  loss: 4.3893 (4.2809)  loss_scale: 32768.0000 (39957.0617)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0618  max mem: 15572
Epoch: [6]  [1030/1404]  eta: 0:03:50  lr: 0.000093  min_lr: 0.000001  loss: 4.1535 (4.2778)  loss_scale: 32768.0000 (39887.3327)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0627  max mem: 15572
Epoch: [6]  [1040/1404]  eta: 0:03:44  lr: 0.000093  min_lr: 0.000001  loss: 4.0902 (4.2778)  loss_scale: 32768.0000 (39818.9433)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0610  max mem: 15572
Epoch: [6]  [1050/1404]  eta: 0:03:38  lr: 0.000093  min_lr: 0.000001  loss: 4.2286 (4.2777)  loss_scale: 32768.0000 (39751.8554)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0600  max mem: 15572
Epoch: [6]  [1060/1404]  eta: 0:03:32  lr: 0.000093  min_lr: 0.000001  loss: 4.2375 (4.2796)  loss_scale: 32768.0000 (39686.0320)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0685  max mem: 15572
Epoch: [6]  [1070/1404]  eta: 0:03:26  lr: 0.000093  min_lr: 0.000001  loss: 4.3664 (4.2797)  loss_scale: 32768.0000 (39621.4379)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0459  max mem: 15572
Epoch: [6]  [1080/1404]  eta: 0:03:19  lr: 0.000093  min_lr: 0.000001  loss: 4.0779 (4.2775)  loss_scale: 32768.0000 (39558.0389)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0007  max mem: 15572
Epoch: [6]  [1090/1404]  eta: 0:03:13  lr: 0.000093  min_lr: 0.000001  loss: 3.9852 (4.2751)  loss_scale: 32768.0000 (39495.8020)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0006  max mem: 15572
Epoch: [6]  [1100/1404]  eta: 0:03:07  lr: 0.000093  min_lr: 0.000001  loss: 4.2245 (4.2751)  loss_scale: 32768.0000 (39434.6957)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.0007  max mem: 15572
Epoch: [6]  [1110/1404]  eta: 0:03:01  lr: 0.000093  min_lr: 0.000001  loss: 4.3558 (4.2761)  loss_scale: 32768.0000 (39374.6895)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0008  max mem: 15572
Epoch: [6]  [1120/1404]  eta: 0:02:55  lr: 0.000093  min_lr: 0.000001  loss: 4.3279 (4.2758)  loss_scale: 32768.0000 (39315.7538)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0446  max mem: 15572
[2025-01-10 17:21:06,994] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:21:06,994] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:21:06,994] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:21:06,994] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1130/1404]  eta: 0:02:49  lr: 0.000093  min_lr: 0.000001  loss: 4.1377 (4.2763)  loss_scale: 32768.0000 (39489.6410)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0445  max mem: 15572
Epoch: [6]  [1140/1404]  eta: 0:02:42  lr: 0.000093  min_lr: 0.000001  loss: 4.2732 (4.2762)  loss_scale: 65536.0000 (39717.9176)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0010  max mem: 15572
Epoch: [6]  [1150/1404]  eta: 0:02:36  lr: 0.000093  min_lr: 0.000001  loss: 4.2764 (4.2754)  loss_scale: 65536.0000 (39942.2276)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0007  max mem: 15572
Epoch: [6]  [1160/1404]  eta: 0:02:30  lr: 0.000093  min_lr: 0.000001  loss: 4.1759 (4.2743)  loss_scale: 65536.0000 (40162.6736)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0082  max mem: 15572
Epoch: [6]  [1170/1404]  eta: 0:02:24  lr: 0.000093  min_lr: 0.000001  loss: 4.0937 (4.2732)  loss_scale: 65536.0000 (40379.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0443  max mem: 15572
[2025-01-10 17:21:38,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9599
[2025-01-10 17:21:38,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9599
[2025-01-10 17:21:38,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:21:38,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:21:38,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1180/1404]  eta: 0:02:18  lr: 0.000093  min_lr: 0.000001  loss: 4.0790 (4.2718)  loss_scale: 65536.0000 (40425.8899)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0367  max mem: 15572
Epoch: [6]  [1190/1404]  eta: 0:02:12  lr: 0.000093  min_lr: 0.000001  loss: 4.1577 (4.2708)  loss_scale: 32768.0000 (40361.5919)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0007  max mem: 15572
Epoch: [6]  [1200/1404]  eta: 0:02:05  lr: 0.000093  min_lr: 0.000001  loss: 4.1982 (4.2714)  loss_scale: 32768.0000 (40298.3647)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.0010  max mem: 15572
Epoch: [6]  [1210/1404]  eta: 0:01:59  lr: 0.000093  min_lr: 0.000001  loss: 4.0518 (4.2695)  loss_scale: 32768.0000 (40236.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0010  max mem: 15572
Epoch: [6]  [1220/1404]  eta: 0:01:53  lr: 0.000093  min_lr: 0.000001  loss: 4.0236 (4.2687)  loss_scale: 32768.0000 (40175.0172)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0009  max mem: 15572
Epoch: [6]  [1230/1404]  eta: 0:01:47  lr: 0.000093  min_lr: 0.000001  loss: 4.2039 (4.2687)  loss_scale: 32768.0000 (40114.8465)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0010  max mem: 15572
Epoch: [6]  [1240/1404]  eta: 0:01:41  lr: 0.000093  min_lr: 0.000001  loss: 4.2895 (4.2691)  loss_scale: 32768.0000 (40055.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0011  max mem: 15572
Epoch: [6]  [1250/1404]  eta: 0:01:34  lr: 0.000093  min_lr: 0.000001  loss: 4.3451 (4.2704)  loss_scale: 32768.0000 (39997.3909)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0008  max mem: 15572
Epoch: [6]  [1260/1404]  eta: 0:01:28  lr: 0.000093  min_lr: 0.000001  loss: 4.4106 (4.2710)  loss_scale: 32768.0000 (39940.0603)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0008  max mem: 15572
Epoch: [6]  [1270/1404]  eta: 0:01:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3843 (4.2712)  loss_scale: 32768.0000 (39883.6318)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0008  max mem: 15572
Epoch: [6]  [1280/1404]  eta: 0:01:16  lr: 0.000093  min_lr: 0.000001  loss: 4.3071 (4.2718)  loss_scale: 32768.0000 (39828.0843)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0010  max mem: 15572
Epoch: [6]  [1290/1404]  eta: 0:01:10  lr: 0.000093  min_lr: 0.000001  loss: 4.3071 (4.2718)  loss_scale: 32768.0000 (39773.3974)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0013  max mem: 15572
Epoch: [6]  [1300/1404]  eta: 0:01:04  lr: 0.000093  min_lr: 0.000001  loss: 4.2263 (4.2712)  loss_scale: 32768.0000 (39719.5511)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.0011  max mem: 15572
[2025-01-10 17:22:57,739] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:22:57,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:22:57,740] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:22:57,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1310/1404]  eta: 0:00:57  lr: 0.000093  min_lr: 0.000001  loss: 4.1163 (4.2704)  loss_scale: 32768.0000 (39841.4889)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0010  max mem: 15572
Epoch: [6]  [1320/1404]  eta: 0:00:51  lr: 0.000093  min_lr: 0.000001  loss: 4.0931 (4.2690)  loss_scale: 65536.0000 (40035.9970)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0008  max mem: 15572
Epoch: [6]  [1330/1404]  eta: 0:00:45  lr: 0.000093  min_lr: 0.000001  loss: 3.9462 (4.2678)  loss_scale: 65536.0000 (40227.5823)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0006  max mem: 15572
Epoch: [6]  [1340/1404]  eta: 0:00:39  lr: 0.000093  min_lr: 0.000001  loss: 3.9424 (4.2668)  loss_scale: 65536.0000 (40416.3102)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0007  max mem: 15572
Epoch: [6]  [1350/1404]  eta: 0:00:33  lr: 0.000093  min_lr: 0.000001  loss: 4.0748 (4.2659)  loss_scale: 65536.0000 (40602.2443)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0008  max mem: 15572
Epoch: [6]  [1360/1404]  eta: 0:00:27  lr: 0.000093  min_lr: 0.000001  loss: 4.2249 (4.2673)  loss_scale: 65536.0000 (40785.4460)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0010  max mem: 15572
Epoch: [6]  [1370/1404]  eta: 0:00:20  lr: 0.000093  min_lr: 0.000001  loss: 4.3353 (4.2660)  loss_scale: 65536.0000 (40965.9752)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0011  max mem: 15572
[2025-01-10 17:23:43,656] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9803
[2025-01-10 17:23:43,656] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:23:43,720] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9803
[2025-01-10 17:23:43,721] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:23:43,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.1549 (4.2659)  loss_scale: 65536.0000 (41096.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0008  max mem: 15572
Epoch: [6]  [1390/1404]  eta: 0:00:08  lr: 0.000093  min_lr: 0.000001  loss: 4.2914 (4.2662)  loss_scale: 32768.0000 (41036.5607)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0008  max mem: 15572
Epoch: [6]  [1400/1404]  eta: 0:00:02  lr: 0.000093  min_lr: 0.000001  loss: 4.2737 (4.2656)  loss_scale: 32768.0000 (40977.5418)  weight_decay: 0.0500 (0.0500)  time: 0.4677  data: 0.0006  max mem: 15572
Epoch: [6]  [1403/1404]  eta: 0:00:00  lr: 0.000093  min_lr: 0.000001  loss: 4.2598 (4.2655)  loss_scale: 32768.0000 (40960.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4469  data: 0.0005  max mem: 15572
Epoch: [6] Total time: 0:14:21 (0.6136 s / it)
Averaged stats: lr: 0.000093  min_lr: 0.000001  loss: 4.2598 (4.2582)  loss_scale: 32768.0000 (40960.0000)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:33  loss: 1.9783 (1.9783)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.2146  data: 4.0410  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 3.5054 (3.3075)  acc1: 5.5556 (23.7374)  acc5: 50.0000 (43.4343)  time: 0.7903  data: 0.5912  max mem: 15572
Val:  [ 20/136]  eta: 0:01:02  loss: 3.4970 (3.3952)  acc1: 11.1111 (19.5767)  acc5: 50.0000 (47.8836)  time: 0.3532  data: 0.1600  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 3.2299 (3.1220)  acc1: 16.6667 (29.0323)  acc5: 61.1111 (54.8387)  time: 0.2949  data: 0.0945  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 2.2002 (3.0216)  acc1: 38.8889 (29.9458)  acc5: 77.7778 (59.4851)  time: 0.3410  data: 0.1329  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 3.1726 (3.1485)  acc1: 11.1111 (27.0153)  acc5: 66.6667 (57.7342)  time: 0.3808  data: 0.1714  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 3.6163 (3.2615)  acc1: 5.5556 (24.0437)  acc5: 44.4444 (55.0091)  time: 0.3719  data: 0.1553  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 3.5911 (3.2042)  acc1: 5.5556 (26.6823)  acc5: 50.0000 (55.2426)  time: 0.3472  data: 0.1403  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 3.0391 (3.1812)  acc1: 38.8889 (27.5034)  acc5: 66.6667 (57.0645)  time: 0.3988  data: 0.1861  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 3.0485 (3.1971)  acc1: 27.7778 (26.8010)  acc5: 72.2222 (57.2039)  time: 0.4360  data: 0.2160  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.4794 (3.2503)  acc1: 11.1111 (25.4125)  acc5: 50.0000 (55.7206)  time: 0.4184  data: 0.1978  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.2550 (3.2412)  acc1: 22.2222 (26.2763)  acc5: 50.0000 (56.1061)  time: 0.3861  data: 0.1698  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.9536 (3.1746)  acc1: 33.3333 (28.4665)  acc5: 77.7778 (58.6777)  time: 0.3775  data: 0.1752  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4307 (3.1310)  acc1: 50.0000 (30.2375)  acc5: 83.3333 (59.2875)  time: 0.2618  data: 0.0923  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.7170 (3.1384)  acc1: 38.8889 (29.9754)  acc5: 77.7778 (59.2547)  time: 0.2395  data: 0.0922  max mem: 15572
Val: Total time: 0:00:51 (0.3802 s / it)
* Acc@1 29.464 Acc@5 57.473 loss 3.165
Accuracy of the network on the 4883 val videos: 29.5%
[2025-01-10 17:24:46,567] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 17:24:46,569] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 17:24:46,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 17:24:46,570] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 17:24:49,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 17:24:49,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 29.46%
Epoch: [7]  [   0/1404]  eta: 3:19:24  lr: 0.000093  min_lr: 0.000001  loss: 3.9453 (3.9453)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5220  data: 5.9211  max mem: 15572
Epoch: [7]  [  10/1404]  eta: 0:29:52  lr: 0.000093  min_lr: 0.000001  loss: 4.1230 (4.0836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2860  data: 0.5388  max mem: 15572
Epoch: [7]  [  20/1404]  eta: 0:22:06  lr: 0.000093  min_lr: 0.000001  loss: 4.1254 (4.0963)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0030  max mem: 15572
Epoch: [7]  [  30/1404]  eta: 0:19:28  lr: 0.000093  min_lr: 0.000001  loss: 4.0531 (4.0967)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0034  max mem: 15572
Epoch: [7]  [  40/1404]  eta: 0:17:44  lr: 0.000093  min_lr: 0.000001  loss: 4.1512 (4.1338)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0009  max mem: 15572
Epoch: [7]  [  50/1404]  eta: 0:16:48  lr: 0.000093  min_lr: 0.000001  loss: 4.3427 (4.1682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0104  max mem: 15572
Epoch: [7]  [  60/1404]  eta: 0:16:00  lr: 0.000093  min_lr: 0.000001  loss: 4.2962 (4.1708)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.0105  max mem: 15572
Epoch: [7]  [  70/1404]  eta: 0:15:30  lr: 0.000093  min_lr: 0.000001  loss: 4.0057 (4.1450)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0112  max mem: 15572
Epoch: [7]  [  80/1404]  eta: 0:15:08  lr: 0.000093  min_lr: 0.000001  loss: 4.1052 (4.1666)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0111  max mem: 15572
Epoch: [7]  [  90/1404]  eta: 0:14:56  lr: 0.000093  min_lr: 0.000001  loss: 4.1422 (4.1659)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0185  max mem: 15572
Epoch: [7]  [ 100/1404]  eta: 0:14:46  lr: 0.000093  min_lr: 0.000001  loss: 4.1482 (4.1802)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.0848  max mem: 15572
[2025-01-10 17:26:00,202] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:26:00,203] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:26:00,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:26:00,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 110/1404]  eta: 0:14:25  lr: 0.000093  min_lr: 0.000001  loss: 4.1956 (4.1822)  loss_scale: 32768.0000 (34834.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0900  max mem: 15572
Epoch: [7]  [ 120/1404]  eta: 0:14:09  lr: 0.000093  min_lr: 0.000001  loss: 4.2946 (4.1924)  loss_scale: 65536.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0304  max mem: 15572
Epoch: [7]  [ 130/1404]  eta: 0:14:02  lr: 0.000093  min_lr: 0.000001  loss: 4.3047 (4.1813)  loss_scale: 65536.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0171  max mem: 15572
Epoch: [7]  [ 140/1404]  eta: 0:13:53  lr: 0.000093  min_lr: 0.000001  loss: 4.2223 (4.1878)  loss_scale: 65536.0000 (41366.6950)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.0216  max mem: 15572
Epoch: [7]  [ 150/1404]  eta: 0:13:40  lr: 0.000093  min_lr: 0.000001  loss: 4.2223 (4.1811)  loss_scale: 65536.0000 (42967.3113)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.0371  max mem: 15572
Epoch: [7]  [ 160/1404]  eta: 0:13:31  lr: 0.000093  min_lr: 0.000001  loss: 4.1843 (4.1851)  loss_scale: 65536.0000 (44369.0932)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0699  max mem: 15572
Epoch: [7]  [ 170/1404]  eta: 0:13:20  lr: 0.000093  min_lr: 0.000001  loss: 4.1683 (4.1741)  loss_scale: 65536.0000 (45606.9240)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0719  max mem: 15572
[2025-01-10 17:26:40,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=53, lr=[9.001307943001361e-07, 9.001307943001361e-07, 1.2859011347144805e-06, 1.2859011347144805e-06, 1.8370016210206865e-06, 1.8370016210206865e-06, 2.624288030029552e-06, 2.624288030029552e-06, 3.7489829000422176e-06, 3.7489829000422176e-06, 5.355689857203168e-06, 5.355689857203168e-06, 7.650985510290241e-06, 7.650985510290241e-06, 1.092997930041463e-05, 1.092997930041463e-05, 1.561425614344947e-05, 1.561425614344947e-05, 2.230608020492782e-05, 2.230608020492782e-05, 3.18658288641826e-05, 3.18658288641826e-05, 4.552261266311801e-05, 4.552261266311801e-05, 6.50323038044543e-05, 6.50323038044543e-05, 9.290329114922043e-05, 9.290329114922043e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:26:40,634] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=45.24509919125445, CurrSamplesPerSec=50.23497079869791, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 17:26:44,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10003
[2025-01-10 17:26:44,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:26:44,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10003
[2025-01-10 17:26:44,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:26:44,138] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 180/1404]  eta: 0:13:14  lr: 0.000093  min_lr: 0.000001  loss: 4.1113 (4.1718)  loss_scale: 65536.0000 (45621.7459)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1036  max mem: 15572
Epoch: [7]  [ 190/1404]  eta: 0:13:08  lr: 0.000093  min_lr: 0.000001  loss: 4.2266 (4.1812)  loss_scale: 32768.0000 (44948.7749)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.1632  max mem: 15572
Epoch: [7]  [ 200/1404]  eta: 0:13:02  lr: 0.000093  min_lr: 0.000001  loss: 4.2745 (4.1864)  loss_scale: 32768.0000 (44342.7662)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.1507  max mem: 15572
Epoch: [7]  [ 210/1404]  eta: 0:12:50  lr: 0.000093  min_lr: 0.000001  loss: 4.3424 (4.1903)  loss_scale: 32768.0000 (43794.1991)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0637  max mem: 15572
Epoch: [7]  [ 220/1404]  eta: 0:12:42  lr: 0.000093  min_lr: 0.000001  loss: 4.3097 (4.1844)  loss_scale: 32768.0000 (43295.2760)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0576  max mem: 15572
Epoch: [7]  [ 230/1404]  eta: 0:12:36  lr: 0.000093  min_lr: 0.000001  loss: 4.2680 (4.1914)  loss_scale: 32768.0000 (42839.5498)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1044  max mem: 15572
Epoch: [7]  [ 240/1404]  eta: 0:12:26  lr: 0.000093  min_lr: 0.000001  loss: 4.3111 (4.1913)  loss_scale: 32768.0000 (42421.6432)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0931  max mem: 15572
Epoch: [7]  [ 250/1404]  eta: 0:12:19  lr: 0.000093  min_lr: 0.000001  loss: 4.2060 (4.1916)  loss_scale: 32768.0000 (42037.0359)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1042  max mem: 15572
Epoch: [7]  [ 260/1404]  eta: 0:12:12  lr: 0.000093  min_lr: 0.000001  loss: 4.1554 (4.1886)  loss_scale: 32768.0000 (41681.9004)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1182  max mem: 15572
Epoch: [7]  [ 270/1404]  eta: 0:12:03  lr: 0.000093  min_lr: 0.000001  loss: 4.0922 (4.1890)  loss_scale: 32768.0000 (41352.9742)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0986  max mem: 15572
Epoch: [7]  [ 280/1404]  eta: 0:11:55  lr: 0.000093  min_lr: 0.000001  loss: 4.1294 (4.1875)  loss_scale: 32768.0000 (41047.4591)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0846  max mem: 15572
Epoch: [7]  [ 290/1404]  eta: 0:11:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2765 (4.1886)  loss_scale: 32768.0000 (40762.9416)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1342  max mem: 15572
Epoch: [7]  [ 300/1404]  eta: 0:11:45  lr: 0.000093  min_lr: 0.000001  loss: 4.2965 (4.1889)  loss_scale: 32768.0000 (40497.3289)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.1402  max mem: 15572
[2025-01-10 17:28:03,538] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:28:03,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:28:03,538] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:28:03,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 310/1404]  eta: 0:11:38  lr: 0.000093  min_lr: 0.000001  loss: 4.3338 (4.1907)  loss_scale: 32768.0000 (40986.3408)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1029  max mem: 15572
Epoch: [7]  [ 320/1404]  eta: 0:11:34  lr: 0.000093  min_lr: 0.000001  loss: 4.2253 (4.1875)  loss_scale: 65536.0000 (41751.1277)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.1495  max mem: 15572
Epoch: [7]  [ 330/1404]  eta: 0:11:23  lr: 0.000093  min_lr: 0.000001  loss: 4.1295 (4.1838)  loss_scale: 65536.0000 (42469.7039)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0989  max mem: 15572
Epoch: [7]  [ 340/1404]  eta: 0:11:13  lr: 0.000093  min_lr: 0.000001  loss: 4.2125 (4.1864)  loss_scale: 65536.0000 (43146.1349)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0013  max mem: 15572
Epoch: [7]  [ 350/1404]  eta: 0:11:06  lr: 0.000093  min_lr: 0.000001  loss: 4.2980 (4.1882)  loss_scale: 65536.0000 (43784.0228)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0442  max mem: 15572
[2025-01-10 17:28:31,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10179
[2025-01-10 17:28:31,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:28:31,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:28:31,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10179
[2025-01-10 17:28:31,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 360/1404]  eta: 0:10:59  lr: 0.000093  min_lr: 0.000001  loss: 4.2009 (4.1861)  loss_scale: 32768.0000 (43478.8698)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1051  max mem: 15572
Epoch: [7]  [ 370/1404]  eta: 0:10:53  lr: 0.000093  min_lr: 0.000001  loss: 4.3258 (4.1944)  loss_scale: 32768.0000 (43190.1671)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1299  max mem: 15572
Epoch: [7]  [ 380/1404]  eta: 0:10:47  lr: 0.000093  min_lr: 0.000001  loss: 4.3601 (4.1957)  loss_scale: 32768.0000 (42916.6194)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1330  max mem: 15572
Epoch: [7]  [ 390/1404]  eta: 0:10:38  lr: 0.000093  min_lr: 0.000001  loss: 4.1343 (4.1944)  loss_scale: 32768.0000 (42657.0639)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0965  max mem: 15572
Epoch: [7]  [ 400/1404]  eta: 0:10:31  lr: 0.000093  min_lr: 0.000001  loss: 4.1029 (4.1899)  loss_scale: 32768.0000 (42410.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0999  max mem: 15572
Epoch: [7]  [ 410/1404]  eta: 0:10:24  lr: 0.000093  min_lr: 0.000001  loss: 3.8788 (4.1864)  loss_scale: 32768.0000 (42175.8443)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1018  max mem: 15572
Epoch: [7]  [ 420/1404]  eta: 0:10:17  lr: 0.000093  min_lr: 0.000001  loss: 4.2668 (4.1887)  loss_scale: 32768.0000 (41952.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0888  max mem: 15572
Epoch: [7]  [ 430/1404]  eta: 0:10:08  lr: 0.000093  min_lr: 0.000001  loss: 4.3688 (4.1941)  loss_scale: 32768.0000 (41739.2854)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0671  max mem: 15572
Epoch: [7]  [ 440/1404]  eta: 0:10:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3369 (4.1952)  loss_scale: 32768.0000 (41535.8549)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0962  max mem: 15572
Epoch: [7]  [ 450/1404]  eta: 0:09:57  lr: 0.000093  min_lr: 0.000001  loss: 4.2096 (4.1938)  loss_scale: 32768.0000 (41341.4457)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.1139  max mem: 15572
Epoch: [7]  [ 460/1404]  eta: 0:09:49  lr: 0.000093  min_lr: 0.000001  loss: 4.2790 (4.1966)  loss_scale: 32768.0000 (41155.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0430  max mem: 15572
Epoch: [7]  [ 470/1404]  eta: 0:09:41  lr: 0.000093  min_lr: 0.000001  loss: 4.2790 (4.1958)  loss_scale: 32768.0000 (40977.3928)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0139  max mem: 15572
[2025-01-10 17:29:48,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:29:48,545] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:29:48,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:29:48,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 480/1404]  eta: 0:09:34  lr: 0.000093  min_lr: 0.000001  loss: 4.2710 (4.1970)  loss_scale: 32768.0000 (40874.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0131  max mem: 15572
Epoch: [7]  [ 490/1404]  eta: 0:09:29  lr: 0.000093  min_lr: 0.000001  loss: 4.0478 (4.1881)  loss_scale: 65536.0000 (41377.1079)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0330  max mem: 15572
Epoch: [7]  [ 500/1404]  eta: 0:09:24  lr: 0.000093  min_lr: 0.000001  loss: 3.9549 (4.1870)  loss_scale: 65536.0000 (41859.3214)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.0208  max mem: 15572
Epoch: [7]  [ 510/1404]  eta: 0:09:18  lr: 0.000093  min_lr: 0.000001  loss: 4.2171 (4.1882)  loss_scale: 65536.0000 (42322.6614)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.0419  max mem: 15572
Epoch: [7]  [ 520/1404]  eta: 0:09:11  lr: 0.000093  min_lr: 0.000001  loss: 4.3455 (4.1920)  loss_scale: 65536.0000 (42768.2150)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0877  max mem: 15572
Epoch: [7]  [ 530/1404]  eta: 0:09:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3328 (4.1917)  loss_scale: 65536.0000 (43196.9868)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0886  max mem: 15572
Epoch: [7]  [ 540/1404]  eta: 0:08:58  lr: 0.000093  min_lr: 0.000001  loss: 4.2093 (4.1903)  loss_scale: 65536.0000 (43609.9076)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0585  max mem: 15572
Epoch: [7]  [ 550/1404]  eta: 0:08:51  lr: 0.000093  min_lr: 0.000001  loss: 3.9680 (4.1834)  loss_scale: 65536.0000 (44007.8403)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0208  max mem: 15572
Epoch: [7]  [ 560/1404]  eta: 0:08:45  lr: 0.000093  min_lr: 0.000001  loss: 4.0772 (4.1850)  loss_scale: 65536.0000 (44391.5865)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0756  max mem: 15572
Epoch: [7]  [ 570/1404]  eta: 0:08:38  lr: 0.000093  min_lr: 0.000001  loss: 4.1431 (4.1831)  loss_scale: 65536.0000 (44761.8914)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1023  max mem: 15572
Epoch: [7]  [ 580/1404]  eta: 0:08:32  lr: 0.000093  min_lr: 0.000001  loss: 3.9821 (4.1785)  loss_scale: 65536.0000 (45119.4492)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1242  max mem: 15572
Epoch: [7]  [ 590/1404]  eta: 0:08:26  lr: 0.000093  min_lr: 0.000001  loss: 3.9674 (4.1770)  loss_scale: 65536.0000 (45464.9069)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1598  max mem: 15572
Epoch: [7]  [ 600/1404]  eta: 0:08:20  lr: 0.000093  min_lr: 0.000001  loss: 4.0359 (4.1775)  loss_scale: 65536.0000 (45798.8686)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1454  max mem: 15572
[2025-01-10 17:31:08,143] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:31:08,143] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:31:08,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:31:08,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-10 17:31:08,651] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10437
[2025-01-10 17:31:08,652] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:31:08,667] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10437
[2025-01-10 17:31:08,668] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-10 17:31:08,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 610/1404]  eta: 0:08:13  lr: 0.000093  min_lr: 0.000001  loss: 4.0840 (4.1763)  loss_scale: 65536.0000 (46229.1588)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.0897  max mem: 15572
[2025-01-10 17:31:13,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10445
[2025-01-10 17:31:13,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:31:13,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10445
[2025-01-10 17:31:13,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:31:13,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 620/1404]  eta: 0:08:07  lr: 0.000093  min_lr: 0.000001  loss: 4.1739 (4.1797)  loss_scale: 65536.0000 (46328.9919)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0325  max mem: 15572
Epoch: [7]  [ 630/1404]  eta: 0:08:00  lr: 0.000093  min_lr: 0.000001  loss: 4.2546 (4.1816)  loss_scale: 32768.0000 (46114.0792)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0218  max mem: 15572
Epoch: [7]  [ 640/1404]  eta: 0:07:54  lr: 0.000093  min_lr: 0.000001  loss: 4.2100 (4.1796)  loss_scale: 32768.0000 (45905.8721)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0011  max mem: 15572
Epoch: [7]  [ 650/1404]  eta: 0:07:48  lr: 0.000093  min_lr: 0.000001  loss: 4.0431 (4.1802)  loss_scale: 32768.0000 (45704.0614)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0014  max mem: 15572
Epoch: [7]  [ 660/1404]  eta: 0:07:42  lr: 0.000093  min_lr: 0.000001  loss: 4.0431 (4.1785)  loss_scale: 32768.0000 (45508.3570)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0019  max mem: 15572
Epoch: [7]  [ 670/1404]  eta: 0:07:36  lr: 0.000093  min_lr: 0.000001  loss: 3.9767 (4.1770)  loss_scale: 32768.0000 (45318.4858)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.0018  max mem: 15572
Epoch: [7]  [ 680/1404]  eta: 0:07:30  lr: 0.000093  min_lr: 0.000001  loss: 4.3254 (4.1792)  loss_scale: 32768.0000 (45134.1909)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0014  max mem: 15572
Epoch: [7]  [ 690/1404]  eta: 0:07:24  lr: 0.000093  min_lr: 0.000001  loss: 4.2819 (4.1794)  loss_scale: 32768.0000 (44955.2301)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.0016  max mem: 15572
Epoch: [7]  [ 700/1404]  eta: 0:07:17  lr: 0.000093  min_lr: 0.000001  loss: 4.2610 (4.1804)  loss_scale: 32768.0000 (44781.3752)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0013  max mem: 15572
Epoch: [7]  [ 710/1404]  eta: 0:07:12  lr: 0.000093  min_lr: 0.000001  loss: 4.1615 (4.1784)  loss_scale: 32768.0000 (44612.4107)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0007  max mem: 15572
Epoch: [7]  [ 720/1404]  eta: 0:07:05  lr: 0.000093  min_lr: 0.000001  loss: 4.0722 (4.1779)  loss_scale: 32768.0000 (44448.1331)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0007  max mem: 15572
Epoch: [7]  [ 730/1404]  eta: 0:06:59  lr: 0.000093  min_lr: 0.000001  loss: 4.2339 (4.1767)  loss_scale: 32768.0000 (44288.3502)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0010  max mem: 15572
Epoch: [7]  [ 740/1404]  eta: 0:06:52  lr: 0.000093  min_lr: 0.000001  loss: 4.0116 (4.1735)  loss_scale: 32768.0000 (44132.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0011  max mem: 15572
[2025-01-10 17:32:34,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:32:34,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:32:34,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:32:34,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 750/1404]  eta: 0:06:46  lr: 0.000093  min_lr: 0.000001  loss: 4.1452 (4.1751)  loss_scale: 32768.0000 (44199.7124)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0009  max mem: 15572
[2025-01-10 17:32:36,830] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10579
[2025-01-10 17:32:36,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:32:36,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:32:36,868] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10579
[2025-01-10 17:32:36,868] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 760/1404]  eta: 0:06:39  lr: 0.000093  min_lr: 0.000001  loss: 4.2021 (4.1754)  loss_scale: 32768.0000 (44049.4928)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0006  max mem: 15572
Epoch: [7]  [ 770/1404]  eta: 0:06:33  lr: 0.000093  min_lr: 0.000001  loss: 4.2094 (4.1745)  loss_scale: 32768.0000 (43903.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0005  max mem: 15572
Epoch: [7]  [ 780/1404]  eta: 0:06:27  lr: 0.000093  min_lr: 0.000001  loss: 4.2557 (4.1781)  loss_scale: 32768.0000 (43760.5941)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0005  max mem: 15572
Epoch: [7]  [ 790/1404]  eta: 0:06:21  lr: 0.000093  min_lr: 0.000001  loss: 4.2757 (4.1766)  loss_scale: 32768.0000 (43621.6233)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0006  max mem: 15572
Epoch: [7]  [ 800/1404]  eta: 0:06:15  lr: 0.000093  min_lr: 0.000001  loss: 4.1175 (4.1770)  loss_scale: 32768.0000 (43486.1223)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.0009  max mem: 15572
Epoch: [7]  [ 810/1404]  eta: 0:06:08  lr: 0.000093  min_lr: 0.000001  loss: 4.2385 (4.1788)  loss_scale: 32768.0000 (43353.9630)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0011  max mem: 15572
Epoch: [7]  [ 820/1404]  eta: 0:06:02  lr: 0.000092  min_lr: 0.000001  loss: 4.2385 (4.1761)  loss_scale: 32768.0000 (43225.0231)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0009  max mem: 15572
Epoch: [7]  [ 830/1404]  eta: 0:05:55  lr: 0.000092  min_lr: 0.000001  loss: 3.8581 (4.1727)  loss_scale: 32768.0000 (43099.1865)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0007  max mem: 15572
Epoch: [7]  [ 840/1404]  eta: 0:05:49  lr: 0.000092  min_lr: 0.000001  loss: 4.0542 (4.1710)  loss_scale: 32768.0000 (42976.3424)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0035  max mem: 15572
Epoch: [7]  [ 850/1404]  eta: 0:05:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1190 (4.1706)  loss_scale: 32768.0000 (42856.3854)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.0035  max mem: 15572
Epoch: [7]  [ 860/1404]  eta: 0:05:36  lr: 0.000092  min_lr: 0.000001  loss: 4.1895 (4.1714)  loss_scale: 32768.0000 (42739.2149)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0009  max mem: 15572
Epoch: [7]  [ 870/1404]  eta: 0:05:31  lr: 0.000092  min_lr: 0.000001  loss: 4.1895 (4.1715)  loss_scale: 32768.0000 (42624.7348)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0010  max mem: 15572
[2025-01-10 17:33:55,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:33:55,568] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:33:55,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:33:55,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 880/1404]  eta: 0:05:24  lr: 0.000092  min_lr: 0.000001  loss: 4.0628 (4.1715)  loss_scale: 32768.0000 (42550.0477)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.0008  max mem: 15572
Epoch: [7]  [ 890/1404]  eta: 0:05:18  lr: 0.000092  min_lr: 0.000001  loss: 4.0536 (4.1709)  loss_scale: 65536.0000 (42808.0269)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0006  max mem: 15572
[2025-01-10 17:34:02,878] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10721
[2025-01-10 17:34:02,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:34:02,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:34:02,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10721
[2025-01-10 17:34:02,895] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 900/1404]  eta: 0:05:12  lr: 0.000092  min_lr: 0.000001  loss: 4.1632 (4.1721)  loss_scale: 65536.0000 (42769.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0007  max mem: 15572
Epoch: [7]  [ 910/1404]  eta: 0:05:05  lr: 0.000092  min_lr: 0.000001  loss: 4.1834 (4.1707)  loss_scale: 32768.0000 (42659.5477)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0008  max mem: 15572
Epoch: [7]  [ 920/1404]  eta: 0:04:59  lr: 0.000092  min_lr: 0.000001  loss: 4.2479 (4.1715)  loss_scale: 32768.0000 (42552.1477)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0007  max mem: 15572
Epoch: [7]  [ 930/1404]  eta: 0:04:53  lr: 0.000092  min_lr: 0.000001  loss: 4.2630 (4.1713)  loss_scale: 32768.0000 (42447.0548)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0006  max mem: 15572
Epoch: [7]  [ 940/1404]  eta: 0:04:46  lr: 0.000092  min_lr: 0.000001  loss: 4.1096 (4.1696)  loss_scale: 32768.0000 (42344.1955)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0008  max mem: 15572
Epoch: [7]  [ 950/1404]  eta: 0:04:40  lr: 0.000092  min_lr: 0.000001  loss: 4.1050 (4.1694)  loss_scale: 32768.0000 (42243.4995)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0008  max mem: 15572
Epoch: [7]  [ 960/1404]  eta: 0:04:34  lr: 0.000092  min_lr: 0.000001  loss: 4.1050 (4.1682)  loss_scale: 32768.0000 (42144.8991)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0007  max mem: 15572
Epoch: [7]  [ 970/1404]  eta: 0:04:28  lr: 0.000092  min_lr: 0.000001  loss: 4.0083 (4.1669)  loss_scale: 32768.0000 (42048.3296)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0007  max mem: 15572
Epoch: [7]  [ 980/1404]  eta: 0:04:21  lr: 0.000092  min_lr: 0.000001  loss: 4.1103 (4.1672)  loss_scale: 32768.0000 (41953.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0009  max mem: 15572
Epoch: [7]  [ 990/1404]  eta: 0:04:15  lr: 0.000092  min_lr: 0.000001  loss: 4.2883 (4.1669)  loss_scale: 32768.0000 (41861.0373)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0009  max mem: 15572
Epoch: [7]  [1000/1404]  eta: 0:04:09  lr: 0.000092  min_lr: 0.000001  loss: 4.0618 (4.1646)  loss_scale: 32768.0000 (41770.1978)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0010  max mem: 15572
Epoch: [7]  [1010/1404]  eta: 0:04:03  lr: 0.000092  min_lr: 0.000001  loss: 4.1172 (4.1662)  loss_scale: 32768.0000 (41681.1553)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.0012  max mem: 15572
Epoch: [7]  [1020/1404]  eta: 0:03:57  lr: 0.000092  min_lr: 0.000001  loss: 4.2978 (4.1660)  loss_scale: 32768.0000 (41593.8570)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.0013  max mem: 15572
[2025-01-10 17:35:21,822] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:35:21,823] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:35:21,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:35:21,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1030/1404]  eta: 0:03:50  lr: 0.000092  min_lr: 0.000001  loss: 4.2792 (4.1661)  loss_scale: 32768.0000 (41794.2968)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0009  max mem: 15572
Epoch: [7]  [1040/1404]  eta: 0:03:44  lr: 0.000092  min_lr: 0.000001  loss: 4.2930 (4.1674)  loss_scale: 65536.0000 (42022.3631)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0010  max mem: 15572
Epoch: [7]  [1050/1404]  eta: 0:03:38  lr: 0.000092  min_lr: 0.000001  loss: 4.2718 (4.1679)  loss_scale: 65536.0000 (42246.0894)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0323  max mem: 15572
Epoch: [7]  [1060/1404]  eta: 0:03:32  lr: 0.000092  min_lr: 0.000001  loss: 4.2563 (4.1669)  loss_scale: 65536.0000 (42465.5985)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0490  max mem: 15572
[2025-01-10 17:35:45,013] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10890
[2025-01-10 17:35:45,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:35:45,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10890
[2025-01-10 17:35:45,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:35:45,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1070/1404]  eta: 0:03:26  lr: 0.000092  min_lr: 0.000001  loss: 3.9744 (4.1664)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0805  max mem: 15572
Epoch: [7]  [1080/1404]  eta: 0:03:20  lr: 0.000092  min_lr: 0.000001  loss: 4.1946 (4.1671)  loss_scale: 32768.0000 (42316.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6682  data: 0.0635  max mem: 15572
Epoch: [7]  [1090/1404]  eta: 0:03:13  lr: 0.000092  min_lr: 0.000001  loss: 4.0918 (4.1653)  loss_scale: 32768.0000 (42228.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0035  max mem: 15572
Epoch: [7]  [1100/1404]  eta: 0:03:07  lr: 0.000092  min_lr: 0.000001  loss: 4.0530 (4.1658)  loss_scale: 32768.0000 (42143.0409)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0036  max mem: 15572
Epoch: [7]  [1110/1404]  eta: 0:03:01  lr: 0.000092  min_lr: 0.000001  loss: 4.0817 (4.1640)  loss_scale: 32768.0000 (42058.6571)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0007  max mem: 15572
Epoch: [7]  [1120/1404]  eta: 0:02:55  lr: 0.000092  min_lr: 0.000001  loss: 4.0088 (4.1635)  loss_scale: 32768.0000 (41975.7788)  weight_decay: 0.0500 (0.0500)  time: 0.6799  data: 0.0004  max mem: 15572
Epoch: [7]  [1130/1404]  eta: 0:02:49  lr: 0.000092  min_lr: 0.000001  loss: 3.9858 (4.1615)  loss_scale: 32768.0000 (41894.3660)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.0004  max mem: 15572
Epoch: [7]  [1140/1404]  eta: 0:02:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1075 (4.1628)  loss_scale: 32768.0000 (41814.3804)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0005  max mem: 15572
Epoch: [7]  [1150/1404]  eta: 0:02:37  lr: 0.000092  min_lr: 0.000001  loss: 4.3313 (4.1634)  loss_scale: 32768.0000 (41735.7845)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0006  max mem: 15572
Epoch: [7]  [1160/1404]  eta: 0:02:30  lr: 0.000092  min_lr: 0.000001  loss: 4.1610 (4.1615)  loss_scale: 32768.0000 (41658.5426)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0006  max mem: 15572
Epoch: [7]  [1170/1404]  eta: 0:02:24  lr: 0.000092  min_lr: 0.000001  loss: 4.1408 (4.1613)  loss_scale: 32768.0000 (41582.6200)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0009  max mem: 15572
[2025-01-10 17:36:53,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=60, lr=[8.93733379008233e-07, 8.93733379008233e-07, 1.2767619700117616e-06, 1.2767619700117616e-06, 1.823945671445374e-06, 1.823945671445374e-06, 2.6056366734933912e-06, 2.6056366734933912e-06, 3.7223381049905594e-06, 3.7223381049905594e-06, 5.317625864272228e-06, 5.317625864272228e-06, 7.596608377531754e-06, 7.596608377531754e-06, 1.0852297682188223e-05, 1.0852297682188223e-05, 1.550328240312603e-05, 1.550328240312603e-05, 2.2147546290180047e-05, 2.2147546290180047e-05, 3.163935184311435e-05, 3.163935184311435e-05, 4.5199074061591934e-05, 4.5199074061591934e-05, 6.45701058022742e-05, 6.45701058022742e-05, 9.224300828896315e-05, 9.224300828896315e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:36:53,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=45.160132469657476, CurrSamplesPerSec=51.814261272038095, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1180/1404]  eta: 0:02:18  lr: 0.000092  min_lr: 0.000001  loss: 4.2644 (4.1639)  loss_scale: 32768.0000 (41507.9831)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0010  max mem: 15572
Epoch: [7]  [1190/1404]  eta: 0:02:12  lr: 0.000092  min_lr: 0.000001  loss: 4.3632 (4.1652)  loss_scale: 32768.0000 (41434.5995)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0009  max mem: 15572
[2025-01-10 17:37:05,554] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:37:05,554] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:37:05,567] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:37:05,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:37:08,967] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11025
[2025-01-10 17:37:08,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:37:09,074] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11025
[2025-01-10 17:37:09,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:37:09,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1200/1404]  eta: 0:02:05  lr: 0.000092  min_lr: 0.000001  loss: 4.2203 (4.1649)  loss_scale: 32768.0000 (41526.1415)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0010  max mem: 15572
Epoch: [7]  [1210/1404]  eta: 0:01:59  lr: 0.000092  min_lr: 0.000001  loss: 4.1739 (4.1632)  loss_scale: 32768.0000 (41453.8200)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0008  max mem: 15572
Epoch: [7]  [1220/1404]  eta: 0:01:53  lr: 0.000092  min_lr: 0.000001  loss: 4.1437 (4.1646)  loss_scale: 32768.0000 (41382.6830)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0008  max mem: 15572
Epoch: [7]  [1230/1404]  eta: 0:01:47  lr: 0.000092  min_lr: 0.000001  loss: 4.1548 (4.1640)  loss_scale: 32768.0000 (41312.7019)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0007  max mem: 15572
Epoch: [7]  [1240/1404]  eta: 0:01:41  lr: 0.000092  min_lr: 0.000001  loss: 4.0655 (4.1647)  loss_scale: 32768.0000 (41243.8485)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0009  max mem: 15572
Epoch: [7]  [1250/1404]  eta: 0:01:34  lr: 0.000092  min_lr: 0.000001  loss: 4.0229 (4.1633)  loss_scale: 32768.0000 (41176.0959)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.0011  max mem: 15572
Epoch: [7]  [1260/1404]  eta: 0:01:28  lr: 0.000092  min_lr: 0.000001  loss: 4.0196 (4.1632)  loss_scale: 32768.0000 (41109.4179)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0010  max mem: 15572
Epoch: [7]  [1270/1404]  eta: 0:01:22  lr: 0.000092  min_lr: 0.000001  loss: 4.2381 (4.1647)  loss_scale: 32768.0000 (41043.7891)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0010  max mem: 15572
Epoch: [7]  [1280/1404]  eta: 0:01:16  lr: 0.000092  min_lr: 0.000001  loss: 4.1354 (4.1629)  loss_scale: 32768.0000 (40979.1850)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0008  max mem: 15572
Epoch: [7]  [1290/1404]  eta: 0:01:10  lr: 0.000092  min_lr: 0.000001  loss: 3.9392 (4.1624)  loss_scale: 32768.0000 (40915.5817)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0009  max mem: 15572
Epoch: [7]  [1300/1404]  eta: 0:01:03  lr: 0.000092  min_lr: 0.000001  loss: 4.0948 (4.1627)  loss_scale: 32768.0000 (40852.9562)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0191  max mem: 15572
Epoch: [7]  [1310/1404]  eta: 0:00:57  lr: 0.000092  min_lr: 0.000001  loss: 4.0948 (4.1628)  loss_scale: 32768.0000 (40791.2860)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0446  max mem: 15572
Epoch: [7]  [1320/1404]  eta: 0:00:51  lr: 0.000092  min_lr: 0.000001  loss: 4.1280 (4.1632)  loss_scale: 32768.0000 (40730.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0266  max mem: 15572
[2025-01-10 17:38:25,263] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:38:25,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:38:25,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:38:25,327] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1330/1404]  eta: 0:00:45  lr: 0.000092  min_lr: 0.000001  loss: 4.1280 (4.1630)  loss_scale: 32768.0000 (40793.8212)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0006  max mem: 15572
[2025-01-10 17:38:27,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11159
[2025-01-10 17:38:27,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11159
[2025-01-10 17:38:27,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:38:27,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:38:27,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1340/1404]  eta: 0:00:39  lr: 0.000092  min_lr: 0.000001  loss: 4.1444 (4.1624)  loss_scale: 32768.0000 (40733.9717)  weight_decay: 0.0500 (0.0500)  time: 0.7187  data: 0.0005  max mem: 15572
Epoch: [7]  [1350/1404]  eta: 0:00:33  lr: 0.000092  min_lr: 0.000001  loss: 4.0988 (4.1618)  loss_scale: 32768.0000 (40675.0081)  weight_decay: 0.0500 (0.0500)  time: 0.7121  data: 0.0006  max mem: 15572
Epoch: [7]  [1360/1404]  eta: 0:00:27  lr: 0.000092  min_lr: 0.000001  loss: 4.0408 (4.1611)  loss_scale: 32768.0000 (40616.9111)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.0007  max mem: 15572
Epoch: [7]  [1370/1404]  eta: 0:00:20  lr: 0.000092  min_lr: 0.000001  loss: 4.1860 (4.1612)  loss_scale: 32768.0000 (40559.6616)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0010  max mem: 15572
Epoch: [7]  [1380/1404]  eta: 0:00:14  lr: 0.000092  min_lr: 0.000001  loss: 4.2006 (4.1620)  loss_scale: 32768.0000 (40503.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0973  max mem: 15572
Epoch: [7]  [1390/1404]  eta: 0:00:08  lr: 0.000092  min_lr: 0.000001  loss: 4.2785 (4.1624)  loss_scale: 32768.0000 (40447.6319)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1334  max mem: 15572
Epoch: [7]  [1400/1404]  eta: 0:00:02  lr: 0.000092  min_lr: 0.000001  loss: 4.1772 (4.1611)  loss_scale: 32768.0000 (40392.8166)  weight_decay: 0.0500 (0.0500)  time: 0.4772  data: 0.0368  max mem: 15572
Epoch: [7]  [1403/1404]  eta: 0:00:00  lr: 0.000092  min_lr: 0.000001  loss: 4.1641 (4.1617)  loss_scale: 32768.0000 (40376.5242)  weight_decay: 0.0500 (0.0500)  time: 0.4611  data: 0.0368  max mem: 15572
Epoch: [7] Total time: 0:14:21 (0.6137 s / it)
Averaged stats: lr: 0.000092  min_lr: 0.000001  loss: 4.1641 (4.1650)  loss_scale: 32768.0000 (40376.5242)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:03  loss: 1.8116 (1.8116)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.8771  data: 4.5516  max mem: 15572
Val:  [ 10/136]  eta: 0:01:46  loss: 3.1371 (3.1433)  acc1: 16.6667 (24.7475)  acc5: 61.1111 (52.0202)  time: 0.8466  data: 0.6349  max mem: 15572
Val:  [ 20/136]  eta: 0:01:11  loss: 3.1371 (3.2081)  acc1: 16.6667 (21.1640)  acc5: 61.1111 (54.7619)  time: 0.4054  data: 0.2001  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 3.0240 (2.9664)  acc1: 22.2222 (29.5699)  acc5: 66.6667 (59.8566)  time: 0.3044  data: 0.0879  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.1994 (2.8704)  acc1: 44.4444 (32.1138)  acc5: 77.7778 (63.2791)  time: 0.3015  data: 0.0853  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.8799 (2.9725)  acc1: 22.2222 (30.0654)  acc5: 66.6667 (61.5468)  time: 0.3977  data: 0.1945  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 3.4599 (3.0789)  acc1: 11.1111 (26.8670)  acc5: 50.0000 (58.6521)  time: 0.4252  data: 0.2179  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 3.2677 (3.0234)  acc1: 16.6667 (29.5775)  acc5: 55.5556 (59.1549)  time: 0.4159  data: 0.2077  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.7242 (2.9945)  acc1: 44.4444 (30.2469)  acc5: 72.2222 (60.9739)  time: 0.3989  data: 0.1892  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.9385 (3.0105)  acc1: 27.7778 (28.9377)  acc5: 72.2222 (61.1722)  time: 0.3430  data: 0.1291  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 3.1923 (3.0736)  acc1: 5.5556 (27.3377)  acc5: 55.5556 (59.6810)  time: 0.3498  data: 0.1463  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.1605 (3.0685)  acc1: 16.6667 (27.8278)  acc5: 55.5556 (59.7598)  time: 0.4078  data: 0.2110  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.7464 (2.9998)  acc1: 38.8889 (30.1653)  acc5: 72.2222 (61.5243)  time: 0.3884  data: 0.1954  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4826 (2.9448)  acc1: 50.0000 (31.9338)  acc5: 77.7778 (62.0865)  time: 0.2495  data: 0.0824  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.5585 (2.9554)  acc1: 44.4444 (31.6134)  acc5: 61.1111 (61.8346)  time: 0.2083  data: 0.0629  max mem: 15572
Val: Total time: 0:00:52 (0.3863 s / it)
* Acc@1 31.347 Acc@5 60.995 loss 2.988
Accuracy of the network on the 4883 val videos: 31.3%
[2025-01-10 17:40:03,326] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 17:40:03,328] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 17:40:03,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 17:40:03,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 17:40:05,803] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 17:40:05,803] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 31.35%
Epoch: [8]  [   0/1404]  eta: 3:14:54  lr: 0.000092  min_lr: 0.000001  loss: 4.9896 (4.9896)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3297  data: 6.3053  max mem: 15572
Epoch: [8]  [  10/1404]  eta: 0:30:50  lr: 0.000092  min_lr: 0.000001  loss: 4.1766 (4.1485)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3276  data: 0.5741  max mem: 15572
Epoch: [8]  [  20/1404]  eta: 0:22:25  lr: 0.000092  min_lr: 0.000001  loss: 4.1766 (4.1280)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0009  max mem: 15572
Epoch: [8]  [  30/1404]  eta: 0:19:57  lr: 0.000092  min_lr: 0.000001  loss: 4.1689 (4.1350)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0007  max mem: 15572
Epoch: [8]  [  40/1404]  eta: 0:18:06  lr: 0.000092  min_lr: 0.000001  loss: 4.1689 (4.1516)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0005  max mem: 15572
Epoch: [8]  [  50/1404]  eta: 0:17:12  lr: 0.000092  min_lr: 0.000001  loss: 4.0567 (4.1441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0007  max mem: 15572
[2025-01-10 17:40:47,526] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:40:47,527] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:40:47,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:40:47,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:40:48,634] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11290
[2025-01-10 17:40:48,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:40:48,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11290
[2025-01-10 17:40:48,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:40:48,635] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [  60/1404]  eta: 0:16:21  lr: 0.000092  min_lr: 0.000001  loss: 4.0049 (4.1174)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0010  max mem: 15572
Epoch: [8]  [  70/1404]  eta: 0:15:33  lr: 0.000092  min_lr: 0.000001  loss: 4.1178 (4.1290)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0008  max mem: 15572
Epoch: [8]  [  80/1404]  eta: 0:15:24  lr: 0.000092  min_lr: 0.000001  loss: 4.1965 (4.1544)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0006  max mem: 15572
Epoch: [8]  [  90/1404]  eta: 0:15:15  lr: 0.000092  min_lr: 0.000001  loss: 4.1346 (4.1374)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.0008  max mem: 15572
Epoch: [8]  [ 100/1404]  eta: 0:14:58  lr: 0.000092  min_lr: 0.000001  loss: 4.1346 (4.1405)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.0093  max mem: 15572
Epoch: [8]  [ 110/1404]  eta: 0:14:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1976 (4.1434)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0091  max mem: 15572
Epoch: [8]  [ 120/1404]  eta: 0:14:22  lr: 0.000092  min_lr: 0.000001  loss: 4.2169 (4.1449)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0009  max mem: 15572
Epoch: [8]  [ 130/1404]  eta: 0:14:03  lr: 0.000092  min_lr: 0.000001  loss: 3.9418 (4.1346)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0012  max mem: 15572
Epoch: [8]  [ 140/1404]  eta: 0:13:55  lr: 0.000092  min_lr: 0.000001  loss: 3.9415 (4.1382)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0611  max mem: 15572
Epoch: [8]  [ 150/1404]  eta: 0:13:42  lr: 0.000092  min_lr: 0.000001  loss: 4.1126 (4.1323)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0927  max mem: 15572
Epoch: [8]  [ 160/1404]  eta: 0:13:27  lr: 0.000092  min_lr: 0.000001  loss: 3.8893 (4.1297)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0445  max mem: 15572
Epoch: [8]  [ 170/1404]  eta: 0:13:25  lr: 0.000092  min_lr: 0.000001  loss: 4.0304 (4.1247)  loss_scale: 32768.0000 (33151.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0667  max mem: 15572
Epoch: [8]  [ 180/1404]  eta: 0:13:14  lr: 0.000092  min_lr: 0.000001  loss: 4.1896 (4.1259)  loss_scale: 32768.0000 (33130.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1149  max mem: 15572
[2025-01-10 17:42:07,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:42:07,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:42:07,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:42:07,742] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:42:08,174] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11420
[2025-01-10 17:42:08,175] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:42:08,175] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11420
[2025-01-10 17:42:08,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:42:08,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 190/1404]  eta: 0:13:03  lr: 0.000092  min_lr: 0.000001  loss: 4.2930 (4.1378)  loss_scale: 32768.0000 (33282.6806)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1020  max mem: 15572
Epoch: [8]  [ 200/1404]  eta: 0:12:56  lr: 0.000092  min_lr: 0.000001  loss: 4.1708 (4.1347)  loss_scale: 32768.0000 (33257.0746)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0983  max mem: 15572
Epoch: [8]  [ 210/1404]  eta: 0:12:45  lr: 0.000092  min_lr: 0.000001  loss: 4.0853 (4.1379)  loss_scale: 32768.0000 (33233.8957)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0839  max mem: 15572
Epoch: [8]  [ 220/1404]  eta: 0:12:35  lr: 0.000092  min_lr: 0.000001  loss: 4.2569 (4.1397)  loss_scale: 32768.0000 (33212.8145)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0754  max mem: 15572
Epoch: [8]  [ 230/1404]  eta: 0:12:30  lr: 0.000092  min_lr: 0.000001  loss: 4.2113 (4.1414)  loss_scale: 32768.0000 (33193.5584)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0487  max mem: 15572
Epoch: [8]  [ 240/1404]  eta: 0:12:24  lr: 0.000092  min_lr: 0.000001  loss: 4.2124 (4.1460)  loss_scale: 32768.0000 (33175.9004)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.0010  max mem: 15572
Epoch: [8]  [ 250/1404]  eta: 0:12:16  lr: 0.000092  min_lr: 0.000001  loss: 4.2124 (4.1537)  loss_scale: 32768.0000 (33159.6494)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0014  max mem: 15572
Epoch: [8]  [ 260/1404]  eta: 0:12:08  lr: 0.000092  min_lr: 0.000001  loss: 4.1513 (4.1586)  loss_scale: 32768.0000 (33144.6437)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0483  max mem: 15572
Epoch: [8]  [ 270/1404]  eta: 0:11:59  lr: 0.000092  min_lr: 0.000001  loss: 4.4024 (4.1732)  loss_scale: 32768.0000 (33130.7454)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0479  max mem: 15572
Epoch: [8]  [ 280/1404]  eta: 0:11:51  lr: 0.000092  min_lr: 0.000001  loss: 4.3423 (4.1742)  loss_scale: 32768.0000 (33117.8363)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0062  max mem: 15572
Epoch: [8]  [ 290/1404]  eta: 0:11:47  lr: 0.000092  min_lr: 0.000001  loss: 4.1687 (4.1697)  loss_scale: 32768.0000 (33105.8144)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.0237  max mem: 15572
Epoch: [8]  [ 300/1404]  eta: 0:11:40  lr: 0.000092  min_lr: 0.000001  loss: 4.1676 (4.1650)  loss_scale: 32768.0000 (33094.5914)  weight_decay: 0.0500 (0.0500)  time: 0.6614  data: 0.0859  max mem: 15572
Epoch: [8]  [ 310/1404]  eta: 0:11:31  lr: 0.000092  min_lr: 0.000001  loss: 4.1356 (4.1646)  loss_scale: 32768.0000 (33084.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0927  max mem: 15572
[2025-01-10 17:43:27,944] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:43:27,944] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:43:28,033] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:43:28,034] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 320/1404]  eta: 0:11:27  lr: 0.000092  min_lr: 0.000001  loss: 4.1509 (4.1665)  loss_scale: 32768.0000 (33482.5670)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0875  max mem: 15572
Epoch: [8]  [ 330/1404]  eta: 0:11:19  lr: 0.000092  min_lr: 0.000001  loss: 4.0840 (4.1621)  loss_scale: 65536.0000 (34450.9486)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0758  max mem: 15572
Epoch: [8]  [ 340/1404]  eta: 0:11:15  lr: 0.000092  min_lr: 0.000001  loss: 4.1383 (4.1661)  loss_scale: 65536.0000 (35362.5337)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0921  max mem: 15572
Epoch: [8]  [ 350/1404]  eta: 0:11:04  lr: 0.000092  min_lr: 0.000001  loss: 4.1383 (4.1559)  loss_scale: 65536.0000 (36222.1766)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0795  max mem: 15572
Epoch: [8]  [ 360/1404]  eta: 0:10:58  lr: 0.000092  min_lr: 0.000001  loss: 4.0278 (4.1586)  loss_scale: 65536.0000 (37034.1939)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0137  max mem: 15572
[2025-01-10 17:44:00,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11601
[2025-01-10 17:44:00,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:44:00,251] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:44:00,283] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11601
[2025-01-10 17:44:00,284] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 370/1404]  eta: 0:10:54  lr: 0.000092  min_lr: 0.000001  loss: 4.1069 (4.1571)  loss_scale: 65536.0000 (37625.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.0975  max mem: 15572
Epoch: [8]  [ 380/1404]  eta: 0:10:44  lr: 0.000092  min_lr: 0.000001  loss: 4.0862 (4.1536)  loss_scale: 32768.0000 (37498.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0843  max mem: 15572
Epoch: [8]  [ 390/1404]  eta: 0:10:40  lr: 0.000092  min_lr: 0.000001  loss: 4.1758 (4.1541)  loss_scale: 32768.0000 (37377.3095)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1278  max mem: 15572
Epoch: [8]  [ 400/1404]  eta: 0:10:33  lr: 0.000092  min_lr: 0.000001  loss: 4.1625 (4.1503)  loss_scale: 32768.0000 (37262.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6659  data: 0.1947  max mem: 15572
Epoch: [8]  [ 410/1404]  eta: 0:10:26  lr: 0.000092  min_lr: 0.000001  loss: 4.0060 (4.1497)  loss_scale: 32768.0000 (37153.0122)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0921  max mem: 15572
Epoch: [8]  [ 420/1404]  eta: 0:10:19  lr: 0.000092  min_lr: 0.000001  loss: 4.2489 (4.1503)  loss_scale: 32768.0000 (37048.8551)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0797  max mem: 15572
Epoch: [8]  [ 430/1404]  eta: 0:10:11  lr: 0.000092  min_lr: 0.000001  loss: 4.1132 (4.1462)  loss_scale: 32768.0000 (36949.5313)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0807  max mem: 15572
Epoch: [8]  [ 440/1404]  eta: 0:10:06  lr: 0.000092  min_lr: 0.000001  loss: 4.0585 (4.1427)  loss_scale: 32768.0000 (36854.7120)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1039  max mem: 15572
Epoch: [8]  [ 450/1404]  eta: 0:09:58  lr: 0.000092  min_lr: 0.000001  loss: 4.1388 (4.1413)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1177  max mem: 15572
Epoch: [8]  [ 460/1404]  eta: 0:09:52  lr: 0.000092  min_lr: 0.000001  loss: 4.2310 (4.1393)  loss_scale: 32768.0000 (36677.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1008  max mem: 15572
Epoch: [8]  [ 470/1404]  eta: 0:09:44  lr: 0.000092  min_lr: 0.000001  loss: 4.1002 (4.1400)  loss_scale: 32768.0000 (36594.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0785  max mem: 15572
Epoch: [8]  [ 480/1404]  eta: 0:09:36  lr: 0.000092  min_lr: 0.000001  loss: 4.0532 (4.1401)  loss_scale: 32768.0000 (36514.8607)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0178  max mem: 15572
Epoch: [8]  [ 490/1404]  eta: 0:09:30  lr: 0.000092  min_lr: 0.000001  loss: 3.8485 (4.1350)  loss_scale: 32768.0000 (36438.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0586  max mem: 15572
[2025-01-10 17:45:17,850] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:45:17,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:45:17,850] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:45:17,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 500/1404]  eta: 0:09:24  lr: 0.000092  min_lr: 0.000001  loss: 3.9021 (4.1351)  loss_scale: 32768.0000 (36561.5010)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.1221  max mem: 15572
[2025-01-10 17:45:21,675] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11735
[2025-01-10 17:45:21,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:45:21,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:45:21,679] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11735
[2025-01-10 17:45:21,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 510/1404]  eta: 0:09:18  lr: 0.000092  min_lr: 0.000001  loss: 3.9792 (4.1342)  loss_scale: 32768.0000 (36615.5147)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1140  max mem: 15572
Epoch: [8]  [ 520/1404]  eta: 0:09:11  lr: 0.000092  min_lr: 0.000001  loss: 4.1656 (4.1358)  loss_scale: 32768.0000 (36541.6660)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0801  max mem: 15572
Epoch: [8]  [ 530/1404]  eta: 0:09:05  lr: 0.000092  min_lr: 0.000001  loss: 4.1881 (4.1365)  loss_scale: 32768.0000 (36470.5989)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0724  max mem: 15572
Epoch: [8]  [ 540/1404]  eta: 0:08:58  lr: 0.000092  min_lr: 0.000001  loss: 4.2369 (4.1367)  loss_scale: 32768.0000 (36402.1590)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0761  max mem: 15572
Epoch: [8]  [ 550/1404]  eta: 0:08:53  lr: 0.000092  min_lr: 0.000001  loss: 4.1915 (4.1361)  loss_scale: 32768.0000 (36336.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.1594  max mem: 15572
Epoch: [8]  [ 560/1404]  eta: 0:08:46  lr: 0.000092  min_lr: 0.000001  loss: 4.0982 (4.1329)  loss_scale: 32768.0000 (36272.5989)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1650  max mem: 15572
Epoch: [8]  [ 570/1404]  eta: 0:08:39  lr: 0.000092  min_lr: 0.000001  loss: 4.1351 (4.1345)  loss_scale: 32768.0000 (36211.2224)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1005  max mem: 15572
Epoch: [8]  [ 580/1404]  eta: 0:08:32  lr: 0.000092  min_lr: 0.000001  loss: 4.0953 (4.1342)  loss_scale: 32768.0000 (36151.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0851  max mem: 15572
Epoch: [8]  [ 590/1404]  eta: 0:08:25  lr: 0.000092  min_lr: 0.000001  loss: 4.1172 (4.1351)  loss_scale: 32768.0000 (36094.7005)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0810  max mem: 15572
Epoch: [8]  [ 600/1404]  eta: 0:08:18  lr: 0.000092  min_lr: 0.000001  loss: 4.1691 (4.1319)  loss_scale: 32768.0000 (36039.3478)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0575  max mem: 15572
Epoch: [8]  [ 610/1404]  eta: 0:08:10  lr: 0.000092  min_lr: 0.000001  loss: 4.0807 (4.1310)  loss_scale: 32768.0000 (35985.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0007  max mem: 15572
Epoch: [8]  [ 620/1404]  eta: 0:08:03  lr: 0.000092  min_lr: 0.000001  loss: 4.2684 (4.1339)  loss_scale: 32768.0000 (35933.9903)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0397  max mem: 15572
Epoch: [8]  [ 630/1404]  eta: 0:07:57  lr: 0.000092  min_lr: 0.000001  loss: 4.2191 (4.1320)  loss_scale: 32768.0000 (35883.8162)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1109  max mem: 15572
[2025-01-10 17:46:36,941] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:46:36,941] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:46:37,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:46:37,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 640/1404]  eta: 0:07:51  lr: 0.000092  min_lr: 0.000001  loss: 4.0021 (4.1310)  loss_scale: 32768.0000 (36295.2886)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1282  max mem: 15572
[2025-01-10 17:46:42,422] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11873
[2025-01-10 17:46:42,422] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:46:42,422] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:46:42,422] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11873
[2025-01-10 17:46:42,423] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 650/1404]  eta: 0:07:46  lr: 0.000092  min_lr: 0.000001  loss: 4.1550 (4.1320)  loss_scale: 32768.0000 (36241.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1332  max mem: 15572
Epoch: [8]  [ 660/1404]  eta: 0:07:40  lr: 0.000091  min_lr: 0.000001  loss: 4.1058 (4.1297)  loss_scale: 32768.0000 (36188.5628)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1122  max mem: 15572
Epoch: [8]  [ 670/1404]  eta: 0:07:33  lr: 0.000091  min_lr: 0.000001  loss: 4.1198 (4.1319)  loss_scale: 32768.0000 (36137.5857)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0877  max mem: 15572
Epoch: [8]  [ 680/1404]  eta: 0:07:27  lr: 0.000091  min_lr: 0.000001  loss: 3.9691 (4.1286)  loss_scale: 32768.0000 (36088.1057)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1063  max mem: 15572
Epoch: [8]  [ 690/1404]  eta: 0:07:21  lr: 0.000091  min_lr: 0.000001  loss: 3.9691 (4.1299)  loss_scale: 32768.0000 (36040.0579)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0930  max mem: 15572
Epoch: [8]  [ 700/1404]  eta: 0:07:15  lr: 0.000091  min_lr: 0.000001  loss: 4.2743 (4.1310)  loss_scale: 32768.0000 (35993.3809)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0783  max mem: 15572
Epoch: [8]  [ 710/1404]  eta: 0:07:09  lr: 0.000091  min_lr: 0.000001  loss: 4.2226 (4.1327)  loss_scale: 32768.0000 (35948.0169)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.0968  max mem: 15572
Epoch: [8]  [ 720/1404]  eta: 0:07:03  lr: 0.000091  min_lr: 0.000001  loss: 4.2456 (4.1345)  loss_scale: 32768.0000 (35903.9112)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1321  max mem: 15572
Epoch: [8]  [ 730/1404]  eta: 0:06:57  lr: 0.000091  min_lr: 0.000001  loss: 4.1672 (4.1323)  loss_scale: 32768.0000 (35861.0123)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0939  max mem: 15572
Epoch: [8]  [ 740/1404]  eta: 0:06:52  lr: 0.000091  min_lr: 0.000001  loss: 4.0654 (4.1321)  loss_scale: 32768.0000 (35819.2713)  weight_decay: 0.0500 (0.0500)  time: 0.6829  data: 0.1342  max mem: 15572
Epoch: [8]  [ 750/1404]  eta: 0:06:47  lr: 0.000091  min_lr: 0.000001  loss: 4.1161 (4.1321)  loss_scale: 32768.0000 (35778.6418)  weight_decay: 0.0500 (0.0500)  time: 0.7199  data: 0.2133  max mem: 15572
Epoch: [8]  [ 760/1404]  eta: 0:06:40  lr: 0.000091  min_lr: 0.000001  loss: 4.0786 (4.1277)  loss_scale: 32768.0000 (35739.0802)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1327  max mem: 15572
[2025-01-10 17:48:04,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=67, lr=[8.855418217882891e-07, 8.855418217882891e-07, 1.2650597454118418e-06, 1.2650597454118418e-06, 1.8072282077312026e-06, 1.8072282077312026e-06, 2.581754582473147e-06, 2.581754582473147e-06, 3.6882208321044958e-06, 3.6882208321044958e-06, 5.268886903006422e-06, 5.268886903006422e-06, 7.526981290009176e-06, 7.526981290009176e-06, 1.0752830414298824e-05, 1.0752830414298824e-05, 1.5361186306141176e-05, 1.5361186306141176e-05, 2.194455186591597e-05, 2.194455186591597e-05, 3.1349359808451386e-05, 3.1349359808451386e-05, 4.478479972635913e-05, 4.478479972635913e-05, 6.397828532337019e-05, 6.397828532337019e-05, 9.139755046195741e-05, 9.139755046195741e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:48:04,387] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=45.24240753964265, CurrSamplesPerSec=57.297441166174785, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 17:48:06,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:48:06,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:48:06,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:48:06,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 770/1404]  eta: 0:06:34  lr: 0.000091  min_lr: 0.000001  loss: 4.0935 (4.1278)  loss_scale: 32768.0000 (35743.0454)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1000  max mem: 15572
[2025-01-10 17:48:09,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12006
[2025-01-10 17:48:09,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12006
[2025-01-10 17:48:09,550] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:48:09,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:48:09,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 780/1404]  eta: 0:06:28  lr: 0.000091  min_lr: 0.000001  loss: 4.2621 (4.1288)  loss_scale: 32768.0000 (35830.8220)  weight_decay: 0.0500 (0.0500)  time: 0.6655  data: 0.1271  max mem: 15572
Epoch: [8]  [ 790/1404]  eta: 0:06:22  lr: 0.000091  min_lr: 0.000001  loss: 4.1981 (4.1289)  loss_scale: 32768.0000 (35792.1011)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0920  max mem: 15572
Epoch: [8]  [ 800/1404]  eta: 0:06:15  lr: 0.000091  min_lr: 0.000001  loss: 4.1981 (4.1296)  loss_scale: 32768.0000 (35754.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0588  max mem: 15572
Epoch: [8]  [ 810/1404]  eta: 0:06:10  lr: 0.000091  min_lr: 0.000001  loss: 4.2218 (4.1333)  loss_scale: 32768.0000 (35717.5240)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.1364  max mem: 15572
Epoch: [8]  [ 820/1404]  eta: 0:06:04  lr: 0.000091  min_lr: 0.000001  loss: 4.0902 (4.1325)  loss_scale: 32768.0000 (35681.5981)  weight_decay: 0.0500 (0.0500)  time: 0.6727  data: 0.1679  max mem: 15572
Epoch: [8]  [ 830/1404]  eta: 0:05:58  lr: 0.000091  min_lr: 0.000001  loss: 4.1546 (4.1325)  loss_scale: 32768.0000 (35646.5367)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.1543  max mem: 15572
Epoch: [8]  [ 840/1404]  eta: 0:05:51  lr: 0.000091  min_lr: 0.000001  loss: 4.2332 (4.1329)  loss_scale: 32768.0000 (35612.3092)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1201  max mem: 15572
Epoch: [8]  [ 850/1404]  eta: 0:05:46  lr: 0.000091  min_lr: 0.000001  loss: 4.0888 (4.1321)  loss_scale: 32768.0000 (35578.8860)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1340  max mem: 15572
Epoch: [8]  [ 860/1404]  eta: 0:05:40  lr: 0.000091  min_lr: 0.000001  loss: 4.0888 (4.1318)  loss_scale: 32768.0000 (35546.2393)  weight_decay: 0.0500 (0.0500)  time: 0.7106  data: 0.2027  max mem: 15572
Epoch: [8]  [ 870/1404]  eta: 0:05:34  lr: 0.000091  min_lr: 0.000001  loss: 4.0690 (4.1298)  loss_scale: 32768.0000 (35514.3421)  weight_decay: 0.0500 (0.0500)  time: 0.6752  data: 0.1846  max mem: 15572
Epoch: [8]  [ 880/1404]  eta: 0:05:29  lr: 0.000091  min_lr: 0.000001  loss: 4.0690 (4.1281)  loss_scale: 32768.0000 (35483.1691)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.1996  max mem: 15572
Epoch: [8]  [ 890/1404]  eta: 0:05:23  lr: 0.000091  min_lr: 0.000001  loss: 4.0341 (4.1267)  loss_scale: 32768.0000 (35452.6958)  weight_decay: 0.0500 (0.0500)  time: 0.7405  data: 0.1961  max mem: 15572
Epoch: [8]  [ 900/1404]  eta: 0:05:16  lr: 0.000091  min_lr: 0.000001  loss: 3.9976 (4.1252)  loss_scale: 32768.0000 (35422.8990)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.1163  max mem: 15572
[2025-01-10 17:49:34,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:49:34,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:49:34,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:49:34,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 910/1404]  eta: 0:05:11  lr: 0.000091  min_lr: 0.000001  loss: 3.9137 (4.1223)  loss_scale: 32768.0000 (35681.5104)  weight_decay: 0.0500 (0.0500)  time: 0.6623  data: 0.1167  max mem: 15572
Epoch: [8]  [ 920/1404]  eta: 0:05:04  lr: 0.000091  min_lr: 0.000001  loss: 4.0499 (4.1235)  loss_scale: 65536.0000 (36005.6634)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.0874  max mem: 15572
[2025-01-10 17:49:46,896] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12154
[2025-01-10 17:49:46,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:49:46,897] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12154
[2025-01-10 17:49:46,897] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:49:46,897] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 930/1404]  eta: 0:04:58  lr: 0.000091  min_lr: 0.000001  loss: 4.1396 (4.1234)  loss_scale: 65536.0000 (36006.0838)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0331  max mem: 15572
Epoch: [8]  [ 940/1404]  eta: 0:04:52  lr: 0.000091  min_lr: 0.000001  loss: 3.9987 (4.1199)  loss_scale: 32768.0000 (35971.6727)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.1026  max mem: 15572
Epoch: [8]  [ 950/1404]  eta: 0:04:46  lr: 0.000091  min_lr: 0.000001  loss: 3.8544 (4.1186)  loss_scale: 32768.0000 (35937.9853)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.0977  max mem: 15572
Epoch: [8]  [ 960/1404]  eta: 0:04:39  lr: 0.000091  min_lr: 0.000001  loss: 3.9140 (4.1169)  loss_scale: 32768.0000 (35904.9990)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0282  max mem: 15572
Epoch: [8]  [ 970/1404]  eta: 0:04:33  lr: 0.000091  min_lr: 0.000001  loss: 4.0579 (4.1166)  loss_scale: 32768.0000 (35872.6921)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0376  max mem: 15572
Epoch: [8]  [ 980/1404]  eta: 0:04:27  lr: 0.000091  min_lr: 0.000001  loss: 4.1055 (4.1172)  loss_scale: 32768.0000 (35841.0438)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0763  max mem: 15572
Epoch: [8]  [ 990/1404]  eta: 0:04:20  lr: 0.000091  min_lr: 0.000001  loss: 4.1277 (4.1172)  loss_scale: 32768.0000 (35810.0343)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0793  max mem: 15572
Epoch: [8]  [1000/1404]  eta: 0:04:14  lr: 0.000091  min_lr: 0.000001  loss: 4.1833 (4.1167)  loss_scale: 32768.0000 (35779.6444)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0483  max mem: 15572
Epoch: [8]  [1010/1404]  eta: 0:04:08  lr: 0.000091  min_lr: 0.000001  loss: 4.1941 (4.1174)  loss_scale: 32768.0000 (35749.8556)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0090  max mem: 15572
Epoch: [8]  [1020/1404]  eta: 0:04:01  lr: 0.000091  min_lr: 0.000001  loss: 4.1941 (4.1180)  loss_scale: 32768.0000 (35720.6503)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.0181  max mem: 15572
Epoch: [8]  [1030/1404]  eta: 0:03:55  lr: 0.000091  min_lr: 0.000001  loss: 4.0971 (4.1178)  loss_scale: 32768.0000 (35692.0116)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.0429  max mem: 15572
Epoch: [8]  [1040/1404]  eta: 0:03:49  lr: 0.000091  min_lr: 0.000001  loss: 3.9600 (4.1164)  loss_scale: 32768.0000 (35663.9232)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.0409  max mem: 15572
Epoch: [8]  [1050/1404]  eta: 0:03:43  lr: 0.000091  min_lr: 0.000001  loss: 3.9600 (4.1152)  loss_scale: 32768.0000 (35636.3692)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.0435  max mem: 15572
[2025-01-10 17:51:10,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:51:10,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:51:10,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:51:10,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1060/1404]  eta: 0:03:36  lr: 0.000091  min_lr: 0.000001  loss: 3.9808 (4.1138)  loss_scale: 32768.0000 (35918.1753)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0333  max mem: 15572
Epoch: [8]  [1070/1404]  eta: 0:03:30  lr: 0.000091  min_lr: 0.000001  loss: 3.9808 (4.1148)  loss_scale: 65536.0000 (36194.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0996  max mem: 15572
[2025-01-10 17:51:28,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12312
[2025-01-10 17:51:28,542] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:51:28,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1080/1404]  eta: 0:03:24  lr: 0.000091  min_lr: 0.000001  loss: 4.2708 (4.1160)  loss_scale: 65536.0000 (36435.8335)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.1862  max mem: 15572
[2025-01-10 17:51:28,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12312
[2025-01-10 17:51:28,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1090/1404]  eta: 0:03:18  lr: 0.000091  min_lr: 0.000001  loss: 4.2708 (4.1168)  loss_scale: 32768.0000 (36402.2145)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.1742  max mem: 15572
Epoch: [8]  [1100/1404]  eta: 0:03:12  lr: 0.000091  min_lr: 0.000001  loss: 4.0549 (4.1146)  loss_scale: 32768.0000 (36369.2062)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.1750  max mem: 15572
Epoch: [8]  [1110/1404]  eta: 0:03:05  lr: 0.000091  min_lr: 0.000001  loss: 4.0959 (4.1157)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1358  max mem: 15572
Epoch: [8]  [1120/1404]  eta: 0:02:59  lr: 0.000091  min_lr: 0.000001  loss: 4.0959 (4.1143)  loss_scale: 32768.0000 (36304.9563)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1344  max mem: 15572
Epoch: [8]  [1130/1404]  eta: 0:02:53  lr: 0.000091  min_lr: 0.000001  loss: 3.8898 (4.1135)  loss_scale: 32768.0000 (36273.6835)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.1726  max mem: 15572
Epoch: [8]  [1140/1404]  eta: 0:02:46  lr: 0.000091  min_lr: 0.000001  loss: 3.8676 (4.1104)  loss_scale: 32768.0000 (36242.9588)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.1687  max mem: 15572
Epoch: [8]  [1150/1404]  eta: 0:02:40  lr: 0.000091  min_lr: 0.000001  loss: 3.8749 (4.1090)  loss_scale: 32768.0000 (36212.7680)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.1548  max mem: 15572
Epoch: [8]  [1160/1404]  eta: 0:02:34  lr: 0.000091  min_lr: 0.000001  loss: 3.9558 (4.1078)  loss_scale: 32768.0000 (36183.0973)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.1075  max mem: 15572
Epoch: [8]  [1170/1404]  eta: 0:02:28  lr: 0.000091  min_lr: 0.000001  loss: 4.0843 (4.1068)  loss_scale: 32768.0000 (36153.9334)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0930  max mem: 15572
Epoch: [8]  [1180/1404]  eta: 0:02:22  lr: 0.000091  min_lr: 0.000001  loss: 4.1056 (4.1071)  loss_scale: 32768.0000 (36125.2633)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.1269  max mem: 15572
Epoch: [8]  [1190/1404]  eta: 0:02:15  lr: 0.000091  min_lr: 0.000001  loss: 4.2069 (4.1071)  loss_scale: 32768.0000 (36097.0747)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.1442  max mem: 15572
Epoch: [8]  [1200/1404]  eta: 0:02:09  lr: 0.000091  min_lr: 0.000001  loss: 3.9170 (4.1059)  loss_scale: 32768.0000 (36069.3555)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1085  max mem: 15572
[2025-01-10 17:52:54,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:52:54,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:52:54,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:52:54,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:52:55,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12442
[2025-01-10 17:52:55,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:52:55,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12442
[2025-01-10 17:52:55,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:52:55,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1210/1404]  eta: 0:02:03  lr: 0.000091  min_lr: 0.000001  loss: 3.7825 (4.1035)  loss_scale: 32768.0000 (36069.1528)  weight_decay: 0.0500 (0.0500)  time: 0.6897  data: 0.1307  max mem: 15572
Epoch: [8]  [1220/1404]  eta: 0:01:56  lr: 0.000091  min_lr: 0.000001  loss: 3.8640 (4.1013)  loss_scale: 32768.0000 (36042.1163)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.1618  max mem: 15572
Epoch: [8]  [1230/1404]  eta: 0:01:50  lr: 0.000091  min_lr: 0.000001  loss: 4.0577 (4.1018)  loss_scale: 32768.0000 (36015.5191)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1123  max mem: 15572
Epoch: [8]  [1240/1404]  eta: 0:01:44  lr: 0.000091  min_lr: 0.000001  loss: 4.1364 (4.1029)  loss_scale: 32768.0000 (35989.3505)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.1282  max mem: 15572
Epoch: [8]  [1250/1404]  eta: 0:01:37  lr: 0.000091  min_lr: 0.000001  loss: 4.1219 (4.1023)  loss_scale: 32768.0000 (35963.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1139  max mem: 15572
Epoch: [8]  [1260/1404]  eta: 0:01:31  lr: 0.000091  min_lr: 0.000001  loss: 4.1219 (4.1016)  loss_scale: 32768.0000 (35938.2585)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0288  max mem: 15572
Epoch: [8]  [1270/1404]  eta: 0:01:24  lr: 0.000091  min_lr: 0.000001  loss: 4.1405 (4.1026)  loss_scale: 32768.0000 (35913.3155)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0007  max mem: 15572
Epoch: [8]  [1280/1404]  eta: 0:01:18  lr: 0.000091  min_lr: 0.000001  loss: 4.2231 (4.1033)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0008  max mem: 15572
Epoch: [8]  [1290/1404]  eta: 0:01:12  lr: 0.000091  min_lr: 0.000001  loss: 4.2208 (4.1038)  loss_scale: 32768.0000 (35864.5887)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0101  max mem: 15572
Epoch: [8]  [1300/1404]  eta: 0:01:05  lr: 0.000091  min_lr: 0.000001  loss: 4.1967 (4.1046)  loss_scale: 32768.0000 (35840.7871)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0104  max mem: 15572
Epoch: [8]  [1310/1404]  eta: 0:00:59  lr: 0.000091  min_lr: 0.000001  loss: 4.1165 (4.1037)  loss_scale: 32768.0000 (35817.3486)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.0013  max mem: 15572
Epoch: [8]  [1320/1404]  eta: 0:00:53  lr: 0.000091  min_lr: 0.000001  loss: 4.1743 (4.1043)  loss_scale: 32768.0000 (35794.2650)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0012  max mem: 15572
Epoch: [8]  [1330/1404]  eta: 0:00:46  lr: 0.000091  min_lr: 0.000001  loss: 4.2710 (4.1029)  loss_scale: 32768.0000 (35771.5282)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0011  max mem: 15572
[2025-01-10 17:54:15,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:54:15,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:54:15,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:54:15,014] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:54:15,439] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12572
[2025-01-10 17:54:15,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:54:15,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:54:15,441] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12572
[2025-01-10 17:54:15,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1340/1404]  eta: 0:00:40  lr: 0.000091  min_lr: 0.000001  loss: 4.0947 (4.1029)  loss_scale: 32768.0000 (35773.5660)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.0013  max mem: 15572
Epoch: [8]  [1350/1404]  eta: 0:00:34  lr: 0.000091  min_lr: 0.000001  loss: 4.1523 (4.1034)  loss_scale: 32768.0000 (35751.3190)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0011  max mem: 15572
Epoch: [8]  [1360/1404]  eta: 0:00:27  lr: 0.000091  min_lr: 0.000001  loss: 4.0044 (4.1028)  loss_scale: 32768.0000 (35729.3990)  weight_decay: 0.0500 (0.0500)  time: 0.6540  data: 0.0254  max mem: 15572
Epoch: [8]  [1370/1404]  eta: 0:00:21  lr: 0.000091  min_lr: 0.000001  loss: 3.9388 (4.1011)  loss_scale: 32768.0000 (35707.7987)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0255  max mem: 15572
Epoch: [8]  [1380/1404]  eta: 0:00:15  lr: 0.000091  min_lr: 0.000001  loss: 3.8957 (4.1010)  loss_scale: 32768.0000 (35686.5112)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0011  max mem: 15572
Epoch: [8]  [1390/1404]  eta: 0:00:08  lr: 0.000091  min_lr: 0.000001  loss: 3.9817 (4.1002)  loss_scale: 32768.0000 (35665.5298)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0011  max mem: 15572
Epoch: [8]  [1400/1404]  eta: 0:00:02  lr: 0.000091  min_lr: 0.000001  loss: 4.0276 (4.1003)  loss_scale: 32768.0000 (35644.8480)  weight_decay: 0.0500 (0.0500)  time: 0.4732  data: 0.0005  max mem: 15572
Epoch: [8]  [1403/1404]  eta: 0:00:00  lr: 0.000091  min_lr: 0.000001  loss: 4.0880 (4.1008)  loss_scale: 32768.0000 (35638.7009)  weight_decay: 0.0500 (0.0500)  time: 0.4541  data: 0.0005  max mem: 15572
Epoch: [8] Total time: 0:14:45 (0.6307 s / it)
Averaged stats: lr: 0.000091  min_lr: 0.000001  loss: 4.0880 (4.1059)  loss_scale: 32768.0000 (35638.7009)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:03  loss: 1.6798 (1.6798)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 6.6408  data: 6.4512  max mem: 15572
Val:  [ 10/136]  eta: 0:01:46  loss: 3.0946 (2.9840)  acc1: 33.3333 (29.7980)  acc5: 66.6667 (57.5758)  time: 0.8442  data: 0.6206  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 3.2280 (3.1198)  acc1: 22.2222 (27.2487)  acc5: 61.1111 (57.9365)  time: 0.3106  data: 0.0962  max mem: 15572
Val:  [ 30/136]  eta: 0:00:56  loss: 3.1373 (2.8420)  acc1: 27.7778 (34.5878)  acc5: 61.1111 (62.7240)  time: 0.3660  data: 0.1480  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 1.9703 (2.7773)  acc1: 55.5556 (35.6369)  acc5: 77.7778 (64.7696)  time: 0.3761  data: 0.1577  max mem: 15572
Val:  [ 50/136]  eta: 0:00:42  loss: 2.8908 (2.8651)  acc1: 27.7778 (33.7691)  acc5: 72.2222 (64.0523)  time: 0.4155  data: 0.2033  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 3.1194 (2.9421)  acc1: 16.6667 (31.2386)  acc5: 61.1111 (62.5683)  time: 0.3378  data: 0.1231  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.9290 (2.8826)  acc1: 22.2222 (33.5681)  acc5: 66.6667 (63.5368)  time: 0.2757  data: 0.0714  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.6619 (2.8733)  acc1: 33.3333 (33.0590)  acc5: 66.6667 (64.8834)  time: 0.3402  data: 0.1426  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.8564 (2.8785)  acc1: 27.7778 (32.1734)  acc5: 66.6667 (65.2625)  time: 0.3563  data: 0.1449  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.0903 (2.9432)  acc1: 16.6667 (30.9681)  acc5: 61.1111 (63.5314)  time: 0.3697  data: 0.1390  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.9664 (2.9275)  acc1: 27.7778 (32.0320)  acc5: 61.1111 (63.8639)  time: 0.3744  data: 0.1564  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.4595 (2.8583)  acc1: 50.0000 (34.0680)  acc5: 77.7778 (65.4729)  time: 0.4121  data: 0.1924  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.3645 (2.8133)  acc1: 55.5556 (35.5386)  acc5: 77.7778 (65.6913)  time: 0.3051  data: 0.1084  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.4650 (2.8184)  acc1: 50.0000 (35.3399)  acc5: 72.2222 (65.4791)  time: 0.1971  data: 0.0273  max mem: 15572
Val: Total time: 0:00:52 (0.3825 s / it)
* Acc@1 34.828 Acc@5 64.292 loss 2.853
Accuracy of the network on the 4883 val videos: 34.8%
[2025-01-10 17:55:43,934] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 17:55:43,936] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 17:55:43,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 17:55:43,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 17:55:46,436] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 17:55:46,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.83%
Epoch: [9]  [   0/1404]  eta: 3:33:07  lr: 0.000091  min_lr: 0.000001  loss: 3.1464 (3.1464)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1077  data: 6.5224  max mem: 15572
Epoch: [9]  [  10/1404]  eta: 0:32:08  lr: 0.000091  min_lr: 0.000001  loss: 3.8210 (3.8804)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3833  data: 0.5937  max mem: 15572
Epoch: [9]  [  20/1404]  eta: 0:22:24  lr: 0.000091  min_lr: 0.000001  loss: 4.1264 (4.0720)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0008  max mem: 15572
Epoch: [9]  [  30/1404]  eta: 0:19:32  lr: 0.000091  min_lr: 0.000001  loss: 4.1264 (4.0708)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0007  max mem: 15572
Epoch: [9]  [  40/1404]  eta: 0:17:32  lr: 0.000091  min_lr: 0.000001  loss: 4.1116 (4.0893)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0065  max mem: 15572
Epoch: [9]  [  50/1404]  eta: 0:16:41  lr: 0.000091  min_lr: 0.000001  loss: 4.2060 (4.0898)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0498  max mem: 15572
Epoch: [9]  [  60/1404]  eta: 0:16:25  lr: 0.000091  min_lr: 0.000001  loss: 4.2170 (4.1058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1352  max mem: 15572
[2025-01-10 17:56:35,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:56:35,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:56:35,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:56:35,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:56:35,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12702
[2025-01-10 17:56:35,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:56:35,735] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12702
[2025-01-10 17:56:35,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:56:35,738] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [  70/1404]  eta: 0:16:04  lr: 0.000091  min_lr: 0.000001  loss: 4.2055 (4.1065)  loss_scale: 32768.0000 (33229.5211)  weight_decay: 0.0500 (0.0500)  time: 0.6810  data: 0.1015  max mem: 15572
Epoch: [9]  [  80/1404]  eta: 0:15:39  lr: 0.000091  min_lr: 0.000001  loss: 4.2055 (4.1230)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0165  max mem: 15572
Epoch: [9]  [  90/1404]  eta: 0:15:47  lr: 0.000091  min_lr: 0.000001  loss: 4.2873 (4.1253)  loss_scale: 32768.0000 (33128.0879)  weight_decay: 0.0500 (0.0500)  time: 0.7134  data: 0.0067  max mem: 15572
Epoch: [9]  [ 100/1404]  eta: 0:15:10  lr: 0.000091  min_lr: 0.000001  loss: 4.0533 (4.1084)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.0007  max mem: 15572
Epoch: [9]  [ 110/1404]  eta: 0:14:55  lr: 0.000091  min_lr: 0.000001  loss: 4.0083 (4.0966)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0007  max mem: 15572
Epoch: [9]  [ 120/1404]  eta: 0:14:39  lr: 0.000091  min_lr: 0.000001  loss: 4.0451 (4.0845)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0007  max mem: 15572
Epoch: [9]  [ 130/1404]  eta: 0:14:26  lr: 0.000091  min_lr: 0.000001  loss: 4.0722 (4.0785)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0008  max mem: 15572
Epoch: [9]  [ 140/1404]  eta: 0:14:17  lr: 0.000091  min_lr: 0.000001  loss: 4.0937 (4.0722)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0045  max mem: 15572
Epoch: [9]  [ 150/1404]  eta: 0:14:08  lr: 0.000091  min_lr: 0.000001  loss: 4.0089 (4.0710)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.0044  max mem: 15572
Epoch: [9]  [ 160/1404]  eta: 0:13:52  lr: 0.000091  min_lr: 0.000001  loss: 3.9136 (4.0641)  loss_scale: 32768.0000 (32971.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0007  max mem: 15572
Epoch: [9]  [ 170/1404]  eta: 0:13:38  lr: 0.000091  min_lr: 0.000001  loss: 3.8741 (4.0475)  loss_scale: 32768.0000 (32959.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0007  max mem: 15572
Epoch: [9]  [ 180/1404]  eta: 0:13:27  lr: 0.000091  min_lr: 0.000001  loss: 4.1029 (4.0586)  loss_scale: 32768.0000 (32949.0387)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0007  max mem: 15572
Epoch: [9]  [ 190/1404]  eta: 0:13:14  lr: 0.000091  min_lr: 0.000001  loss: 4.1440 (4.0515)  loss_scale: 32768.0000 (32939.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0008  max mem: 15572
[2025-01-10 17:57:55,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:57:55,852] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:57:55,920] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:57:55,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 200/1404]  eta: 0:13:11  lr: 0.000091  min_lr: 0.000001  loss: 3.9903 (4.0498)  loss_scale: 32768.0000 (33909.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0009  max mem: 15572
Epoch: [9]  [ 210/1404]  eta: 0:12:59  lr: 0.000091  min_lr: 0.000001  loss: 4.2074 (4.0555)  loss_scale: 65536.0000 (35408.0758)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0009  max mem: 15572
Epoch: [9]  [ 220/1404]  eta: 0:12:51  lr: 0.000091  min_lr: 0.000001  loss: 4.2347 (4.0582)  loss_scale: 65536.0000 (36771.3303)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0008  max mem: 15572
[2025-01-10 17:58:14,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12862
[2025-01-10 17:58:14,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:58:14,152] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12862
[2025-01-10 17:58:14,152] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:58:14,152] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 230/1404]  eta: 0:12:40  lr: 0.000091  min_lr: 0.000001  loss: 3.9571 (4.0479)  loss_scale: 65536.0000 (37307.2900)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0007  max mem: 15572
Epoch: [9]  [ 240/1404]  eta: 0:12:39  lr: 0.000091  min_lr: 0.000001  loss: 3.8888 (4.0441)  loss_scale: 32768.0000 (37118.9378)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0007  max mem: 15572
Epoch: [9]  [ 250/1404]  eta: 0:12:31  lr: 0.000090  min_lr: 0.000001  loss: 3.9854 (4.0435)  loss_scale: 32768.0000 (36945.5936)  weight_decay: 0.0500 (0.0500)  time: 0.6911  data: 0.0010  max mem: 15572
Epoch: [9]  [ 260/1404]  eta: 0:12:23  lr: 0.000090  min_lr: 0.000001  loss: 4.0901 (4.0433)  loss_scale: 32768.0000 (36785.5326)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0008  max mem: 15572
Epoch: [9]  [ 270/1404]  eta: 0:12:10  lr: 0.000090  min_lr: 0.000001  loss: 4.1249 (4.0445)  loss_scale: 32768.0000 (36637.2841)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0007  max mem: 15572
Epoch: [9]  [ 280/1404]  eta: 0:12:05  lr: 0.000090  min_lr: 0.000001  loss: 4.1401 (4.0458)  loss_scale: 32768.0000 (36499.5872)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0011  max mem: 15572
Epoch: [9]  [ 290/1404]  eta: 0:11:57  lr: 0.000090  min_lr: 0.000001  loss: 4.2331 (4.0554)  loss_scale: 32768.0000 (36371.3540)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0012  max mem: 15572
Epoch: [9]  [ 300/1404]  eta: 0:11:55  lr: 0.000090  min_lr: 0.000001  loss: 4.2327 (4.0532)  loss_scale: 32768.0000 (36251.6412)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.0012  max mem: 15572
Epoch: [9]  [ 310/1404]  eta: 0:11:45  lr: 0.000090  min_lr: 0.000001  loss: 4.0111 (4.0505)  loss_scale: 32768.0000 (36139.6270)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.0010  max mem: 15572
Epoch: [9]  [ 320/1404]  eta: 0:11:42  lr: 0.000090  min_lr: 0.000001  loss: 4.0304 (4.0508)  loss_scale: 32768.0000 (36034.5919)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0006  max mem: 15572
Epoch: [9]  [ 330/1404]  eta: 0:11:32  lr: 0.000090  min_lr: 0.000001  loss: 4.1292 (4.0549)  loss_scale: 32768.0000 (35935.9033)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0007  max mem: 15572
Epoch: [9]  [ 340/1404]  eta: 0:11:25  lr: 0.000090  min_lr: 0.000001  loss: 3.9997 (4.0550)  loss_scale: 32768.0000 (35843.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0008  max mem: 15572
Epoch: [9]  [ 350/1404]  eta: 0:11:19  lr: 0.000090  min_lr: 0.000001  loss: 3.8890 (4.0511)  loss_scale: 32768.0000 (35755.3960)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0008  max mem: 15572
[2025-01-10 17:59:35,430] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:59:35,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 17:59:35,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 17:59:35,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 360/1404]  eta: 0:11:10  lr: 0.000090  min_lr: 0.000001  loss: 3.7657 (4.0436)  loss_scale: 32768.0000 (36217.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0008  max mem: 15572
[2025-01-10 17:59:39,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=74, lr=[8.755895920446586e-07, 8.755895920446586e-07, 1.2508422743495125e-06, 1.2508422743495125e-06, 1.786917534785018e-06, 1.786917534785018e-06, 2.5527393354071685e-06, 2.5527393354071685e-06, 3.6467704791530984e-06, 3.6467704791530984e-06, 5.209672113075855e-06, 5.209672113075855e-06, 7.442388732965508e-06, 7.442388732965508e-06, 1.0631983904236441e-05, 1.0631983904236441e-05, 1.5188548434623486e-05, 1.5188548434623486e-05, 2.169792633517641e-05, 2.169792633517641e-05, 3.0997037621680587e-05, 3.0997037621680587e-05, 4.428148231668656e-05, 4.428148231668656e-05, 6.325926045240937e-05, 6.325926045240937e-05, 9.037037207487054e-05, 9.037037207487054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 17:59:39,852] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=45.13745256420123, CurrSamplesPerSec=55.182522854004084, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 370/1404]  eta: 0:11:03  lr: 0.000090  min_lr: 0.000001  loss: 3.9892 (4.0429)  loss_scale: 65536.0000 (37007.5256)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0008  max mem: 15572
Epoch: [9]  [ 380/1404]  eta: 0:10:54  lr: 0.000090  min_lr: 0.000001  loss: 4.1793 (4.0484)  loss_scale: 65536.0000 (37756.3045)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0009  max mem: 15572
[2025-01-10 17:59:54,016] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13022
[2025-01-10 17:59:54,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 17:59:54,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 17:59:54,095] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13022
[2025-01-10 17:59:54,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 390/1404]  eta: 0:10:46  lr: 0.000090  min_lr: 0.000001  loss: 4.2786 (4.0507)  loss_scale: 65536.0000 (38047.7545)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0008  max mem: 15572
Epoch: [9]  [ 400/1404]  eta: 0:10:40  lr: 0.000090  min_lr: 0.000001  loss: 4.0785 (4.0528)  loss_scale: 32768.0000 (37916.0898)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0007  max mem: 15572
Epoch: [9]  [ 410/1404]  eta: 0:10:33  lr: 0.000090  min_lr: 0.000001  loss: 4.2619 (4.0567)  loss_scale: 32768.0000 (37790.8321)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0008  max mem: 15572
Epoch: [9]  [ 420/1404]  eta: 0:10:29  lr: 0.000090  min_lr: 0.000001  loss: 4.2691 (4.0605)  loss_scale: 32768.0000 (37671.5249)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.0007  max mem: 15572
Epoch: [9]  [ 430/1404]  eta: 0:10:20  lr: 0.000090  min_lr: 0.000001  loss: 3.9884 (4.0577)  loss_scale: 32768.0000 (37557.7541)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0046  max mem: 15572
Epoch: [9]  [ 440/1404]  eta: 0:10:13  lr: 0.000090  min_lr: 0.000001  loss: 3.9538 (4.0552)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0046  max mem: 15572
Epoch: [9]  [ 450/1404]  eta: 0:10:05  lr: 0.000090  min_lr: 0.000001  loss: 3.9538 (4.0495)  loss_scale: 32768.0000 (37345.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0008  max mem: 15572
Epoch: [9]  [ 460/1404]  eta: 0:09:59  lr: 0.000090  min_lr: 0.000001  loss: 4.0353 (4.0517)  loss_scale: 32768.0000 (37246.0564)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0009  max mem: 15572
Epoch: [9]  [ 470/1404]  eta: 0:09:52  lr: 0.000090  min_lr: 0.000001  loss: 4.0843 (4.0528)  loss_scale: 32768.0000 (37150.9809)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0010  max mem: 15572
Epoch: [9]  [ 480/1404]  eta: 0:09:44  lr: 0.000090  min_lr: 0.000001  loss: 3.9932 (4.0489)  loss_scale: 32768.0000 (37059.8586)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0010  max mem: 15572
Epoch: [9]  [ 490/1404]  eta: 0:09:38  lr: 0.000090  min_lr: 0.000001  loss: 4.0832 (4.0545)  loss_scale: 32768.0000 (36972.4481)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0007  max mem: 15572
Epoch: [9]  [ 500/1404]  eta: 0:09:33  lr: 0.000090  min_lr: 0.000001  loss: 4.2790 (4.0557)  loss_scale: 32768.0000 (36888.5269)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0011  max mem: 15572
Epoch: [9]  [ 510/1404]  eta: 0:09:26  lr: 0.000090  min_lr: 0.000001  loss: 4.2128 (4.0567)  loss_scale: 32768.0000 (36807.8904)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.0013  max mem: 15572
[2025-01-10 18:01:13,564] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:01:13,564] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:01:13,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:01:13,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:01:14,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13152
[2025-01-10 18:01:14,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:01:14,019] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:01:14,020] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13152
[2025-01-10 18:01:14,020] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 520/1404]  eta: 0:09:18  lr: 0.000090  min_lr: 0.000001  loss: 4.2017 (4.0601)  loss_scale: 32768.0000 (36793.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0010  max mem: 15572
Epoch: [9]  [ 530/1404]  eta: 0:09:12  lr: 0.000090  min_lr: 0.000001  loss: 4.0204 (4.0563)  loss_scale: 32768.0000 (36717.4388)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0009  max mem: 15572
Epoch: [9]  [ 540/1404]  eta: 0:09:05  lr: 0.000090  min_lr: 0.000001  loss: 3.9967 (4.0568)  loss_scale: 32768.0000 (36644.4362)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0008  max mem: 15572
Epoch: [9]  [ 550/1404]  eta: 0:08:58  lr: 0.000090  min_lr: 0.000001  loss: 4.1773 (4.0567)  loss_scale: 32768.0000 (36574.0835)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0010  max mem: 15572
Epoch: [9]  [ 560/1404]  eta: 0:08:51  lr: 0.000090  min_lr: 0.000001  loss: 4.0185 (4.0547)  loss_scale: 32768.0000 (36506.2389)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0011  max mem: 15572
Epoch: [9]  [ 570/1404]  eta: 0:08:45  lr: 0.000090  min_lr: 0.000001  loss: 4.0185 (4.0565)  loss_scale: 32768.0000 (36440.7706)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0010  max mem: 15572
Epoch: [9]  [ 580/1404]  eta: 0:08:38  lr: 0.000090  min_lr: 0.000001  loss: 4.1704 (4.0590)  loss_scale: 32768.0000 (36377.5559)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0009  max mem: 15572
Epoch: [9]  [ 590/1404]  eta: 0:08:32  lr: 0.000090  min_lr: 0.000001  loss: 4.1655 (4.0603)  loss_scale: 32768.0000 (36316.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0011  max mem: 15572
Epoch: [9]  [ 600/1404]  eta: 0:08:25  lr: 0.000090  min_lr: 0.000001  loss: 4.0482 (4.0586)  loss_scale: 32768.0000 (36257.4376)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0010  max mem: 15572
Epoch: [9]  [ 610/1404]  eta: 0:08:17  lr: 0.000090  min_lr: 0.000001  loss: 3.8766 (4.0554)  loss_scale: 32768.0000 (36200.3273)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0008  max mem: 15572
Epoch: [9]  [ 620/1404]  eta: 0:08:12  lr: 0.000090  min_lr: 0.000001  loss: 4.0744 (4.0599)  loss_scale: 32768.0000 (36145.0564)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0009  max mem: 15572
Epoch: [9]  [ 630/1404]  eta: 0:08:05  lr: 0.000090  min_lr: 0.000001  loss: 4.0744 (4.0598)  loss_scale: 32768.0000 (36091.5372)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0009  max mem: 15572
Epoch: [9]  [ 640/1404]  eta: 0:08:00  lr: 0.000090  min_lr: 0.000001  loss: 3.9524 (4.0575)  loss_scale: 32768.0000 (36039.6880)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0010  max mem: 15572
[2025-01-10 18:02:32,408] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:02:32,409] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:02:32,409] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:02:32,409] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:02:35,321] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13285
[2025-01-10 18:02:35,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:02:35,322] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:02:35,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13285
[2025-01-10 18:02:35,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 650/1404]  eta: 0:07:53  lr: 0.000090  min_lr: 0.000001  loss: 3.9000 (4.0566)  loss_scale: 32768.0000 (36190.7711)  weight_decay: 0.0500 (0.0500)  time: 0.6709  data: 0.0010  max mem: 15572
Epoch: [9]  [ 660/1404]  eta: 0:07:47  lr: 0.000090  min_lr: 0.000001  loss: 4.1885 (4.0593)  loss_scale: 32768.0000 (36138.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0009  max mem: 15572
Epoch: [9]  [ 670/1404]  eta: 0:07:41  lr: 0.000090  min_lr: 0.000001  loss: 3.9442 (4.0558)  loss_scale: 32768.0000 (36088.7511)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0014  max mem: 15572
Epoch: [9]  [ 680/1404]  eta: 0:07:36  lr: 0.000090  min_lr: 0.000001  loss: 3.8252 (4.0538)  loss_scale: 32768.0000 (36039.9883)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.0014  max mem: 15572
Epoch: [9]  [ 690/1404]  eta: 0:07:28  lr: 0.000090  min_lr: 0.000001  loss: 3.9433 (4.0567)  loss_scale: 32768.0000 (35992.6368)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0007  max mem: 15572
Epoch: [9]  [ 700/1404]  eta: 0:07:21  lr: 0.000090  min_lr: 0.000001  loss: 4.1904 (4.0563)  loss_scale: 32768.0000 (35946.6362)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0008  max mem: 15572
Epoch: [9]  [ 710/1404]  eta: 0:07:14  lr: 0.000090  min_lr: 0.000001  loss: 4.1348 (4.0593)  loss_scale: 32768.0000 (35901.9297)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0013  max mem: 15572
Epoch: [9]  [ 720/1404]  eta: 0:07:08  lr: 0.000090  min_lr: 0.000001  loss: 4.1442 (4.0605)  loss_scale: 32768.0000 (35858.4632)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0016  max mem: 15572
Epoch: [9]  [ 730/1404]  eta: 0:07:02  lr: 0.000090  min_lr: 0.000001  loss: 4.1370 (4.0606)  loss_scale: 32768.0000 (35816.1860)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0013  max mem: 15572
Epoch: [9]  [ 740/1404]  eta: 0:06:55  lr: 0.000090  min_lr: 0.000001  loss: 4.1382 (4.0625)  loss_scale: 32768.0000 (35775.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0011  max mem: 15572
Epoch: [9]  [ 750/1404]  eta: 0:06:49  lr: 0.000090  min_lr: 0.000001  loss: 4.1635 (4.0620)  loss_scale: 32768.0000 (35735.0093)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0012  max mem: 15572
Epoch: [9]  [ 760/1404]  eta: 0:06:44  lr: 0.000090  min_lr: 0.000001  loss: 4.0633 (4.0597)  loss_scale: 32768.0000 (35696.0210)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.0010  max mem: 15572
Epoch: [9]  [ 770/1404]  eta: 0:06:37  lr: 0.000090  min_lr: 0.000001  loss: 3.8188 (4.0573)  loss_scale: 32768.0000 (35658.0441)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.0009  max mem: 15572
[2025-01-10 18:03:55,283] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:03:55,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:03:55,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:03:55,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 780/1404]  eta: 0:06:31  lr: 0.000090  min_lr: 0.000001  loss: 3.9083 (4.0571)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0009  max mem: 15572
[2025-01-10 18:03:58,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13419
[2025-01-10 18:03:58,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:03:58,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13419
[2025-01-10 18:03:58,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:03:58,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 790/1404]  eta: 0:06:25  lr: 0.000090  min_lr: 0.000001  loss: 3.9992 (4.0564)  loss_scale: 32768.0000 (35792.1011)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0007  max mem: 15572
Epoch: [9]  [ 800/1404]  eta: 0:06:18  lr: 0.000090  min_lr: 0.000001  loss: 4.2275 (4.0603)  loss_scale: 32768.0000 (35754.3471)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0007  max mem: 15572
Epoch: [9]  [ 810/1404]  eta: 0:06:11  lr: 0.000090  min_lr: 0.000001  loss: 4.2159 (4.0625)  loss_scale: 32768.0000 (35717.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0008  max mem: 15572
Epoch: [9]  [ 820/1404]  eta: 0:06:05  lr: 0.000090  min_lr: 0.000001  loss: 4.1374 (4.0596)  loss_scale: 32768.0000 (35681.5981)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0009  max mem: 15572
Epoch: [9]  [ 830/1404]  eta: 0:05:58  lr: 0.000090  min_lr: 0.000001  loss: 3.8436 (4.0587)  loss_scale: 32768.0000 (35646.5367)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0010  max mem: 15572
Epoch: [9]  [ 840/1404]  eta: 0:05:52  lr: 0.000090  min_lr: 0.000001  loss: 3.8447 (4.0570)  loss_scale: 32768.0000 (35612.3092)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0010  max mem: 15572
Epoch: [9]  [ 850/1404]  eta: 0:05:45  lr: 0.000090  min_lr: 0.000001  loss: 3.8904 (4.0550)  loss_scale: 32768.0000 (35578.8860)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0056  max mem: 15572
Epoch: [9]  [ 860/1404]  eta: 0:05:39  lr: 0.000090  min_lr: 0.000001  loss: 4.0606 (4.0563)  loss_scale: 32768.0000 (35546.2393)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0404  max mem: 15572
Epoch: [9]  [ 870/1404]  eta: 0:05:33  lr: 0.000090  min_lr: 0.000001  loss: 4.2378 (4.0579)  loss_scale: 32768.0000 (35514.3421)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0997  max mem: 15572
Epoch: [9]  [ 880/1404]  eta: 0:05:27  lr: 0.000090  min_lr: 0.000001  loss: 4.1361 (4.0580)  loss_scale: 32768.0000 (35483.1691)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.0944  max mem: 15572
Epoch: [9]  [ 890/1404]  eta: 0:05:21  lr: 0.000090  min_lr: 0.000001  loss: 3.9972 (4.0566)  loss_scale: 32768.0000 (35452.6958)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0981  max mem: 15572
Epoch: [9]  [ 900/1404]  eta: 0:05:15  lr: 0.000090  min_lr: 0.000001  loss: 3.9972 (4.0569)  loss_scale: 32768.0000 (35422.8990)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.0687  max mem: 15572
Epoch: [9]  [ 910/1404]  eta: 0:05:08  lr: 0.000090  min_lr: 0.000001  loss: 4.0076 (4.0540)  loss_scale: 32768.0000 (35393.7563)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0007  max mem: 15572
[2025-01-10 18:05:17,202] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:05:17,202] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:05:17,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:05:17,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:05:18,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13550
[2025-01-10 18:05:18,201] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:05:18,221] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13550
[2025-01-10 18:05:18,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:05:18,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 920/1404]  eta: 0:05:02  lr: 0.000090  min_lr: 0.000001  loss: 4.0076 (4.0538)  loss_scale: 32768.0000 (35436.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0010  max mem: 15572
Epoch: [9]  [ 930/1404]  eta: 0:04:56  lr: 0.000090  min_lr: 0.000001  loss: 4.0740 (4.0522)  loss_scale: 32768.0000 (35407.7422)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0013  max mem: 15572
Epoch: [9]  [ 940/1404]  eta: 0:04:49  lr: 0.000090  min_lr: 0.000001  loss: 3.9715 (4.0512)  loss_scale: 32768.0000 (35379.6897)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0010  max mem: 15572
Epoch: [9]  [ 950/1404]  eta: 0:04:43  lr: 0.000090  min_lr: 0.000001  loss: 4.0121 (4.0513)  loss_scale: 32768.0000 (35352.2271)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0164  max mem: 15572
Epoch: [9]  [ 960/1404]  eta: 0:04:37  lr: 0.000090  min_lr: 0.000001  loss: 3.9697 (4.0506)  loss_scale: 32768.0000 (35325.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0335  max mem: 15572
Epoch: [9]  [ 970/1404]  eta: 0:04:30  lr: 0.000090  min_lr: 0.000001  loss: 4.0593 (4.0515)  loss_scale: 32768.0000 (35298.9990)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0378  max mem: 15572
Epoch: [9]  [ 980/1404]  eta: 0:04:24  lr: 0.000090  min_lr: 0.000001  loss: 4.2158 (4.0519)  loss_scale: 32768.0000 (35273.1988)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0834  max mem: 15572
Epoch: [9]  [ 990/1404]  eta: 0:04:17  lr: 0.000090  min_lr: 0.000001  loss: 4.1969 (4.0535)  loss_scale: 32768.0000 (35247.9193)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0786  max mem: 15572
Epoch: [9]  [1000/1404]  eta: 0:04:11  lr: 0.000090  min_lr: 0.000001  loss: 4.2684 (4.0548)  loss_scale: 32768.0000 (35223.1449)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0706  max mem: 15572
Epoch: [9]  [1010/1404]  eta: 0:04:05  lr: 0.000090  min_lr: 0.000001  loss: 4.1564 (4.0523)  loss_scale: 32768.0000 (35198.8605)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1215  max mem: 15572
Epoch: [9]  [1020/1404]  eta: 0:03:59  lr: 0.000090  min_lr: 0.000001  loss: 3.6105 (4.0495)  loss_scale: 32768.0000 (35175.0519)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1182  max mem: 15572
Epoch: [9]  [1030/1404]  eta: 0:03:53  lr: 0.000090  min_lr: 0.000001  loss: 4.0390 (4.0507)  loss_scale: 32768.0000 (35151.7051)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1579  max mem: 15572
Epoch: [9]  [1040/1404]  eta: 0:03:47  lr: 0.000090  min_lr: 0.000001  loss: 4.3407 (4.0520)  loss_scale: 32768.0000 (35128.8069)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.1607  max mem: 15572
[2025-01-10 18:06:37,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:06:37,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:06:37,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:06:37,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1050/1404]  eta: 0:03:40  lr: 0.000090  min_lr: 0.000001  loss: 4.1101 (4.0519)  loss_scale: 32768.0000 (35355.7678)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1050  max mem: 15572
[2025-01-10 18:06:43,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13688
[2025-01-10 18:06:43,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:06:43,214] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13688
[2025-01-10 18:06:43,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:06:43,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1060/1404]  eta: 0:03:34  lr: 0.000090  min_lr: 0.000001  loss: 4.0227 (4.0515)  loss_scale: 32768.0000 (35362.2620)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.1205  max mem: 15572
Epoch: [9]  [1070/1404]  eta: 0:03:27  lr: 0.000090  min_lr: 0.000001  loss: 4.0069 (4.0501)  loss_scale: 32768.0000 (35338.0392)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0706  max mem: 15572
Epoch: [9]  [1080/1404]  eta: 0:03:21  lr: 0.000090  min_lr: 0.000001  loss: 4.1491 (4.0514)  loss_scale: 32768.0000 (35314.2646)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0228  max mem: 15572
Epoch: [9]  [1090/1404]  eta: 0:03:15  lr: 0.000090  min_lr: 0.000001  loss: 4.1445 (4.0515)  loss_scale: 32768.0000 (35290.9258)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0888  max mem: 15572
Epoch: [9]  [1100/1404]  eta: 0:03:09  lr: 0.000089  min_lr: 0.000001  loss: 3.9559 (4.0517)  loss_scale: 32768.0000 (35268.0109)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1081  max mem: 15572
Epoch: [9]  [1110/1404]  eta: 0:03:02  lr: 0.000089  min_lr: 0.000001  loss: 4.1619 (4.0531)  loss_scale: 32768.0000 (35245.5086)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0422  max mem: 15572
Epoch: [9]  [1120/1404]  eta: 0:02:56  lr: 0.000089  min_lr: 0.000001  loss: 4.2797 (4.0552)  loss_scale: 32768.0000 (35223.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0438  max mem: 15572
Epoch: [9]  [1130/1404]  eta: 0:02:50  lr: 0.000089  min_lr: 0.000001  loss: 4.1536 (4.0556)  loss_scale: 32768.0000 (35201.6976)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1279  max mem: 15572
Epoch: [9]  [1140/1404]  eta: 0:02:44  lr: 0.000089  min_lr: 0.000001  loss: 4.1079 (4.0577)  loss_scale: 32768.0000 (35180.3681)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.1524  max mem: 15572
Epoch: [9]  [1150/1404]  eta: 0:02:37  lr: 0.000089  min_lr: 0.000001  loss: 4.0783 (4.0576)  loss_scale: 32768.0000 (35159.4092)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0960  max mem: 15572
Epoch: [9]  [1160/1404]  eta: 0:02:31  lr: 0.000089  min_lr: 0.000001  loss: 4.0451 (4.0582)  loss_scale: 32768.0000 (35138.8114)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1277  max mem: 15572
Epoch: [9]  [1170/1404]  eta: 0:02:25  lr: 0.000089  min_lr: 0.000001  loss: 4.0777 (4.0576)  loss_scale: 32768.0000 (35118.5653)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1330  max mem: 15572
Epoch: [9]  [1180/1404]  eta: 0:02:19  lr: 0.000089  min_lr: 0.000001  loss: 3.8162 (4.0559)  loss_scale: 32768.0000 (35098.6622)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.0980  max mem: 15572
[2025-01-10 18:08:01,952] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:08:01,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:08:02,050] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:08:02,050] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1190/1404]  eta: 0:02:13  lr: 0.000089  min_lr: 0.000001  loss: 4.0249 (4.0558)  loss_scale: 32768.0000 (35354.2233)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0934  max mem: 15572
[2025-01-10 18:08:07,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13827
[2025-01-10 18:08:07,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:08:07,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13827
[2025-01-10 18:08:07,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:08:07,767] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1200/1404]  eta: 0:02:06  lr: 0.000089  min_lr: 0.000001  loss: 3.9544 (4.0551)  loss_scale: 32768.0000 (35332.6894)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0706  max mem: 15572
[2025-01-10 18:08:14,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13838
[2025-01-10 18:08:14,169] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 18:08:14,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-10 18:08:14,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13838
[2025-01-10 18:08:14,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [9]  [1210/1404]  eta: 0:02:00  lr: 0.000089  min_lr: 0.000001  loss: 4.1503 (4.0567)  loss_scale: 32768.0000 (35189.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0845  max mem: 15572
Epoch: [9]  [1220/1404]  eta: 0:01:54  lr: 0.000089  min_lr: 0.000001  loss: 4.3667 (4.0579)  loss_scale: 16384.0000 (35035.7281)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1189  max mem: 15572
Epoch: [9]  [1230/1404]  eta: 0:01:48  lr: 0.000089  min_lr: 0.000001  loss: 4.1180 (4.0580)  loss_scale: 16384.0000 (34884.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.1640  max mem: 15572
Epoch: [9]  [1240/1404]  eta: 0:01:41  lr: 0.000089  min_lr: 0.000001  loss: 3.9954 (4.0587)  loss_scale: 16384.0000 (34735.1362)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.1393  max mem: 15572
Epoch: [9]  [1250/1404]  eta: 0:01:35  lr: 0.000089  min_lr: 0.000001  loss: 3.9954 (4.0560)  loss_scale: 16384.0000 (34588.4444)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0918  max mem: 15572
Epoch: [9]  [1260/1404]  eta: 0:01:29  lr: 0.000089  min_lr: 0.000001  loss: 3.9838 (4.0560)  loss_scale: 16384.0000 (34444.0793)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.0808  max mem: 15572
Epoch: [9]  [1270/1404]  eta: 0:01:23  lr: 0.000089  min_lr: 0.000001  loss: 4.0746 (4.0571)  loss_scale: 16384.0000 (34301.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0669  max mem: 15572
Epoch: [9]  [1280/1404]  eta: 0:01:17  lr: 0.000089  min_lr: 0.000001  loss: 4.2361 (4.0577)  loss_scale: 16384.0000 (34162.1109)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0416  max mem: 15572
Epoch: [9]  [1290/1404]  eta: 0:01:10  lr: 0.000089  min_lr: 0.000001  loss: 4.0544 (4.0564)  loss_scale: 16384.0000 (34024.4028)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0411  max mem: 15572
Epoch: [9]  [1300/1404]  eta: 0:01:04  lr: 0.000089  min_lr: 0.000001  loss: 3.8340 (4.0563)  loss_scale: 16384.0000 (33888.8117)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.0964  max mem: 15572
Epoch: [9]  [1310/1404]  eta: 0:00:58  lr: 0.000089  min_lr: 0.000001  loss: 3.9708 (4.0559)  loss_scale: 16384.0000 (33755.2891)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1126  max mem: 15572
Epoch: [9]  [1320/1404]  eta: 0:00:52  lr: 0.000089  min_lr: 0.000001  loss: 3.8989 (4.0540)  loss_scale: 16384.0000 (33623.7880)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0871  max mem: 15572
Epoch: [9]  [1330/1404]  eta: 0:00:46  lr: 0.000089  min_lr: 0.000001  loss: 3.8176 (4.0533)  loss_scale: 16384.0000 (33494.2630)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1101  max mem: 15572
[2025-01-10 18:09:35,475] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:09:35,476] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 18:09:35,493] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:09:35,494] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [1340/1404]  eta: 0:00:39  lr: 0.000089  min_lr: 0.000001  loss: 4.0848 (4.0526)  loss_scale: 16384.0000 (33488.8471)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1143  max mem: 15572
Epoch: [9]  [1350/1404]  eta: 0:00:33  lr: 0.000089  min_lr: 0.000001  loss: 4.1292 (4.0533)  loss_scale: 32768.0000 (33483.5115)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1302  max mem: 15572
Epoch: [9]  [1360/1404]  eta: 0:00:27  lr: 0.000089  min_lr: 0.000001  loss: 4.1292 (4.0533)  loss_scale: 32768.0000 (33478.2542)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1198  max mem: 15572
[2025-01-10 18:09:55,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=82, lr=[8.63917353010542e-07, 8.63917353010542e-07, 1.2341676471579173e-06, 1.2341676471579173e-06, 1.763096638797025e-06, 1.763096638797025e-06, 2.51870948399575e-06, 2.51870948399575e-06, 3.5981564057082143e-06, 3.5981564057082143e-06, 5.140223436726021e-06, 5.140223436726021e-06, 7.34317633818003e-06, 7.34317633818003e-06, 1.0490251911685759e-05, 1.0490251911685759e-05, 1.4986074159551083e-05, 1.4986074159551083e-05, 2.1408677370787264e-05, 2.1408677370787264e-05, 3.058382481541038e-05, 3.058382481541038e-05, 4.3691178307729115e-05, 4.3691178307729115e-05, 6.241596901104159e-05, 6.241596901104159e-05, 8.916567001577372e-05, 8.916567001577372e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 18:09:55,304] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=45.077179006333374, CurrSamplesPerSec=44.064295169457694, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1370/1404]  eta: 0:00:21  lr: 0.000089  min_lr: 0.000001  loss: 3.9863 (4.0526)  loss_scale: 32768.0000 (33473.0737)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0970  max mem: 15572
Epoch: [9]  [1380/1404]  eta: 0:00:14  lr: 0.000089  min_lr: 0.000001  loss: 3.9882 (4.0523)  loss_scale: 32768.0000 (33467.9681)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1084  max mem: 15572
Epoch: [9]  [1390/1404]  eta: 0:00:08  lr: 0.000089  min_lr: 0.000001  loss: 4.0245 (4.0522)  loss_scale: 32768.0000 (33462.9360)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0687  max mem: 15572
Epoch: [9]  [1400/1404]  eta: 0:00:02  lr: 0.000089  min_lr: 0.000001  loss: 3.9711 (4.0517)  loss_scale: 32768.0000 (33457.9757)  weight_decay: 0.0500 (0.0500)  time: 0.4657  data: 0.0188  max mem: 15572
Epoch: [9]  [1403/1404]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000001  loss: 3.9711 (4.0520)  loss_scale: 32768.0000 (33456.5014)  weight_decay: 0.0500 (0.0500)  time: 0.4460  data: 0.0187  max mem: 15572
Epoch: [9] Total time: 0:14:30 (0.6199 s / it)
Averaged stats: lr: 0.000089  min_lr: 0.000001  loss: 3.9711 (4.0528)  loss_scale: 32768.0000 (33456.5014)  weight_decay: 0.0500 (0.0500)
[2025-01-10 18:10:16,781] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-10 18:10:16,783] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-10 18:10:16,783] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-10 18:10:16,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
[2025-01-10 18:10:17,019] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-10 18:10:17,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/136]  eta: 0:11:14  loss: 1.7826 (1.7826)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.9572  data: 4.7321  max mem: 15572
Val:  [ 10/136]  eta: 0:01:47  loss: 2.8408 (2.8526)  acc1: 33.3333 (32.3232)  acc5: 66.6667 (60.1010)  time: 0.8547  data: 0.6588  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 2.9876 (2.9626)  acc1: 33.3333 (29.8942)  acc5: 61.1111 (61.1111)  time: 0.3874  data: 0.1797  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.8784 (2.7077)  acc1: 33.3333 (37.0968)  acc5: 66.6667 (66.3082)  time: 0.2764  data: 0.0544  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.1201 (2.6664)  acc1: 38.8889 (37.1274)  acc5: 77.7778 (67.7507)  time: 0.2860  data: 0.0749  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.6609 (2.7198)  acc1: 27.7778 (37.5817)  acc5: 72.2222 (68.4096)  time: 0.3822  data: 0.1816  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.9769 (2.8132)  acc1: 22.2222 (33.9709)  acc5: 66.6667 (66.2113)  time: 0.4125  data: 0.2108  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.7824 (2.7715)  acc1: 27.7778 (36.2285)  acc5: 61.1111 (66.4319)  time: 0.4005  data: 0.1918  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.6149 (2.7716)  acc1: 44.4444 (35.9396)  acc5: 66.6667 (66.6667)  time: 0.3857  data: 0.1881  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.6948 (2.7755)  acc1: 27.7778 (35.4701)  acc5: 72.2222 (66.7277)  time: 0.3738  data: 0.1811  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9895 (2.8555)  acc1: 22.2222 (33.2783)  acc5: 61.1111 (64.9065)  time: 0.3681  data: 0.1674  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.0518 (2.8543)  acc1: 22.2222 (33.3834)  acc5: 55.5556 (64.7147)  time: 0.3639  data: 0.1655  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.5434 (2.7808)  acc1: 38.8889 (35.3994)  acc5: 72.2222 (66.5289)  time: 0.3572  data: 0.1603  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8201 (2.7188)  acc1: 55.5556 (37.1077)  acc5: 83.3333 (67.1332)  time: 0.2830  data: 0.1114  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2280 (2.7277)  acc1: 38.8889 (36.8550)  acc5: 77.7778 (67.1171)  time: 0.2082  data: 0.0544  max mem: 15572
Val: Total time: 0:00:51 (0.3813 s / it)
* Acc@1 36.036 Acc@5 66.626 loss 2.762
Accuracy of the network on the 4883 val videos: 36.0%
[2025-01-10 18:11:09,532] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 18:11:09,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 18:11:09,535] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 18:11:09,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 18:11:12,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 18:11:12,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 36.04%
Epoch: [10]  [   0/1404]  eta: 2:52:59  lr: 0.000089  min_lr: 0.000001  loss: 3.9880 (3.9880)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3930  data: 5.5452  max mem: 15572
Epoch: [10]  [  10/1404]  eta: 0:32:36  lr: 0.000089  min_lr: 0.000001  loss: 3.9600 (3.9319)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4035  data: 0.7993  max mem: 15572
Epoch: [10]  [  20/1404]  eta: 0:22:32  lr: 0.000089  min_lr: 0.000001  loss: 3.9053 (3.8858)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.1630  max mem: 15572
Epoch: [10]  [  30/1404]  eta: 0:19:40  lr: 0.000089  min_lr: 0.000001  loss: 3.9770 (3.9894)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0561  max mem: 15572
Epoch: [10]  [  40/1404]  eta: 0:18:26  lr: 0.000089  min_lr: 0.000001  loss: 4.0222 (3.9757)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1388  max mem: 15572
Epoch: [10]  [  50/1404]  eta: 0:16:53  lr: 0.000089  min_lr: 0.000001  loss: 4.0222 (3.9924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0838  max mem: 15572
[2025-01-10 18:11:53,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:11:53,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:11:53,858] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:11:53,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:11:55,392] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14098
[2025-01-10 18:11:55,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:11:55,392] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14098
[2025-01-10 18:11:55,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:11:55,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [  60/1404]  eta: 0:16:15  lr: 0.000089  min_lr: 0.000001  loss: 4.0487 (3.9924)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0562  max mem: 15572
Epoch: [10]  [  70/1404]  eta: 0:15:51  lr: 0.000089  min_lr: 0.000001  loss: 3.8818 (3.9612)  loss_scale: 32768.0000 (34152.5634)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1103  max mem: 15572
Epoch: [10]  [  80/1404]  eta: 0:15:19  lr: 0.000089  min_lr: 0.000001  loss: 3.8076 (3.9442)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0863  max mem: 15572
Epoch: [10]  [  90/1404]  eta: 0:15:02  lr: 0.000089  min_lr: 0.000001  loss: 3.9028 (3.9631)  loss_scale: 32768.0000 (33848.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0906  max mem: 15572
Epoch: [10]  [ 100/1404]  eta: 0:14:46  lr: 0.000089  min_lr: 0.000001  loss: 4.0876 (3.9730)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1068  max mem: 15572
Epoch: [10]  [ 110/1404]  eta: 0:14:26  lr: 0.000089  min_lr: 0.000001  loss: 4.0131 (3.9749)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0798  max mem: 15572
Epoch: [10]  [ 120/1404]  eta: 0:14:19  lr: 0.000089  min_lr: 0.000001  loss: 4.0225 (3.9850)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1117  max mem: 15572
Epoch: [10]  [ 130/1404]  eta: 0:14:03  lr: 0.000089  min_lr: 0.000001  loss: 4.0467 (3.9868)  loss_scale: 32768.0000 (33518.4122)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0805  max mem: 15572
Epoch: [10]  [ 140/1404]  eta: 0:13:55  lr: 0.000089  min_lr: 0.000001  loss: 3.9988 (3.9958)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0781  max mem: 15572
Epoch: [10]  [ 150/1404]  eta: 0:13:45  lr: 0.000089  min_lr: 0.000001  loss: 4.1626 (4.0179)  loss_scale: 32768.0000 (33419.0199)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1300  max mem: 15572
Epoch: [10]  [ 160/1404]  eta: 0:13:29  lr: 0.000089  min_lr: 0.000001  loss: 4.1818 (4.0321)  loss_scale: 32768.0000 (33378.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0653  max mem: 15572
Epoch: [10]  [ 170/1404]  eta: 0:13:21  lr: 0.000089  min_lr: 0.000001  loss: 4.0882 (4.0123)  loss_scale: 32768.0000 (33342.8772)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0854  max mem: 15572
Epoch: [10]  [ 180/1404]  eta: 0:13:17  lr: 0.000089  min_lr: 0.000001  loss: 3.8445 (4.0032)  loss_scale: 32768.0000 (33311.1160)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.1493  max mem: 15572
[2025-01-10 18:13:13,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:13:13,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:13:13,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:13:13,644] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 190/1404]  eta: 0:13:11  lr: 0.000089  min_lr: 0.000001  loss: 3.9136 (4.0002)  loss_scale: 32768.0000 (33968.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6661  data: 0.1482  max mem: 15572
[2025-01-10 18:13:19,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14235
[2025-01-10 18:13:19,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:13:19,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14235
[2025-01-10 18:13:19,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:13:19,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 200/1404]  eta: 0:13:02  lr: 0.000089  min_lr: 0.000001  loss: 4.0333 (4.0113)  loss_scale: 32768.0000 (34561.2736)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1256  max mem: 15572
Epoch: [10]  [ 210/1404]  eta: 0:12:51  lr: 0.000089  min_lr: 0.000001  loss: 4.0495 (4.0136)  loss_scale: 32768.0000 (34476.2844)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0824  max mem: 15572
Epoch: [10]  [ 220/1404]  eta: 0:12:44  lr: 0.000089  min_lr: 0.000001  loss: 4.0495 (4.0129)  loss_scale: 32768.0000 (34398.9864)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0977  max mem: 15572
Epoch: [10]  [ 230/1404]  eta: 0:12:38  lr: 0.000089  min_lr: 0.000001  loss: 4.0093 (4.0105)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1351  max mem: 15572
Epoch: [10]  [ 240/1404]  eta: 0:12:30  lr: 0.000089  min_lr: 0.000001  loss: 3.8650 (4.0070)  loss_scale: 32768.0000 (34263.6349)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1115  max mem: 15572
Epoch: [10]  [ 250/1404]  eta: 0:12:21  lr: 0.000089  min_lr: 0.000001  loss: 3.9346 (4.0098)  loss_scale: 32768.0000 (34204.0478)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1062  max mem: 15572
Epoch: [10]  [ 260/1404]  eta: 0:12:16  lr: 0.000089  min_lr: 0.000001  loss: 4.0856 (4.0076)  loss_scale: 32768.0000 (34149.0268)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1347  max mem: 15572
Epoch: [10]  [ 270/1404]  eta: 0:12:06  lr: 0.000089  min_lr: 0.000001  loss: 3.9494 (4.0031)  loss_scale: 32768.0000 (34098.0664)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0750  max mem: 15572
Epoch: [10]  [ 280/1404]  eta: 0:11:58  lr: 0.000089  min_lr: 0.000001  loss: 3.8026 (3.9997)  loss_scale: 32768.0000 (34050.7331)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0553  max mem: 15572
Epoch: [10]  [ 290/1404]  eta: 0:11:48  lr: 0.000089  min_lr: 0.000001  loss: 3.9163 (3.9996)  loss_scale: 32768.0000 (34006.6529)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0675  max mem: 15572
Epoch: [10]  [ 300/1404]  eta: 0:11:45  lr: 0.000089  min_lr: 0.000001  loss: 4.1012 (4.0036)  loss_scale: 32768.0000 (33965.5017)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1158  max mem: 15572
Epoch: [10]  [ 310/1404]  eta: 0:11:36  lr: 0.000089  min_lr: 0.000001  loss: 4.1625 (4.0075)  loss_scale: 32768.0000 (33926.9968)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1032  max mem: 15572
Epoch: [10]  [ 320/1404]  eta: 0:11:27  lr: 0.000089  min_lr: 0.000001  loss: 4.1677 (4.0080)  loss_scale: 32768.0000 (33890.8910)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0093  max mem: 15572
[2025-01-10 18:14:37,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:14:37,653] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:14:37,673] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:14:37,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 330/1404]  eta: 0:11:24  lr: 0.000089  min_lr: 0.000001  loss: 3.8324 (4.0064)  loss_scale: 32768.0000 (34549.9456)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.0092  max mem: 15572
[2025-01-10 18:14:47,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14378
[2025-01-10 18:14:47,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:14:47,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14378
[2025-01-10 18:14:47,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:14:47,126] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 340/1404]  eta: 0:11:16  lr: 0.000089  min_lr: 0.000001  loss: 3.9703 (4.0026)  loss_scale: 65536.0000 (35170.3460)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0009  max mem: 15572
Epoch: [10]  [ 350/1404]  eta: 0:11:09  lr: 0.000089  min_lr: 0.000001  loss: 3.9615 (4.0015)  loss_scale: 32768.0000 (35101.9031)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0010  max mem: 15572
Epoch: [10]  [ 360/1404]  eta: 0:11:04  lr: 0.000089  min_lr: 0.000001  loss: 3.9188 (4.0048)  loss_scale: 32768.0000 (35037.2521)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0010  max mem: 15572
Epoch: [10]  [ 370/1404]  eta: 0:10:56  lr: 0.000089  min_lr: 0.000001  loss: 4.0906 (4.0078)  loss_scale: 32768.0000 (34976.0863)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0010  max mem: 15572
Epoch: [10]  [ 380/1404]  eta: 0:10:51  lr: 0.000089  min_lr: 0.000001  loss: 3.9844 (4.0052)  loss_scale: 32768.0000 (34918.1312)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0009  max mem: 15572
Epoch: [10]  [ 390/1404]  eta: 0:10:43  lr: 0.000089  min_lr: 0.000001  loss: 4.1466 (4.0104)  loss_scale: 32768.0000 (34863.1407)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0006  max mem: 15572
Epoch: [10]  [ 400/1404]  eta: 0:10:37  lr: 0.000089  min_lr: 0.000001  loss: 4.1288 (4.0065)  loss_scale: 32768.0000 (34810.8928)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0006  max mem: 15572
Epoch: [10]  [ 410/1404]  eta: 0:10:28  lr: 0.000089  min_lr: 0.000001  loss: 4.0170 (4.0090)  loss_scale: 32768.0000 (34761.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0007  max mem: 15572
Epoch: [10]  [ 420/1404]  eta: 0:10:19  lr: 0.000089  min_lr: 0.000001  loss: 4.0158 (4.0093)  loss_scale: 32768.0000 (34713.8432)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0012  max mem: 15572
Epoch: [10]  [ 430/1404]  eta: 0:10:14  lr: 0.000089  min_lr: 0.000001  loss: 4.0555 (4.0136)  loss_scale: 32768.0000 (34668.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0013  max mem: 15572
Epoch: [10]  [ 440/1404]  eta: 0:10:08  lr: 0.000089  min_lr: 0.000001  loss: 3.9987 (4.0096)  loss_scale: 32768.0000 (34625.5964)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0011  max mem: 15572
Epoch: [10]  [ 450/1404]  eta: 0:10:00  lr: 0.000089  min_lr: 0.000001  loss: 3.9400 (4.0126)  loss_scale: 32768.0000 (34584.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0012  max mem: 15572
Epoch: [10]  [ 460/1404]  eta: 0:09:55  lr: 0.000088  min_lr: 0.000001  loss: 4.0159 (4.0149)  loss_scale: 32768.0000 (34545.0065)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0008  max mem: 15572
[2025-01-10 18:16:06,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:16:06,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:16:06,424] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:16:06,425] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:16:08,697] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14509
[2025-01-10 18:16:08,698] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:16:08,699] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14509
[2025-01-10 18:16:08,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:16:08,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 470/1404]  eta: 0:09:49  lr: 0.000088  min_lr: 0.000001  loss: 3.9858 (4.0135)  loss_scale: 32768.0000 (34646.4204)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0006  max mem: 15572
Epoch: [10]  [ 480/1404]  eta: 0:09:42  lr: 0.000088  min_lr: 0.000001  loss: 3.9858 (4.0147)  loss_scale: 32768.0000 (34607.3680)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0008  max mem: 15572
Epoch: [10]  [ 490/1404]  eta: 0:09:36  lr: 0.000088  min_lr: 0.000001  loss: 4.1276 (4.0140)  loss_scale: 32768.0000 (34569.9063)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0008  max mem: 15572
Epoch: [10]  [ 500/1404]  eta: 0:09:27  lr: 0.000088  min_lr: 0.000001  loss: 4.1809 (4.0147)  loss_scale: 32768.0000 (34533.9401)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0007  max mem: 15572
Epoch: [10]  [ 510/1404]  eta: 0:09:19  lr: 0.000088  min_lr: 0.000001  loss: 4.0826 (4.0138)  loss_scale: 32768.0000 (34499.3816)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0007  max mem: 15572
Epoch: [10]  [ 520/1404]  eta: 0:09:13  lr: 0.000088  min_lr: 0.000001  loss: 4.0817 (4.0157)  loss_scale: 32768.0000 (34466.1497)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0009  max mem: 15572
Epoch: [10]  [ 530/1404]  eta: 0:09:06  lr: 0.000088  min_lr: 0.000001  loss: 3.9672 (4.0133)  loss_scale: 32768.0000 (34434.1695)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.0010  max mem: 15572
Epoch: [10]  [ 540/1404]  eta: 0:09:00  lr: 0.000088  min_lr: 0.000001  loss: 3.9662 (4.0127)  loss_scale: 32768.0000 (34403.3715)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0009  max mem: 15572
Epoch: [10]  [ 550/1404]  eta: 0:08:55  lr: 0.000088  min_lr: 0.000001  loss: 4.0094 (4.0130)  loss_scale: 32768.0000 (34373.6915)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0012  max mem: 15572
Epoch: [10]  [ 560/1404]  eta: 0:08:47  lr: 0.000088  min_lr: 0.000001  loss: 3.9530 (4.0130)  loss_scale: 32768.0000 (34345.0695)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0014  max mem: 15572
Epoch: [10]  [ 570/1404]  eta: 0:08:40  lr: 0.000088  min_lr: 0.000001  loss: 4.0256 (4.0189)  loss_scale: 32768.0000 (34317.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0011  max mem: 15572
Epoch: [10]  [ 580/1404]  eta: 0:08:34  lr: 0.000088  min_lr: 0.000001  loss: 4.1156 (4.0198)  loss_scale: 32768.0000 (34290.7814)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0009  max mem: 15572
Epoch: [10]  [ 590/1404]  eta: 0:08:29  lr: 0.000088  min_lr: 0.000001  loss: 4.0815 (4.0196)  loss_scale: 32768.0000 (34265.0152)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0008  max mem: 15572
[2025-01-10 18:17:26,709] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:17:26,710] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:17:26,760] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:17:26,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 600/1404]  eta: 0:08:22  lr: 0.000088  min_lr: 0.000001  loss: 4.0606 (4.0148)  loss_scale: 32768.0000 (34403.6739)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0010  max mem: 15572
[2025-01-10 18:17:30,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14645
[2025-01-10 18:17:30,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:17:30,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:17:30,483] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14645
[2025-01-10 18:17:30,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [ 610/1404]  eta: 0:08:16  lr: 0.000088  min_lr: 0.000001  loss: 4.0970 (4.0167)  loss_scale: 32768.0000 (34591.4239)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0010  max mem: 15572
Epoch: [10]  [ 620/1404]  eta: 0:08:09  lr: 0.000088  min_lr: 0.000001  loss: 4.2062 (4.0194)  loss_scale: 32768.0000 (34562.0612)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0010  max mem: 15572
Epoch: [10]  [ 630/1404]  eta: 0:08:04  lr: 0.000088  min_lr: 0.000001  loss: 3.7969 (4.0154)  loss_scale: 32768.0000 (34533.6292)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.0012  max mem: 15572
Epoch: [10]  [ 640/1404]  eta: 0:07:57  lr: 0.000088  min_lr: 0.000001  loss: 3.7656 (4.0131)  loss_scale: 32768.0000 (34506.0842)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0014  max mem: 15572
Epoch: [10]  [ 650/1404]  eta: 0:07:51  lr: 0.000088  min_lr: 0.000001  loss: 4.0120 (4.0140)  loss_scale: 32768.0000 (34479.3856)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0016  max mem: 15572
Epoch: [10]  [ 660/1404]  eta: 0:07:46  lr: 0.000088  min_lr: 0.000001  loss: 3.8141 (4.0074)  loss_scale: 32768.0000 (34453.4947)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.0014  max mem: 15572
Epoch: [10]  [ 670/1404]  eta: 0:07:39  lr: 0.000088  min_lr: 0.000001  loss: 3.7266 (4.0052)  loss_scale: 32768.0000 (34428.3756)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0012  max mem: 15572
Epoch: [10]  [ 680/1404]  eta: 0:07:32  lr: 0.000088  min_lr: 0.000001  loss: 3.8248 (4.0048)  loss_scale: 32768.0000 (34403.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0013  max mem: 15572
Epoch: [10]  [ 690/1404]  eta: 0:07:26  lr: 0.000088  min_lr: 0.000001  loss: 4.0090 (4.0045)  loss_scale: 32768.0000 (34380.3184)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.0026  max mem: 15572
Epoch: [10]  [ 700/1404]  eta: 0:07:20  lr: 0.000088  min_lr: 0.000001  loss: 4.0090 (4.0043)  loss_scale: 32768.0000 (34357.3181)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.0026  max mem: 15572
Epoch: [10]  [ 710/1404]  eta: 0:07:13  lr: 0.000088  min_lr: 0.000001  loss: 3.9898 (4.0037)  loss_scale: 32768.0000 (34334.9648)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0017  max mem: 15572
Epoch: [10]  [ 720/1404]  eta: 0:07:08  lr: 0.000088  min_lr: 0.000001  loss: 3.9949 (4.0041)  loss_scale: 32768.0000 (34313.2316)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0017  max mem: 15572
Epoch: [10]  [ 730/1404]  eta: 0:07:11  lr: 0.000088  min_lr: 0.000001  loss: 3.9907 (4.0034)  loss_scale: 32768.0000 (34292.0930)  weight_decay: 0.0500 (0.0500)  time: 1.1629  data: 0.5532  max mem: 15572
[2025-01-10 18:19:01,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:19:01,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:19:01,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:19:01,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:19:01,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14775
[2025-01-10 18:19:01,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:19:01,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:19:01,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14775
[2025-01-10 18:19:01,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [ 740/1404]  eta: 0:07:02  lr: 0.000088  min_lr: 0.000001  loss: 3.9330 (4.0023)  loss_scale: 32768.0000 (34315.7463)  weight_decay: 0.0500 (0.0500)  time: 1.0314  data: 0.5528  max mem: 15572
Epoch: [10]  [ 750/1404]  eta: 0:06:55  lr: 0.000088  min_lr: 0.000001  loss: 3.9864 (4.0042)  loss_scale: 32768.0000 (34295.1372)  weight_decay: 0.0500 (0.0500)  time: 0.4716  data: 0.0006  max mem: 15572
Epoch: [10]  [ 760/1404]  eta: 0:06:49  lr: 0.000088  min_lr: 0.000001  loss: 4.1478 (4.0073)  loss_scale: 32768.0000 (34275.0696)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0519  max mem: 15572
Epoch: [10]  [ 770/1404]  eta: 0:06:43  lr: 0.000088  min_lr: 0.000001  loss: 3.9147 (4.0054)  loss_scale: 32768.0000 (34255.5227)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.1266  max mem: 15572
Epoch: [10]  [ 780/1404]  eta: 0:06:36  lr: 0.000088  min_lr: 0.000001  loss: 3.9649 (4.0054)  loss_scale: 32768.0000 (34236.4763)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0754  max mem: 15572
Epoch: [10]  [ 790/1404]  eta: 0:06:29  lr: 0.000088  min_lr: 0.000001  loss: 4.0368 (4.0051)  loss_scale: 32768.0000 (34217.9115)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0450  max mem: 15572
Epoch: [10]  [ 800/1404]  eta: 0:06:23  lr: 0.000088  min_lr: 0.000001  loss: 4.1590 (4.0066)  loss_scale: 32768.0000 (34199.8102)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1399  max mem: 15572
Epoch: [10]  [ 810/1404]  eta: 0:06:17  lr: 0.000088  min_lr: 0.000001  loss: 4.0825 (4.0082)  loss_scale: 32768.0000 (34182.1554)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.1673  max mem: 15572
Epoch: [10]  [ 820/1404]  eta: 0:06:10  lr: 0.000088  min_lr: 0.000001  loss: 3.9237 (4.0055)  loss_scale: 32768.0000 (34164.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1245  max mem: 15572
Epoch: [10]  [ 830/1404]  eta: 0:06:03  lr: 0.000088  min_lr: 0.000001  loss: 3.9043 (4.0043)  loss_scale: 32768.0000 (34148.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0530  max mem: 15572
Epoch: [10]  [ 840/1404]  eta: 0:05:57  lr: 0.000088  min_lr: 0.000001  loss: 4.0222 (4.0051)  loss_scale: 32768.0000 (34131.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0533  max mem: 15572
Epoch: [10]  [ 850/1404]  eta: 0:05:50  lr: 0.000088  min_lr: 0.000001  loss: 4.0119 (4.0042)  loss_scale: 32768.0000 (34115.6851)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0927  max mem: 15572
Epoch: [10]  [ 860/1404]  eta: 0:05:43  lr: 0.000088  min_lr: 0.000001  loss: 3.9531 (4.0032)  loss_scale: 32768.0000 (34100.0325)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0799  max mem: 15572
[2025-01-10 18:20:18,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:20:18,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:20:18,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:20:18,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 870/1404]  eta: 0:05:37  lr: 0.000088  min_lr: 0.000001  loss: 3.7802 (4.0009)  loss_scale: 32768.0000 (34348.0873)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0802  max mem: 15572
[2025-01-10 18:20:26,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14918
[2025-01-10 18:20:26,555] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:20:26,572] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14918
[2025-01-10 18:20:26,572] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:20:26,572] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 880/1404]  eta: 0:05:30  lr: 0.000088  min_lr: 0.000001  loss: 3.7242 (4.0006)  loss_scale: 65536.0000 (34590.5108)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0940  max mem: 15572
Epoch: [10]  [ 890/1404]  eta: 0:05:24  lr: 0.000088  min_lr: 0.000001  loss: 3.9258 (4.0011)  loss_scale: 32768.0000 (34570.0561)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0764  max mem: 15572
Epoch: [10]  [ 900/1404]  eta: 0:05:17  lr: 0.000088  min_lr: 0.000001  loss: 4.0784 (4.0027)  loss_scale: 32768.0000 (34550.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0842  max mem: 15572
Epoch: [10]  [ 910/1404]  eta: 0:05:11  lr: 0.000088  min_lr: 0.000001  loss: 4.0175 (4.0018)  loss_scale: 32768.0000 (34530.4940)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1388  max mem: 15572
Epoch: [10]  [ 920/1404]  eta: 0:05:05  lr: 0.000088  min_lr: 0.000001  loss: 4.0414 (4.0033)  loss_scale: 32768.0000 (34511.3572)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1389  max mem: 15572
Epoch: [10]  [ 930/1404]  eta: 0:04:58  lr: 0.000088  min_lr: 0.000001  loss: 4.0644 (4.0015)  loss_scale: 32768.0000 (34492.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1191  max mem: 15572
Epoch: [10]  [ 940/1404]  eta: 0:04:52  lr: 0.000088  min_lr: 0.000001  loss: 3.8388 (4.0004)  loss_scale: 32768.0000 (34474.3039)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0943  max mem: 15572
Epoch: [10]  [ 950/1404]  eta: 0:04:45  lr: 0.000088  min_lr: 0.000001  loss: 3.9554 (3.9995)  loss_scale: 32768.0000 (34456.3617)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0747  max mem: 15572
[2025-01-10 18:21:16,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=89, lr=[8.505727956044617e-07, 8.505727956044617e-07, 1.2151039937206597e-06, 1.2151039937206597e-06, 1.7358628481723713e-06, 1.7358628481723713e-06, 2.4798040688176734e-06, 2.4798040688176734e-06, 3.542577241168105e-06, 3.542577241168105e-06, 5.0608246302401504e-06, 5.0608246302401504e-06, 7.229749471771644e-06, 7.229749471771644e-06, 1.032821353110235e-05, 1.032821353110235e-05, 1.4754590758717642e-05, 1.4754590758717642e-05, 2.1077986798168063e-05, 2.1077986798168063e-05, 3.011140971166866e-05, 3.011140971166866e-05, 4.3016299588098086e-05, 4.3016299588098086e-05, 6.145185655442585e-05, 6.145185655442585e-05, 8.778836650632264e-05, 8.778836650632264e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 18:21:16,636] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=45.1221087583404, CurrSamplesPerSec=50.52008792797133, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 960/1404]  eta: 0:04:39  lr: 0.000088  min_lr: 0.000001  loss: 4.0678 (3.9997)  loss_scale: 32768.0000 (34438.7929)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1064  max mem: 15572
Epoch: [10]  [ 970/1404]  eta: 0:04:33  lr: 0.000088  min_lr: 0.000001  loss: 4.1389 (4.0007)  loss_scale: 32768.0000 (34421.5860)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1284  max mem: 15572
Epoch: [10]  [ 980/1404]  eta: 0:04:26  lr: 0.000088  min_lr: 0.000001  loss: 4.0729 (3.9996)  loss_scale: 32768.0000 (34404.7299)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0989  max mem: 15572
Epoch: [10]  [ 990/1404]  eta: 0:04:20  lr: 0.000088  min_lr: 0.000001  loss: 3.8580 (3.9993)  loss_scale: 32768.0000 (34388.2139)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0874  max mem: 15572
Epoch: [10]  [1000/1404]  eta: 0:04:14  lr: 0.000088  min_lr: 0.000001  loss: 3.9881 (3.9982)  loss_scale: 32768.0000 (34372.0280)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1198  max mem: 15572
[2025-01-10 18:21:45,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:21:45,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:21:45,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:21:45,706] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:21:48,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15050
[2025-01-10 18:21:48,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:21:48,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1010/1404]  eta: 0:04:07  lr: 0.000088  min_lr: 0.000001  loss: 3.9098 (3.9979)  loss_scale: 32768.0000 (34453.3966)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1271  max mem: 15572
[2025-01-10 18:21:48,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15050
[2025-01-10 18:21:48,174] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [1020/1404]  eta: 0:04:01  lr: 0.000088  min_lr: 0.000001  loss: 3.9790 (3.9985)  loss_scale: 32768.0000 (34436.8893)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.1573  max mem: 15572
Epoch: [10]  [1030/1404]  eta: 0:03:55  lr: 0.000088  min_lr: 0.000001  loss: 3.9249 (3.9973)  loss_scale: 32768.0000 (34420.7022)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1359  max mem: 15572
Epoch: [10]  [1040/1404]  eta: 0:03:48  lr: 0.000088  min_lr: 0.000001  loss: 4.0013 (3.9976)  loss_scale: 32768.0000 (34404.8261)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0644  max mem: 15572
Epoch: [10]  [1050/1404]  eta: 0:03:42  lr: 0.000088  min_lr: 0.000001  loss: 4.0356 (3.9984)  loss_scale: 32768.0000 (34389.2521)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0995  max mem: 15572
Epoch: [10]  [1060/1404]  eta: 0:03:35  lr: 0.000088  min_lr: 0.000001  loss: 4.0007 (3.9968)  loss_scale: 32768.0000 (34373.9717)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0786  max mem: 15572
Epoch: [10]  [1070/1404]  eta: 0:03:29  lr: 0.000088  min_lr: 0.000001  loss: 3.8831 (3.9963)  loss_scale: 32768.0000 (34358.9767)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0693  max mem: 15572
Epoch: [10]  [1080/1404]  eta: 0:03:23  lr: 0.000088  min_lr: 0.000001  loss: 3.8946 (3.9946)  loss_scale: 32768.0000 (34344.2590)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1293  max mem: 15572
Epoch: [10]  [1090/1404]  eta: 0:03:17  lr: 0.000088  min_lr: 0.000001  loss: 3.9344 (3.9935)  loss_scale: 32768.0000 (34329.8112)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1050  max mem: 15572
Epoch: [10]  [1100/1404]  eta: 0:03:10  lr: 0.000088  min_lr: 0.000001  loss: 3.7848 (3.9931)  loss_scale: 32768.0000 (34315.6258)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0414  max mem: 15572
Epoch: [10]  [1110/1404]  eta: 0:03:04  lr: 0.000088  min_lr: 0.000001  loss: 4.0639 (3.9957)  loss_scale: 32768.0000 (34301.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0568  max mem: 15572
Epoch: [10]  [1120/1404]  eta: 0:02:58  lr: 0.000088  min_lr: 0.000001  loss: 4.1154 (3.9945)  loss_scale: 32768.0000 (34288.0143)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0515  max mem: 15572
Epoch: [10]  [1130/1404]  eta: 0:02:51  lr: 0.000088  min_lr: 0.000001  loss: 4.0303 (3.9955)  loss_scale: 32768.0000 (34274.5747)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0094  max mem: 15572
[2025-01-10 18:23:07,924] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:23:07,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:23:07,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:23:07,929] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1140/1404]  eta: 0:02:45  lr: 0.000088  min_lr: 0.000001  loss: 4.1801 (3.9968)  loss_scale: 32768.0000 (34318.8081)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.0639  max mem: 15572
Epoch: [10]  [1150/1404]  eta: 0:02:39  lr: 0.000088  min_lr: 0.000001  loss: 4.1363 (3.9973)  loss_scale: 65536.0000 (34590.0261)  weight_decay: 0.0500 (0.0500)  time: 0.6816  data: 0.1170  max mem: 15572
[2025-01-10 18:23:19,208] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15199
[2025-01-10 18:23:19,208] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:23:19,215] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15199
[2025-01-10 18:23:19,216] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:23:19,216] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1160/1404]  eta: 0:02:32  lr: 0.000087  min_lr: 0.000001  loss: 4.0143 (3.9971)  loss_scale: 65536.0000 (34800.1240)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0624  max mem: 15572
Epoch: [10]  [1170/1404]  eta: 0:02:26  lr: 0.000087  min_lr: 0.000001  loss: 3.9851 (3.9967)  loss_scale: 32768.0000 (34782.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0129  max mem: 15572
Epoch: [10]  [1180/1404]  eta: 0:02:20  lr: 0.000087  min_lr: 0.000001  loss: 4.0678 (3.9975)  loss_scale: 32768.0000 (34765.7104)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0136  max mem: 15572
Epoch: [10]  [1190/1404]  eta: 0:02:14  lr: 0.000087  min_lr: 0.000001  loss: 4.0678 (3.9974)  loss_scale: 32768.0000 (34748.9370)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0171  max mem: 15572
Epoch: [10]  [1200/1404]  eta: 0:02:07  lr: 0.000087  min_lr: 0.000001  loss: 3.9549 (3.9974)  loss_scale: 32768.0000 (34732.4430)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0169  max mem: 15572
Epoch: [10]  [1210/1404]  eta: 0:02:01  lr: 0.000087  min_lr: 0.000001  loss: 4.1432 (3.9985)  loss_scale: 32768.0000 (34716.2213)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0013  max mem: 15572
Epoch: [10]  [1220/1404]  eta: 0:01:55  lr: 0.000087  min_lr: 0.000001  loss: 4.1076 (3.9977)  loss_scale: 32768.0000 (34700.2654)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0010  max mem: 15572
Epoch: [10]  [1230/1404]  eta: 0:01:49  lr: 0.000087  min_lr: 0.000001  loss: 3.9422 (3.9967)  loss_scale: 32768.0000 (34684.5686)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0009  max mem: 15572
Epoch: [10]  [1240/1404]  eta: 0:01:42  lr: 0.000087  min_lr: 0.000001  loss: 4.0866 (3.9985)  loss_scale: 32768.0000 (34669.1249)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.0556  max mem: 15572
Epoch: [10]  [1250/1404]  eta: 0:01:36  lr: 0.000087  min_lr: 0.000001  loss: 4.1513 (3.9989)  loss_scale: 32768.0000 (34653.9281)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1065  max mem: 15572
Epoch: [10]  [1260/1404]  eta: 0:01:30  lr: 0.000087  min_lr: 0.000001  loss: 3.9431 (3.9997)  loss_scale: 32768.0000 (34638.9722)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0920  max mem: 15572
Epoch: [10]  [1270/1404]  eta: 0:01:23  lr: 0.000087  min_lr: 0.000001  loss: 4.0631 (3.9995)  loss_scale: 32768.0000 (34624.2518)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0788  max mem: 15572
Epoch: [10]  [1280/1404]  eta: 0:01:17  lr: 0.000087  min_lr: 0.000001  loss: 4.1966 (3.9999)  loss_scale: 32768.0000 (34609.7611)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0828  max mem: 15572
[2025-01-10 18:24:39,362] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:24:39,362] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:24:39,363] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:24:39,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1290/1404]  eta: 0:01:11  lr: 0.000087  min_lr: 0.000001  loss: 3.9771 (3.9975)  loss_scale: 32768.0000 (34671.6406)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0455  max mem: 15572
Epoch: [10]  [1300/1404]  eta: 0:01:05  lr: 0.000087  min_lr: 0.000001  loss: 3.9377 (3.9974)  loss_scale: 65536.0000 (34908.8762)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0353  max mem: 15572
[2025-01-10 18:24:48,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15344
[2025-01-10 18:24:48,408] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:24:48,408] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:24:48,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15344
[2025-01-10 18:24:48,421] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [1310/1404]  eta: 0:00:58  lr: 0.000087  min_lr: 0.000001  loss: 3.9872 (3.9969)  loss_scale: 65536.0000 (34967.5301)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0349  max mem: 15572
Epoch: [10]  [1320/1404]  eta: 0:00:52  lr: 0.000087  min_lr: 0.000001  loss: 3.8999 (3.9971)  loss_scale: 32768.0000 (34950.8796)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.0723  max mem: 15572
Epoch: [10]  [1330/1404]  eta: 0:00:46  lr: 0.000087  min_lr: 0.000001  loss: 3.8764 (3.9970)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1067  max mem: 15572
Epoch: [10]  [1340/1404]  eta: 0:00:40  lr: 0.000087  min_lr: 0.000001  loss: 3.9719 (3.9977)  loss_scale: 32768.0000 (34918.3236)  weight_decay: 0.0500 (0.0500)  time: 0.9419  data: 0.3993  max mem: 15572
Epoch: [10]  [1350/1404]  eta: 0:00:33  lr: 0.000087  min_lr: 0.000001  loss: 3.9719 (3.9963)  loss_scale: 32768.0000 (34902.4071)  weight_decay: 0.0500 (0.0500)  time: 0.8569  data: 0.3649  max mem: 15572
Epoch: [10]  [1360/1404]  eta: 0:00:27  lr: 0.000087  min_lr: 0.000001  loss: 3.8507 (3.9950)  loss_scale: 32768.0000 (34886.7245)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0007  max mem: 15572
Epoch: [10]  [1370/1404]  eta: 0:00:21  lr: 0.000087  min_lr: 0.000001  loss: 3.9573 (3.9948)  loss_scale: 32768.0000 (34871.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0078  max mem: 15572
Epoch: [10]  [1380/1404]  eta: 0:00:15  lr: 0.000087  min_lr: 0.000001  loss: 4.0690 (3.9964)  loss_scale: 32768.0000 (34856.0406)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0186  max mem: 15572
Epoch: [10]  [1390/1404]  eta: 0:00:08  lr: 0.000087  min_lr: 0.000001  loss: 4.0823 (3.9953)  loss_scale: 32768.0000 (34841.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0468  max mem: 15572
Epoch: [10]  [1400/1404]  eta: 0:00:02  lr: 0.000087  min_lr: 0.000001  loss: 3.9216 (3.9961)  loss_scale: 32768.0000 (34826.2327)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0357  max mem: 15572
Epoch: [10]  [1403/1404]  eta: 0:00:00  lr: 0.000087  min_lr: 0.000001  loss: 3.9216 (3.9968)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)  time: 0.4684  data: 0.0356  max mem: 15572
Epoch: [10] Total time: 0:14:38 (0.6255 s / it)
Averaged stats: lr: 0.000087  min_lr: 0.000001  loss: 3.9216 (3.9989)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:34  loss: 1.5985 (1.5985)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 4.6643  data: 4.4799  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 3.0379 (2.8012)  acc1: 27.7778 (32.3232)  acc5: 61.1111 (61.1111)  time: 0.8125  data: 0.5979  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 2.9584 (2.8839)  acc1: 27.7778 (31.2169)  acc5: 66.6667 (63.7566)  time: 0.3991  data: 0.1853  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.7670 (2.6182)  acc1: 33.3333 (37.8136)  acc5: 72.2222 (69.1756)  time: 0.3106  data: 0.0808  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.0371 (2.5581)  acc1: 44.4444 (38.3469)  acc5: 77.7778 (70.7317)  time: 0.3159  data: 0.0837  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.4352 (2.6029)  acc1: 33.3333 (37.7996)  acc5: 77.7778 (70.1525)  time: 0.4051  data: 0.1919  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.8224 (2.6895)  acc1: 27.7778 (34.9727)  acc5: 66.6667 (68.6703)  time: 0.4224  data: 0.2125  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.7014 (2.6675)  acc1: 33.3333 (36.2285)  acc5: 66.6667 (68.4664)  time: 0.4291  data: 0.2187  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.6421 (2.6704)  acc1: 33.3333 (35.6653)  acc5: 66.6667 (69.2730)  time: 0.4264  data: 0.2065  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.6674 (2.6821)  acc1: 27.7778 (34.9206)  acc5: 66.6667 (69.1087)  time: 0.3425  data: 0.1289  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 2.9410 (2.7550)  acc1: 22.2222 (33.1133)  acc5: 61.1111 (67.2717)  time: 0.3220  data: 0.1133  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.8526 (2.7455)  acc1: 22.2222 (33.8839)  acc5: 66.6667 (67.6677)  time: 0.3804  data: 0.1641  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2913 (2.6692)  acc1: 44.4444 (35.8586)  acc5: 77.7778 (69.3756)  time: 0.3455  data: 0.1344  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9693 (2.6181)  acc1: 55.5556 (37.5318)  acc5: 83.3333 (69.8473)  time: 0.2450  data: 0.0668  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1260 (2.6204)  acc1: 50.0000 (37.3874)  acc5: 77.7778 (69.8608)  time: 0.2063  data: 0.0516  max mem: 15572
Val: Total time: 0:00:52 (0.3826 s / it)
* Acc@1 37.469 Acc@5 69.021 loss 2.656
Accuracy of the network on the 4883 val videos: 37.5%
[2025-01-10 18:26:42,242] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 18:26:42,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 18:26:42,245] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 18:26:42,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 18:26:45,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 18:26:45,061] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.47%
Epoch: [11]  [   0/1404]  eta: 3:04:24  lr: 0.000087  min_lr: 0.000001  loss: 4.1476 (4.1476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8810  data: 7.3439  max mem: 15572
Epoch: [11]  [  10/1404]  eta: 0:30:33  lr: 0.000087  min_lr: 0.000001  loss: 3.9777 (3.9789)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3155  data: 0.7252  max mem: 15572
Epoch: [11]  [  20/1404]  eta: 0:23:14  lr: 0.000087  min_lr: 0.000001  loss: 3.9100 (3.8804)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0320  max mem: 15572
[2025-01-10 18:27:10,681] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:27:10,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:27:10,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:27:10,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [  30/1404]  eta: 0:19:39  lr: 0.000087  min_lr: 0.000001  loss: 3.6651 (3.8320)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0006  max mem: 15572
[2025-01-10 18:27:13,505] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15478
[2025-01-10 18:27:13,505] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:27:13,583] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15478
[2025-01-10 18:27:13,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:27:13,584] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [  40/1404]  eta: 0:18:03  lr: 0.000087  min_lr: 0.000001  loss: 3.7322 (3.8773)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0007  max mem: 15572
Epoch: [11]  [  50/1404]  eta: 0:16:53  lr: 0.000087  min_lr: 0.000001  loss: 4.1123 (3.8984)  loss_scale: 32768.0000 (35980.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0034  max mem: 15572
Epoch: [11]  [  60/1404]  eta: 0:16:20  lr: 0.000087  min_lr: 0.000001  loss: 3.9668 (3.9105)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0033  max mem: 15572
Epoch: [11]  [  70/1404]  eta: 0:16:00  lr: 0.000087  min_lr: 0.000001  loss: 4.0167 (3.9238)  loss_scale: 32768.0000 (35075.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.0009  max mem: 15572
Epoch: [11]  [  80/1404]  eta: 0:15:33  lr: 0.000087  min_lr: 0.000001  loss: 4.0167 (3.9423)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0012  max mem: 15572
Epoch: [11]  [  90/1404]  eta: 0:15:14  lr: 0.000087  min_lr: 0.000001  loss: 3.9156 (3.9271)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0011  max mem: 15572
Epoch: [11]  [ 100/1404]  eta: 0:14:58  lr: 0.000087  min_lr: 0.000001  loss: 3.8919 (3.9322)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0010  max mem: 15572
Epoch: [11]  [ 110/1404]  eta: 0:14:36  lr: 0.000087  min_lr: 0.000001  loss: 4.1728 (3.9566)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0012  max mem: 15572
Epoch: [11]  [ 120/1404]  eta: 0:14:16  lr: 0.000087  min_lr: 0.000001  loss: 4.1939 (3.9666)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0011  max mem: 15572
Epoch: [11]  [ 130/1404]  eta: 0:14:04  lr: 0.000087  min_lr: 0.000001  loss: 3.9918 (3.9690)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0009  max mem: 15572
Epoch: [11]  [ 140/1404]  eta: 0:13:49  lr: 0.000087  min_lr: 0.000001  loss: 3.8906 (3.9608)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0009  max mem: 15572
Epoch: [11]  [ 150/1404]  eta: 0:13:42  lr: 0.000087  min_lr: 0.000001  loss: 3.8281 (3.9493)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0009  max mem: 15572
Epoch: [11]  [ 160/1404]  eta: 0:13:32  lr: 0.000087  min_lr: 0.000001  loss: 3.8281 (3.9430)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0011  max mem: 15572
[2025-01-10 18:28:32,736] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:28:32,736] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:28:32,740] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:28:32,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:28:33,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15609
[2025-01-10 18:28:33,881] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:28:33,883] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15609
[2025-01-10 18:28:33,884] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:28:33,884] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 170/1404]  eta: 0:13:24  lr: 0.000087  min_lr: 0.000001  loss: 3.8982 (3.9424)  loss_scale: 32768.0000 (34109.3801)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0011  max mem: 15572
Epoch: [11]  [ 180/1404]  eta: 0:13:15  lr: 0.000087  min_lr: 0.000001  loss: 4.0797 (3.9552)  loss_scale: 32768.0000 (34035.2707)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0010  max mem: 15572
Epoch: [11]  [ 190/1404]  eta: 0:13:08  lr: 0.000087  min_lr: 0.000001  loss: 4.0140 (3.9506)  loss_scale: 32768.0000 (33968.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0010  max mem: 15572
Epoch: [11]  [ 200/1404]  eta: 0:13:03  lr: 0.000087  min_lr: 0.000001  loss: 3.8826 (3.9477)  loss_scale: 32768.0000 (33909.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.0010  max mem: 15572
Epoch: [11]  [ 210/1404]  eta: 0:12:57  lr: 0.000087  min_lr: 0.000001  loss: 4.1070 (3.9532)  loss_scale: 32768.0000 (33855.0900)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0009  max mem: 15572
Epoch: [11]  [ 220/1404]  eta: 0:12:48  lr: 0.000087  min_lr: 0.000001  loss: 4.1070 (3.9556)  loss_scale: 32768.0000 (33805.9005)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0008  max mem: 15572
Epoch: [11]  [ 230/1404]  eta: 0:12:43  lr: 0.000087  min_lr: 0.000001  loss: 4.1307 (3.9619)  loss_scale: 32768.0000 (33760.9697)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.0009  max mem: 15572
Epoch: [11]  [ 240/1404]  eta: 0:12:37  lr: 0.000087  min_lr: 0.000001  loss: 3.9407 (3.9522)  loss_scale: 32768.0000 (33719.7676)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.0010  max mem: 15572
Epoch: [11]  [ 250/1404]  eta: 0:12:27  lr: 0.000087  min_lr: 0.000001  loss: 3.8662 (3.9592)  loss_scale: 32768.0000 (33681.8486)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0011  max mem: 15572
Epoch: [11]  [ 260/1404]  eta: 0:12:21  lr: 0.000087  min_lr: 0.000001  loss: 3.9390 (3.9621)  loss_scale: 32768.0000 (33646.8352)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0011  max mem: 15572
Epoch: [11]  [ 270/1404]  eta: 0:12:12  lr: 0.000087  min_lr: 0.000001  loss: 4.1871 (3.9664)  loss_scale: 32768.0000 (33614.4059)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0012  max mem: 15572
Epoch: [11]  [ 280/1404]  eta: 0:12:03  lr: 0.000087  min_lr: 0.000001  loss: 4.0547 (3.9631)  loss_scale: 32768.0000 (33584.2847)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0013  max mem: 15572
Epoch: [11]  [ 290/1404]  eta: 0:11:56  lr: 0.000087  min_lr: 0.000001  loss: 4.0413 (3.9667)  loss_scale: 32768.0000 (33556.2337)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0012  max mem: 15572
[2025-01-10 18:29:54,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:29:54,888] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:29:54,938] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:29:54,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 300/1404]  eta: 0:11:47  lr: 0.000087  min_lr: 0.000001  loss: 4.0756 (3.9654)  loss_scale: 32768.0000 (34292.0930)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0010  max mem: 15572
[2025-01-10 18:30:03,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15754
[2025-01-10 18:30:03,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:30:03,765] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15754
[2025-01-10 18:30:03,765] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:30:03,765] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 310/1404]  eta: 0:11:38  lr: 0.000087  min_lr: 0.000001  loss: 4.0194 (3.9670)  loss_scale: 65536.0000 (35191.3569)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0010  max mem: 15572
Epoch: [11]  [ 320/1404]  eta: 0:11:31  lr: 0.000087  min_lr: 0.000001  loss: 3.8930 (3.9646)  loss_scale: 32768.0000 (35115.8629)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0010  max mem: 15572
Epoch: [11]  [ 330/1404]  eta: 0:11:24  lr: 0.000087  min_lr: 0.000001  loss: 3.9670 (3.9647)  loss_scale: 32768.0000 (35044.9305)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0008  max mem: 15572
Epoch: [11]  [ 340/1404]  eta: 0:11:15  lr: 0.000087  min_lr: 0.000001  loss: 3.9750 (3.9661)  loss_scale: 32768.0000 (34978.1584)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0008  max mem: 15572
Epoch: [11]  [ 350/1404]  eta: 0:11:09  lr: 0.000087  min_lr: 0.000001  loss: 4.0552 (3.9664)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0010  max mem: 15572
Epoch: [11]  [ 360/1404]  eta: 0:11:03  lr: 0.000087  min_lr: 0.000001  loss: 4.0564 (3.9616)  loss_scale: 32768.0000 (34855.7119)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.0009  max mem: 15572
Epoch: [11]  [ 370/1404]  eta: 0:10:59  lr: 0.000087  min_lr: 0.000001  loss: 3.8601 (3.9645)  loss_scale: 32768.0000 (34799.4394)  weight_decay: 0.0500 (0.0500)  time: 0.6812  data: 0.0009  max mem: 15572
Epoch: [11]  [ 380/1404]  eta: 0:10:49  lr: 0.000087  min_lr: 0.000001  loss: 4.2392 (3.9717)  loss_scale: 32768.0000 (34746.1207)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0008  max mem: 15572
Epoch: [11]  [ 390/1404]  eta: 0:10:40  lr: 0.000087  min_lr: 0.000001  loss: 4.2019 (3.9730)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0007  max mem: 15572
Epoch: [11]  [ 400/1404]  eta: 0:10:34  lr: 0.000086  min_lr: 0.000001  loss: 3.9686 (3.9757)  loss_scale: 32768.0000 (34647.4613)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0008  max mem: 15572
Epoch: [11]  [ 410/1404]  eta: 0:10:28  lr: 0.000086  min_lr: 0.000001  loss: 3.9643 (3.9742)  loss_scale: 32768.0000 (34601.7324)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0008  max mem: 15572
Epoch: [11]  [ 420/1404]  eta: 0:10:20  lr: 0.000086  min_lr: 0.000001  loss: 4.0660 (3.9765)  loss_scale: 32768.0000 (34558.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0008  max mem: 15572
Epoch: [11]  [ 430/1404]  eta: 0:10:14  lr: 0.000086  min_lr: 0.000001  loss: 4.0491 (3.9764)  loss_scale: 32768.0000 (34516.6404)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0186  max mem: 15572
[2025-01-10 18:31:22,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:31:22,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:31:22,502] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:31:22,502] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 440/1404]  eta: 0:10:07  lr: 0.000086  min_lr: 0.000001  loss: 4.0816 (3.9821)  loss_scale: 32768.0000 (34625.5964)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0364  max mem: 15572
[2025-01-10 18:31:23,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15885
[2025-01-10 18:31:23,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:31:23,414] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15885
[2025-01-10 18:31:23,414] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:31:23,414] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 450/1404]  eta: 0:10:00  lr: 0.000086  min_lr: 0.000001  loss: 4.1654 (3.9835)  loss_scale: 32768.0000 (34584.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0627  max mem: 15572
Epoch: [11]  [ 460/1404]  eta: 0:09:53  lr: 0.000086  min_lr: 0.000001  loss: 3.9923 (3.9823)  loss_scale: 32768.0000 (34545.0065)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0699  max mem: 15572
Epoch: [11]  [ 470/1404]  eta: 0:09:46  lr: 0.000086  min_lr: 0.000001  loss: 3.8529 (3.9784)  loss_scale: 32768.0000 (34507.2781)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0745  max mem: 15572
Epoch: [11]  [ 480/1404]  eta: 0:09:40  lr: 0.000086  min_lr: 0.000001  loss: 3.7813 (3.9737)  loss_scale: 32768.0000 (34471.1185)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1088  max mem: 15572
Epoch: [11]  [ 490/1404]  eta: 0:09:33  lr: 0.000086  min_lr: 0.000001  loss: 3.7244 (3.9697)  loss_scale: 32768.0000 (34436.4318)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1041  max mem: 15572
Epoch: [11]  [ 500/1404]  eta: 0:09:28  lr: 0.000086  min_lr: 0.000001  loss: 3.8177 (3.9683)  loss_scale: 32768.0000 (34403.1297)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1163  max mem: 15572
Epoch: [11]  [ 510/1404]  eta: 0:09:20  lr: 0.000086  min_lr: 0.000001  loss: 3.8916 (3.9667)  loss_scale: 32768.0000 (34371.1311)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0886  max mem: 15572
Epoch: [11]  [ 520/1404]  eta: 0:09:14  lr: 0.000086  min_lr: 0.000001  loss: 3.9982 (3.9647)  loss_scale: 32768.0000 (34340.3608)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0964  max mem: 15572
Epoch: [11]  [ 530/1404]  eta: 0:09:08  lr: 0.000086  min_lr: 0.000001  loss: 4.0345 (3.9646)  loss_scale: 32768.0000 (34310.7495)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1417  max mem: 15572
Epoch: [11]  [ 540/1404]  eta: 0:09:01  lr: 0.000086  min_lr: 0.000001  loss: 3.8066 (3.9604)  loss_scale: 32768.0000 (34282.2329)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0978  max mem: 15572
Epoch: [11]  [ 550/1404]  eta: 0:08:55  lr: 0.000086  min_lr: 0.000001  loss: 3.7781 (3.9609)  loss_scale: 32768.0000 (34254.7514)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0364  max mem: 15572
[2025-01-10 18:32:33,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=96, lr=[8.35610443572728e-07, 8.35610443572728e-07, 1.1937292051038973e-06, 1.1937292051038973e-06, 1.7053274358627106e-06, 1.7053274358627106e-06, 2.436182051232444e-06, 2.436182051232444e-06, 3.480260073189206e-06, 3.480260073189206e-06, 4.9718001045560085e-06, 4.9718001045560085e-06, 7.102571577937155e-06, 7.102571577937155e-06, 1.0146530825624509e-05, 1.0146530825624509e-05, 1.4495044036606441e-05, 1.4495044036606441e-05, 2.070720576658063e-05, 2.070720576658063e-05, 2.9581722523686616e-05, 2.9581722523686616e-05, 4.2259603605266596e-05, 4.2259603605266596e-05, 6.0370862293238e-05, 6.0370862293238e-05, 8.624408899034001e-05, 8.624408899034001e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 18:32:33,576] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=45.07520335641192, CurrSamplesPerSec=48.60868872383916, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 560/1404]  eta: 0:08:47  lr: 0.000086  min_lr: 0.000001  loss: 4.0002 (3.9655)  loss_scale: 32768.0000 (34228.2496)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0010  max mem: 15572
[2025-01-10 18:32:42,335] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:32:42,336] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 570/1404]  eta: 0:08:41  lr: 0.000086  min_lr: 0.000001  loss: 3.9508 (3.9601)  loss_scale: 32768.0000 (34260.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0679  max mem: 15572
[2025-01-10 18:32:42,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:32:42,374] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:32:45,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16019
[2025-01-10 18:32:45,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:32:45,556] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16019
[2025-01-10 18:32:45,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:32:45,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 580/1404]  eta: 0:08:34  lr: 0.000086  min_lr: 0.000001  loss: 3.9063 (3.9612)  loss_scale: 32768.0000 (34459.9793)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1069  max mem: 15572
Epoch: [11]  [ 590/1404]  eta: 0:08:28  lr: 0.000086  min_lr: 0.000001  loss: 3.9839 (3.9625)  loss_scale: 32768.0000 (34431.3503)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0986  max mem: 15572
Epoch: [11]  [ 600/1404]  eta: 0:08:22  lr: 0.000086  min_lr: 0.000001  loss: 3.8337 (3.9595)  loss_scale: 32768.0000 (34403.6739)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1017  max mem: 15572
Epoch: [11]  [ 610/1404]  eta: 0:08:16  lr: 0.000086  min_lr: 0.000001  loss: 3.8082 (3.9588)  loss_scale: 32768.0000 (34376.9034)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1468  max mem: 15572
Epoch: [11]  [ 620/1404]  eta: 0:08:10  lr: 0.000086  min_lr: 0.000001  loss: 3.9467 (3.9605)  loss_scale: 32768.0000 (34350.9952)  weight_decay: 0.0500 (0.0500)  time: 0.6813  data: 0.1867  max mem: 15572
Epoch: [11]  [ 630/1404]  eta: 0:08:03  lr: 0.000086  min_lr: 0.000001  loss: 4.1042 (3.9609)  loss_scale: 32768.0000 (34325.9081)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1048  max mem: 15572
Epoch: [11]  [ 640/1404]  eta: 0:07:57  lr: 0.000086  min_lr: 0.000001  loss: 4.1398 (3.9638)  loss_scale: 32768.0000 (34301.6037)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0736  max mem: 15572
Epoch: [11]  [ 650/1404]  eta: 0:07:51  lr: 0.000086  min_lr: 0.000001  loss: 4.0185 (3.9611)  loss_scale: 32768.0000 (34278.0461)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1223  max mem: 15572
Epoch: [11]  [ 660/1404]  eta: 0:07:44  lr: 0.000086  min_lr: 0.000001  loss: 3.9313 (3.9591)  loss_scale: 32768.0000 (34255.2012)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1311  max mem: 15572
Epoch: [11]  [ 670/1404]  eta: 0:07:38  lr: 0.000086  min_lr: 0.000001  loss: 3.7416 (3.9573)  loss_scale: 32768.0000 (34233.0373)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1223  max mem: 15572
Epoch: [11]  [ 680/1404]  eta: 0:07:32  lr: 0.000086  min_lr: 0.000001  loss: 3.8001 (3.9556)  loss_scale: 32768.0000 (34211.5242)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0632  max mem: 15572
Epoch: [11]  [ 690/1404]  eta: 0:07:27  lr: 0.000086  min_lr: 0.000001  loss: 3.9107 (3.9566)  loss_scale: 32768.0000 (34190.6339)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.0014  max mem: 15572
Epoch: [11]  [ 700/1404]  eta: 0:07:20  lr: 0.000086  min_lr: 0.000001  loss: 4.0129 (3.9581)  loss_scale: 32768.0000 (34170.3395)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0012  max mem: 15572
[2025-01-10 18:34:06,459] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:34:06,459] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:34:06,504] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:34:06,505] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:34:08,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16151
[2025-01-10 18:34:08,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:34:08,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16151
[2025-01-10 18:34:08,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:34:08,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 710/1404]  eta: 0:07:13  lr: 0.000086  min_lr: 0.000001  loss: 3.9598 (3.9577)  loss_scale: 32768.0000 (34288.8776)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0011  max mem: 15572
Epoch: [11]  [ 720/1404]  eta: 0:07:07  lr: 0.000086  min_lr: 0.000001  loss: 4.0441 (3.9596)  loss_scale: 32768.0000 (34267.7836)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0013  max mem: 15572
Epoch: [11]  [ 730/1404]  eta: 0:07:01  lr: 0.000086  min_lr: 0.000001  loss: 3.9616 (3.9553)  loss_scale: 32768.0000 (34247.2668)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0013  max mem: 15572
Epoch: [11]  [ 740/1404]  eta: 0:06:54  lr: 0.000086  min_lr: 0.000001  loss: 3.6826 (3.9530)  loss_scale: 32768.0000 (34227.3036)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0013  max mem: 15572
Epoch: [11]  [ 750/1404]  eta: 0:06:48  lr: 0.000086  min_lr: 0.000001  loss: 3.8294 (3.9550)  loss_scale: 32768.0000 (34207.8722)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0013  max mem: 15572
Epoch: [11]  [ 760/1404]  eta: 0:06:41  lr: 0.000086  min_lr: 0.000001  loss: 4.1411 (3.9559)  loss_scale: 32768.0000 (34188.9514)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0018  max mem: 15572
Epoch: [11]  [ 770/1404]  eta: 0:06:35  lr: 0.000086  min_lr: 0.000001  loss: 3.7790 (3.9529)  loss_scale: 32768.0000 (34170.5214)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0020  max mem: 15572
Epoch: [11]  [ 780/1404]  eta: 0:06:29  lr: 0.000086  min_lr: 0.000001  loss: 3.9025 (3.9536)  loss_scale: 32768.0000 (34152.5634)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0012  max mem: 15572
Epoch: [11]  [ 790/1404]  eta: 0:06:23  lr: 0.000086  min_lr: 0.000001  loss: 4.0228 (3.9548)  loss_scale: 32768.0000 (34135.0594)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0011  max mem: 15572
Epoch: [11]  [ 800/1404]  eta: 0:06:17  lr: 0.000086  min_lr: 0.000001  loss: 4.0084 (3.9562)  loss_scale: 32768.0000 (34117.9925)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.0013  max mem: 15572
Epoch: [11]  [ 810/1404]  eta: 0:06:10  lr: 0.000086  min_lr: 0.000001  loss: 3.8339 (3.9538)  loss_scale: 32768.0000 (34101.3465)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0015  max mem: 15572
Epoch: [11]  [ 820/1404]  eta: 0:06:03  lr: 0.000086  min_lr: 0.000001  loss: 3.7912 (3.9537)  loss_scale: 32768.0000 (34085.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0013  max mem: 15572
Epoch: [11]  [ 830/1404]  eta: 0:05:56  lr: 0.000086  min_lr: 0.000001  loss: 3.8448 (3.9517)  loss_scale: 32768.0000 (34069.2563)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0012  max mem: 15572
[2025-01-10 18:35:25,138] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:35:25,138] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:35:25,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:35:25,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:35:27,143] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16284
[2025-01-10 18:35:27,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:35:27,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 840/1404]  eta: 0:05:49  lr: 0.000086  min_lr: 0.000001  loss: 3.9946 (3.9539)  loss_scale: 32768.0000 (34209.6361)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0012  max mem: 15572
[2025-01-10 18:35:27,150] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16284
[2025-01-10 18:35:27,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [ 850/1404]  eta: 0:05:43  lr: 0.000086  min_lr: 0.000001  loss: 3.9946 (3.9538)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0472  max mem: 15572
Epoch: [11]  [ 860/1404]  eta: 0:05:37  lr: 0.000086  min_lr: 0.000001  loss: 3.8794 (3.9535)  loss_scale: 32768.0000 (34176.1487)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0645  max mem: 15572
Epoch: [11]  [ 870/1404]  eta: 0:05:31  lr: 0.000086  min_lr: 0.000001  loss: 3.9455 (3.9541)  loss_scale: 32768.0000 (34159.9816)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.0849  max mem: 15572
Epoch: [11]  [ 880/1404]  eta: 0:05:25  lr: 0.000086  min_lr: 0.000001  loss: 3.8973 (3.9519)  loss_scale: 32768.0000 (34144.1816)  weight_decay: 0.0500 (0.0500)  time: 0.7095  data: 0.0675  max mem: 15572
Epoch: [11]  [ 890/1404]  eta: 0:05:19  lr: 0.000086  min_lr: 0.000001  loss: 4.0206 (3.9527)  loss_scale: 32768.0000 (34128.7363)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0009  max mem: 15572
Epoch: [11]  [ 900/1404]  eta: 0:05:13  lr: 0.000086  min_lr: 0.000001  loss: 3.9364 (3.9506)  loss_scale: 32768.0000 (34113.6337)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0008  max mem: 15572
Epoch: [11]  [ 910/1404]  eta: 0:05:06  lr: 0.000086  min_lr: 0.000001  loss: 3.7239 (3.9502)  loss_scale: 32768.0000 (34098.8628)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0007  max mem: 15572
Epoch: [11]  [ 920/1404]  eta: 0:05:00  lr: 0.000086  min_lr: 0.000001  loss: 3.9796 (3.9523)  loss_scale: 32768.0000 (34084.4126)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0008  max mem: 15572
Epoch: [11]  [ 930/1404]  eta: 0:04:54  lr: 0.000086  min_lr: 0.000001  loss: 4.1002 (3.9533)  loss_scale: 32768.0000 (34070.2728)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0009  max mem: 15572
Epoch: [11]  [ 940/1404]  eta: 0:04:47  lr: 0.000086  min_lr: 0.000001  loss: 4.0508 (3.9522)  loss_scale: 32768.0000 (34056.4336)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0009  max mem: 15572
Epoch: [11]  [ 950/1404]  eta: 0:04:41  lr: 0.000086  min_lr: 0.000001  loss: 3.9560 (3.9511)  loss_scale: 32768.0000 (34042.8854)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0376  max mem: 15572
Epoch: [11]  [ 960/1404]  eta: 0:04:35  lr: 0.000086  min_lr: 0.000001  loss: 3.8452 (3.9500)  loss_scale: 32768.0000 (34029.6191)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0614  max mem: 15572
[2025-01-10 18:36:47,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:36:47,066] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:36:47,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:36:47,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 970/1404]  eta: 0:04:29  lr: 0.000086  min_lr: 0.000001  loss: 3.9846 (3.9513)  loss_scale: 32768.0000 (34084.1195)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.0381  max mem: 15572
[2025-01-10 18:36:49,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16416
[2025-01-10 18:36:49,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16416
[2025-01-10 18:36:49,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:36:49,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:36:49,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 980/1404]  eta: 0:04:23  lr: 0.000086  min_lr: 0.000001  loss: 3.7819 (3.9487)  loss_scale: 32768.0000 (34104.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.0144  max mem: 15572
Epoch: [11]  [ 990/1404]  eta: 0:04:17  lr: 0.000086  min_lr: 0.000001  loss: 3.6272 (3.9495)  loss_scale: 32768.0000 (34090.6236)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0010  max mem: 15572
Epoch: [11]  [1000/1404]  eta: 0:04:10  lr: 0.000086  min_lr: 0.000001  loss: 4.1361 (3.9521)  loss_scale: 32768.0000 (34077.4106)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0007  max mem: 15572
Epoch: [11]  [1010/1404]  eta: 0:04:04  lr: 0.000085  min_lr: 0.000001  loss: 3.9516 (3.9478)  loss_scale: 32768.0000 (34064.4590)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0009  max mem: 15572
Epoch: [11]  [1020/1404]  eta: 0:03:58  lr: 0.000085  min_lr: 0.000001  loss: 3.6859 (3.9469)  loss_scale: 32768.0000 (34051.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0010  max mem: 15572
Epoch: [11]  [1030/1404]  eta: 0:03:51  lr: 0.000085  min_lr: 0.000001  loss: 3.9709 (3.9481)  loss_scale: 32768.0000 (34039.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0392  max mem: 15572
Epoch: [11]  [1040/1404]  eta: 0:03:45  lr: 0.000085  min_lr: 0.000001  loss: 4.0682 (3.9495)  loss_scale: 32768.0000 (34027.0970)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0450  max mem: 15572
Epoch: [11]  [1050/1404]  eta: 0:03:39  lr: 0.000085  min_lr: 0.000001  loss: 3.9999 (3.9495)  loss_scale: 32768.0000 (34015.1170)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.0585  max mem: 15572
Epoch: [11]  [1060/1404]  eta: 0:03:33  lr: 0.000085  min_lr: 0.000001  loss: 3.9889 (3.9506)  loss_scale: 32768.0000 (34003.3629)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.0890  max mem: 15572
Epoch: [11]  [1070/1404]  eta: 0:03:27  lr: 0.000085  min_lr: 0.000001  loss: 4.0958 (3.9526)  loss_scale: 32768.0000 (33991.8282)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0371  max mem: 15572
Epoch: [11]  [1080/1404]  eta: 0:03:21  lr: 0.000085  min_lr: 0.000001  loss: 4.1044 (3.9527)  loss_scale: 32768.0000 (33980.5069)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0408  max mem: 15572
Epoch: [11]  [1090/1404]  eta: 0:03:14  lr: 0.000085  min_lr: 0.000001  loss: 3.8643 (3.9515)  loss_scale: 32768.0000 (33969.3932)  weight_decay: 0.0500 (0.0500)  time: 0.6829  data: 0.0410  max mem: 15572
Epoch: [11]  [1100/1404]  eta: 0:03:08  lr: 0.000085  min_lr: 0.000001  loss: 3.8289 (3.9509)  loss_scale: 32768.0000 (33958.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0011  max mem: 15572
[2025-01-10 18:38:09,132] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:38:09,132] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:38:09,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:38:09,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:38:14,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16552
[2025-01-10 18:38:14,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:38:14,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:38:14,813] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16552
[2025-01-10 18:38:14,814] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [1110/1404]  eta: 0:03:02  lr: 0.000085  min_lr: 0.000001  loss: 4.0326 (3.9520)  loss_scale: 32768.0000 (34154.2250)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.0507  max mem: 15572
Epoch: [11]  [1120/1404]  eta: 0:02:56  lr: 0.000085  min_lr: 0.000001  loss: 4.0326 (3.9515)  loss_scale: 32768.0000 (34141.8591)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0504  max mem: 15572
Epoch: [11]  [1130/1404]  eta: 0:02:50  lr: 0.000085  min_lr: 0.000001  loss: 3.9522 (3.9512)  loss_scale: 32768.0000 (34129.7118)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0681  max mem: 15572
Epoch: [11]  [1140/1404]  eta: 0:02:43  lr: 0.000085  min_lr: 0.000001  loss: 3.8394 (3.9512)  loss_scale: 32768.0000 (34117.7774)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0771  max mem: 15572
Epoch: [11]  [1150/1404]  eta: 0:02:37  lr: 0.000085  min_lr: 0.000001  loss: 4.0651 (3.9522)  loss_scale: 32768.0000 (34106.0504)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0770  max mem: 15572
Epoch: [11]  [1160/1404]  eta: 0:02:31  lr: 0.000085  min_lr: 0.000001  loss: 4.0299 (3.9504)  loss_scale: 32768.0000 (34094.5254)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1590  max mem: 15572
Epoch: [11]  [1170/1404]  eta: 0:02:25  lr: 0.000085  min_lr: 0.000001  loss: 3.8204 (3.9494)  loss_scale: 32768.0000 (34083.1973)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1285  max mem: 15572
Epoch: [11]  [1180/1404]  eta: 0:02:19  lr: 0.000085  min_lr: 0.000001  loss: 3.8673 (3.9483)  loss_scale: 32768.0000 (34072.0610)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1486  max mem: 15572
Epoch: [11]  [1190/1404]  eta: 0:02:12  lr: 0.000085  min_lr: 0.000001  loss: 3.9646 (3.9503)  loss_scale: 32768.0000 (34061.1117)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.1453  max mem: 15572
Epoch: [11]  [1200/1404]  eta: 0:02:06  lr: 0.000085  min_lr: 0.000001  loss: 4.2654 (3.9511)  loss_scale: 32768.0000 (34050.3447)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0350  max mem: 15572
Epoch: [11]  [1210/1404]  eta: 0:02:00  lr: 0.000085  min_lr: 0.000001  loss: 4.1388 (3.9515)  loss_scale: 32768.0000 (34039.7556)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0591  max mem: 15572
Epoch: [11]  [1220/1404]  eta: 0:01:54  lr: 0.000085  min_lr: 0.000001  loss: 4.0579 (3.9499)  loss_scale: 32768.0000 (34029.3399)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1027  max mem: 15572
Epoch: [11]  [1230/1404]  eta: 0:01:47  lr: 0.000085  min_lr: 0.000001  loss: 3.8477 (3.9496)  loss_scale: 32768.0000 (34019.0934)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1310  max mem: 15572
[2025-01-10 18:39:34,198] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:39:34,199] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:39:34,205] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:39:34,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [1240/1404]  eta: 0:01:41  lr: 0.000085  min_lr: 0.000001  loss: 3.9891 (3.9493)  loss_scale: 32768.0000 (34114.6301)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.1429  max mem: 15572
[2025-01-10 18:39:37,906] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16687
[2025-01-10 18:39:37,907] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:39:37,943] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16687
[2025-01-10 18:39:37,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:39:37,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1250/1404]  eta: 0:01:35  lr: 0.000085  min_lr: 0.000001  loss: 4.0705 (3.9512)  loss_scale: 32768.0000 (34156.2526)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0762  max mem: 15572
Epoch: [11]  [1260/1404]  eta: 0:01:29  lr: 0.000085  min_lr: 0.000001  loss: 4.2633 (3.9540)  loss_scale: 32768.0000 (34145.2435)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0872  max mem: 15572
Epoch: [11]  [1270/1404]  eta: 0:01:23  lr: 0.000085  min_lr: 0.000001  loss: 4.2986 (3.9557)  loss_scale: 32768.0000 (34134.4076)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1348  max mem: 15572
Epoch: [11]  [1280/1404]  eta: 0:01:16  lr: 0.000085  min_lr: 0.000001  loss: 4.0012 (3.9547)  loss_scale: 32768.0000 (34123.7408)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0677  max mem: 15572
Epoch: [11]  [1290/1404]  eta: 0:01:10  lr: 0.000085  min_lr: 0.000001  loss: 3.7232 (3.9531)  loss_scale: 32768.0000 (34113.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0962  max mem: 15572
Epoch: [11]  [1300/1404]  eta: 0:01:04  lr: 0.000085  min_lr: 0.000001  loss: 3.9456 (3.9555)  loss_scale: 32768.0000 (34102.8993)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.1312  max mem: 15572
Epoch: [11]  [1310/1404]  eta: 0:00:58  lr: 0.000085  min_lr: 0.000001  loss: 4.1271 (3.9559)  loss_scale: 32768.0000 (34092.7170)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1231  max mem: 15572
Epoch: [11]  [1320/1404]  eta: 0:00:52  lr: 0.000085  min_lr: 0.000001  loss: 4.1143 (3.9574)  loss_scale: 32768.0000 (34082.6889)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1022  max mem: 15572
Epoch: [11]  [1330/1404]  eta: 0:00:45  lr: 0.000085  min_lr: 0.000001  loss: 4.1006 (3.9580)  loss_scale: 32768.0000 (34072.8114)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0243  max mem: 15572
Epoch: [11]  [1340/1404]  eta: 0:00:39  lr: 0.000085  min_lr: 0.000001  loss: 4.0788 (3.9580)  loss_scale: 32768.0000 (34063.0813)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0967  max mem: 15572
Epoch: [11]  [1350/1404]  eta: 0:00:33  lr: 0.000085  min_lr: 0.000001  loss: 3.8936 (3.9565)  loss_scale: 32768.0000 (34053.4952)  weight_decay: 0.0500 (0.0500)  time: 0.6943  data: 0.1677  max mem: 15572
Epoch: [11]  [1360/1404]  eta: 0:00:27  lr: 0.000085  min_lr: 0.000001  loss: 3.8894 (3.9564)  loss_scale: 32768.0000 (34044.0500)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0817  max mem: 15572
Epoch: [11]  [1370/1404]  eta: 0:00:21  lr: 0.000085  min_lr: 0.000001  loss: 3.7700 (3.9563)  loss_scale: 32768.0000 (34034.7425)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0013  max mem: 15572
[2025-01-10 18:40:56,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:40:56,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:40:56,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:40:56,555] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:40:58,449] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16820
[2025-01-10 18:40:58,449] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:40:58,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16820
[2025-01-10 18:40:58,451] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:40:58,451] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1380/1404]  eta: 0:00:14  lr: 0.000085  min_lr: 0.000001  loss: 3.7460 (3.9546)  loss_scale: 32768.0000 (34120.4808)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0568  max mem: 15572
Epoch: [11]  [1390/1404]  eta: 0:00:08  lr: 0.000085  min_lr: 0.000001  loss: 3.8736 (3.9551)  loss_scale: 32768.0000 (34110.7577)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0708  max mem: 15572
Epoch: [11]  [1400/1404]  eta: 0:00:02  lr: 0.000085  min_lr: 0.000001  loss: 4.1151 (3.9560)  loss_scale: 32768.0000 (34101.1734)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0147  max mem: 15572
Epoch: [11]  [1403/1404]  eta: 0:00:00  lr: 0.000085  min_lr: 0.000001  loss: 3.9798 (3.9563)  loss_scale: 32768.0000 (34098.3248)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0008  max mem: 15572
Epoch: [11] Total time: 0:14:27 (0.6176 s / it)
Averaged stats: lr: 0.000085  min_lr: 0.000001  loss: 3.9798 (3.9650)  loss_scale: 32768.0000 (34098.3248)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:46  loss: 1.6953 (1.6953)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 6.0780  data: 5.8283  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 2.7382 (2.6340)  acc1: 38.8889 (32.8283)  acc5: 72.2222 (67.6768)  time: 0.7813  data: 0.5878  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.8632 (2.7772)  acc1: 27.7778 (30.6878)  acc5: 72.2222 (68.2540)  time: 0.3117  data: 0.1214  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.5844 (2.5730)  acc1: 33.3333 (36.5591)  acc5: 77.7778 (71.5054)  time: 0.3479  data: 0.1504  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 2.0664 (2.5427)  acc1: 50.0000 (38.6179)  acc5: 77.7778 (71.2737)  time: 0.3646  data: 0.1629  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 2.4257 (2.5671)  acc1: 44.4444 (38.8889)  acc5: 77.7778 (72.0044)  time: 0.4283  data: 0.2205  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.6114 (2.6515)  acc1: 27.7778 (36.9763)  acc5: 72.2222 (70.0364)  time: 0.4232  data: 0.2070  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.5623 (2.6191)  acc1: 38.8889 (38.4194)  acc5: 66.6667 (70.1095)  time: 0.3912  data: 0.1797  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.4484 (2.6135)  acc1: 38.8889 (38.2716)  acc5: 72.2222 (70.8505)  time: 0.3739  data: 0.1584  max mem: 15572
Val:  [ 90/136]  eta: 0:00:20  loss: 2.7077 (2.6337)  acc1: 27.7778 (36.6911)  acc5: 72.2222 (70.4518)  time: 0.3943  data: 0.1736  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 2.9322 (2.7054)  acc1: 16.6667 (34.4884)  acc5: 61.1111 (68.7019)  time: 0.4029  data: 0.1962  max mem: 15572
Val:  [110/136]  eta: 0:00:11  loss: 2.9322 (2.6997)  acc1: 27.7778 (35.1351)  acc5: 61.1111 (68.6687)  time: 0.4202  data: 0.2177  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2853 (2.6365)  acc1: 44.4444 (36.6850)  acc5: 77.7778 (70.2938)  time: 0.4187  data: 0.2259  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0286 (2.5791)  acc1: 61.1111 (38.4648)  acc5: 88.8889 (71.0348)  time: 0.2597  data: 0.0980  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1992 (2.5925)  acc1: 44.4444 (38.3292)  acc5: 77.7778 (70.8026)  time: 0.1505  data: 0.0002  max mem: 15572
Val: Total time: 0:00:54 (0.3996 s / it)
* Acc@1 38.247 Acc@5 69.533 loss 2.634
Accuracy of the network on the 4883 val videos: 38.2%
[2025-01-10 18:42:06,492] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 18:42:06,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 18:42:06,496] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 18:42:06,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 18:42:09,156] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 18:42:09,156] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.25%
Epoch: [12]  [   0/1404]  eta: 3:52:58  lr: 0.000085  min_lr: 0.000001  loss: 4.6619 (4.6619)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.9560  data: 9.5167  max mem: 15572
Epoch: [12]  [  10/1404]  eta: 0:32:54  lr: 0.000085  min_lr: 0.000001  loss: 3.9837 (3.8028)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4168  data: 0.8736  max mem: 15572
Epoch: [12]  [  20/1404]  eta: 0:23:05  lr: 0.000085  min_lr: 0.000001  loss: 3.9970 (3.9416)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0052  max mem: 15572
Epoch: [12]  [  30/1404]  eta: 0:21:00  lr: 0.000085  min_lr: 0.000001  loss: 4.0545 (3.9450)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1082  max mem: 15572
Epoch: [12]  [  40/1404]  eta: 0:19:55  lr: 0.000085  min_lr: 0.000001  loss: 3.9148 (3.9156)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7455  data: 0.2178  max mem: 15572
Epoch: [12]  [  50/1404]  eta: 0:18:47  lr: 0.000085  min_lr: 0.000001  loss: 3.8034 (3.8944)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7026  data: 0.1715  max mem: 15572
Epoch: [12]  [  60/1404]  eta: 0:17:58  lr: 0.000085  min_lr: 0.000001  loss: 3.8303 (3.9070)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1449  max mem: 15572
Epoch: [12]  [  70/1404]  eta: 0:17:03  lr: 0.000085  min_lr: 0.000001  loss: 3.9272 (3.9315)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1049  max mem: 15572
Epoch: [12]  [  80/1404]  eta: 0:16:16  lr: 0.000085  min_lr: 0.000001  loss: 3.9272 (3.9001)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0302  max mem: 15572
Epoch: [12]  [  90/1404]  eta: 0:15:47  lr: 0.000085  min_lr: 0.000001  loss: 3.7986 (3.9045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0457  max mem: 15572
Epoch: [12]  [ 100/1404]  eta: 0:15:35  lr: 0.000085  min_lr: 0.000001  loss: 3.9756 (3.9148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1089  max mem: 15572
[2025-01-10 18:43:22,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:43:22,110] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:43:22,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:43:22,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 110/1404]  eta: 0:15:06  lr: 0.000085  min_lr: 0.000001  loss: 3.9232 (3.9090)  loss_scale: 32768.0000 (35720.0721)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0724  max mem: 15572
Epoch: [12]  [ 120/1404]  eta: 0:14:51  lr: 0.000085  min_lr: 0.000001  loss: 3.7977 (3.9057)  loss_scale: 65536.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0723  max mem: 15572
[2025-01-10 18:43:37,634] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16976
[2025-01-10 18:43:37,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:43:37,689] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16976
[2025-01-10 18:43:37,690] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:43:37,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 130/1404]  eta: 0:14:37  lr: 0.000085  min_lr: 0.000001  loss: 3.8308 (3.9021)  loss_scale: 65536.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1138  max mem: 15572
Epoch: [12]  [ 140/1404]  eta: 0:14:24  lr: 0.000085  min_lr: 0.000001  loss: 3.8185 (3.8906)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0595  max mem: 15572
Epoch: [12]  [ 150/1404]  eta: 0:14:07  lr: 0.000085  min_lr: 0.000001  loss: 3.8400 (3.8912)  loss_scale: 32768.0000 (38627.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0369  max mem: 15572
[2025-01-10 18:43:51,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=104, lr=[8.190914307140542e-07, 8.190914307140542e-07, 1.1701306153057918e-06, 1.1701306153057918e-06, 1.6716151647225601e-06, 1.6716151647225601e-06, 2.3880216638893716e-06, 2.3880216638893716e-06, 3.4114595198419597e-06, 3.4114595198419597e-06, 4.873513599774229e-06, 4.873513599774229e-06, 6.962162285391755e-06, 6.962162285391755e-06, 9.945946121988223e-06, 9.945946121988223e-06, 1.4208494459983175e-05, 1.4208494459983175e-05, 2.0297849228547397e-05, 2.0297849228547397e-05, 2.899692746935342e-05, 2.899692746935342e-05, 4.142418209907632e-05, 4.142418209907632e-05, 5.917740299868046e-05, 5.917740299868046e-05, 8.45391471409721e-05, 8.45391471409721e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 18:43:51,810] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=45.13036048865049, CurrSamplesPerSec=51.3836511824846, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 160/1404]  eta: 0:13:56  lr: 0.000085  min_lr: 0.000001  loss: 3.8907 (3.8808)  loss_scale: 32768.0000 (38263.2547)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0614  max mem: 15572
Epoch: [12]  [ 170/1404]  eta: 0:13:47  lr: 0.000085  min_lr: 0.000001  loss: 3.8717 (3.8826)  loss_scale: 32768.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0719  max mem: 15572
Epoch: [12]  [ 180/1404]  eta: 0:13:37  lr: 0.000084  min_lr: 0.000001  loss: 3.8862 (3.8776)  loss_scale: 32768.0000 (37656.0442)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0424  max mem: 15572
Epoch: [12]  [ 190/1404]  eta: 0:13:26  lr: 0.000084  min_lr: 0.000001  loss: 3.7745 (3.8719)  loss_scale: 32768.0000 (37400.1257)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0317  max mem: 15572
Epoch: [12]  [ 200/1404]  eta: 0:13:14  lr: 0.000084  min_lr: 0.000001  loss: 3.9051 (3.8720)  loss_scale: 32768.0000 (37169.6716)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0454  max mem: 15572
Epoch: [12]  [ 210/1404]  eta: 0:13:07  lr: 0.000084  min_lr: 0.000001  loss: 3.9575 (3.8762)  loss_scale: 32768.0000 (36961.0616)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1036  max mem: 15572
Epoch: [12]  [ 220/1404]  eta: 0:13:03  lr: 0.000084  min_lr: 0.000001  loss: 3.9826 (3.8764)  loss_scale: 32768.0000 (36771.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.1045  max mem: 15572
Epoch: [12]  [ 230/1404]  eta: 0:12:53  lr: 0.000084  min_lr: 0.000001  loss: 3.9976 (3.8766)  loss_scale: 32768.0000 (36598.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0277  max mem: 15572
Epoch: [12]  [ 240/1404]  eta: 0:12:43  lr: 0.000084  min_lr: 0.000001  loss: 4.1849 (3.8935)  loss_scale: 32768.0000 (36439.1037)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0006  max mem: 15572
Epoch: [12]  [ 250/1404]  eta: 0:12:32  lr: 0.000084  min_lr: 0.000001  loss: 4.1849 (3.8909)  loss_scale: 32768.0000 (36292.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0007  max mem: 15572
[2025-01-10 18:44:56,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:44:56,675] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:44:56,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:44:56,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:44:57,142] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17106
[2025-01-10 18:44:57,143] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:44:57,143] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17106
[2025-01-10 18:44:57,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:44:57,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 260/1404]  eta: 0:12:27  lr: 0.000084  min_lr: 0.000001  loss: 3.8624 (3.8940)  loss_scale: 32768.0000 (36283.3410)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0822  max mem: 15572
Epoch: [12]  [ 270/1404]  eta: 0:12:14  lr: 0.000084  min_lr: 0.000001  loss: 4.1193 (3.9003)  loss_scale: 32768.0000 (36153.6236)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0903  max mem: 15572
Epoch: [12]  [ 280/1404]  eta: 0:12:07  lr: 0.000084  min_lr: 0.000001  loss: 3.9565 (3.8997)  loss_scale: 32768.0000 (36033.1388)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0664  max mem: 15572
Epoch: [12]  [ 290/1404]  eta: 0:11:59  lr: 0.000084  min_lr: 0.000001  loss: 3.9565 (3.9054)  loss_scale: 32768.0000 (35920.9347)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1040  max mem: 15572
Epoch: [12]  [ 300/1404]  eta: 0:11:55  lr: 0.000084  min_lr: 0.000001  loss: 4.1284 (3.9106)  loss_scale: 32768.0000 (35816.1860)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.1308  max mem: 15572
Epoch: [12]  [ 310/1404]  eta: 0:11:48  lr: 0.000084  min_lr: 0.000001  loss: 3.9869 (3.9134)  loss_scale: 32768.0000 (35718.1736)  weight_decay: 0.0500 (0.0500)  time: 0.6741  data: 0.1551  max mem: 15572
Epoch: [12]  [ 320/1404]  eta: 0:11:39  lr: 0.000084  min_lr: 0.000001  loss: 3.9810 (3.9155)  loss_scale: 32768.0000 (35626.2679)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1003  max mem: 15572
Epoch: [12]  [ 330/1404]  eta: 0:11:33  lr: 0.000084  min_lr: 0.000001  loss: 3.9810 (3.9156)  loss_scale: 32768.0000 (35539.9154)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1060  max mem: 15572
Epoch: [12]  [ 340/1404]  eta: 0:11:23  lr: 0.000084  min_lr: 0.000001  loss: 3.6924 (3.9092)  loss_scale: 32768.0000 (35458.6276)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0767  max mem: 15572
Epoch: [12]  [ 350/1404]  eta: 0:11:17  lr: 0.000084  min_lr: 0.000001  loss: 3.6766 (3.9027)  loss_scale: 32768.0000 (35381.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0635  max mem: 15572
Epoch: [12]  [ 360/1404]  eta: 0:11:10  lr: 0.000084  min_lr: 0.000001  loss: 3.8798 (3.9059)  loss_scale: 32768.0000 (35309.5623)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1216  max mem: 15572
Epoch: [12]  [ 370/1404]  eta: 0:11:01  lr: 0.000084  min_lr: 0.000001  loss: 3.9604 (3.9052)  loss_scale: 32768.0000 (35241.0566)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0893  max mem: 15572
Epoch: [12]  [ 380/1404]  eta: 0:10:55  lr: 0.000084  min_lr: 0.000001  loss: 3.8301 (3.9044)  loss_scale: 32768.0000 (35176.1470)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1064  max mem: 15572
[2025-01-10 18:46:17,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:46:17,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:46:17,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:46:17,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 390/1404]  eta: 0:10:48  lr: 0.000084  min_lr: 0.000001  loss: 3.7758 (3.9025)  loss_scale: 32768.0000 (35449.7801)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1227  max mem: 15572
[2025-01-10 18:46:25,523] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17248
[2025-01-10 18:46:25,523] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:46:25,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17248
[2025-01-10 18:46:25,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:46:25,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 400/1404]  eta: 0:10:41  lr: 0.000084  min_lr: 0.000001  loss: 3.8631 (3.9063)  loss_scale: 65536.0000 (36118.3441)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0939  max mem: 15572
Epoch: [12]  [ 410/1404]  eta: 0:10:32  lr: 0.000084  min_lr: 0.000001  loss: 4.0484 (3.9070)  loss_scale: 32768.0000 (36036.8273)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0633  max mem: 15572
Epoch: [12]  [ 420/1404]  eta: 0:10:27  lr: 0.000084  min_lr: 0.000001  loss: 3.9006 (3.9066)  loss_scale: 32768.0000 (35959.1829)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1108  max mem: 15572
Epoch: [12]  [ 430/1404]  eta: 0:10:20  lr: 0.000084  min_lr: 0.000001  loss: 3.9006 (3.9058)  loss_scale: 32768.0000 (35885.1415)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1289  max mem: 15572
Epoch: [12]  [ 440/1404]  eta: 0:10:15  lr: 0.000084  min_lr: 0.000001  loss: 3.8700 (3.9060)  loss_scale: 32768.0000 (35814.4580)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.1227  max mem: 15572
Epoch: [12]  [ 450/1404]  eta: 0:10:06  lr: 0.000084  min_lr: 0.000001  loss: 3.7750 (3.9025)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0917  max mem: 15572
Epoch: [12]  [ 460/1404]  eta: 0:10:00  lr: 0.000084  min_lr: 0.000001  loss: 3.6461 (3.9009)  loss_scale: 32768.0000 (35682.2907)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0108  max mem: 15572
Epoch: [12]  [ 470/1404]  eta: 0:09:53  lr: 0.000084  min_lr: 0.000001  loss: 3.9442 (3.9026)  loss_scale: 32768.0000 (35620.4161)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0077  max mem: 15572
Epoch: [12]  [ 480/1404]  eta: 0:09:46  lr: 0.000084  min_lr: 0.000001  loss: 4.0156 (3.9054)  loss_scale: 32768.0000 (35561.1143)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0009  max mem: 15572
Epoch: [12]  [ 490/1404]  eta: 0:09:40  lr: 0.000084  min_lr: 0.000001  loss: 3.8831 (3.9032)  loss_scale: 32768.0000 (35504.2281)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0285  max mem: 15572
Epoch: [12]  [ 500/1404]  eta: 0:09:34  lr: 0.000084  min_lr: 0.000001  loss: 3.8469 (3.9025)  loss_scale: 32768.0000 (35449.6128)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0414  max mem: 15572
Epoch: [12]  [ 510/1404]  eta: 0:09:26  lr: 0.000084  min_lr: 0.000001  loss: 3.8920 (3.9023)  loss_scale: 32768.0000 (35397.1350)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0312  max mem: 15572
Epoch: [12]  [ 520/1404]  eta: 0:09:20  lr: 0.000084  min_lr: 0.000001  loss: 3.8920 (3.9024)  loss_scale: 32768.0000 (35346.6718)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0747  max mem: 15572
[2025-01-10 18:47:44,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:47:44,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:47:44,210] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:47:44,210] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 530/1404]  eta: 0:09:12  lr: 0.000084  min_lr: 0.000001  loss: 3.8828 (3.9015)  loss_scale: 32768.0000 (35421.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0943  max mem: 15572
[2025-01-10 18:47:46,287] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17380
[2025-01-10 18:47:46,287] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:47:46,287] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:47:46,287] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17380
[2025-01-10 18:47:46,288] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [ 540/1404]  eta: 0:09:06  lr: 0.000084  min_lr: 0.000001  loss: 3.8273 (3.8996)  loss_scale: 32768.0000 (35433.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0840  max mem: 15572
Epoch: [12]  [ 550/1404]  eta: 0:08:59  lr: 0.000084  min_lr: 0.000001  loss: 3.7779 (3.8977)  loss_scale: 32768.0000 (35384.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0473  max mem: 15572
Epoch: [12]  [ 560/1404]  eta: 0:08:53  lr: 0.000084  min_lr: 0.000001  loss: 3.8848 (3.8976)  loss_scale: 32768.0000 (35338.0392)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0804  max mem: 15572
Epoch: [12]  [ 570/1404]  eta: 0:08:46  lr: 0.000084  min_lr: 0.000001  loss: 4.0113 (3.9027)  loss_scale: 32768.0000 (35293.0298)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1221  max mem: 15572
Epoch: [12]  [ 580/1404]  eta: 0:08:40  lr: 0.000084  min_lr: 0.000001  loss: 4.0291 (3.9051)  loss_scale: 32768.0000 (35249.5697)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1308  max mem: 15572
Epoch: [12]  [ 590/1404]  eta: 0:08:35  lr: 0.000084  min_lr: 0.000001  loss: 4.0086 (3.9075)  loss_scale: 32768.0000 (35207.5804)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.1684  max mem: 15572
Epoch: [12]  [ 600/1404]  eta: 0:08:27  lr: 0.000084  min_lr: 0.000001  loss: 3.8599 (3.9047)  loss_scale: 32768.0000 (35166.9884)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1088  max mem: 15572
Epoch: [12]  [ 610/1404]  eta: 0:08:20  lr: 0.000084  min_lr: 0.000001  loss: 3.8764 (3.9018)  loss_scale: 32768.0000 (35127.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0471  max mem: 15572
Epoch: [12]  [ 620/1404]  eta: 0:08:12  lr: 0.000084  min_lr: 0.000001  loss: 4.0165 (3.9036)  loss_scale: 32768.0000 (35089.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0323  max mem: 15572
Epoch: [12]  [ 630/1404]  eta: 0:08:05  lr: 0.000084  min_lr: 0.000001  loss: 4.1456 (3.9051)  loss_scale: 32768.0000 (35052.9319)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0256  max mem: 15572
Epoch: [12]  [ 640/1404]  eta: 0:07:58  lr: 0.000084  min_lr: 0.000001  loss: 4.2106 (3.9085)  loss_scale: 32768.0000 (35017.2855)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0718  max mem: 15572
Epoch: [12]  [ 650/1404]  eta: 0:07:52  lr: 0.000084  min_lr: 0.000001  loss: 3.9547 (3.9065)  loss_scale: 32768.0000 (34982.7343)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0959  max mem: 15572
Epoch: [12]  [ 660/1404]  eta: 0:07:46  lr: 0.000084  min_lr: 0.000001  loss: 3.8268 (3.9088)  loss_scale: 32768.0000 (34949.2284)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.0733  max mem: 15572
[2025-01-10 18:49:04,542] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:49:04,542] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:49:04,679] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:49:04,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:49:06,564] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17513
[2025-01-10 18:49:06,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:49:06,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:49:06,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17513
[2025-01-10 18:49:06,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [ 670/1404]  eta: 0:07:41  lr: 0.000084  min_lr: 0.000001  loss: 4.0314 (3.9092)  loss_scale: 32768.0000 (35112.0596)  weight_decay: 0.0500 (0.0500)  time: 0.7068  data: 0.0383  max mem: 15572
Epoch: [12]  [ 680/1404]  eta: 0:07:34  lr: 0.000084  min_lr: 0.000001  loss: 4.0314 (3.9115)  loss_scale: 32768.0000 (35077.6388)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.0011  max mem: 15572
Epoch: [12]  [ 690/1404]  eta: 0:07:27  lr: 0.000084  min_lr: 0.000001  loss: 4.1941 (3.9151)  loss_scale: 32768.0000 (35044.2142)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0011  max mem: 15572
Epoch: [12]  [ 700/1404]  eta: 0:07:21  lr: 0.000084  min_lr: 0.000001  loss: 3.9167 (3.9130)  loss_scale: 32768.0000 (35011.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0009  max mem: 15572
Epoch: [12]  [ 710/1404]  eta: 0:07:15  lr: 0.000084  min_lr: 0.000001  loss: 3.7885 (3.9101)  loss_scale: 32768.0000 (34980.1857)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0011  max mem: 15572
Epoch: [12]  [ 720/1404]  eta: 0:07:08  lr: 0.000083  min_lr: 0.000001  loss: 3.8644 (3.9106)  loss_scale: 32768.0000 (34949.5035)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0091  max mem: 15572
Epoch: [12]  [ 730/1404]  eta: 0:07:02  lr: 0.000083  min_lr: 0.000001  loss: 3.9571 (3.9111)  loss_scale: 32768.0000 (34919.6607)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.0699  max mem: 15572
Epoch: [12]  [ 740/1404]  eta: 0:06:56  lr: 0.000083  min_lr: 0.000001  loss: 3.9571 (3.9147)  loss_scale: 32768.0000 (34890.6235)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.1201  max mem: 15572
Epoch: [12]  [ 750/1404]  eta: 0:06:49  lr: 0.000083  min_lr: 0.000001  loss: 4.1260 (3.9169)  loss_scale: 32768.0000 (34862.3595)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0704  max mem: 15572
Epoch: [12]  [ 760/1404]  eta: 0:06:43  lr: 0.000083  min_lr: 0.000001  loss: 4.0684 (3.9190)  loss_scale: 32768.0000 (34834.8384)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0206  max mem: 15572
Epoch: [12]  [ 770/1404]  eta: 0:06:37  lr: 0.000083  min_lr: 0.000001  loss: 4.0053 (3.9179)  loss_scale: 32768.0000 (34808.0311)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.0095  max mem: 15572
Epoch: [12]  [ 780/1404]  eta: 0:06:31  lr: 0.000083  min_lr: 0.000001  loss: 3.8456 (3.9164)  loss_scale: 32768.0000 (34781.9104)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.0266  max mem: 15572
Epoch: [12]  [ 790/1404]  eta: 0:06:25  lr: 0.000083  min_lr: 0.000001  loss: 4.0053 (3.9184)  loss_scale: 32768.0000 (34756.4501)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0265  max mem: 15572
[2025-01-10 18:50:27,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:50:27,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:50:27,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:50:27,568] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 800/1404]  eta: 0:06:19  lr: 0.000083  min_lr: 0.000001  loss: 3.9774 (3.9187)  loss_scale: 32768.0000 (35017.9875)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.0008  max mem: 15572
[2025-01-10 18:50:33,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17650
[2025-01-10 18:50:33,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:50:33,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:50:33,172] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17650
[2025-01-10 18:50:33,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [ 810/1404]  eta: 0:06:12  lr: 0.000083  min_lr: 0.000001  loss: 3.9610 (3.9207)  loss_scale: 32768.0000 (35030.6486)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0056  max mem: 15572
Epoch: [12]  [ 820/1404]  eta: 0:06:06  lr: 0.000083  min_lr: 0.000001  loss: 3.9192 (3.9186)  loss_scale: 32768.0000 (35003.0889)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0061  max mem: 15572
Epoch: [12]  [ 830/1404]  eta: 0:06:00  lr: 0.000083  min_lr: 0.000001  loss: 3.9192 (3.9197)  loss_scale: 32768.0000 (34976.1925)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0012  max mem: 15572
Epoch: [12]  [ 840/1404]  eta: 0:05:53  lr: 0.000083  min_lr: 0.000001  loss: 3.8789 (3.9154)  loss_scale: 32768.0000 (34949.9358)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0008  max mem: 15572
Epoch: [12]  [ 850/1404]  eta: 0:05:47  lr: 0.000083  min_lr: 0.000001  loss: 3.5233 (3.9131)  loss_scale: 32768.0000 (34924.2961)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0010  max mem: 15572
Epoch: [12]  [ 860/1404]  eta: 0:05:41  lr: 0.000083  min_lr: 0.000001  loss: 3.6579 (3.9133)  loss_scale: 32768.0000 (34899.2520)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.0009  max mem: 15572
Epoch: [12]  [ 870/1404]  eta: 0:05:34  lr: 0.000083  min_lr: 0.000001  loss: 3.9709 (3.9132)  loss_scale: 32768.0000 (34874.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0009  max mem: 15572
Epoch: [12]  [ 880/1404]  eta: 0:05:28  lr: 0.000083  min_lr: 0.000001  loss: 3.6380 (3.9083)  loss_scale: 32768.0000 (34850.8695)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0010  max mem: 15572
Epoch: [12]  [ 890/1404]  eta: 0:05:22  lr: 0.000083  min_lr: 0.000001  loss: 3.7675 (3.9118)  loss_scale: 32768.0000 (34827.4927)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0009  max mem: 15572
Epoch: [12]  [ 900/1404]  eta: 0:05:15  lr: 0.000083  min_lr: 0.000001  loss: 4.0636 (3.9117)  loss_scale: 32768.0000 (34804.6349)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0010  max mem: 15572
Epoch: [12]  [ 910/1404]  eta: 0:05:09  lr: 0.000083  min_lr: 0.000001  loss: 3.9644 (3.9130)  loss_scale: 32768.0000 (34782.2788)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0010  max mem: 15572
Epoch: [12]  [ 920/1404]  eta: 0:05:02  lr: 0.000083  min_lr: 0.000001  loss: 4.0331 (3.9122)  loss_scale: 32768.0000 (34760.4083)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.0009  max mem: 15572
Epoch: [12]  [ 930/1404]  eta: 0:04:56  lr: 0.000083  min_lr: 0.000001  loss: 4.0331 (3.9116)  loss_scale: 32768.0000 (34739.0075)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0008  max mem: 15572
[2025-01-10 18:51:52,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:51:52,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:51:52,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:51:52,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 940/1404]  eta: 0:04:49  lr: 0.000083  min_lr: 0.000001  loss: 3.9402 (3.9116)  loss_scale: 32768.0000 (35066.2869)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0010  max mem: 15572
Epoch: [12]  [ 950/1404]  eta: 0:04:43  lr: 0.000083  min_lr: 0.000001  loss: 3.9375 (3.9120)  loss_scale: 65536.0000 (35386.6835)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0119  max mem: 15572
[2025-01-10 18:52:06,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17804
[2025-01-10 18:52:06,470] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:52:06,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17804
[2025-01-10 18:52:06,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:52:06,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 960/1404]  eta: 0:04:37  lr: 0.000083  min_lr: 0.000001  loss: 3.9720 (3.9131)  loss_scale: 65536.0000 (35529.9230)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0257  max mem: 15572
Epoch: [12]  [ 970/1404]  eta: 0:04:30  lr: 0.000083  min_lr: 0.000001  loss: 3.9051 (3.9106)  loss_scale: 32768.0000 (35501.4789)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0706  max mem: 15572
Epoch: [12]  [ 980/1404]  eta: 0:04:24  lr: 0.000083  min_lr: 0.000001  loss: 3.7567 (3.9093)  loss_scale: 32768.0000 (35473.6147)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1406  max mem: 15572
Epoch: [12]  [ 990/1404]  eta: 0:04:18  lr: 0.000083  min_lr: 0.000001  loss: 3.9437 (3.9104)  loss_scale: 32768.0000 (35446.3128)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1284  max mem: 15572
Epoch: [12]  [1000/1404]  eta: 0:04:11  lr: 0.000083  min_lr: 0.000001  loss: 3.9183 (3.9095)  loss_scale: 32768.0000 (35419.5564)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0737  max mem: 15572
Epoch: [12]  [1010/1404]  eta: 0:04:05  lr: 0.000083  min_lr: 0.000001  loss: 3.7115 (3.9102)  loss_scale: 32768.0000 (35393.3294)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1095  max mem: 15572
Epoch: [12]  [1020/1404]  eta: 0:03:59  lr: 0.000083  min_lr: 0.000001  loss: 3.9689 (3.9093)  loss_scale: 32768.0000 (35367.6161)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.1545  max mem: 15572
Epoch: [12]  [1030/1404]  eta: 0:03:53  lr: 0.000083  min_lr: 0.000001  loss: 3.9629 (3.9101)  loss_scale: 32768.0000 (35342.4016)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0950  max mem: 15572
Epoch: [12]  [1040/1404]  eta: 0:03:47  lr: 0.000083  min_lr: 0.000001  loss: 3.8387 (3.9085)  loss_scale: 32768.0000 (35317.6715)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0742  max mem: 15572
Epoch: [12]  [1050/1404]  eta: 0:03:40  lr: 0.000083  min_lr: 0.000001  loss: 3.8387 (3.9085)  loss_scale: 32768.0000 (35293.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0600  max mem: 15572
Epoch: [12]  [1060/1404]  eta: 0:03:34  lr: 0.000083  min_lr: 0.000001  loss: 4.1305 (3.9099)  loss_scale: 32768.0000 (35269.6098)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0066  max mem: 15572
Epoch: [12]  [1070/1404]  eta: 0:03:28  lr: 0.000083  min_lr: 0.000001  loss: 4.0837 (3.9109)  loss_scale: 32768.0000 (35246.2521)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.0007  max mem: 15572
Epoch: [12]  [1080/1404]  eta: 0:03:22  lr: 0.000083  min_lr: 0.000001  loss: 3.9860 (3.9106)  loss_scale: 32768.0000 (35223.3265)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.0572  max mem: 15572
[2025-01-10 18:53:26,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:53:26,462] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:53:26,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:53:26,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:53:27,427] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17935
[2025-01-10 18:53:27,427] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:53:27,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 18:53:27,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17935
[2025-01-10 18:53:27,429] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [1090/1404]  eta: 0:03:15  lr: 0.000083  min_lr: 0.000001  loss: 3.9214 (3.9114)  loss_scale: 32768.0000 (35260.8909)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0950  max mem: 15572
Epoch: [12]  [1100/1404]  eta: 0:03:09  lr: 0.000083  min_lr: 0.000001  loss: 4.0353 (3.9125)  loss_scale: 32768.0000 (35238.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0876  max mem: 15572
Epoch: [12]  [1110/1404]  eta: 0:03:03  lr: 0.000083  min_lr: 0.000001  loss: 3.9373 (3.9104)  loss_scale: 32768.0000 (35216.0144)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1171  max mem: 15572
Epoch: [12]  [1120/1404]  eta: 0:02:56  lr: 0.000083  min_lr: 0.000001  loss: 3.8035 (3.9109)  loss_scale: 32768.0000 (35194.1766)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0927  max mem: 15572
Epoch: [12]  [1130/1404]  eta: 0:02:50  lr: 0.000083  min_lr: 0.000001  loss: 4.0073 (3.9104)  loss_scale: 32768.0000 (35172.7250)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1059  max mem: 15572
Epoch: [12]  [1140/1404]  eta: 0:02:44  lr: 0.000083  min_lr: 0.000001  loss: 4.0073 (3.9099)  loss_scale: 32768.0000 (35151.6494)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.1556  max mem: 15572
Epoch: [12]  [1150/1404]  eta: 0:02:38  lr: 0.000083  min_lr: 0.000001  loss: 3.8227 (3.9083)  loss_scale: 32768.0000 (35130.9401)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1080  max mem: 15572
[2025-01-10 18:54:07,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=111, lr=[8.01083251096546e-07, 8.01083251096546e-07, 1.1444046444236373e-06, 1.1444046444236373e-06, 1.6348637777480533e-06, 1.6348637777480533e-06, 2.3355196824972193e-06, 2.3355196824972193e-06, 3.336456689281742e-06, 3.336456689281742e-06, 4.7663666989739175e-06, 4.7663666989739175e-06, 6.809095284248453e-06, 6.809095284248453e-06, 9.727278977497792e-06, 9.727278977497792e-06, 1.3896112824996845e-05, 1.3896112824996845e-05, 1.9851589749995496e-05, 1.9851589749995496e-05, 2.8359413928564995e-05, 2.8359413928564995e-05, 4.0513448469378566e-05, 4.0513448469378566e-05, 5.78763549562551e-05, 5.78763549562551e-05, 8.268050708036443e-05, 8.268050708036443e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 18:54:07,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=45.15923761532417, CurrSamplesPerSec=52.26354376344382, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 18:54:10,009] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18004
[2025-01-10 18:54:10,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 18:54:10,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18004
[2025-01-10 18:54:10,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 18:54:10,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [1160/1404]  eta: 0:02:32  lr: 0.000083  min_lr: 0.000001  loss: 3.8256 (3.9085)  loss_scale: 32768.0000 (35040.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0786  max mem: 15572
Epoch: [12]  [1170/1404]  eta: 0:02:25  lr: 0.000083  min_lr: 0.000001  loss: 4.0140 (3.9095)  loss_scale: 16384.0000 (34880.7105)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0842  max mem: 15572
Epoch: [12]  [1180/1404]  eta: 0:02:19  lr: 0.000083  min_lr: 0.000001  loss: 3.9364 (3.9087)  loss_scale: 16384.0000 (34724.0914)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0749  max mem: 15572
Epoch: [12]  [1190/1404]  eta: 0:02:13  lr: 0.000083  min_lr: 0.000001  loss: 4.0729 (3.9107)  loss_scale: 16384.0000 (34570.1024)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0395  max mem: 15572
Epoch: [12]  [1200/1404]  eta: 0:02:07  lr: 0.000083  min_lr: 0.000001  loss: 4.0729 (3.9121)  loss_scale: 16384.0000 (34418.6778)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0041  max mem: 15572
Epoch: [12]  [1210/1404]  eta: 0:02:00  lr: 0.000083  min_lr: 0.000001  loss: 3.9658 (3.9114)  loss_scale: 16384.0000 (34269.7539)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0011  max mem: 15572
Epoch: [12]  [1220/1404]  eta: 0:01:54  lr: 0.000083  min_lr: 0.000001  loss: 3.7895 (3.9111)  loss_scale: 16384.0000 (34123.2695)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0012  max mem: 15572
Epoch: [12]  [1230/1404]  eta: 0:01:48  lr: 0.000083  min_lr: 0.000001  loss: 4.0112 (3.9135)  loss_scale: 16384.0000 (33979.1649)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0013  max mem: 15572
Epoch: [12]  [1240/1404]  eta: 0:01:42  lr: 0.000083  min_lr: 0.000001  loss: 4.1106 (3.9128)  loss_scale: 16384.0000 (33837.3828)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0010  max mem: 15572
Epoch: [12]  [1250/1404]  eta: 0:01:35  lr: 0.000082  min_lr: 0.000001  loss: 4.0434 (3.9148)  loss_scale: 16384.0000 (33697.8673)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0012  max mem: 15572
Epoch: [12]  [1260/1404]  eta: 0:01:29  lr: 0.000082  min_lr: 0.000001  loss: 4.1400 (3.9156)  loss_scale: 16384.0000 (33560.5646)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0012  max mem: 15572
Epoch: [12]  [1270/1404]  eta: 0:01:23  lr: 0.000082  min_lr: 0.000001  loss: 4.1400 (3.9162)  loss_scale: 16384.0000 (33425.4225)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0448  max mem: 15572
Epoch: [12]  [1280/1404]  eta: 0:01:17  lr: 0.000082  min_lr: 0.000001  loss: 3.9085 (3.9160)  loss_scale: 16384.0000 (33292.3903)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.0685  max mem: 15572
[2025-01-10 18:55:29,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:55:29,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 18:55:29,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:55:29,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [1290/1404]  eta: 0:01:11  lr: 0.000082  min_lr: 0.000001  loss: 3.8002 (3.9133)  loss_scale: 16384.0000 (33237.5647)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0831  max mem: 15572
Epoch: [12]  [1300/1404]  eta: 0:01:04  lr: 0.000082  min_lr: 0.000001  loss: 3.9098 (3.9146)  loss_scale: 32768.0000 (33233.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0593  max mem: 15572
Epoch: [12]  [1310/1404]  eta: 0:00:58  lr: 0.000082  min_lr: 0.000001  loss: 3.9098 (3.9143)  loss_scale: 32768.0000 (33230.4012)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0393  max mem: 15572
Epoch: [12]  [1320/1404]  eta: 0:00:52  lr: 0.000082  min_lr: 0.000001  loss: 4.0457 (3.9149)  loss_scale: 32768.0000 (33226.9008)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.0396  max mem: 15572
Epoch: [12]  [1330/1404]  eta: 0:00:46  lr: 0.000082  min_lr: 0.000001  loss: 4.0457 (3.9146)  loss_scale: 32768.0000 (33223.4530)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0012  max mem: 15572
Epoch: [12]  [1340/1404]  eta: 0:00:39  lr: 0.000082  min_lr: 0.000001  loss: 3.8450 (3.9154)  loss_scale: 32768.0000 (33220.0567)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0377  max mem: 15572
Epoch: [12]  [1350/1404]  eta: 0:00:33  lr: 0.000082  min_lr: 0.000001  loss: 4.0835 (3.9174)  loss_scale: 32768.0000 (33216.7106)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0490  max mem: 15572
Epoch: [12]  [1360/1404]  eta: 0:00:27  lr: 0.000082  min_lr: 0.000001  loss: 4.0980 (3.9168)  loss_scale: 32768.0000 (33213.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0558  max mem: 15572
Epoch: [12]  [1370/1404]  eta: 0:00:21  lr: 0.000082  min_lr: 0.000001  loss: 3.9689 (3.9171)  loss_scale: 32768.0000 (33210.1648)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0442  max mem: 15572
Epoch: [12]  [1380/1404]  eta: 0:00:14  lr: 0.000082  min_lr: 0.000001  loss: 3.9301 (3.9167)  loss_scale: 32768.0000 (33206.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0008  max mem: 15572
Epoch: [12]  [1390/1404]  eta: 0:00:08  lr: 0.000082  min_lr: 0.000001  loss: 3.9301 (3.9174)  loss_scale: 32768.0000 (33203.8073)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0481  max mem: 15572
Epoch: [12]  [1400/1404]  eta: 0:00:02  lr: 0.000082  min_lr: 0.000001  loss: 4.1241 (3.9171)  loss_scale: 32768.0000 (33200.6966)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0479  max mem: 15572
Epoch: [12]  [1403/1404]  eta: 0:00:00  lr: 0.000082  min_lr: 0.000001  loss: 3.9240 (3.9168)  loss_scale: 32768.0000 (33199.7721)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0478  max mem: 15572
Epoch: [12] Total time: 0:14:30 (0.6197 s / it)
Averaged stats: lr: 0.000082  min_lr: 0.000001  loss: 3.9240 (3.9067)  loss_scale: 32768.0000 (33199.7721)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:07  loss: 1.7493 (1.7493)  acc1: 66.6667 (66.6667)  acc5: 72.2222 (72.2222)  time: 5.7924  data: 5.5277  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 2.7007 (2.5922)  acc1: 38.8889 (38.3838)  acc5: 72.2222 (69.1919)  time: 0.7985  data: 0.5922  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.7680 (2.6733)  acc1: 33.3333 (36.2434)  acc5: 72.2222 (70.8995)  time: 0.3306  data: 0.1220  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.6337 (2.4734)  acc1: 38.8889 (41.0394)  acc5: 77.7778 (73.4767)  time: 0.3204  data: 0.1030  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.1166 (2.4479)  acc1: 50.0000 (41.1924)  acc5: 77.7778 (74.3902)  time: 0.3374  data: 0.1327  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.4804 (2.4920)  acc1: 38.8889 (40.9586)  acc5: 77.7778 (74.1830)  time: 0.3872  data: 0.1873  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.6776 (2.5850)  acc1: 33.3333 (37.6138)  acc5: 72.2222 (72.1311)  time: 0.3761  data: 0.1767  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.6290 (2.5480)  acc1: 33.3333 (39.5149)  acc5: 72.2222 (72.1440)  time: 0.3866  data: 0.1836  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.3251 (2.5219)  acc1: 50.0000 (40.1920)  acc5: 77.7778 (73.3196)  time: 0.3781  data: 0.1625  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.3889 (2.5444)  acc1: 27.7778 (39.0110)  acc5: 77.7778 (72.7106)  time: 0.3660  data: 0.1523  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9317 (2.6216)  acc1: 22.2222 (37.2937)  acc5: 61.1111 (70.7371)  time: 0.3407  data: 0.1361  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.8258 (2.6105)  acc1: 33.3333 (38.2883)  acc5: 66.6667 (70.9710)  time: 0.3086  data: 0.1050  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.0420 (2.5565)  acc1: 55.5556 (39.8072)  acc5: 77.7778 (72.2222)  time: 0.3346  data: 0.1350  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8747 (2.5140)  acc1: 61.1111 (41.2214)  acc5: 83.3333 (72.6463)  time: 0.3320  data: 0.1511  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1361 (2.5174)  acc1: 44.4444 (41.2776)  acc5: 83.3333 (72.6454)  time: 0.2448  data: 0.0782  max mem: 15572
Val: Total time: 0:00:51 (0.3798 s / it)
* Acc@1 39.865 Acc@5 71.007 loss 2.568
Accuracy of the network on the 4883 val videos: 39.9%
[2025-01-10 18:57:31,469] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 18:57:31,471] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 18:57:31,471] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 18:57:31,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 18:57:33,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 18:57:33,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.86%
Epoch: [13]  [   0/1404]  eta: 3:14:20  lr: 0.000082  min_lr: 0.000001  loss: 4.4798 (4.4798)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3050  data: 7.8672  max mem: 15572
[2025-01-10 18:57:46,490] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:57:46,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:57:46,543] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:57:46,544] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [  10/1404]  eta: 0:29:58  lr: 0.000082  min_lr: 0.000001  loss: 4.0315 (4.1052)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.2904  data: 0.8374  max mem: 15572
[2025-01-10 18:57:48,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18263
[2025-01-10 18:57:48,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:57:48,579] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18263
[2025-01-10 18:57:48,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:57:48,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [  20/1404]  eta: 0:22:53  lr: 0.000082  min_lr: 0.000001  loss: 4.0172 (4.0329)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1558  max mem: 15572
Epoch: [13]  [  30/1404]  eta: 0:19:02  lr: 0.000082  min_lr: 0.000001  loss: 3.8667 (3.9993)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1051  max mem: 15572
Epoch: [13]  [  40/1404]  eta: 0:17:21  lr: 0.000082  min_lr: 0.000001  loss: 3.8657 (3.9457)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0464  max mem: 15572
Epoch: [13]  [  50/1404]  eta: 0:16:38  lr: 0.000082  min_lr: 0.000001  loss: 3.9602 (3.9697)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0838  max mem: 15572
Epoch: [13]  [  60/1404]  eta: 0:16:09  lr: 0.000082  min_lr: 0.000001  loss: 4.0075 (3.9686)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1169  max mem: 15572
Epoch: [13]  [  70/1404]  eta: 0:15:38  lr: 0.000082  min_lr: 0.000001  loss: 3.7868 (3.9545)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1208  max mem: 15572
Epoch: [13]  [  80/1404]  eta: 0:15:27  lr: 0.000082  min_lr: 0.000001  loss: 3.9917 (3.9377)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1451  max mem: 15572
Epoch: [13]  [  90/1404]  eta: 0:15:11  lr: 0.000082  min_lr: 0.000001  loss: 3.9377 (3.9269)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.1654  max mem: 15572
Epoch: [13]  [ 100/1404]  eta: 0:14:49  lr: 0.000082  min_lr: 0.000001  loss: 3.7891 (3.9117)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1017  max mem: 15572
Epoch: [13]  [ 110/1404]  eta: 0:14:31  lr: 0.000082  min_lr: 0.000001  loss: 3.8790 (3.8994)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0697  max mem: 15572
Epoch: [13]  [ 120/1404]  eta: 0:14:26  lr: 0.000082  min_lr: 0.000001  loss: 3.8932 (3.8958)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.1035  max mem: 15572
Epoch: [13]  [ 130/1404]  eta: 0:14:16  lr: 0.000082  min_lr: 0.000001  loss: 3.8932 (3.9101)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6672  data: 0.0651  max mem: 15572
[2025-01-10 18:59:08,714] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:59:08,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 18:59:08,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 18:59:08,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 140/1404]  eta: 0:14:09  lr: 0.000082  min_lr: 0.000001  loss: 4.1570 (3.9240)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0502  max mem: 15572
[2025-01-10 18:59:10,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18395
[2025-01-10 18:59:10,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:59:10,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18395
[2025-01-10 18:59:10,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 18:59:10,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 150/1404]  eta: 0:13:57  lr: 0.000082  min_lr: 0.000001  loss: 3.9977 (3.9279)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0535  max mem: 15572
Epoch: [13]  [ 160/1404]  eta: 0:13:47  lr: 0.000082  min_lr: 0.000001  loss: 3.9381 (3.9245)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0357  max mem: 15572
Epoch: [13]  [ 170/1404]  eta: 0:13:33  lr: 0.000082  min_lr: 0.000001  loss: 3.9348 (3.9184)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0565  max mem: 15572
Epoch: [13]  [ 180/1404]  eta: 0:13:22  lr: 0.000082  min_lr: 0.000001  loss: 3.8144 (3.9094)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0841  max mem: 15572
Epoch: [13]  [ 190/1404]  eta: 0:13:13  lr: 0.000082  min_lr: 0.000001  loss: 3.8153 (3.9030)  loss_scale: 32768.0000 (33625.8010)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0974  max mem: 15572
Epoch: [13]  [ 200/1404]  eta: 0:13:04  lr: 0.000082  min_lr: 0.000001  loss: 3.7312 (3.8853)  loss_scale: 32768.0000 (33583.1244)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1064  max mem: 15572
Epoch: [13]  [ 210/1404]  eta: 0:12:58  lr: 0.000082  min_lr: 0.000001  loss: 3.6754 (3.8768)  loss_scale: 32768.0000 (33544.4929)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0951  max mem: 15572
Epoch: [13]  [ 220/1404]  eta: 0:12:49  lr: 0.000082  min_lr: 0.000001  loss: 3.9474 (3.8791)  loss_scale: 32768.0000 (33509.3575)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0597  max mem: 15572
Epoch: [13]  [ 230/1404]  eta: 0:12:39  lr: 0.000082  min_lr: 0.000001  loss: 3.9979 (3.8766)  loss_scale: 32768.0000 (33477.2641)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0264  max mem: 15572
Epoch: [13]  [ 240/1404]  eta: 0:12:28  lr: 0.000082  min_lr: 0.000001  loss: 3.9156 (3.8744)  loss_scale: 32768.0000 (33447.8340)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0009  max mem: 15572
Epoch: [13]  [ 250/1404]  eta: 0:12:19  lr: 0.000082  min_lr: 0.000001  loss: 3.9782 (3.8850)  loss_scale: 32768.0000 (33420.7490)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0453  max mem: 15572
Epoch: [13]  [ 260/1404]  eta: 0:12:12  lr: 0.000082  min_lr: 0.000001  loss: 4.1015 (3.8876)  loss_scale: 32768.0000 (33395.7395)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0452  max mem: 15572
Epoch: [13]  [ 270/1404]  eta: 0:12:04  lr: 0.000082  min_lr: 0.000001  loss: 4.0492 (3.8919)  loss_scale: 32768.0000 (33372.5756)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0007  max mem: 15572
[2025-01-10 19:00:28,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:00:28,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:00:29,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:00:29,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:00:30,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18526
[2025-01-10 19:00:30,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:00:30,417] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:00:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18526
[2025-01-10 19:00:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [ 280/1404]  eta: 0:11:58  lr: 0.000082  min_lr: 0.000001  loss: 3.8933 (3.8915)  loss_scale: 32768.0000 (33584.2847)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0009  max mem: 15572
Epoch: [13]  [ 290/1404]  eta: 0:11:51  lr: 0.000082  min_lr: 0.000001  loss: 3.7644 (3.8782)  loss_scale: 32768.0000 (33556.2337)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0372  max mem: 15572
Epoch: [13]  [ 300/1404]  eta: 0:11:44  lr: 0.000082  min_lr: 0.000001  loss: 3.7690 (3.8786)  loss_scale: 32768.0000 (33530.0465)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1002  max mem: 15572
Epoch: [13]  [ 310/1404]  eta: 0:11:38  lr: 0.000082  min_lr: 0.000001  loss: 3.9256 (3.8840)  loss_scale: 32768.0000 (33505.5434)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.1179  max mem: 15572
Epoch: [13]  [ 320/1404]  eta: 0:11:28  lr: 0.000082  min_lr: 0.000001  loss: 3.8643 (3.8820)  loss_scale: 32768.0000 (33482.5670)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0549  max mem: 15572
Epoch: [13]  [ 330/1404]  eta: 0:11:26  lr: 0.000082  min_lr: 0.000001  loss: 3.7477 (3.8800)  loss_scale: 32768.0000 (33460.9789)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.1281  max mem: 15572
Epoch: [13]  [ 340/1404]  eta: 0:11:19  lr: 0.000082  min_lr: 0.000001  loss: 3.8738 (3.8811)  loss_scale: 32768.0000 (33440.6569)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.1804  max mem: 15572
Epoch: [13]  [ 350/1404]  eta: 0:11:11  lr: 0.000081  min_lr: 0.000001  loss: 4.0916 (3.8812)  loss_scale: 32768.0000 (33421.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0782  max mem: 15572
Epoch: [13]  [ 360/1404]  eta: 0:11:03  lr: 0.000081  min_lr: 0.000001  loss: 3.8760 (3.8758)  loss_scale: 32768.0000 (33403.3906)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0528  max mem: 15572
Epoch: [13]  [ 370/1404]  eta: 0:10:58  lr: 0.000081  min_lr: 0.000001  loss: 3.6663 (3.8735)  loss_scale: 32768.0000 (33386.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1169  max mem: 15572
Epoch: [13]  [ 380/1404]  eta: 0:10:49  lr: 0.000081  min_lr: 0.000001  loss: 3.7291 (3.8700)  loss_scale: 32768.0000 (33370.0367)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1040  max mem: 15572
Epoch: [13]  [ 390/1404]  eta: 0:10:42  lr: 0.000081  min_lr: 0.000001  loss: 3.9938 (3.8749)  loss_scale: 32768.0000 (33354.6394)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0455  max mem: 15572
Epoch: [13]  [ 400/1404]  eta: 0:10:35  lr: 0.000081  min_lr: 0.000001  loss: 4.0009 (3.8750)  loss_scale: 32768.0000 (33340.0100)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1031  max mem: 15572
[2025-01-10 19:01:50,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:01:50,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:01:50,419] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:01:50,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 410/1404]  eta: 0:10:28  lr: 0.000081  min_lr: 0.000001  loss: 3.9399 (3.8781)  loss_scale: 32768.0000 (33963.9124)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1188  max mem: 15572
Epoch: [13]  [ 420/1404]  eta: 0:10:22  lr: 0.000081  min_lr: 0.000001  loss: 4.0841 (3.8807)  loss_scale: 65536.0000 (34713.8432)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1269  max mem: 15572
[2025-01-10 19:02:02,172] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18673
[2025-01-10 19:02:02,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:02:02,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:02:02,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18673
[2025-01-10 19:02:02,177] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [ 430/1404]  eta: 0:10:15  lr: 0.000081  min_lr: 0.000001  loss: 4.0841 (3.8789)  loss_scale: 32768.0000 (34668.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1515  max mem: 15572
Epoch: [13]  [ 440/1404]  eta: 0:10:08  lr: 0.000081  min_lr: 0.000001  loss: 4.0583 (3.8870)  loss_scale: 32768.0000 (34625.5964)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0900  max mem: 15572
Epoch: [13]  [ 450/1404]  eta: 0:10:01  lr: 0.000081  min_lr: 0.000001  loss: 4.1091 (3.8889)  loss_scale: 32768.0000 (34584.4080)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0942  max mem: 15572
Epoch: [13]  [ 460/1404]  eta: 0:09:56  lr: 0.000081  min_lr: 0.000001  loss: 3.8734 (3.8879)  loss_scale: 32768.0000 (34545.0065)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1402  max mem: 15572
Epoch: [13]  [ 470/1404]  eta: 0:09:49  lr: 0.000081  min_lr: 0.000001  loss: 3.7382 (3.8868)  loss_scale: 32768.0000 (34507.2781)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1276  max mem: 15572
Epoch: [13]  [ 480/1404]  eta: 0:09:43  lr: 0.000081  min_lr: 0.000001  loss: 3.6052 (3.8808)  loss_scale: 32768.0000 (34471.1185)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1370  max mem: 15572
Epoch: [13]  [ 490/1404]  eta: 0:09:34  lr: 0.000081  min_lr: 0.000001  loss: 3.8095 (3.8793)  loss_scale: 32768.0000 (34436.4318)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0794  max mem: 15572
Epoch: [13]  [ 500/1404]  eta: 0:09:28  lr: 0.000081  min_lr: 0.000001  loss: 3.8701 (3.8779)  loss_scale: 32768.0000 (34403.1297)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0771  max mem: 15572
Epoch: [13]  [ 510/1404]  eta: 0:09:22  lr: 0.000081  min_lr: 0.000001  loss: 3.7592 (3.8733)  loss_scale: 32768.0000 (34371.1311)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1423  max mem: 15572
Epoch: [13]  [ 520/1404]  eta: 0:09:15  lr: 0.000081  min_lr: 0.000001  loss: 3.9461 (3.8795)  loss_scale: 32768.0000 (34340.3608)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1230  max mem: 15572
Epoch: [13]  [ 530/1404]  eta: 0:09:09  lr: 0.000081  min_lr: 0.000001  loss: 4.0596 (3.8781)  loss_scale: 32768.0000 (34310.7495)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0910  max mem: 15572
Epoch: [13]  [ 540/1404]  eta: 0:09:03  lr: 0.000081  min_lr: 0.000001  loss: 3.9786 (3.8778)  loss_scale: 32768.0000 (34282.2329)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1174  max mem: 15572
[2025-01-10 19:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:03:19,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:03:19,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 550/1404]  eta: 0:08:55  lr: 0.000081  min_lr: 0.000001  loss: 3.9346 (3.8772)  loss_scale: 32768.0000 (34314.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0794  max mem: 15572
Epoch: [13]  [ 560/1404]  eta: 0:08:48  lr: 0.000081  min_lr: 0.000001  loss: 3.8805 (3.8783)  loss_scale: 65536.0000 (34870.7594)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0374  max mem: 15572
[2025-01-10 19:03:28,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18815
[2025-01-10 19:03:28,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:03:28,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18815
[2025-01-10 19:03:28,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:03:28,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 570/1404]  eta: 0:08:42  lr: 0.000081  min_lr: 0.000001  loss: 3.8688 (3.8778)  loss_scale: 65536.0000 (34948.7075)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1192  max mem: 15572
Epoch: [13]  [ 580/1404]  eta: 0:08:36  lr: 0.000081  min_lr: 0.000001  loss: 3.7347 (3.8754)  loss_scale: 32768.0000 (34911.1738)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1419  max mem: 15572
Epoch: [13]  [ 590/1404]  eta: 0:08:29  lr: 0.000081  min_lr: 0.000001  loss: 3.7779 (3.8756)  loss_scale: 32768.0000 (34874.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1150  max mem: 15572
Epoch: [13]  [ 600/1404]  eta: 0:08:22  lr: 0.000081  min_lr: 0.000001  loss: 3.6821 (3.8726)  loss_scale: 32768.0000 (34839.8536)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0986  max mem: 15572
Epoch: [13]  [ 610/1404]  eta: 0:08:17  lr: 0.000081  min_lr: 0.000001  loss: 3.7343 (3.8733)  loss_scale: 32768.0000 (34805.9444)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1205  max mem: 15572
Epoch: [13]  [ 620/1404]  eta: 0:08:11  lr: 0.000081  min_lr: 0.000001  loss: 3.9569 (3.8721)  loss_scale: 32768.0000 (34773.1272)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.1573  max mem: 15572
Epoch: [13]  [ 630/1404]  eta: 0:08:04  lr: 0.000081  min_lr: 0.000001  loss: 4.0323 (3.8761)  loss_scale: 32768.0000 (34741.3502)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0944  max mem: 15572
Epoch: [13]  [ 640/1404]  eta: 0:07:58  lr: 0.000081  min_lr: 0.000001  loss: 4.0720 (3.8791)  loss_scale: 32768.0000 (34710.5647)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0605  max mem: 15572
Epoch: [13]  [ 650/1404]  eta: 0:07:51  lr: 0.000081  min_lr: 0.000001  loss: 4.0037 (3.8800)  loss_scale: 32768.0000 (34680.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0468  max mem: 15572
Epoch: [13]  [ 660/1404]  eta: 0:07:45  lr: 0.000081  min_lr: 0.000001  loss: 3.9497 (3.8783)  loss_scale: 32768.0000 (34651.7882)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0009  max mem: 15572
Epoch: [13]  [ 670/1404]  eta: 0:07:38  lr: 0.000081  min_lr: 0.000001  loss: 3.7431 (3.8773)  loss_scale: 32768.0000 (34623.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.0009  max mem: 15572
Epoch: [13]  [ 680/1404]  eta: 0:07:32  lr: 0.000081  min_lr: 0.000001  loss: 3.9258 (3.8791)  loss_scale: 32768.0000 (34596.4640)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0308  max mem: 15572
Epoch: [13]  [ 690/1404]  eta: 0:07:25  lr: 0.000081  min_lr: 0.000001  loss: 3.8983 (3.8772)  loss_scale: 32768.0000 (34570.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0840  max mem: 15572
[2025-01-10 19:04:46,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:04:46,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:04:46,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:04:46,721] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:04:48,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18946
[2025-01-10 19:04:48,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:04:48,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:04:48,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18946
[2025-01-10 19:04:48,938] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [ 700/1404]  eta: 0:07:19  lr: 0.000081  min_lr: 0.000001  loss: 3.8104 (3.8768)  loss_scale: 32768.0000 (34637.7860)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1069  max mem: 15572
Epoch: [13]  [ 710/1404]  eta: 0:07:13  lr: 0.000081  min_lr: 0.000001  loss: 3.8104 (3.8737)  loss_scale: 32768.0000 (34611.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0792  max mem: 15572
Epoch: [13]  [ 720/1404]  eta: 0:07:07  lr: 0.000081  min_lr: 0.000001  loss: 3.8252 (3.8744)  loss_scale: 32768.0000 (34585.9196)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0622  max mem: 15572
Epoch: [13]  [ 730/1404]  eta: 0:07:01  lr: 0.000081  min_lr: 0.000001  loss: 3.9310 (3.8757)  loss_scale: 32768.0000 (34561.0506)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1067  max mem: 15572
Epoch: [13]  [ 740/1404]  eta: 0:06:54  lr: 0.000081  min_lr: 0.000001  loss: 3.8404 (3.8742)  loss_scale: 32768.0000 (34536.8529)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0920  max mem: 15572
[2025-01-10 19:05:20,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=118, lr=[7.816594832876371e-07, 7.816594832876371e-07, 1.1166564046966245e-06, 1.1166564046966245e-06, 1.5952234352808924e-06, 1.5952234352808924e-06, 2.2788906218298463e-06, 2.2788906218298463e-06, 3.255558031185495e-06, 3.255558031185495e-06, 4.65079718740785e-06, 4.65079718740785e-06, 6.643995982011215e-06, 6.643995982011215e-06, 9.491422831444594e-06, 9.491422831444594e-06, 1.3559175473492277e-05, 1.3559175473492277e-05, 1.937025067641754e-05, 1.937025067641754e-05, 2.7671786680596486e-05, 2.7671786680596486e-05, 3.9531123829423554e-05, 3.9531123829423554e-05, 5.647303404203365e-05, 5.647303404203365e-05, 8.067576291719094e-05, 8.067576291719094e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:05:20,958] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=45.26533857743069, CurrSamplesPerSec=46.16195409177518, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [ 750/1404]  eta: 0:06:48  lr: 0.000081  min_lr: 0.000001  loss: 3.9934 (3.8727)  loss_scale: 32768.0000 (34513.2996)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0502  max mem: 15572
Epoch: [13]  [ 760/1404]  eta: 0:06:42  lr: 0.000081  min_lr: 0.000001  loss: 4.0292 (3.8729)  loss_scale: 32768.0000 (34490.3653)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0412  max mem: 15572
Epoch: [13]  [ 770/1404]  eta: 0:06:36  lr: 0.000081  min_lr: 0.000001  loss: 4.1352 (3.8746)  loss_scale: 32768.0000 (34468.0259)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0442  max mem: 15572
Epoch: [13]  [ 780/1404]  eta: 0:06:29  lr: 0.000081  min_lr: 0.000001  loss: 4.1185 (3.8756)  loss_scale: 32768.0000 (34446.2586)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0409  max mem: 15572
Epoch: [13]  [ 790/1404]  eta: 0:06:22  lr: 0.000081  min_lr: 0.000001  loss: 3.9126 (3.8745)  loss_scale: 32768.0000 (34425.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0345  max mem: 15572
Epoch: [13]  [ 800/1404]  eta: 0:06:16  lr: 0.000081  min_lr: 0.000001  loss: 3.8037 (3.8740)  loss_scale: 32768.0000 (34404.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0257  max mem: 15572
Epoch: [13]  [ 810/1404]  eta: 0:06:10  lr: 0.000081  min_lr: 0.000001  loss: 3.8235 (3.8738)  loss_scale: 32768.0000 (34384.1776)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0014  max mem: 15572
Epoch: [13]  [ 820/1404]  eta: 0:06:03  lr: 0.000081  min_lr: 0.000001  loss: 3.8645 (3.8763)  loss_scale: 32768.0000 (34364.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0013  max mem: 15572
[2025-01-10 19:06:08,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:06:08,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:06:08,303] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:06:08,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:06:12,732] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19082
[2025-01-10 19:06:12,733] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19082
[2025-01-10 19:06:12,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:06:12,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:06:12,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 830/1404]  eta: 0:05:58  lr: 0.000081  min_lr: 0.000001  loss: 4.0473 (3.8772)  loss_scale: 32768.0000 (34621.3045)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.0356  max mem: 15572
Epoch: [13]  [ 840/1404]  eta: 0:05:51  lr: 0.000080  min_lr: 0.000001  loss: 3.9731 (3.8743)  loss_scale: 32768.0000 (34599.2675)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.0530  max mem: 15572
Epoch: [13]  [ 850/1404]  eta: 0:05:45  lr: 0.000080  min_lr: 0.000001  loss: 3.9822 (3.8788)  loss_scale: 32768.0000 (34577.7485)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0614  max mem: 15572
Epoch: [13]  [ 860/1404]  eta: 0:05:39  lr: 0.000080  min_lr: 0.000001  loss: 4.0862 (3.8813)  loss_scale: 32768.0000 (34556.7294)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1507  max mem: 15572
Epoch: [13]  [ 870/1404]  eta: 0:05:33  lr: 0.000080  min_lr: 0.000001  loss: 4.0024 (3.8810)  loss_scale: 32768.0000 (34536.1929)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.1809  max mem: 15572
Epoch: [13]  [ 880/1404]  eta: 0:05:27  lr: 0.000080  min_lr: 0.000001  loss: 3.7846 (3.8775)  loss_scale: 32768.0000 (34516.1226)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.1834  max mem: 15572
Epoch: [13]  [ 890/1404]  eta: 0:05:21  lr: 0.000080  min_lr: 0.000001  loss: 3.7469 (3.8764)  loss_scale: 32768.0000 (34496.5028)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1668  max mem: 15572
Epoch: [13]  [ 900/1404]  eta: 0:05:14  lr: 0.000080  min_lr: 0.000001  loss: 3.8029 (3.8770)  loss_scale: 32768.0000 (34477.3185)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0576  max mem: 15572
Epoch: [13]  [ 910/1404]  eta: 0:05:07  lr: 0.000080  min_lr: 0.000001  loss: 3.8309 (3.8738)  loss_scale: 32768.0000 (34458.5554)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0102  max mem: 15572
Epoch: [13]  [ 920/1404]  eta: 0:05:01  lr: 0.000080  min_lr: 0.000001  loss: 3.9342 (3.8750)  loss_scale: 32768.0000 (34440.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0519  max mem: 15572
Epoch: [13]  [ 930/1404]  eta: 0:04:55  lr: 0.000080  min_lr: 0.000001  loss: 4.0018 (3.8759)  loss_scale: 32768.0000 (34422.2385)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1059  max mem: 15572
Epoch: [13]  [ 940/1404]  eta: 0:04:48  lr: 0.000080  min_lr: 0.000001  loss: 3.9434 (3.8767)  loss_scale: 32768.0000 (34404.6589)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1361  max mem: 15572
Epoch: [13]  [ 950/1404]  eta: 0:04:42  lr: 0.000080  min_lr: 0.000001  loss: 4.0904 (3.8804)  loss_scale: 32768.0000 (34387.4490)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1483  max mem: 15572
[2025-01-10 19:07:31,836] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:07:31,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:07:31,871] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:07:31,872] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:07:32,450] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19212
[2025-01-10 19:07:32,450] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:07:32,451] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:07:32,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19212
[2025-01-10 19:07:32,451] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [ 960/1404]  eta: 0:04:36  lr: 0.000080  min_lr: 0.000001  loss: 4.1334 (3.8823)  loss_scale: 32768.0000 (34404.6951)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1074  max mem: 15572
Epoch: [13]  [ 970/1404]  eta: 0:04:29  lr: 0.000080  min_lr: 0.000001  loss: 4.0343 (3.8832)  loss_scale: 32768.0000 (34387.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0543  max mem: 15572
Epoch: [13]  [ 980/1404]  eta: 0:04:23  lr: 0.000080  min_lr: 0.000001  loss: 4.0159 (3.8858)  loss_scale: 32768.0000 (34371.3272)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0727  max mem: 15572
Epoch: [13]  [ 990/1404]  eta: 0:04:17  lr: 0.000080  min_lr: 0.000001  loss: 3.9734 (3.8853)  loss_scale: 32768.0000 (34355.1483)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1206  max mem: 15572
Epoch: [13]  [1000/1404]  eta: 0:04:11  lr: 0.000080  min_lr: 0.000001  loss: 3.9135 (3.8844)  loss_scale: 32768.0000 (34339.2927)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1220  max mem: 15572
Epoch: [13]  [1010/1404]  eta: 0:04:04  lr: 0.000080  min_lr: 0.000001  loss: 3.7352 (3.8838)  loss_scale: 32768.0000 (34323.7507)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0934  max mem: 15572
Epoch: [13]  [1020/1404]  eta: 0:03:58  lr: 0.000080  min_lr: 0.000001  loss: 3.6938 (3.8816)  loss_scale: 32768.0000 (34308.5132)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0780  max mem: 15572
Epoch: [13]  [1030/1404]  eta: 0:03:52  lr: 0.000080  min_lr: 0.000001  loss: 3.5428 (3.8789)  loss_scale: 32768.0000 (34293.5713)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0520  max mem: 15572
Epoch: [13]  [1040/1404]  eta: 0:03:46  lr: 0.000080  min_lr: 0.000001  loss: 3.7679 (3.8782)  loss_scale: 32768.0000 (34278.9164)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.1100  max mem: 15572
Epoch: [13]  [1050/1404]  eta: 0:03:40  lr: 0.000080  min_lr: 0.000001  loss: 4.0205 (3.8795)  loss_scale: 32768.0000 (34264.5404)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.1243  max mem: 15572
Epoch: [13]  [1060/1404]  eta: 0:03:34  lr: 0.000080  min_lr: 0.000001  loss: 4.0407 (3.8800)  loss_scale: 32768.0000 (34250.4354)  weight_decay: 0.0500 (0.0500)  time: 0.6847  data: 0.1468  max mem: 15572
Epoch: [13]  [1070/1404]  eta: 0:03:28  lr: 0.000080  min_lr: 0.000001  loss: 3.8690 (3.8795)  loss_scale: 32768.0000 (34236.5938)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.2429  max mem: 15572
Epoch: [13]  [1080/1404]  eta: 0:03:21  lr: 0.000080  min_lr: 0.000001  loss: 3.6210 (3.8772)  loss_scale: 32768.0000 (34223.0083)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1271  max mem: 15572
[2025-01-10 19:08:52,715] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:08:52,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:08:52,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:08:52,800] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [1090/1404]  eta: 0:03:15  lr: 0.000080  min_lr: 0.000001  loss: 3.7005 (3.8772)  loss_scale: 32768.0000 (34269.7415)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0095  max mem: 15572
[2025-01-10 19:08:56,686] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19348
[2025-01-10 19:08:56,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:08:56,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:08:56,714] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19348
[2025-01-10 19:08:56,715] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [1100/1404]  eta: 0:03:09  lr: 0.000080  min_lr: 0.000001  loss: 3.9155 (3.8787)  loss_scale: 32768.0000 (34404.9119)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0807  max mem: 15572
Epoch: [13]  [1110/1404]  eta: 0:03:02  lr: 0.000080  min_lr: 0.000001  loss: 3.9809 (3.8799)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1017  max mem: 15572
Epoch: [13]  [1120/1404]  eta: 0:02:56  lr: 0.000080  min_lr: 0.000001  loss: 3.9367 (3.8785)  loss_scale: 32768.0000 (34375.7074)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1139  max mem: 15572
Epoch: [13]  [1130/1404]  eta: 0:02:50  lr: 0.000080  min_lr: 0.000001  loss: 3.8019 (3.8773)  loss_scale: 32768.0000 (34361.4925)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1267  max mem: 15572
Epoch: [13]  [1140/1404]  eta: 0:02:44  lr: 0.000080  min_lr: 0.000001  loss: 3.7082 (3.8746)  loss_scale: 32768.0000 (34347.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6795  data: 0.1715  max mem: 15572
Epoch: [13]  [1150/1404]  eta: 0:02:38  lr: 0.000080  min_lr: 0.000001  loss: 3.7037 (3.8742)  loss_scale: 32768.0000 (34333.8036)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1289  max mem: 15572
Epoch: [13]  [1160/1404]  eta: 0:02:31  lr: 0.000080  min_lr: 0.000001  loss: 3.9984 (3.8762)  loss_scale: 32768.0000 (34320.3170)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0130  max mem: 15572
Epoch: [13]  [1170/1404]  eta: 0:02:25  lr: 0.000080  min_lr: 0.000001  loss: 4.0104 (3.8766)  loss_scale: 32768.0000 (34307.0606)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0458  max mem: 15572
Epoch: [13]  [1180/1404]  eta: 0:02:19  lr: 0.000080  min_lr: 0.000001  loss: 3.9256 (3.8762)  loss_scale: 32768.0000 (34294.0288)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0934  max mem: 15572
Epoch: [13]  [1190/1404]  eta: 0:02:12  lr: 0.000080  min_lr: 0.000001  loss: 3.9099 (3.8770)  loss_scale: 32768.0000 (34281.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0900  max mem: 15572
Epoch: [13]  [1200/1404]  eta: 0:02:06  lr: 0.000080  min_lr: 0.000001  loss: 4.0320 (3.8775)  loss_scale: 32768.0000 (34268.6162)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0391  max mem: 15572
Epoch: [13]  [1210/1404]  eta: 0:02:00  lr: 0.000080  min_lr: 0.000001  loss: 4.0448 (3.8787)  loss_scale: 32768.0000 (34256.2246)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0660  max mem: 15572
Epoch: [13]  [1220/1404]  eta: 0:01:54  lr: 0.000080  min_lr: 0.000001  loss: 3.8702 (3.8783)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0912  max mem: 15572
[2025-01-10 19:10:15,037] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:10:15,037] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:10:15,040] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:10:15,041] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:10:17,207] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19480
[2025-01-10 19:10:17,208] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:10:17,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:10:17,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19480
[2025-01-10 19:10:17,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [13]  [1230/1404]  eta: 0:01:47  lr: 0.000080  min_lr: 0.000001  loss: 3.8702 (3.8782)  loss_scale: 32768.0000 (34311.9025)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0412  max mem: 15572
Epoch: [13]  [1240/1404]  eta: 0:01:41  lr: 0.000080  min_lr: 0.000001  loss: 3.8460 (3.8776)  loss_scale: 32768.0000 (34299.4617)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0230  max mem: 15572
Epoch: [13]  [1250/1404]  eta: 0:01:35  lr: 0.000080  min_lr: 0.000001  loss: 3.9099 (3.8790)  loss_scale: 32768.0000 (34287.2198)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0174  max mem: 15572
Epoch: [13]  [1260/1404]  eta: 0:01:29  lr: 0.000080  min_lr: 0.000001  loss: 3.9168 (3.8788)  loss_scale: 32768.0000 (34275.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0185  max mem: 15572
Epoch: [13]  [1270/1404]  eta: 0:01:23  lr: 0.000080  min_lr: 0.000001  loss: 3.7195 (3.8783)  loss_scale: 32768.0000 (34263.3139)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0181  max mem: 15572
Epoch: [13]  [1280/1404]  eta: 0:01:17  lr: 0.000080  min_lr: 0.000001  loss: 3.7560 (3.8777)  loss_scale: 32768.0000 (34251.6409)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0009  max mem: 15572
Epoch: [13]  [1290/1404]  eta: 0:01:10  lr: 0.000080  min_lr: 0.000001  loss: 3.9663 (3.8799)  loss_scale: 32768.0000 (34240.1487)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.0011  max mem: 15572
Epoch: [13]  [1300/1404]  eta: 0:01:04  lr: 0.000080  min_lr: 0.000001  loss: 3.8631 (3.8798)  loss_scale: 32768.0000 (34228.8332)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0156  max mem: 15572
Epoch: [13]  [1310/1404]  eta: 0:00:58  lr: 0.000079  min_lr: 0.000001  loss: 3.8723 (3.8809)  loss_scale: 32768.0000 (34217.6903)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0154  max mem: 15572
Epoch: [13]  [1320/1404]  eta: 0:00:52  lr: 0.000079  min_lr: 0.000001  loss: 4.0286 (3.8826)  loss_scale: 32768.0000 (34206.7161)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0011  max mem: 15572
Epoch: [13]  [1330/1404]  eta: 0:00:45  lr: 0.000079  min_lr: 0.000001  loss: 4.1208 (3.8831)  loss_scale: 32768.0000 (34195.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0010  max mem: 15572
Epoch: [13]  [1340/1404]  eta: 0:00:39  lr: 0.000079  min_lr: 0.000001  loss: 3.9533 (3.8832)  loss_scale: 32768.0000 (34185.2588)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0010  max mem: 15572
Epoch: [13]  [1350/1404]  eta: 0:00:33  lr: 0.000079  min_lr: 0.000001  loss: 3.9533 (3.8838)  loss_scale: 32768.0000 (34174.7683)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0827  max mem: 15572
[2025-01-10 19:11:37,498] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:11:37,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:11:37,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:11:37,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:11:37,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19610
[2025-01-10 19:11:37,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:11:37,981] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19610
[2025-01-10 19:11:37,981] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:11:37,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [1360/1404]  eta: 0:00:27  lr: 0.000079  min_lr: 0.000001  loss: 3.9828 (3.8845)  loss_scale: 32768.0000 (34188.5084)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.1200  max mem: 15572
Epoch: [13]  [1370/1404]  eta: 0:00:21  lr: 0.000079  min_lr: 0.000001  loss: 3.8481 (3.8842)  loss_scale: 32768.0000 (34178.1473)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0614  max mem: 15572
Epoch: [13]  [1380/1404]  eta: 0:00:14  lr: 0.000079  min_lr: 0.000001  loss: 3.8288 (3.8858)  loss_scale: 32768.0000 (34167.9363)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0537  max mem: 15572
Epoch: [13]  [1390/1404]  eta: 0:00:08  lr: 0.000079  min_lr: 0.000001  loss: 3.8813 (3.8843)  loss_scale: 32768.0000 (34157.8720)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0307  max mem: 15572
Epoch: [13]  [1400/1404]  eta: 0:00:02  lr: 0.000079  min_lr: 0.000001  loss: 3.7698 (3.8819)  loss_scale: 32768.0000 (34147.9515)  weight_decay: 0.0500 (0.0500)  time: 0.4702  data: 0.0007  max mem: 15572
Epoch: [13]  [1403/1404]  eta: 0:00:00  lr: 0.000079  min_lr: 0.000001  loss: 3.7183 (3.8812)  loss_scale: 32768.0000 (34145.0028)  weight_decay: 0.0500 (0.0500)  time: 0.4230  data: 0.0005  max mem: 15572
Epoch: [13] Total time: 0:14:28 (0.6183 s / it)
Averaged stats: lr: 0.000079  min_lr: 0.000001  loss: 3.7183 (3.8920)  loss_scale: 32768.0000 (34145.0028)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:52  loss: 1.6707 (1.6707)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 3.9154  data: 3.7354  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 2.5252 (2.4613)  acc1: 44.4444 (40.4040)  acc5: 72.2222 (71.2121)  time: 0.8157  data: 0.6095  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 2.7278 (2.5993)  acc1: 38.8889 (37.5661)  acc5: 66.6667 (70.8995)  time: 0.4454  data: 0.2385  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.5713 (2.4144)  acc1: 38.8889 (42.1147)  acc5: 72.2222 (73.4767)  time: 0.3367  data: 0.1304  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 1.9609 (2.3865)  acc1: 50.0000 (42.8184)  acc5: 77.7778 (73.8482)  time: 0.3307  data: 0.1239  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.3511 (2.4272)  acc1: 38.8889 (42.3747)  acc5: 77.7778 (74.2919)  time: 0.3919  data: 0.1861  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.6342 (2.5182)  acc1: 33.3333 (39.4353)  acc5: 72.2222 (72.4954)  time: 0.4049  data: 0.1922  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.5225 (2.4739)  acc1: 38.8889 (41.2363)  acc5: 72.2222 (72.6135)  time: 0.3544  data: 0.1465  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.1680 (2.4845)  acc1: 44.4444 (40.3292)  acc5: 72.2222 (72.9081)  time: 0.3635  data: 0.1614  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.5389 (2.5036)  acc1: 33.3333 (39.4383)  acc5: 72.2222 (72.5275)  time: 0.3889  data: 0.1726  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.6601 (2.5668)  acc1: 33.3333 (38.2288)  acc5: 66.6667 (70.7371)  time: 0.3396  data: 0.1272  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.6601 (2.5401)  acc1: 38.8889 (39.3393)  acc5: 66.6667 (71.0210)  time: 0.3819  data: 0.1835  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.9592 (2.4725)  acc1: 50.0000 (40.6336)  acc5: 83.3333 (72.4518)  time: 0.4202  data: 0.2224  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.7855 (2.4284)  acc1: 55.5556 (41.7303)  acc5: 88.8889 (72.9856)  time: 0.2927  data: 0.1129  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1479 (2.4284)  acc1: 44.4444 (41.7690)  acc5: 83.3333 (73.3006)  time: 0.2061  data: 0.0401  max mem: 15572
Val: Total time: 0:00:52 (0.3881 s / it)
* Acc@1 41.216 Acc@5 72.154 loss 2.470
Accuracy of the network on the 4883 val videos: 41.2%
[2025-01-10 19:12:55,034] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 19:12:55,036] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 19:12:55,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 19:12:55,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 19:12:57,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 19:12:57,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.22%
Epoch: [14]  [   0/1404]  eta: 3:10:47  lr: 0.000079  min_lr: 0.000001  loss: 3.4005 (3.4005)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.1534  data: 7.6388  max mem: 15572
Epoch: [14]  [  10/1404]  eta: 0:30:25  lr: 0.000079  min_lr: 0.000001  loss: 4.2369 (3.9081)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3096  data: 0.8190  max mem: 15572
Epoch: [14]  [  20/1404]  eta: 0:22:05  lr: 0.000079  min_lr: 0.000001  loss: 4.0882 (3.8511)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0914  max mem: 15572
Epoch: [14]  [  30/1404]  eta: 0:21:05  lr: 0.000079  min_lr: 0.000001  loss: 3.6473 (3.8132)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.0915  max mem: 15572
Epoch: [14]  [  40/1404]  eta: 0:19:13  lr: 0.000079  min_lr: 0.000001  loss: 3.7443 (3.8484)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7274  data: 0.0719  max mem: 15572
Epoch: [14]  [  50/1404]  eta: 0:17:38  lr: 0.000079  min_lr: 0.000001  loss: 4.0235 (3.8661)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0037  max mem: 15572
Epoch: [14]  [  60/1404]  eta: 0:17:10  lr: 0.000079  min_lr: 0.000001  loss: 3.9256 (3.8353)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0010  max mem: 15572
Epoch: [14]  [  70/1404]  eta: 0:16:47  lr: 0.000079  min_lr: 0.000001  loss: 3.8245 (3.8945)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6880  data: 0.0011  max mem: 15572
Epoch: [14]  [  80/1404]  eta: 0:16:13  lr: 0.000079  min_lr: 0.000001  loss: 4.2025 (3.9117)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0011  max mem: 15572
[2025-01-10 19:13:58,686] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:13:58,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:13:58,804] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:13:58,805] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:14:00,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19743
[2025-01-10 19:14:00,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:14:00,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:14:00,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19743
[2025-01-10 19:14:00,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [14]  [  90/1404]  eta: 0:15:53  lr: 0.000079  min_lr: 0.000001  loss: 3.8119 (3.8994)  loss_scale: 32768.0000 (34208.3516)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0010  max mem: 15572
Epoch: [14]  [ 100/1404]  eta: 0:15:34  lr: 0.000079  min_lr: 0.000001  loss: 3.7501 (3.8994)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0010  max mem: 15572
Epoch: [14]  [ 110/1404]  eta: 0:15:10  lr: 0.000079  min_lr: 0.000001  loss: 3.9735 (3.9128)  loss_scale: 32768.0000 (33948.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0010  max mem: 15572
Epoch: [14]  [ 120/1404]  eta: 0:14:54  lr: 0.000079  min_lr: 0.000001  loss: 3.9418 (3.8986)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0008  max mem: 15572
Epoch: [14]  [ 130/1404]  eta: 0:14:37  lr: 0.000079  min_lr: 0.000001  loss: 3.7571 (3.8949)  loss_scale: 32768.0000 (33768.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0009  max mem: 15572
Epoch: [14]  [ 140/1404]  eta: 0:14:23  lr: 0.000079  min_lr: 0.000001  loss: 3.8216 (3.8914)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0008  max mem: 15572
Epoch: [14]  [ 150/1404]  eta: 0:14:12  lr: 0.000079  min_lr: 0.000001  loss: 3.8023 (3.8861)  loss_scale: 32768.0000 (33636.0265)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0008  max mem: 15572
Epoch: [14]  [ 160/1404]  eta: 0:14:01  lr: 0.000079  min_lr: 0.000001  loss: 3.8596 (3.8787)  loss_scale: 32768.0000 (33582.1118)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0010  max mem: 15572
Epoch: [14]  [ 170/1404]  eta: 0:13:56  lr: 0.000079  min_lr: 0.000001  loss: 3.8596 (3.8738)  loss_scale: 32768.0000 (33534.5029)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0008  max mem: 15572
Epoch: [14]  [ 180/1404]  eta: 0:13:39  lr: 0.000079  min_lr: 0.000001  loss: 4.0170 (3.8834)  loss_scale: 32768.0000 (33492.1547)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0007  max mem: 15572
Epoch: [14]  [ 190/1404]  eta: 0:13:30  lr: 0.000079  min_lr: 0.000001  loss: 4.0170 (3.8821)  loss_scale: 32768.0000 (33454.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0010  max mem: 15572
Epoch: [14]  [ 200/1404]  eta: 0:13:15  lr: 0.000079  min_lr: 0.000001  loss: 3.6904 (3.8694)  loss_scale: 32768.0000 (33420.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0009  max mem: 15572
Epoch: [14]  [ 210/1404]  eta: 0:13:09  lr: 0.000079  min_lr: 0.000001  loss: 3.6904 (3.8673)  loss_scale: 32768.0000 (33389.1943)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0006  max mem: 15572
[2025-01-10 19:15:20,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:15:20,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:15:20,384] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:15:20,385] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 220/1404]  eta: 0:12:56  lr: 0.000079  min_lr: 0.000001  loss: 3.7657 (3.8606)  loss_scale: 32768.0000 (34102.4434)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0006  max mem: 15572
[2025-01-10 19:15:23,882] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19878
[2025-01-10 19:15:23,883] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:15:23,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19878
[2025-01-10 19:15:23,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:15:23,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 230/1404]  eta: 0:12:44  lr: 0.000079  min_lr: 0.000001  loss: 3.7959 (3.8664)  loss_scale: 32768.0000 (34186.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0009  max mem: 15572
Epoch: [14]  [ 240/1404]  eta: 0:12:36  lr: 0.000079  min_lr: 0.000001  loss: 3.8864 (3.8640)  loss_scale: 32768.0000 (34127.6680)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0010  max mem: 15572
Epoch: [14]  [ 250/1404]  eta: 0:12:30  lr: 0.000079  min_lr: 0.000001  loss: 3.8502 (3.8594)  loss_scale: 32768.0000 (34073.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0009  max mem: 15572
[2025-01-10 19:15:41,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19908
[2025-01-10 19:15:41,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19908
[2025-01-10 19:15:41,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:15:41,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:15:41,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 260/1404]  eta: 0:12:20  lr: 0.000079  min_lr: 0.000001  loss: 3.9436 (3.8618)  loss_scale: 32768.0000 (33458.5134)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0011  max mem: 15572
Epoch: [14]  [ 270/1404]  eta: 0:12:10  lr: 0.000079  min_lr: 0.000001  loss: 3.9716 (3.8614)  loss_scale: 16384.0000 (32828.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0010  max mem: 15572
Epoch: [14]  [ 280/1404]  eta: 0:11:59  lr: 0.000079  min_lr: 0.000001  loss: 4.1160 (3.8694)  loss_scale: 16384.0000 (32243.2456)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0007  max mem: 15572
Epoch: [14]  [ 290/1404]  eta: 0:11:51  lr: 0.000079  min_lr: 0.000001  loss: 3.9832 (3.8661)  loss_scale: 16384.0000 (31698.2543)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0207  max mem: 15572
Epoch: [14]  [ 300/1404]  eta: 0:11:43  lr: 0.000079  min_lr: 0.000001  loss: 3.7846 (3.8697)  loss_scale: 16384.0000 (31189.4751)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0396  max mem: 15572
Epoch: [14]  [ 310/1404]  eta: 0:11:37  lr: 0.000079  min_lr: 0.000001  loss: 3.9465 (3.8689)  loss_scale: 16384.0000 (30713.4148)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0273  max mem: 15572
Epoch: [14]  [ 320/1404]  eta: 0:11:29  lr: 0.000079  min_lr: 0.000001  loss: 3.7554 (3.8649)  loss_scale: 16384.0000 (30267.0156)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0586  max mem: 15572
Epoch: [14]  [ 330/1404]  eta: 0:11:20  lr: 0.000079  min_lr: 0.000001  loss: 3.7554 (3.8633)  loss_scale: 16384.0000 (29847.5891)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0509  max mem: 15572
Epoch: [14]  [ 340/1404]  eta: 0:11:13  lr: 0.000079  min_lr: 0.000001  loss: 3.8616 (3.8640)  loss_scale: 16384.0000 (29452.7625)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0695  max mem: 15572
[2025-01-10 19:16:35,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=126, lr=[7.608994897237253e-07, 7.608994897237253e-07, 1.0869992710338935e-06, 1.0869992710338935e-06, 1.5528561014769908e-06, 1.5528561014769908e-06, 2.218365859252844e-06, 2.218365859252844e-06, 3.1690940846469206e-06, 3.1690940846469206e-06, 4.527277263781315e-06, 4.527277263781315e-06, 6.4675389482590216e-06, 6.4675389482590216e-06, 9.239341354655746e-06, 9.239341354655746e-06, 1.3199059078079638e-05, 1.3199059078079638e-05, 1.8855798682970913e-05, 1.8855798682970913e-05, 2.693685526138702e-05, 2.693685526138702e-05, 3.848122180198146e-05, 3.848122180198146e-05, 5.497317400283066e-05, 5.497317400283066e-05, 7.853310571832952e-05, 7.853310571832952e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:16:35,203] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=45.26866473054199, CurrSamplesPerSec=44.37174600585815, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [ 350/1404]  eta: 0:11:07  lr: 0.000079  min_lr: 0.000001  loss: 3.8959 (3.8627)  loss_scale: 16384.0000 (29080.4330)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1408  max mem: 15572
Epoch: [14]  [ 360/1404]  eta: 0:11:03  lr: 0.000078  min_lr: 0.000001  loss: 3.9247 (3.8638)  loss_scale: 16384.0000 (28728.7313)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.1557  max mem: 15572
Epoch: [14]  [ 370/1404]  eta: 0:10:55  lr: 0.000078  min_lr: 0.000001  loss: 3.9472 (3.8642)  loss_scale: 16384.0000 (28395.9892)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1241  max mem: 15572
Epoch: [14]  [ 380/1404]  eta: 0:10:50  lr: 0.000078  min_lr: 0.000001  loss: 3.9499 (3.8646)  loss_scale: 16384.0000 (28080.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1556  max mem: 15572
[2025-01-10 19:17:00,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:17:00,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 19:17:00,490] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:17:00,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 390/1404]  eta: 0:10:41  lr: 0.000078  min_lr: 0.000001  loss: 3.9111 (3.8707)  loss_scale: 16384.0000 (28200.5934)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1335  max mem: 15572
Epoch: [14]  [ 400/1404]  eta: 0:10:33  lr: 0.000078  min_lr: 0.000001  loss: 4.1303 (3.8699)  loss_scale: 32768.0000 (28314.4938)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0468  max mem: 15572
Epoch: [14]  [ 410/1404]  eta: 0:10:26  lr: 0.000078  min_lr: 0.000001  loss: 4.0197 (3.8709)  loss_scale: 32768.0000 (28422.8516)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0812  max mem: 15572
Epoch: [14]  [ 420/1404]  eta: 0:10:18  lr: 0.000078  min_lr: 0.000001  loss: 3.9692 (3.8718)  loss_scale: 32768.0000 (28526.0618)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0724  max mem: 15572
Epoch: [14]  [ 430/1404]  eta: 0:10:13  lr: 0.000078  min_lr: 0.000001  loss: 3.8880 (3.8737)  loss_scale: 32768.0000 (28624.4826)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0807  max mem: 15572
Epoch: [14]  [ 440/1404]  eta: 0:10:08  lr: 0.000078  min_lr: 0.000001  loss: 3.8880 (3.8774)  loss_scale: 32768.0000 (28718.4399)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.1564  max mem: 15572
Epoch: [14]  [ 450/1404]  eta: 0:10:00  lr: 0.000078  min_lr: 0.000001  loss: 3.9947 (3.8799)  loss_scale: 32768.0000 (28808.2306)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1309  max mem: 15572
Epoch: [14]  [ 460/1404]  eta: 0:09:54  lr: 0.000078  min_lr: 0.000001  loss: 3.8602 (3.8786)  loss_scale: 32768.0000 (28894.1258)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0988  max mem: 15572
Epoch: [14]  [ 470/1404]  eta: 0:09:47  lr: 0.000078  min_lr: 0.000001  loss: 4.0116 (3.8788)  loss_scale: 32768.0000 (28976.3737)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1069  max mem: 15572
Epoch: [14]  [ 480/1404]  eta: 0:09:42  lr: 0.000078  min_lr: 0.000001  loss: 3.8840 (3.8763)  loss_scale: 32768.0000 (29055.2017)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1128  max mem: 15572
Epoch: [14]  [ 490/1404]  eta: 0:09:37  lr: 0.000078  min_lr: 0.000001  loss: 3.6992 (3.8715)  loss_scale: 32768.0000 (29130.8187)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.1896  max mem: 15572
Epoch: [14]  [ 500/1404]  eta: 0:09:30  lr: 0.000078  min_lr: 0.000001  loss: 3.7570 (3.8733)  loss_scale: 32768.0000 (29203.4172)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1626  max mem: 15572
[2025-01-10 19:18:18,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:18:18,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:18:18,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:18:18,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 510/1404]  eta: 0:09:22  lr: 0.000078  min_lr: 0.000001  loss: 3.9319 (3.8721)  loss_scale: 32768.0000 (29401.4247)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0712  max mem: 15572
[2025-01-10 19:18:19,771] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20167
[2025-01-10 19:18:19,771] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:18:19,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20167
[2025-01-10 19:18:19,800] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:18:19,801] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 520/1404]  eta: 0:09:15  lr: 0.000078  min_lr: 0.000001  loss: 3.8799 (3.8697)  loss_scale: 32768.0000 (29466.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0902  max mem: 15572
Epoch: [14]  [ 530/1404]  eta: 0:09:08  lr: 0.000078  min_lr: 0.000001  loss: 3.6728 (3.8660)  loss_scale: 32768.0000 (29528.2260)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0928  max mem: 15572
Epoch: [14]  [ 540/1404]  eta: 0:09:00  lr: 0.000078  min_lr: 0.000001  loss: 3.6925 (3.8625)  loss_scale: 32768.0000 (29588.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0572  max mem: 15572
Epoch: [14]  [ 550/1404]  eta: 0:08:54  lr: 0.000078  min_lr: 0.000001  loss: 3.7178 (3.8607)  loss_scale: 32768.0000 (29645.8221)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0811  max mem: 15572
Epoch: [14]  [ 560/1404]  eta: 0:08:47  lr: 0.000078  min_lr: 0.000001  loss: 3.7252 (3.8578)  loss_scale: 32768.0000 (29701.4759)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1026  max mem: 15572
Epoch: [14]  [ 570/1404]  eta: 0:08:40  lr: 0.000078  min_lr: 0.000001  loss: 3.8765 (3.8590)  loss_scale: 32768.0000 (29755.1804)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0828  max mem: 15572
Epoch: [14]  [ 580/1404]  eta: 0:08:34  lr: 0.000078  min_lr: 0.000001  loss: 3.8920 (3.8590)  loss_scale: 32768.0000 (29807.0361)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0360  max mem: 15572
Epoch: [14]  [ 590/1404]  eta: 0:08:28  lr: 0.000078  min_lr: 0.000001  loss: 3.8920 (3.8601)  loss_scale: 32768.0000 (29857.1371)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.0686  max mem: 15572
Epoch: [14]  [ 600/1404]  eta: 0:08:21  lr: 0.000078  min_lr: 0.000001  loss: 3.7602 (3.8564)  loss_scale: 32768.0000 (29905.5707)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0795  max mem: 15572
Epoch: [14]  [ 610/1404]  eta: 0:08:15  lr: 0.000078  min_lr: 0.000001  loss: 3.7602 (3.8581)  loss_scale: 32768.0000 (29952.4190)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0707  max mem: 15572
Epoch: [14]  [ 620/1404]  eta: 0:08:09  lr: 0.000078  min_lr: 0.000001  loss: 3.8934 (3.8591)  loss_scale: 32768.0000 (29997.7585)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.1148  max mem: 15572
Epoch: [14]  [ 630/1404]  eta: 0:08:03  lr: 0.000078  min_lr: 0.000001  loss: 4.0335 (3.8638)  loss_scale: 32768.0000 (30041.6609)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1110  max mem: 15572
[2025-01-10 19:19:38,027] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:19:38,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 640/1404]  eta: 0:07:56  lr: 0.000078  min_lr: 0.000001  loss: 4.1479 (3.8682)  loss_scale: 32768.0000 (30135.3136)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1130  max mem: 15572
[2025-01-10 19:19:38,046] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:19:38,047] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 650/1404]  eta: 0:07:50  lr: 0.000078  min_lr: 0.000001  loss: 3.9424 (3.8628)  loss_scale: 65536.0000 (30679.1029)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1145  max mem: 15572
[2025-01-10 19:19:45,815] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20309
[2025-01-10 19:19:45,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:19:45,817] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:19:45,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20309
[2025-01-10 19:19:45,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [14]  [ 660/1404]  eta: 0:07:44  lr: 0.000078  min_lr: 0.000001  loss: 3.8623 (3.8636)  loss_scale: 65536.0000 (30809.8517)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1389  max mem: 15572
Epoch: [14]  [ 670/1404]  eta: 0:07:37  lr: 0.000078  min_lr: 0.000001  loss: 3.8631 (3.8647)  loss_scale: 32768.0000 (30839.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0820  max mem: 15572
Epoch: [14]  [ 680/1404]  eta: 0:07:30  lr: 0.000078  min_lr: 0.000001  loss: 3.8738 (3.8655)  loss_scale: 32768.0000 (30867.3598)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0210  max mem: 15572
Epoch: [14]  [ 690/1404]  eta: 0:07:24  lr: 0.000078  min_lr: 0.000001  loss: 3.7438 (3.8621)  loss_scale: 32768.0000 (30894.8654)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0849  max mem: 15572
Epoch: [14]  [ 700/1404]  eta: 0:07:17  lr: 0.000078  min_lr: 0.000001  loss: 3.6150 (3.8614)  loss_scale: 32768.0000 (30921.5863)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0650  max mem: 15572
Epoch: [14]  [ 710/1404]  eta: 0:07:10  lr: 0.000078  min_lr: 0.000001  loss: 3.6337 (3.8608)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0012  max mem: 15572
Epoch: [14]  [ 720/1404]  eta: 0:07:03  lr: 0.000078  min_lr: 0.000001  loss: 3.9758 (3.8632)  loss_scale: 32768.0000 (30972.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0191  max mem: 15572
Epoch: [14]  [ 730/1404]  eta: 0:06:57  lr: 0.000078  min_lr: 0.000001  loss: 4.0465 (3.8615)  loss_scale: 32768.0000 (30997.3625)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0554  max mem: 15572
Epoch: [14]  [ 740/1404]  eta: 0:06:51  lr: 0.000078  min_lr: 0.000001  loss: 3.9165 (3.8637)  loss_scale: 32768.0000 (31021.2578)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0712  max mem: 15572
Epoch: [14]  [ 750/1404]  eta: 0:06:45  lr: 0.000078  min_lr: 0.000001  loss: 3.9165 (3.8649)  loss_scale: 32768.0000 (31044.5166)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0349  max mem: 15572
Epoch: [14]  [ 760/1404]  eta: 0:06:39  lr: 0.000078  min_lr: 0.000001  loss: 3.8049 (3.8642)  loss_scale: 32768.0000 (31067.1643)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.0010  max mem: 15572
Epoch: [14]  [ 770/1404]  eta: 0:06:33  lr: 0.000078  min_lr: 0.000001  loss: 3.9863 (3.8656)  loss_scale: 32768.0000 (31089.2244)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.0011  max mem: 15572
Epoch: [14]  [ 780/1404]  eta: 0:06:27  lr: 0.000078  min_lr: 0.000001  loss: 3.9863 (3.8644)  loss_scale: 32768.0000 (31110.7196)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0008  max mem: 15572
[2025-01-10 19:21:03,802] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:21:03,803] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:21:03,832] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:21:03,832] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:21:06,649] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20444
[2025-01-10 19:21:06,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:21:06,685] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20444
[2025-01-10 19:21:06,686] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:21:06,686] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 790/1404]  eta: 0:06:21  lr: 0.000078  min_lr: 0.000001  loss: 3.5848 (3.8608)  loss_scale: 32768.0000 (31380.2276)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0007  max mem: 15572
Epoch: [14]  [ 800/1404]  eta: 0:06:15  lr: 0.000078  min_lr: 0.000001  loss: 3.7239 (3.8631)  loss_scale: 32768.0000 (31397.5531)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0006  max mem: 15572
Epoch: [14]  [ 810/1404]  eta: 0:06:09  lr: 0.000077  min_lr: 0.000001  loss: 3.8860 (3.8609)  loss_scale: 32768.0000 (31414.4513)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0008  max mem: 15572
Epoch: [14]  [ 820/1404]  eta: 0:06:02  lr: 0.000077  min_lr: 0.000001  loss: 3.8860 (3.8627)  loss_scale: 32768.0000 (31430.9379)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0007  max mem: 15572
Epoch: [14]  [ 830/1404]  eta: 0:05:56  lr: 0.000077  min_lr: 0.000001  loss: 3.8922 (3.8636)  loss_scale: 32768.0000 (31447.0277)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0007  max mem: 15572
Epoch: [14]  [ 840/1404]  eta: 0:05:49  lr: 0.000077  min_lr: 0.000001  loss: 3.7146 (3.8627)  loss_scale: 32768.0000 (31462.7348)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0007  max mem: 15572
Epoch: [14]  [ 850/1404]  eta: 0:05:43  lr: 0.000077  min_lr: 0.000001  loss: 3.7897 (3.8634)  loss_scale: 32768.0000 (31478.0729)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0008  max mem: 15572
Epoch: [14]  [ 860/1404]  eta: 0:05:36  lr: 0.000077  min_lr: 0.000001  loss: 3.7897 (3.8638)  loss_scale: 32768.0000 (31493.0546)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0011  max mem: 15572
Epoch: [14]  [ 870/1404]  eta: 0:05:31  lr: 0.000077  min_lr: 0.000001  loss: 3.9771 (3.8654)  loss_scale: 32768.0000 (31507.6923)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.0013  max mem: 15572
Epoch: [14]  [ 880/1404]  eta: 0:05:24  lr: 0.000077  min_lr: 0.000001  loss: 3.9771 (3.8671)  loss_scale: 32768.0000 (31521.9977)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0011  max mem: 15572
Epoch: [14]  [ 890/1404]  eta: 0:05:17  lr: 0.000077  min_lr: 0.000001  loss: 4.0736 (3.8704)  loss_scale: 32768.0000 (31535.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0009  max mem: 15572
Epoch: [14]  [ 900/1404]  eta: 0:05:11  lr: 0.000077  min_lr: 0.000001  loss: 3.9516 (3.8673)  loss_scale: 32768.0000 (31549.6559)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0009  max mem: 15572
Epoch: [14]  [ 910/1404]  eta: 0:05:05  lr: 0.000077  min_lr: 0.000001  loss: 3.6416 (3.8689)  loss_scale: 32768.0000 (31563.0296)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0630  max mem: 15572
[2025-01-10 19:22:25,673] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:22:25,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:22:25,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:22:25,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 920/1404]  eta: 0:04:59  lr: 0.000077  min_lr: 0.000001  loss: 3.8750 (3.8676)  loss_scale: 32768.0000 (31718.4278)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.0756  max mem: 15572
[2025-01-10 19:22:29,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20579
[2025-01-10 19:22:29,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:22:29,758] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:22:29,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20579
[2025-01-10 19:22:29,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [14]  [ 930/1404]  eta: 0:04:53  lr: 0.000077  min_lr: 0.000001  loss: 4.0201 (3.8699)  loss_scale: 32768.0000 (31800.0945)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0767  max mem: 15572
Epoch: [14]  [ 940/1404]  eta: 0:04:47  lr: 0.000077  min_lr: 0.000001  loss: 4.0295 (3.8695)  loss_scale: 32768.0000 (31810.3804)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1329  max mem: 15572
Epoch: [14]  [ 950/1404]  eta: 0:04:41  lr: 0.000077  min_lr: 0.000001  loss: 3.7419 (3.8686)  loss_scale: 32768.0000 (31820.4501)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1491  max mem: 15572
Epoch: [14]  [ 960/1404]  eta: 0:04:34  lr: 0.000077  min_lr: 0.000001  loss: 3.8202 (3.8699)  loss_scale: 32768.0000 (31830.3101)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0800  max mem: 15572
Epoch: [14]  [ 970/1404]  eta: 0:04:28  lr: 0.000077  min_lr: 0.000001  loss: 3.8526 (3.8704)  loss_scale: 32768.0000 (31839.9670)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0742  max mem: 15572
Epoch: [14]  [ 980/1404]  eta: 0:04:22  lr: 0.000077  min_lr: 0.000001  loss: 3.7962 (3.8698)  loss_scale: 32768.0000 (31849.4271)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.1766  max mem: 15572
Epoch: [14]  [ 990/1404]  eta: 0:04:15  lr: 0.000077  min_lr: 0.000001  loss: 3.7975 (3.8698)  loss_scale: 32768.0000 (31858.6963)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1293  max mem: 15572
Epoch: [14]  [1000/1404]  eta: 0:04:09  lr: 0.000077  min_lr: 0.000001  loss: 3.9536 (3.8717)  loss_scale: 32768.0000 (31867.7802)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0932  max mem: 15572
Epoch: [14]  [1010/1404]  eta: 0:04:03  lr: 0.000077  min_lr: 0.000001  loss: 3.9167 (3.8701)  loss_scale: 32768.0000 (31876.6845)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0728  max mem: 15572
Epoch: [14]  [1020/1404]  eta: 0:03:57  lr: 0.000077  min_lr: 0.000001  loss: 3.7507 (3.8695)  loss_scale: 32768.0000 (31885.4143)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0145  max mem: 15572
Epoch: [14]  [1030/1404]  eta: 0:03:51  lr: 0.000077  min_lr: 0.000001  loss: 3.7782 (3.8677)  loss_scale: 32768.0000 (31893.9748)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0782  max mem: 15572
Epoch: [14]  [1040/1404]  eta: 0:03:44  lr: 0.000077  min_lr: 0.000001  loss: 3.7344 (3.8670)  loss_scale: 32768.0000 (31902.3708)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0871  max mem: 15572
Epoch: [14]  [1050/1404]  eta: 0:03:38  lr: 0.000077  min_lr: 0.000001  loss: 3.8800 (3.8669)  loss_scale: 32768.0000 (31910.6070)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1014  max mem: 15572
[2025-01-10 19:23:48,907] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:23:48,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:23:48,938] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:23:48,939] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1060/1404]  eta: 0:03:32  lr: 0.000077  min_lr: 0.000001  loss: 4.0779 (3.8675)  loss_scale: 32768.0000 (32196.6447)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.1454  max mem: 15572
Epoch: [14]  [1070/1404]  eta: 0:03:26  lr: 0.000077  min_lr: 0.000001  loss: 3.9826 (3.8676)  loss_scale: 65536.0000 (32507.9365)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.0764  max mem: 15572
Epoch: [14]  [1080/1404]  eta: 0:03:20  lr: 0.000077  min_lr: 0.000001  loss: 3.8499 (3.8669)  loss_scale: 65536.0000 (32813.4690)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0451  max mem: 15572
[2025-01-10 19:24:12,485] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20745
[2025-01-10 19:24:12,486] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:24:12,573] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20745
[2025-01-10 19:24:12,573] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:24:12,574] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1090/1404]  eta: 0:03:14  lr: 0.000077  min_lr: 0.000001  loss: 3.8499 (3.8672)  loss_scale: 65536.0000 (33053.3309)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0782  max mem: 15572
Epoch: [14]  [1100/1404]  eta: 0:03:07  lr: 0.000077  min_lr: 0.000001  loss: 3.9529 (3.8683)  loss_scale: 32768.0000 (33050.7393)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0802  max mem: 15572
Epoch: [14]  [1110/1404]  eta: 0:03:01  lr: 0.000077  min_lr: 0.000001  loss: 4.0741 (3.8704)  loss_scale: 32768.0000 (33048.1944)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1038  max mem: 15572
Epoch: [14]  [1120/1404]  eta: 0:02:55  lr: 0.000077  min_lr: 0.000001  loss: 4.1231 (3.8711)  loss_scale: 32768.0000 (33045.6949)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0825  max mem: 15572
Epoch: [14]  [1130/1404]  eta: 0:02:49  lr: 0.000077  min_lr: 0.000001  loss: 4.0743 (3.8734)  loss_scale: 32768.0000 (33043.2396)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0660  max mem: 15572
Epoch: [14]  [1140/1404]  eta: 0:02:43  lr: 0.000077  min_lr: 0.000001  loss: 3.9738 (3.8711)  loss_scale: 32768.0000 (33040.8273)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.1494  max mem: 15572
Epoch: [14]  [1150/1404]  eta: 0:02:36  lr: 0.000077  min_lr: 0.000001  loss: 3.9721 (3.8720)  loss_scale: 32768.0000 (33038.4570)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1148  max mem: 15572
Epoch: [14]  [1160/1404]  eta: 0:02:30  lr: 0.000077  min_lr: 0.000001  loss: 4.0869 (3.8741)  loss_scale: 32768.0000 (33036.1275)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1090  max mem: 15572
Epoch: [14]  [1170/1404]  eta: 0:02:24  lr: 0.000077  min_lr: 0.000001  loss: 3.8187 (3.8723)  loss_scale: 32768.0000 (33033.8377)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.1484  max mem: 15572
Epoch: [14]  [1180/1404]  eta: 0:02:18  lr: 0.000077  min_lr: 0.000001  loss: 3.7569 (3.8738)  loss_scale: 32768.0000 (33031.5868)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1321  max mem: 15572
Epoch: [14]  [1190/1404]  eta: 0:02:12  lr: 0.000077  min_lr: 0.000001  loss: 4.0081 (3.8735)  loss_scale: 32768.0000 (33029.3736)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1122  max mem: 15572
Epoch: [14]  [1200/1404]  eta: 0:02:05  lr: 0.000077  min_lr: 0.000001  loss: 3.8653 (3.8732)  loss_scale: 32768.0000 (33027.1973)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0525  max mem: 15572
Epoch: [14]  [1210/1404]  eta: 0:01:59  lr: 0.000077  min_lr: 0.000001  loss: 3.6167 (3.8698)  loss_scale: 32768.0000 (33025.0570)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0813  max mem: 15572
[2025-01-10 19:25:30,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:25:30,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:25:30,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:25:30,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:25:31,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20875
[2025-01-10 19:25:31,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:25:31,330] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20875
[2025-01-10 19:25:31,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:25:31,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1220/1404]  eta: 0:01:53  lr: 0.000077  min_lr: 0.000001  loss: 3.6167 (3.8696)  loss_scale: 32768.0000 (33049.7887)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1211  max mem: 15572
Epoch: [14]  [1230/1404]  eta: 0:01:47  lr: 0.000077  min_lr: 0.000001  loss: 3.6411 (3.8665)  loss_scale: 32768.0000 (33047.4996)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1482  max mem: 15572
Epoch: [14]  [1240/1404]  eta: 0:01:41  lr: 0.000077  min_lr: 0.000001  loss: 3.7455 (3.8664)  loss_scale: 32768.0000 (33045.2474)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1632  max mem: 15572
Epoch: [14]  [1250/1404]  eta: 0:01:35  lr: 0.000076  min_lr: 0.000001  loss: 3.7805 (3.8656)  loss_scale: 32768.0000 (33043.0312)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0801  max mem: 15572
Epoch: [14]  [1260/1404]  eta: 0:01:28  lr: 0.000076  min_lr: 0.000001  loss: 3.7222 (3.8659)  loss_scale: 32768.0000 (33040.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0302  max mem: 15572
Epoch: [14]  [1270/1404]  eta: 0:01:22  lr: 0.000076  min_lr: 0.000001  loss: 3.8626 (3.8665)  loss_scale: 32768.0000 (33038.7034)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1225  max mem: 15572
Epoch: [14]  [1280/1404]  eta: 0:01:16  lr: 0.000076  min_lr: 0.000001  loss: 3.8626 (3.8655)  loss_scale: 32768.0000 (33036.5902)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.1389  max mem: 15572
Epoch: [14]  [1290/1404]  eta: 0:01:10  lr: 0.000076  min_lr: 0.000001  loss: 3.5550 (3.8624)  loss_scale: 32768.0000 (33034.5097)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0714  max mem: 15572
Epoch: [14]  [1300/1404]  eta: 0:01:04  lr: 0.000076  min_lr: 0.000001  loss: 3.6772 (3.8636)  loss_scale: 32768.0000 (33032.4612)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1155  max mem: 15572
Epoch: [14]  [1310/1404]  eta: 0:00:57  lr: 0.000076  min_lr: 0.000001  loss: 4.0119 (3.8633)  loss_scale: 32768.0000 (33030.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0780  max mem: 15572
Epoch: [14]  [1320/1404]  eta: 0:00:51  lr: 0.000076  min_lr: 0.000001  loss: 3.8811 (3.8605)  loss_scale: 32768.0000 (33028.4572)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0410  max mem: 15572
Epoch: [14]  [1330/1404]  eta: 0:00:45  lr: 0.000076  min_lr: 0.000001  loss: 3.6575 (3.8592)  loss_scale: 32768.0000 (33026.5004)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1137  max mem: 15572
Epoch: [14]  [1340/1404]  eta: 0:00:39  lr: 0.000076  min_lr: 0.000001  loss: 3.7857 (3.8595)  loss_scale: 32768.0000 (33024.5727)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1079  max mem: 15572
[2025-01-10 19:26:46,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=132, lr=[7.38888092447839e-07, 7.38888092447839e-07, 1.0555544177826271e-06, 1.0555544177826271e-06, 1.5079348825466106e-06, 1.5079348825466106e-06, 2.154192689352301e-06, 2.154192689352301e-06, 3.077418127646144e-06, 3.077418127646144e-06, 4.396311610923064e-06, 4.396311610923064e-06, 6.280445158461519e-06, 6.280445158461519e-06, 8.972064512087886e-06, 8.972064512087886e-06, 1.2817235017268407e-05, 1.2817235017268407e-05, 1.8310335738954873e-05, 1.8310335738954873e-05, 2.6157622484221243e-05, 2.6157622484221243e-05, 3.736803212031606e-05, 3.736803212031606e-05, 5.3382903029022955e-05, 5.3382903029022955e-05, 7.626129004146137e-05, 7.626129004146137e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:26:46,787] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=45.38413900835904, CurrSamplesPerSec=40.23242436983522, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 19:26:51,255] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:26:51,255] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:26:51,258] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:26:51,258] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:26:51,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21005
[2025-01-10 19:26:51,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:26:51,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21005
[2025-01-10 19:26:51,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:26:51,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1350/1404]  eta: 0:00:33  lr: 0.000076  min_lr: 0.000001  loss: 3.9367 (3.8596)  loss_scale: 32768.0000 (33046.9282)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1260  max mem: 15572
Epoch: [14]  [1360/1404]  eta: 0:00:27  lr: 0.000076  min_lr: 0.000001  loss: 3.8237 (3.8592)  loss_scale: 32768.0000 (33044.8788)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.1920  max mem: 15572
Epoch: [14]  [1370/1404]  eta: 0:00:20  lr: 0.000076  min_lr: 0.000001  loss: 3.6512 (3.8582)  loss_scale: 32768.0000 (33042.8592)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1035  max mem: 15572
Epoch: [14]  [1380/1404]  eta: 0:00:14  lr: 0.000076  min_lr: 0.000001  loss: 3.6601 (3.8577)  loss_scale: 32768.0000 (33040.8689)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0214  max mem: 15572
Epoch: [14]  [1390/1404]  eta: 0:00:08  lr: 0.000076  min_lr: 0.000001  loss: 3.6975 (3.8575)  loss_scale: 32768.0000 (33038.9073)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0703  max mem: 15572
Epoch: [14]  [1400/1404]  eta: 0:00:02  lr: 0.000076  min_lr: 0.000001  loss: 3.7828 (3.8567)  loss_scale: 32768.0000 (33036.9736)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0516  max mem: 15572
Epoch: [14]  [1403/1404]  eta: 0:00:00  lr: 0.000076  min_lr: 0.000001  loss: 3.8252 (3.8574)  loss_scale: 32768.0000 (33036.3989)  weight_decay: 0.0500 (0.0500)  time: 0.4589  data: 0.0516  max mem: 15572
Epoch: [14] Total time: 0:14:23 (0.6148 s / it)
Averaged stats: lr: 0.000076  min_lr: 0.000001  loss: 3.8252 (3.8505)  loss_scale: 32768.0000 (33036.3989)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:24  loss: 1.4617 (1.4617)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 6.3543  data: 6.1287  max mem: 15572
Val:  [ 10/136]  eta: 0:01:56  loss: 2.6044 (2.3810)  acc1: 38.8889 (41.4141)  acc5: 72.2222 (73.2323)  time: 0.9270  data: 0.7185  max mem: 15572
Val:  [ 20/136]  eta: 0:01:16  loss: 2.7511 (2.5849)  acc1: 33.3333 (38.0952)  acc5: 66.6667 (69.3122)  time: 0.3708  data: 0.1563  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.7013 (2.3635)  acc1: 33.3333 (44.2652)  acc5: 66.6667 (72.4014)  time: 0.2784  data: 0.0679  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 1.7831 (2.3066)  acc1: 55.5556 (45.6640)  acc5: 83.3333 (74.1192)  time: 0.3217  data: 0.1259  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 2.1814 (2.3472)  acc1: 50.0000 (44.6623)  acc5: 77.7778 (74.0741)  time: 0.4129  data: 0.1940  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.6038 (2.4612)  acc1: 33.3333 (41.6211)  acc5: 72.2222 (72.4044)  time: 0.3863  data: 0.1578  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.4783 (2.4121)  acc1: 44.4444 (43.2707)  acc5: 72.2222 (73.0829)  time: 0.3666  data: 0.1483  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.0999 (2.4104)  acc1: 50.0000 (43.2785)  acc5: 77.7778 (73.4568)  time: 0.3829  data: 0.1580  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.3766 (2.4165)  acc1: 38.8889 (42.6129)  acc5: 77.7778 (73.5653)  time: 0.3967  data: 0.1803  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 2.6195 (2.4997)  acc1: 38.8889 (40.8691)  acc5: 66.6667 (71.5622)  time: 0.3311  data: 0.1222  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.6591 (2.4819)  acc1: 38.8889 (41.3413)  acc5: 66.6667 (71.7217)  time: 0.3729  data: 0.1570  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.0403 (2.4269)  acc1: 50.0000 (42.8375)  acc5: 83.3333 (72.9109)  time: 0.3789  data: 0.1793  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8692 (2.3799)  acc1: 55.5556 (44.1052)  acc5: 88.8889 (73.5793)  time: 0.2249  data: 0.0633  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0618 (2.3831)  acc1: 50.0000 (43.9803)  acc5: 83.3333 (73.5053)  time: 0.1869  data: 0.0359  max mem: 15572
Val: Total time: 0:00:52 (0.3834 s / it)
* Acc@1 43.161 Acc@5 72.871 loss 2.416
Accuracy of the network on the 4883 val videos: 43.2%
[2025-01-10 19:28:12,985] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 19:28:12,987] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 19:28:12,988] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 19:28:12,988] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 19:28:15,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 19:28:15,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.16%
Epoch: [15]  [   0/1404]  eta: 3:26:17  lr: 0.000076  min_lr: 0.000001  loss: 4.0192 (4.0192)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8158  data: 8.2872  max mem: 15572
Epoch: [15]  [  10/1404]  eta: 0:30:21  lr: 0.000076  min_lr: 0.000001  loss: 3.5182 (3.7128)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3064  data: 0.7681  max mem: 15572
Epoch: [15]  [  20/1404]  eta: 0:21:22  lr: 0.000076  min_lr: 0.000001  loss: 3.5182 (3.6352)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0086  max mem: 15572
Epoch: [15]  [  30/1404]  eta: 0:18:11  lr: 0.000076  min_lr: 0.000001  loss: 3.7164 (3.7008)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0124  max mem: 15572
Epoch: [15]  [  40/1404]  eta: 0:17:51  lr: 0.000076  min_lr: 0.000001  loss: 3.7164 (3.7139)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1159  max mem: 15572
Epoch: [15]  [  50/1404]  eta: 0:16:24  lr: 0.000076  min_lr: 0.000001  loss: 3.7622 (3.6925)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1043  max mem: 15572
Epoch: [15]  [  60/1404]  eta: 0:16:19  lr: 0.000076  min_lr: 0.000001  loss: 3.7622 (3.7023)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0007  max mem: 15572
Epoch: [15]  [  70/1404]  eta: 0:15:49  lr: 0.000076  min_lr: 0.000001  loss: 3.7872 (3.7124)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0009  max mem: 15572
[2025-01-10 19:29:09,362] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:29:09,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:29:09,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:29:09,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [  80/1404]  eta: 0:15:27  lr: 0.000076  min_lr: 0.000001  loss: 3.6902 (3.7116)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0007  max mem: 15572
[2025-01-10 19:29:12,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21141
[2025-01-10 19:29:12,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21141
[2025-01-10 19:29:12,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:29:12,858] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:29:12,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [  90/1404]  eta: 0:15:10  lr: 0.000076  min_lr: 0.000001  loss: 3.7653 (3.7329)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0005  max mem: 15572
Epoch: [15]  [ 100/1404]  eta: 0:14:54  lr: 0.000076  min_lr: 0.000001  loss: 3.8740 (3.7444)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.0006  max mem: 15572
Epoch: [15]  [ 110/1404]  eta: 0:14:32  lr: 0.000076  min_lr: 0.000001  loss: 3.9272 (3.7393)  loss_scale: 32768.0000 (34834.4505)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0006  max mem: 15572
Epoch: [15]  [ 120/1404]  eta: 0:14:22  lr: 0.000076  min_lr: 0.000001  loss: 3.8431 (3.7465)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0007  max mem: 15572
Epoch: [15]  [ 130/1404]  eta: 0:14:12  lr: 0.000076  min_lr: 0.000001  loss: 3.8454 (3.7644)  loss_scale: 32768.0000 (34518.9618)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0007  max mem: 15572
Epoch: [15]  [ 140/1404]  eta: 0:13:52  lr: 0.000076  min_lr: 0.000001  loss: 3.9078 (3.7754)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0008  max mem: 15572
Epoch: [15]  [ 150/1404]  eta: 0:13:41  lr: 0.000076  min_lr: 0.000001  loss: 3.8707 (3.7823)  loss_scale: 32768.0000 (34287.0464)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0010  max mem: 15572
Epoch: [15]  [ 160/1404]  eta: 0:13:30  lr: 0.000076  min_lr: 0.000001  loss: 3.8213 (3.7811)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0012  max mem: 15572
Epoch: [15]  [ 170/1404]  eta: 0:13:18  lr: 0.000076  min_lr: 0.000001  loss: 3.8242 (3.7886)  loss_scale: 32768.0000 (34109.3801)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0013  max mem: 15572
Epoch: [15]  [ 180/1404]  eta: 0:13:15  lr: 0.000076  min_lr: 0.000001  loss: 3.9806 (3.7900)  loss_scale: 32768.0000 (34035.2707)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0010  max mem: 15572
Epoch: [15]  [ 190/1404]  eta: 0:13:09  lr: 0.000076  min_lr: 0.000001  loss: 3.9799 (3.7965)  loss_scale: 32768.0000 (33968.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0012  max mem: 15572
Epoch: [15]  [ 200/1404]  eta: 0:12:59  lr: 0.000076  min_lr: 0.000001  loss: 3.9084 (3.8071)  loss_scale: 32768.0000 (33909.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.0013  max mem: 15572
[2025-01-10 19:30:31,128] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:30:31,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:30:31,131] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:30:31,132] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 210/1404]  eta: 0:12:46  lr: 0.000076  min_lr: 0.000001  loss: 3.8503 (3.8061)  loss_scale: 32768.0000 (34010.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0008  max mem: 15572
[2025-01-10 19:30:32,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21271
[2025-01-10 19:30:32,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:30:32,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21271
[2025-01-10 19:30:32,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:30:32,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 220/1404]  eta: 0:12:39  lr: 0.000076  min_lr: 0.000001  loss: 3.8503 (3.8114)  loss_scale: 32768.0000 (33954.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0010  max mem: 15572
Epoch: [15]  [ 230/1404]  eta: 0:12:35  lr: 0.000076  min_lr: 0.000001  loss: 3.8661 (3.8045)  loss_scale: 32768.0000 (33902.8225)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0010  max mem: 15572
Epoch: [15]  [ 240/1404]  eta: 0:12:28  lr: 0.000076  min_lr: 0.000001  loss: 3.6101 (3.8024)  loss_scale: 32768.0000 (33855.7344)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0010  max mem: 15572
Epoch: [15]  [ 250/1404]  eta: 0:12:26  lr: 0.000076  min_lr: 0.000001  loss: 3.5722 (3.7872)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.0012  max mem: 15572
Epoch: [15]  [ 260/1404]  eta: 0:12:14  lr: 0.000076  min_lr: 0.000001  loss: 3.5008 (3.7788)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0010  max mem: 15572
Epoch: [15]  [ 270/1404]  eta: 0:12:05  lr: 0.000075  min_lr: 0.000001  loss: 3.9171 (3.7877)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0008  max mem: 15572
Epoch: [15]  [ 280/1404]  eta: 0:11:57  lr: 0.000075  min_lr: 0.000001  loss: 3.9376 (3.7939)  loss_scale: 32768.0000 (33700.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0011  max mem: 15572
Epoch: [15]  [ 290/1404]  eta: 0:11:48  lr: 0.000075  min_lr: 0.000001  loss: 3.9154 (3.8042)  loss_scale: 32768.0000 (33668.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0012  max mem: 15572
Epoch: [15]  [ 300/1404]  eta: 0:11:39  lr: 0.000075  min_lr: 0.000001  loss: 4.1404 (3.8165)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0008  max mem: 15572
Epoch: [15]  [ 310/1404]  eta: 0:11:29  lr: 0.000075  min_lr: 0.000001  loss: 4.1840 (3.8293)  loss_scale: 32768.0000 (33610.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0007  max mem: 15572
Epoch: [15]  [ 320/1404]  eta: 0:11:27  lr: 0.000075  min_lr: 0.000001  loss: 4.0297 (3.8308)  loss_scale: 32768.0000 (33584.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0007  max mem: 15572
Epoch: [15]  [ 330/1404]  eta: 0:11:17  lr: 0.000075  min_lr: 0.000001  loss: 3.7097 (3.8313)  loss_scale: 32768.0000 (33559.9758)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0007  max mem: 15572
[2025-01-10 19:31:51,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:31:51,033] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:31:51,034] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:31:51,034] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 340/1404]  eta: 0:11:11  lr: 0.000075  min_lr: 0.000001  loss: 3.9816 (3.8364)  loss_scale: 32768.0000 (33632.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0007  max mem: 15572
[2025-01-10 19:31:52,109] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21402
[2025-01-10 19:31:52,109] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:31:52,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:31:52,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21402
[2025-01-10 19:31:52,163] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [ 350/1404]  eta: 0:11:02  lr: 0.000075  min_lr: 0.000001  loss: 3.9571 (3.8380)  loss_scale: 32768.0000 (33701.5613)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0220  max mem: 15572
Epoch: [15]  [ 360/1404]  eta: 0:10:56  lr: 0.000075  min_lr: 0.000001  loss: 3.8556 (3.8373)  loss_scale: 32768.0000 (33675.7008)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0709  max mem: 15572
Epoch: [15]  [ 370/1404]  eta: 0:10:52  lr: 0.000075  min_lr: 0.000001  loss: 3.9617 (3.8349)  loss_scale: 32768.0000 (33651.2345)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.1517  max mem: 15572
Epoch: [15]  [ 380/1404]  eta: 0:10:44  lr: 0.000075  min_lr: 0.000001  loss: 3.8137 (3.8333)  loss_scale: 32768.0000 (33628.0525)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1516  max mem: 15572
Epoch: [15]  [ 390/1404]  eta: 0:10:35  lr: 0.000075  min_lr: 0.000001  loss: 3.7493 (3.8317)  loss_scale: 32768.0000 (33606.0563)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0494  max mem: 15572
Epoch: [15]  [ 400/1404]  eta: 0:10:27  lr: 0.000075  min_lr: 0.000001  loss: 3.5274 (3.8251)  loss_scale: 32768.0000 (33585.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0179  max mem: 15572
Epoch: [15]  [ 410/1404]  eta: 0:10:18  lr: 0.000075  min_lr: 0.000001  loss: 3.6567 (3.8223)  loss_scale: 32768.0000 (33565.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0316  max mem: 15572
Epoch: [15]  [ 420/1404]  eta: 0:10:12  lr: 0.000075  min_lr: 0.000001  loss: 3.7532 (3.8183)  loss_scale: 32768.0000 (33546.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0693  max mem: 15572
Epoch: [15]  [ 430/1404]  eta: 0:10:05  lr: 0.000075  min_lr: 0.000001  loss: 3.8039 (3.8169)  loss_scale: 32768.0000 (33528.2784)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0917  max mem: 15572
Epoch: [15]  [ 440/1404]  eta: 0:09:59  lr: 0.000075  min_lr: 0.000001  loss: 3.6473 (3.8141)  loss_scale: 32768.0000 (33511.0385)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0369  max mem: 15572
Epoch: [15]  [ 450/1404]  eta: 0:09:51  lr: 0.000075  min_lr: 0.000001  loss: 3.5632 (3.8088)  loss_scale: 32768.0000 (33494.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0008  max mem: 15572
Epoch: [15]  [ 460/1404]  eta: 0:09:45  lr: 0.000075  min_lr: 0.000001  loss: 3.7963 (3.8109)  loss_scale: 32768.0000 (33478.8026)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0008  max mem: 15572
Epoch: [15]  [ 470/1404]  eta: 0:09:39  lr: 0.000075  min_lr: 0.000001  loss: 3.9597 (3.8118)  loss_scale: 32768.0000 (33463.7113)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0010  max mem: 15572
[2025-01-10 19:33:08,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:33:08,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:33:08,808] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:33:08,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:33:13,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21539
[2025-01-10 19:33:13,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:33:13,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:33:13,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21539
[2025-01-10 19:33:13,534] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [ 480/1404]  eta: 0:09:33  lr: 0.000075  min_lr: 0.000001  loss: 3.5575 (3.8065)  loss_scale: 32768.0000 (33994.2453)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0008  max mem: 15572
Epoch: [15]  [ 490/1404]  eta: 0:09:25  lr: 0.000075  min_lr: 0.000001  loss: 3.7085 (3.8070)  loss_scale: 32768.0000 (33969.2709)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0009  max mem: 15572
Epoch: [15]  [ 500/1404]  eta: 0:09:18  lr: 0.000075  min_lr: 0.000001  loss: 3.6720 (3.8024)  loss_scale: 32768.0000 (33945.2934)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0010  max mem: 15572
Epoch: [15]  [ 510/1404]  eta: 0:09:12  lr: 0.000075  min_lr: 0.000001  loss: 3.6720 (3.8067)  loss_scale: 32768.0000 (33922.2544)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0137  max mem: 15572
Epoch: [15]  [ 520/1404]  eta: 0:09:05  lr: 0.000075  min_lr: 0.000001  loss: 3.8086 (3.8060)  loss_scale: 32768.0000 (33900.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0138  max mem: 15572
Epoch: [15]  [ 530/1404]  eta: 0:08:59  lr: 0.000075  min_lr: 0.000001  loss: 3.8542 (3.8049)  loss_scale: 32768.0000 (33878.7797)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0009  max mem: 15572
Epoch: [15]  [ 540/1404]  eta: 0:08:53  lr: 0.000075  min_lr: 0.000001  loss: 3.5297 (3.7975)  loss_scale: 32768.0000 (33858.2477)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0009  max mem: 15572
Epoch: [15]  [ 550/1404]  eta: 0:08:48  lr: 0.000075  min_lr: 0.000001  loss: 3.5297 (3.7943)  loss_scale: 32768.0000 (33838.4610)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.0008  max mem: 15572
Epoch: [15]  [ 560/1404]  eta: 0:08:40  lr: 0.000075  min_lr: 0.000001  loss: 3.8388 (3.7963)  loss_scale: 32768.0000 (33819.3797)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0007  max mem: 15572
Epoch: [15]  [ 570/1404]  eta: 0:08:33  lr: 0.000075  min_lr: 0.000001  loss: 4.0142 (3.8015)  loss_scale: 32768.0000 (33800.9667)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0008  max mem: 15572
Epoch: [15]  [ 580/1404]  eta: 0:08:27  lr: 0.000075  min_lr: 0.000001  loss: 4.0466 (3.8049)  loss_scale: 32768.0000 (33783.1876)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0008  max mem: 15572
Epoch: [15]  [ 590/1404]  eta: 0:08:22  lr: 0.000075  min_lr: 0.000001  loss: 4.0249 (3.8087)  loss_scale: 32768.0000 (33766.0102)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0007  max mem: 15572
Epoch: [15]  [ 600/1404]  eta: 0:08:14  lr: 0.000075  min_lr: 0.000001  loss: 3.9228 (3.8094)  loss_scale: 32768.0000 (33749.4043)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0008  max mem: 15572
[2025-01-10 19:34:30,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:34:30,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:34:30,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:34:30,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 610/1404]  eta: 0:08:08  lr: 0.000075  min_lr: 0.000001  loss: 3.8287 (3.8099)  loss_scale: 32768.0000 (33894.2324)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0010  max mem: 15572
[2025-01-10 19:34:32,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21673
[2025-01-10 19:34:32,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:34:32,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:34:33,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21673
[2025-01-10 19:34:33,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [ 620/1404]  eta: 0:08:00  lr: 0.000075  min_lr: 0.000001  loss: 3.9479 (3.8100)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0008  max mem: 15572
Epoch: [15]  [ 630/1404]  eta: 0:07:54  lr: 0.000075  min_lr: 0.000001  loss: 3.9479 (3.8127)  loss_scale: 32768.0000 (33962.3962)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0008  max mem: 15572
Epoch: [15]  [ 640/1404]  eta: 0:07:47  lr: 0.000075  min_lr: 0.000001  loss: 3.9453 (3.8153)  loss_scale: 32768.0000 (33943.7629)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0010  max mem: 15572
Epoch: [15]  [ 650/1404]  eta: 0:07:40  lr: 0.000075  min_lr: 0.000001  loss: 3.9627 (3.8174)  loss_scale: 32768.0000 (33925.7020)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0008  max mem: 15572
Epoch: [15]  [ 660/1404]  eta: 0:07:34  lr: 0.000075  min_lr: 0.000001  loss: 3.9807 (3.8210)  loss_scale: 32768.0000 (33908.1876)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0006  max mem: 15572
Epoch: [15]  [ 670/1404]  eta: 0:07:28  lr: 0.000075  min_lr: 0.000001  loss: 3.9484 (3.8207)  loss_scale: 32768.0000 (33891.1952)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0006  max mem: 15572
Epoch: [15]  [ 680/1404]  eta: 0:07:22  lr: 0.000075  min_lr: 0.000001  loss: 3.7633 (3.8185)  loss_scale: 32768.0000 (33874.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0006  max mem: 15572
Epoch: [15]  [ 690/1404]  eta: 0:07:16  lr: 0.000074  min_lr: 0.000001  loss: 3.7633 (3.8189)  loss_scale: 32768.0000 (33858.6860)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.0430  max mem: 15572
Epoch: [15]  [ 700/1404]  eta: 0:07:10  lr: 0.000074  min_lr: 0.000001  loss: 3.9723 (3.8228)  loss_scale: 32768.0000 (33843.1270)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0784  max mem: 15572
Epoch: [15]  [ 710/1404]  eta: 0:07:04  lr: 0.000074  min_lr: 0.000001  loss: 4.0263 (3.8237)  loss_scale: 32768.0000 (33828.0056)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1164  max mem: 15572
Epoch: [15]  [ 720/1404]  eta: 0:06:57  lr: 0.000074  min_lr: 0.000001  loss: 3.8206 (3.8218)  loss_scale: 32768.0000 (33813.3037)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1250  max mem: 15572
Epoch: [15]  [ 730/1404]  eta: 0:06:52  lr: 0.000074  min_lr: 0.000001  loss: 3.6289 (3.8208)  loss_scale: 32768.0000 (33799.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1021  max mem: 15572
Epoch: [15]  [ 740/1404]  eta: 0:06:46  lr: 0.000074  min_lr: 0.000001  loss: 3.7490 (3.8192)  loss_scale: 32768.0000 (33785.0904)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1039  max mem: 15572
[2025-01-10 19:35:51,976] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:35:51,976] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:35:51,976] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:35:51,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:35:54,139] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21806
[2025-01-10 19:35:54,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:35:54,141] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21806
[2025-01-10 19:35:54,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:35:54,142] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 750/1404]  eta: 0:06:40  lr: 0.000074  min_lr: 0.000001  loss: 3.9155 (3.8200)  loss_scale: 32768.0000 (33946.0772)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1321  max mem: 15572
Epoch: [15]  [ 760/1404]  eta: 0:06:33  lr: 0.000074  min_lr: 0.000001  loss: 3.9263 (3.8209)  loss_scale: 32768.0000 (33930.5966)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0960  max mem: 15572
Epoch: [15]  [ 770/1404]  eta: 0:06:27  lr: 0.000074  min_lr: 0.000001  loss: 3.9509 (3.8227)  loss_scale: 32768.0000 (33915.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0742  max mem: 15572
Epoch: [15]  [ 780/1404]  eta: 0:06:21  lr: 0.000074  min_lr: 0.000001  loss: 4.0404 (3.8241)  loss_scale: 32768.0000 (33900.8246)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0846  max mem: 15572
Epoch: [15]  [ 790/1404]  eta: 0:06:15  lr: 0.000074  min_lr: 0.000001  loss: 3.9363 (3.8219)  loss_scale: 32768.0000 (33886.5032)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0914  max mem: 15572
Epoch: [15]  [ 800/1404]  eta: 0:06:09  lr: 0.000074  min_lr: 0.000001  loss: 3.5632 (3.8209)  loss_scale: 32768.0000 (33872.5393)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1068  max mem: 15572
Epoch: [15]  [ 810/1404]  eta: 0:06:02  lr: 0.000074  min_lr: 0.000001  loss: 3.8490 (3.8210)  loss_scale: 32768.0000 (33858.9199)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0364  max mem: 15572
Epoch: [15]  [ 820/1404]  eta: 0:05:56  lr: 0.000074  min_lr: 0.000001  loss: 3.9167 (3.8237)  loss_scale: 32768.0000 (33845.6322)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0727  max mem: 15572
Epoch: [15]  [ 830/1404]  eta: 0:05:50  lr: 0.000074  min_lr: 0.000001  loss: 3.7336 (3.8213)  loss_scale: 32768.0000 (33832.6643)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1178  max mem: 15572
Epoch: [15]  [ 840/1404]  eta: 0:05:43  lr: 0.000074  min_lr: 0.000001  loss: 3.6883 (3.8238)  loss_scale: 32768.0000 (33820.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0715  max mem: 15572
Epoch: [15]  [ 850/1404]  eta: 0:05:37  lr: 0.000074  min_lr: 0.000001  loss: 3.8784 (3.8216)  loss_scale: 32768.0000 (33807.6428)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0352  max mem: 15572
Epoch: [15]  [ 860/1404]  eta: 0:05:31  lr: 0.000074  min_lr: 0.000001  loss: 3.6413 (3.8221)  loss_scale: 32768.0000 (33795.5679)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0477  max mem: 15572
Epoch: [15]  [ 870/1404]  eta: 0:05:24  lr: 0.000074  min_lr: 0.000001  loss: 3.9613 (3.8247)  loss_scale: 32768.0000 (33783.7704)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0567  max mem: 15572
[2025-01-10 19:37:09,526] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:37:09,527] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:37:09,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:37:09,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:37:11,562] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21939
[2025-01-10 19:37:11,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:37:11,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:37:11,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21939
[2025-01-10 19:37:11,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [ 880/1404]  eta: 0:05:18  lr: 0.000074  min_lr: 0.000001  loss: 3.7717 (3.8235)  loss_scale: 32768.0000 (33921.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0780  max mem: 15572
Epoch: [15]  [ 890/1404]  eta: 0:05:12  lr: 0.000074  min_lr: 0.000001  loss: 3.7717 (3.8258)  loss_scale: 32768.0000 (33908.0763)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0984  max mem: 15572
Epoch: [15]  [ 900/1404]  eta: 0:05:06  lr: 0.000074  min_lr: 0.000001  loss: 4.0342 (3.8259)  loss_scale: 32768.0000 (33895.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0572  max mem: 15572
Epoch: [15]  [ 910/1404]  eta: 0:05:00  lr: 0.000074  min_lr: 0.000001  loss: 3.8951 (3.8265)  loss_scale: 32768.0000 (33883.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0800  max mem: 15572
Epoch: [15]  [ 920/1404]  eta: 0:04:53  lr: 0.000074  min_lr: 0.000001  loss: 3.8947 (3.8276)  loss_scale: 32768.0000 (33870.9403)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0877  max mem: 15572
Epoch: [15]  [ 930/1404]  eta: 0:04:48  lr: 0.000074  min_lr: 0.000001  loss: 3.9078 (3.8287)  loss_scale: 32768.0000 (33859.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0855  max mem: 15572
[2025-01-10 19:37:47,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=140, lr=[7.157152265402117e-07, 7.157152265402117e-07, 1.0224503236288742e-06, 1.0224503236288742e-06, 1.4606433194698203e-06, 1.4606433194698203e-06, 2.086633313528315e-06, 2.086633313528315e-06, 2.9809047336118782e-06, 2.9809047336118782e-06, 4.258435333731255e-06, 4.258435333731255e-06, 6.083479048187507e-06, 6.083479048187507e-06, 8.690684354553583e-06, 8.690684354553583e-06, 1.2415263363647975e-05, 1.2415263363647975e-05, 1.773609051949711e-05, 1.773609051949711e-05, 2.5337272170710155e-05, 2.5337272170710155e-05, 3.619610310101451e-05, 3.619610310101451e-05, 5.1708718715735025e-05, 5.1708718715735025e-05, 7.386959816533575e-05, 7.386959816533575e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:37:47,739] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=45.376879680076684, CurrSamplesPerSec=48.776766622006974, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [ 940/1404]  eta: 0:04:42  lr: 0.000074  min_lr: 0.000001  loss: 3.9747 (3.8269)  loss_scale: 32768.0000 (33847.4984)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1366  max mem: 15572
Epoch: [15]  [ 950/1404]  eta: 0:04:35  lr: 0.000074  min_lr: 0.000001  loss: 3.8918 (3.8281)  loss_scale: 32768.0000 (33836.1472)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1056  max mem: 15572
Epoch: [15]  [ 960/1404]  eta: 0:04:29  lr: 0.000074  min_lr: 0.000001  loss: 3.8918 (3.8283)  loss_scale: 32768.0000 (33825.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0437  max mem: 15572
Epoch: [15]  [ 970/1404]  eta: 0:04:23  lr: 0.000074  min_lr: 0.000001  loss: 3.6591 (3.8261)  loss_scale: 32768.0000 (33814.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0771  max mem: 15572
Epoch: [15]  [ 980/1404]  eta: 0:04:17  lr: 0.000074  min_lr: 0.000001  loss: 3.6591 (3.8250)  loss_scale: 32768.0000 (33803.4822)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1113  max mem: 15572
Epoch: [15]  [ 990/1404]  eta: 0:04:11  lr: 0.000074  min_lr: 0.000001  loss: 3.8388 (3.8258)  loss_scale: 32768.0000 (33793.0333)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1625  max mem: 15572
Epoch: [15]  [1000/1404]  eta: 0:04:04  lr: 0.000074  min_lr: 0.000001  loss: 3.9834 (3.8250)  loss_scale: 32768.0000 (33782.7932)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1129  max mem: 15572
[2025-01-10 19:38:28,054] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:38:28,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:38:28,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:38:28,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1010/1404]  eta: 0:03:58  lr: 0.000074  min_lr: 0.000001  loss: 3.9779 (3.8276)  loss_scale: 32768.0000 (33869.9901)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0658  max mem: 15572
[2025-01-10 19:38:29,536] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22071
[2025-01-10 19:38:29,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:38:29,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22071
[2025-01-10 19:38:29,694] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:38:29,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1020/1404]  eta: 0:03:52  lr: 0.000074  min_lr: 0.000001  loss: 3.9779 (3.8288)  loss_scale: 32768.0000 (33859.1969)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1116  max mem: 15572
Epoch: [15]  [1030/1404]  eta: 0:03:46  lr: 0.000074  min_lr: 0.000001  loss: 3.8686 (3.8285)  loss_scale: 32768.0000 (33848.6130)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0465  max mem: 15572
Epoch: [15]  [1040/1404]  eta: 0:03:40  lr: 0.000074  min_lr: 0.000001  loss: 3.7789 (3.8278)  loss_scale: 32768.0000 (33838.2325)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0091  max mem: 15572
Epoch: [15]  [1050/1404]  eta: 0:03:33  lr: 0.000074  min_lr: 0.000001  loss: 3.8792 (3.8288)  loss_scale: 32768.0000 (33828.0495)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0090  max mem: 15572
Epoch: [15]  [1060/1404]  eta: 0:03:28  lr: 0.000074  min_lr: 0.000001  loss: 3.8288 (3.8253)  loss_scale: 32768.0000 (33818.0584)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0850  max mem: 15572
Epoch: [15]  [1070/1404]  eta: 0:03:22  lr: 0.000074  min_lr: 0.000001  loss: 3.5341 (3.8253)  loss_scale: 32768.0000 (33808.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0959  max mem: 15572
Epoch: [15]  [1080/1404]  eta: 0:03:15  lr: 0.000074  min_lr: 0.000001  loss: 3.7043 (3.8236)  loss_scale: 32768.0000 (33798.6309)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0504  max mem: 15572
Epoch: [15]  [1090/1404]  eta: 0:03:10  lr: 0.000073  min_lr: 0.000001  loss: 3.7280 (3.8258)  loss_scale: 32768.0000 (33789.1842)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1404  max mem: 15572
Epoch: [15]  [1100/1404]  eta: 0:03:03  lr: 0.000073  min_lr: 0.000001  loss: 3.7243 (3.8237)  loss_scale: 32768.0000 (33779.9092)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1017  max mem: 15572
Epoch: [15]  [1110/1404]  eta: 0:02:57  lr: 0.000073  min_lr: 0.000001  loss: 3.7220 (3.8241)  loss_scale: 32768.0000 (33770.8011)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0007  max mem: 15572
Epoch: [15]  [1120/1404]  eta: 0:02:51  lr: 0.000073  min_lr: 0.000001  loss: 3.9526 (3.8262)  loss_scale: 32768.0000 (33761.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0586  max mem: 15572
Epoch: [15]  [1130/1404]  eta: 0:02:45  lr: 0.000073  min_lr: 0.000001  loss: 3.9405 (3.8251)  loss_scale: 32768.0000 (33753.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0954  max mem: 15572
[2025-01-10 19:39:45,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:39:45,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:39:45,699] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:39:45,699] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1140/1404]  eta: 0:02:39  lr: 0.000073  min_lr: 0.000001  loss: 3.8205 (3.8242)  loss_scale: 32768.0000 (33773.1534)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1045  max mem: 15572
[2025-01-10 19:39:47,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22202
[2025-01-10 19:39:47,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:39:47,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:39:47,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22202
[2025-01-10 19:39:47,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [1150/1404]  eta: 0:02:33  lr: 0.000073  min_lr: 0.000001  loss: 3.9166 (3.8249)  loss_scale: 32768.0000 (33792.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0849  max mem: 15572
Epoch: [15]  [1160/1404]  eta: 0:02:27  lr: 0.000073  min_lr: 0.000001  loss: 3.8118 (3.8238)  loss_scale: 32768.0000 (33784.0620)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1134  max mem: 15572
Epoch: [15]  [1170/1404]  eta: 0:02:21  lr: 0.000073  min_lr: 0.000001  loss: 3.7038 (3.8226)  loss_scale: 32768.0000 (33775.3851)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1009  max mem: 15572
Epoch: [15]  [1180/1404]  eta: 0:02:15  lr: 0.000073  min_lr: 0.000001  loss: 3.7262 (3.8218)  loss_scale: 32768.0000 (33766.8552)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0285  max mem: 15572
Epoch: [15]  [1190/1404]  eta: 0:02:09  lr: 0.000073  min_lr: 0.000001  loss: 3.8488 (3.8216)  loss_scale: 32768.0000 (33758.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0582  max mem: 15572
Epoch: [15]  [1200/1404]  eta: 0:02:02  lr: 0.000073  min_lr: 0.000001  loss: 3.9171 (3.8220)  loss_scale: 32768.0000 (33750.2215)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0353  max mem: 15572
Epoch: [15]  [1210/1404]  eta: 0:01:56  lr: 0.000073  min_lr: 0.000001  loss: 3.9436 (3.8237)  loss_scale: 32768.0000 (33742.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0196  max mem: 15572
Epoch: [15]  [1220/1404]  eta: 0:01:50  lr: 0.000073  min_lr: 0.000001  loss: 4.0388 (3.8260)  loss_scale: 32768.0000 (33734.1327)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0264  max mem: 15572
Epoch: [15]  [1230/1404]  eta: 0:01:44  lr: 0.000073  min_lr: 0.000001  loss: 3.9863 (3.8251)  loss_scale: 32768.0000 (33726.2843)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0472  max mem: 15572
Epoch: [15]  [1240/1404]  eta: 0:01:38  lr: 0.000073  min_lr: 0.000001  loss: 3.5760 (3.8237)  loss_scale: 32768.0000 (33718.5624)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0403  max mem: 15572
Epoch: [15]  [1250/1404]  eta: 0:01:32  lr: 0.000073  min_lr: 0.000001  loss: 3.6413 (3.8241)  loss_scale: 32768.0000 (33710.9640)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0007  max mem: 15572
Epoch: [15]  [1260/1404]  eta: 0:01:26  lr: 0.000073  min_lr: 0.000001  loss: 3.8352 (3.8245)  loss_scale: 32768.0000 (33703.4861)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0006  max mem: 15572
Epoch: [15]  [1270/1404]  eta: 0:01:20  lr: 0.000073  min_lr: 0.000001  loss: 3.9692 (3.8261)  loss_scale: 32768.0000 (33696.1259)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0005  max mem: 15572
[2025-01-10 19:41:02,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:41:02,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:41:02,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:41:02,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:41:03,109] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22332
[2025-01-10 19:41:03,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:41:03,110] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:41:03,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22332
[2025-01-10 19:41:03,117] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [15]  [1280/1404]  eta: 0:01:14  lr: 0.000073  min_lr: 0.000001  loss: 3.9978 (3.8272)  loss_scale: 32768.0000 (33714.4606)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0006  max mem: 15572
Epoch: [15]  [1290/1404]  eta: 0:01:08  lr: 0.000073  min_lr: 0.000001  loss: 3.6543 (3.8261)  loss_scale: 32768.0000 (33707.1294)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0007  max mem: 15572
Epoch: [15]  [1300/1404]  eta: 0:01:02  lr: 0.000073  min_lr: 0.000001  loss: 3.7534 (3.8265)  loss_scale: 32768.0000 (33699.9108)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0239  max mem: 15572
Epoch: [15]  [1310/1404]  eta: 0:00:56  lr: 0.000073  min_lr: 0.000001  loss: 3.8651 (3.8268)  loss_scale: 32768.0000 (33692.8024)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0240  max mem: 15572
Epoch: [15]  [1320/1404]  eta: 0:00:50  lr: 0.000073  min_lr: 0.000001  loss: 3.9917 (3.8285)  loss_scale: 32768.0000 (33685.8017)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0006  max mem: 15572
Epoch: [15]  [1330/1404]  eta: 0:00:44  lr: 0.000073  min_lr: 0.000001  loss: 4.0903 (3.8302)  loss_scale: 32768.0000 (33678.9061)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0009  max mem: 15572
Epoch: [15]  [1340/1404]  eta: 0:00:38  lr: 0.000073  min_lr: 0.000001  loss: 3.9591 (3.8296)  loss_scale: 32768.0000 (33672.1133)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0010  max mem: 15572
Epoch: [15]  [1350/1404]  eta: 0:00:32  lr: 0.000073  min_lr: 0.000001  loss: 3.9591 (3.8305)  loss_scale: 32768.0000 (33665.4212)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0144  max mem: 15572
Epoch: [15]  [1360/1404]  eta: 0:00:26  lr: 0.000073  min_lr: 0.000001  loss: 4.0347 (3.8311)  loss_scale: 32768.0000 (33658.8273)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0328  max mem: 15572
Epoch: [15]  [1370/1404]  eta: 0:00:20  lr: 0.000073  min_lr: 0.000001  loss: 4.0556 (3.8314)  loss_scale: 32768.0000 (33652.3297)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0191  max mem: 15572
Epoch: [15]  [1380/1404]  eta: 0:00:14  lr: 0.000073  min_lr: 0.000001  loss: 3.8903 (3.8320)  loss_scale: 32768.0000 (33645.9261)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0010  max mem: 15572
Epoch: [15]  [1390/1404]  eta: 0:00:08  lr: 0.000073  min_lr: 0.000001  loss: 3.8177 (3.8313)  loss_scale: 32768.0000 (33639.6147)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0337  max mem: 15572
Epoch: [15]  [1400/1404]  eta: 0:00:02  lr: 0.000073  min_lr: 0.000001  loss: 3.8257 (3.8323)  loss_scale: 32768.0000 (33633.3933)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0332  max mem: 15572
[2025-01-10 19:42:16,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:42:16,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:42:16,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:42:16,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:42:16,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22463
[2025-01-10 19:42:16,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:42:16,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22463
[2025-01-10 19:42:16,971] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:42:16,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1403/1404]  eta: 0:00:00  lr: 0.000073  min_lr: 0.000001  loss: 3.8689 (3.8329)  loss_scale: 32768.0000 (33678.2222)  weight_decay: 0.0500 (0.0500)  time: 0.4574  data: 0.0004  max mem: 15572
Epoch: [15] Total time: 0:14:01 (0.5994 s / it)
Averaged stats: lr: 0.000073  min_lr: 0.000001  loss: 3.8689 (3.8315)  loss_scale: 32768.0000 (33678.2222)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:54  loss: 1.7381 (1.7381)  acc1: 66.6667 (66.6667)  acc5: 72.2222 (72.2222)  time: 7.0220  data: 6.8246  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 2.3531 (2.4083)  acc1: 50.0000 (41.4141)  acc5: 72.2222 (72.2222)  time: 0.7914  data: 0.6209  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 2.5981 (2.5201)  acc1: 33.3333 (38.0952)  acc5: 72.2222 (71.9577)  time: 0.2296  data: 0.0511  max mem: 15572
Val:  [ 30/136]  eta: 0:00:47  loss: 2.5171 (2.3287)  acc1: 33.3333 (43.0108)  acc5: 77.7778 (74.3728)  time: 0.2656  data: 0.0692  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 1.8613 (2.2898)  acc1: 55.5556 (44.5799)  acc5: 77.7778 (75.3388)  time: 0.3085  data: 0.0941  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.1988 (2.3298)  acc1: 50.0000 (44.9891)  acc5: 77.7778 (75.0545)  time: 0.3689  data: 0.1604  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.4433 (2.4353)  acc1: 33.3333 (42.1676)  acc5: 66.6667 (73.1330)  time: 0.3763  data: 0.1870  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.3280 (2.3893)  acc1: 38.8889 (43.6620)  acc5: 72.2222 (73.7872)  time: 0.3921  data: 0.1917  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.1111 (2.3829)  acc1: 50.0000 (43.7586)  acc5: 77.7778 (74.4856)  time: 0.4175  data: 0.2076  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.3054 (2.3921)  acc1: 44.4444 (42.9792)  acc5: 77.7778 (74.3590)  time: 0.4223  data: 0.2156  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.4602 (2.4500)  acc1: 33.3333 (41.6942)  acc5: 72.2222 (72.8823)  time: 0.3725  data: 0.1693  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4238 (2.4329)  acc1: 33.3333 (42.2923)  acc5: 72.2222 (73.2733)  time: 0.3571  data: 0.1621  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.9058 (2.3803)  acc1: 55.5556 (43.7098)  acc5: 83.3333 (74.2424)  time: 0.3838  data: 0.2044  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.7810 (2.3350)  acc1: 55.5556 (44.8261)  acc5: 83.3333 (74.6395)  time: 0.2685  data: 0.1141  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9058 (2.3421)  acc1: 50.0000 (44.5946)  acc5: 83.3333 (74.4472)  time: 0.2009  data: 0.0602  max mem: 15572
Val: Total time: 0:00:51 (0.3752 s / it)
* Acc@1 43.673 Acc@5 73.587 loss 2.390
Accuracy of the network on the 4883 val videos: 43.7%
[2025-01-10 19:43:08,224] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 19:43:08,226] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 19:43:08,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 19:43:08,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 19:43:10,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 19:43:10,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.67%
Epoch: [16]  [   0/1404]  eta: 3:04:50  lr: 0.000073  min_lr: 0.000001  loss: 2.6297 (2.6297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8991  data: 5.7247  max mem: 15572
Epoch: [16]  [  10/1404]  eta: 0:28:27  lr: 0.000073  min_lr: 0.000001  loss: 3.7604 (3.4040)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2246  data: 0.5214  max mem: 15572
Epoch: [16]  [  20/1404]  eta: 0:22:41  lr: 0.000073  min_lr: 0.000001  loss: 3.7870 (3.5829)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0010  max mem: 15572
Epoch: [16]  [  30/1404]  eta: 0:18:54  lr: 0.000073  min_lr: 0.000001  loss: 3.7425 (3.5631)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0008  max mem: 15572
Epoch: [16]  [  40/1404]  eta: 0:17:12  lr: 0.000073  min_lr: 0.000001  loss: 3.6341 (3.6287)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0007  max mem: 15572
Epoch: [16]  [  50/1404]  eta: 0:16:17  lr: 0.000073  min_lr: 0.000001  loss: 3.9273 (3.7340)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0007  max mem: 15572
Epoch: [16]  [  60/1404]  eta: 0:15:42  lr: 0.000073  min_lr: 0.000001  loss: 3.9273 (3.7439)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0006  max mem: 15572
Epoch: [16]  [  70/1404]  eta: 0:15:27  lr: 0.000073  min_lr: 0.000001  loss: 3.8553 (3.7550)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0006  max mem: 15572
Epoch: [16]  [  80/1404]  eta: 0:14:48  lr: 0.000073  min_lr: 0.000001  loss: 3.7125 (3.7294)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0006  max mem: 15572
Epoch: [16]  [  90/1404]  eta: 0:14:45  lr: 0.000072  min_lr: 0.000001  loss: 3.9299 (3.7525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0006  max mem: 15572
Epoch: [16]  [ 100/1404]  eta: 0:14:20  lr: 0.000072  min_lr: 0.000001  loss: 3.9565 (3.7507)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0006  max mem: 15572
Epoch: [16]  [ 110/1404]  eta: 0:14:02  lr: 0.000072  min_lr: 0.000001  loss: 3.9212 (3.7586)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0119  max mem: 15572
Epoch: [16]  [ 120/1404]  eta: 0:13:55  lr: 0.000072  min_lr: 0.000001  loss: 3.9343 (3.7549)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0119  max mem: 15572
[2025-01-10 19:44:34,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:44:34,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:44:34,425] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:44:34,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:44:34,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22593
[2025-01-10 19:44:34,885] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:44:34,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:44:34,967] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22593
[2025-01-10 19:44:34,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [16]  [ 130/1404]  eta: 0:13:45  lr: 0.000072  min_lr: 0.000001  loss: 3.5622 (3.7323)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.0008  max mem: 15572
Epoch: [16]  [ 140/1404]  eta: 0:13:36  lr: 0.000072  min_lr: 0.000001  loss: 3.6746 (3.7456)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0007  max mem: 15572
Epoch: [16]  [ 150/1404]  eta: 0:13:18  lr: 0.000072  min_lr: 0.000001  loss: 3.9962 (3.7546)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0008  max mem: 15572
Epoch: [16]  [ 160/1404]  eta: 0:13:09  lr: 0.000072  min_lr: 0.000001  loss: 3.8110 (3.7524)  loss_scale: 32768.0000 (32971.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0007  max mem: 15572
Epoch: [16]  [ 170/1404]  eta: 0:13:07  lr: 0.000072  min_lr: 0.000001  loss: 3.9323 (3.7661)  loss_scale: 32768.0000 (32959.6257)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.0006  max mem: 15572
Epoch: [16]  [ 180/1404]  eta: 0:13:00  lr: 0.000072  min_lr: 0.000001  loss: 3.9848 (3.7749)  loss_scale: 32768.0000 (32949.0387)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0005  max mem: 15572
Epoch: [16]  [ 190/1404]  eta: 0:12:44  lr: 0.000072  min_lr: 0.000001  loss: 4.0056 (3.7923)  loss_scale: 32768.0000 (32939.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0006  max mem: 15572
Epoch: [16]  [ 200/1404]  eta: 0:12:31  lr: 0.000072  min_lr: 0.000001  loss: 4.0156 (3.8007)  loss_scale: 32768.0000 (32931.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0008  max mem: 15572
Epoch: [16]  [ 210/1404]  eta: 0:12:20  lr: 0.000072  min_lr: 0.000001  loss: 3.7560 (3.7860)  loss_scale: 32768.0000 (32923.2986)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0008  max mem: 15572
Epoch: [16]  [ 220/1404]  eta: 0:12:10  lr: 0.000072  min_lr: 0.000001  loss: 3.5603 (3.7766)  loss_scale: 32768.0000 (32916.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0007  max mem: 15572
Epoch: [16]  [ 230/1404]  eta: 0:11:59  lr: 0.000072  min_lr: 0.000001  loss: 3.5956 (3.7748)  loss_scale: 32768.0000 (32909.8528)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0008  max mem: 15572
Epoch: [16]  [ 240/1404]  eta: 0:11:50  lr: 0.000072  min_lr: 0.000001  loss: 3.8872 (3.7754)  loss_scale: 32768.0000 (32903.9668)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0009  max mem: 15572
Epoch: [16]  [ 250/1404]  eta: 0:11:46  lr: 0.000072  min_lr: 0.000001  loss: 3.8872 (3.7738)  loss_scale: 32768.0000 (32898.5498)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0007  max mem: 15572
[2025-01-10 19:45:48,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:45:48,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:45:48,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:45:48,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 260/1404]  eta: 0:11:38  lr: 0.000072  min_lr: 0.000001  loss: 3.9163 (3.7770)  loss_scale: 32768.0000 (33270.1916)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0006  max mem: 15572
Epoch: [16]  [ 270/1404]  eta: 0:11:34  lr: 0.000072  min_lr: 0.000001  loss: 3.9163 (3.7791)  loss_scale: 65536.0000 (34460.8118)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0007  max mem: 15572
[2025-01-10 19:45:58,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22737
[2025-01-10 19:45:58,622] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:45:58,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 19:45:58,637] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22737
[2025-01-10 19:45:58,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [16]  [ 280/1404]  eta: 0:11:26  lr: 0.000072  min_lr: 0.000001  loss: 3.9016 (3.7817)  loss_scale: 65536.0000 (34633.7936)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0006  max mem: 15572
Epoch: [16]  [ 290/1404]  eta: 0:11:20  lr: 0.000072  min_lr: 0.000001  loss: 3.8976 (3.7846)  loss_scale: 32768.0000 (34569.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0010  max mem: 15572
Epoch: [16]  [ 300/1404]  eta: 0:11:16  lr: 0.000072  min_lr: 0.000001  loss: 3.9414 (3.7856)  loss_scale: 32768.0000 (34509.8206)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.0012  max mem: 15572
Epoch: [16]  [ 310/1404]  eta: 0:11:06  lr: 0.000072  min_lr: 0.000001  loss: 3.6724 (3.7748)  loss_scale: 32768.0000 (34453.8135)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0008  max mem: 15572
Epoch: [16]  [ 320/1404]  eta: 0:11:02  lr: 0.000072  min_lr: 0.000001  loss: 3.8749 (3.7845)  loss_scale: 32768.0000 (34401.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0006  max mem: 15572
Epoch: [16]  [ 330/1404]  eta: 0:10:53  lr: 0.000072  min_lr: 0.000001  loss: 3.9002 (3.7775)  loss_scale: 32768.0000 (34351.9517)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0007  max mem: 15572
Epoch: [16]  [ 340/1404]  eta: 0:10:48  lr: 0.000072  min_lr: 0.000001  loss: 3.9381 (3.7877)  loss_scale: 32768.0000 (34305.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0009  max mem: 15572
Epoch: [16]  [ 350/1404]  eta: 0:10:42  lr: 0.000072  min_lr: 0.000001  loss: 4.0264 (3.7865)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0008  max mem: 15572
Epoch: [16]  [ 360/1404]  eta: 0:10:33  lr: 0.000072  min_lr: 0.000001  loss: 3.8260 (3.7881)  loss_scale: 32768.0000 (34220.3213)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0008  max mem: 15572
Epoch: [16]  [ 370/1404]  eta: 0:10:25  lr: 0.000072  min_lr: 0.000001  loss: 3.8260 (3.7888)  loss_scale: 32768.0000 (34181.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0053  max mem: 15572
Epoch: [16]  [ 380/1404]  eta: 0:10:19  lr: 0.000072  min_lr: 0.000001  loss: 3.6006 (3.7846)  loss_scale: 32768.0000 (34144.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0131  max mem: 15572
Epoch: [16]  [ 390/1404]  eta: 0:10:14  lr: 0.000072  min_lr: 0.000001  loss: 3.6897 (3.7820)  loss_scale: 32768.0000 (34108.8900)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0085  max mem: 15572
Epoch: [16]  [ 400/1404]  eta: 0:10:08  lr: 0.000072  min_lr: 0.000001  loss: 3.7913 (3.7828)  loss_scale: 32768.0000 (34075.4514)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0012  max mem: 15572
[2025-01-10 19:47:14,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:47:14,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:47:14,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:47:14,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:47:15,057] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22867
[2025-01-10 19:47:15,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:47:15,059] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22867
[2025-01-10 19:47:15,059] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:47:15,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 410/1404]  eta: 0:10:02  lr: 0.000072  min_lr: 0.000001  loss: 3.7818 (3.7790)  loss_scale: 32768.0000 (34123.3674)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0298  max mem: 15572
Epoch: [16]  [ 420/1404]  eta: 0:09:56  lr: 0.000072  min_lr: 0.000001  loss: 3.7673 (3.7762)  loss_scale: 32768.0000 (34091.1734)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0353  max mem: 15572
Epoch: [16]  [ 430/1404]  eta: 0:09:48  lr: 0.000072  min_lr: 0.000001  loss: 3.9798 (3.7798)  loss_scale: 32768.0000 (34060.4733)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0067  max mem: 15572
Epoch: [16]  [ 440/1404]  eta: 0:09:43  lr: 0.000072  min_lr: 0.000001  loss: 3.9655 (3.7790)  loss_scale: 32768.0000 (34031.1655)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0006  max mem: 15572
Epoch: [16]  [ 450/1404]  eta: 0:09:36  lr: 0.000072  min_lr: 0.000001  loss: 3.7593 (3.7800)  loss_scale: 32768.0000 (34003.1574)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0232  max mem: 15572
Epoch: [16]  [ 460/1404]  eta: 0:09:32  lr: 0.000072  min_lr: 0.000001  loss: 3.6877 (3.7732)  loss_scale: 32768.0000 (33976.3644)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0731  max mem: 15572
Epoch: [16]  [ 470/1404]  eta: 0:09:25  lr: 0.000072  min_lr: 0.000001  loss: 3.5662 (3.7737)  loss_scale: 32768.0000 (33950.7091)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0761  max mem: 15572
[2025-01-10 19:47:59,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22941
[2025-01-10 19:47:59,094] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:47:59,100] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22941
[2025-01-10 19:47:59,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:47:59,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 480/1404]  eta: 0:09:17  lr: 0.000072  min_lr: 0.000001  loss: 3.6351 (3.7750)  loss_scale: 32768.0000 (33789.8711)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0262  max mem: 15572
Epoch: [16]  [ 490/1404]  eta: 0:09:10  lr: 0.000071  min_lr: 0.000001  loss: 3.8959 (3.7811)  loss_scale: 16384.0000 (33435.3727)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0007  max mem: 15572
Epoch: [16]  [ 500/1404]  eta: 0:09:03  lr: 0.000071  min_lr: 0.000001  loss: 3.9522 (3.7822)  loss_scale: 16384.0000 (33095.0259)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0006  max mem: 15572
Epoch: [16]  [ 510/1404]  eta: 0:08:57  lr: 0.000071  min_lr: 0.000001  loss: 3.8696 (3.7806)  loss_scale: 16384.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0005  max mem: 15572
Epoch: [16]  [ 520/1404]  eta: 0:08:51  lr: 0.000071  min_lr: 0.000001  loss: 3.8103 (3.7833)  loss_scale: 16384.0000 (32453.5278)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0006  max mem: 15572
Epoch: [16]  [ 530/1404]  eta: 0:08:46  lr: 0.000071  min_lr: 0.000001  loss: 3.8980 (3.7831)  loss_scale: 16384.0000 (32150.9002)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0007  max mem: 15572
[2025-01-10 19:48:33,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=148, lr=[6.914755726578027e-07, 6.914755726578027e-07, 9.87822246654004e-07, 9.87822246654004e-07, 1.4111746380771487e-06, 1.4111746380771487e-06, 2.015963768681641e-06, 2.015963768681641e-06, 2.879948240973773e-06, 2.879948240973773e-06, 4.114211772819676e-06, 4.114211772819676e-06, 5.877445389742394e-06, 5.877445389742394e-06, 8.39635055677485e-06, 8.39635055677485e-06, 1.1994786509678356e-05, 1.1994786509678356e-05, 1.7135409299540513e-05, 1.7135409299540513e-05, 2.4479156142200732e-05, 2.4479156142200732e-05, 3.497022306028676e-05, 3.497022306028676e-05, 4.995746151469538e-05, 4.995746151469538e-05, 7.136780216385054e-05, 7.136780216385054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:48:33,913] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=45.36408656669427, CurrSamplesPerSec=56.08345497536331, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [ 540/1404]  eta: 0:08:41  lr: 0.000071  min_lr: 0.000001  loss: 3.7729 (3.7823)  loss_scale: 16384.0000 (31859.4603)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0296  max mem: 15572
Epoch: [16]  [ 550/1404]  eta: 0:08:34  lr: 0.000071  min_lr: 0.000001  loss: 3.7729 (3.7838)  loss_scale: 16384.0000 (31578.5989)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0296  max mem: 15572
Epoch: [16]  [ 560/1404]  eta: 0:08:28  lr: 0.000071  min_lr: 0.000001  loss: 3.5577 (3.7780)  loss_scale: 16384.0000 (31307.7504)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0008  max mem: 15572
Epoch: [16]  [ 570/1404]  eta: 0:08:22  lr: 0.000071  min_lr: 0.000001  loss: 3.5577 (3.7782)  loss_scale: 16384.0000 (31046.3888)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0008  max mem: 15572
Epoch: [16]  [ 580/1404]  eta: 0:08:15  lr: 0.000071  min_lr: 0.000001  loss: 3.7591 (3.7798)  loss_scale: 16384.0000 (30794.0241)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0007  max mem: 15572
Epoch: [16]  [ 590/1404]  eta: 0:08:08  lr: 0.000071  min_lr: 0.000001  loss: 3.6863 (3.7765)  loss_scale: 16384.0000 (30550.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0008  max mem: 15572
Epoch: [16]  [ 600/1404]  eta: 0:08:01  lr: 0.000071  min_lr: 0.000001  loss: 3.3986 (3.7731)  loss_scale: 16384.0000 (30314.4892)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0010  max mem: 15572
[2025-01-10 19:49:14,684] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:49:14,685] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 19:49:14,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:49:14,713] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 610/1404]  eta: 0:07:55  lr: 0.000071  min_lr: 0.000001  loss: 3.6976 (3.7723)  loss_scale: 16384.0000 (30220.5696)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0008  max mem: 15572
Epoch: [16]  [ 620/1404]  eta: 0:07:50  lr: 0.000071  min_lr: 0.000001  loss: 3.7616 (3.7707)  loss_scale: 32768.0000 (30261.5910)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0005  max mem: 15572
Epoch: [16]  [ 630/1404]  eta: 0:07:44  lr: 0.000071  min_lr: 0.000001  loss: 3.7643 (3.7703)  loss_scale: 32768.0000 (30301.3122)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0007  max mem: 15572
[2025-01-10 19:49:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23101
[2025-01-10 19:49:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:49:34,387] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23101
[2025-01-10 19:49:34,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:49:34,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 640/1404]  eta: 0:07:39  lr: 0.000071  min_lr: 0.000001  loss: 3.8914 (3.7718)  loss_scale: 32768.0000 (30237.5538)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0009  max mem: 15572
Epoch: [16]  [ 650/1404]  eta: 0:07:33  lr: 0.000071  min_lr: 0.000001  loss: 3.9833 (3.7710)  loss_scale: 16384.0000 (30024.7496)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0008  max mem: 15572
Epoch: [16]  [ 660/1404]  eta: 0:07:27  lr: 0.000071  min_lr: 0.000001  loss: 3.9828 (3.7718)  loss_scale: 16384.0000 (29818.3843)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0008  max mem: 15572
Epoch: [16]  [ 670/1404]  eta: 0:07:21  lr: 0.000071  min_lr: 0.000001  loss: 3.8083 (3.7717)  loss_scale: 16384.0000 (29618.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0008  max mem: 15572
Epoch: [16]  [ 680/1404]  eta: 0:07:14  lr: 0.000071  min_lr: 0.000001  loss: 3.8083 (3.7716)  loss_scale: 16384.0000 (29423.8355)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0008  max mem: 15572
Epoch: [16]  [ 690/1404]  eta: 0:07:07  lr: 0.000071  min_lr: 0.000001  loss: 3.8608 (3.7720)  loss_scale: 16384.0000 (29235.1259)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0008  max mem: 15572
Epoch: [16]  [ 700/1404]  eta: 0:07:02  lr: 0.000071  min_lr: 0.000001  loss: 3.7537 (3.7712)  loss_scale: 16384.0000 (29051.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0007  max mem: 15572
Epoch: [16]  [ 710/1404]  eta: 0:06:56  lr: 0.000071  min_lr: 0.000001  loss: 3.7015 (3.7713)  loss_scale: 16384.0000 (28873.6315)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0005  max mem: 15572
Epoch: [16]  [ 720/1404]  eta: 0:06:50  lr: 0.000071  min_lr: 0.000001  loss: 3.7460 (3.7704)  loss_scale: 16384.0000 (28700.4050)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0004  max mem: 15572
Epoch: [16]  [ 730/1404]  eta: 0:06:43  lr: 0.000071  min_lr: 0.000001  loss: 3.7460 (3.7733)  loss_scale: 16384.0000 (28531.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0036  max mem: 15572
Epoch: [16]  [ 740/1404]  eta: 0:06:37  lr: 0.000071  min_lr: 0.000001  loss: 3.9437 (3.7735)  loss_scale: 16384.0000 (28367.9784)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0037  max mem: 15572
Epoch: [16]  [ 750/1404]  eta: 0:06:31  lr: 0.000071  min_lr: 0.000001  loss: 3.8126 (3.7739)  loss_scale: 16384.0000 (28208.4048)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0006  max mem: 15572
Epoch: [16]  [ 760/1404]  eta: 0:06:25  lr: 0.000071  min_lr: 0.000001  loss: 3.8823 (3.7762)  loss_scale: 16384.0000 (28053.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0006  max mem: 15572
[2025-01-10 19:50:49,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:50:49,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 19:50:49,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:50:49,653] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 770/1404]  eta: 0:06:19  lr: 0.000071  min_lr: 0.000001  loss: 3.8117 (3.7762)  loss_scale: 16384.0000 (28007.9274)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0007  max mem: 15572
Epoch: [16]  [ 780/1404]  eta: 0:06:12  lr: 0.000071  min_lr: 0.000001  loss: 3.6747 (3.7763)  loss_scale: 32768.0000 (28068.8758)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0009  max mem: 15572
Epoch: [16]  [ 790/1404]  eta: 0:06:06  lr: 0.000071  min_lr: 0.000001  loss: 3.7142 (3.7781)  loss_scale: 32768.0000 (28128.2832)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0008  max mem: 15572
Epoch: [16]  [ 800/1404]  eta: 0:06:00  lr: 0.000071  min_lr: 0.000001  loss: 3.9769 (3.7808)  loss_scale: 32768.0000 (28186.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0006  max mem: 15572
Epoch: [16]  [ 810/1404]  eta: 0:05:54  lr: 0.000071  min_lr: 0.000001  loss: 3.9385 (3.7803)  loss_scale: 32768.0000 (28242.7028)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0008  max mem: 15572
Epoch: [16]  [ 820/1404]  eta: 0:05:48  lr: 0.000071  min_lr: 0.000001  loss: 3.5715 (3.7782)  loss_scale: 32768.0000 (28297.8222)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0008  max mem: 15572
Epoch: [16]  [ 830/1404]  eta: 0:05:42  lr: 0.000071  min_lr: 0.000001  loss: 3.6270 (3.7761)  loss_scale: 32768.0000 (28351.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0006  max mem: 15572
Epoch: [16]  [ 840/1404]  eta: 0:05:36  lr: 0.000071  min_lr: 0.000001  loss: 3.7272 (3.7756)  loss_scale: 32768.0000 (28404.1284)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0009  max mem: 15572
Epoch: [16]  [ 850/1404]  eta: 0:05:30  lr: 0.000071  min_lr: 0.000001  loss: 4.0363 (3.7789)  loss_scale: 32768.0000 (28455.4078)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0010  max mem: 15572
Epoch: [16]  [ 860/1404]  eta: 0:05:24  lr: 0.000071  min_lr: 0.000001  loss: 4.0363 (3.7804)  loss_scale: 32768.0000 (28505.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0009  max mem: 15572
Epoch: [16]  [ 870/1404]  eta: 0:05:18  lr: 0.000071  min_lr: 0.000001  loss: 3.8350 (3.7813)  loss_scale: 32768.0000 (28554.4340)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0009  max mem: 15572
Epoch: [16]  [ 880/1404]  eta: 0:05:12  lr: 0.000070  min_lr: 0.000001  loss: 3.7024 (3.7810)  loss_scale: 32768.0000 (28602.2611)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0008  max mem: 15572
Epoch: [16]  [ 890/1404]  eta: 0:05:06  lr: 0.000070  min_lr: 0.000001  loss: 3.7024 (3.7826)  loss_scale: 32768.0000 (28649.0146)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0009  max mem: 15572
[2025-01-10 19:52:04,544] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:52:04,544] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:52:04,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:52:04,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:52:06,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23362
[2025-01-10 19:52:06,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:52:06,765] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23362
[2025-01-10 19:52:06,766] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:52:06,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 900/1404]  eta: 0:05:00  lr: 0.000070  min_lr: 0.000001  loss: 4.0860 (3.7841)  loss_scale: 32768.0000 (28840.2042)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0008  max mem: 15572
Epoch: [16]  [ 910/1404]  eta: 0:04:54  lr: 0.000070  min_lr: 0.000001  loss: 3.7184 (3.7820)  loss_scale: 32768.0000 (28883.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0105  max mem: 15572
Epoch: [16]  [ 920/1404]  eta: 0:04:48  lr: 0.000070  min_lr: 0.000001  loss: 3.4141 (3.7798)  loss_scale: 32768.0000 (28925.4984)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0150  max mem: 15572
Epoch: [16]  [ 930/1404]  eta: 0:04:42  lr: 0.000070  min_lr: 0.000001  loss: 3.7619 (3.7805)  loss_scale: 32768.0000 (28966.7712)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.0830  max mem: 15572
Epoch: [16]  [ 940/1404]  eta: 0:04:36  lr: 0.000070  min_lr: 0.000001  loss: 3.7261 (3.7777)  loss_scale: 32768.0000 (29007.1668)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0784  max mem: 15572
Epoch: [16]  [ 950/1404]  eta: 0:04:30  lr: 0.000070  min_lr: 0.000001  loss: 3.7095 (3.7774)  loss_scale: 32768.0000 (29046.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0008  max mem: 15572
Epoch: [16]  [ 960/1404]  eta: 0:04:24  lr: 0.000070  min_lr: 0.000001  loss: 3.8034 (3.7766)  loss_scale: 32768.0000 (29085.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0113  max mem: 15572
Epoch: [16]  [ 970/1404]  eta: 0:04:18  lr: 0.000070  min_lr: 0.000001  loss: 3.8578 (3.7777)  loss_scale: 32768.0000 (29123.3615)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0113  max mem: 15572
Epoch: [16]  [ 980/1404]  eta: 0:04:13  lr: 0.000070  min_lr: 0.000001  loss: 3.8707 (3.7782)  loss_scale: 32768.0000 (29160.5138)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0010  max mem: 15572
Epoch: [16]  [ 990/1404]  eta: 0:04:06  lr: 0.000070  min_lr: 0.000001  loss: 4.0056 (3.7800)  loss_scale: 32768.0000 (29196.9162)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0010  max mem: 15572
Epoch: [16]  [1000/1404]  eta: 0:04:00  lr: 0.000070  min_lr: 0.000001  loss: 4.0056 (3.7808)  loss_scale: 32768.0000 (29232.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0008  max mem: 15572
Epoch: [16]  [1010/1404]  eta: 0:03:54  lr: 0.000070  min_lr: 0.000001  loss: 3.9668 (3.7821)  loss_scale: 32768.0000 (29267.5608)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0604  max mem: 15572
Epoch: [16]  [1020/1404]  eta: 0:03:48  lr: 0.000070  min_lr: 0.000001  loss: 3.9400 (3.7817)  loss_scale: 32768.0000 (29301.8452)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0711  max mem: 15572
[2025-01-10 19:53:23,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:53:23,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:53:23,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:53:23,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1030/1404]  eta: 0:03:42  lr: 0.000070  min_lr: 0.000001  loss: 3.6419 (3.7788)  loss_scale: 32768.0000 (29462.5955)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0114  max mem: 15572
[2025-01-10 19:53:28,845] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23501
[2025-01-10 19:53:28,845] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:53:28,849] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23501
[2025-01-10 19:53:28,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:53:28,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1040/1404]  eta: 0:03:36  lr: 0.000070  min_lr: 0.000001  loss: 3.8231 (3.7796)  loss_scale: 32768.0000 (29683.2123)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0006  max mem: 15572
Epoch: [16]  [1050/1404]  eta: 0:03:30  lr: 0.000070  min_lr: 0.000001  loss: 3.8310 (3.7804)  loss_scale: 32768.0000 (29712.5633)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0011  max mem: 15572
Epoch: [16]  [1060/1404]  eta: 0:03:24  lr: 0.000070  min_lr: 0.000001  loss: 3.8310 (3.7816)  loss_scale: 32768.0000 (29741.3610)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.0015  max mem: 15572
Epoch: [16]  [1070/1404]  eta: 0:03:18  lr: 0.000070  min_lr: 0.000001  loss: 3.8876 (3.7811)  loss_scale: 32768.0000 (29769.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0012  max mem: 15572
Epoch: [16]  [1080/1404]  eta: 0:03:12  lr: 0.000070  min_lr: 0.000001  loss: 3.7364 (3.7793)  loss_scale: 32768.0000 (29797.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0007  max mem: 15572
[2025-01-10 19:53:58,475] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23551
[2025-01-10 19:53:58,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 19:53:58,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-10 19:53:58,502] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23551
[2025-01-10 19:53:58,503] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [16]  [1090/1404]  eta: 0:03:06  lr: 0.000070  min_lr: 0.000001  loss: 3.5716 (3.7795)  loss_scale: 32768.0000 (29764.5170)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0007  max mem: 15572
Epoch: [16]  [1100/1404]  eta: 0:03:00  lr: 0.000070  min_lr: 0.000001  loss: 3.8360 (3.7809)  loss_scale: 16384.0000 (29642.9864)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0233  max mem: 15572
Epoch: [16]  [1110/1404]  eta: 0:02:54  lr: 0.000070  min_lr: 0.000001  loss: 3.8290 (3.7803)  loss_scale: 16384.0000 (29523.6436)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0233  max mem: 15572
Epoch: [16]  [1120/1404]  eta: 0:02:48  lr: 0.000070  min_lr: 0.000001  loss: 3.7797 (3.7812)  loss_scale: 16384.0000 (29406.4300)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0008  max mem: 15572
Epoch: [16]  [1130/1404]  eta: 0:02:42  lr: 0.000070  min_lr: 0.000001  loss: 3.8361 (3.7820)  loss_scale: 16384.0000 (29291.2891)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0258  max mem: 15572
Epoch: [16]  [1140/1404]  eta: 0:02:37  lr: 0.000070  min_lr: 0.000001  loss: 3.9444 (3.7835)  loss_scale: 16384.0000 (29178.1665)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0715  max mem: 15572
Epoch: [16]  [1150/1404]  eta: 0:02:31  lr: 0.000070  min_lr: 0.000001  loss: 4.0606 (3.7859)  loss_scale: 16384.0000 (29067.0096)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0835  max mem: 15572
Epoch: [16]  [1160/1404]  eta: 0:02:25  lr: 0.000070  min_lr: 0.000001  loss: 3.8995 (3.7868)  loss_scale: 16384.0000 (28957.7674)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0862  max mem: 15572
Epoch: [16]  [1170/1404]  eta: 0:02:19  lr: 0.000070  min_lr: 0.000001  loss: 3.7214 (3.7862)  loss_scale: 16384.0000 (28850.3911)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.1396  max mem: 15572
Epoch: [16]  [1180/1404]  eta: 0:02:13  lr: 0.000070  min_lr: 0.000001  loss: 3.7088 (3.7863)  loss_scale: 16384.0000 (28744.8332)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0912  max mem: 15572
Epoch: [16]  [1190/1404]  eta: 0:02:07  lr: 0.000070  min_lr: 0.000001  loss: 3.7554 (3.7860)  loss_scale: 16384.0000 (28641.0479)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0008  max mem: 15572
Epoch: [16]  [1200/1404]  eta: 0:02:01  lr: 0.000070  min_lr: 0.000001  loss: 3.8077 (3.7875)  loss_scale: 16384.0000 (28538.9908)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0008  max mem: 15572
Epoch: [16]  [1210/1404]  eta: 0:01:55  lr: 0.000070  min_lr: 0.000001  loss: 3.9249 (3.7880)  loss_scale: 16384.0000 (28438.6193)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0010  max mem: 15572
[2025-01-10 19:55:14,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:55:14,015] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 19:55:14,025] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:55:14,026] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [1220/1404]  eta: 0:01:49  lr: 0.000070  min_lr: 0.000001  loss: 3.9041 (3.7868)  loss_scale: 16384.0000 (28406.9844)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0532  max mem: 15572
Epoch: [16]  [1230/1404]  eta: 0:01:43  lr: 0.000070  min_lr: 0.000001  loss: 3.8347 (3.7866)  loss_scale: 32768.0000 (28442.4110)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0872  max mem: 15572
Epoch: [16]  [1240/1404]  eta: 0:01:37  lr: 0.000070  min_lr: 0.000001  loss: 3.8347 (3.7863)  loss_scale: 32768.0000 (28477.2667)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0940  max mem: 15572
Epoch: [16]  [1250/1404]  eta: 0:01:31  lr: 0.000070  min_lr: 0.000001  loss: 3.9458 (3.7877)  loss_scale: 32768.0000 (28511.5651)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0864  max mem: 15572
Epoch: [16]  [1260/1404]  eta: 0:01:25  lr: 0.000069  min_lr: 0.000001  loss: 3.9598 (3.7883)  loss_scale: 32768.0000 (28545.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0856  max mem: 15572
Epoch: [16]  [1270/1404]  eta: 0:01:19  lr: 0.000069  min_lr: 0.000001  loss: 3.7495 (3.7874)  loss_scale: 32768.0000 (28578.5429)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1153  max mem: 15572
Epoch: [16]  [1280/1404]  eta: 0:01:13  lr: 0.000069  min_lr: 0.000001  loss: 3.7735 (3.7881)  loss_scale: 32768.0000 (28611.2475)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1080  max mem: 15572
Epoch: [16]  [1290/1404]  eta: 0:01:07  lr: 0.000069  min_lr: 0.000001  loss: 3.8844 (3.7890)  loss_scale: 32768.0000 (28643.4454)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0874  max mem: 15572
Epoch: [16]  [1300/1404]  eta: 0:01:01  lr: 0.000069  min_lr: 0.000001  loss: 3.9811 (3.7900)  loss_scale: 32768.0000 (28675.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0683  max mem: 15572
Epoch: [16]  [1310/1404]  eta: 0:00:55  lr: 0.000069  min_lr: 0.000001  loss: 3.7078 (3.7889)  loss_scale: 32768.0000 (28706.3677)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1116  max mem: 15572
Epoch: [16]  [1320/1404]  eta: 0:00:49  lr: 0.000069  min_lr: 0.000001  loss: 3.5683 (3.7890)  loss_scale: 32768.0000 (28737.1143)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0993  max mem: 15572
Epoch: [16]  [1330/1404]  eta: 0:00:43  lr: 0.000069  min_lr: 0.000001  loss: 3.5986 (3.7882)  loss_scale: 32768.0000 (28767.3989)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0369  max mem: 15572
Epoch: [16]  [1340/1404]  eta: 0:00:38  lr: 0.000069  min_lr: 0.000001  loss: 3.7058 (3.7884)  loss_scale: 32768.0000 (28797.2319)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0695  max mem: 15572
[2025-01-10 19:56:30,263] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:56:30,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:56:30,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:56:30,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1350/1404]  eta: 0:00:32  lr: 0.000069  min_lr: 0.000001  loss: 3.7698 (3.7889)  loss_scale: 32768.0000 (28996.4056)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0971  max mem: 15572
[2025-01-10 19:56:35,294] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23816
[2025-01-10 19:56:35,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:56:35,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23816
[2025-01-10 19:56:35,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:56:35,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1360/1404]  eta: 0:00:26  lr: 0.000069  min_lr: 0.000001  loss: 3.5961 (3.7867)  loss_scale: 32768.0000 (29048.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0449  max mem: 15572
Epoch: [16]  [1370/1404]  eta: 0:00:20  lr: 0.000069  min_lr: 0.000001  loss: 3.7030 (3.7871)  loss_scale: 32768.0000 (29075.3260)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0011  max mem: 15572
Epoch: [16]  [1380/1404]  eta: 0:00:14  lr: 0.000069  min_lr: 0.000001  loss: 3.7785 (3.7866)  loss_scale: 32768.0000 (29102.0652)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0135  max mem: 15572
Epoch: [16]  [1390/1404]  eta: 0:00:08  lr: 0.000069  min_lr: 0.000001  loss: 3.9842 (3.7868)  loss_scale: 32768.0000 (29128.4198)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0525  max mem: 15572
Epoch: [16]  [1400/1404]  eta: 0:00:02  lr: 0.000069  min_lr: 0.000001  loss: 4.0291 (3.7880)  loss_scale: 32768.0000 (29154.3983)  weight_decay: 0.0500 (0.0500)  time: 0.4855  data: 0.0398  max mem: 15572
Epoch: [16]  [1403/1404]  eta: 0:00:00  lr: 0.000069  min_lr: 0.000001  loss: 4.0746 (3.7885)  loss_scale: 32768.0000 (29162.1197)  weight_decay: 0.0500 (0.0500)  time: 0.4732  data: 0.0397  max mem: 15572
Epoch: [16] Total time: 0:13:50 (0.5917 s / it)
Averaged stats: lr: 0.000069  min_lr: 0.000001  loss: 4.0746 (3.8004)  loss_scale: 32768.0000 (29162.1197)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:35  loss: 1.5535 (1.5535)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 6.8817  data: 6.6776  max mem: 15572
Val:  [ 10/136]  eta: 0:01:53  loss: 2.2890 (2.3693)  acc1: 50.0000 (43.4343)  acc5: 72.2222 (73.7374)  time: 0.8976  data: 0.7026  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.4525 (2.4359)  acc1: 44.4444 (41.0053)  acc5: 72.2222 (74.6032)  time: 0.2796  data: 0.0795  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.2977 (2.2717)  acc1: 44.4444 (44.6237)  acc5: 77.7778 (76.5233)  time: 0.2992  data: 0.0714  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 1.9509 (2.2380)  acc1: 55.5556 (46.4770)  acc5: 77.7778 (77.9133)  time: 0.3484  data: 0.1164  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 2.1118 (2.2652)  acc1: 50.0000 (46.6231)  acc5: 77.7778 (78.2135)  time: 0.4130  data: 0.1856  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.5968 (2.3891)  acc1: 33.3333 (42.9872)  acc5: 77.7778 (75.5009)  time: 0.4192  data: 0.1861  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.4429 (2.3503)  acc1: 33.3333 (44.6792)  acc5: 72.2222 (75.6651)  time: 0.3568  data: 0.1390  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.0699 (2.3396)  acc1: 55.5556 (45.1303)  acc5: 83.3333 (76.2003)  time: 0.3333  data: 0.1174  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.2700 (2.3454)  acc1: 44.4444 (44.6886)  acc5: 72.2222 (75.8852)  time: 0.3549  data: 0.1204  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 2.4118 (2.4037)  acc1: 33.3333 (43.1793)  acc5: 66.6667 (74.3674)  time: 0.3880  data: 0.1604  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.3800 (2.3921)  acc1: 33.3333 (43.6436)  acc5: 72.2222 (74.4745)  time: 0.3748  data: 0.1555  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8899 (2.3361)  acc1: 61.1111 (44.9036)  acc5: 83.3333 (75.5280)  time: 0.3160  data: 0.1137  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6981 (2.2887)  acc1: 61.1111 (45.8439)  acc5: 83.3333 (75.9966)  time: 0.2085  data: 0.0491  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8935 (2.2939)  acc1: 44.4444 (45.7002)  acc5: 83.3333 (76.0033)  time: 0.1689  data: 0.0239  max mem: 15572
Val: Total time: 0:00:50 (0.3741 s / it)
* Acc@1 44.656 Acc@5 74.816 loss 2.340
Accuracy of the network on the 4883 val videos: 44.7%
[2025-01-10 19:57:52,149] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 19:57:52,150] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 19:57:52,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 19:57:52,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 19:57:54,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 19:57:54,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.66%
Epoch: [17]  [   0/1404]  eta: 3:35:13  lr: 0.000069  min_lr: 0.000001  loss: 3.7009 (3.7009)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1973  data: 7.5067  max mem: 15572
Epoch: [17]  [  10/1404]  eta: 0:31:13  lr: 0.000069  min_lr: 0.000001  loss: 3.7009 (3.6553)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3441  data: 0.6830  max mem: 15572
Epoch: [17]  [  20/1404]  eta: 0:21:54  lr: 0.000069  min_lr: 0.000001  loss: 3.9051 (3.7024)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0007  max mem: 15572
Epoch: [17]  [  30/1404]  eta: 0:18:41  lr: 0.000069  min_lr: 0.000001  loss: 3.8093 (3.7370)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0008  max mem: 15572
Epoch: [17]  [  40/1404]  eta: 0:17:26  lr: 0.000069  min_lr: 0.000001  loss: 3.7527 (3.7658)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0008  max mem: 15572
Epoch: [17]  [  50/1404]  eta: 0:16:28  lr: 0.000069  min_lr: 0.000001  loss: 3.7331 (3.7570)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0008  max mem: 15572
Epoch: [17]  [  60/1404]  eta: 0:15:54  lr: 0.000069  min_lr: 0.000001  loss: 3.7331 (3.7553)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0007  max mem: 15572
Epoch: [17]  [  70/1404]  eta: 0:15:23  lr: 0.000069  min_lr: 0.000001  loss: 3.7808 (3.7601)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0006  max mem: 15572
[2025-01-10 19:58:47,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:58:47,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:58:47,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 19:58:47,413] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 19:58:49,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23948
[2025-01-10 19:58:49,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 19:58:49,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [  80/1404]  eta: 0:15:01  lr: 0.000069  min_lr: 0.000001  loss: 3.8100 (3.7593)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0005  max mem: 15572
[2025-01-10 19:58:49,794] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23948
[2025-01-10 19:58:49,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [17]  [  90/1404]  eta: 0:14:29  lr: 0.000069  min_lr: 0.000001  loss: 3.9239 (3.7716)  loss_scale: 32768.0000 (33848.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0006  max mem: 15572
Epoch: [17]  [ 100/1404]  eta: 0:14:27  lr: 0.000069  min_lr: 0.000001  loss: 3.9387 (3.7701)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0939  max mem: 15572
Epoch: [17]  [ 110/1404]  eta: 0:14:14  lr: 0.000069  min_lr: 0.000001  loss: 3.7650 (3.7616)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.1164  max mem: 15572
Epoch: [17]  [ 120/1404]  eta: 0:13:52  lr: 0.000069  min_lr: 0.000001  loss: 3.6537 (3.7530)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0277  max mem: 15572
Epoch: [17]  [ 130/1404]  eta: 0:13:41  lr: 0.000069  min_lr: 0.000001  loss: 3.6537 (3.7431)  loss_scale: 32768.0000 (33518.4122)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0388  max mem: 15572
[2025-01-10 19:59:19,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=154, lr=[6.66268170184138e-07, 6.66268170184138e-07, 9.518116716916258e-07, 9.518116716916258e-07, 1.3597309595594655e-06, 1.3597309595594655e-06, 1.942472799370665e-06, 1.942472799370665e-06, 2.7749611419580934e-06, 2.7749611419580934e-06, 3.964230202797277e-06, 3.964230202797277e-06, 5.6631860039961095e-06, 5.6631860039961095e-06, 8.090265719994443e-06, 8.090265719994443e-06, 1.1557522457134919e-05, 1.1557522457134919e-05, 1.65107463673356e-05, 1.65107463673356e-05, 2.3586780524765142e-05, 2.3586780524765142e-05, 3.369540074966449e-05, 3.369540074966449e-05, 4.813628678523499e-05, 4.813628678523499e-05, 6.876612397890714e-05, 6.876612397890714e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 19:59:19,604] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=45.36368509479654, CurrSamplesPerSec=53.77881634916513, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [ 140/1404]  eta: 0:13:21  lr: 0.000069  min_lr: 0.000001  loss: 4.0376 (3.7758)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0345  max mem: 15572
Epoch: [17]  [ 150/1404]  eta: 0:13:07  lr: 0.000069  min_lr: 0.000001  loss: 4.0502 (3.7707)  loss_scale: 32768.0000 (33419.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0204  max mem: 15572
Epoch: [17]  [ 160/1404]  eta: 0:12:55  lr: 0.000069  min_lr: 0.000001  loss: 3.7249 (3.7572)  loss_scale: 32768.0000 (33378.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0556  max mem: 15572
Epoch: [17]  [ 170/1404]  eta: 0:12:46  lr: 0.000069  min_lr: 0.000001  loss: 3.6937 (3.7482)  loss_scale: 32768.0000 (33342.8772)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0824  max mem: 15572
Epoch: [17]  [ 180/1404]  eta: 0:12:42  lr: 0.000069  min_lr: 0.000001  loss: 3.6766 (3.7521)  loss_scale: 32768.0000 (33311.1160)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0966  max mem: 15572
Epoch: [17]  [ 190/1404]  eta: 0:12:36  lr: 0.000069  min_lr: 0.000001  loss: 3.8498 (3.7563)  loss_scale: 32768.0000 (33282.6806)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0501  max mem: 15572
Epoch: [17]  [ 200/1404]  eta: 0:12:31  lr: 0.000069  min_lr: 0.000001  loss: 3.8240 (3.7612)  loss_scale: 32768.0000 (33257.0746)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0786  max mem: 15572
[2025-01-10 20:00:05,416] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:00:05,417] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:00:05,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:00:05,464] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 210/1404]  eta: 0:12:22  lr: 0.000069  min_lr: 0.000001  loss: 3.6148 (3.7589)  loss_scale: 32768.0000 (33544.4929)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1191  max mem: 15572
Epoch: [17]  [ 220/1404]  eta: 0:12:13  lr: 0.000069  min_lr: 0.000001  loss: 3.6978 (3.7582)  loss_scale: 65536.0000 (34992.0724)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0568  max mem: 15572
[2025-01-10 20:00:14,324] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24093
[2025-01-10 20:00:14,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:00:14,333] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24093
[2025-01-10 20:00:14,333] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:00:14,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 230/1404]  eta: 0:12:07  lr: 0.000069  min_lr: 0.000001  loss: 3.7893 (3.7545)  loss_scale: 65536.0000 (35463.2035)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0654  max mem: 15572
Epoch: [17]  [ 240/1404]  eta: 0:11:57  lr: 0.000068  min_lr: 0.000001  loss: 3.8994 (3.7536)  loss_scale: 32768.0000 (35351.3693)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0496  max mem: 15572
Epoch: [17]  [ 250/1404]  eta: 0:11:49  lr: 0.000068  min_lr: 0.000001  loss: 4.0046 (3.7554)  loss_scale: 32768.0000 (35248.4462)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0009  max mem: 15572
Epoch: [17]  [ 260/1404]  eta: 0:11:44  lr: 0.000068  min_lr: 0.000001  loss: 3.7944 (3.7575)  loss_scale: 32768.0000 (35153.4100)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.0236  max mem: 15572
Epoch: [17]  [ 270/1404]  eta: 0:11:38  lr: 0.000068  min_lr: 0.000001  loss: 3.8525 (3.7630)  loss_scale: 32768.0000 (35065.3875)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0756  max mem: 15572
Epoch: [17]  [ 280/1404]  eta: 0:11:31  lr: 0.000068  min_lr: 0.000001  loss: 3.9213 (3.7651)  loss_scale: 32768.0000 (34983.6299)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1222  max mem: 15572
Epoch: [17]  [ 290/1404]  eta: 0:11:26  lr: 0.000068  min_lr: 0.000001  loss: 3.8578 (3.7678)  loss_scale: 32768.0000 (34907.4914)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1464  max mem: 15572
Epoch: [17]  [ 300/1404]  eta: 0:11:18  lr: 0.000068  min_lr: 0.000001  loss: 3.9267 (3.7726)  loss_scale: 32768.0000 (34836.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0959  max mem: 15572
Epoch: [17]  [ 310/1404]  eta: 0:11:11  lr: 0.000068  min_lr: 0.000001  loss: 3.7323 (3.7654)  loss_scale: 32768.0000 (34769.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0627  max mem: 15572
Epoch: [17]  [ 320/1404]  eta: 0:11:06  lr: 0.000068  min_lr: 0.000001  loss: 3.6755 (3.7696)  loss_scale: 32768.0000 (34707.5389)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1052  max mem: 15572
Epoch: [17]  [ 330/1404]  eta: 0:10:58  lr: 0.000068  min_lr: 0.000001  loss: 4.0112 (3.7746)  loss_scale: 32768.0000 (34648.9426)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0854  max mem: 15572
Epoch: [17]  [ 340/1404]  eta: 0:10:52  lr: 0.000068  min_lr: 0.000001  loss: 4.0196 (3.7805)  loss_scale: 32768.0000 (34593.7830)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0722  max mem: 15572
Epoch: [17]  [ 350/1404]  eta: 0:10:45  lr: 0.000068  min_lr: 0.000001  loss: 3.9624 (3.7822)  loss_scale: 32768.0000 (34541.7664)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0781  max mem: 15572
[2025-01-10 20:01:33,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:01:33,309] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:01:33,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:01:33,313] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 360/1404]  eta: 0:10:40  lr: 0.000068  min_lr: 0.000001  loss: 3.9259 (3.7843)  loss_scale: 32768.0000 (35128.0222)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1223  max mem: 15572
[2025-01-10 20:01:41,369] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24238
[2025-01-10 20:01:41,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:01:41,394] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24238
[2025-01-10 20:01:41,394] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:01:41,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 370/1404]  eta: 0:10:31  lr: 0.000068  min_lr: 0.000001  loss: 3.7115 (3.7803)  loss_scale: 65536.0000 (35859.3208)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0929  max mem: 15572
Epoch: [17]  [ 380/1404]  eta: 0:10:23  lr: 0.000068  min_lr: 0.000001  loss: 3.7728 (3.7840)  loss_scale: 32768.0000 (35778.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0008  max mem: 15572
Epoch: [17]  [ 390/1404]  eta: 0:10:15  lr: 0.000068  min_lr: 0.000001  loss: 3.7728 (3.7839)  loss_scale: 32768.0000 (35701.1969)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0152  max mem: 15572
Epoch: [17]  [ 400/1404]  eta: 0:10:06  lr: 0.000068  min_lr: 0.000001  loss: 3.8849 (3.7892)  loss_scale: 32768.0000 (35628.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0151  max mem: 15572
Epoch: [17]  [ 410/1404]  eta: 0:10:02  lr: 0.000068  min_lr: 0.000001  loss: 3.9091 (3.7907)  loss_scale: 32768.0000 (35558.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0955  max mem: 15572
Epoch: [17]  [ 420/1404]  eta: 0:09:53  lr: 0.000068  min_lr: 0.000001  loss: 3.6713 (3.7854)  loss_scale: 32768.0000 (35492.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0953  max mem: 15572
Epoch: [17]  [ 430/1404]  eta: 0:09:49  lr: 0.000068  min_lr: 0.000001  loss: 3.5263 (3.7839)  loss_scale: 32768.0000 (35428.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0844  max mem: 15572
Epoch: [17]  [ 440/1404]  eta: 0:09:42  lr: 0.000068  min_lr: 0.000001  loss: 3.8777 (3.7888)  loss_scale: 32768.0000 (35368.6349)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0846  max mem: 15572
Epoch: [17]  [ 450/1404]  eta: 0:09:36  lr: 0.000068  min_lr: 0.000001  loss: 4.0023 (3.7905)  loss_scale: 32768.0000 (35310.9712)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0482  max mem: 15572
Epoch: [17]  [ 460/1404]  eta: 0:09:31  lr: 0.000068  min_lr: 0.000001  loss: 3.7471 (3.7870)  loss_scale: 32768.0000 (35255.8091)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1352  max mem: 15572
Epoch: [17]  [ 470/1404]  eta: 0:09:24  lr: 0.000068  min_lr: 0.000001  loss: 3.6726 (3.7845)  loss_scale: 32768.0000 (35202.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1261  max mem: 15572
Epoch: [17]  [ 480/1404]  eta: 0:09:16  lr: 0.000068  min_lr: 0.000001  loss: 3.6402 (3.7826)  loss_scale: 32768.0000 (35152.3659)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0588  max mem: 15572
Epoch: [17]  [ 490/1404]  eta: 0:09:11  lr: 0.000068  min_lr: 0.000001  loss: 3.5237 (3.7797)  loss_scale: 32768.0000 (35103.8045)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1045  max mem: 15572
[2025-01-10 20:02:55,718] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:02:55,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:02:55,771] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:02:55,771] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:02:56,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24368
[2025-01-10 20:02:56,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:02:56,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 500/1404]  eta: 0:09:03  lr: 0.000068  min_lr: 0.000001  loss: 3.4264 (3.7802)  loss_scale: 32768.0000 (35122.5868)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1017  max mem: 15572
[2025-01-10 20:02:56,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24368
[2025-01-10 20:02:56,300] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [17]  [ 510/1404]  eta: 0:08:57  lr: 0.000068  min_lr: 0.000001  loss: 3.5779 (3.7756)  loss_scale: 32768.0000 (35076.5088)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0485  max mem: 15572
Epoch: [17]  [ 520/1404]  eta: 0:08:51  lr: 0.000068  min_lr: 0.000001  loss: 3.5539 (3.7721)  loss_scale: 32768.0000 (35032.1996)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0914  max mem: 15572
Epoch: [17]  [ 530/1404]  eta: 0:08:46  lr: 0.000068  min_lr: 0.000001  loss: 3.6222 (3.7741)  loss_scale: 32768.0000 (34989.5593)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1110  max mem: 15572
Epoch: [17]  [ 540/1404]  eta: 0:08:41  lr: 0.000068  min_lr: 0.000001  loss: 3.8084 (3.7757)  loss_scale: 32768.0000 (34948.4954)  weight_decay: 0.0500 (0.0500)  time: 0.6614  data: 0.1363  max mem: 15572
Epoch: [17]  [ 550/1404]  eta: 0:08:34  lr: 0.000068  min_lr: 0.000001  loss: 3.7863 (3.7752)  loss_scale: 32768.0000 (34908.9220)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0858  max mem: 15572
Epoch: [17]  [ 560/1404]  eta: 0:08:27  lr: 0.000068  min_lr: 0.000001  loss: 3.6294 (3.7698)  loss_scale: 32768.0000 (34870.7594)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0074  max mem: 15572
Epoch: [17]  [ 570/1404]  eta: 0:08:23  lr: 0.000068  min_lr: 0.000001  loss: 3.6310 (3.7714)  loss_scale: 32768.0000 (34833.9335)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0845  max mem: 15572
Epoch: [17]  [ 580/1404]  eta: 0:08:16  lr: 0.000068  min_lr: 0.000001  loss: 3.7313 (3.7651)  loss_scale: 32768.0000 (34798.3752)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0889  max mem: 15572
Epoch: [17]  [ 590/1404]  eta: 0:08:09  lr: 0.000068  min_lr: 0.000001  loss: 3.6902 (3.7653)  loss_scale: 32768.0000 (34764.0203)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0118  max mem: 15572
Epoch: [17]  [ 600/1404]  eta: 0:08:02  lr: 0.000068  min_lr: 0.000001  loss: 3.8605 (3.7632)  loss_scale: 32768.0000 (34730.8087)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0009  max mem: 15572
Epoch: [17]  [ 610/1404]  eta: 0:07:56  lr: 0.000067  min_lr: 0.000001  loss: 3.8706 (3.7657)  loss_scale: 32768.0000 (34698.6841)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0326  max mem: 15572
Epoch: [17]  [ 620/1404]  eta: 0:07:50  lr: 0.000067  min_lr: 0.000001  loss: 3.9420 (3.7673)  loss_scale: 32768.0000 (34667.5942)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0625  max mem: 15572
[2025-01-10 20:04:12,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:04:12,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:04:12,381] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:04:12,381] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 630/1404]  eta: 0:07:43  lr: 0.000067  min_lr: 0.000001  loss: 3.9419 (3.7667)  loss_scale: 32768.0000 (34741.3502)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0395  max mem: 15572
[2025-01-10 20:04:15,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24504
[2025-01-10 20:04:15,927] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:04:15,960] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24504
[2025-01-10 20:04:15,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:04:15,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 640/1404]  eta: 0:07:36  lr: 0.000067  min_lr: 0.000001  loss: 3.8682 (3.7683)  loss_scale: 32768.0000 (34966.1654)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0100  max mem: 15572
Epoch: [17]  [ 650/1404]  eta: 0:07:30  lr: 0.000067  min_lr: 0.000001  loss: 3.8606 (3.7688)  loss_scale: 32768.0000 (34932.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0351  max mem: 15572
Epoch: [17]  [ 660/1404]  eta: 0:07:24  lr: 0.000067  min_lr: 0.000001  loss: 3.7144 (3.7684)  loss_scale: 32768.0000 (34899.6551)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0758  max mem: 15572
Epoch: [17]  [ 670/1404]  eta: 0:07:18  lr: 0.000067  min_lr: 0.000001  loss: 3.7748 (3.7718)  loss_scale: 32768.0000 (34867.8867)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1025  max mem: 15572
Epoch: [17]  [ 680/1404]  eta: 0:07:12  lr: 0.000067  min_lr: 0.000001  loss: 4.0167 (3.7760)  loss_scale: 32768.0000 (34837.0514)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0719  max mem: 15572
Epoch: [17]  [ 690/1404]  eta: 0:07:06  lr: 0.000067  min_lr: 0.000001  loss: 3.9455 (3.7729)  loss_scale: 32768.0000 (34807.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0842  max mem: 15572
Epoch: [17]  [ 700/1404]  eta: 0:07:00  lr: 0.000067  min_lr: 0.000001  loss: 3.8040 (3.7730)  loss_scale: 32768.0000 (34778.0200)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1094  max mem: 15572
Epoch: [17]  [ 710/1404]  eta: 0:06:56  lr: 0.000067  min_lr: 0.000001  loss: 3.9471 (3.7756)  loss_scale: 32768.0000 (34749.7496)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.1194  max mem: 15572
Epoch: [17]  [ 720/1404]  eta: 0:06:49  lr: 0.000067  min_lr: 0.000001  loss: 3.9129 (3.7743)  loss_scale: 32768.0000 (34722.2635)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.0836  max mem: 15572
Epoch: [17]  [ 730/1404]  eta: 0:06:43  lr: 0.000067  min_lr: 0.000001  loss: 3.7491 (3.7744)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0250  max mem: 15572
Epoch: [17]  [ 740/1404]  eta: 0:06:37  lr: 0.000067  min_lr: 0.000001  loss: 3.7046 (3.7722)  loss_scale: 32768.0000 (34669.5169)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0702  max mem: 15572
Epoch: [17]  [ 750/1404]  eta: 0:06:31  lr: 0.000067  min_lr: 0.000001  loss: 3.3719 (3.7692)  loss_scale: 32768.0000 (34644.1971)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1017  max mem: 15572
Epoch: [17]  [ 760/1404]  eta: 0:06:25  lr: 0.000067  min_lr: 0.000001  loss: 4.0898 (3.7733)  loss_scale: 32768.0000 (34619.5427)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1130  max mem: 15572
[2025-01-10 20:05:33,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:05:33,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:05:33,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:05:33,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 770/1404]  eta: 0:06:18  lr: 0.000067  min_lr: 0.000001  loss: 4.0323 (3.7721)  loss_scale: 32768.0000 (34850.5318)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0909  max mem: 15572
[2025-01-10 20:05:37,680] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24642
[2025-01-10 20:05:37,680] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:05:37,710] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24642
[2025-01-10 20:05:37,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:05:37,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 780/1404]  eta: 0:06:13  lr: 0.000067  min_lr: 0.000001  loss: 3.8746 (3.7744)  loss_scale: 32768.0000 (34949.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1206  max mem: 15572
Epoch: [17]  [ 790/1404]  eta: 0:06:07  lr: 0.000067  min_lr: 0.000001  loss: 3.9613 (3.7752)  loss_scale: 32768.0000 (34922.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1444  max mem: 15572
Epoch: [17]  [ 800/1404]  eta: 0:06:02  lr: 0.000067  min_lr: 0.000001  loss: 3.8243 (3.7731)  loss_scale: 32768.0000 (34895.2609)  weight_decay: 0.0500 (0.0500)  time: 0.6799  data: 0.1942  max mem: 15572
Epoch: [17]  [ 810/1404]  eta: 0:05:55  lr: 0.000067  min_lr: 0.000001  loss: 3.7562 (3.7749)  loss_scale: 32768.0000 (34869.0308)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1367  max mem: 15572
Epoch: [17]  [ 820/1404]  eta: 0:05:49  lr: 0.000067  min_lr: 0.000001  loss: 3.8070 (3.7745)  loss_scale: 32768.0000 (34843.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0006  max mem: 15572
Epoch: [17]  [ 830/1404]  eta: 0:05:42  lr: 0.000067  min_lr: 0.000001  loss: 3.9639 (3.7764)  loss_scale: 32768.0000 (34818.4645)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0007  max mem: 15572
Epoch: [17]  [ 840/1404]  eta: 0:05:36  lr: 0.000067  min_lr: 0.000001  loss: 3.8476 (3.7740)  loss_scale: 32768.0000 (34794.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0140  max mem: 15572
Epoch: [17]  [ 850/1404]  eta: 0:05:30  lr: 0.000067  min_lr: 0.000001  loss: 3.8476 (3.7761)  loss_scale: 32768.0000 (34770.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0310  max mem: 15572
Epoch: [17]  [ 860/1404]  eta: 0:05:24  lr: 0.000067  min_lr: 0.000001  loss: 3.9338 (3.7763)  loss_scale: 32768.0000 (34747.0197)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0495  max mem: 15572
Epoch: [17]  [ 870/1404]  eta: 0:05:18  lr: 0.000067  min_lr: 0.000001  loss: 3.8954 (3.7791)  loss_scale: 32768.0000 (34724.2985)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0323  max mem: 15572
Epoch: [17]  [ 880/1404]  eta: 0:05:12  lr: 0.000067  min_lr: 0.000001  loss: 3.9935 (3.7832)  loss_scale: 32768.0000 (34702.0931)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0424  max mem: 15572
Epoch: [17]  [ 890/1404]  eta: 0:05:06  lr: 0.000067  min_lr: 0.000001  loss: 3.9935 (3.7840)  loss_scale: 32768.0000 (34680.3861)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0859  max mem: 15572
Epoch: [17]  [ 900/1404]  eta: 0:05:01  lr: 0.000067  min_lr: 0.000001  loss: 3.9538 (3.7851)  loss_scale: 32768.0000 (34659.1609)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1427  max mem: 15572
[2025-01-10 20:06:55,556] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:06:55,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:06:55,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:06:55,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:06:57,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24774
[2025-01-10 20:06:57,201] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:06:57,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:06:57,227] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24774
[2025-01-10 20:06:57,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [17]  [ 910/1404]  eta: 0:04:55  lr: 0.000067  min_lr: 0.000001  loss: 3.8569 (3.7848)  loss_scale: 32768.0000 (34746.3095)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1391  max mem: 15572
Epoch: [17]  [ 920/1404]  eta: 0:04:49  lr: 0.000067  min_lr: 0.000001  loss: 3.5979 (3.7783)  loss_scale: 32768.0000 (34724.8295)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0852  max mem: 15572
Epoch: [17]  [ 930/1404]  eta: 0:04:43  lr: 0.000067  min_lr: 0.000001  loss: 3.4253 (3.7769)  loss_scale: 32768.0000 (34703.8110)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1069  max mem: 15572
Epoch: [17]  [ 940/1404]  eta: 0:04:37  lr: 0.000067  min_lr: 0.000001  loss: 3.6358 (3.7756)  loss_scale: 32768.0000 (34683.2391)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0662  max mem: 15572
Epoch: [17]  [ 950/1404]  eta: 0:04:31  lr: 0.000067  min_lr: 0.000001  loss: 3.6546 (3.7754)  loss_scale: 32768.0000 (34663.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0971  max mem: 15572
Epoch: [17]  [ 960/1404]  eta: 0:04:25  lr: 0.000067  min_lr: 0.000001  loss: 3.8135 (3.7761)  loss_scale: 32768.0000 (34643.3798)  weight_decay: 0.0500 (0.0500)  time: 0.6672  data: 0.1572  max mem: 15572
Epoch: [17]  [ 970/1404]  eta: 0:04:19  lr: 0.000067  min_lr: 0.000001  loss: 3.8135 (3.7749)  loss_scale: 32768.0000 (34624.0659)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1041  max mem: 15572
Epoch: [17]  [ 980/1404]  eta: 0:04:13  lr: 0.000066  min_lr: 0.000001  loss: 3.7285 (3.7727)  loss_scale: 32768.0000 (34605.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0400  max mem: 15572
Epoch: [17]  [ 990/1404]  eta: 0:04:07  lr: 0.000066  min_lr: 0.000001  loss: 3.7301 (3.7724)  loss_scale: 32768.0000 (34586.6075)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0009  max mem: 15572
Epoch: [17]  [1000/1404]  eta: 0:04:01  lr: 0.000066  min_lr: 0.000001  loss: 3.8136 (3.7723)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0118  max mem: 15572
Epoch: [17]  [1010/1404]  eta: 0:03:55  lr: 0.000066  min_lr: 0.000001  loss: 3.9218 (3.7741)  loss_scale: 32768.0000 (34550.6311)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0832  max mem: 15572
Epoch: [17]  [1020/1404]  eta: 0:03:49  lr: 0.000066  min_lr: 0.000001  loss: 3.9783 (3.7769)  loss_scale: 32768.0000 (34533.1714)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.1380  max mem: 15572
Epoch: [17]  [1030/1404]  eta: 0:03:43  lr: 0.000066  min_lr: 0.000001  loss: 4.0419 (3.7759)  loss_scale: 32768.0000 (34516.0504)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0791  max mem: 15572
[2025-01-10 20:08:13,324] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:08:13,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:08:13,329] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:08:13,329] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:08:13,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24904
[2025-01-10 20:08:13,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:08:13,754] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24904
[2025-01-10 20:08:13,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:08:13,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1040/1404]  eta: 0:03:37  lr: 0.000066  min_lr: 0.000001  loss: 3.7944 (3.7751)  loss_scale: 32768.0000 (34530.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0234  max mem: 15572
Epoch: [17]  [1050/1404]  eta: 0:03:31  lr: 0.000066  min_lr: 0.000001  loss: 3.6706 (3.7730)  loss_scale: 32768.0000 (34513.9638)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0751  max mem: 15572
Epoch: [17]  [1060/1404]  eta: 0:03:25  lr: 0.000066  min_lr: 0.000001  loss: 3.6706 (3.7723)  loss_scale: 32768.0000 (34497.5080)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.1815  max mem: 15572
Epoch: [17]  [1070/1404]  eta: 0:03:19  lr: 0.000066  min_lr: 0.000001  loss: 3.8572 (3.7740)  loss_scale: 32768.0000 (34481.3595)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1174  max mem: 15572
Epoch: [17]  [1080/1404]  eta: 0:03:13  lr: 0.000066  min_lr: 0.000001  loss: 4.0570 (3.7760)  loss_scale: 32768.0000 (34465.5097)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0430  max mem: 15572
Epoch: [17]  [1090/1404]  eta: 0:03:07  lr: 0.000066  min_lr: 0.000001  loss: 3.9640 (3.7783)  loss_scale: 32768.0000 (34449.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0880  max mem: 15572
Epoch: [17]  [1100/1404]  eta: 0:03:01  lr: 0.000066  min_lr: 0.000001  loss: 3.8830 (3.7791)  loss_scale: 32768.0000 (34434.6739)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0821  max mem: 15572
Epoch: [17]  [1110/1404]  eta: 0:02:55  lr: 0.000066  min_lr: 0.000001  loss: 3.9723 (3.7823)  loss_scale: 32768.0000 (34419.6724)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0451  max mem: 15572
Epoch: [17]  [1120/1404]  eta: 0:02:49  lr: 0.000066  min_lr: 0.000001  loss: 3.9422 (3.7813)  loss_scale: 32768.0000 (34404.9384)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0258  max mem: 15572
Epoch: [17]  [1130/1404]  eta: 0:02:43  lr: 0.000066  min_lr: 0.000001  loss: 3.6537 (3.7813)  loss_scale: 32768.0000 (34390.4651)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0288  max mem: 15572
[2025-01-10 20:09:09,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=161, lr=[6.401960125700905e-07, 6.401960125700905e-07, 9.145657322429866e-07, 9.145657322429866e-07, 1.3065224746328381e-06, 1.3065224746328381e-06, 1.8664606780469118e-06, 1.8664606780469118e-06, 2.666372397209874e-06, 2.666372397209874e-06, 3.8091034245855346e-06, 3.8091034245855346e-06, 5.441576320836478e-06, 5.441576320836478e-06, 7.773680458337827e-06, 7.773680458337827e-06, 1.1105257797625467e-05, 1.1105257797625467e-05, 1.586465399660781e-05, 1.586465399660781e-05, 2.2663791423725444e-05, 2.2663791423725444e-05, 3.2376844891036355e-05, 3.2376844891036355e-05, 4.6252635558623366e-05, 4.6252635558623366e-05, 6.607519365517624e-05, 6.607519365517624e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 20:09:09,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=45.50713811459133, CurrSamplesPerSec=42.92533257941417, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [1140/1404]  eta: 0:02:37  lr: 0.000066  min_lr: 0.000001  loss: 4.0221 (3.7826)  loss_scale: 32768.0000 (34376.2454)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0292  max mem: 15572
Epoch: [17]  [1150/1404]  eta: 0:02:31  lr: 0.000066  min_lr: 0.000001  loss: 4.0574 (3.7817)  loss_scale: 32768.0000 (34362.2728)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0364  max mem: 15572
Epoch: [17]  [1160/1404]  eta: 0:02:25  lr: 0.000066  min_lr: 0.000001  loss: 4.0654 (3.7827)  loss_scale: 32768.0000 (34348.5409)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0939  max mem: 15572
[2025-01-10 20:09:29,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:09:29,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:09:29,242] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:09:29,242] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:09:31,412] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25037
[2025-01-10 20:09:31,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:09:31,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25037
[2025-01-10 20:09:31,414] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:09:31,414] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1170/1404]  eta: 0:02:19  lr: 0.000066  min_lr: 0.000001  loss: 3.7500 (3.7812)  loss_scale: 32768.0000 (34446.9752)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0756  max mem: 15572
Epoch: [17]  [1180/1404]  eta: 0:02:13  lr: 0.000066  min_lr: 0.000001  loss: 3.6131 (3.7810)  loss_scale: 32768.0000 (34432.7587)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0078  max mem: 15572
Epoch: [17]  [1190/1404]  eta: 0:02:07  lr: 0.000066  min_lr: 0.000001  loss: 3.7192 (3.7806)  loss_scale: 32768.0000 (34418.7809)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0077  max mem: 15572
Epoch: [17]  [1200/1404]  eta: 0:02:01  lr: 0.000066  min_lr: 0.000001  loss: 3.9368 (3.7822)  loss_scale: 32768.0000 (34405.0358)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0008  max mem: 15572
Epoch: [17]  [1210/1404]  eta: 0:01:55  lr: 0.000066  min_lr: 0.000001  loss: 3.9193 (3.7814)  loss_scale: 32768.0000 (34391.5178)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0008  max mem: 15572
Epoch: [17]  [1220/1404]  eta: 0:01:49  lr: 0.000066  min_lr: 0.000001  loss: 3.7311 (3.7811)  loss_scale: 32768.0000 (34378.2211)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0008  max mem: 15572
Epoch: [17]  [1230/1404]  eta: 0:01:43  lr: 0.000066  min_lr: 0.000001  loss: 3.7311 (3.7812)  loss_scale: 32768.0000 (34365.1405)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0008  max mem: 15572
Epoch: [17]  [1240/1404]  eta: 0:01:37  lr: 0.000066  min_lr: 0.000001  loss: 3.7757 (3.7816)  loss_scale: 32768.0000 (34352.2707)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0008  max mem: 15572
Epoch: [17]  [1250/1404]  eta: 0:01:31  lr: 0.000066  min_lr: 0.000001  loss: 3.6345 (3.7794)  loss_scale: 32768.0000 (34339.6067)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0006  max mem: 15572
Epoch: [17]  [1260/1404]  eta: 0:01:25  lr: 0.000066  min_lr: 0.000001  loss: 3.5838 (3.7782)  loss_scale: 32768.0000 (34327.1435)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0005  max mem: 15572
Epoch: [17]  [1270/1404]  eta: 0:01:19  lr: 0.000066  min_lr: 0.000001  loss: 3.6974 (3.7769)  loss_scale: 32768.0000 (34314.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0005  max mem: 15572
Epoch: [17]  [1280/1404]  eta: 0:01:13  lr: 0.000066  min_lr: 0.000001  loss: 3.7377 (3.7768)  loss_scale: 32768.0000 (34302.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0008  max mem: 15572
Epoch: [17]  [1290/1404]  eta: 0:01:07  lr: 0.000066  min_lr: 0.000001  loss: 3.7169 (3.7762)  loss_scale: 32768.0000 (34290.9125)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0011  max mem: 15572
[2025-01-10 20:10:48,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:10:48,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:10:48,300] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:10:48,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [1300/1404]  eta: 0:01:01  lr: 0.000066  min_lr: 0.000001  loss: 3.6468 (3.7769)  loss_scale: 32768.0000 (34354.7671)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0009  max mem: 15572
[2025-01-10 20:10:51,338] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25170
[2025-01-10 20:10:51,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:10:51,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25170
[2025-01-10 20:10:51,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:10:51,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [1310/1404]  eta: 0:00:56  lr: 0.000066  min_lr: 0.000001  loss: 3.8915 (3.7765)  loss_scale: 32768.0000 (34367.6583)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0005  max mem: 15572
Epoch: [17]  [1320/1404]  eta: 0:00:50  lr: 0.000066  min_lr: 0.000001  loss: 3.6819 (3.7756)  loss_scale: 32768.0000 (34355.5488)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0004  max mem: 15572
Epoch: [17]  [1330/1404]  eta: 0:00:44  lr: 0.000066  min_lr: 0.000001  loss: 3.7247 (3.7770)  loss_scale: 32768.0000 (34343.6213)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0004  max mem: 15572
Epoch: [17]  [1340/1404]  eta: 0:00:38  lr: 0.000066  min_lr: 0.000001  loss: 3.9612 (3.7783)  loss_scale: 32768.0000 (34331.8717)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0006  max mem: 15572
Epoch: [17]  [1350/1404]  eta: 0:00:32  lr: 0.000065  min_lr: 0.000001  loss: 3.7908 (3.7790)  loss_scale: 32768.0000 (34320.2961)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0007  max mem: 15572
Epoch: [17]  [1360/1404]  eta: 0:00:26  lr: 0.000065  min_lr: 0.000001  loss: 3.7908 (3.7802)  loss_scale: 32768.0000 (34308.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0005  max mem: 15572
Epoch: [17]  [1370/1404]  eta: 0:00:20  lr: 0.000065  min_lr: 0.000001  loss: 4.0019 (3.7802)  loss_scale: 32768.0000 (34297.6513)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0009  max mem: 15572
Epoch: [17]  [1380/1404]  eta: 0:00:14  lr: 0.000065  min_lr: 0.000001  loss: 3.7717 (3.7788)  loss_scale: 32768.0000 (34286.5749)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0010  max mem: 15572
Epoch: [17]  [1390/1404]  eta: 0:00:08  lr: 0.000065  min_lr: 0.000001  loss: 3.8538 (3.7786)  loss_scale: 32768.0000 (34275.6578)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0007  max mem: 15572
Epoch: [17]  [1400/1404]  eta: 0:00:02  lr: 0.000065  min_lr: 0.000001  loss: 3.8392 (3.7764)  loss_scale: 32768.0000 (34264.8965)  weight_decay: 0.0500 (0.0500)  time: 0.4307  data: 0.0004  max mem: 15572
Epoch: [17]  [1403/1404]  eta: 0:00:00  lr: 0.000065  min_lr: 0.000001  loss: 3.8265 (3.7760)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)  time: 0.4128  data: 0.0004  max mem: 15572
Epoch: [17] Total time: 0:13:52 (0.5931 s / it)
Averaged stats: lr: 0.000065  min_lr: 0.000001  loss: 3.8265 (3.7811)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:25  loss: 1.3776 (1.3776)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.0371  data: 4.7784  max mem: 15572
Val:  [ 10/136]  eta: 0:01:50  loss: 2.1878 (2.2226)  acc1: 50.0000 (45.4545)  acc5: 77.7778 (77.2727)  time: 0.8785  data: 0.6709  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 2.4609 (2.4202)  acc1: 38.8889 (39.1534)  acc5: 77.7778 (75.3968)  time: 0.3785  data: 0.1656  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.4609 (2.2656)  acc1: 38.8889 (44.0860)  acc5: 77.7778 (77.2401)  time: 0.3081  data: 0.0953  max mem: 15572
Val:  [ 40/136]  eta: 0:00:46  loss: 1.9591 (2.2350)  acc1: 55.5556 (45.2575)  acc5: 83.3333 (77.7778)  time: 0.3528  data: 0.1495  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 2.0244 (2.2099)  acc1: 44.4444 (45.8606)  acc5: 83.3333 (79.3028)  time: 0.3875  data: 0.1824  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.2643 (2.3173)  acc1: 33.3333 (43.3515)  acc5: 77.7778 (77.1403)  time: 0.3944  data: 0.1811  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 2.2887 (2.2983)  acc1: 38.8889 (44.3662)  acc5: 72.2222 (77.2300)  time: 0.4030  data: 0.1886  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 2.0447 (2.2777)  acc1: 50.0000 (45.1303)  acc5: 83.3333 (77.9835)  time: 0.3885  data: 0.1848  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.2082 (2.2835)  acc1: 38.8889 (44.2002)  acc5: 77.7778 (77.6557)  time: 0.3870  data: 0.1693  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.6050 (2.3683)  acc1: 27.7778 (41.8592)  acc5: 66.6667 (75.8526)  time: 0.3189  data: 0.0908  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.6949 (2.3573)  acc1: 33.3333 (42.4424)  acc5: 72.2222 (75.8258)  time: 0.3656  data: 0.1560  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1468 (2.2983)  acc1: 55.5556 (44.0771)  acc5: 77.7778 (76.6759)  time: 0.3445  data: 0.1588  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6708 (2.2523)  acc1: 61.1111 (45.3774)  acc5: 83.3333 (77.0992)  time: 0.1641  data: 0.0089  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.0141 (2.2623)  acc1: 55.5556 (45.1679)  acc5: 77.7778 (76.9861)  time: 0.1498  data: 0.0087  max mem: 15572
Val: Total time: 0:00:50 (0.3737 s / it)
* Acc@1 44.881 Acc@5 75.573 loss 2.309
Accuracy of the network on the 4883 val videos: 44.9%
[2025-01-10 20:12:38,173] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 20:12:38,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 20:12:38,176] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 20:12:38,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 20:12:40,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 20:12:40,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.88%
Epoch: [18]  [   0/1404]  eta: 2:58:55  lr: 0.000065  min_lr: 0.000001  loss: 3.5060 (3.5060)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6464  data: 7.0678  max mem: 15572
Epoch: [18]  [  10/1404]  eta: 0:29:40  lr: 0.000065  min_lr: 0.000001  loss: 3.8574 (3.8228)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2773  data: 0.6431  max mem: 15572
Epoch: [18]  [  20/1404]  eta: 0:21:28  lr: 0.000065  min_lr: 0.000001  loss: 3.8574 (3.8046)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0009  max mem: 15572
[2025-01-10 20:13:03,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:13:03,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:13:03,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:13:03,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:13:06,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25302
[2025-01-10 20:13:06,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:13:06,927] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25302
[2025-01-10 20:13:06,928] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:13:06,930] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [  30/1404]  eta: 0:19:25  lr: 0.000065  min_lr: 0.000001  loss: 3.9103 (3.8084)  loss_scale: 32768.0000 (35939.0968)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0009  max mem: 15572
[2025-01-10 20:13:12,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25312
[2025-01-10 20:13:12,419] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 20:13:12,420] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-10 20:13:12,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25312
[2025-01-10 20:13:12,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [18]  [  40/1404]  eta: 0:17:36  lr: 0.000065  min_lr: 0.000001  loss: 3.7933 (3.7819)  loss_scale: 32768.0000 (34766.0488)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0010  max mem: 15572
Epoch: [18]  [  50/1404]  eta: 0:16:52  lr: 0.000065  min_lr: 0.000001  loss: 3.6571 (3.7676)  loss_scale: 16384.0000 (31161.7255)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0733  max mem: 15572
Epoch: [18]  [  60/1404]  eta: 0:15:47  lr: 0.000065  min_lr: 0.000001  loss: 3.9190 (3.8054)  loss_scale: 16384.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0729  max mem: 15572
Epoch: [18]  [  70/1404]  eta: 0:15:19  lr: 0.000065  min_lr: 0.000001  loss: 3.9190 (3.7941)  loss_scale: 16384.0000 (26998.9859)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0008  max mem: 15572
Epoch: [18]  [  80/1404]  eta: 0:14:49  lr: 0.000065  min_lr: 0.000001  loss: 3.8855 (3.8037)  loss_scale: 16384.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0008  max mem: 15572
Epoch: [18]  [  90/1404]  eta: 0:14:32  lr: 0.000065  min_lr: 0.000001  loss: 3.8881 (3.8111)  loss_scale: 16384.0000 (24666.0220)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0468  max mem: 15572
Epoch: [18]  [ 100/1404]  eta: 0:14:23  lr: 0.000065  min_lr: 0.000001  loss: 3.8364 (3.8008)  loss_scale: 16384.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1119  max mem: 15572
Epoch: [18]  [ 110/1404]  eta: 0:14:00  lr: 0.000065  min_lr: 0.000001  loss: 3.5740 (3.7684)  loss_scale: 16384.0000 (23173.7658)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0737  max mem: 15572
Epoch: [18]  [ 120/1404]  eta: 0:13:50  lr: 0.000065  min_lr: 0.000001  loss: 3.6082 (3.7608)  loss_scale: 16384.0000 (22612.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0791  max mem: 15572
Epoch: [18]  [ 130/1404]  eta: 0:13:44  lr: 0.000065  min_lr: 0.000001  loss: 3.8369 (3.7672)  loss_scale: 16384.0000 (22137.1603)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1448  max mem: 15572
Epoch: [18]  [ 140/1404]  eta: 0:13:36  lr: 0.000065  min_lr: 0.000001  loss: 3.5588 (3.7393)  loss_scale: 16384.0000 (21729.1348)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1226  max mem: 15572
Epoch: [18]  [ 150/1404]  eta: 0:13:24  lr: 0.000065  min_lr: 0.000001  loss: 3.5063 (3.7318)  loss_scale: 16384.0000 (21375.1523)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0889  max mem: 15572
Epoch: [18]  [ 160/1404]  eta: 0:13:13  lr: 0.000065  min_lr: 0.000001  loss: 3.6517 (3.7316)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0851  max mem: 15572
[2025-01-10 20:14:28,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:14:28,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:14:28,662] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 20:14:28,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 170/1404]  eta: 0:13:02  lr: 0.000065  min_lr: 0.000001  loss: 4.0247 (3.7515)  loss_scale: 16384.0000 (20983.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0709  max mem: 15572
Epoch: [18]  [ 180/1404]  eta: 0:12:51  lr: 0.000065  min_lr: 0.000001  loss: 3.8938 (3.7484)  loss_scale: 32768.0000 (21634.1215)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0588  max mem: 15572
Epoch: [18]  [ 190/1404]  eta: 0:12:48  lr: 0.000065  min_lr: 0.000001  loss: 3.7183 (3.7402)  loss_scale: 32768.0000 (22217.0471)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1336  max mem: 15572
Epoch: [18]  [ 200/1404]  eta: 0:12:35  lr: 0.000065  min_lr: 0.000001  loss: 3.7183 (3.7300)  loss_scale: 32768.0000 (22741.9701)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1012  max mem: 15572
Epoch: [18]  [ 210/1404]  eta: 0:12:28  lr: 0.000065  min_lr: 0.000001  loss: 3.8517 (3.7435)  loss_scale: 32768.0000 (23217.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0531  max mem: 15572
Epoch: [18]  [ 220/1404]  eta: 0:12:21  lr: 0.000065  min_lr: 0.000001  loss: 3.9350 (3.7453)  loss_scale: 32768.0000 (23649.3032)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1023  max mem: 15572
Epoch: [18]  [ 230/1404]  eta: 0:12:13  lr: 0.000065  min_lr: 0.000001  loss: 3.5962 (3.7411)  loss_scale: 32768.0000 (24044.0519)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0703  max mem: 15572
Epoch: [18]  [ 240/1404]  eta: 0:12:01  lr: 0.000065  min_lr: 0.000001  loss: 3.7283 (3.7463)  loss_scale: 32768.0000 (24406.0415)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0344  max mem: 15572
Epoch: [18]  [ 250/1404]  eta: 0:11:55  lr: 0.000065  min_lr: 0.000001  loss: 3.6548 (3.7357)  loss_scale: 32768.0000 (24739.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0736  max mem: 15572
Epoch: [18]  [ 260/1404]  eta: 0:11:47  lr: 0.000065  min_lr: 0.000001  loss: 3.3087 (3.7209)  loss_scale: 32768.0000 (25046.8046)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1119  max mem: 15572
Epoch: [18]  [ 270/1404]  eta: 0:11:38  lr: 0.000065  min_lr: 0.000001  loss: 3.4959 (3.7229)  loss_scale: 32768.0000 (25331.7196)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0763  max mem: 15572
Epoch: [18]  [ 280/1404]  eta: 0:11:32  lr: 0.000065  min_lr: 0.000001  loss: 3.9743 (3.7274)  loss_scale: 32768.0000 (25596.3559)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0984  max mem: 15572
Epoch: [18]  [ 290/1404]  eta: 0:11:26  lr: 0.000065  min_lr: 0.000001  loss: 3.8850 (3.7232)  loss_scale: 32768.0000 (25842.8041)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1309  max mem: 15572
[2025-01-10 20:15:44,027] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:15:44,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:15:44,030] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:15:44,031] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 300/1404]  eta: 0:11:18  lr: 0.000064  min_lr: 0.000001  loss: 3.5718 (3.7212)  loss_scale: 32768.0000 (26508.3322)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0900  max mem: 15572
[2025-01-10 20:15:49,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25577
[2025-01-10 20:15:49,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:15:49,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:15:49,514] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25577
[2025-01-10 20:15:49,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [18]  [ 310/1404]  eta: 0:11:13  lr: 0.000064  min_lr: 0.000001  loss: 3.7385 (3.7237)  loss_scale: 32768.0000 (27131.0611)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1029  max mem: 15572
Epoch: [18]  [ 320/1404]  eta: 0:11:05  lr: 0.000064  min_lr: 0.000001  loss: 3.8228 (3.7269)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0701  max mem: 15572
Epoch: [18]  [ 330/1404]  eta: 0:10:56  lr: 0.000064  min_lr: 0.000001  loss: 3.7180 (3.7257)  loss_scale: 32768.0000 (27471.6616)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0189  max mem: 15572
Epoch: [18]  [ 340/1404]  eta: 0:10:50  lr: 0.000064  min_lr: 0.000001  loss: 3.7180 (3.7291)  loss_scale: 32768.0000 (27626.9795)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0815  max mem: 15572
Epoch: [18]  [ 350/1404]  eta: 0:10:46  lr: 0.000064  min_lr: 0.000001  loss: 3.8354 (3.7305)  loss_scale: 32768.0000 (27773.4473)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1269  max mem: 15572
Epoch: [18]  [ 360/1404]  eta: 0:10:39  lr: 0.000064  min_lr: 0.000001  loss: 3.9617 (3.7344)  loss_scale: 32768.0000 (27911.8006)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0991  max mem: 15572
Epoch: [18]  [ 370/1404]  eta: 0:10:30  lr: 0.000064  min_lr: 0.000001  loss: 3.8783 (3.7310)  loss_scale: 32768.0000 (28042.6954)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0353  max mem: 15572
Epoch: [18]  [ 380/1404]  eta: 0:10:23  lr: 0.000064  min_lr: 0.000001  loss: 3.7111 (3.7311)  loss_scale: 32768.0000 (28166.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0336  max mem: 15572
Epoch: [18]  [ 390/1404]  eta: 0:10:15  lr: 0.000064  min_lr: 0.000001  loss: 3.7730 (3.7291)  loss_scale: 32768.0000 (28284.3990)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0469  max mem: 15572
Epoch: [18]  [ 400/1404]  eta: 0:10:07  lr: 0.000064  min_lr: 0.000001  loss: 3.7470 (3.7242)  loss_scale: 32768.0000 (28396.2095)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0141  max mem: 15572
Epoch: [18]  [ 410/1404]  eta: 0:10:02  lr: 0.000064  min_lr: 0.000001  loss: 3.6496 (3.7256)  loss_scale: 32768.0000 (28502.5791)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0537  max mem: 15572
Epoch: [18]  [ 420/1404]  eta: 0:09:55  lr: 0.000064  min_lr: 0.000001  loss: 3.5520 (3.7205)  loss_scale: 32768.0000 (28603.8955)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0684  max mem: 15572
Epoch: [18]  [ 430/1404]  eta: 0:09:47  lr: 0.000064  min_lr: 0.000001  loss: 3.4019 (3.7171)  loss_scale: 32768.0000 (28700.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0157  max mem: 15572
[2025-01-10 20:17:03,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:17:03,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:17:03,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:17:03,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 440/1404]  eta: 0:09:40  lr: 0.000064  min_lr: 0.000001  loss: 3.9368 (3.7269)  loss_scale: 32768.0000 (29312.8707)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0009  max mem: 15572
[2025-01-10 20:17:11,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25720
[2025-01-10 20:17:11,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25720
[2025-01-10 20:17:11,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:17:11,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:17:11,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 450/1404]  eta: 0:09:36  lr: 0.000064  min_lr: 0.000001  loss: 3.9397 (3.7267)  loss_scale: 65536.0000 (29898.0754)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0560  max mem: 15572
Epoch: [18]  [ 460/1404]  eta: 0:09:30  lr: 0.000064  min_lr: 0.000001  loss: 3.8462 (3.7286)  loss_scale: 32768.0000 (29960.3297)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.0917  max mem: 15572
Epoch: [18]  [ 470/1404]  eta: 0:09:24  lr: 0.000064  min_lr: 0.000001  loss: 3.8919 (3.7341)  loss_scale: 32768.0000 (30019.9406)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0780  max mem: 15572
Epoch: [18]  [ 480/1404]  eta: 0:09:16  lr: 0.000064  min_lr: 0.000001  loss: 3.8074 (3.7318)  loss_scale: 32768.0000 (30077.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0425  max mem: 15572
Epoch: [18]  [ 490/1404]  eta: 0:09:10  lr: 0.000064  min_lr: 0.000001  loss: 3.6223 (3.7328)  loss_scale: 32768.0000 (30131.8778)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0403  max mem: 15572
Epoch: [18]  [ 500/1404]  eta: 0:09:04  lr: 0.000064  min_lr: 0.000001  loss: 3.6195 (3.7283)  loss_scale: 32768.0000 (30184.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0883  max mem: 15572
Epoch: [18]  [ 510/1404]  eta: 0:09:00  lr: 0.000064  min_lr: 0.000001  loss: 3.6195 (3.7284)  loss_scale: 32768.0000 (30235.0528)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.1368  max mem: 15572
Epoch: [18]  [ 520/1404]  eta: 0:08:54  lr: 0.000064  min_lr: 0.000001  loss: 3.6576 (3.7285)  loss_scale: 32768.0000 (30283.6699)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1206  max mem: 15572
Epoch: [18]  [ 530/1404]  eta: 0:08:46  lr: 0.000064  min_lr: 0.000001  loss: 3.7217 (3.7300)  loss_scale: 32768.0000 (30330.4557)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0444  max mem: 15572
Epoch: [18]  [ 540/1404]  eta: 0:08:40  lr: 0.000064  min_lr: 0.000001  loss: 3.7929 (3.7292)  loss_scale: 32768.0000 (30375.5120)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0746  max mem: 15572
Epoch: [18]  [ 550/1404]  eta: 0:08:34  lr: 0.000064  min_lr: 0.000001  loss: 3.5863 (3.7257)  loss_scale: 32768.0000 (30418.9328)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1011  max mem: 15572
Epoch: [18]  [ 560/1404]  eta: 0:08:27  lr: 0.000064  min_lr: 0.000001  loss: 3.6153 (3.7260)  loss_scale: 32768.0000 (30460.8057)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0739  max mem: 15572
Epoch: [18]  [ 570/1404]  eta: 0:08:23  lr: 0.000064  min_lr: 0.000001  loss: 3.8061 (3.7257)  loss_scale: 32768.0000 (30501.2119)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.1728  max mem: 15572
[2025-01-10 20:18:29,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:18:29,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:18:29,291] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:18:29,291] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:18:30,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25851
[2025-01-10 20:18:30,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25851
[2025-01-10 20:18:30,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:18:30,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:18:30,282] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 580/1404]  eta: 0:08:16  lr: 0.000064  min_lr: 0.000001  loss: 3.7894 (3.7266)  loss_scale: 32768.0000 (30653.0258)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1377  max mem: 15572
Epoch: [18]  [ 590/1404]  eta: 0:08:11  lr: 0.000064  min_lr: 0.000001  loss: 3.7894 (3.7271)  loss_scale: 32768.0000 (30688.8122)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0719  max mem: 15572
Epoch: [18]  [ 600/1404]  eta: 0:08:03  lr: 0.000064  min_lr: 0.000001  loss: 3.9557 (3.7300)  loss_scale: 32768.0000 (30723.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0860  max mem: 15572
Epoch: [18]  [ 610/1404]  eta: 0:07:57  lr: 0.000064  min_lr: 0.000001  loss: 3.7795 (3.7280)  loss_scale: 32768.0000 (30756.8707)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0493  max mem: 15572
Epoch: [18]  [ 620/1404]  eta: 0:07:50  lr: 0.000064  min_lr: 0.000001  loss: 3.6137 (3.7222)  loss_scale: 32768.0000 (30789.2560)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0383  max mem: 15572
Epoch: [18]  [ 630/1404]  eta: 0:07:43  lr: 0.000064  min_lr: 0.000001  loss: 3.3829 (3.7209)  loss_scale: 32768.0000 (30820.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0237  max mem: 15572
Epoch: [18]  [ 640/1404]  eta: 0:07:39  lr: 0.000064  min_lr: 0.000001  loss: 3.7281 (3.7213)  loss_scale: 32768.0000 (30850.9953)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1131  max mem: 15572
Epoch: [18]  [ 650/1404]  eta: 0:07:32  lr: 0.000064  min_lr: 0.000001  loss: 3.7281 (3.7201)  loss_scale: 32768.0000 (30880.4424)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1206  max mem: 15572
Epoch: [18]  [ 660/1404]  eta: 0:07:26  lr: 0.000063  min_lr: 0.000001  loss: 3.7181 (3.7197)  loss_scale: 32768.0000 (30908.9985)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0515  max mem: 15572
Epoch: [18]  [ 670/1404]  eta: 0:07:20  lr: 0.000063  min_lr: 0.000001  loss: 3.9650 (3.7237)  loss_scale: 32768.0000 (30936.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0860  max mem: 15572
Epoch: [18]  [ 680/1404]  eta: 0:07:13  lr: 0.000063  min_lr: 0.000001  loss: 3.7683 (3.7210)  loss_scale: 32768.0000 (30963.5947)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0835  max mem: 15572
Epoch: [18]  [ 690/1404]  eta: 0:07:08  lr: 0.000063  min_lr: 0.000001  loss: 3.7415 (3.7224)  loss_scale: 32768.0000 (30989.7077)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1163  max mem: 15572
Epoch: [18]  [ 700/1404]  eta: 0:07:01  lr: 0.000063  min_lr: 0.000001  loss: 3.8572 (3.7211)  loss_scale: 32768.0000 (31015.0756)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0956  max mem: 15572
[2025-01-10 20:19:45,516] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:19:45,517] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:19:45,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:19:45,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 710/1404]  eta: 0:06:55  lr: 0.000063  min_lr: 0.000001  loss: 3.6470 (3.7172)  loss_scale: 32768.0000 (31177.9916)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0010  max mem: 15572
Epoch: [18]  [ 720/1404]  eta: 0:06:49  lr: 0.000063  min_lr: 0.000001  loss: 3.6470 (3.7183)  loss_scale: 65536.0000 (31654.5243)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0607  max mem: 15572
[2025-01-10 20:19:54,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25994
[2025-01-10 20:19:54,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:19:54,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:19:54,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25994
[2025-01-10 20:19:54,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:19:56,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=169, lr=[6.133656265189632e-07, 6.133656265189632e-07, 8.762366093128046e-07, 8.762366093128046e-07, 1.2517665847325782e-06, 1.2517665847325782e-06, 1.7882379781893974e-06, 1.7882379781893974e-06, 2.5546256831277107e-06, 2.5546256831277107e-06, 3.6494652616110157e-06, 3.6494652616110157e-06, 5.213521802301451e-06, 5.213521802301451e-06, 7.447888289002074e-06, 7.447888289002074e-06, 1.0639840412860105e-05, 1.0639840412860105e-05, 1.5199772018371582e-05, 1.5199772018371582e-05, 2.1713960026245116e-05, 2.1713960026245116e-05, 3.101994289463588e-05, 3.101994289463588e-05, 4.431420413519412e-05, 4.431420413519412e-05, 6.330600590742018e-05, 6.330600590742018e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 20:19:56,765] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=45.584596944731345, CurrSamplesPerSec=43.08045372940147, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [ 730/1404]  eta: 0:06:43  lr: 0.000063  min_lr: 0.000001  loss: 3.5357 (3.7132)  loss_scale: 65536.0000 (31714.5828)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0760  max mem: 15572
Epoch: [18]  [ 740/1404]  eta: 0:06:37  lr: 0.000063  min_lr: 0.000001  loss: 3.7567 (3.7179)  loss_scale: 32768.0000 (31728.7989)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0162  max mem: 15572
Epoch: [18]  [ 750/1404]  eta: 0:06:30  lr: 0.000063  min_lr: 0.000001  loss: 3.8470 (3.7156)  loss_scale: 32768.0000 (31742.6365)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.0007  max mem: 15572
Epoch: [18]  [ 760/1404]  eta: 0:06:25  lr: 0.000063  min_lr: 0.000001  loss: 3.5991 (3.7165)  loss_scale: 32768.0000 (31756.1104)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0231  max mem: 15572
Epoch: [18]  [ 770/1404]  eta: 0:06:19  lr: 0.000063  min_lr: 0.000001  loss: 3.7209 (3.7153)  loss_scale: 32768.0000 (31769.2348)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.0771  max mem: 15572
Epoch: [18]  [ 780/1404]  eta: 0:06:13  lr: 0.000063  min_lr: 0.000001  loss: 3.6739 (3.7146)  loss_scale: 32768.0000 (31782.0230)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0546  max mem: 15572
Epoch: [18]  [ 790/1404]  eta: 0:06:06  lr: 0.000063  min_lr: 0.000001  loss: 3.7927 (3.7159)  loss_scale: 32768.0000 (31794.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0091  max mem: 15572
Epoch: [18]  [ 800/1404]  eta: 0:06:01  lr: 0.000063  min_lr: 0.000001  loss: 3.7927 (3.7161)  loss_scale: 32768.0000 (31806.6417)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0870  max mem: 15572
Epoch: [18]  [ 810/1404]  eta: 0:05:55  lr: 0.000063  min_lr: 0.000001  loss: 3.6466 (3.7154)  loss_scale: 32768.0000 (31818.4957)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1224  max mem: 15572
Epoch: [18]  [ 820/1404]  eta: 0:05:50  lr: 0.000063  min_lr: 0.000001  loss: 3.6466 (3.7167)  loss_scale: 32768.0000 (31830.0609)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1098  max mem: 15572
Epoch: [18]  [ 830/1404]  eta: 0:05:43  lr: 0.000063  min_lr: 0.000001  loss: 3.8430 (3.7174)  loss_scale: 32768.0000 (31841.3478)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0940  max mem: 15572
Epoch: [18]  [ 840/1404]  eta: 0:05:37  lr: 0.000063  min_lr: 0.000001  loss: 3.7999 (3.7175)  loss_scale: 32768.0000 (31852.3662)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0506  max mem: 15572
Epoch: [18]  [ 850/1404]  eta: 0:05:31  lr: 0.000063  min_lr: 0.000001  loss: 3.6149 (3.7172)  loss_scale: 32768.0000 (31863.1257)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0935  max mem: 15572
[2025-01-10 20:21:10,566] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:21:10,566] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:21:10,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:21:10,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:21:11,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26124
[2025-01-10 20:21:11,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:21:11,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:21:11,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26124
[2025-01-10 20:21:11,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [18]  [ 860/1404]  eta: 0:05:25  lr: 0.000063  min_lr: 0.000001  loss: 3.6816 (3.7177)  loss_scale: 32768.0000 (31911.6934)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1147  max mem: 15572
Epoch: [18]  [ 870/1404]  eta: 0:05:19  lr: 0.000063  min_lr: 0.000001  loss: 3.6940 (3.7165)  loss_scale: 32768.0000 (31921.5247)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1122  max mem: 15572
Epoch: [18]  [ 880/1404]  eta: 0:05:13  lr: 0.000063  min_lr: 0.000001  loss: 3.7801 (3.7174)  loss_scale: 32768.0000 (31931.1328)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1185  max mem: 15572
Epoch: [18]  [ 890/1404]  eta: 0:05:07  lr: 0.000063  min_lr: 0.000001  loss: 3.7086 (3.7153)  loss_scale: 32768.0000 (31940.5253)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0599  max mem: 15572
Epoch: [18]  [ 900/1404]  eta: 0:05:00  lr: 0.000063  min_lr: 0.000001  loss: 3.7854 (3.7186)  loss_scale: 32768.0000 (31949.7092)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0105  max mem: 15572
Epoch: [18]  [ 910/1404]  eta: 0:04:54  lr: 0.000063  min_lr: 0.000001  loss: 3.9126 (3.7192)  loss_scale: 32768.0000 (31958.6915)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0328  max mem: 15572
Epoch: [18]  [ 920/1404]  eta: 0:04:48  lr: 0.000063  min_lr: 0.000001  loss: 3.8662 (3.7212)  loss_scale: 32768.0000 (31967.4788)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0809  max mem: 15572
Epoch: [18]  [ 930/1404]  eta: 0:04:42  lr: 0.000063  min_lr: 0.000001  loss: 4.0128 (3.7230)  loss_scale: 32768.0000 (31976.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1179  max mem: 15572
Epoch: [18]  [ 940/1404]  eta: 0:04:36  lr: 0.000063  min_lr: 0.000001  loss: 3.7129 (3.7230)  loss_scale: 32768.0000 (31984.4931)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0738  max mem: 15572
Epoch: [18]  [ 950/1404]  eta: 0:04:30  lr: 0.000063  min_lr: 0.000001  loss: 3.8810 (3.7249)  loss_scale: 32768.0000 (31992.7319)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0329  max mem: 15572
Epoch: [18]  [ 960/1404]  eta: 0:04:25  lr: 0.000063  min_lr: 0.000001  loss: 3.9390 (3.7239)  loss_scale: 32768.0000 (32000.7992)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1396  max mem: 15572
Epoch: [18]  [ 970/1404]  eta: 0:04:19  lr: 0.000063  min_lr: 0.000001  loss: 3.7545 (3.7237)  loss_scale: 32768.0000 (32008.7003)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1192  max mem: 15572
Epoch: [18]  [ 980/1404]  eta: 0:04:13  lr: 0.000063  min_lr: 0.000001  loss: 3.8296 (3.7245)  loss_scale: 32768.0000 (32016.4404)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1019  max mem: 15572
[2025-01-10 20:22:28,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:22:28,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:22:28,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:22:28,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:22:30,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26258
[2025-01-10 20:22:30,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:22:30,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:22:30,695] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26258
[2025-01-10 20:22:30,695] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [18]  [ 990/1404]  eta: 0:04:07  lr: 0.000063  min_lr: 0.000001  loss: 3.9631 (3.7272)  loss_scale: 32768.0000 (32189.3522)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0966  max mem: 15572
Epoch: [18]  [1000/1404]  eta: 0:04:00  lr: 0.000063  min_lr: 0.000001  loss: 3.8364 (3.7276)  loss_scale: 32768.0000 (32195.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0032  max mem: 15572
Epoch: [18]  [1010/1404]  eta: 0:03:54  lr: 0.000063  min_lr: 0.000001  loss: 3.7930 (3.7298)  loss_scale: 32768.0000 (32200.7992)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0010  max mem: 15572
Epoch: [18]  [1020/1404]  eta: 0:03:48  lr: 0.000062  min_lr: 0.000001  loss: 3.8989 (3.7326)  loss_scale: 32768.0000 (32206.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0552  max mem: 15572
Epoch: [18]  [1030/1404]  eta: 0:03:42  lr: 0.000062  min_lr: 0.000001  loss: 3.8681 (3.7329)  loss_scale: 32768.0000 (32211.8021)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0548  max mem: 15572
[2025-01-10 20:22:59,368] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26308
[2025-01-10 20:22:59,368] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 20:22:59,368] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26308
[2025-01-10 20:22:59,369] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 20:22:59,369] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [1040/1404]  eta: 0:03:37  lr: 0.000062  min_lr: 0.000001  loss: 3.6742 (3.7303)  loss_scale: 32768.0000 (32138.4515)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0922  max mem: 15572
Epoch: [18]  [1050/1404]  eta: 0:03:30  lr: 0.000062  min_lr: 0.000001  loss: 3.7657 (3.7316)  loss_scale: 16384.0000 (31988.5519)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0922  max mem: 15572
Epoch: [18]  [1060/1404]  eta: 0:03:24  lr: 0.000062  min_lr: 0.000001  loss: 3.8083 (3.7329)  loss_scale: 16384.0000 (31841.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0006  max mem: 15572
Epoch: [18]  [1070/1404]  eta: 0:03:18  lr: 0.000062  min_lr: 0.000001  loss: 3.6959 (3.7327)  loss_scale: 16384.0000 (31697.1503)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0007  max mem: 15572
Epoch: [18]  [1080/1404]  eta: 0:03:12  lr: 0.000062  min_lr: 0.000001  loss: 3.6959 (3.7323)  loss_scale: 16384.0000 (31555.4931)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0007  max mem: 15572
Epoch: [18]  [1090/1404]  eta: 0:03:06  lr: 0.000062  min_lr: 0.000001  loss: 3.9061 (3.7350)  loss_scale: 16384.0000 (31416.4326)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0008  max mem: 15572
Epoch: [18]  [1100/1404]  eta: 0:03:00  lr: 0.000062  min_lr: 0.000001  loss: 3.8822 (3.7344)  loss_scale: 16384.0000 (31279.8983)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0324  max mem: 15572
Epoch: [18]  [1110/1404]  eta: 0:02:54  lr: 0.000062  min_lr: 0.000001  loss: 3.8822 (3.7361)  loss_scale: 16384.0000 (31145.8218)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0750  max mem: 15572
Epoch: [18]  [1120/1404]  eta: 0:02:48  lr: 0.000062  min_lr: 0.000001  loss: 3.8657 (3.7366)  loss_scale: 16384.0000 (31014.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0539  max mem: 15572
Epoch: [18]  [1130/1404]  eta: 0:02:42  lr: 0.000062  min_lr: 0.000001  loss: 3.8347 (3.7363)  loss_scale: 16384.0000 (30884.7816)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0552  max mem: 15572
Epoch: [18]  [1140/1404]  eta: 0:02:36  lr: 0.000062  min_lr: 0.000001  loss: 3.8665 (3.7367)  loss_scale: 16384.0000 (30757.6933)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1023  max mem: 15572
Epoch: [18]  [1150/1404]  eta: 0:02:30  lr: 0.000062  min_lr: 0.000001  loss: 3.8825 (3.7376)  loss_scale: 16384.0000 (30632.8132)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0964  max mem: 15572
Epoch: [18]  [1160/1404]  eta: 0:02:24  lr: 0.000062  min_lr: 0.000001  loss: 3.9147 (3.7388)  loss_scale: 16384.0000 (30510.0844)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0478  max mem: 15572
[2025-01-10 20:24:14,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:24:14,401] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 20:24:14,414] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:24:14,414] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [1170/1404]  eta: 0:02:19  lr: 0.000062  min_lr: 0.000001  loss: 3.7939 (3.7380)  loss_scale: 16384.0000 (30473.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0727  max mem: 15572
Epoch: [18]  [1180/1404]  eta: 0:02:13  lr: 0.000062  min_lr: 0.000001  loss: 3.6906 (3.7379)  loss_scale: 32768.0000 (30492.8298)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.0956  max mem: 15572
Epoch: [18]  [1190/1404]  eta: 0:02:07  lr: 0.000062  min_lr: 0.000001  loss: 3.7511 (3.7389)  loss_scale: 32768.0000 (30511.9328)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0328  max mem: 15572
Epoch: [18]  [1200/1404]  eta: 0:02:01  lr: 0.000062  min_lr: 0.000001  loss: 3.5901 (3.7373)  loss_scale: 32768.0000 (30530.7177)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0338  max mem: 15572
Epoch: [18]  [1210/1404]  eta: 0:01:58  lr: 0.000062  min_lr: 0.000001  loss: 3.5560 (3.7351)  loss_scale: 32768.0000 (30549.1924)  weight_decay: 0.0500 (0.0500)  time: 1.6241  data: 1.0880  max mem: 15572
Epoch: [18]  [1220/1404]  eta: 0:01:52  lr: 0.000062  min_lr: 0.000001  loss: 3.7225 (3.7366)  loss_scale: 32768.0000 (30567.3645)  weight_decay: 0.0500 (0.0500)  time: 1.5408  data: 1.0551  max mem: 15572
Epoch: [18]  [1230/1404]  eta: 0:01:45  lr: 0.000062  min_lr: 0.000001  loss: 3.7595 (3.7361)  loss_scale: 32768.0000 (30585.2413)  weight_decay: 0.0500 (0.0500)  time: 0.4636  data: 0.0006  max mem: 15572
Epoch: [18]  [1240/1404]  eta: 0:01:39  lr: 0.000062  min_lr: 0.000001  loss: 3.5620 (3.7350)  loss_scale: 32768.0000 (30602.8300)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0006  max mem: 15572
Epoch: [18]  [1250/1404]  eta: 0:01:33  lr: 0.000062  min_lr: 0.000001  loss: 3.6920 (3.7345)  loss_scale: 32768.0000 (30620.1375)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0079  max mem: 15572
Epoch: [18]  [1260/1404]  eta: 0:01:27  lr: 0.000062  min_lr: 0.000001  loss: 3.6920 (3.7334)  loss_scale: 32768.0000 (30637.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0149  max mem: 15572
Epoch: [18]  [1270/1404]  eta: 0:01:21  lr: 0.000062  min_lr: 0.000001  loss: 3.9236 (3.7364)  loss_scale: 32768.0000 (30653.9355)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0773  max mem: 15572
Epoch: [18]  [1280/1404]  eta: 0:01:15  lr: 0.000062  min_lr: 0.000001  loss: 4.0048 (3.7380)  loss_scale: 32768.0000 (30670.4387)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0849  max mem: 15572
Epoch: [18]  [1290/1404]  eta: 0:01:09  lr: 0.000062  min_lr: 0.000001  loss: 3.8861 (3.7391)  loss_scale: 32768.0000 (30686.6863)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0154  max mem: 15572
[2025-01-10 20:25:46,558] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:25:46,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:25:46,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:25:46,566] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:25:48,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26569
[2025-01-10 20:25:48,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:25:48,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:25:48,816] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26569
[2025-01-10 20:25:48,817] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [18]  [1300/1404]  eta: 0:01:03  lr: 0.000062  min_lr: 0.000001  loss: 3.8789 (3.7399)  loss_scale: 32768.0000 (30803.4312)  weight_decay: 0.0500 (0.0500)  time: 0.7279  data: 0.0007  max mem: 15572
Epoch: [18]  [1310/1404]  eta: 0:00:57  lr: 0.000062  min_lr: 0.000001  loss: 4.0304 (3.7410)  loss_scale: 32768.0000 (30818.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.0008  max mem: 15572
Epoch: [18]  [1320/1404]  eta: 0:00:51  lr: 0.000062  min_lr: 0.000001  loss: 4.0304 (3.7415)  loss_scale: 32768.0000 (30833.1749)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0011  max mem: 15572
Epoch: [18]  [1330/1404]  eta: 0:00:45  lr: 0.000062  min_lr: 0.000001  loss: 3.7700 (3.7406)  loss_scale: 32768.0000 (30847.7115)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0010  max mem: 15572
Epoch: [18]  [1340/1404]  eta: 0:00:38  lr: 0.000062  min_lr: 0.000001  loss: 3.5965 (3.7393)  loss_scale: 32768.0000 (30862.0313)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0008  max mem: 15572
Epoch: [18]  [1350/1404]  eta: 0:00:32  lr: 0.000062  min_lr: 0.000001  loss: 3.4058 (3.7372)  loss_scale: 32768.0000 (30876.1392)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0010  max mem: 15572
Epoch: [18]  [1360/1404]  eta: 0:00:26  lr: 0.000062  min_lr: 0.000001  loss: 3.5672 (3.7379)  loss_scale: 32768.0000 (30890.0397)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0008  max mem: 15572
Epoch: [18]  [1370/1404]  eta: 0:00:20  lr: 0.000061  min_lr: 0.000001  loss: 3.5672 (3.7366)  loss_scale: 32768.0000 (30903.7374)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0006  max mem: 15572
Epoch: [18]  [1380/1404]  eta: 0:00:14  lr: 0.000061  min_lr: 0.000001  loss: 3.7307 (3.7370)  loss_scale: 32768.0000 (30917.2368)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0011  max mem: 15572
Epoch: [18]  [1390/1404]  eta: 0:00:08  lr: 0.000061  min_lr: 0.000001  loss: 3.8078 (3.7377)  loss_scale: 32768.0000 (30930.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0011  max mem: 15572
Epoch: [18]  [1400/1404]  eta: 0:00:02  lr: 0.000061  min_lr: 0.000001  loss: 3.9821 (3.7390)  loss_scale: 32768.0000 (30943.6574)  weight_decay: 0.0500 (0.0500)  time: 0.4340  data: 0.0004  max mem: 15572
Epoch: [18]  [1403/1404]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000001  loss: 3.8913 (3.7395)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  time: 0.4092  data: 0.0004  max mem: 15572
Epoch: [18] Total time: 0:14:09 (0.6052 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000001  loss: 3.8913 (3.7551)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:01  loss: 1.5455 (1.5455)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 6.6283  data: 6.4376  max mem: 15572
Val:  [ 10/136]  eta: 0:01:52  loss: 2.2413 (2.2025)  acc1: 55.5556 (48.9899)  acc5: 77.7778 (76.7677)  time: 0.8956  data: 0.6606  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.4737 (2.3808)  acc1: 44.4444 (42.8571)  acc5: 72.2222 (75.6614)  time: 0.2741  data: 0.0419  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.3113 (2.2086)  acc1: 44.4444 (47.3118)  acc5: 77.7778 (77.2401)  time: 0.2546  data: 0.0439  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.8527 (2.1676)  acc1: 55.5556 (48.6450)  acc5: 83.3333 (78.3198)  time: 0.3367  data: 0.1464  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.2399 (2.2247)  acc1: 38.8889 (46.7320)  acc5: 83.3333 (77.9956)  time: 0.3675  data: 0.1782  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.4454 (2.2967)  acc1: 33.3333 (44.5355)  acc5: 77.7778 (77.7778)  time: 0.3637  data: 0.1636  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.2538 (2.2631)  acc1: 44.4444 (45.6182)  acc5: 77.7778 (77.6995)  time: 0.3832  data: 0.1652  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.0712 (2.2638)  acc1: 50.0000 (45.5418)  acc5: 77.7778 (77.7778)  time: 0.3650  data: 0.1377  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.3330 (2.2869)  acc1: 33.3333 (44.2002)  acc5: 77.7778 (77.4115)  time: 0.3493  data: 0.1332  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5225 (2.3531)  acc1: 27.7778 (42.5743)  acc5: 66.6667 (75.5776)  time: 0.3620  data: 0.1382  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.5148 (2.3524)  acc1: 38.8889 (42.9930)  acc5: 66.6667 (75.2753)  time: 0.3447  data: 0.1217  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.2000 (2.2878)  acc1: 55.5556 (44.7199)  acc5: 83.3333 (76.4463)  time: 0.3583  data: 0.1538  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.4983 (2.2452)  acc1: 66.6667 (45.7167)  acc5: 83.3333 (76.8448)  time: 0.2855  data: 0.1025  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8972 (2.2508)  acc1: 50.0000 (45.7002)  acc5: 83.3333 (76.6585)  time: 0.1719  data: 0.0047  max mem: 15572
Val: Total time: 0:00:50 (0.3707 s / it)
* Acc@1 45.066 Acc@5 75.799 loss 2.299
Accuracy of the network on the 4883 val videos: 45.1%
[2025-01-10 20:27:41,171] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 20:27:41,173] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 20:27:41,173] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 20:27:41,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 20:27:43,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 20:27:43,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.07%
Epoch: [19]  [   0/1404]  eta: 3:16:47  lr: 0.000061  min_lr: 0.000001  loss: 3.7410 (3.7410)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4101  data: 7.9113  max mem: 15572
Epoch: [19]  [  10/1404]  eta: 0:28:06  lr: 0.000061  min_lr: 0.000001  loss: 3.9904 (3.7440)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2101  data: 0.7198  max mem: 15572
Epoch: [19]  [  20/1404]  eta: 0:20:42  lr: 0.000061  min_lr: 0.000001  loss: 3.9904 (3.8042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0146  max mem: 15572
[2025-01-10 20:28:03,703] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:28:03,704] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:28:03,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:28:03,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:28:05,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26700
[2025-01-10 20:28:05,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:28:05,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26700
[2025-01-10 20:28:05,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:28:05,111] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [  30/1404]  eta: 0:18:07  lr: 0.000061  min_lr: 0.000001  loss: 3.7812 (3.8012)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0146  max mem: 15572
Epoch: [19]  [  40/1404]  eta: 0:17:00  lr: 0.000061  min_lr: 0.000001  loss: 3.6330 (3.7606)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0196  max mem: 15572
Epoch: [19]  [  50/1404]  eta: 0:16:20  lr: 0.000061  min_lr: 0.000001  loss: 3.6668 (3.7840)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0197  max mem: 15572
Epoch: [19]  [  60/1404]  eta: 0:15:24  lr: 0.000061  min_lr: 0.000001  loss: 3.7754 (3.7605)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0008  max mem: 15572
Epoch: [19]  [  70/1404]  eta: 0:15:10  lr: 0.000061  min_lr: 0.000001  loss: 3.6900 (3.7568)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0195  max mem: 15572
Epoch: [19]  [  80/1404]  eta: 0:15:01  lr: 0.000061  min_lr: 0.000001  loss: 3.6349 (3.7257)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.1186  max mem: 15572
Epoch: [19]  [  90/1404]  eta: 0:14:25  lr: 0.000061  min_lr: 0.000001  loss: 3.5753 (3.7068)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0998  max mem: 15572
Epoch: [19]  [ 100/1404]  eta: 0:14:09  lr: 0.000061  min_lr: 0.000001  loss: 3.6861 (3.7200)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0317  max mem: 15572
Epoch: [19]  [ 110/1404]  eta: 0:14:02  lr: 0.000061  min_lr: 0.000001  loss: 3.9304 (3.7417)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0970  max mem: 15572
Epoch: [19]  [ 120/1404]  eta: 0:13:43  lr: 0.000061  min_lr: 0.000001  loss: 3.9450 (3.7431)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0664  max mem: 15572
Epoch: [19]  [ 130/1404]  eta: 0:13:33  lr: 0.000061  min_lr: 0.000001  loss: 3.8169 (3.7452)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0611  max mem: 15572
Epoch: [19]  [ 140/1404]  eta: 0:13:24  lr: 0.000061  min_lr: 0.000001  loss: 3.7998 (3.7296)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1107  max mem: 15572
Epoch: [19]  [ 150/1404]  eta: 0:13:17  lr: 0.000061  min_lr: 0.000001  loss: 3.8236 (3.7462)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0708  max mem: 15572
[2025-01-10 20:29:21,115] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:29:21,115] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:29:21,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:29:21,144] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:29:22,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26830
[2025-01-10 20:29:22,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:29:22,653] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26830
[2025-01-10 20:29:22,654] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:29:22,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 160/1404]  eta: 0:13:09  lr: 0.000061  min_lr: 0.000001  loss: 3.7002 (3.7274)  loss_scale: 32768.0000 (33378.5839)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0208  max mem: 15572
Epoch: [19]  [ 170/1404]  eta: 0:13:02  lr: 0.000061  min_lr: 0.000001  loss: 3.4946 (3.7215)  loss_scale: 32768.0000 (33342.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0006  max mem: 15572
Epoch: [19]  [ 180/1404]  eta: 0:12:47  lr: 0.000061  min_lr: 0.000001  loss: 3.7492 (3.7293)  loss_scale: 32768.0000 (33311.1160)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0006  max mem: 15572
Epoch: [19]  [ 190/1404]  eta: 0:12:36  lr: 0.000061  min_lr: 0.000001  loss: 4.0060 (3.7427)  loss_scale: 32768.0000 (33282.6806)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0007  max mem: 15572
Epoch: [19]  [ 200/1404]  eta: 0:12:34  lr: 0.000061  min_lr: 0.000001  loss: 3.9652 (3.7390)  loss_scale: 32768.0000 (33257.0746)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0055  max mem: 15572
Epoch: [19]  [ 210/1404]  eta: 0:12:24  lr: 0.000061  min_lr: 0.000001  loss: 3.8583 (3.7428)  loss_scale: 32768.0000 (33233.8957)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0057  max mem: 15572
Epoch: [19]  [ 220/1404]  eta: 0:12:17  lr: 0.000061  min_lr: 0.000001  loss: 3.7357 (3.7384)  loss_scale: 32768.0000 (33212.8145)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0009  max mem: 15572
Epoch: [19]  [ 230/1404]  eta: 0:12:07  lr: 0.000061  min_lr: 0.000001  loss: 3.9283 (3.7522)  loss_scale: 32768.0000 (33193.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0007  max mem: 15572
Epoch: [19]  [ 240/1404]  eta: 0:11:57  lr: 0.000061  min_lr: 0.000001  loss: 3.8319 (3.7321)  loss_scale: 32768.0000 (33175.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0010  max mem: 15572
Epoch: [19]  [ 250/1404]  eta: 0:11:53  lr: 0.000061  min_lr: 0.000001  loss: 3.7181 (3.7408)  loss_scale: 32768.0000 (33159.6494)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0011  max mem: 15572
Epoch: [19]  [ 260/1404]  eta: 0:11:45  lr: 0.000061  min_lr: 0.000001  loss: 3.7551 (3.7331)  loss_scale: 32768.0000 (33144.6437)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0008  max mem: 15572
Epoch: [19]  [ 270/1404]  eta: 0:11:38  lr: 0.000061  min_lr: 0.000001  loss: 3.7551 (3.7405)  loss_scale: 32768.0000 (33130.7454)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0010  max mem: 15572
Epoch: [19]  [ 280/1404]  eta: 0:11:32  lr: 0.000061  min_lr: 0.000001  loss: 3.8936 (3.7464)  loss_scale: 32768.0000 (33117.8363)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0009  max mem: 15572
[2025-01-10 20:30:38,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:30:38,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:30:38,825] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:30:38,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:30:40,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26963
[2025-01-10 20:30:40,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:30:40,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26963
[2025-01-10 20:30:40,802] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:30:40,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 290/1404]  eta: 0:11:24  lr: 0.000061  min_lr: 0.000001  loss: 3.8893 (3.7456)  loss_scale: 32768.0000 (33556.2337)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0009  max mem: 15572
Epoch: [19]  [ 300/1404]  eta: 0:11:16  lr: 0.000061  min_lr: 0.000001  loss: 3.7251 (3.7403)  loss_scale: 32768.0000 (33530.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0010  max mem: 15572
Epoch: [19]  [ 310/1404]  eta: 0:11:08  lr: 0.000061  min_lr: 0.000001  loss: 3.7251 (3.7422)  loss_scale: 32768.0000 (33505.5434)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0009  max mem: 15572
Epoch: [19]  [ 320/1404]  eta: 0:11:04  lr: 0.000060  min_lr: 0.000001  loss: 3.7928 (3.7402)  loss_scale: 32768.0000 (33482.5670)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0008  max mem: 15572
[2025-01-10 20:31:02,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=176, lr=[5.858866367352662e-07, 5.858866367352662e-07, 8.369809096218089e-07, 8.369809096218089e-07, 1.1956870137454413e-06, 1.1956870137454413e-06, 1.7081243053506307e-06, 1.7081243053506307e-06, 2.44017757907233e-06, 2.44017757907233e-06, 3.4859679701033285e-06, 3.4859679701033285e-06, 4.979954243004755e-06, 4.979954243004755e-06, 7.114220347149651e-06, 7.114220347149651e-06, 1.0163171924499501e-05, 1.0163171924499501e-05, 1.451881703499929e-05, 1.451881703499929e-05, 2.0741167192856128e-05, 2.0741167192856128e-05, 2.9630238846937328e-05, 2.9630238846937328e-05, 4.2328912638481896e-05, 4.2328912638481896e-05, 6.046987519783129e-05, 6.046987519783129e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 20:31:02,206] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=45.62305151943839, CurrSamplesPerSec=51.755014406199294, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 330/1404]  eta: 0:10:56  lr: 0.000060  min_lr: 0.000001  loss: 3.4995 (3.7349)  loss_scale: 32768.0000 (33460.9789)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0007  max mem: 15572
Epoch: [19]  [ 340/1404]  eta: 0:10:49  lr: 0.000060  min_lr: 0.000001  loss: 3.4341 (3.7311)  loss_scale: 32768.0000 (33440.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0007  max mem: 15572
Epoch: [19]  [ 350/1404]  eta: 0:10:43  lr: 0.000060  min_lr: 0.000001  loss: 3.7187 (3.7342)  loss_scale: 32768.0000 (33421.4929)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0008  max mem: 15572
Epoch: [19]  [ 360/1404]  eta: 0:10:38  lr: 0.000060  min_lr: 0.000001  loss: 3.7615 (3.7298)  loss_scale: 32768.0000 (33403.3906)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0009  max mem: 15572
Epoch: [19]  [ 370/1404]  eta: 0:10:29  lr: 0.000060  min_lr: 0.000001  loss: 3.8541 (3.7345)  loss_scale: 32768.0000 (33386.2642)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0008  max mem: 15572
Epoch: [19]  [ 380/1404]  eta: 0:10:23  lr: 0.000060  min_lr: 0.000001  loss: 3.7700 (3.7328)  loss_scale: 32768.0000 (33370.0367)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0007  max mem: 15572
Epoch: [19]  [ 390/1404]  eta: 0:10:16  lr: 0.000060  min_lr: 0.000001  loss: 3.5079 (3.7228)  loss_scale: 32768.0000 (33354.6394)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0007  max mem: 15572
Epoch: [19]  [ 400/1404]  eta: 0:10:09  lr: 0.000060  min_lr: 0.000001  loss: 3.4558 (3.7231)  loss_scale: 32768.0000 (33340.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0011  max mem: 15572
Epoch: [19]  [ 410/1404]  eta: 0:10:04  lr: 0.000060  min_lr: 0.000001  loss: 3.9382 (3.7279)  loss_scale: 32768.0000 (33326.0925)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0012  max mem: 15572
[2025-01-10 20:31:57,709] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:31:57,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:31:57,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:31:57,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:31:59,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27096
[2025-01-10 20:31:59,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:31:59,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:31:59,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27096
[2025-01-10 20:31:59,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [ 420/1404]  eta: 0:09:58  lr: 0.000060  min_lr: 0.000001  loss: 3.9349 (3.7309)  loss_scale: 32768.0000 (33624.1710)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0007  max mem: 15572
Epoch: [19]  [ 430/1404]  eta: 0:09:51  lr: 0.000060  min_lr: 0.000001  loss: 3.9309 (3.7309)  loss_scale: 32768.0000 (33604.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0007  max mem: 15572
Epoch: [19]  [ 440/1404]  eta: 0:09:43  lr: 0.000060  min_lr: 0.000001  loss: 3.9323 (3.7380)  loss_scale: 32768.0000 (33585.3424)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0007  max mem: 15572
Epoch: [19]  [ 450/1404]  eta: 0:09:36  lr: 0.000060  min_lr: 0.000001  loss: 3.8487 (3.7363)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0005  max mem: 15572
Epoch: [19]  [ 460/1404]  eta: 0:09:30  lr: 0.000060  min_lr: 0.000001  loss: 3.8487 (3.7382)  loss_scale: 32768.0000 (33549.8829)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0006  max mem: 15572
Epoch: [19]  [ 470/1404]  eta: 0:09:25  lr: 0.000060  min_lr: 0.000001  loss: 3.7956 (3.7360)  loss_scale: 32768.0000 (33533.2824)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0007  max mem: 15572
Epoch: [19]  [ 480/1404]  eta: 0:09:21  lr: 0.000060  min_lr: 0.000001  loss: 3.7343 (3.7355)  loss_scale: 32768.0000 (33517.3721)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.0007  max mem: 15572
Epoch: [19]  [ 490/1404]  eta: 0:09:13  lr: 0.000060  min_lr: 0.000001  loss: 3.8588 (3.7414)  loss_scale: 32768.0000 (33502.1100)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0006  max mem: 15572
Epoch: [19]  [ 500/1404]  eta: 0:09:06  lr: 0.000060  min_lr: 0.000001  loss: 4.0062 (3.7418)  loss_scale: 32768.0000 (33487.4571)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0006  max mem: 15572
Epoch: [19]  [ 510/1404]  eta: 0:08:59  lr: 0.000060  min_lr: 0.000001  loss: 3.8223 (3.7423)  loss_scale: 32768.0000 (33473.3777)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0006  max mem: 15572
Epoch: [19]  [ 520/1404]  eta: 0:08:53  lr: 0.000060  min_lr: 0.000001  loss: 3.8720 (3.7444)  loss_scale: 32768.0000 (33459.8388)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
Epoch: [19]  [ 530/1404]  eta: 0:08:47  lr: 0.000060  min_lr: 0.000001  loss: 3.8371 (3.7427)  loss_scale: 32768.0000 (33446.8098)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0006  max mem: 15572
Epoch: [19]  [ 540/1404]  eta: 0:08:40  lr: 0.000060  min_lr: 0.000001  loss: 3.7552 (3.7415)  loss_scale: 32768.0000 (33434.2625)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0006  max mem: 15572
[2025-01-10 20:33:15,607] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:33:15,607] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:33:15,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:33:15,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:33:16,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27226
[2025-01-10 20:33:16,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:33:16,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 550/1404]  eta: 0:08:34  lr: 0.000060  min_lr: 0.000001  loss: 3.4029 (3.7370)  loss_scale: 32768.0000 (33481.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0006  max mem: 15572
[2025-01-10 20:33:16,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27226
[2025-01-10 20:33:16,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [ 560/1404]  eta: 0:08:28  lr: 0.000060  min_lr: 0.000001  loss: 3.3728 (3.7326)  loss_scale: 32768.0000 (33468.9198)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0005  max mem: 15572
Epoch: [19]  [ 570/1404]  eta: 0:08:22  lr: 0.000060  min_lr: 0.000001  loss: 3.3728 (3.7280)  loss_scale: 32768.0000 (33456.6445)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0006  max mem: 15572
Epoch: [19]  [ 580/1404]  eta: 0:08:18  lr: 0.000060  min_lr: 0.000001  loss: 3.6730 (3.7295)  loss_scale: 32768.0000 (33444.7917)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0008  max mem: 15572
Epoch: [19]  [ 590/1404]  eta: 0:08:11  lr: 0.000060  min_lr: 0.000001  loss: 3.7188 (3.7295)  loss_scale: 32768.0000 (33433.3401)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0007  max mem: 15572
Epoch: [19]  [ 600/1404]  eta: 0:08:04  lr: 0.000060  min_lr: 0.000001  loss: 3.7188 (3.7295)  loss_scale: 32768.0000 (33422.2696)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0034  max mem: 15572
Epoch: [19]  [ 610/1404]  eta: 0:07:58  lr: 0.000060  min_lr: 0.000001  loss: 3.7389 (3.7276)  loss_scale: 32768.0000 (33411.5614)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0042  max mem: 15572
Epoch: [19]  [ 620/1404]  eta: 0:07:52  lr: 0.000060  min_lr: 0.000001  loss: 3.8013 (3.7299)  loss_scale: 32768.0000 (33401.1981)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0014  max mem: 15572
Epoch: [19]  [ 630/1404]  eta: 0:07:45  lr: 0.000060  min_lr: 0.000001  loss: 3.6445 (3.7273)  loss_scale: 32768.0000 (33391.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0008  max mem: 15572
Epoch: [19]  [ 640/1404]  eta: 0:07:40  lr: 0.000060  min_lr: 0.000001  loss: 3.6445 (3.7279)  loss_scale: 32768.0000 (33381.4415)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0007  max mem: 15572
Epoch: [19]  [ 650/1404]  eta: 0:07:33  lr: 0.000060  min_lr: 0.000001  loss: 3.9884 (3.7303)  loss_scale: 32768.0000 (33372.0184)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0007  max mem: 15572
Epoch: [19]  [ 660/1404]  eta: 0:07:27  lr: 0.000060  min_lr: 0.000001  loss: 3.9884 (3.7292)  loss_scale: 32768.0000 (33362.8805)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0008  max mem: 15572
Epoch: [19]  [ 670/1404]  eta: 0:07:21  lr: 0.000059  min_lr: 0.000001  loss: 3.8191 (3.7278)  loss_scale: 32768.0000 (33354.0149)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0007  max mem: 15572
[2025-01-10 20:34:32,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:34:32,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:34:32,512] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:34:32,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 680/1404]  eta: 0:07:14  lr: 0.000059  min_lr: 0.000001  loss: 3.2401 (3.7248)  loss_scale: 32768.0000 (33441.6446)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0007  max mem: 15572
Epoch: [19]  [ 690/1404]  eta: 0:07:08  lr: 0.000059  min_lr: 0.000001  loss: 3.8131 (3.7256)  loss_scale: 65536.0000 (33906.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0008  max mem: 15572
[2025-01-10 20:34:42,387] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27372
[2025-01-10 20:34:42,388] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:34:42,411] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27372
[2025-01-10 20:34:42,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:34:42,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 700/1404]  eta: 0:07:02  lr: 0.000059  min_lr: 0.000001  loss: 3.8131 (3.7232)  loss_scale: 65536.0000 (34123.5949)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0008  max mem: 15572
Epoch: [19]  [ 710/1404]  eta: 0:06:56  lr: 0.000059  min_lr: 0.000001  loss: 3.5729 (3.7228)  loss_scale: 32768.0000 (34104.5288)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0006  max mem: 15572
Epoch: [19]  [ 720/1404]  eta: 0:06:49  lr: 0.000059  min_lr: 0.000001  loss: 3.8146 (3.7237)  loss_scale: 32768.0000 (34085.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0007  max mem: 15572
Epoch: [19]  [ 730/1404]  eta: 0:06:44  lr: 0.000059  min_lr: 0.000001  loss: 3.8146 (3.7248)  loss_scale: 32768.0000 (34067.9617)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0007  max mem: 15572
Epoch: [19]  [ 740/1404]  eta: 0:06:37  lr: 0.000059  min_lr: 0.000001  loss: 3.8190 (3.7261)  loss_scale: 32768.0000 (34050.4184)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0007  max mem: 15572
Epoch: [19]  [ 750/1404]  eta: 0:06:32  lr: 0.000059  min_lr: 0.000001  loss: 3.7433 (3.7253)  loss_scale: 32768.0000 (34033.3422)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0007  max mem: 15572
Epoch: [19]  [ 760/1404]  eta: 0:06:26  lr: 0.000059  min_lr: 0.000001  loss: 3.7973 (3.7263)  loss_scale: 32768.0000 (34016.7148)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.0008  max mem: 15572
Epoch: [19]  [ 770/1404]  eta: 0:06:20  lr: 0.000059  min_lr: 0.000001  loss: 3.7565 (3.7253)  loss_scale: 32768.0000 (34000.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
[2025-01-10 20:35:30,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27452
[2025-01-10 20:35:30,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 20:35:30,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27452
[2025-01-10 20:35:30,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 20:35:30,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [ 780/1404]  eta: 0:06:14  lr: 0.000059  min_lr: 0.000001  loss: 3.7133 (3.7259)  loss_scale: 32768.0000 (33879.8464)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0010  max mem: 15572
Epoch: [19]  [ 790/1404]  eta: 0:06:08  lr: 0.000059  min_lr: 0.000001  loss: 3.7720 (3.7238)  loss_scale: 16384.0000 (33658.6599)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0010  max mem: 15572
Epoch: [19]  [ 800/1404]  eta: 0:06:02  lr: 0.000059  min_lr: 0.000001  loss: 3.7382 (3.7233)  loss_scale: 16384.0000 (33442.9963)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0007  max mem: 15572
Epoch: [19]  [ 810/1404]  eta: 0:05:56  lr: 0.000059  min_lr: 0.000001  loss: 3.7763 (3.7267)  loss_scale: 16384.0000 (33232.6510)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0007  max mem: 15572
Epoch: [19]  [ 820/1404]  eta: 0:05:49  lr: 0.000059  min_lr: 0.000001  loss: 3.8691 (3.7273)  loss_scale: 16384.0000 (33027.4300)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0008  max mem: 15572
Epoch: [19]  [ 830/1404]  eta: 0:05:43  lr: 0.000059  min_lr: 0.000001  loss: 3.7521 (3.7255)  loss_scale: 16384.0000 (32827.1480)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0008  max mem: 15572
Epoch: [19]  [ 840/1404]  eta: 0:05:37  lr: 0.000059  min_lr: 0.000001  loss: 3.7213 (3.7230)  loss_scale: 16384.0000 (32631.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0009  max mem: 15572
Epoch: [19]  [ 850/1404]  eta: 0:05:31  lr: 0.000059  min_lr: 0.000001  loss: 3.7725 (3.7238)  loss_scale: 16384.0000 (32440.7051)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0010  max mem: 15572
Epoch: [19]  [ 860/1404]  eta: 0:05:25  lr: 0.000059  min_lr: 0.000001  loss: 3.7701 (3.7236)  loss_scale: 16384.0000 (32254.2160)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0009  max mem: 15572
Epoch: [19]  [ 870/1404]  eta: 0:05:19  lr: 0.000059  min_lr: 0.000001  loss: 3.8876 (3.7258)  loss_scale: 16384.0000 (32072.0092)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0008  max mem: 15572
Epoch: [19]  [ 880/1404]  eta: 0:05:13  lr: 0.000059  min_lr: 0.000001  loss: 3.9147 (3.7256)  loss_scale: 16384.0000 (31893.9387)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0009  max mem: 15572
Epoch: [19]  [ 890/1404]  eta: 0:05:07  lr: 0.000059  min_lr: 0.000001  loss: 3.6106 (3.7234)  loss_scale: 16384.0000 (31719.8653)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0009  max mem: 15572
Epoch: [19]  [ 900/1404]  eta: 0:05:01  lr: 0.000059  min_lr: 0.000001  loss: 3.3883 (3.7227)  loss_scale: 16384.0000 (31549.6559)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0006  max mem: 15572
[2025-01-10 20:36:46,773] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:36:46,774] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 20:36:46,796] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:36:46,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 910/1404]  eta: 0:04:55  lr: 0.000059  min_lr: 0.000001  loss: 3.7728 (3.7229)  loss_scale: 16384.0000 (31491.0911)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0006  max mem: 15572
Epoch: [19]  [ 920/1404]  eta: 0:04:49  lr: 0.000059  min_lr: 0.000001  loss: 3.8102 (3.7219)  loss_scale: 32768.0000 (31504.9555)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0008  max mem: 15572
Epoch: [19]  [ 930/1404]  eta: 0:04:43  lr: 0.000059  min_lr: 0.000001  loss: 3.8410 (3.7223)  loss_scale: 32768.0000 (31518.5220)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0009  max mem: 15572
Epoch: [19]  [ 940/1404]  eta: 0:04:37  lr: 0.000059  min_lr: 0.000001  loss: 3.6430 (3.7190)  loss_scale: 32768.0000 (31531.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0007  max mem: 15572
Epoch: [19]  [ 950/1404]  eta: 0:04:31  lr: 0.000059  min_lr: 0.000001  loss: 3.4000 (3.7190)  loss_scale: 32768.0000 (31544.7992)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0008  max mem: 15572
Epoch: [19]  [ 960/1404]  eta: 0:04:25  lr: 0.000059  min_lr: 0.000001  loss: 3.7111 (3.7203)  loss_scale: 32768.0000 (31557.5276)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0009  max mem: 15572
Epoch: [19]  [ 970/1404]  eta: 0:04:19  lr: 0.000059  min_lr: 0.000001  loss: 3.5865 (3.7173)  loss_scale: 32768.0000 (31569.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0009  max mem: 15572
Epoch: [19]  [ 980/1404]  eta: 0:04:13  lr: 0.000059  min_lr: 0.000001  loss: 3.5865 (3.7176)  loss_scale: 32768.0000 (31582.2059)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0009  max mem: 15572
Epoch: [19]  [ 990/1404]  eta: 0:04:07  lr: 0.000059  min_lr: 0.000001  loss: 3.8798 (3.7194)  loss_scale: 32768.0000 (31594.1715)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0006  max mem: 15572
Epoch: [19]  [1000/1404]  eta: 0:04:01  lr: 0.000059  min_lr: 0.000001  loss: 3.6346 (3.7174)  loss_scale: 32768.0000 (31605.8981)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0005  max mem: 15572
Epoch: [19]  [1010/1404]  eta: 0:03:55  lr: 0.000058  min_lr: 0.000001  loss: 3.5677 (3.7172)  loss_scale: 32768.0000 (31617.3927)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0004  max mem: 15572
Epoch: [19]  [1020/1404]  eta: 0:03:49  lr: 0.000058  min_lr: 0.000001  loss: 3.9104 (3.7194)  loss_scale: 32768.0000 (31628.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0007  max mem: 15572
Epoch: [19]  [1030/1404]  eta: 0:03:43  lr: 0.000058  min_lr: 0.000001  loss: 3.9104 (3.7219)  loss_scale: 32768.0000 (31639.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0009  max mem: 15572
[2025-01-10 20:38:00,690] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:38:00,690] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:38:00,707] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:38:00,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:38:01,261] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27710
[2025-01-10 20:38:01,261] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:38:01,261] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:38:01,328] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27710
[2025-01-10 20:38:01,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [1040/1404]  eta: 0:03:36  lr: 0.000058  min_lr: 0.000001  loss: 3.8093 (3.7203)  loss_scale: 32768.0000 (31682.0288)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0008  max mem: 15572
Epoch: [19]  [1050/1404]  eta: 0:03:31  lr: 0.000058  min_lr: 0.000001  loss: 3.6963 (3.7203)  loss_scale: 32768.0000 (31692.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0012  max mem: 15572
Epoch: [19]  [1060/1404]  eta: 0:03:24  lr: 0.000058  min_lr: 0.000001  loss: 3.6471 (3.7190)  loss_scale: 32768.0000 (31702.4995)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0013  max mem: 15572
Epoch: [19]  [1070/1404]  eta: 0:03:19  lr: 0.000058  min_lr: 0.000001  loss: 3.6471 (3.7183)  loss_scale: 32768.0000 (31712.4482)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0007  max mem: 15572
Epoch: [19]  [1080/1404]  eta: 0:03:13  lr: 0.000058  min_lr: 0.000001  loss: 3.7881 (3.7176)  loss_scale: 32768.0000 (31722.2128)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0008  max mem: 15572
Epoch: [19]  [1090/1404]  eta: 0:03:07  lr: 0.000058  min_lr: 0.000001  loss: 3.4356 (3.7152)  loss_scale: 32768.0000 (31731.7984)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0009  max mem: 15572
Epoch: [19]  [1100/1404]  eta: 0:03:00  lr: 0.000058  min_lr: 0.000001  loss: 3.5403 (3.7167)  loss_scale: 32768.0000 (31741.2098)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0008  max mem: 15572
Epoch: [19]  [1110/1404]  eta: 0:02:54  lr: 0.000058  min_lr: 0.000001  loss: 3.8223 (3.7157)  loss_scale: 32768.0000 (31750.4518)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0008  max mem: 15572
Epoch: [19]  [1120/1404]  eta: 0:02:49  lr: 0.000058  min_lr: 0.000001  loss: 3.8223 (3.7178)  loss_scale: 32768.0000 (31759.5290)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0007  max mem: 15572
Epoch: [19]  [1130/1404]  eta: 0:02:43  lr: 0.000058  min_lr: 0.000001  loss: 3.9362 (3.7199)  loss_scale: 32768.0000 (31768.4456)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0007  max mem: 15572
Epoch: [19]  [1140/1404]  eta: 0:02:37  lr: 0.000058  min_lr: 0.000001  loss: 3.8773 (3.7208)  loss_scale: 32768.0000 (31777.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0009  max mem: 15572
Epoch: [19]  [1150/1404]  eta: 0:02:31  lr: 0.000058  min_lr: 0.000001  loss: 3.7611 (3.7201)  loss_scale: 32768.0000 (31785.8141)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0012  max mem: 15572
Epoch: [19]  [1160/1404]  eta: 0:02:25  lr: 0.000058  min_lr: 0.000001  loss: 3.7611 (3.7223)  loss_scale: 32768.0000 (31794.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0113  max mem: 15572
[2025-01-10 20:39:17,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:39:17,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:39:17,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:39:17,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:39:21,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27845
[2025-01-10 20:39:21,775] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:39:21,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:39:21,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27845
[2025-01-10 20:39:21,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [1170/1404]  eta: 0:02:19  lr: 0.000058  min_lr: 0.000001  loss: 3.7681 (3.7226)  loss_scale: 32768.0000 (31970.4868)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0111  max mem: 15572
Epoch: [19]  [1180/1404]  eta: 0:02:13  lr: 0.000058  min_lr: 0.000001  loss: 3.7305 (3.7239)  loss_scale: 32768.0000 (31977.2396)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0011  max mem: 15572
Epoch: [19]  [1190/1404]  eta: 0:02:07  lr: 0.000058  min_lr: 0.000001  loss: 4.0310 (3.7266)  loss_scale: 32768.0000 (31983.8791)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0011  max mem: 15572
Epoch: [19]  [1200/1404]  eta: 0:02:01  lr: 0.000058  min_lr: 0.000001  loss: 3.9509 (3.7262)  loss_scale: 32768.0000 (31990.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0225  max mem: 15572
Epoch: [19]  [1210/1404]  eta: 0:01:55  lr: 0.000058  min_lr: 0.000001  loss: 3.5940 (3.7253)  loss_scale: 32768.0000 (31996.8291)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0223  max mem: 15572
Epoch: [19]  [1220/1404]  eta: 0:01:49  lr: 0.000058  min_lr: 0.000001  loss: 3.7512 (3.7258)  loss_scale: 32768.0000 (32003.1450)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0009  max mem: 15572
Epoch: [19]  [1230/1404]  eta: 0:01:43  lr: 0.000058  min_lr: 0.000001  loss: 3.7872 (3.7240)  loss_scale: 32768.0000 (32009.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0011  max mem: 15572
Epoch: [19]  [1240/1404]  eta: 0:01:37  lr: 0.000058  min_lr: 0.000001  loss: 3.6378 (3.7239)  loss_scale: 32768.0000 (32015.4714)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0195  max mem: 15572
Epoch: [19]  [1250/1404]  eta: 0:01:31  lr: 0.000058  min_lr: 0.000001  loss: 3.6579 (3.7229)  loss_scale: 32768.0000 (32021.4868)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0300  max mem: 15572
Epoch: [19]  [1260/1404]  eta: 0:01:25  lr: 0.000058  min_lr: 0.000001  loss: 3.5682 (3.7217)  loss_scale: 32768.0000 (32027.4068)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0418  max mem: 15572
Epoch: [19]  [1270/1404]  eta: 0:01:19  lr: 0.000058  min_lr: 0.000001  loss: 3.6215 (3.7211)  loss_scale: 32768.0000 (32033.2337)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0460  max mem: 15572
Epoch: [19]  [1280/1404]  eta: 0:01:13  lr: 0.000058  min_lr: 0.000001  loss: 3.6398 (3.7209)  loss_scale: 32768.0000 (32038.9696)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0738  max mem: 15572
Epoch: [19]  [1290/1404]  eta: 0:01:07  lr: 0.000058  min_lr: 0.000001  loss: 3.8519 (3.7226)  loss_scale: 32768.0000 (32044.6166)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1595  max mem: 15572
[2025-01-10 20:40:38,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:40:38,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:40:38,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:40:38,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:40:39,583] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27976
[2025-01-10 20:40:39,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:40:39,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1300/1404]  eta: 0:01:01  lr: 0.000058  min_lr: 0.000001  loss: 3.8034 (3.7226)  loss_scale: 32768.0000 (32100.5503)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.1842  max mem: 15572
[2025-01-10 20:40:39,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27976
[2025-01-10 20:40:39,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [1310/1404]  eta: 0:00:55  lr: 0.000058  min_lr: 0.000001  loss: 3.7408 (3.7216)  loss_scale: 32768.0000 (32105.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1198  max mem: 15572
Epoch: [19]  [1320/1404]  eta: 0:00:50  lr: 0.000058  min_lr: 0.000001  loss: 3.7408 (3.7217)  loss_scale: 32768.0000 (32110.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0778  max mem: 15572
[2025-01-10 20:40:53,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=183, lr=[5.578713180155505e-07, 5.578713180155505e-07, 7.969590257365008e-07, 7.969590257365008e-07, 1.138512893909287e-06, 1.138512893909287e-06, 1.6264469912989816e-06, 1.6264469912989816e-06, 2.323495701855688e-06, 2.323495701855688e-06, 3.3192795740795545e-06, 3.3192795740795545e-06, 4.7418279629707926e-06, 4.7418279629707926e-06, 6.774039947101132e-06, 6.774039947101132e-06, 9.67719992443019e-06, 9.67719992443019e-06, 1.3824571320614558e-05, 1.3824571320614558e-05, 1.9749387600877938e-05, 1.9749387600877938e-05, 2.821341085839706e-05, 2.821341085839706e-05, 4.0304872654852946e-05, 4.0304872654852946e-05, 5.757838950693278e-05, 5.757838950693278e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 20:40:53,360] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=45.541914822492956, CurrSamplesPerSec=37.08505262692539, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [1330/1404]  eta: 0:00:44  lr: 0.000058  min_lr: 0.000001  loss: 3.7867 (3.7225)  loss_scale: 32768.0000 (32115.5943)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0798  max mem: 15572
Epoch: [19]  [1340/1404]  eta: 0:00:38  lr: 0.000058  min_lr: 0.000001  loss: 3.6771 (3.7227)  loss_scale: 32768.0000 (32120.4594)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0518  max mem: 15572
Epoch: [19]  [1350/1404]  eta: 0:00:32  lr: 0.000057  min_lr: 0.000001  loss: 3.6541 (3.7217)  loss_scale: 32768.0000 (32125.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0356  max mem: 15572
Epoch: [19]  [1360/1404]  eta: 0:00:26  lr: 0.000057  min_lr: 0.000001  loss: 3.5062 (3.7220)  loss_scale: 32768.0000 (32129.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0227  max mem: 15572
Epoch: [19]  [1370/1404]  eta: 0:00:20  lr: 0.000057  min_lr: 0.000001  loss: 3.5062 (3.7213)  loss_scale: 32768.0000 (32134.6287)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0067  max mem: 15572
Epoch: [19]  [1380/1404]  eta: 0:00:14  lr: 0.000057  min_lr: 0.000001  loss: 3.6333 (3.7208)  loss_scale: 32768.0000 (32139.2151)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0067  max mem: 15572
Epoch: [19]  [1390/1404]  eta: 0:00:08  lr: 0.000057  min_lr: 0.000001  loss: 3.7986 (3.7210)  loss_scale: 32768.0000 (32143.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0009  max mem: 15572
Epoch: [19]  [1400/1404]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000001  loss: 3.8888 (3.7223)  loss_scale: 32768.0000 (32148.1913)  weight_decay: 0.0500 (0.0500)  time: 0.4593  data: 0.0006  max mem: 15572
Epoch: [19]  [1403/1404]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000001  loss: 3.8849 (3.7225)  loss_scale: 32768.0000 (32149.5157)  weight_decay: 0.0500 (0.0500)  time: 0.4322  data: 0.0005  max mem: 15572
Epoch: [19] Total time: 0:13:53 (0.5934 s / it)
Averaged stats: lr: 0.000057  min_lr: 0.000001  loss: 3.8849 (3.7285)  loss_scale: 32768.0000 (32149.5157)  weight_decay: 0.0500 (0.0500)
[2025-01-10 20:41:36,785] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-10 20:41:36,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2025-01-10 20:41:36,810] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-10 20:41:36,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-10 20:41:37,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-10 20:41:37,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/136]  eta: 0:14:37  loss: 1.6369 (1.6369)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 6.4513  data: 6.2252  max mem: 15572
Val:  [ 10/136]  eta: 0:01:43  loss: 2.1171 (2.1407)  acc1: 55.5556 (46.9697)  acc5: 77.7778 (77.7778)  time: 0.8193  data: 0.6203  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 2.3419 (2.2875)  acc1: 38.8889 (43.1217)  acc5: 77.7778 (75.9259)  time: 0.3065  data: 0.1014  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.2956 (2.1515)  acc1: 38.8889 (47.8495)  acc5: 77.7778 (78.1362)  time: 0.3072  data: 0.0974  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.7597 (2.1124)  acc1: 61.1111 (49.4580)  acc5: 83.3333 (78.9973)  time: 0.2888  data: 0.0592  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.0641 (2.1588)  acc1: 44.4444 (47.1678)  acc5: 83.3333 (78.5403)  time: 0.3735  data: 0.1376  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.4109 (2.2552)  acc1: 33.3333 (44.8087)  acc5: 72.2222 (76.8670)  time: 0.4181  data: 0.2126  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.2338 (2.2266)  acc1: 44.4444 (45.7746)  acc5: 72.2222 (76.9171)  time: 0.3878  data: 0.1990  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9911 (2.2153)  acc1: 55.5556 (46.6392)  acc5: 83.3333 (77.5720)  time: 0.3868  data: 0.1946  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.1444 (2.2324)  acc1: 44.4444 (45.3602)  acc5: 77.7778 (77.3504)  time: 0.3834  data: 0.1932  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.6417 (2.3080)  acc1: 27.7778 (43.2893)  acc5: 72.2222 (75.6326)  time: 0.3532  data: 0.1568  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2420 (2.2956)  acc1: 38.8889 (43.7437)  acc5: 72.2222 (75.9259)  time: 0.3611  data: 0.1475  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.0343 (2.2463)  acc1: 50.0000 (45.2709)  acc5: 83.3333 (76.6299)  time: 0.3479  data: 0.1535  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.7527 (2.2028)  acc1: 55.5556 (46.5225)  acc5: 88.8889 (77.2265)  time: 0.2304  data: 0.0735  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8366 (2.2070)  acc1: 55.5556 (46.4783)  acc5: 83.3333 (77.1908)  time: 0.1432  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.3731 s / it)
* Acc@1 45.721 Acc@5 75.778 loss 2.264
Accuracy of the network on the 4883 val videos: 45.7%
[2025-01-10 20:42:27,873] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 20:42:27,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 20:42:27,876] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 20:42:27,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 20:42:30,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 20:42:30,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.72%
Epoch: [20]  [   0/1404]  eta: 3:15:23  lr: 0.000057  min_lr: 0.000001  loss: 3.1133 (3.1133)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3502  data: 7.7565  max mem: 15572
Epoch: [20]  [  10/1404]  eta: 0:29:00  lr: 0.000057  min_lr: 0.000001  loss: 4.0004 (3.9499)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2484  data: 0.7260  max mem: 15572
Epoch: [20]  [  20/1404]  eta: 0:22:03  lr: 0.000057  min_lr: 0.000001  loss: 3.7534 (3.7058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0818  max mem: 15572
[2025-01-10 20:42:52,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:42:52,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:42:52,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:42:52,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:42:53,473] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28106
[2025-01-10 20:42:53,474] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:42:53,474] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:42:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28106
[2025-01-10 20:42:53,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [  30/1404]  eta: 0:18:57  lr: 0.000057  min_lr: 0.000001  loss: 3.4871 (3.6976)  loss_scale: 32768.0000 (33825.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0883  max mem: 15572
Epoch: [20]  [  40/1404]  eta: 0:17:39  lr: 0.000057  min_lr: 0.000001  loss: 3.4871 (3.6459)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0643  max mem: 15572
Epoch: [20]  [  50/1404]  eta: 0:16:27  lr: 0.000057  min_lr: 0.000001  loss: 3.4237 (3.6364)  loss_scale: 32768.0000 (33410.5098)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0467  max mem: 15572
Epoch: [20]  [  60/1404]  eta: 0:15:48  lr: 0.000057  min_lr: 0.000001  loss: 3.6263 (3.6258)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0176  max mem: 15572
Epoch: [20]  [  70/1404]  eta: 0:15:13  lr: 0.000057  min_lr: 0.000001  loss: 3.7564 (3.6637)  loss_scale: 32768.0000 (33229.5211)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0341  max mem: 15572
Epoch: [20]  [  80/1404]  eta: 0:14:43  lr: 0.000057  min_lr: 0.000001  loss: 3.8331 (3.6693)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0487  max mem: 15572
Epoch: [20]  [  90/1404]  eta: 0:14:30  lr: 0.000057  min_lr: 0.000001  loss: 3.7904 (3.6891)  loss_scale: 32768.0000 (33128.0879)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0323  max mem: 15572
Epoch: [20]  [ 100/1404]  eta: 0:14:17  lr: 0.000057  min_lr: 0.000001  loss: 3.8365 (3.7073)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0009  max mem: 15572
Epoch: [20]  [ 110/1404]  eta: 0:14:05  lr: 0.000057  min_lr: 0.000001  loss: 3.8547 (3.7035)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0009  max mem: 15572
Epoch: [20]  [ 120/1404]  eta: 0:13:45  lr: 0.000057  min_lr: 0.000001  loss: 3.7263 (3.6928)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0007  max mem: 15572
Epoch: [20]  [ 130/1404]  eta: 0:13:33  lr: 0.000057  min_lr: 0.000001  loss: 3.5783 (3.6881)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0161  max mem: 15572
Epoch: [20]  [ 140/1404]  eta: 0:13:25  lr: 0.000057  min_lr: 0.000001  loss: 3.4868 (3.6769)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0163  max mem: 15572
Epoch: [20]  [ 150/1404]  eta: 0:13:18  lr: 0.000057  min_lr: 0.000001  loss: 3.5880 (3.6867)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.0007  max mem: 15572
[2025-01-10 20:44:09,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:44:09,297] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:44:09,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:44:09,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 160/1404]  eta: 0:13:03  lr: 0.000057  min_lr: 0.000001  loss: 3.8686 (3.6887)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0008  max mem: 15572
[2025-01-10 20:44:12,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28242
[2025-01-10 20:44:12,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:44:12,895] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28242
[2025-01-10 20:44:12,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:44:12,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 170/1404]  eta: 0:12:54  lr: 0.000057  min_lr: 0.000001  loss: 3.6051 (3.6702)  loss_scale: 32768.0000 (34301.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0007  max mem: 15572
Epoch: [20]  [ 180/1404]  eta: 0:12:48  lr: 0.000057  min_lr: 0.000001  loss: 3.6936 (3.6782)  loss_scale: 32768.0000 (34216.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.0279  max mem: 15572
Epoch: [20]  [ 190/1404]  eta: 0:12:46  lr: 0.000057  min_lr: 0.000001  loss: 3.8337 (3.6829)  loss_scale: 32768.0000 (34140.4817)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0279  max mem: 15572
Epoch: [20]  [ 200/1404]  eta: 0:12:31  lr: 0.000057  min_lr: 0.000001  loss: 3.7419 (3.6873)  loss_scale: 32768.0000 (34072.1990)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0007  max mem: 15572
Epoch: [20]  [ 210/1404]  eta: 0:12:21  lr: 0.000057  min_lr: 0.000001  loss: 3.6703 (3.6838)  loss_scale: 32768.0000 (34010.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0007  max mem: 15572
Epoch: [20]  [ 220/1404]  eta: 0:12:14  lr: 0.000057  min_lr: 0.000001  loss: 3.6339 (3.6873)  loss_scale: 32768.0000 (33954.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0006  max mem: 15572
Epoch: [20]  [ 230/1404]  eta: 0:12:06  lr: 0.000057  min_lr: 0.000001  loss: 3.6944 (3.6931)  loss_scale: 32768.0000 (33902.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0349  max mem: 15572
Epoch: [20]  [ 240/1404]  eta: 0:11:58  lr: 0.000057  min_lr: 0.000001  loss: 3.8759 (3.6928)  loss_scale: 32768.0000 (33855.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0816  max mem: 15572
Epoch: [20]  [ 250/1404]  eta: 0:11:51  lr: 0.000057  min_lr: 0.000001  loss: 3.7145 (3.6941)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0965  max mem: 15572
Epoch: [20]  [ 260/1404]  eta: 0:11:44  lr: 0.000057  min_lr: 0.000001  loss: 3.6246 (3.6944)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0988  max mem: 15572
Epoch: [20]  [ 270/1404]  eta: 0:11:39  lr: 0.000057  min_lr: 0.000001  loss: 3.5401 (3.6853)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1200  max mem: 15572
Epoch: [20]  [ 280/1404]  eta: 0:11:33  lr: 0.000057  min_lr: 0.000001  loss: 3.7202 (3.6936)  loss_scale: 32768.0000 (33700.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1213  max mem: 15572
Epoch: [20]  [ 290/1404]  eta: 0:11:27  lr: 0.000056  min_lr: 0.000001  loss: 3.9390 (3.7007)  loss_scale: 32768.0000 (33668.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1073  max mem: 15572
[2025-01-10 20:45:30,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:45:30,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:45:30,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:45:30,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:45:31,037] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28372
[2025-01-10 20:45:31,037] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:45:31,037] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:45:31,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28372
[2025-01-10 20:45:31,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [ 300/1404]  eta: 0:11:17  lr: 0.000056  min_lr: 0.000001  loss: 3.9256 (3.7023)  loss_scale: 32768.0000 (33747.7741)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0570  max mem: 15572
Epoch: [20]  [ 310/1404]  eta: 0:11:13  lr: 0.000056  min_lr: 0.000001  loss: 3.6092 (3.6952)  loss_scale: 32768.0000 (33716.2701)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0678  max mem: 15572
Epoch: [20]  [ 320/1404]  eta: 0:11:05  lr: 0.000056  min_lr: 0.000001  loss: 3.4571 (3.6896)  loss_scale: 32768.0000 (33686.7290)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1033  max mem: 15572
Epoch: [20]  [ 330/1404]  eta: 0:10:57  lr: 0.000056  min_lr: 0.000001  loss: 3.6169 (3.6872)  loss_scale: 32768.0000 (33658.9728)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0514  max mem: 15572
Epoch: [20]  [ 340/1404]  eta: 0:10:51  lr: 0.000056  min_lr: 0.000001  loss: 3.6491 (3.6922)  loss_scale: 32768.0000 (33632.8446)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.0544  max mem: 15572
Epoch: [20]  [ 350/1404]  eta: 0:10:43  lr: 0.000056  min_lr: 0.000001  loss: 3.8444 (3.6942)  loss_scale: 32768.0000 (33608.2051)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0690  max mem: 15572
Epoch: [20]  [ 360/1404]  eta: 0:10:38  lr: 0.000056  min_lr: 0.000001  loss: 3.7537 (3.6912)  loss_scale: 32768.0000 (33584.9307)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0933  max mem: 15572
Epoch: [20]  [ 370/1404]  eta: 0:10:29  lr: 0.000056  min_lr: 0.000001  loss: 3.7176 (3.6915)  loss_scale: 32768.0000 (33562.9111)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0639  max mem: 15572
Epoch: [20]  [ 380/1404]  eta: 0:10:21  lr: 0.000056  min_lr: 0.000001  loss: 3.7854 (3.6944)  loss_scale: 32768.0000 (33542.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0012  max mem: 15572
Epoch: [20]  [ 390/1404]  eta: 0:10:15  lr: 0.000056  min_lr: 0.000001  loss: 3.7427 (3.6914)  loss_scale: 32768.0000 (33522.2506)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0569  max mem: 15572
Epoch: [20]  [ 400/1404]  eta: 0:10:07  lr: 0.000056  min_lr: 0.000001  loss: 3.6392 (3.6909)  loss_scale: 32768.0000 (33503.4414)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0569  max mem: 15572
Epoch: [20]  [ 410/1404]  eta: 0:10:00  lr: 0.000056  min_lr: 0.000001  loss: 3.8252 (3.6955)  loss_scale: 32768.0000 (33485.5474)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0349  max mem: 15572
Epoch: [20]  [ 420/1404]  eta: 0:09:56  lr: 0.000056  min_lr: 0.000001  loss: 3.7085 (3.6924)  loss_scale: 32768.0000 (33468.5036)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1114  max mem: 15572
[2025-01-10 20:46:46,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:46:46,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:46:46,243] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:46:46,243] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:46:49,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28507
[2025-01-10 20:46:49,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:46:49,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:46:49,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28507
[2025-01-10 20:46:49,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [ 430/1404]  eta: 0:09:49  lr: 0.000056  min_lr: 0.000001  loss: 3.7085 (3.6956)  loss_scale: 32768.0000 (33908.4176)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1056  max mem: 15572
Epoch: [20]  [ 440/1404]  eta: 0:09:43  lr: 0.000056  min_lr: 0.000001  loss: 3.7515 (3.6970)  loss_scale: 32768.0000 (33882.5578)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0769  max mem: 15572
Epoch: [20]  [ 450/1404]  eta: 0:09:38  lr: 0.000056  min_lr: 0.000001  loss: 3.5782 (3.6941)  loss_scale: 32768.0000 (33857.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1010  max mem: 15572
Epoch: [20]  [ 460/1404]  eta: 0:09:30  lr: 0.000056  min_lr: 0.000001  loss: 3.5714 (3.6968)  loss_scale: 32768.0000 (33834.2039)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0702  max mem: 15572
Epoch: [20]  [ 470/1404]  eta: 0:09:26  lr: 0.000056  min_lr: 0.000001  loss: 3.7135 (3.6964)  loss_scale: 32768.0000 (33811.5669)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1043  max mem: 15572
Epoch: [20]  [ 480/1404]  eta: 0:09:18  lr: 0.000056  min_lr: 0.000001  loss: 3.8215 (3.7017)  loss_scale: 32768.0000 (33789.8711)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0874  max mem: 15572
Epoch: [20]  [ 490/1404]  eta: 0:09:11  lr: 0.000056  min_lr: 0.000001  loss: 3.6627 (3.6944)  loss_scale: 32768.0000 (33769.0591)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0380  max mem: 15572
Epoch: [20]  [ 500/1404]  eta: 0:09:05  lr: 0.000056  min_lr: 0.000001  loss: 3.6391 (3.6974)  loss_scale: 32768.0000 (33749.0778)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0854  max mem: 15572
Epoch: [20]  [ 510/1404]  eta: 0:08:59  lr: 0.000056  min_lr: 0.000001  loss: 3.7872 (3.6985)  loss_scale: 32768.0000 (33729.8787)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0893  max mem: 15572
Epoch: [20]  [ 520/1404]  eta: 0:08:51  lr: 0.000056  min_lr: 0.000001  loss: 3.7774 (3.7004)  loss_scale: 32768.0000 (33711.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0685  max mem: 15572
Epoch: [20]  [ 530/1404]  eta: 0:08:44  lr: 0.000056  min_lr: 0.000001  loss: 3.7462 (3.7024)  loss_scale: 32768.0000 (33693.6497)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0274  max mem: 15572
Epoch: [20]  [ 540/1404]  eta: 0:08:39  lr: 0.000056  min_lr: 0.000001  loss: 3.7827 (3.7082)  loss_scale: 32768.0000 (33676.5397)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0833  max mem: 15572
Epoch: [20]  [ 550/1404]  eta: 0:08:32  lr: 0.000056  min_lr: 0.000001  loss: 3.7629 (3.7063)  loss_scale: 32768.0000 (33660.0508)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1097  max mem: 15572
[2025-01-10 20:48:05,188] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:48:05,188] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:48:05,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:48:05,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:48:06,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28639
[2025-01-10 20:48:06,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:48:06,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28639
[2025-01-10 20:48:06,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:48:06,854] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 560/1404]  eta: 0:08:26  lr: 0.000056  min_lr: 0.000001  loss: 3.6399 (3.7072)  loss_scale: 32768.0000 (33819.3797)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0719  max mem: 15572
Epoch: [20]  [ 570/1404]  eta: 0:08:21  lr: 0.000056  min_lr: 0.000001  loss: 3.9598 (3.7091)  loss_scale: 32768.0000 (33800.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1339  max mem: 15572
Epoch: [20]  [ 580/1404]  eta: 0:08:16  lr: 0.000056  min_lr: 0.000001  loss: 3.8815 (3.7111)  loss_scale: 32768.0000 (33783.1876)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.1376  max mem: 15572
Epoch: [20]  [ 590/1404]  eta: 0:08:10  lr: 0.000056  min_lr: 0.000001  loss: 3.6034 (3.7053)  loss_scale: 32768.0000 (33766.0102)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1003  max mem: 15572
Epoch: [20]  [ 600/1404]  eta: 0:08:04  lr: 0.000056  min_lr: 0.000001  loss: 3.4955 (3.7069)  loss_scale: 32768.0000 (33749.4043)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0960  max mem: 15572
Epoch: [20]  [ 610/1404]  eta: 0:07:58  lr: 0.000056  min_lr: 0.000001  loss: 3.8450 (3.7074)  loss_scale: 32768.0000 (33733.3421)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0523  max mem: 15572
Epoch: [20]  [ 620/1404]  eta: 0:07:51  lr: 0.000056  min_lr: 0.000001  loss: 3.8938 (3.7088)  loss_scale: 32768.0000 (33717.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0082  max mem: 15572
Epoch: [20]  [ 630/1404]  eta: 0:07:45  lr: 0.000055  min_lr: 0.000001  loss: 3.8947 (3.7108)  loss_scale: 32768.0000 (33702.7448)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0272  max mem: 15572
Epoch: [20]  [ 640/1404]  eta: 0:07:38  lr: 0.000055  min_lr: 0.000001  loss: 3.8947 (3.7108)  loss_scale: 32768.0000 (33688.1622)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0481  max mem: 15572
Epoch: [20]  [ 650/1404]  eta: 0:07:32  lr: 0.000055  min_lr: 0.000001  loss: 3.8598 (3.7121)  loss_scale: 32768.0000 (33674.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0558  max mem: 15572
Epoch: [20]  [ 660/1404]  eta: 0:07:25  lr: 0.000055  min_lr: 0.000001  loss: 3.7337 (3.7105)  loss_scale: 32768.0000 (33660.3207)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0349  max mem: 15572
Epoch: [20]  [ 670/1404]  eta: 0:07:19  lr: 0.000055  min_lr: 0.000001  loss: 3.8039 (3.7133)  loss_scale: 32768.0000 (33647.0224)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0461  max mem: 15572
Epoch: [20]  [ 680/1404]  eta: 0:07:13  lr: 0.000055  min_lr: 0.000001  loss: 3.7564 (3.7127)  loss_scale: 32768.0000 (33634.1145)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0843  max mem: 15572
[2025-01-10 20:49:23,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:49:23,657] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:49:23,669] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:49:23,669] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 690/1404]  eta: 0:07:07  lr: 0.000055  min_lr: 0.000001  loss: 3.6273 (3.7109)  loss_scale: 32768.0000 (33763.8437)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0772  max mem: 15572
[2025-01-10 20:49:29,951] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28779
[2025-01-10 20:49:29,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:49:29,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:49:30,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28779
[2025-01-10 20:49:30,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [ 700/1404]  eta: 0:07:01  lr: 0.000055  min_lr: 0.000001  loss: 3.6234 (3.7104)  loss_scale: 65536.0000 (34123.5949)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0582  max mem: 15572
Epoch: [20]  [ 710/1404]  eta: 0:06:55  lr: 0.000055  min_lr: 0.000001  loss: 3.6234 (3.7099)  loss_scale: 32768.0000 (34104.5288)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0563  max mem: 15572
Epoch: [20]  [ 720/1404]  eta: 0:06:49  lr: 0.000055  min_lr: 0.000001  loss: 3.5851 (3.7062)  loss_scale: 32768.0000 (34085.9917)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0373  max mem: 15572
Epoch: [20]  [ 730/1404]  eta: 0:06:44  lr: 0.000055  min_lr: 0.000001  loss: 3.6459 (3.7077)  loss_scale: 32768.0000 (34067.9617)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.0009  max mem: 15572
Epoch: [20]  [ 740/1404]  eta: 0:06:38  lr: 0.000055  min_lr: 0.000001  loss: 3.8461 (3.7087)  loss_scale: 32768.0000 (34050.4184)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.0008  max mem: 15572
Epoch: [20]  [ 750/1404]  eta: 0:06:32  lr: 0.000055  min_lr: 0.000001  loss: 3.8461 (3.7118)  loss_scale: 32768.0000 (34033.3422)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0010  max mem: 15572
Epoch: [20]  [ 760/1404]  eta: 0:06:25  lr: 0.000055  min_lr: 0.000001  loss: 3.9496 (3.7121)  loss_scale: 32768.0000 (34016.7148)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0009  max mem: 15572
Epoch: [20]  [ 770/1404]  eta: 0:06:19  lr: 0.000055  min_lr: 0.000001  loss: 3.5921 (3.7101)  loss_scale: 32768.0000 (34000.5188)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0008  max mem: 15572
Epoch: [20]  [ 780/1404]  eta: 0:06:14  lr: 0.000055  min_lr: 0.000001  loss: 3.5315 (3.7073)  loss_scale: 32768.0000 (33984.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.0008  max mem: 15572
Epoch: [20]  [ 790/1404]  eta: 0:06:08  lr: 0.000055  min_lr: 0.000001  loss: 3.6905 (3.7087)  loss_scale: 32768.0000 (33969.3552)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0008  max mem: 15572
Epoch: [20]  [ 800/1404]  eta: 0:06:02  lr: 0.000055  min_lr: 0.000001  loss: 3.6905 (3.7071)  loss_scale: 32768.0000 (33954.3571)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0007  max mem: 15572
Epoch: [20]  [ 810/1404]  eta: 0:05:55  lr: 0.000055  min_lr: 0.000001  loss: 3.5980 (3.7067)  loss_scale: 32768.0000 (33939.7287)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0009  max mem: 15572
Epoch: [20]  [ 820/1404]  eta: 0:05:49  lr: 0.000055  min_lr: 0.000001  loss: 3.5570 (3.7055)  loss_scale: 32768.0000 (33925.4568)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0009  max mem: 15572
[2025-01-10 20:50:47,139] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:50:47,139] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:50:47,176] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:50:47,176] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 830/1404]  eta: 0:05:43  lr: 0.000055  min_lr: 0.000001  loss: 3.5570 (3.7037)  loss_scale: 32768.0000 (34029.8243)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0006  max mem: 15572
[2025-01-10 20:50:49,904] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28913
[2025-01-10 20:50:49,905] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:50:49,905] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:50:49,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28913
[2025-01-10 20:50:49,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [ 840/1404]  eta: 0:05:38  lr: 0.000055  min_lr: 0.000001  loss: 3.9249 (3.7061)  loss_scale: 32768.0000 (34092.7467)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0007  max mem: 15572
Epoch: [20]  [ 850/1404]  eta: 0:05:31  lr: 0.000055  min_lr: 0.000001  loss: 3.8245 (3.7024)  loss_scale: 32768.0000 (34077.1798)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0009  max mem: 15572
Epoch: [20]  [ 860/1404]  eta: 0:05:25  lr: 0.000055  min_lr: 0.000001  loss: 3.5009 (3.7011)  loss_scale: 32768.0000 (34061.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0008  max mem: 15572
Epoch: [20]  [ 870/1404]  eta: 0:05:19  lr: 0.000055  min_lr: 0.000001  loss: 3.4747 (3.6964)  loss_scale: 32768.0000 (34047.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0007  max mem: 15572
Epoch: [20]  [ 880/1404]  eta: 0:05:13  lr: 0.000055  min_lr: 0.000001  loss: 3.6318 (3.6984)  loss_scale: 32768.0000 (34032.5993)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0009  max mem: 15572
Epoch: [20]  [ 890/1404]  eta: 0:05:07  lr: 0.000055  min_lr: 0.000001  loss: 3.9242 (3.7005)  loss_scale: 32768.0000 (34018.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0009  max mem: 15572
Epoch: [20]  [ 900/1404]  eta: 0:05:01  lr: 0.000055  min_lr: 0.000001  loss: 3.9328 (3.7024)  loss_scale: 32768.0000 (34004.5283)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0008  max mem: 15572
Epoch: [20]  [ 910/1404]  eta: 0:04:55  lr: 0.000055  min_lr: 0.000001  loss: 3.8910 (3.7013)  loss_scale: 32768.0000 (33990.9550)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0007  max mem: 15572
[2025-01-10 20:51:40,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=190, lr=[5.29434136511386e-07, 5.29434136511386e-07, 7.563344807305515e-07, 7.563344807305515e-07, 1.0804778296150736e-06, 1.0804778296150736e-06, 1.5435397565929626e-06, 1.5435397565929626e-06, 2.2050567951328037e-06, 2.2050567951328037e-06, 3.1500811359040052e-06, 3.1500811359040052e-06, 4.500115908434293e-06, 4.500115908434293e-06, 6.4287370120489915e-06, 6.4287370120489915e-06, 9.183910017212845e-06, 9.183910017212845e-06, 1.3119871453161209e-05, 1.3119871453161209e-05, 1.874267350451601e-05, 1.874267350451601e-05, 2.6775247863594308e-05, 2.6775247863594308e-05, 3.825035409084901e-05, 3.825035409084901e-05, 5.464336298692716e-05, 5.464336298692716e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 20:51:40,693] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=45.579872686510534, CurrSamplesPerSec=60.950063273127995, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [ 920/1404]  eta: 0:04:49  lr: 0.000055  min_lr: 0.000001  loss: 3.8541 (3.7025)  loss_scale: 32768.0000 (33977.6764)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0007  max mem: 15572
Epoch: [20]  [ 930/1404]  eta: 0:04:43  lr: 0.000055  min_lr: 0.000001  loss: 3.7926 (3.7015)  loss_scale: 32768.0000 (33964.6831)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0010  max mem: 15572
Epoch: [20]  [ 940/1404]  eta: 0:04:37  lr: 0.000055  min_lr: 0.000001  loss: 3.7728 (3.7019)  loss_scale: 32768.0000 (33951.9660)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0009  max mem: 15572
Epoch: [20]  [ 950/1404]  eta: 0:04:31  lr: 0.000055  min_lr: 0.000001  loss: 3.9041 (3.7032)  loss_scale: 32768.0000 (33939.5163)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0007  max mem: 15572
Epoch: [20]  [ 960/1404]  eta: 0:04:25  lr: 0.000055  min_lr: 0.000001  loss: 3.7772 (3.7027)  loss_scale: 32768.0000 (33927.3257)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0009  max mem: 15572
[2025-01-10 20:52:06,934] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:52:06,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:52:06,984] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:52:06,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:52:07,995] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29044
[2025-01-10 20:52:07,996] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:52:07,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29044
[2025-01-10 20:52:07,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:52:07,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 970/1404]  eta: 0:04:19  lr: 0.000054  min_lr: 0.000001  loss: 3.8727 (3.7051)  loss_scale: 32768.0000 (33982.8795)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0007  max mem: 15572
Epoch: [20]  [ 980/1404]  eta: 0:04:13  lr: 0.000054  min_lr: 0.000001  loss: 3.9515 (3.7057)  loss_scale: 32768.0000 (33970.4954)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0006  max mem: 15572
Epoch: [20]  [ 990/1404]  eta: 0:04:07  lr: 0.000054  min_lr: 0.000001  loss: 3.7158 (3.7073)  loss_scale: 32768.0000 (33958.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0006  max mem: 15572
Epoch: [20]  [1000/1404]  eta: 0:04:01  lr: 0.000054  min_lr: 0.000001  loss: 3.8202 (3.7086)  loss_scale: 32768.0000 (33946.4695)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0006  max mem: 15572
Epoch: [20]  [1010/1404]  eta: 0:03:55  lr: 0.000054  min_lr: 0.000001  loss: 3.8202 (3.7086)  loss_scale: 32768.0000 (33934.8131)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0008  max mem: 15572
Epoch: [20]  [1020/1404]  eta: 0:03:49  lr: 0.000054  min_lr: 0.000001  loss: 3.8650 (3.7103)  loss_scale: 32768.0000 (33923.3849)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0008  max mem: 15572
Epoch: [20]  [1030/1404]  eta: 0:03:43  lr: 0.000054  min_lr: 0.000001  loss: 3.7938 (3.7103)  loss_scale: 32768.0000 (33912.1785)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0007  max mem: 15572
Epoch: [20]  [1040/1404]  eta: 0:03:37  lr: 0.000054  min_lr: 0.000001  loss: 3.7003 (3.7097)  loss_scale: 32768.0000 (33901.1873)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0007  max mem: 15572
Epoch: [20]  [1050/1404]  eta: 0:03:31  lr: 0.000054  min_lr: 0.000001  loss: 3.4819 (3.7070)  loss_scale: 32768.0000 (33890.4053)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0007  max mem: 15572
Epoch: [20]  [1060/1404]  eta: 0:03:25  lr: 0.000054  min_lr: 0.000001  loss: 3.5826 (3.7103)  loss_scale: 32768.0000 (33879.8266)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0005  max mem: 15572
Epoch: [20]  [1070/1404]  eta: 0:03:19  lr: 0.000054  min_lr: 0.000001  loss: 3.9202 (3.7108)  loss_scale: 32768.0000 (33869.4454)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0005  max mem: 15572
Epoch: [20]  [1080/1404]  eta: 0:03:13  lr: 0.000054  min_lr: 0.000001  loss: 3.8730 (3.7125)  loss_scale: 32768.0000 (33859.2562)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0006  max mem: 15572
Epoch: [20]  [1090/1404]  eta: 0:03:07  lr: 0.000054  min_lr: 0.000001  loss: 3.8626 (3.7108)  loss_scale: 32768.0000 (33849.2539)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0007  max mem: 15572
[2025-01-10 20:53:23,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:53:23,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:53:23,138] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:53:23,138] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1100/1404]  eta: 0:03:01  lr: 0.000054  min_lr: 0.000001  loss: 3.6302 (3.7101)  loss_scale: 32768.0000 (34077.5295)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0007  max mem: 15572
[2025-01-10 20:53:31,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29185
[2025-01-10 20:53:31,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:53:31,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 20:53:31,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29185
[2025-01-10 20:53:31,077] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [1110/1404]  eta: 0:02:55  lr: 0.000054  min_lr: 0.000001  loss: 3.6631 (3.7095)  loss_scale: 65536.0000 (34183.7192)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0006  max mem: 15572
Epoch: [20]  [1120/1404]  eta: 0:02:49  lr: 0.000054  min_lr: 0.000001  loss: 3.8565 (3.7093)  loss_scale: 32768.0000 (34171.0901)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0006  max mem: 15572
Epoch: [20]  [1130/1404]  eta: 0:02:43  lr: 0.000054  min_lr: 0.000001  loss: 3.8321 (3.7094)  loss_scale: 32768.0000 (34158.6844)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0009  max mem: 15572
Epoch: [20]  [1140/1404]  eta: 0:02:37  lr: 0.000054  min_lr: 0.000001  loss: 3.7761 (3.7095)  loss_scale: 32768.0000 (34146.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0009  max mem: 15572
Epoch: [20]  [1150/1404]  eta: 0:02:31  lr: 0.000054  min_lr: 0.000001  loss: 3.6671 (3.7094)  loss_scale: 32768.0000 (34134.5195)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0007  max mem: 15572
Epoch: [20]  [1160/1404]  eta: 0:02:25  lr: 0.000054  min_lr: 0.000001  loss: 3.5106 (3.7082)  loss_scale: 32768.0000 (34122.7494)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0010  max mem: 15572
Epoch: [20]  [1170/1404]  eta: 0:02:19  lr: 0.000054  min_lr: 0.000001  loss: 3.5197 (3.7069)  loss_scale: 32768.0000 (34111.1802)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0009  max mem: 15572
Epoch: [20]  [1180/1404]  eta: 0:02:13  lr: 0.000054  min_lr: 0.000001  loss: 3.5786 (3.7063)  loss_scale: 32768.0000 (34099.8069)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0007  max mem: 15572
Epoch: [20]  [1190/1404]  eta: 0:02:07  lr: 0.000054  min_lr: 0.000001  loss: 3.7615 (3.7070)  loss_scale: 32768.0000 (34088.6247)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0008  max mem: 15572
Epoch: [20]  [1200/1404]  eta: 0:02:01  lr: 0.000054  min_lr: 0.000001  loss: 3.7283 (3.7072)  loss_scale: 32768.0000 (34077.6286)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0006  max mem: 15572
Epoch: [20]  [1210/1404]  eta: 0:01:55  lr: 0.000054  min_lr: 0.000001  loss: 3.9304 (3.7105)  loss_scale: 32768.0000 (34066.8142)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0008  max mem: 15572
Epoch: [20]  [1220/1404]  eta: 0:01:49  lr: 0.000054  min_lr: 0.000001  loss: 3.9798 (3.7109)  loss_scale: 32768.0000 (34056.1769)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0011  max mem: 15572
Epoch: [20]  [1230/1404]  eta: 0:01:43  lr: 0.000054  min_lr: 0.000001  loss: 3.7146 (3.7112)  loss_scale: 32768.0000 (34045.7124)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0009  max mem: 15572
[2025-01-10 20:54:46,976] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:54:46,976] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:54:46,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:54:46,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1240/1404]  eta: 0:01:37  lr: 0.000054  min_lr: 0.000001  loss: 3.8914 (3.7125)  loss_scale: 32768.0000 (34220.2482)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0007  max mem: 15572
[2025-01-10 20:54:53,235] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29324
[2025-01-10 20:54:53,235] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:54:53,257] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29324
[2025-01-10 20:54:53,257] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:54:53,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1250/1404]  eta: 0:01:31  lr: 0.000054  min_lr: 0.000001  loss: 3.8914 (3.7122)  loss_scale: 32768.0000 (34287.2198)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0008  max mem: 15572
Epoch: [20]  [1260/1404]  eta: 0:01:25  lr: 0.000054  min_lr: 0.000001  loss: 3.7828 (3.7128)  loss_scale: 32768.0000 (34275.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0011  max mem: 15572
Epoch: [20]  [1270/1404]  eta: 0:01:19  lr: 0.000054  min_lr: 0.000001  loss: 3.8051 (3.7133)  loss_scale: 32768.0000 (34263.3139)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0010  max mem: 15572
Epoch: [20]  [1280/1404]  eta: 0:01:13  lr: 0.000054  min_lr: 0.000001  loss: 3.5879 (3.7099)  loss_scale: 32768.0000 (34251.6409)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0008  max mem: 15572
Epoch: [20]  [1290/1404]  eta: 0:01:07  lr: 0.000054  min_lr: 0.000001  loss: 3.5732 (3.7104)  loss_scale: 32768.0000 (34240.1487)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0007  max mem: 15572
Epoch: [20]  [1300/1404]  eta: 0:01:01  lr: 0.000054  min_lr: 0.000001  loss: 3.7827 (3.7098)  loss_scale: 32768.0000 (34228.8332)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0007  max mem: 15572
Epoch: [20]  [1310/1404]  eta: 0:00:55  lr: 0.000053  min_lr: 0.000001  loss: 3.6937 (3.7090)  loss_scale: 32768.0000 (34217.6903)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0006  max mem: 15572
Epoch: [20]  [1320/1404]  eta: 0:00:49  lr: 0.000053  min_lr: 0.000001  loss: 3.6938 (3.7074)  loss_scale: 32768.0000 (34206.7161)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0006  max mem: 15572
Epoch: [20]  [1330/1404]  eta: 0:00:44  lr: 0.000053  min_lr: 0.000001  loss: 3.7447 (3.7083)  loss_scale: 32768.0000 (34195.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0005  max mem: 15572
Epoch: [20]  [1340/1404]  eta: 0:00:38  lr: 0.000053  min_lr: 0.000001  loss: 3.7066 (3.7077)  loss_scale: 32768.0000 (34185.2588)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0006  max mem: 15572
Epoch: [20]  [1350/1404]  eta: 0:00:32  lr: 0.000053  min_lr: 0.000001  loss: 3.7896 (3.7087)  loss_scale: 32768.0000 (34174.7683)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0008  max mem: 15572
Epoch: [20]  [1360/1404]  eta: 0:00:26  lr: 0.000053  min_lr: 0.000001  loss: 3.8419 (3.7089)  loss_scale: 32768.0000 (34164.4320)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0011  max mem: 15572
Epoch: [20]  [1370/1404]  eta: 0:00:20  lr: 0.000053  min_lr: 0.000001  loss: 3.7383 (3.7064)  loss_scale: 32768.0000 (34154.2465)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0010  max mem: 15572
[2025-01-10 20:56:08,451] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:56:08,451] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:56:08,517] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:56:08,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1380/1404]  eta: 0:00:14  lr: 0.000053  min_lr: 0.000001  loss: 3.6348 (3.7066)  loss_scale: 32768.0000 (34334.0304)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.0008  max mem: 15572
[2025-01-10 20:56:18,267] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29468
[2025-01-10 20:56:18,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:56:18,268] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29468
[2025-01-10 20:56:18,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:56:18,268] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1390/1404]  eta: 0:00:08  lr: 0.000053  min_lr: 0.000001  loss: 3.6348 (3.7065)  loss_scale: 65536.0000 (34487.6722)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0008  max mem: 15572
Epoch: [20]  [1400/1404]  eta: 0:00:02  lr: 0.000053  min_lr: 0.000001  loss: 3.8233 (3.7079)  loss_scale: 32768.0000 (34475.3976)  weight_decay: 0.0500 (0.0500)  time: 0.5144  data: 0.0006  max mem: 15572
Epoch: [20]  [1403/1404]  eta: 0:00:00  lr: 0.000053  min_lr: 0.000001  loss: 3.9478 (3.7087)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)  time: 0.4947  data: 0.0005  max mem: 15572
Epoch: [20] Total time: 0:13:53 (0.5939 s / it)
Averaged stats: lr: 0.000053  min_lr: 0.000001  loss: 3.9478 (3.7022)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:29  loss: 1.4649 (1.4649)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 6.3967  data: 6.2123  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 2.2321 (2.1811)  acc1: 50.0000 (45.9596)  acc5: 77.7778 (77.2727)  time: 0.8267  data: 0.6390  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.4170 (2.3116)  acc1: 44.4444 (43.1217)  acc5: 72.2222 (75.3968)  time: 0.2928  data: 0.0846  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.2993 (2.1577)  acc1: 44.4444 (48.0287)  acc5: 83.3333 (78.3154)  time: 0.2633  data: 0.0525  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.7340 (2.1007)  acc1: 61.1111 (50.0000)  acc5: 83.3333 (79.2683)  time: 0.2890  data: 0.0732  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 1.9428 (2.1169)  acc1: 50.0000 (49.8911)  acc5: 83.3333 (79.1939)  time: 0.3931  data: 0.1736  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.2461 (2.2014)  acc1: 38.8889 (47.1767)  acc5: 72.2222 (78.0510)  time: 0.4274  data: 0.2145  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.0309 (2.1663)  acc1: 44.4444 (48.2003)  acc5: 83.3333 (78.7167)  time: 0.4182  data: 0.2074  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9801 (2.1743)  acc1: 50.0000 (47.8052)  acc5: 83.3333 (78.8066)  time: 0.4050  data: 0.1982  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.1797 (2.1764)  acc1: 44.4444 (47.6190)  acc5: 77.7778 (78.7546)  time: 0.3782  data: 0.1720  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3414 (2.2663)  acc1: 33.3333 (45.4345)  acc5: 66.6667 (76.6227)  time: 0.3591  data: 0.1515  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4823 (2.2553)  acc1: 33.3333 (45.8458)  acc5: 72.2222 (76.6266)  time: 0.3602  data: 0.1587  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.9000 (2.2089)  acc1: 55.5556 (47.1993)  acc5: 77.7778 (77.3186)  time: 0.3344  data: 0.1557  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5526 (2.1680)  acc1: 61.1111 (48.2188)  acc5: 88.8889 (77.8202)  time: 0.2291  data: 0.0728  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.9146 (2.1695)  acc1: 55.5556 (48.2801)  acc5: 83.3333 (77.8460)  time: 0.1966  data: 0.0541  max mem: 15572
Val: Total time: 0:00:50 (0.3734 s / it)
* Acc@1 46.806 Acc@5 76.556 loss 2.225
Accuracy of the network on the 4883 val videos: 46.8%
[2025-01-10 20:57:14,995] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 20:57:14,996] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 20:57:14,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 20:57:14,997] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 20:57:17,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 20:57:17,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.81%
Epoch: [21]  [   0/1404]  eta: 3:16:15  lr: 0.000053  min_lr: 0.000001  loss: 2.8902 (2.8902)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3870  data: 7.1824  max mem: 15572
Epoch: [21]  [  10/1404]  eta: 0:29:05  lr: 0.000053  min_lr: 0.000001  loss: 3.9571 (3.9070)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2525  data: 0.6773  max mem: 15572
Epoch: [21]  [  20/1404]  eta: 0:21:06  lr: 0.000053  min_lr: 0.000001  loss: 3.8317 (3.8137)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0137  max mem: 15572
Epoch: [21]  [  30/1404]  eta: 0:19:00  lr: 0.000053  min_lr: 0.000001  loss: 3.6813 (3.8076)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0771  max mem: 15572
Epoch: [21]  [  40/1404]  eta: 0:17:04  lr: 0.000053  min_lr: 0.000001  loss: 3.4652 (3.6747)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0816  max mem: 15572
Epoch: [21]  [  50/1404]  eta: 0:16:58  lr: 0.000053  min_lr: 0.000001  loss: 3.1953 (3.6293)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1401  max mem: 15572
Epoch: [21]  [  60/1404]  eta: 0:15:54  lr: 0.000053  min_lr: 0.000001  loss: 3.6525 (3.6488)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1355  max mem: 15572
Epoch: [21]  [  70/1404]  eta: 0:15:20  lr: 0.000053  min_lr: 0.000001  loss: 3.6525 (3.6178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0479  max mem: 15572
Epoch: [21]  [  80/1404]  eta: 0:14:55  lr: 0.000053  min_lr: 0.000001  loss: 3.4470 (3.6326)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0823  max mem: 15572
Epoch: [21]  [  90/1404]  eta: 0:14:44  lr: 0.000053  min_lr: 0.000001  loss: 3.8620 (3.6447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0913  max mem: 15572
Epoch: [21]  [ 100/1404]  eta: 0:14:18  lr: 0.000053  min_lr: 0.000001  loss: 3.6734 (3.6143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0652  max mem: 15572
Epoch: [21]  [ 110/1404]  eta: 0:13:54  lr: 0.000053  min_lr: 0.000001  loss: 3.4677 (3.6299)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0088  max mem: 15572
[2025-01-10 20:58:31,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:58:31,308] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:58:31,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:58:31,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:58:32,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29599
[2025-01-10 20:58:32,341] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:58:32,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29599
[2025-01-10 20:58:32,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:58:32,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 120/1404]  eta: 0:13:41  lr: 0.000053  min_lr: 0.000001  loss: 3.7057 (3.6275)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0471  max mem: 15572
Epoch: [21]  [ 130/1404]  eta: 0:13:32  lr: 0.000053  min_lr: 0.000001  loss: 3.5316 (3.6102)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0891  max mem: 15572
Epoch: [21]  [ 140/1404]  eta: 0:13:29  lr: 0.000053  min_lr: 0.000001  loss: 3.3792 (3.6040)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1290  max mem: 15572
Epoch: [21]  [ 150/1404]  eta: 0:13:22  lr: 0.000053  min_lr: 0.000001  loss: 3.7034 (3.6174)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.1555  max mem: 15572
Epoch: [21]  [ 160/1404]  eta: 0:13:12  lr: 0.000053  min_lr: 0.000001  loss: 3.7369 (3.6213)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1138  max mem: 15572
Epoch: [21]  [ 170/1404]  eta: 0:13:03  lr: 0.000053  min_lr: 0.000001  loss: 3.7021 (3.6241)  loss_scale: 32768.0000 (33151.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0813  max mem: 15572
Epoch: [21]  [ 180/1404]  eta: 0:12:50  lr: 0.000053  min_lr: 0.000001  loss: 3.7021 (3.6336)  loss_scale: 32768.0000 (33130.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0367  max mem: 15572
Epoch: [21]  [ 190/1404]  eta: 0:12:39  lr: 0.000053  min_lr: 0.000001  loss: 3.7910 (3.6424)  loss_scale: 32768.0000 (33111.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0191  max mem: 15572
Epoch: [21]  [ 200/1404]  eta: 0:12:34  lr: 0.000053  min_lr: 0.000001  loss: 3.9287 (3.6614)  loss_scale: 32768.0000 (33094.0498)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0883  max mem: 15572
Epoch: [21]  [ 210/1404]  eta: 0:12:23  lr: 0.000053  min_lr: 0.000001  loss: 4.0683 (3.6687)  loss_scale: 32768.0000 (33078.5972)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0890  max mem: 15572
Epoch: [21]  [ 220/1404]  eta: 0:12:16  lr: 0.000053  min_lr: 0.000001  loss: 3.9580 (3.6684)  loss_scale: 32768.0000 (33064.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0742  max mem: 15572
Epoch: [21]  [ 230/1404]  eta: 0:12:11  lr: 0.000053  min_lr: 0.000001  loss: 3.7858 (3.6750)  loss_scale: 32768.0000 (33051.7056)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1060  max mem: 15572
Epoch: [21]  [ 240/1404]  eta: 0:11:59  lr: 0.000052  min_lr: 0.000001  loss: 3.7007 (3.6791)  loss_scale: 32768.0000 (33039.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0610  max mem: 15572
[2025-01-10 20:59:50,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:59:50,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:59:50,397] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 20:59:50,398] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 20:59:51,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29730
[2025-01-10 20:59:51,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:59:51,412] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29730
[2025-01-10 20:59:51,412] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 20:59:51,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 250/1404]  eta: 0:11:57  lr: 0.000052  min_lr: 0.000001  loss: 3.6670 (3.6742)  loss_scale: 32768.0000 (33290.1992)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1125  max mem: 15572
Epoch: [21]  [ 260/1404]  eta: 0:11:45  lr: 0.000052  min_lr: 0.000001  loss: 3.6032 (3.6654)  loss_scale: 32768.0000 (33270.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1030  max mem: 15572
Epoch: [21]  [ 270/1404]  eta: 0:11:34  lr: 0.000052  min_lr: 0.000001  loss: 3.6382 (3.6664)  loss_scale: 32768.0000 (33251.6605)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0006  max mem: 15572
Epoch: [21]  [ 280/1404]  eta: 0:11:27  lr: 0.000052  min_lr: 0.000001  loss: 3.6626 (3.6593)  loss_scale: 32768.0000 (33234.4484)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0438  max mem: 15572
Epoch: [21]  [ 290/1404]  eta: 0:11:21  lr: 0.000052  min_lr: 0.000001  loss: 3.6078 (3.6589)  loss_scale: 32768.0000 (33218.4192)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0811  max mem: 15572
Epoch: [21]  [ 300/1404]  eta: 0:11:13  lr: 0.000052  min_lr: 0.000001  loss: 3.6078 (3.6584)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0714  max mem: 15572
Epoch: [21]  [ 310/1404]  eta: 0:11:04  lr: 0.000052  min_lr: 0.000001  loss: 3.7002 (3.6645)  loss_scale: 32768.0000 (33189.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0573  max mem: 15572
Epoch: [21]  [ 320/1404]  eta: 0:10:59  lr: 0.000052  min_lr: 0.000001  loss: 3.7557 (3.6689)  loss_scale: 32768.0000 (33176.3240)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0949  max mem: 15572
Epoch: [21]  [ 330/1404]  eta: 0:10:52  lr: 0.000052  min_lr: 0.000001  loss: 3.6208 (3.6699)  loss_scale: 32768.0000 (33163.9879)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1107  max mem: 15572
Epoch: [21]  [ 340/1404]  eta: 0:10:50  lr: 0.000052  min_lr: 0.000001  loss: 3.9591 (3.6786)  loss_scale: 32768.0000 (33152.3754)  weight_decay: 0.0500 (0.0500)  time: 0.6623  data: 0.1412  max mem: 15572
Epoch: [21]  [ 350/1404]  eta: 0:10:42  lr: 0.000052  min_lr: 0.000001  loss: 3.9085 (3.6814)  loss_scale: 32768.0000 (33141.4245)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1023  max mem: 15572
Epoch: [21]  [ 360/1404]  eta: 0:10:35  lr: 0.000052  min_lr: 0.000001  loss: 3.8547 (3.6796)  loss_scale: 32768.0000 (33131.0803)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0373  max mem: 15572
Epoch: [21]  [ 370/1404]  eta: 0:10:28  lr: 0.000052  min_lr: 0.000001  loss: 3.5446 (3.6763)  loss_scale: 32768.0000 (33121.2938)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0857  max mem: 15572
[2025-01-10 21:01:07,154] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:01:07,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:01:07,175] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:01:07,176] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:01:08,101] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29861
[2025-01-10 21:01:08,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:01:08,189] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29861
[2025-01-10 21:01:08,189] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:01:08,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 380/1404]  eta: 0:10:24  lr: 0.000052  min_lr: 0.000001  loss: 3.5187 (3.6747)  loss_scale: 32768.0000 (33284.0315)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1172  max mem: 15572
Epoch: [21]  [ 390/1404]  eta: 0:10:15  lr: 0.000052  min_lr: 0.000001  loss: 3.6828 (3.6794)  loss_scale: 32768.0000 (33270.8338)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0793  max mem: 15572
Epoch: [21]  [ 400/1404]  eta: 0:10:10  lr: 0.000052  min_lr: 0.000001  loss: 3.6828 (3.6714)  loss_scale: 32768.0000 (33258.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0670  max mem: 15572
Epoch: [21]  [ 410/1404]  eta: 0:10:02  lr: 0.000052  min_lr: 0.000001  loss: 3.7129 (3.6729)  loss_scale: 32768.0000 (33246.3650)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0805  max mem: 15572
Epoch: [21]  [ 420/1404]  eta: 0:09:56  lr: 0.000052  min_lr: 0.000001  loss: 3.7735 (3.6752)  loss_scale: 32768.0000 (33235.0024)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0477  max mem: 15572
Epoch: [21]  [ 430/1404]  eta: 0:09:48  lr: 0.000052  min_lr: 0.000001  loss: 3.8077 (3.6772)  loss_scale: 32768.0000 (33224.1671)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0532  max mem: 15572
Epoch: [21]  [ 440/1404]  eta: 0:09:42  lr: 0.000052  min_lr: 0.000001  loss: 3.8398 (3.6759)  loss_scale: 32768.0000 (33213.8231)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0825  max mem: 15572
Epoch: [21]  [ 450/1404]  eta: 0:09:35  lr: 0.000052  min_lr: 0.000001  loss: 3.6981 (3.6746)  loss_scale: 32768.0000 (33203.9379)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0808  max mem: 15572
Epoch: [21]  [ 460/1404]  eta: 0:09:28  lr: 0.000052  min_lr: 0.000001  loss: 3.6986 (3.6765)  loss_scale: 32768.0000 (33194.4816)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0446  max mem: 15572
Epoch: [21]  [ 470/1404]  eta: 0:09:22  lr: 0.000052  min_lr: 0.000001  loss: 4.0518 (3.6817)  loss_scale: 32768.0000 (33185.4268)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0601  max mem: 15572
Epoch: [21]  [ 480/1404]  eta: 0:09:17  lr: 0.000052  min_lr: 0.000001  loss: 4.0144 (3.6836)  loss_scale: 32768.0000 (33176.7484)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0653  max mem: 15572
Epoch: [21]  [ 490/1404]  eta: 0:09:12  lr: 0.000052  min_lr: 0.000001  loss: 3.9884 (3.6841)  loss_scale: 32768.0000 (33168.4236)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0683  max mem: 15572
Epoch: [21]  [ 500/1404]  eta: 0:09:06  lr: 0.000052  min_lr: 0.000001  loss: 3.8559 (3.6818)  loss_scale: 32768.0000 (33160.4311)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.0922  max mem: 15572
[2025-01-10 21:02:23,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:02:23,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:02:23,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:02:23,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:02:26,261] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29994
[2025-01-10 21:02:26,261] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:02:26,261] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29994
[2025-01-10 21:02:26,261] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:02:26,261] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 510/1404]  eta: 0:08:59  lr: 0.000052  min_lr: 0.000001  loss: 3.7068 (3.6867)  loss_scale: 32768.0000 (33409.2524)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0457  max mem: 15572
[2025-01-10 21:02:28,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=198, lr=[5.006912820388102e-07, 5.006912820388102e-07, 7.152732600554432e-07, 7.152732600554432e-07, 1.0218189429363475e-06, 1.0218189429363475e-06, 1.4597413470519252e-06, 1.4597413470519252e-06, 2.0853447815027504e-06, 2.0853447815027504e-06, 2.9790639735753574e-06, 2.9790639735753574e-06, 4.255805676536225e-06, 4.255805676536225e-06, 6.079722395051751e-06, 6.079722395051751e-06, 8.685317707216788e-06, 8.685317707216788e-06, 1.2407596724595412e-05, 1.2407596724595412e-05, 1.7725138177993445e-05, 1.7725138177993445e-05, 2.5321625968562067e-05, 2.5321625968562067e-05, 3.61737513836601e-05, 3.61737513836601e-05, 5.1676787690943e-05, 5.1676787690943e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:02:28,927] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=45.598917117905344, CurrSamplesPerSec=57.098149901275505, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [ 520/1404]  eta: 0:08:53  lr: 0.000052  min_lr: 0.000001  loss: 3.7337 (3.6878)  loss_scale: 32768.0000 (33396.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0006  max mem: 15572
Epoch: [21]  [ 530/1404]  eta: 0:08:47  lr: 0.000052  min_lr: 0.000001  loss: 3.6760 (3.6868)  loss_scale: 32768.0000 (33385.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0827  max mem: 15572
Epoch: [21]  [ 540/1404]  eta: 0:08:41  lr: 0.000052  min_lr: 0.000000  loss: 3.6589 (3.6860)  loss_scale: 32768.0000 (33373.6932)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0830  max mem: 15572
Epoch: [21]  [ 550/1404]  eta: 0:08:34  lr: 0.000052  min_lr: 0.000000  loss: 3.6489 (3.6860)  loss_scale: 32768.0000 (33362.7005)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0263  max mem: 15572
Epoch: [21]  [ 560/1404]  eta: 0:08:27  lr: 0.000052  min_lr: 0.000000  loss: 3.5721 (3.6834)  loss_scale: 32768.0000 (33352.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0261  max mem: 15572
Epoch: [21]  [ 570/1404]  eta: 0:08:22  lr: 0.000052  min_lr: 0.000000  loss: 3.8841 (3.6857)  loss_scale: 32768.0000 (33341.8704)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0008  max mem: 15572
Epoch: [21]  [ 580/1404]  eta: 0:08:16  lr: 0.000051  min_lr: 0.000000  loss: 3.8573 (3.6855)  loss_scale: 32768.0000 (33331.9931)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0012  max mem: 15572
Epoch: [21]  [ 590/1404]  eta: 0:08:09  lr: 0.000051  min_lr: 0.000000  loss: 3.6791 (3.6886)  loss_scale: 32768.0000 (33322.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0011  max mem: 15572
Epoch: [21]  [ 600/1404]  eta: 0:08:05  lr: 0.000051  min_lr: 0.000000  loss: 3.5635 (3.6829)  loss_scale: 32768.0000 (33313.2246)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.0009  max mem: 15572
Epoch: [21]  [ 610/1404]  eta: 0:07:58  lr: 0.000051  min_lr: 0.000000  loss: 3.4463 (3.6818)  loss_scale: 32768.0000 (33304.3011)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0010  max mem: 15572
Epoch: [21]  [ 620/1404]  eta: 0:07:51  lr: 0.000051  min_lr: 0.000000  loss: 3.7932 (3.6832)  loss_scale: 32768.0000 (33295.6651)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0009  max mem: 15572
Epoch: [21]  [ 630/1404]  eta: 0:07:45  lr: 0.000051  min_lr: 0.000000  loss: 3.8760 (3.6840)  loss_scale: 32768.0000 (33287.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0010  max mem: 15572
[2025-01-10 21:03:42,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:03:42,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:03:42,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:03:42,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 640/1404]  eta: 0:07:38  lr: 0.000051  min_lr: 0.000000  loss: 3.7782 (3.6838)  loss_scale: 32768.0000 (33381.4415)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0009  max mem: 15572
[2025-01-10 21:03:44,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30128
[2025-01-10 21:03:44,578] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:03:44,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30128
[2025-01-10 21:03:44,605] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:03:44,606] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 650/1404]  eta: 0:07:33  lr: 0.000051  min_lr: 0.000000  loss: 3.7782 (3.6879)  loss_scale: 32768.0000 (33523.0230)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0010  max mem: 15572
Epoch: [21]  [ 660/1404]  eta: 0:07:26  lr: 0.000051  min_lr: 0.000000  loss: 3.7560 (3.6876)  loss_scale: 32768.0000 (33511.6006)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0009  max mem: 15572
Epoch: [21]  [ 670/1404]  eta: 0:07:21  lr: 0.000051  min_lr: 0.000000  loss: 3.7950 (3.6892)  loss_scale: 32768.0000 (33500.5186)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0007  max mem: 15572
Epoch: [21]  [ 680/1404]  eta: 0:07:14  lr: 0.000051  min_lr: 0.000000  loss: 3.8074 (3.6874)  loss_scale: 32768.0000 (33489.7621)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0007  max mem: 15572
Epoch: [21]  [ 690/1404]  eta: 0:07:08  lr: 0.000051  min_lr: 0.000000  loss: 3.6426 (3.6888)  loss_scale: 32768.0000 (33479.3169)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0008  max mem: 15572
Epoch: [21]  [ 700/1404]  eta: 0:07:01  lr: 0.000051  min_lr: 0.000000  loss: 3.6482 (3.6877)  loss_scale: 32768.0000 (33469.1698)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0011  max mem: 15572
Epoch: [21]  [ 710/1404]  eta: 0:06:56  lr: 0.000051  min_lr: 0.000000  loss: 3.6858 (3.6903)  loss_scale: 32768.0000 (33459.3080)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0010  max mem: 15572
Epoch: [21]  [ 720/1404]  eta: 0:06:50  lr: 0.000051  min_lr: 0.000000  loss: 3.7904 (3.6912)  loss_scale: 32768.0000 (33449.7198)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0006  max mem: 15572
Epoch: [21]  [ 730/1404]  eta: 0:06:44  lr: 0.000051  min_lr: 0.000000  loss: 3.6785 (3.6904)  loss_scale: 32768.0000 (33440.3940)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0006  max mem: 15572
Epoch: [21]  [ 740/1404]  eta: 0:06:38  lr: 0.000051  min_lr: 0.000000  loss: 3.4242 (3.6875)  loss_scale: 32768.0000 (33431.3198)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0007  max mem: 15572
Epoch: [21]  [ 750/1404]  eta: 0:06:32  lr: 0.000051  min_lr: 0.000000  loss: 3.2869 (3.6833)  loss_scale: 32768.0000 (33422.4874)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0009  max mem: 15572
Epoch: [21]  [ 760/1404]  eta: 0:06:26  lr: 0.000051  min_lr: 0.000000  loss: 3.6011 (3.6848)  loss_scale: 32768.0000 (33413.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0009  max mem: 15572
Epoch: [21]  [ 770/1404]  eta: 0:06:19  lr: 0.000051  min_lr: 0.000000  loss: 3.7278 (3.6871)  loss_scale: 32768.0000 (33405.5097)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0007  max mem: 15572
[2025-01-10 21:05:00,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:05:00,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:05:00,819] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:05:00,819] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:05:01,769] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30259
[2025-01-10 21:05:01,769] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:05:01,770] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30259
[2025-01-10 21:05:01,770] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:05:01,770] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 780/1404]  eta: 0:06:12  lr: 0.000051  min_lr: 0.000000  loss: 3.8410 (3.6890)  loss_scale: 32768.0000 (33481.2599)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0006  max mem: 15572
Epoch: [21]  [ 790/1404]  eta: 0:06:06  lr: 0.000051  min_lr: 0.000000  loss: 3.8410 (3.6895)  loss_scale: 32768.0000 (33472.2427)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0006  max mem: 15572
Epoch: [21]  [ 800/1404]  eta: 0:06:00  lr: 0.000051  min_lr: 0.000000  loss: 3.7150 (3.6878)  loss_scale: 32768.0000 (33463.4507)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0007  max mem: 15572
Epoch: [21]  [ 810/1404]  eta: 0:05:54  lr: 0.000051  min_lr: 0.000000  loss: 3.5444 (3.6874)  loss_scale: 32768.0000 (33454.8755)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0006  max mem: 15572
Epoch: [21]  [ 820/1404]  eta: 0:05:48  lr: 0.000051  min_lr: 0.000000  loss: 3.6977 (3.6892)  loss_scale: 32768.0000 (33446.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0381  max mem: 15572
Epoch: [21]  [ 830/1404]  eta: 0:05:42  lr: 0.000051  min_lr: 0.000000  loss: 3.8982 (3.6907)  loss_scale: 32768.0000 (33438.3442)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0381  max mem: 15572
Epoch: [21]  [ 840/1404]  eta: 0:05:37  lr: 0.000051  min_lr: 0.000000  loss: 3.7094 (3.6916)  loss_scale: 32768.0000 (33430.3734)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.0007  max mem: 15572
Epoch: [21]  [ 850/1404]  eta: 0:05:31  lr: 0.000051  min_lr: 0.000000  loss: 3.6939 (3.6951)  loss_scale: 32768.0000 (33422.5899)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0007  max mem: 15572
Epoch: [21]  [ 860/1404]  eta: 0:05:24  lr: 0.000051  min_lr: 0.000000  loss: 3.9885 (3.6970)  loss_scale: 32768.0000 (33414.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0008  max mem: 15572
Epoch: [21]  [ 870/1404]  eta: 0:05:18  lr: 0.000051  min_lr: 0.000000  loss: 3.7662 (3.6962)  loss_scale: 32768.0000 (33407.5591)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0008  max mem: 15572
Epoch: [21]  [ 880/1404]  eta: 0:05:13  lr: 0.000051  min_lr: 0.000000  loss: 3.6689 (3.6967)  loss_scale: 32768.0000 (33400.2997)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0007  max mem: 15572
Epoch: [21]  [ 890/1404]  eta: 0:05:07  lr: 0.000051  min_lr: 0.000000  loss: 3.6300 (3.6947)  loss_scale: 32768.0000 (33393.2031)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0007  max mem: 15572
Epoch: [21]  [ 900/1404]  eta: 0:05:01  lr: 0.000051  min_lr: 0.000000  loss: 3.6300 (3.6947)  loss_scale: 32768.0000 (33386.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0008  max mem: 15572
[2025-01-10 21:06:18,427] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:06:18,427] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:06:18,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:06:18,428] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:06:19,392] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30390
[2025-01-10 21:06:19,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:06:19,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:06:19,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30390
[2025-01-10 21:06:19,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [21]  [ 910/1404]  eta: 0:04:55  lr: 0.000050  min_lr: 0.000000  loss: 3.6328 (3.6929)  loss_scale: 32768.0000 (33451.4160)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0007  max mem: 15572
Epoch: [21]  [ 920/1404]  eta: 0:04:49  lr: 0.000050  min_lr: 0.000000  loss: 3.6328 (3.6931)  loss_scale: 32768.0000 (33443.9957)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0007  max mem: 15572
Epoch: [21]  [ 930/1404]  eta: 0:04:43  lr: 0.000050  min_lr: 0.000000  loss: 3.5236 (3.6897)  loss_scale: 32768.0000 (33436.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0008  max mem: 15572
Epoch: [21]  [ 940/1404]  eta: 0:04:37  lr: 0.000050  min_lr: 0.000000  loss: 3.7320 (3.6913)  loss_scale: 32768.0000 (33429.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0009  max mem: 15572
Epoch: [21]  [ 950/1404]  eta: 0:04:31  lr: 0.000050  min_lr: 0.000000  loss: 3.8044 (3.6914)  loss_scale: 32768.0000 (33422.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0008  max mem: 15572
Epoch: [21]  [ 960/1404]  eta: 0:04:24  lr: 0.000050  min_lr: 0.000000  loss: 3.7190 (3.6926)  loss_scale: 32768.0000 (33415.8585)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0008  max mem: 15572
Epoch: [21]  [ 970/1404]  eta: 0:04:19  lr: 0.000050  min_lr: 0.000000  loss: 3.4803 (3.6893)  loss_scale: 32768.0000 (33409.1864)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0008  max mem: 15572
Epoch: [21]  [ 980/1404]  eta: 0:04:12  lr: 0.000050  min_lr: 0.000000  loss: 3.4709 (3.6897)  loss_scale: 32768.0000 (33402.6504)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0007  max mem: 15572
Epoch: [21]  [ 990/1404]  eta: 0:04:06  lr: 0.000050  min_lr: 0.000000  loss: 3.6548 (3.6901)  loss_scale: 32768.0000 (33396.2462)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0008  max mem: 15572
Epoch: [21]  [1000/1404]  eta: 0:04:00  lr: 0.000050  min_lr: 0.000000  loss: 3.5417 (3.6893)  loss_scale: 32768.0000 (33389.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0009  max mem: 15572
Epoch: [21]  [1010/1404]  eta: 0:03:54  lr: 0.000050  min_lr: 0.000000  loss: 3.6944 (3.6902)  loss_scale: 32768.0000 (33383.8180)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0010  max mem: 15572
Epoch: [21]  [1020/1404]  eta: 0:03:49  lr: 0.000050  min_lr: 0.000000  loss: 3.6722 (3.6900)  loss_scale: 32768.0000 (33377.7865)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0009  max mem: 15572
Epoch: [21]  [1030/1404]  eta: 0:03:43  lr: 0.000050  min_lr: 0.000000  loss: 3.4704 (3.6882)  loss_scale: 32768.0000 (33371.8720)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0007  max mem: 15572
[2025-01-10 21:07:36,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:07:36,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:07:36,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:07:36,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1040/1404]  eta: 0:03:37  lr: 0.000050  min_lr: 0.000000  loss: 3.5384 (3.6866)  loss_scale: 32768.0000 (33554.9356)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0007  max mem: 15572
[2025-01-10 21:07:41,320] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30528
[2025-01-10 21:07:41,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:07:41,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:07:41,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30528
[2025-01-10 21:07:41,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [21]  [1050/1404]  eta: 0:03:30  lr: 0.000050  min_lr: 0.000000  loss: 3.6058 (3.6849)  loss_scale: 32768.0000 (33640.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0006  max mem: 15572
Epoch: [21]  [1060/1404]  eta: 0:03:24  lr: 0.000050  min_lr: 0.000000  loss: 3.7507 (3.6862)  loss_scale: 32768.0000 (33632.7540)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0006  max mem: 15572
Epoch: [21]  [1070/1404]  eta: 0:03:19  lr: 0.000050  min_lr: 0.000000  loss: 3.7507 (3.6854)  loss_scale: 32768.0000 (33624.6797)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0006  max mem: 15572
Epoch: [21]  [1080/1404]  eta: 0:03:13  lr: 0.000050  min_lr: 0.000000  loss: 3.4633 (3.6834)  loss_scale: 32768.0000 (33616.7549)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0005  max mem: 15572
Epoch: [21]  [1090/1404]  eta: 0:03:07  lr: 0.000050  min_lr: 0.000000  loss: 3.4633 (3.6825)  loss_scale: 32768.0000 (33608.9753)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0006  max mem: 15572
Epoch: [21]  [1100/1404]  eta: 0:03:01  lr: 0.000050  min_lr: 0.000000  loss: 3.6143 (3.6837)  loss_scale: 32768.0000 (33601.3370)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0009  max mem: 15572
Epoch: [21]  [1110/1404]  eta: 0:02:55  lr: 0.000050  min_lr: 0.000000  loss: 3.8528 (3.6852)  loss_scale: 32768.0000 (33593.8362)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0008  max mem: 15572
Epoch: [21]  [1120/1404]  eta: 0:02:49  lr: 0.000050  min_lr: 0.000000  loss: 3.7751 (3.6846)  loss_scale: 32768.0000 (33586.4692)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0006  max mem: 15572
Epoch: [21]  [1130/1404]  eta: 0:02:43  lr: 0.000050  min_lr: 0.000000  loss: 3.7751 (3.6857)  loss_scale: 32768.0000 (33579.2325)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0008  max mem: 15572
Epoch: [21]  [1140/1404]  eta: 0:02:37  lr: 0.000050  min_lr: 0.000000  loss: 3.8471 (3.6870)  loss_scale: 32768.0000 (33572.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0007  max mem: 15572
Epoch: [21]  [1150/1404]  eta: 0:02:30  lr: 0.000050  min_lr: 0.000000  loss: 3.8471 (3.6866)  loss_scale: 32768.0000 (33565.1364)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0008  max mem: 15572
Epoch: [21]  [1160/1404]  eta: 0:02:24  lr: 0.000050  min_lr: 0.000000  loss: 3.5387 (3.6863)  loss_scale: 32768.0000 (33558.2705)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0010  max mem: 15572
Epoch: [21]  [1170/1404]  eta: 0:02:19  lr: 0.000050  min_lr: 0.000000  loss: 3.6585 (3.6878)  loss_scale: 32768.0000 (33551.5218)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0009  max mem: 15572
[2025-01-10 21:08:56,592] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:08:56,592] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:08:56,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:08:56,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:08:59,158] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30662
[2025-01-10 21:08:59,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:08:59,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30662
[2025-01-10 21:08:59,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:08:59,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1180/1404]  eta: 0:02:13  lr: 0.000050  min_lr: 0.000000  loss: 3.8900 (3.6900)  loss_scale: 32768.0000 (33683.6173)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0513  max mem: 15572
Epoch: [21]  [1190/1404]  eta: 0:02:07  lr: 0.000050  min_lr: 0.000000  loss: 3.6695 (3.6882)  loss_scale: 32768.0000 (33675.9295)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0998  max mem: 15572
Epoch: [21]  [1200/1404]  eta: 0:02:01  lr: 0.000050  min_lr: 0.000000  loss: 3.3850 (3.6862)  loss_scale: 32768.0000 (33668.3697)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0494  max mem: 15572
Epoch: [21]  [1210/1404]  eta: 0:01:55  lr: 0.000050  min_lr: 0.000000  loss: 3.4661 (3.6856)  loss_scale: 32768.0000 (33660.9348)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0011  max mem: 15572
Epoch: [21]  [1220/1404]  eta: 0:01:49  lr: 0.000050  min_lr: 0.000000  loss: 3.5339 (3.6854)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0010  max mem: 15572
Epoch: [21]  [1230/1404]  eta: 0:01:43  lr: 0.000050  min_lr: 0.000000  loss: 3.7566 (3.6877)  loss_scale: 32768.0000 (33646.4273)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0008  max mem: 15572
Epoch: [21]  [1240/1404]  eta: 0:01:37  lr: 0.000050  min_lr: 0.000000  loss: 3.7566 (3.6871)  loss_scale: 32768.0000 (33639.3489)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0008  max mem: 15572
Epoch: [21]  [1250/1404]  eta: 0:01:31  lr: 0.000049  min_lr: 0.000000  loss: 3.6832 (3.6878)  loss_scale: 32768.0000 (33632.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0006  max mem: 15572
Epoch: [21]  [1260/1404]  eta: 0:01:25  lr: 0.000049  min_lr: 0.000000  loss: 3.7485 (3.6890)  loss_scale: 32768.0000 (33625.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0008  max mem: 15572
Epoch: [21]  [1270/1404]  eta: 0:01:19  lr: 0.000049  min_lr: 0.000000  loss: 3.7485 (3.6894)  loss_scale: 32768.0000 (33618.7821)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0010  max mem: 15572
Epoch: [21]  [1280/1404]  eta: 0:01:13  lr: 0.000049  min_lr: 0.000000  loss: 3.8181 (3.6910)  loss_scale: 32768.0000 (33612.1405)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0483  max mem: 15572
Epoch: [21]  [1290/1404]  eta: 0:01:07  lr: 0.000049  min_lr: 0.000000  loss: 4.0137 (3.6925)  loss_scale: 32768.0000 (33605.6019)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0809  max mem: 15572
Epoch: [21]  [1300/1404]  eta: 0:01:01  lr: 0.000049  min_lr: 0.000000  loss: 3.9255 (3.6936)  loss_scale: 32768.0000 (33599.1637)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.0332  max mem: 15572
[2025-01-10 21:10:16,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:10:16,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:10:16,104] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:10:16,105] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:10:18,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30793
[2025-01-10 21:10:18,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30793
[2025-01-10 21:10:18,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:10:18,483] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:10:18,483] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1310/1404]  eta: 0:00:55  lr: 0.000049  min_lr: 0.000000  loss: 3.9255 (3.6952)  loss_scale: 32768.0000 (33642.8131)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0005  max mem: 15572
Epoch: [21]  [1320/1404]  eta: 0:00:49  lr: 0.000049  min_lr: 0.000000  loss: 3.9053 (3.6962)  loss_scale: 32768.0000 (33636.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0006  max mem: 15572
Epoch: [21]  [1330/1404]  eta: 0:00:44  lr: 0.000049  min_lr: 0.000000  loss: 3.9903 (3.6991)  loss_scale: 32768.0000 (33629.6679)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0414  max mem: 15572
Epoch: [21]  [1340/1404]  eta: 0:00:38  lr: 0.000049  min_lr: 0.000000  loss: 4.0050 (3.7017)  loss_scale: 32768.0000 (33623.2424)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0479  max mem: 15572
Epoch: [21]  [1350/1404]  eta: 0:00:32  lr: 0.000049  min_lr: 0.000000  loss: 3.8928 (3.7016)  loss_scale: 32768.0000 (33616.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0072  max mem: 15572
Epoch: [21]  [1360/1404]  eta: 0:00:26  lr: 0.000049  min_lr: 0.000000  loss: 3.7599 (3.7013)  loss_scale: 32768.0000 (33610.6745)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0007  max mem: 15572
Epoch: [21]  [1370/1404]  eta: 0:00:20  lr: 0.000049  min_lr: 0.000000  loss: 3.5482 (3.7005)  loss_scale: 32768.0000 (33604.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0007  max mem: 15572
Epoch: [21]  [1380/1404]  eta: 0:00:14  lr: 0.000049  min_lr: 0.000000  loss: 3.7302 (3.7011)  loss_scale: 32768.0000 (33598.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0007  max mem: 15572
Epoch: [21]  [1390/1404]  eta: 0:00:08  lr: 0.000049  min_lr: 0.000000  loss: 3.7302 (3.7016)  loss_scale: 32768.0000 (33592.5004)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0016  max mem: 15572
Epoch: [21]  [1400/1404]  eta: 0:00:02  lr: 0.000049  min_lr: 0.000000  loss: 3.6380 (3.7021)  loss_scale: 32768.0000 (33586.6153)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0013  max mem: 15572
Epoch: [21]  [1403/1404]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000000  loss: 3.6380 (3.7022)  loss_scale: 32768.0000 (33584.8661)  weight_decay: 0.0500 (0.0500)  time: 0.4545  data: 0.0012  max mem: 15572
Epoch: [21] Total time: 0:13:52 (0.5932 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000000  loss: 3.6380 (3.6998)  loss_scale: 32768.0000 (33584.8661)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:49  loss: 1.5516 (1.5516)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 6.9783  data: 6.6975  max mem: 15572
Val:  [ 10/136]  eta: 0:01:57  loss: 2.1735 (2.1779)  acc1: 55.5556 (50.5051)  acc5: 77.7778 (76.7677)  time: 0.9334  data: 0.7264  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.3456 (2.3039)  acc1: 50.0000 (46.2963)  acc5: 77.7778 (76.7196)  time: 0.2688  data: 0.0650  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.3350 (2.1671)  acc1: 50.0000 (49.8208)  acc5: 83.3333 (77.9570)  time: 0.2451  data: 0.0459  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.8269 (2.1046)  acc1: 61.1111 (51.2195)  acc5: 83.3333 (78.7263)  time: 0.3377  data: 0.1482  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 1.8800 (2.1270)  acc1: 50.0000 (50.9804)  acc5: 83.3333 (78.7582)  time: 0.4323  data: 0.2395  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.2064 (2.2121)  acc1: 38.8889 (47.9053)  acc5: 77.7778 (77.5046)  time: 0.4192  data: 0.2222  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.0605 (2.1718)  acc1: 44.4444 (48.7480)  acc5: 77.7778 (78.5603)  time: 0.2959  data: 0.1023  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9082 (2.1700)  acc1: 50.0000 (48.6283)  acc5: 83.3333 (78.8066)  time: 0.3072  data: 0.1141  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0980 (2.1823)  acc1: 44.4444 (47.9243)  acc5: 77.7778 (78.6325)  time: 0.4176  data: 0.2190  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3914 (2.2568)  acc1: 38.8889 (46.0396)  acc5: 77.7778 (76.6777)  time: 0.4134  data: 0.2037  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.4222 (2.2489)  acc1: 44.4444 (46.3964)  acc5: 72.2222 (76.6266)  time: 0.3966  data: 0.1814  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.9237 (2.1944)  acc1: 61.1111 (47.7961)  acc5: 83.3333 (77.5941)  time: 0.3158  data: 0.1196  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6550 (2.1506)  acc1: 61.1111 (48.5157)  acc5: 88.8889 (78.2443)  time: 0.1825  data: 0.0200  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8453 (2.1478)  acc1: 55.5556 (48.7305)  acc5: 88.8889 (78.5012)  time: 0.1667  data: 0.0199  max mem: 15572
Val: Total time: 0:00:50 (0.3721 s / it)
* Acc@1 47.461 Acc@5 77.191 loss 2.205
Accuracy of the network on the 4883 val videos: 47.5%
[2025-01-10 21:12:00,805] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 21:12:00,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 21:12:00,808] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 21:12:00,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 21:12:03,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 21:12:03,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.46%
Epoch: [22]  [   0/1404]  eta: 3:18:29  lr: 0.000049  min_lr: 0.000000  loss: 3.6316 (3.6316)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4825  data: 7.0486  max mem: 15572
Epoch: [22]  [  10/1404]  eta: 0:31:20  lr: 0.000049  min_lr: 0.000000  loss: 3.6600 (3.5456)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3492  data: 0.7643  max mem: 15572
Epoch: [22]  [  20/1404]  eta: 0:22:04  lr: 0.000049  min_lr: 0.000000  loss: 3.4728 (3.5106)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.0825  max mem: 15572
Epoch: [22]  [  30/1404]  eta: 0:19:25  lr: 0.000049  min_lr: 0.000000  loss: 3.5367 (3.5887)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0685  max mem: 15572
[2025-01-10 21:12:31,407] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:12:31,408] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:12:31,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:12:31,502] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:12:33,921] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30927
[2025-01-10 21:12:33,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:12:33,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:12:33,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30927
[2025-01-10 21:12:33,925] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [  40/1404]  eta: 0:17:27  lr: 0.000049  min_lr: 0.000000  loss: 3.8437 (3.6144)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0726  max mem: 15572
Epoch: [22]  [  50/1404]  eta: 0:16:23  lr: 0.000049  min_lr: 0.000000  loss: 3.8257 (3.6386)  loss_scale: 32768.0000 (35980.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0560  max mem: 15572
Epoch: [22]  [  60/1404]  eta: 0:15:52  lr: 0.000049  min_lr: 0.000000  loss: 3.7602 (3.6064)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0885  max mem: 15572
Epoch: [22]  [  70/1404]  eta: 0:15:45  lr: 0.000049  min_lr: 0.000000  loss: 3.6412 (3.5977)  loss_scale: 32768.0000 (35075.6056)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.1495  max mem: 15572
Epoch: [22]  [  80/1404]  eta: 0:15:33  lr: 0.000049  min_lr: 0.000000  loss: 3.8028 (3.6146)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.6956  data: 0.2005  max mem: 15572
Epoch: [22]  [  90/1404]  eta: 0:14:57  lr: 0.000049  min_lr: 0.000000  loss: 3.9114 (3.6346)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1024  max mem: 15572
Epoch: [22]  [ 100/1404]  eta: 0:14:25  lr: 0.000049  min_lr: 0.000000  loss: 3.6662 (3.6181)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0005  max mem: 15572
Epoch: [22]  [ 110/1404]  eta: 0:14:01  lr: 0.000049  min_lr: 0.000000  loss: 3.6662 (3.6097)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0008  max mem: 15572
[2025-01-10 21:13:16,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=205, lr=[4.717601933451567e-07, 4.717601933451567e-07, 6.73943133350224e-07, 6.73943133350224e-07, 9.627759047860343e-07, 9.627759047860343e-07, 1.3753941496943348e-06, 1.3753941496943348e-06, 1.964848785277621e-06, 1.964848785277621e-06, 2.8069268361108873e-06, 2.8069268361108873e-06, 4.009895480158411e-06, 4.009895480158411e-06, 5.728422114512017e-06, 5.728422114512017e-06, 8.183460163588594e-06, 8.183460163588594e-06, 1.1690657376555136e-05, 1.1690657376555136e-05, 1.670093910936448e-05, 1.670093910936448e-05, 2.385848444194926e-05, 2.385848444194926e-05, 3.408354920278466e-05, 3.408354920278466e-05, 4.869078457540666e-05, 4.869078457540666e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:13:16,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=45.55783879992218, CurrSamplesPerSec=45.48622760443732, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [ 120/1404]  eta: 0:13:46  lr: 0.000049  min_lr: 0.000000  loss: 3.6212 (3.5966)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0012  max mem: 15572
Epoch: [22]  [ 130/1404]  eta: 0:13:30  lr: 0.000049  min_lr: 0.000000  loss: 3.7015 (3.6068)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0128  max mem: 15572
Epoch: [22]  [ 140/1404]  eta: 0:13:22  lr: 0.000049  min_lr: 0.000000  loss: 3.7535 (3.6028)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0666  max mem: 15572
Epoch: [22]  [ 150/1404]  eta: 0:13:14  lr: 0.000049  min_lr: 0.000000  loss: 3.6703 (3.6149)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0990  max mem: 15572
Epoch: [22]  [ 160/1404]  eta: 0:13:03  lr: 0.000049  min_lr: 0.000000  loss: 4.0226 (3.6441)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0770  max mem: 15572
[2025-01-10 21:13:49,634] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:13:49,635] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:13:49,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:13:49,641] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:13:51,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31058
[2025-01-10 21:13:51,181] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:13:51,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 170/1404]  eta: 0:12:58  lr: 0.000049  min_lr: 0.000000  loss: 4.0785 (3.6535)  loss_scale: 32768.0000 (34109.3801)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0965  max mem: 15572
[2025-01-10 21:13:51,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31058
[2025-01-10 21:13:51,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [ 180/1404]  eta: 0:12:52  lr: 0.000048  min_lr: 0.000000  loss: 3.9315 (3.6633)  loss_scale: 32768.0000 (34035.2707)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1335  max mem: 15572
Epoch: [22]  [ 190/1404]  eta: 0:12:43  lr: 0.000048  min_lr: 0.000000  loss: 3.7690 (3.6581)  loss_scale: 32768.0000 (33968.9215)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1070  max mem: 15572
Epoch: [22]  [ 200/1404]  eta: 0:12:37  lr: 0.000048  min_lr: 0.000000  loss: 3.6046 (3.6592)  loss_scale: 32768.0000 (33909.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1073  max mem: 15572
Epoch: [22]  [ 210/1404]  eta: 0:12:26  lr: 0.000048  min_lr: 0.000000  loss: 3.6383 (3.6584)  loss_scale: 32768.0000 (33855.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0887  max mem: 15572
Epoch: [22]  [ 220/1404]  eta: 0:12:18  lr: 0.000048  min_lr: 0.000000  loss: 3.7525 (3.6647)  loss_scale: 32768.0000 (33805.9005)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0587  max mem: 15572
Epoch: [22]  [ 230/1404]  eta: 0:12:09  lr: 0.000048  min_lr: 0.000000  loss: 3.7615 (3.6673)  loss_scale: 32768.0000 (33760.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0615  max mem: 15572
Epoch: [22]  [ 240/1404]  eta: 0:12:00  lr: 0.000048  min_lr: 0.000000  loss: 3.7894 (3.6717)  loss_scale: 32768.0000 (33719.7676)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0423  max mem: 15572
Epoch: [22]  [ 250/1404]  eta: 0:11:53  lr: 0.000048  min_lr: 0.000000  loss: 3.7238 (3.6730)  loss_scale: 32768.0000 (33681.8486)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0494  max mem: 15572
Epoch: [22]  [ 260/1404]  eta: 0:11:45  lr: 0.000048  min_lr: 0.000000  loss: 3.6269 (3.6671)  loss_scale: 32768.0000 (33646.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0773  max mem: 15572
Epoch: [22]  [ 270/1404]  eta: 0:11:35  lr: 0.000048  min_lr: 0.000000  loss: 3.6194 (3.6684)  loss_scale: 32768.0000 (33614.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0649  max mem: 15572
Epoch: [22]  [ 280/1404]  eta: 0:11:25  lr: 0.000048  min_lr: 0.000000  loss: 3.4837 (3.6616)  loss_scale: 32768.0000 (33584.2847)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0166  max mem: 15572
Epoch: [22]  [ 290/1404]  eta: 0:11:19  lr: 0.000048  min_lr: 0.000000  loss: 3.5714 (3.6605)  loss_scale: 32768.0000 (33556.2337)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0610  max mem: 15572
[2025-01-10 21:15:06,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:15:06,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:15:06,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:15:06,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 300/1404]  eta: 0:11:14  lr: 0.000048  min_lr: 0.000000  loss: 3.6523 (3.6566)  loss_scale: 32768.0000 (33747.7741)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0614  max mem: 15572
[2025-01-10 21:15:08,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31190
[2025-01-10 21:15:08,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:15:08,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:15:08,421] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31190
[2025-01-10 21:15:08,421] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [ 310/1404]  eta: 0:11:07  lr: 0.000048  min_lr: 0.000000  loss: 3.4600 (3.6553)  loss_scale: 32768.0000 (33821.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0344  max mem: 15572
Epoch: [22]  [ 320/1404]  eta: 0:11:01  lr: 0.000048  min_lr: 0.000000  loss: 3.4600 (3.6546)  loss_scale: 32768.0000 (33788.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0802  max mem: 15572
Epoch: [22]  [ 330/1404]  eta: 0:10:53  lr: 0.000048  min_lr: 0.000000  loss: 3.4674 (3.6512)  loss_scale: 32768.0000 (33757.9698)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0469  max mem: 15572
Epoch: [22]  [ 340/1404]  eta: 0:10:46  lr: 0.000048  min_lr: 0.000000  loss: 3.6702 (3.6520)  loss_scale: 32768.0000 (33728.9384)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0420  max mem: 15572
Epoch: [22]  [ 350/1404]  eta: 0:10:41  lr: 0.000048  min_lr: 0.000000  loss: 3.8904 (3.6570)  loss_scale: 32768.0000 (33701.5613)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0804  max mem: 15572
Epoch: [22]  [ 360/1404]  eta: 0:10:36  lr: 0.000048  min_lr: 0.000000  loss: 3.8904 (3.6568)  loss_scale: 32768.0000 (33675.7008)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0954  max mem: 15572
Epoch: [22]  [ 370/1404]  eta: 0:10:28  lr: 0.000048  min_lr: 0.000000  loss: 3.5964 (3.6576)  loss_scale: 32768.0000 (33651.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0832  max mem: 15572
Epoch: [22]  [ 380/1404]  eta: 0:10:21  lr: 0.000048  min_lr: 0.000000  loss: 3.6466 (3.6563)  loss_scale: 32768.0000 (33628.0525)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0538  max mem: 15572
Epoch: [22]  [ 390/1404]  eta: 0:10:14  lr: 0.000048  min_lr: 0.000000  loss: 3.7325 (3.6560)  loss_scale: 32768.0000 (33606.0563)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0379  max mem: 15572
Epoch: [22]  [ 400/1404]  eta: 0:10:08  lr: 0.000048  min_lr: 0.000000  loss: 3.5510 (3.6538)  loss_scale: 32768.0000 (33585.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0114  max mem: 15572
Epoch: [22]  [ 410/1404]  eta: 0:10:02  lr: 0.000048  min_lr: 0.000000  loss: 3.4696 (3.6453)  loss_scale: 32768.0000 (33565.2749)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0007  max mem: 15572
Epoch: [22]  [ 420/1404]  eta: 0:09:55  lr: 0.000048  min_lr: 0.000000  loss: 3.5155 (3.6456)  loss_scale: 32768.0000 (33546.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0006  max mem: 15572
Epoch: [22]  [ 430/1404]  eta: 0:09:50  lr: 0.000048  min_lr: 0.000000  loss: 3.8156 (3.6500)  loss_scale: 32768.0000 (33528.2784)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0008  max mem: 15572
[2025-01-10 21:16:25,009] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:16:25,009] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:16:25,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:16:25,015] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:16:26,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31322
[2025-01-10 21:16:26,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:16:26,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31322
[2025-01-10 21:16:26,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:16:26,648] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 440/1404]  eta: 0:09:43  lr: 0.000048  min_lr: 0.000000  loss: 3.7125 (3.6500)  loss_scale: 32768.0000 (33733.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0007  max mem: 15572
Epoch: [22]  [ 450/1404]  eta: 0:09:35  lr: 0.000048  min_lr: 0.000000  loss: 3.8511 (3.6569)  loss_scale: 32768.0000 (33712.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0007  max mem: 15572
Epoch: [22]  [ 460/1404]  eta: 0:09:31  lr: 0.000048  min_lr: 0.000000  loss: 3.7407 (3.6554)  loss_scale: 32768.0000 (33692.0434)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0007  max mem: 15572
Epoch: [22]  [ 470/1404]  eta: 0:09:24  lr: 0.000048  min_lr: 0.000000  loss: 3.4613 (3.6511)  loss_scale: 32768.0000 (33672.4246)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0007  max mem: 15572
Epoch: [22]  [ 480/1404]  eta: 0:09:18  lr: 0.000048  min_lr: 0.000000  loss: 3.7521 (3.6526)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0008  max mem: 15572
Epoch: [22]  [ 490/1404]  eta: 0:09:11  lr: 0.000048  min_lr: 0.000000  loss: 3.8668 (3.6543)  loss_scale: 32768.0000 (33635.5845)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0010  max mem: 15572
Epoch: [22]  [ 500/1404]  eta: 0:09:04  lr: 0.000048  min_lr: 0.000000  loss: 3.8668 (3.6591)  loss_scale: 32768.0000 (33618.2675)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0008  max mem: 15572
Epoch: [22]  [ 510/1404]  eta: 0:08:59  lr: 0.000047  min_lr: 0.000000  loss: 3.8319 (3.6597)  loss_scale: 32768.0000 (33601.6282)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0315  max mem: 15572
Epoch: [22]  [ 520/1404]  eta: 0:08:52  lr: 0.000047  min_lr: 0.000000  loss: 3.7333 (3.6585)  loss_scale: 32768.0000 (33585.6276)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0314  max mem: 15572
Epoch: [22]  [ 530/1404]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 3.5308 (3.6581)  loss_scale: 32768.0000 (33570.2298)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0004  max mem: 15572
Epoch: [22]  [ 540/1404]  eta: 0:08:40  lr: 0.000047  min_lr: 0.000000  loss: 3.5804 (3.6593)  loss_scale: 32768.0000 (33555.4011)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0004  max mem: 15572
Epoch: [22]  [ 550/1404]  eta: 0:08:33  lr: 0.000047  min_lr: 0.000000  loss: 3.7358 (3.6628)  loss_scale: 32768.0000 (33541.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0004  max mem: 15572
Epoch: [22]  [ 560/1404]  eta: 0:08:29  lr: 0.000047  min_lr: 0.000000  loss: 3.8144 (3.6613)  loss_scale: 32768.0000 (33527.3298)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0006  max mem: 15572
[2025-01-10 21:17:43,128] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:17:43,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:17:43,194] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:17:43,194] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:17:43,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31452
[2025-01-10 21:17:43,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:17:43,692] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31452
[2025-01-10 21:17:43,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:17:43,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 570/1404]  eta: 0:08:21  lr: 0.000047  min_lr: 0.000000  loss: 3.8144 (3.6657)  loss_scale: 32768.0000 (33571.4186)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0008  max mem: 15572
Epoch: [22]  [ 580/1404]  eta: 0:08:14  lr: 0.000047  min_lr: 0.000000  loss: 3.9355 (3.6650)  loss_scale: 32768.0000 (33557.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0010  max mem: 15572
Epoch: [22]  [ 590/1404]  eta: 0:08:09  lr: 0.000047  min_lr: 0.000000  loss: 3.6772 (3.6647)  loss_scale: 32768.0000 (33544.2301)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0009  max mem: 15572
Epoch: [22]  [ 600/1404]  eta: 0:08:03  lr: 0.000047  min_lr: 0.000000  loss: 3.6588 (3.6630)  loss_scale: 32768.0000 (33531.3145)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0007  max mem: 15572
Epoch: [22]  [ 610/1404]  eta: 0:07:56  lr: 0.000047  min_lr: 0.000000  loss: 3.3486 (3.6612)  loss_scale: 32768.0000 (33518.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0007  max mem: 15572
Epoch: [22]  [ 620/1404]  eta: 0:07:51  lr: 0.000047  min_lr: 0.000000  loss: 3.6049 (3.6631)  loss_scale: 32768.0000 (33506.7311)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0006  max mem: 15572
Epoch: [22]  [ 630/1404]  eta: 0:07:44  lr: 0.000047  min_lr: 0.000000  loss: 3.7234 (3.6665)  loss_scale: 32768.0000 (33495.0238)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0007  max mem: 15572
Epoch: [22]  [ 640/1404]  eta: 0:07:38  lr: 0.000047  min_lr: 0.000000  loss: 3.9613 (3.6715)  loss_scale: 32768.0000 (33483.6817)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0007  max mem: 15572
Epoch: [22]  [ 650/1404]  eta: 0:07:32  lr: 0.000047  min_lr: 0.000000  loss: 3.8551 (3.6683)  loss_scale: 32768.0000 (33472.6882)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0006  max mem: 15572
Epoch: [22]  [ 660/1404]  eta: 0:07:25  lr: 0.000047  min_lr: 0.000000  loss: 3.6419 (3.6663)  loss_scale: 32768.0000 (33462.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0006  max mem: 15572
Epoch: [22]  [ 670/1404]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 3.4670 (3.6660)  loss_scale: 32768.0000 (33451.6841)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0006  max mem: 15572
Epoch: [22]  [ 680/1404]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 3.6575 (3.6687)  loss_scale: 32768.0000 (33441.6446)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0008  max mem: 15572
Epoch: [22]  [ 690/1404]  eta: 0:07:08  lr: 0.000047  min_lr: 0.000000  loss: 3.9550 (3.6724)  loss_scale: 32768.0000 (33431.8958)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0009  max mem: 15572
[2025-01-10 21:18:59,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:18:59,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:18:59,108] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:18:59,109] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:19:03,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31588
[2025-01-10 21:19:03,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:19:03,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:19:03,463] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31588
[2025-01-10 21:19:03,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [ 700/1404]  eta: 0:07:01  lr: 0.000047  min_lr: 0.000000  loss: 3.9550 (3.6737)  loss_scale: 32768.0000 (33749.6377)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0006  max mem: 15572
Epoch: [22]  [ 710/1404]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 3.5945 (3.6744)  loss_scale: 32768.0000 (33735.8312)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0005  max mem: 15572
Epoch: [22]  [ 720/1404]  eta: 0:06:49  lr: 0.000047  min_lr: 0.000000  loss: 3.4996 (3.6729)  loss_scale: 32768.0000 (33722.4078)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0006  max mem: 15572
Epoch: [22]  [ 730/1404]  eta: 0:06:42  lr: 0.000047  min_lr: 0.000000  loss: 3.8102 (3.6766)  loss_scale: 32768.0000 (33709.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0007  max mem: 15572
Epoch: [22]  [ 740/1404]  eta: 0:06:36  lr: 0.000047  min_lr: 0.000000  loss: 3.7322 (3.6760)  loss_scale: 32768.0000 (33696.6478)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0010  max mem: 15572
Epoch: [22]  [ 750/1404]  eta: 0:06:30  lr: 0.000047  min_lr: 0.000000  loss: 3.6263 (3.6776)  loss_scale: 32768.0000 (33684.2823)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0051  max mem: 15572
Epoch: [22]  [ 760/1404]  eta: 0:06:24  lr: 0.000047  min_lr: 0.000000  loss: 3.7911 (3.6797)  loss_scale: 32768.0000 (33672.2418)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.0049  max mem: 15572
Epoch: [22]  [ 770/1404]  eta: 0:06:18  lr: 0.000047  min_lr: 0.000000  loss: 3.7706 (3.6795)  loss_scale: 32768.0000 (33660.5136)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0010  max mem: 15572
Epoch: [22]  [ 780/1404]  eta: 0:06:12  lr: 0.000047  min_lr: 0.000000  loss: 3.6387 (3.6804)  loss_scale: 32768.0000 (33649.0858)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0013  max mem: 15572
Epoch: [22]  [ 790/1404]  eta: 0:06:06  lr: 0.000047  min_lr: 0.000000  loss: 3.6128 (3.6793)  loss_scale: 32768.0000 (33637.9469)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0012  max mem: 15572
Epoch: [22]  [ 800/1404]  eta: 0:06:00  lr: 0.000047  min_lr: 0.000000  loss: 3.6845 (3.6815)  loss_scale: 32768.0000 (33627.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0011  max mem: 15572
Epoch: [22]  [ 810/1404]  eta: 0:05:54  lr: 0.000047  min_lr: 0.000000  loss: 3.8955 (3.6828)  loss_scale: 32768.0000 (33616.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0009  max mem: 15572
Epoch: [22]  [ 820/1404]  eta: 0:05:47  lr: 0.000047  min_lr: 0.000000  loss: 3.8416 (3.6838)  loss_scale: 32768.0000 (33606.1583)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0005  max mem: 15572
[2025-01-10 21:20:18,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:20:18,169] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:20:18,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:20:18,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:20:18,632] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31718
[2025-01-10 21:20:18,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:20:18,676] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31718
[2025-01-10 21:20:18,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:20:18,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 830/1404]  eta: 0:05:41  lr: 0.000047  min_lr: 0.000000  loss: 3.8416 (3.6850)  loss_scale: 32768.0000 (33635.5042)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0006  max mem: 15572
Epoch: [22]  [ 840/1404]  eta: 0:05:36  lr: 0.000047  min_lr: 0.000000  loss: 3.9029 (3.6867)  loss_scale: 32768.0000 (33625.1891)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0006  max mem: 15572
Epoch: [22]  [ 850/1404]  eta: 0:05:30  lr: 0.000046  min_lr: 0.000000  loss: 3.5980 (3.6825)  loss_scale: 32768.0000 (33615.1163)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0005  max mem: 15572
Epoch: [22]  [ 860/1404]  eta: 0:05:24  lr: 0.000046  min_lr: 0.000000  loss: 3.4628 (3.6829)  loss_scale: 32768.0000 (33605.2776)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0006  max mem: 15572
Epoch: [22]  [ 870/1404]  eta: 0:05:18  lr: 0.000046  min_lr: 0.000000  loss: 3.6367 (3.6822)  loss_scale: 32768.0000 (33595.6648)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0008  max mem: 15572
Epoch: [22]  [ 880/1404]  eta: 0:05:12  lr: 0.000046  min_lr: 0.000000  loss: 3.6889 (3.6855)  loss_scale: 32768.0000 (33586.2701)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0007  max mem: 15572
Epoch: [22]  [ 890/1404]  eta: 0:05:06  lr: 0.000046  min_lr: 0.000000  loss: 3.8978 (3.6860)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0008  max mem: 15572
Epoch: [22]  [ 900/1404]  eta: 0:05:00  lr: 0.000046  min_lr: 0.000000  loss: 3.7719 (3.6868)  loss_scale: 32768.0000 (33568.1065)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0011  max mem: 15572
Epoch: [22]  [ 910/1404]  eta: 0:04:54  lr: 0.000046  min_lr: 0.000000  loss: 3.7840 (3.6885)  loss_scale: 32768.0000 (33559.3238)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0009  max mem: 15572
Epoch: [22]  [ 920/1404]  eta: 0:04:48  lr: 0.000046  min_lr: 0.000000  loss: 3.7713 (3.6861)  loss_scale: 32768.0000 (33550.7318)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0009  max mem: 15572
Epoch: [22]  [ 930/1404]  eta: 0:04:42  lr: 0.000046  min_lr: 0.000000  loss: 3.6743 (3.6854)  loss_scale: 32768.0000 (33542.3244)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0010  max mem: 15572
Epoch: [22]  [ 940/1404]  eta: 0:04:36  lr: 0.000046  min_lr: 0.000000  loss: 3.5431 (3.6849)  loss_scale: 32768.0000 (33534.0956)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0009  max mem: 15572
Epoch: [22]  [ 950/1404]  eta: 0:04:30  lr: 0.000046  min_lr: 0.000000  loss: 3.5431 (3.6841)  loss_scale: 32768.0000 (33526.0400)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0007  max mem: 15572
[2025-01-10 21:21:35,444] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:21:35,445] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:21:35,445] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:21:35,445] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 960/1404]  eta: 0:04:24  lr: 0.000046  min_lr: 0.000000  loss: 3.6596 (3.6851)  loss_scale: 32768.0000 (33586.3476)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0008  max mem: 15572
Epoch: [22]  [ 970/1404]  eta: 0:04:18  lr: 0.000046  min_lr: 0.000000  loss: 3.6596 (3.6838)  loss_scale: 65536.0000 (33915.3862)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.0009  max mem: 15572
[2025-01-10 21:21:42,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31861
[2025-01-10 21:21:42,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:21:42,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:21:43,016] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31861
[2025-01-10 21:21:43,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [ 980/1404]  eta: 0:04:12  lr: 0.000046  min_lr: 0.000000  loss: 3.6855 (3.6835)  loss_scale: 65536.0000 (33970.4954)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0011  max mem: 15572
Epoch: [22]  [ 990/1404]  eta: 0:04:06  lr: 0.000046  min_lr: 0.000000  loss: 3.7100 (3.6845)  loss_scale: 32768.0000 (33958.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0011  max mem: 15572
Epoch: [22]  [1000/1404]  eta: 0:04:00  lr: 0.000046  min_lr: 0.000000  loss: 3.6927 (3.6830)  loss_scale: 32768.0000 (33946.4695)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.0008  max mem: 15572
Epoch: [22]  [1010/1404]  eta: 0:03:54  lr: 0.000046  min_lr: 0.000000  loss: 3.5021 (3.6807)  loss_scale: 32768.0000 (33934.8131)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0007  max mem: 15572
Epoch: [22]  [1020/1404]  eta: 0:03:48  lr: 0.000046  min_lr: 0.000000  loss: 3.6376 (3.6818)  loss_scale: 32768.0000 (33923.3849)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0008  max mem: 15572
Epoch: [22]  [1030/1404]  eta: 0:03:42  lr: 0.000046  min_lr: 0.000000  loss: 3.7240 (3.6820)  loss_scale: 32768.0000 (33912.1785)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0007  max mem: 15572
Epoch: [22]  [1040/1404]  eta: 0:03:36  lr: 0.000046  min_lr: 0.000000  loss: 3.6265 (3.6815)  loss_scale: 32768.0000 (33901.1873)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0009  max mem: 15572
Epoch: [22]  [1050/1404]  eta: 0:03:30  lr: 0.000046  min_lr: 0.000000  loss: 3.6800 (3.6842)  loss_scale: 32768.0000 (33890.4053)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0008  max mem: 15572
Epoch: [22]  [1060/1404]  eta: 0:03:24  lr: 0.000046  min_lr: 0.000000  loss: 3.8194 (3.6846)  loss_scale: 32768.0000 (33879.8266)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0007  max mem: 15572
Epoch: [22]  [1070/1404]  eta: 0:03:18  lr: 0.000046  min_lr: 0.000000  loss: 3.6270 (3.6831)  loss_scale: 32768.0000 (33869.4454)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0008  max mem: 15572
Epoch: [22]  [1080/1404]  eta: 0:03:12  lr: 0.000046  min_lr: 0.000000  loss: 3.6065 (3.6816)  loss_scale: 32768.0000 (33859.2562)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0007  max mem: 15572
Epoch: [22]  [1090/1404]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 3.4730 (3.6801)  loss_scale: 32768.0000 (33849.2539)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0006  max mem: 15572
Epoch: [22]  [1100/1404]  eta: 0:03:00  lr: 0.000046  min_lr: 0.000000  loss: 3.3374 (3.6780)  loss_scale: 32768.0000 (33839.4332)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0006  max mem: 15572
[2025-01-10 21:23:00,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:23:00,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:23:00,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:23:00,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:23:01,128] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31992
[2025-01-10 21:23:01,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:23:01,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:23:01,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31992
[2025-01-10 21:23:01,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [1110/1404]  eta: 0:02:54  lr: 0.000046  min_lr: 0.000000  loss: 3.7548 (3.6785)  loss_scale: 32768.0000 (33888.7777)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0007  max mem: 15572
[2025-01-10 21:23:05,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=213, lr=[4.427590782729502e-07, 4.427590782729502e-07, 6.325129689613575e-07, 6.325129689613575e-07, 9.035899556590823e-07, 9.035899556590823e-07, 1.2908427937986889e-06, 1.2908427937986889e-06, 1.844061133998127e-06, 1.844061133998127e-06, 2.634373048568753e-06, 2.634373048568753e-06, 3.7633900693839335e-06, 3.7633900693839335e-06, 5.376271527691334e-06, 5.376271527691334e-06, 7.680387896701905e-06, 7.680387896701905e-06, 1.0971982709574152e-05, 1.0971982709574152e-05, 1.567426101367736e-05, 1.567426101367736e-05, 2.2391801448110516e-05, 2.2391801448110516e-05, 3.198828778301503e-05, 3.198828778301503e-05, 4.5697553975735754e-05, 4.5697553975735754e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:23:05,554] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=45.519374272672366, CurrSamplesPerSec=49.793430806413475, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [1120/1404]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000000  loss: 3.6909 (3.6774)  loss_scale: 32768.0000 (33878.7797)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0007  max mem: 15572
Epoch: [22]  [1130/1404]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000000  loss: 3.5704 (3.6771)  loss_scale: 32768.0000 (33868.9584)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0008  max mem: 15572
Epoch: [22]  [1140/1404]  eta: 0:02:36  lr: 0.000046  min_lr: 0.000000  loss: 3.5704 (3.6743)  loss_scale: 32768.0000 (33859.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0008  max mem: 15572
Epoch: [22]  [1150/1404]  eta: 0:02:31  lr: 0.000046  min_lr: 0.000000  loss: 3.4317 (3.6736)  loss_scale: 32768.0000 (33849.8280)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0008  max mem: 15572
Epoch: [22]  [1160/1404]  eta: 0:02:25  lr: 0.000046  min_lr: 0.000000  loss: 3.4772 (3.6728)  loss_scale: 32768.0000 (33840.5099)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0009  max mem: 15572
Epoch: [22]  [1170/1404]  eta: 0:02:19  lr: 0.000046  min_lr: 0.000000  loss: 3.5814 (3.6718)  loss_scale: 32768.0000 (33831.3510)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0008  max mem: 15572
Epoch: [22]  [1180/1404]  eta: 0:02:13  lr: 0.000045  min_lr: 0.000000  loss: 3.5814 (3.6715)  loss_scale: 32768.0000 (33822.3472)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0009  max mem: 15572
Epoch: [22]  [1190/1404]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000000  loss: 3.6730 (3.6734)  loss_scale: 32768.0000 (33813.4945)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0009  max mem: 15572
Epoch: [22]  [1200/1404]  eta: 0:02:01  lr: 0.000045  min_lr: 0.000000  loss: 3.9039 (3.6727)  loss_scale: 32768.0000 (33804.7893)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0008  max mem: 15572
Epoch: [22]  [1210/1404]  eta: 0:01:55  lr: 0.000045  min_lr: 0.000000  loss: 3.8472 (3.6736)  loss_scale: 32768.0000 (33796.2279)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0008  max mem: 15572
Epoch: [22]  [1220/1404]  eta: 0:01:49  lr: 0.000045  min_lr: 0.000000  loss: 3.7968 (3.6752)  loss_scale: 32768.0000 (33787.8067)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0007  max mem: 15572
Epoch: [22]  [1230/1404]  eta: 0:01:43  lr: 0.000045  min_lr: 0.000000  loss: 3.7345 (3.6745)  loss_scale: 32768.0000 (33779.5223)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0008  max mem: 15572
[2025-01-10 21:24:16,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:24:16,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:24:16,172] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:24:16,172] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:24:18,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32126
[2025-01-10 21:24:18,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32126
[2025-01-10 21:24:18,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:24:18,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:24:18,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1240/1404]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000000  loss: 3.7354 (3.6757)  loss_scale: 32768.0000 (33903.3940)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0010  max mem: 15572
Epoch: [22]  [1250/1404]  eta: 0:01:31  lr: 0.000045  min_lr: 0.000000  loss: 3.8432 (3.6743)  loss_scale: 32768.0000 (33894.3181)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0008  max mem: 15572
Epoch: [22]  [1260/1404]  eta: 0:01:25  lr: 0.000045  min_lr: 0.000000  loss: 3.7890 (3.6759)  loss_scale: 32768.0000 (33885.3862)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0008  max mem: 15572
Epoch: [22]  [1270/1404]  eta: 0:01:19  lr: 0.000045  min_lr: 0.000000  loss: 3.8891 (3.6770)  loss_scale: 32768.0000 (33876.5948)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0012  max mem: 15572
Epoch: [22]  [1280/1404]  eta: 0:01:13  lr: 0.000045  min_lr: 0.000000  loss: 3.7529 (3.6768)  loss_scale: 32768.0000 (33867.9407)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0043  max mem: 15572
Epoch: [22]  [1290/1404]  eta: 0:01:07  lr: 0.000045  min_lr: 0.000000  loss: 3.6207 (3.6774)  loss_scale: 32768.0000 (33859.4206)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.0041  max mem: 15572
Epoch: [22]  [1300/1404]  eta: 0:01:01  lr: 0.000045  min_lr: 0.000000  loss: 3.9622 (3.6788)  loss_scale: 32768.0000 (33851.0315)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0013  max mem: 15572
Epoch: [22]  [1310/1404]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000000  loss: 3.7056 (3.6780)  loss_scale: 32768.0000 (33842.7704)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0014  max mem: 15572
Epoch: [22]  [1320/1404]  eta: 0:00:49  lr: 0.000045  min_lr: 0.000000  loss: 3.7056 (3.6795)  loss_scale: 32768.0000 (33834.6344)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0009  max mem: 15572
Epoch: [22]  [1330/1404]  eta: 0:00:43  lr: 0.000045  min_lr: 0.000000  loss: 3.7742 (3.6793)  loss_scale: 32768.0000 (33826.6206)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0006  max mem: 15572
Epoch: [22]  [1340/1404]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000000  loss: 3.7050 (3.6792)  loss_scale: 32768.0000 (33818.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0008  max mem: 15572
Epoch: [22]  [1350/1404]  eta: 0:00:32  lr: 0.000045  min_lr: 0.000000  loss: 3.4552 (3.6767)  loss_scale: 32768.0000 (33810.9489)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0009  max mem: 15572
Epoch: [22]  [1360/1404]  eta: 0:00:26  lr: 0.000045  min_lr: 0.000000  loss: 3.7904 (3.6788)  loss_scale: 32768.0000 (33803.2858)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0008  max mem: 15572
[2025-01-10 21:25:35,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:25:35,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:25:35,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:25:35,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:25:36,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32258
[2025-01-10 21:25:36,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:25:36,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1370/1404]  eta: 0:00:20  lr: 0.000045  min_lr: 0.000000  loss: 3.8983 (3.6791)  loss_scale: 32768.0000 (33867.4369)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0007  max mem: 15572
[2025-01-10 21:25:36,828] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32258
[2025-01-10 21:25:36,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [1380/1404]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000000  loss: 3.7322 (3.6791)  loss_scale: 32768.0000 (33859.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0008  max mem: 15572
Epoch: [22]  [1390/1404]  eta: 0:00:08  lr: 0.000045  min_lr: 0.000000  loss: 3.7829 (3.6808)  loss_scale: 32768.0000 (33851.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0010  max mem: 15572
Epoch: [22]  [1400/1404]  eta: 0:00:02  lr: 0.000045  min_lr: 0.000000  loss: 3.9646 (3.6804)  loss_scale: 32768.0000 (33843.8944)  weight_decay: 0.0500 (0.0500)  time: 0.4799  data: 0.0006  max mem: 15572
Epoch: [22]  [1403/1404]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 3.5414 (3.6803)  loss_scale: 32768.0000 (33841.5954)  weight_decay: 0.0500 (0.0500)  time: 0.4535  data: 0.0006  max mem: 15572
Epoch: [22] Total time: 0:13:50 (0.5915 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 3.5414 (3.6826)  loss_scale: 32768.0000 (33841.5954)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:18  loss: 1.5119 (1.5119)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.8691  data: 5.6336  max mem: 15572
Val:  [ 10/136]  eta: 0:01:41  loss: 2.1676 (2.1926)  acc1: 55.5556 (47.4747)  acc5: 77.7778 (76.7677)  time: 0.8046  data: 0.6165  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 2.2922 (2.2950)  acc1: 38.8889 (44.4444)  acc5: 77.7778 (77.2487)  time: 0.3464  data: 0.1350  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.1698 (2.1523)  acc1: 50.0000 (48.0287)  acc5: 83.3333 (79.5699)  time: 0.3591  data: 0.1433  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.8145 (2.1206)  acc1: 55.5556 (49.5935)  acc5: 83.3333 (79.8103)  time: 0.2684  data: 0.0661  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 2.0270 (2.1144)  acc1: 50.0000 (49.7821)  acc5: 83.3333 (80.3922)  time: 0.2898  data: 0.0760  max mem: 15572
Val:  [ 60/136]  eta: 0:00:31  loss: 2.1974 (2.2203)  acc1: 44.4444 (45.8106)  acc5: 77.7778 (78.5975)  time: 0.3545  data: 0.1471  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.1621 (2.1907)  acc1: 44.4444 (46.6354)  acc5: 77.7778 (79.0297)  time: 0.3924  data: 0.1734  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 1.9946 (2.1756)  acc1: 44.4444 (46.8450)  acc5: 83.3333 (79.4239)  time: 0.3899  data: 0.1768  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.1346 (2.1832)  acc1: 38.8889 (46.3980)  acc5: 83.3333 (79.1209)  time: 0.3976  data: 0.2084  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3973 (2.2554)  acc1: 33.3333 (44.6645)  acc5: 72.2222 (77.1727)  time: 0.4145  data: 0.2171  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.3973 (2.2416)  acc1: 33.3333 (45.3453)  acc5: 72.2222 (77.2272)  time: 0.3310  data: 0.1251  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8540 (2.1894)  acc1: 61.1111 (46.6942)  acc5: 83.3333 (78.0073)  time: 0.3188  data: 0.1149  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6505 (2.1431)  acc1: 61.1111 (47.8372)  acc5: 88.8889 (78.5411)  time: 0.2663  data: 0.0957  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8540 (2.1481)  acc1: 55.5556 (47.8296)  acc5: 83.3333 (78.5422)  time: 0.2067  data: 0.0587  max mem: 15572
Val: Total time: 0:00:50 (0.3708 s / it)
* Acc@1 47.502 Acc@5 77.150 loss 2.201
Accuracy of the network on the 4883 val videos: 47.5%
[2025-01-10 21:26:44,071] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 21:26:44,073] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 21:26:44,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 21:26:44,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 21:26:46,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 21:26:46,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.50%
Epoch: [23]  [   0/1404]  eta: 3:37:52  lr: 0.000045  min_lr: 0.000000  loss: 4.5281 (4.5281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3108  data: 8.7422  max mem: 15572
Epoch: [23]  [  10/1404]  eta: 0:29:56  lr: 0.000045  min_lr: 0.000000  loss: 3.9063 (3.8210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2885  data: 0.7953  max mem: 15572
Epoch: [23]  [  20/1404]  eta: 0:22:01  lr: 0.000045  min_lr: 0.000000  loss: 3.8759 (3.7936)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0426  max mem: 15572
Epoch: [23]  [  30/1404]  eta: 0:18:58  lr: 0.000045  min_lr: 0.000000  loss: 3.9184 (3.8653)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0811  max mem: 15572
Epoch: [23]  [  40/1404]  eta: 0:17:50  lr: 0.000045  min_lr: 0.000000  loss: 3.9785 (3.8490)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1051  max mem: 15572
Epoch: [23]  [  50/1404]  eta: 0:16:37  lr: 0.000045  min_lr: 0.000000  loss: 3.9703 (3.8391)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0668  max mem: 15572
Epoch: [23]  [  60/1404]  eta: 0:15:54  lr: 0.000045  min_lr: 0.000000  loss: 3.7974 (3.8080)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0335  max mem: 15572
Epoch: [23]  [  70/1404]  eta: 0:15:15  lr: 0.000045  min_lr: 0.000000  loss: 3.7066 (3.7890)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0337  max mem: 15572
Epoch: [23]  [  80/1404]  eta: 0:14:49  lr: 0.000045  min_lr: 0.000000  loss: 3.6684 (3.7493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0010  max mem: 15572
Epoch: [23]  [  90/1404]  eta: 0:14:33  lr: 0.000045  min_lr: 0.000000  loss: 3.7180 (3.7539)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0009  max mem: 15572
[2025-01-10 21:27:49,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:27:49,726] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:27:49,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:27:49,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:27:50,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32388
[2025-01-10 21:27:50,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:27:50,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:27:50,299] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32388
[2025-01-10 21:27:50,300] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [ 100/1404]  eta: 0:14:18  lr: 0.000045  min_lr: 0.000000  loss: 3.7805 (3.7443)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0160  max mem: 15572
Epoch: [23]  [ 110/1404]  eta: 0:14:06  lr: 0.000044  min_lr: 0.000000  loss: 3.6820 (3.7218)  loss_scale: 32768.0000 (33063.2072)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0159  max mem: 15572
Epoch: [23]  [ 120/1404]  eta: 0:13:51  lr: 0.000044  min_lr: 0.000000  loss: 3.6820 (3.7183)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0007  max mem: 15572
Epoch: [23]  [ 130/1404]  eta: 0:13:31  lr: 0.000044  min_lr: 0.000000  loss: 3.6481 (3.7153)  loss_scale: 32768.0000 (33018.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0005  max mem: 15572
Epoch: [23]  [ 140/1404]  eta: 0:13:30  lr: 0.000044  min_lr: 0.000000  loss: 3.5378 (3.6995)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0006  max mem: 15572
Epoch: [23]  [ 150/1404]  eta: 0:13:14  lr: 0.000044  min_lr: 0.000000  loss: 3.5766 (3.7080)  loss_scale: 32768.0000 (32985.0066)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0006  max mem: 15572
Epoch: [23]  [ 160/1404]  eta: 0:13:07  lr: 0.000044  min_lr: 0.000000  loss: 3.7153 (3.6969)  loss_scale: 32768.0000 (32971.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0005  max mem: 15572
Epoch: [23]  [ 170/1404]  eta: 0:12:51  lr: 0.000044  min_lr: 0.000000  loss: 3.6225 (3.6955)  loss_scale: 32768.0000 (32959.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0007  max mem: 15572
Epoch: [23]  [ 180/1404]  eta: 0:12:49  lr: 0.000044  min_lr: 0.000000  loss: 3.9270 (3.6989)  loss_scale: 32768.0000 (32949.0387)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0128  max mem: 15572
Epoch: [23]  [ 190/1404]  eta: 0:12:35  lr: 0.000044  min_lr: 0.000000  loss: 3.9270 (3.6961)  loss_scale: 32768.0000 (32939.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0129  max mem: 15572
Epoch: [23]  [ 200/1404]  eta: 0:12:26  lr: 0.000044  min_lr: 0.000000  loss: 3.5120 (3.6808)  loss_scale: 32768.0000 (32931.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0397  max mem: 15572
Epoch: [23]  [ 210/1404]  eta: 0:12:22  lr: 0.000044  min_lr: 0.000000  loss: 3.4515 (3.6850)  loss_scale: 32768.0000 (32923.2986)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1153  max mem: 15572
Epoch: [23]  [ 220/1404]  eta: 0:12:12  lr: 0.000044  min_lr: 0.000000  loss: 3.4515 (3.6732)  loss_scale: 32768.0000 (32916.2715)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0808  max mem: 15572
[2025-01-10 21:29:06,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:29:06,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:29:06,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:29:06,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 230/1404]  eta: 0:12:02  lr: 0.000044  min_lr: 0.000000  loss: 3.4539 (3.6746)  loss_scale: 32768.0000 (33760.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0110  max mem: 15572
[2025-01-10 21:29:11,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32524
[2025-01-10 21:29:11,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:29:11,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:29:11,127] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32524
[2025-01-10 21:29:11,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [ 240/1404]  eta: 0:11:59  lr: 0.000044  min_lr: 0.000000  loss: 3.6652 (3.6714)  loss_scale: 32768.0000 (33855.7344)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0868  max mem: 15572
Epoch: [23]  [ 250/1404]  eta: 0:11:55  lr: 0.000044  min_lr: 0.000000  loss: 3.3510 (3.6565)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.1244  max mem: 15572
Epoch: [23]  [ 260/1404]  eta: 0:11:47  lr: 0.000044  min_lr: 0.000000  loss: 3.2865 (3.6522)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.0760  max mem: 15572
Epoch: [23]  [ 270/1404]  eta: 0:11:36  lr: 0.000044  min_lr: 0.000000  loss: 3.7960 (3.6534)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0324  max mem: 15572
Epoch: [23]  [ 280/1404]  eta: 0:11:28  lr: 0.000044  min_lr: 0.000000  loss: 3.8519 (3.6592)  loss_scale: 32768.0000 (33700.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0006  max mem: 15572
Epoch: [23]  [ 290/1404]  eta: 0:11:24  lr: 0.000044  min_lr: 0.000000  loss: 3.8747 (3.6649)  loss_scale: 32768.0000 (33668.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0657  max mem: 15572
Epoch: [23]  [ 300/1404]  eta: 0:11:18  lr: 0.000044  min_lr: 0.000000  loss: 3.7944 (3.6594)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.1236  max mem: 15572
Epoch: [23]  [ 310/1404]  eta: 0:11:14  lr: 0.000044  min_lr: 0.000000  loss: 3.6381 (3.6601)  loss_scale: 32768.0000 (33610.9068)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.1273  max mem: 15572
Epoch: [23]  [ 320/1404]  eta: 0:11:05  lr: 0.000044  min_lr: 0.000000  loss: 3.7042 (3.6611)  loss_scale: 32768.0000 (33584.6480)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0695  max mem: 15572
Epoch: [23]  [ 330/1404]  eta: 0:10:56  lr: 0.000044  min_lr: 0.000000  loss: 3.6824 (3.6560)  loss_scale: 32768.0000 (33559.9758)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0009  max mem: 15572
Epoch: [23]  [ 340/1404]  eta: 0:10:49  lr: 0.000044  min_lr: 0.000000  loss: 3.5539 (3.6589)  loss_scale: 32768.0000 (33536.7507)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0363  max mem: 15572
Epoch: [23]  [ 350/1404]  eta: 0:10:42  lr: 0.000044  min_lr: 0.000000  loss: 3.9253 (3.6652)  loss_scale: 32768.0000 (33514.8490)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0623  max mem: 15572
Epoch: [23]  [ 360/1404]  eta: 0:10:36  lr: 0.000044  min_lr: 0.000000  loss: 3.9084 (3.6660)  loss_scale: 32768.0000 (33494.1607)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0922  max mem: 15572
[2025-01-10 21:30:27,098] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:30:27,098] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:30:27,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:30:27,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:30:31,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32659
[2025-01-10 21:30:31,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:30:31,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32659
[2025-01-10 21:30:31,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:30:31,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 370/1404]  eta: 0:10:30  lr: 0.000044  min_lr: 0.000000  loss: 3.4170 (3.6601)  loss_scale: 32768.0000 (34004.5283)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1113  max mem: 15572
Epoch: [23]  [ 380/1404]  eta: 0:10:21  lr: 0.000044  min_lr: 0.000000  loss: 3.6268 (3.6653)  loss_scale: 32768.0000 (33972.0735)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0525  max mem: 15572
Epoch: [23]  [ 390/1404]  eta: 0:10:15  lr: 0.000044  min_lr: 0.000000  loss: 3.8069 (3.6676)  loss_scale: 32768.0000 (33941.2788)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0505  max mem: 15572
Epoch: [23]  [ 400/1404]  eta: 0:10:06  lr: 0.000044  min_lr: 0.000000  loss: 3.7841 (3.6607)  loss_scale: 32768.0000 (33912.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0572  max mem: 15572
Epoch: [23]  [ 410/1404]  eta: 0:09:59  lr: 0.000044  min_lr: 0.000000  loss: 3.2505 (3.6530)  loss_scale: 32768.0000 (33884.1849)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0141  max mem: 15572
Epoch: [23]  [ 420/1404]  eta: 0:09:53  lr: 0.000044  min_lr: 0.000000  loss: 3.3078 (3.6534)  loss_scale: 32768.0000 (33857.6722)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0007  max mem: 15572
Epoch: [23]  [ 430/1404]  eta: 0:09:48  lr: 0.000044  min_lr: 0.000000  loss: 3.6676 (3.6546)  loss_scale: 32768.0000 (33832.3898)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0006  max mem: 15572
Epoch: [23]  [ 440/1404]  eta: 0:09:41  lr: 0.000044  min_lr: 0.000000  loss: 3.9662 (3.6624)  loss_scale: 32768.0000 (33808.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0008  max mem: 15572
Epoch: [23]  [ 450/1404]  eta: 0:09:35  lr: 0.000043  min_lr: 0.000000  loss: 3.8173 (3.6644)  loss_scale: 32768.0000 (33785.1885)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0008  max mem: 15572
Epoch: [23]  [ 460/1404]  eta: 0:09:29  lr: 0.000043  min_lr: 0.000000  loss: 3.5403 (3.6607)  loss_scale: 32768.0000 (33763.1236)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0007  max mem: 15572
Epoch: [23]  [ 470/1404]  eta: 0:09:24  lr: 0.000043  min_lr: 0.000000  loss: 3.5403 (3.6591)  loss_scale: 32768.0000 (33741.9958)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.0011  max mem: 15572
Epoch: [23]  [ 480/1404]  eta: 0:09:18  lr: 0.000043  min_lr: 0.000000  loss: 3.8831 (3.6635)  loss_scale: 32768.0000 (33721.7464)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0010  max mem: 15572
Epoch: [23]  [ 490/1404]  eta: 0:09:11  lr: 0.000043  min_lr: 0.000000  loss: 3.7725 (3.6631)  loss_scale: 32768.0000 (33702.3218)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0008  max mem: 15572
[2025-01-10 21:31:46,787] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:31:46,787] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:31:46,863] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:31:46,864] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:31:48,374] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32791
[2025-01-10 21:31:48,374] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32791
[2025-01-10 21:31:48,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:31:48,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:31:48,375] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 500/1404]  eta: 0:09:05  lr: 0.000043  min_lr: 0.000000  loss: 3.7490 (3.6668)  loss_scale: 32768.0000 (33879.8882)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0006  max mem: 15572
Epoch: [23]  [ 510/1404]  eta: 0:08:59  lr: 0.000043  min_lr: 0.000000  loss: 3.6658 (3.6630)  loss_scale: 32768.0000 (33858.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0008  max mem: 15572
Epoch: [23]  [ 520/1404]  eta: 0:08:55  lr: 0.000043  min_lr: 0.000000  loss: 3.6221 (3.6616)  loss_scale: 32768.0000 (33837.2054)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.0009  max mem: 15572
Epoch: [23]  [ 530/1404]  eta: 0:08:48  lr: 0.000043  min_lr: 0.000000  loss: 3.8517 (3.6655)  loss_scale: 32768.0000 (33817.0697)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0007  max mem: 15572
Epoch: [23]  [ 540/1404]  eta: 0:08:43  lr: 0.000043  min_lr: 0.000000  loss: 3.8435 (3.6669)  loss_scale: 32768.0000 (33797.6784)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0007  max mem: 15572
Epoch: [23]  [ 550/1404]  eta: 0:08:35  lr: 0.000043  min_lr: 0.000000  loss: 3.8004 (3.6671)  loss_scale: 32768.0000 (33778.9909)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0007  max mem: 15572
Epoch: [23]  [ 560/1404]  eta: 0:08:28  lr: 0.000043  min_lr: 0.000000  loss: 3.8670 (3.6715)  loss_scale: 32768.0000 (33760.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0007  max mem: 15572
Epoch: [23]  [ 570/1404]  eta: 0:08:22  lr: 0.000043  min_lr: 0.000000  loss: 3.8670 (3.6728)  loss_scale: 32768.0000 (33743.5797)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0008  max mem: 15572
Epoch: [23]  [ 580/1404]  eta: 0:08:17  lr: 0.000043  min_lr: 0.000000  loss: 3.6981 (3.6711)  loss_scale: 32768.0000 (33726.7883)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0007  max mem: 15572
Epoch: [23]  [ 590/1404]  eta: 0:08:10  lr: 0.000043  min_lr: 0.000000  loss: 3.3695 (3.6663)  loss_scale: 32768.0000 (33710.5651)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0006  max mem: 15572
Epoch: [23]  [ 600/1404]  eta: 0:08:05  lr: 0.000043  min_lr: 0.000000  loss: 3.4577 (3.6666)  loss_scale: 32768.0000 (33694.8819)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0008  max mem: 15572
Epoch: [23]  [ 610/1404]  eta: 0:07:58  lr: 0.000043  min_lr: 0.000000  loss: 3.7699 (3.6668)  loss_scale: 32768.0000 (33679.7119)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0008  max mem: 15572
Epoch: [23]  [ 620/1404]  eta: 0:07:52  lr: 0.000043  min_lr: 0.000000  loss: 3.7974 (3.6701)  loss_scale: 32768.0000 (33665.0306)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0008  max mem: 15572
[2025-01-10 21:33:05,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:33:05,609] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:33:05,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:33:05,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 630/1404]  eta: 0:07:46  lr: 0.000043  min_lr: 0.000000  loss: 3.7498 (3.6690)  loss_scale: 32768.0000 (33806.6054)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0009  max mem: 15572
[2025-01-10 21:33:08,713] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32926
[2025-01-10 21:33:08,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:33:08,738] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32926
[2025-01-10 21:33:08,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:33:08,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 640/1404]  eta: 0:07:40  lr: 0.000043  min_lr: 0.000000  loss: 3.4594 (3.6667)  loss_scale: 32768.0000 (33943.7629)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0011  max mem: 15572
Epoch: [23]  [ 650/1404]  eta: 0:07:33  lr: 0.000043  min_lr: 0.000000  loss: 3.7379 (3.6699)  loss_scale: 32768.0000 (33925.7020)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0011  max mem: 15572
Epoch: [23]  [ 660/1404]  eta: 0:07:27  lr: 0.000043  min_lr: 0.000000  loss: 3.7135 (3.6689)  loss_scale: 32768.0000 (33908.1876)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0010  max mem: 15572
Epoch: [23]  [ 670/1404]  eta: 0:07:22  lr: 0.000043  min_lr: 0.000000  loss: 3.5578 (3.6669)  loss_scale: 32768.0000 (33891.1952)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0007  max mem: 15572
Epoch: [23]  [ 680/1404]  eta: 0:07:15  lr: 0.000043  min_lr: 0.000000  loss: 3.5584 (3.6693)  loss_scale: 32768.0000 (33874.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0004  max mem: 15572
Epoch: [23]  [ 690/1404]  eta: 0:07:08  lr: 0.000043  min_lr: 0.000000  loss: 3.4956 (3.6644)  loss_scale: 32768.0000 (33858.6860)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0005  max mem: 15572
Epoch: [23]  [ 700/1404]  eta: 0:07:02  lr: 0.000043  min_lr: 0.000000  loss: 3.5469 (3.6653)  loss_scale: 32768.0000 (33843.1270)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0006  max mem: 15572
[2025-01-10 21:33:51,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=220, lr=[4.138064307813978e-07, 4.138064307813978e-07, 5.911520439734255e-07, 5.911520439734255e-07, 8.445029199620364e-07, 8.445029199620364e-07, 1.2064327428029093e-06, 1.2064327428029093e-06, 1.7234753468612991e-06, 1.7234753468612991e-06, 2.4621076383732845e-06, 2.4621076383732845e-06, 3.517296626247549e-06, 3.517296626247549e-06, 5.024709466067928e-06, 5.024709466067928e-06, 7.17815638009704e-06, 7.17815638009704e-06, 1.0254509114424345e-05, 1.0254509114424345e-05, 1.4649298734891921e-05, 1.4649298734891921e-05, 2.0927569621274175e-05, 2.0927569621274175e-05, 2.989652803039168e-05, 2.989652803039168e-05, 4.27093257577024e-05, 4.27093257577024e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:33:51,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=45.494465211023275, CurrSamplesPerSec=51.35693790840337, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [ 710/1404]  eta: 0:06:56  lr: 0.000043  min_lr: 0.000000  loss: 3.6551 (3.6641)  loss_scale: 32768.0000 (33828.0056)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0008  max mem: 15572
Epoch: [23]  [ 720/1404]  eta: 0:06:51  lr: 0.000043  min_lr: 0.000000  loss: 3.6165 (3.6650)  loss_scale: 32768.0000 (33813.3037)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0012  max mem: 15572
Epoch: [23]  [ 730/1404]  eta: 0:06:44  lr: 0.000043  min_lr: 0.000000  loss: 3.7878 (3.6670)  loss_scale: 32768.0000 (33799.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0012  max mem: 15572
Epoch: [23]  [ 740/1404]  eta: 0:06:38  lr: 0.000043  min_lr: 0.000000  loss: 3.8566 (3.6679)  loss_scale: 32768.0000 (33785.0904)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0008  max mem: 15572
Epoch: [23]  [ 750/1404]  eta: 0:06:31  lr: 0.000043  min_lr: 0.000000  loss: 3.8566 (3.6721)  loss_scale: 32768.0000 (33771.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0008  max mem: 15572
Epoch: [23]  [ 760/1404]  eta: 0:06:25  lr: 0.000043  min_lr: 0.000000  loss: 3.9112 (3.6756)  loss_scale: 32768.0000 (33758.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0009  max mem: 15572
[2025-01-10 21:34:24,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:34:24,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:34:24,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:34:24,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 770/1404]  eta: 0:06:20  lr: 0.000043  min_lr: 0.000000  loss: 3.9112 (3.6765)  loss_scale: 32768.0000 (34085.5201)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0009  max mem: 15572
[2025-01-10 21:34:30,992] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33066
[2025-01-10 21:34:30,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:34:30,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:34:30,998] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33066
[2025-01-10 21:34:30,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [ 780/1404]  eta: 0:06:14  lr: 0.000042  min_lr: 0.000000  loss: 3.6511 (3.6756)  loss_scale: 65536.0000 (34194.5198)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0008  max mem: 15572
Epoch: [23]  [ 790/1404]  eta: 0:06:08  lr: 0.000042  min_lr: 0.000000  loss: 3.5852 (3.6750)  loss_scale: 32768.0000 (34176.4855)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0009  max mem: 15572
Epoch: [23]  [ 800/1404]  eta: 0:06:01  lr: 0.000042  min_lr: 0.000000  loss: 3.6242 (3.6741)  loss_scale: 32768.0000 (34158.9014)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0009  max mem: 15572
Epoch: [23]  [ 810/1404]  eta: 0:05:56  lr: 0.000042  min_lr: 0.000000  loss: 3.4699 (3.6728)  loss_scale: 32768.0000 (34141.7509)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0007  max mem: 15572
Epoch: [23]  [ 820/1404]  eta: 0:05:49  lr: 0.000042  min_lr: 0.000000  loss: 3.5507 (3.6749)  loss_scale: 32768.0000 (34125.0183)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0007  max mem: 15572
Epoch: [23]  [ 830/1404]  eta: 0:05:43  lr: 0.000042  min_lr: 0.000000  loss: 3.7695 (3.6730)  loss_scale: 32768.0000 (34108.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0008  max mem: 15572
Epoch: [23]  [ 840/1404]  eta: 0:05:37  lr: 0.000042  min_lr: 0.000000  loss: 3.6420 (3.6733)  loss_scale: 32768.0000 (34092.7467)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0011  max mem: 15572
Epoch: [23]  [ 850/1404]  eta: 0:05:31  lr: 0.000042  min_lr: 0.000000  loss: 3.4093 (3.6672)  loss_scale: 32768.0000 (34077.1798)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0010  max mem: 15572
Epoch: [23]  [ 860/1404]  eta: 0:05:25  lr: 0.000042  min_lr: 0.000000  loss: 3.3989 (3.6665)  loss_scale: 32768.0000 (34061.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0006  max mem: 15572
Epoch: [23]  [ 870/1404]  eta: 0:05:19  lr: 0.000042  min_lr: 0.000000  loss: 3.7208 (3.6668)  loss_scale: 32768.0000 (34047.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0006  max mem: 15572
Epoch: [23]  [ 880/1404]  eta: 0:05:12  lr: 0.000042  min_lr: 0.000000  loss: 3.6869 (3.6673)  loss_scale: 32768.0000 (34032.5993)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0008  max mem: 15572
Epoch: [23]  [ 890/1404]  eta: 0:05:06  lr: 0.000042  min_lr: 0.000000  loss: 3.6190 (3.6650)  loss_scale: 32768.0000 (34018.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0007  max mem: 15572
Epoch: [23]  [ 900/1404]  eta: 0:05:00  lr: 0.000042  min_lr: 0.000000  loss: 3.6711 (3.6661)  loss_scale: 32768.0000 (34004.5283)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0006  max mem: 15572
[2025-01-10 21:35:44,825] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:35:44,825] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:35:44,911] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:35:44,912] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:35:48,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33199
[2025-01-10 21:35:48,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:35:48,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33199
[2025-01-10 21:35:48,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:35:48,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 910/1404]  eta: 0:04:54  lr: 0.000042  min_lr: 0.000000  loss: 3.7274 (3.6646)  loss_scale: 32768.0000 (34134.8321)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0007  max mem: 15572
Epoch: [23]  [ 920/1404]  eta: 0:04:48  lr: 0.000042  min_lr: 0.000000  loss: 3.4761 (3.6625)  loss_scale: 32768.0000 (34119.9913)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0008  max mem: 15572
Epoch: [23]  [ 930/1404]  eta: 0:04:42  lr: 0.000042  min_lr: 0.000000  loss: 3.4761 (3.6599)  loss_scale: 32768.0000 (34105.4694)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0008  max mem: 15572
Epoch: [23]  [ 940/1404]  eta: 0:04:36  lr: 0.000042  min_lr: 0.000000  loss: 3.5423 (3.6586)  loss_scale: 32768.0000 (34091.2561)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.0007  max mem: 15572
Epoch: [23]  [ 950/1404]  eta: 0:04:30  lr: 0.000042  min_lr: 0.000000  loss: 3.6350 (3.6598)  loss_scale: 32768.0000 (34077.3417)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0007  max mem: 15572
Epoch: [23]  [ 960/1404]  eta: 0:04:24  lr: 0.000042  min_lr: 0.000000  loss: 3.6599 (3.6589)  loss_scale: 32768.0000 (34063.7170)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0007  max mem: 15572
Epoch: [23]  [ 970/1404]  eta: 0:04:18  lr: 0.000042  min_lr: 0.000000  loss: 3.6501 (3.6589)  loss_scale: 32768.0000 (34050.3728)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0008  max mem: 15572
Epoch: [23]  [ 980/1404]  eta: 0:04:12  lr: 0.000042  min_lr: 0.000000  loss: 3.7940 (3.6603)  loss_scale: 32768.0000 (34037.3007)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0684  max mem: 15572
Epoch: [23]  [ 990/1404]  eta: 0:04:06  lr: 0.000042  min_lr: 0.000000  loss: 3.8040 (3.6605)  loss_scale: 32768.0000 (34024.4924)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0826  max mem: 15572
Epoch: [23]  [1000/1404]  eta: 0:04:01  lr: 0.000042  min_lr: 0.000000  loss: 3.7709 (3.6618)  loss_scale: 32768.0000 (34011.9401)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.0150  max mem: 15572
Epoch: [23]  [1010/1404]  eta: 0:03:55  lr: 0.000042  min_lr: 0.000000  loss: 3.6644 (3.6623)  loss_scale: 32768.0000 (33999.6360)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0007  max mem: 15572
Epoch: [23]  [1020/1404]  eta: 0:03:48  lr: 0.000042  min_lr: 0.000000  loss: 3.7189 (3.6624)  loss_scale: 32768.0000 (33987.5730)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0006  max mem: 15572
Epoch: [23]  [1030/1404]  eta: 0:03:43  lr: 0.000042  min_lr: 0.000000  loss: 3.7355 (3.6638)  loss_scale: 32768.0000 (33975.7439)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0714  max mem: 15572
[2025-01-10 21:37:05,180] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:37:05,180] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:37:05,195] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:37:05,195] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1040/1404]  eta: 0:03:37  lr: 0.000042  min_lr: 0.000000  loss: 3.7355 (3.6647)  loss_scale: 32768.0000 (34121.5293)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0973  max mem: 15572
[2025-01-10 21:37:09,313] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33335
[2025-01-10 21:37:09,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:37:09,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:37:09,355] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33335
[2025-01-10 21:37:09,356] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [1050/1404]  eta: 0:03:31  lr: 0.000042  min_lr: 0.000000  loss: 3.6407 (3.6639)  loss_scale: 32768.0000 (34171.0067)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0514  max mem: 15572
Epoch: [23]  [1060/1404]  eta: 0:03:25  lr: 0.000042  min_lr: 0.000000  loss: 3.6909 (3.6657)  loss_scale: 32768.0000 (34157.7832)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1082  max mem: 15572
Epoch: [23]  [1070/1404]  eta: 0:03:19  lr: 0.000042  min_lr: 0.000000  loss: 3.8427 (3.6646)  loss_scale: 32768.0000 (34144.8067)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0834  max mem: 15572
Epoch: [23]  [1080/1404]  eta: 0:03:13  lr: 0.000042  min_lr: 0.000000  loss: 3.4599 (3.6618)  loss_scale: 32768.0000 (34132.0703)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0570  max mem: 15572
Epoch: [23]  [1090/1404]  eta: 0:03:07  lr: 0.000042  min_lr: 0.000000  loss: 3.6812 (3.6633)  loss_scale: 32768.0000 (34119.5674)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0570  max mem: 15572
Epoch: [23]  [1100/1404]  eta: 0:03:01  lr: 0.000042  min_lr: 0.000000  loss: 3.8205 (3.6632)  loss_scale: 32768.0000 (34107.2916)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0125  max mem: 15572
Epoch: [23]  [1110/1404]  eta: 0:02:55  lr: 0.000042  min_lr: 0.000000  loss: 3.6127 (3.6618)  loss_scale: 32768.0000 (34095.2367)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0775  max mem: 15572
Epoch: [23]  [1120/1404]  eta: 0:02:49  lr: 0.000041  min_lr: 0.000000  loss: 3.5700 (3.6598)  loss_scale: 32768.0000 (34083.3970)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0657  max mem: 15572
Epoch: [23]  [1130/1404]  eta: 0:02:43  lr: 0.000041  min_lr: 0.000000  loss: 3.7048 (3.6586)  loss_scale: 32768.0000 (34071.7666)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0005  max mem: 15572
Epoch: [23]  [1140/1404]  eta: 0:02:36  lr: 0.000041  min_lr: 0.000000  loss: 3.5693 (3.6572)  loss_scale: 32768.0000 (34060.3401)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0007  max mem: 15572
Epoch: [23]  [1150/1404]  eta: 0:02:30  lr: 0.000041  min_lr: 0.000000  loss: 3.4215 (3.6565)  loss_scale: 32768.0000 (34049.1121)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0254  max mem: 15572
Epoch: [23]  [1160/1404]  eta: 0:02:24  lr: 0.000041  min_lr: 0.000000  loss: 3.5695 (3.6571)  loss_scale: 32768.0000 (34038.0775)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0254  max mem: 15572
Epoch: [23]  [1170/1404]  eta: 0:02:18  lr: 0.000041  min_lr: 0.000000  loss: 3.7140 (3.6566)  loss_scale: 32768.0000 (34027.2314)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0480  max mem: 15572
[2025-01-10 21:38:25,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:38:25,490] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:38:25,559] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:38:25,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1180/1404]  eta: 0:02:13  lr: 0.000041  min_lr: 0.000000  loss: 3.7140 (3.6593)  loss_scale: 32768.0000 (34266.2828)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.1659  max mem: 15572
[2025-01-10 21:38:34,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33480
[2025-01-10 21:38:34,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:38:34,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:38:34,432] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33480
[2025-01-10 21:38:34,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [1190/1404]  eta: 0:02:07  lr: 0.000041  min_lr: 0.000000  loss: 3.9696 (3.6609)  loss_scale: 65536.0000 (34446.2939)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.1187  max mem: 15572
Epoch: [23]  [1200/1404]  eta: 0:02:01  lr: 0.000041  min_lr: 0.000000  loss: 3.8047 (3.6595)  loss_scale: 32768.0000 (34432.3197)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0007  max mem: 15572
Epoch: [23]  [1210/1404]  eta: 0:01:55  lr: 0.000041  min_lr: 0.000000  loss: 3.7547 (3.6616)  loss_scale: 32768.0000 (34418.5764)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0007  max mem: 15572
Epoch: [23]  [1220/1404]  eta: 0:01:49  lr: 0.000041  min_lr: 0.000000  loss: 3.7970 (3.6634)  loss_scale: 32768.0000 (34405.0581)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0007  max mem: 15572
Epoch: [23]  [1230/1404]  eta: 0:01:43  lr: 0.000041  min_lr: 0.000000  loss: 3.7818 (3.6630)  loss_scale: 32768.0000 (34391.7595)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0006  max mem: 15572
Epoch: [23]  [1240/1404]  eta: 0:01:37  lr: 0.000041  min_lr: 0.000000  loss: 3.7842 (3.6637)  loss_scale: 32768.0000 (34378.6753)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0446  max mem: 15572
Epoch: [23]  [1250/1404]  eta: 0:01:31  lr: 0.000041  min_lr: 0.000000  loss: 3.7659 (3.6634)  loss_scale: 32768.0000 (34365.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0544  max mem: 15572
Epoch: [23]  [1260/1404]  eta: 0:01:25  lr: 0.000041  min_lr: 0.000000  loss: 3.6432 (3.6636)  loss_scale: 32768.0000 (34353.1293)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0696  max mem: 15572
Epoch: [23]  [1270/1404]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000000  loss: 3.9169 (3.6640)  loss_scale: 32768.0000 (34340.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.0598  max mem: 15572
Epoch: [23]  [1280/1404]  eta: 0:01:13  lr: 0.000041  min_lr: 0.000000  loss: 3.7432 (3.6630)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0008  max mem: 15572
Epoch: [23]  [1290/1404]  eta: 0:01:07  lr: 0.000041  min_lr: 0.000000  loss: 3.6546 (3.6639)  loss_scale: 32768.0000 (34316.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0007  max mem: 15572
Epoch: [23]  [1300/1404]  eta: 0:01:01  lr: 0.000041  min_lr: 0.000000  loss: 3.9200 (3.6646)  loss_scale: 32768.0000 (34304.3935)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0006  max mem: 15572
Epoch: [23]  [1310/1404]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000000  loss: 3.8532 (3.6655)  loss_scale: 32768.0000 (34292.6743)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0006  max mem: 15572
[2025-01-10 21:39:50,127] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:39:50,127] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:39:50,127] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:39:50,128] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:39:50,591] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33610
[2025-01-10 21:39:50,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:39:50,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33610
[2025-01-10 21:39:50,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:39:50,614] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1320/1404]  eta: 0:00:49  lr: 0.000041  min_lr: 0.000000  loss: 3.7452 (3.6663)  loss_scale: 32768.0000 (34305.9379)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0010  max mem: 15572
Epoch: [23]  [1330/1404]  eta: 0:00:43  lr: 0.000041  min_lr: 0.000000  loss: 3.6125 (3.6651)  loss_scale: 32768.0000 (34294.3832)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0340  max mem: 15572
Epoch: [23]  [1340/1404]  eta: 0:00:38  lr: 0.000041  min_lr: 0.000000  loss: 3.6810 (3.6663)  loss_scale: 32768.0000 (34283.0007)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0337  max mem: 15572
Epoch: [23]  [1350/1404]  eta: 0:00:32  lr: 0.000041  min_lr: 0.000000  loss: 3.5191 (3.6643)  loss_scale: 32768.0000 (34271.7868)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0008  max mem: 15572
Epoch: [23]  [1360/1404]  eta: 0:00:26  lr: 0.000041  min_lr: 0.000000  loss: 3.1442 (3.6611)  loss_scale: 32768.0000 (34260.7377)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0009  max mem: 15572
Epoch: [23]  [1370/1404]  eta: 0:00:20  lr: 0.000041  min_lr: 0.000000  loss: 3.3200 (3.6616)  loss_scale: 32768.0000 (34249.8497)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0132  max mem: 15572
Epoch: [23]  [1380/1404]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000000  loss: 3.7207 (3.6609)  loss_scale: 32768.0000 (34239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0129  max mem: 15572
Epoch: [23]  [1390/1404]  eta: 0:00:08  lr: 0.000041  min_lr: 0.000000  loss: 3.5024 (3.6607)  loss_scale: 32768.0000 (34228.5435)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0008  max mem: 15572
Epoch: [23]  [1400/1404]  eta: 0:00:02  lr: 0.000041  min_lr: 0.000000  loss: 3.3817 (3.6591)  loss_scale: 32768.0000 (34218.1185)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0007  max mem: 15572
Epoch: [23]  [1403/1404]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 3.4696 (3.6597)  loss_scale: 32768.0000 (34215.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0006  max mem: 15572
Epoch: [23] Total time: 0:13:52 (0.5928 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 3.4696 (3.6644)  loss_scale: 32768.0000 (34215.0199)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:19  loss: 1.4452 (1.4452)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.4410  data: 5.2138  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 2.1512 (2.1298)  acc1: 50.0000 (48.4848)  acc5: 77.7778 (79.2929)  time: 0.8270  data: 0.6357  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.3389 (2.2724)  acc1: 44.4444 (45.2381)  acc5: 72.2222 (76.7196)  time: 0.3431  data: 0.1504  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.0475 (2.1263)  acc1: 44.4444 (48.5663)  acc5: 77.7778 (78.6738)  time: 0.2686  data: 0.0621  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.8496 (2.1033)  acc1: 55.5556 (49.8645)  acc5: 83.3333 (79.5393)  time: 0.2977  data: 0.0857  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.9180 (2.0931)  acc1: 55.5556 (50.7625)  acc5: 83.3333 (80.1743)  time: 0.4056  data: 0.1786  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1493 (2.1960)  acc1: 50.0000 (47.5410)  acc5: 77.7778 (78.4153)  time: 0.4065  data: 0.1766  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.1772 (2.1621)  acc1: 44.4444 (48.1221)  acc5: 83.3333 (79.0297)  time: 0.3742  data: 0.1636  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9482 (2.1446)  acc1: 50.0000 (48.2167)  acc5: 88.8889 (79.6982)  time: 0.3574  data: 0.1546  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.0731 (2.1501)  acc1: 44.4444 (47.6190)  acc5: 83.3333 (79.3040)  time: 0.3610  data: 0.1693  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.4519 (2.2247)  acc1: 27.7778 (45.5446)  acc5: 72.2222 (77.6128)  time: 0.4072  data: 0.1974  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.3444 (2.2132)  acc1: 33.3333 (46.1461)  acc5: 77.7778 (77.6777)  time: 0.3873  data: 0.1699  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8764 (2.1665)  acc1: 61.1111 (47.4288)  acc5: 83.3333 (78.3747)  time: 0.3342  data: 0.1346  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.7134 (2.1272)  acc1: 61.1111 (48.3461)  acc5: 88.8889 (79.0076)  time: 0.2411  data: 0.0691  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7687 (2.1234)  acc1: 50.0000 (48.8124)  acc5: 83.3333 (79.1564)  time: 0.1859  data: 0.0317  max mem: 15572
Val: Total time: 0:00:50 (0.3728 s / it)
* Acc@1 47.891 Acc@5 78.030 loss 2.172
Accuracy of the network on the 4883 val videos: 47.9%
[2025-01-10 21:41:29,555] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 21:41:29,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 21:41:29,558] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 21:41:29,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 21:41:32,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 21:41:32,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.89%
Epoch: [24]  [   0/1404]  eta: 3:17:32  lr: 0.000041  min_lr: 0.000000  loss: 3.9333 (3.9333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4418  data: 6.0498  max mem: 15572
Epoch: [24]  [  10/1404]  eta: 0:31:07  lr: 0.000041  min_lr: 0.000000  loss: 3.8697 (3.6993)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3397  data: 0.5749  max mem: 15572
Epoch: [24]  [  20/1404]  eta: 0:22:07  lr: 0.000041  min_lr: 0.000000  loss: 3.6846 (3.6359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0141  max mem: 15572
Epoch: [24]  [  30/1404]  eta: 0:19:08  lr: 0.000041  min_lr: 0.000000  loss: 3.3446 (3.4772)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0008  max mem: 15572
Epoch: [24]  [  40/1404]  eta: 0:18:00  lr: 0.000041  min_lr: 0.000000  loss: 3.3446 (3.4975)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0007  max mem: 15572
[2025-01-10 21:42:06,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:42:06,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:42:06,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:42:06,130] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:42:09,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33746
[2025-01-10 21:42:09,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:42:09,602] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33746
[2025-01-10 21:42:09,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:42:09,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [  50/1404]  eta: 0:16:37  lr: 0.000040  min_lr: 0.000000  loss: 3.7616 (3.5819)  loss_scale: 32768.0000 (37265.5686)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0010  max mem: 15572
Epoch: [24]  [  60/1404]  eta: 0:15:57  lr: 0.000040  min_lr: 0.000000  loss: 3.6561 (3.5589)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0011  max mem: 15572
Epoch: [24]  [  70/1404]  eta: 0:15:26  lr: 0.000040  min_lr: 0.000000  loss: 3.4409 (3.5540)  loss_scale: 32768.0000 (35998.6479)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0009  max mem: 15572
Epoch: [24]  [  80/1404]  eta: 0:15:17  lr: 0.000040  min_lr: 0.000000  loss: 3.6219 (3.5497)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.0009  max mem: 15572
Epoch: [24]  [  90/1404]  eta: 0:15:06  lr: 0.000040  min_lr: 0.000000  loss: 3.4672 (3.5552)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.0008  max mem: 15572
Epoch: [24]  [ 100/1404]  eta: 0:14:40  lr: 0.000040  min_lr: 0.000000  loss: 3.3982 (3.5471)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0009  max mem: 15572
Epoch: [24]  [ 110/1404]  eta: 0:14:17  lr: 0.000040  min_lr: 0.000000  loss: 3.6829 (3.5747)  loss_scale: 32768.0000 (34834.4505)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0006  max mem: 15572
Epoch: [24]  [ 120/1404]  eta: 0:14:03  lr: 0.000040  min_lr: 0.000000  loss: 3.6860 (3.5736)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0007  max mem: 15572
Epoch: [24]  [ 130/1404]  eta: 0:13:47  lr: 0.000040  min_lr: 0.000000  loss: 3.7281 (3.5852)  loss_scale: 32768.0000 (34518.9618)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0009  max mem: 15572
Epoch: [24]  [ 140/1404]  eta: 0:13:36  lr: 0.000040  min_lr: 0.000000  loss: 3.8181 (3.5927)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0010  max mem: 15572
Epoch: [24]  [ 150/1404]  eta: 0:13:25  lr: 0.000040  min_lr: 0.000000  loss: 3.6564 (3.5948)  loss_scale: 32768.0000 (34287.0464)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0009  max mem: 15572
Epoch: [24]  [ 160/1404]  eta: 0:13:18  lr: 0.000040  min_lr: 0.000000  loss: 3.6564 (3.6073)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0006  max mem: 15572
Epoch: [24]  [ 170/1404]  eta: 0:13:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9450 (3.6287)  loss_scale: 32768.0000 (34109.3801)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0006  max mem: 15572
[2025-01-10 21:43:25,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:43:25,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:43:25,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:43:25,884] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 180/1404]  eta: 0:12:56  lr: 0.000040  min_lr: 0.000000  loss: 3.9112 (3.6366)  loss_scale: 32768.0000 (34397.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0005  max mem: 15572
[2025-01-10 21:43:28,321] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33879
[2025-01-10 21:43:28,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:43:28,321] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:43:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33879
[2025-01-10 21:43:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [24]  [ 190/1404]  eta: 0:12:49  lr: 0.000040  min_lr: 0.000000  loss: 3.7802 (3.6324)  loss_scale: 32768.0000 (34655.1623)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0005  max mem: 15572
Epoch: [24]  [ 200/1404]  eta: 0:12:36  lr: 0.000040  min_lr: 0.000000  loss: 3.5827 (3.6344)  loss_scale: 32768.0000 (34561.2736)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0007  max mem: 15572
Epoch: [24]  [ 210/1404]  eta: 0:12:29  lr: 0.000040  min_lr: 0.000000  loss: 3.4063 (3.6243)  loss_scale: 32768.0000 (34476.2844)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0009  max mem: 15572
Epoch: [24]  [ 220/1404]  eta: 0:12:22  lr: 0.000040  min_lr: 0.000000  loss: 3.3523 (3.6140)  loss_scale: 32768.0000 (34398.9864)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0006  max mem: 15572
Epoch: [24]  [ 230/1404]  eta: 0:12:10  lr: 0.000040  min_lr: 0.000000  loss: 3.3976 (3.6121)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0006  max mem: 15572
Epoch: [24]  [ 240/1404]  eta: 0:12:02  lr: 0.000040  min_lr: 0.000000  loss: 3.6775 (3.6172)  loss_scale: 32768.0000 (34263.6349)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0006  max mem: 15572
Epoch: [24]  [ 250/1404]  eta: 0:11:51  lr: 0.000040  min_lr: 0.000000  loss: 3.7918 (3.6191)  loss_scale: 32768.0000 (34204.0478)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0006  max mem: 15572
Epoch: [24]  [ 260/1404]  eta: 0:11:41  lr: 0.000040  min_lr: 0.000000  loss: 3.5691 (3.6157)  loss_scale: 32768.0000 (34149.0268)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0006  max mem: 15572
Epoch: [24]  [ 270/1404]  eta: 0:11:36  lr: 0.000040  min_lr: 0.000000  loss: 3.4910 (3.6155)  loss_scale: 32768.0000 (34098.0664)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0008  max mem: 15572
Epoch: [24]  [ 280/1404]  eta: 0:11:33  lr: 0.000040  min_lr: 0.000000  loss: 3.6824 (3.6178)  loss_scale: 32768.0000 (34050.7331)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0008  max mem: 15572
Epoch: [24]  [ 290/1404]  eta: 0:11:23  lr: 0.000040  min_lr: 0.000000  loss: 3.6435 (3.6145)  loss_scale: 32768.0000 (34006.6529)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0007  max mem: 15572
[2025-01-10 21:44:31,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33988
[2025-01-10 21:44:31,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 21:44:31,603] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33988
[2025-01-10 21:44:31,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 21:44:31,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 300/1404]  eta: 0:11:15  lr: 0.000040  min_lr: 0.000000  loss: 3.8489 (3.6208)  loss_scale: 32768.0000 (33475.6146)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0008  max mem: 15572
[2025-01-10 21:44:37,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=228, lr=[3.850205467988544e-07, 3.850205467988544e-07, 5.500293525697921e-07, 5.500293525697921e-07, 7.857562179568459e-07, 7.857562179568459e-07, 1.1225088827954943e-06, 1.1225088827954943e-06, 1.6035841182792775e-06, 1.6035841182792775e-06, 2.290834454684682e-06, 2.290834454684682e-06, 3.272620649549546e-06, 3.272620649549546e-06, 4.675172356499352e-06, 4.675172356499352e-06, 6.678817652141932e-06, 6.678817652141932e-06, 9.541168074488476e-06, 9.541168074488476e-06, 1.3630240106412107e-05, 1.3630240106412107e-05, 1.9471771580588725e-05, 1.9471771580588725e-05, 2.7816816543698182e-05, 2.7816816543698182e-05, 3.973830934814026e-05, 3.973830934814026e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:44:37,890] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=45.45714699970473, CurrSamplesPerSec=47.96144152145139, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [ 310/1404]  eta: 0:11:13  lr: 0.000040  min_lr: 0.000000  loss: 3.9078 (3.6283)  loss_scale: 16384.0000 (32926.0450)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.0010  max mem: 15572
Epoch: [24]  [ 320/1404]  eta: 0:11:03  lr: 0.000040  min_lr: 0.000000  loss: 3.7602 (3.6215)  loss_scale: 16384.0000 (32410.7165)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0009  max mem: 15572
Epoch: [24]  [ 330/1404]  eta: 0:10:58  lr: 0.000040  min_lr: 0.000000  loss: 3.4991 (3.6243)  loss_scale: 16384.0000 (31926.5257)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0007  max mem: 15572
Epoch: [24]  [ 340/1404]  eta: 0:10:50  lr: 0.000040  min_lr: 0.000000  loss: 3.4779 (3.6176)  loss_scale: 16384.0000 (31470.7331)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0008  max mem: 15572
Epoch: [24]  [ 350/1404]  eta: 0:10:43  lr: 0.000040  min_lr: 0.000000  loss: 3.4868 (3.6179)  loss_scale: 16384.0000 (31040.9117)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0008  max mem: 15572
Epoch: [24]  [ 360/1404]  eta: 0:10:39  lr: 0.000040  min_lr: 0.000000  loss: 3.6042 (3.6134)  loss_scale: 16384.0000 (30634.9030)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0007  max mem: 15572
Epoch: [24]  [ 370/1404]  eta: 0:10:30  lr: 0.000040  min_lr: 0.000000  loss: 3.7482 (3.6184)  loss_scale: 16384.0000 (30250.7817)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0007  max mem: 15572
Epoch: [24]  [ 380/1404]  eta: 0:10:22  lr: 0.000040  min_lr: 0.000000  loss: 3.7607 (3.6198)  loss_scale: 16384.0000 (29886.8241)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0010  max mem: 15572
Epoch: [24]  [ 390/1404]  eta: 0:10:15  lr: 0.000039  min_lr: 0.000000  loss: 3.5572 (3.6172)  loss_scale: 16384.0000 (29541.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0014  max mem: 15572
Epoch: [24]  [ 400/1404]  eta: 0:10:07  lr: 0.000039  min_lr: 0.000000  loss: 3.6269 (3.6178)  loss_scale: 16384.0000 (29213.3666)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0015  max mem: 15572
Epoch: [24]  [ 410/1404]  eta: 0:10:00  lr: 0.000039  min_lr: 0.000000  loss: 3.8613 (3.6227)  loss_scale: 16384.0000 (28901.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0011  max mem: 15572
Epoch: [24]  [ 420/1404]  eta: 0:09:54  lr: 0.000039  min_lr: 0.000000  loss: 3.7513 (3.6228)  loss_scale: 16384.0000 (28603.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0010  max mem: 15572
[2025-01-10 21:45:46,923] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:45:46,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 21:45:46,987] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:45:46,987] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 430/1404]  eta: 0:09:45  lr: 0.000039  min_lr: 0.000000  loss: 3.7513 (3.6290)  loss_scale: 16384.0000 (28700.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0010  max mem: 15572
Epoch: [24]  [ 440/1404]  eta: 0:09:40  lr: 0.000039  min_lr: 0.000000  loss: 3.8408 (3.6344)  loss_scale: 32768.0000 (28792.7438)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0616  max mem: 15572
Epoch: [24]  [ 450/1404]  eta: 0:09:33  lr: 0.000039  min_lr: 0.000000  loss: 3.6932 (3.6358)  loss_scale: 32768.0000 (28880.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1049  max mem: 15572
Epoch: [24]  [ 460/1404]  eta: 0:09:29  lr: 0.000039  min_lr: 0.000000  loss: 3.6469 (3.6353)  loss_scale: 32768.0000 (28965.2061)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1304  max mem: 15572
Epoch: [24]  [ 470/1404]  eta: 0:09:21  lr: 0.000039  min_lr: 0.000000  loss: 3.7581 (3.6417)  loss_scale: 32768.0000 (29045.9448)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0871  max mem: 15572
Epoch: [24]  [ 480/1404]  eta: 0:09:15  lr: 0.000039  min_lr: 0.000000  loss: 3.5973 (3.6366)  loss_scale: 32768.0000 (29123.3264)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0177  max mem: 15572
Epoch: [24]  [ 490/1404]  eta: 0:09:10  lr: 0.000039  min_lr: 0.000000  loss: 3.5990 (3.6390)  loss_scale: 32768.0000 (29197.5560)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0863  max mem: 15572
Epoch: [24]  [ 500/1404]  eta: 0:09:06  lr: 0.000039  min_lr: 0.000000  loss: 3.6135 (3.6356)  loss_scale: 32768.0000 (29268.8224)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1879  max mem: 15572
Epoch: [24]  [ 510/1404]  eta: 0:09:00  lr: 0.000039  min_lr: 0.000000  loss: 3.5106 (3.6346)  loss_scale: 32768.0000 (29337.2994)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.1567  max mem: 15572
Epoch: [24]  [ 520/1404]  eta: 0:08:53  lr: 0.000039  min_lr: 0.000000  loss: 3.4406 (3.6305)  loss_scale: 32768.0000 (29403.1478)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0381  max mem: 15572
Epoch: [24]  [ 530/1404]  eta: 0:08:47  lr: 0.000039  min_lr: 0.000000  loss: 3.2705 (3.6266)  loss_scale: 32768.0000 (29466.5160)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0510  max mem: 15572
Epoch: [24]  [ 540/1404]  eta: 0:08:40  lr: 0.000039  min_lr: 0.000000  loss: 3.4480 (3.6276)  loss_scale: 32768.0000 (29527.5416)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0830  max mem: 15572
[2025-01-10 21:47:04,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:47:04,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:47:04,148] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:47:04,149] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 550/1404]  eta: 0:08:35  lr: 0.000039  min_lr: 0.000000  loss: 3.6271 (3.6270)  loss_scale: 32768.0000 (29705.2922)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0867  max mem: 15572
Epoch: [24]  [ 560/1404]  eta: 0:08:29  lr: 0.000039  min_lr: 0.000000  loss: 3.5643 (3.6268)  loss_scale: 65536.0000 (30343.9857)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1466  max mem: 15572
[2025-01-10 21:47:16,395] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34266
[2025-01-10 21:47:16,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:47:16,402] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34266
[2025-01-10 21:47:16,402] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:47:16,402] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 570/1404]  eta: 0:08:22  lr: 0.000039  min_lr: 0.000000  loss: 3.5643 (3.6245)  loss_scale: 65536.0000 (30902.9212)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0926  max mem: 15572
Epoch: [24]  [ 580/1404]  eta: 0:08:15  lr: 0.000039  min_lr: 0.000000  loss: 3.7310 (3.6297)  loss_scale: 32768.0000 (30935.0224)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0211  max mem: 15572
Epoch: [24]  [ 590/1404]  eta: 0:08:09  lr: 0.000039  min_lr: 0.000000  loss: 3.7310 (3.6300)  loss_scale: 32768.0000 (30966.0372)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0210  max mem: 15572
Epoch: [24]  [ 600/1404]  eta: 0:08:02  lr: 0.000039  min_lr: 0.000000  loss: 3.5392 (3.6290)  loss_scale: 32768.0000 (30996.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0007  max mem: 15572
Epoch: [24]  [ 610/1404]  eta: 0:07:55  lr: 0.000039  min_lr: 0.000000  loss: 3.7224 (3.6316)  loss_scale: 32768.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0009  max mem: 15572
Epoch: [24]  [ 620/1404]  eta: 0:07:48  lr: 0.000039  min_lr: 0.000000  loss: 3.7946 (3.6346)  loss_scale: 32768.0000 (31053.0886)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0008  max mem: 15572
Epoch: [24]  [ 630/1404]  eta: 0:07:41  lr: 0.000039  min_lr: 0.000000  loss: 3.7315 (3.6383)  loss_scale: 32768.0000 (31080.2662)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0007  max mem: 15572
Epoch: [24]  [ 640/1404]  eta: 0:07:36  lr: 0.000039  min_lr: 0.000000  loss: 3.6686 (3.6377)  loss_scale: 32768.0000 (31106.5959)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0623  max mem: 15572
Epoch: [24]  [ 650/1404]  eta: 0:07:31  lr: 0.000039  min_lr: 0.000000  loss: 3.4161 (3.6331)  loss_scale: 32768.0000 (31132.1167)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1350  max mem: 15572
Epoch: [24]  [ 660/1404]  eta: 0:07:24  lr: 0.000039  min_lr: 0.000000  loss: 3.7097 (3.6333)  loss_scale: 32768.0000 (31156.8654)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0899  max mem: 15572
Epoch: [24]  [ 670/1404]  eta: 0:07:19  lr: 0.000039  min_lr: 0.000000  loss: 3.7055 (3.6336)  loss_scale: 32768.0000 (31180.8763)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0571  max mem: 15572
Epoch: [24]  [ 680/1404]  eta: 0:07:12  lr: 0.000039  min_lr: 0.000000  loss: 3.7318 (3.6374)  loss_scale: 32768.0000 (31204.1821)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0478  max mem: 15572
Epoch: [24]  [ 690/1404]  eta: 0:07:07  lr: 0.000039  min_lr: 0.000000  loss: 3.7528 (3.6374)  loss_scale: 32768.0000 (31226.8133)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0515  max mem: 15572
[2025-01-10 21:48:30,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:48:30,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:48:30,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:48:30,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:48:31,523] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34396
[2025-01-10 21:48:31,523] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:48:31,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34396
[2025-01-10 21:48:31,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:48:31,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 700/1404]  eta: 0:07:01  lr: 0.000039  min_lr: 0.000000  loss: 3.6944 (3.6364)  loss_scale: 32768.0000 (31295.5435)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0608  max mem: 15572
Epoch: [24]  [ 710/1404]  eta: 0:06:55  lr: 0.000039  min_lr: 0.000000  loss: 3.6729 (3.6374)  loss_scale: 32768.0000 (31316.2532)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0846  max mem: 15572
Epoch: [24]  [ 720/1404]  eta: 0:06:49  lr: 0.000039  min_lr: 0.000000  loss: 3.6729 (3.6361)  loss_scale: 32768.0000 (31336.3883)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1517  max mem: 15572
Epoch: [24]  [ 730/1404]  eta: 0:06:43  lr: 0.000038  min_lr: 0.000000  loss: 3.2164 (3.6313)  loss_scale: 32768.0000 (31355.9726)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1570  max mem: 15572
Epoch: [24]  [ 740/1404]  eta: 0:06:37  lr: 0.000038  min_lr: 0.000000  loss: 3.5574 (3.6308)  loss_scale: 32768.0000 (31375.0283)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0794  max mem: 15572
Epoch: [24]  [ 750/1404]  eta: 0:06:30  lr: 0.000038  min_lr: 0.000000  loss: 3.7276 (3.6341)  loss_scale: 32768.0000 (31393.5766)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0453  max mem: 15572
Epoch: [24]  [ 760/1404]  eta: 0:06:24  lr: 0.000038  min_lr: 0.000000  loss: 3.7202 (3.6348)  loss_scale: 32768.0000 (31411.6373)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0458  max mem: 15572
Epoch: [24]  [ 770/1404]  eta: 0:06:18  lr: 0.000038  min_lr: 0.000000  loss: 3.7158 (3.6327)  loss_scale: 32768.0000 (31429.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0809  max mem: 15572
Epoch: [24]  [ 780/1404]  eta: 0:06:12  lr: 0.000038  min_lr: 0.000000  loss: 3.7998 (3.6355)  loss_scale: 32768.0000 (31446.3713)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1350  max mem: 15572
Epoch: [24]  [ 790/1404]  eta: 0:06:07  lr: 0.000038  min_lr: 0.000000  loss: 3.8453 (3.6384)  loss_scale: 32768.0000 (31463.0796)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1322  max mem: 15572
Epoch: [24]  [ 800/1404]  eta: 0:06:00  lr: 0.000038  min_lr: 0.000000  loss: 3.9133 (3.6401)  loss_scale: 32768.0000 (31479.3708)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0974  max mem: 15572
Epoch: [24]  [ 810/1404]  eta: 0:05:54  lr: 0.000038  min_lr: 0.000000  loss: 3.7897 (3.6424)  loss_scale: 32768.0000 (31495.2602)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0653  max mem: 15572
Epoch: [24]  [ 820/1404]  eta: 0:05:48  lr: 0.000038  min_lr: 0.000000  loss: 3.6416 (3.6409)  loss_scale: 32768.0000 (31510.7625)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0997  max mem: 15572
[2025-01-10 21:49:48,477] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:49:48,477] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:49:48,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:49:48,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 830/1404]  eta: 0:05:42  lr: 0.000038  min_lr: 0.000000  loss: 3.5601 (3.6401)  loss_scale: 32768.0000 (31604.7557)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1026  max mem: 15572
[2025-01-10 21:49:54,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34535
[2025-01-10 21:49:54,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:49:54,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:49:54,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34535
[2025-01-10 21:49:54,626] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [24]  [ 840/1404]  eta: 0:05:37  lr: 0.000038  min_lr: 0.000000  loss: 3.6839 (3.6417)  loss_scale: 32768.0000 (31930.2925)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0870  max mem: 15572
Epoch: [24]  [ 850/1404]  eta: 0:05:30  lr: 0.000038  min_lr: 0.000000  loss: 3.6839 (3.6407)  loss_scale: 32768.0000 (31940.1363)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0579  max mem: 15572
Epoch: [24]  [ 860/1404]  eta: 0:05:24  lr: 0.000038  min_lr: 0.000000  loss: 3.6306 (3.6410)  loss_scale: 32768.0000 (31949.7515)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0466  max mem: 15572
Epoch: [24]  [ 870/1404]  eta: 0:05:18  lr: 0.000038  min_lr: 0.000000  loss: 3.6306 (3.6399)  loss_scale: 32768.0000 (31959.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0343  max mem: 15572
Epoch: [24]  [ 880/1404]  eta: 0:05:13  lr: 0.000038  min_lr: 0.000000  loss: 3.6626 (3.6409)  loss_scale: 32768.0000 (31968.3269)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0721  max mem: 15572
Epoch: [24]  [ 890/1404]  eta: 0:05:07  lr: 0.000038  min_lr: 0.000000  loss: 3.7496 (3.6415)  loss_scale: 32768.0000 (31977.3019)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0718  max mem: 15572
Epoch: [24]  [ 900/1404]  eta: 0:05:01  lr: 0.000038  min_lr: 0.000000  loss: 3.5965 (3.6383)  loss_scale: 32768.0000 (31986.0777)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0102  max mem: 15572
Epoch: [24]  [ 910/1404]  eta: 0:04:55  lr: 0.000038  min_lr: 0.000000  loss: 3.5424 (3.6388)  loss_scale: 32768.0000 (31994.6608)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0101  max mem: 15572
Epoch: [24]  [ 920/1404]  eta: 0:04:48  lr: 0.000038  min_lr: 0.000000  loss: 3.5424 (3.6366)  loss_scale: 32768.0000 (32003.0575)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0007  max mem: 15572
Epoch: [24]  [ 930/1404]  eta: 0:04:42  lr: 0.000038  min_lr: 0.000000  loss: 3.5854 (3.6363)  loss_scale: 32768.0000 (32011.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0007  max mem: 15572
Epoch: [24]  [ 940/1404]  eta: 0:04:36  lr: 0.000038  min_lr: 0.000000  loss: 3.7364 (3.6369)  loss_scale: 32768.0000 (32019.3156)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0343  max mem: 15572
Epoch: [24]  [ 950/1404]  eta: 0:04:30  lr: 0.000038  min_lr: 0.000000  loss: 3.3402 (3.6320)  loss_scale: 32768.0000 (32027.1882)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0343  max mem: 15572
Epoch: [24]  [ 960/1404]  eta: 0:04:24  lr: 0.000038  min_lr: 0.000000  loss: 3.4423 (3.6318)  loss_scale: 32768.0000 (32034.8970)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0006  max mem: 15572
[2025-01-10 21:51:09,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:51:09,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:51:09,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:51:09,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 970/1404]  eta: 0:04:18  lr: 0.000038  min_lr: 0.000000  loss: 3.7556 (3.6330)  loss_scale: 32768.0000 (32143.6869)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0008  max mem: 15572
[2025-01-10 21:51:11,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34668
[2025-01-10 21:51:11,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:51:11,562] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34668
[2025-01-10 21:51:11,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:51:11,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 980/1404]  eta: 0:04:12  lr: 0.000038  min_lr: 0.000000  loss: 3.7871 (3.6332)  loss_scale: 32768.0000 (32183.4536)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0009  max mem: 15572
Epoch: [24]  [ 990/1404]  eta: 0:04:06  lr: 0.000038  min_lr: 0.000000  loss: 3.7971 (3.6338)  loss_scale: 32768.0000 (32189.3522)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0007  max mem: 15572
Epoch: [24]  [1000/1404]  eta: 0:04:00  lr: 0.000038  min_lr: 0.000000  loss: 3.8137 (3.6347)  loss_scale: 32768.0000 (32195.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0009  max mem: 15572
Epoch: [24]  [1010/1404]  eta: 0:03:54  lr: 0.000038  min_lr: 0.000000  loss: 3.8101 (3.6371)  loss_scale: 32768.0000 (32200.7992)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0066  max mem: 15572
Epoch: [24]  [1020/1404]  eta: 0:03:48  lr: 0.000038  min_lr: 0.000000  loss: 3.8217 (3.6401)  loss_scale: 32768.0000 (32206.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0064  max mem: 15572
Epoch: [24]  [1030/1404]  eta: 0:03:42  lr: 0.000038  min_lr: 0.000000  loss: 3.8603 (3.6417)  loss_scale: 32768.0000 (32211.8021)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0369  max mem: 15572
Epoch: [24]  [1040/1404]  eta: 0:03:36  lr: 0.000038  min_lr: 0.000000  loss: 3.6140 (3.6395)  loss_scale: 32768.0000 (32217.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0551  max mem: 15572
Epoch: [24]  [1050/1404]  eta: 0:03:30  lr: 0.000038  min_lr: 0.000000  loss: 3.4905 (3.6393)  loss_scale: 32768.0000 (32222.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0622  max mem: 15572
Epoch: [24]  [1060/1404]  eta: 0:03:24  lr: 0.000038  min_lr: 0.000000  loss: 3.9488 (3.6422)  loss_scale: 32768.0000 (32227.5287)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0909  max mem: 15572
Epoch: [24]  [1070/1404]  eta: 0:03:18  lr: 0.000037  min_lr: 0.000000  loss: 4.0309 (3.6435)  loss_scale: 32768.0000 (32232.5752)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0852  max mem: 15572
Epoch: [24]  [1080/1404]  eta: 0:03:12  lr: 0.000037  min_lr: 0.000000  loss: 4.0240 (3.6454)  loss_scale: 32768.0000 (32237.5282)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0466  max mem: 15572
Epoch: [24]  [1090/1404]  eta: 0:03:07  lr: 0.000037  min_lr: 0.000000  loss: 3.7763 (3.6456)  loss_scale: 32768.0000 (32242.3905)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0482  max mem: 15572
Epoch: [24]  [1100/1404]  eta: 0:03:01  lr: 0.000037  min_lr: 0.000000  loss: 3.6790 (3.6464)  loss_scale: 32768.0000 (32247.1644)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0546  max mem: 15572
[2025-01-10 21:52:29,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:52:29,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:52:29,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:52:29,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:52:29,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34798
[2025-01-10 21:52:29,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:52:29,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34798
[2025-01-10 21:52:29,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:52:29,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1110/1404]  eta: 0:02:55  lr: 0.000037  min_lr: 0.000000  loss: 3.6790 (3.6456)  loss_scale: 32768.0000 (32281.3465)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0153  max mem: 15572
Epoch: [24]  [1120/1404]  eta: 0:02:49  lr: 0.000037  min_lr: 0.000000  loss: 3.7902 (3.6472)  loss_scale: 32768.0000 (32285.6878)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0007  max mem: 15572
Epoch: [24]  [1130/1404]  eta: 0:02:43  lr: 0.000037  min_lr: 0.000000  loss: 3.7311 (3.6474)  loss_scale: 32768.0000 (32289.9523)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.0212  max mem: 15572
Epoch: [24]  [1140/1404]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000000  loss: 3.7443 (3.6496)  loss_scale: 32768.0000 (32294.1420)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.0214  max mem: 15572
Epoch: [24]  [1150/1404]  eta: 0:02:31  lr: 0.000037  min_lr: 0.000000  loss: 3.7880 (3.6483)  loss_scale: 32768.0000 (32298.2589)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0010  max mem: 15572
Epoch: [24]  [1160/1404]  eta: 0:02:25  lr: 0.000037  min_lr: 0.000000  loss: 3.7885 (3.6498)  loss_scale: 32768.0000 (32302.3049)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0047  max mem: 15572
Epoch: [24]  [1170/1404]  eta: 0:02:19  lr: 0.000037  min_lr: 0.000000  loss: 3.6911 (3.6485)  loss_scale: 32768.0000 (32306.2818)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0049  max mem: 15572
Epoch: [24]  [1180/1404]  eta: 0:02:13  lr: 0.000037  min_lr: 0.000000  loss: 3.6200 (3.6495)  loss_scale: 32768.0000 (32310.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0013  max mem: 15572
Epoch: [24]  [1190/1404]  eta: 0:02:07  lr: 0.000037  min_lr: 0.000000  loss: 3.5842 (3.6456)  loss_scale: 32768.0000 (32314.0353)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.0013  max mem: 15572
Epoch: [24]  [1200/1404]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000000  loss: 3.3940 (3.6459)  loss_scale: 32768.0000 (32317.8152)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0013  max mem: 15572
Epoch: [24]  [1210/1404]  eta: 0:01:55  lr: 0.000037  min_lr: 0.000000  loss: 3.6928 (3.6465)  loss_scale: 32768.0000 (32321.5326)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0012  max mem: 15572
Epoch: [24]  [1220/1404]  eta: 0:01:49  lr: 0.000037  min_lr: 0.000000  loss: 3.8237 (3.6467)  loss_scale: 32768.0000 (32325.1892)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.0009  max mem: 15572
Epoch: [24]  [1230/1404]  eta: 0:01:43  lr: 0.000037  min_lr: 0.000000  loss: 3.8190 (3.6479)  loss_scale: 32768.0000 (32328.7864)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0009  max mem: 15572
[2025-01-10 21:53:46,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:53:46,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:53:46,611] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:53:46,611] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1240/1404]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000000  loss: 3.6943 (3.6473)  loss_scale: 32768.0000 (32596.3707)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0007  max mem: 15572
Epoch: [24]  [1250/1404]  eta: 0:01:31  lr: 0.000037  min_lr: 0.000000  loss: 3.6943 (3.6489)  loss_scale: 65536.0000 (32859.6771)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0112  max mem: 15572
[2025-01-10 21:53:58,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34947
[2025-01-10 21:53:58,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:53:58,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:53:58,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34947
[2025-01-10 21:53:58,023] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [24]  [1260/1404]  eta: 0:01:25  lr: 0.000037  min_lr: 0.000000  loss: 3.6950 (3.6500)  loss_scale: 32768.0000 (32858.9500)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0117  max mem: 15572
Epoch: [24]  [1270/1404]  eta: 0:01:19  lr: 0.000037  min_lr: 0.000000  loss: 3.6950 (3.6514)  loss_scale: 32768.0000 (32858.2345)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0011  max mem: 15572
Epoch: [24]  [1280/1404]  eta: 0:01:13  lr: 0.000037  min_lr: 0.000000  loss: 3.8092 (3.6514)  loss_scale: 32768.0000 (32857.5301)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0008  max mem: 15572
Epoch: [24]  [1290/1404]  eta: 0:01:07  lr: 0.000037  min_lr: 0.000000  loss: 3.5641 (3.6496)  loss_scale: 32768.0000 (32856.8366)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0007  max mem: 15572
Epoch: [24]  [1300/1404]  eta: 0:01:01  lr: 0.000037  min_lr: 0.000000  loss: 3.5641 (3.6488)  loss_scale: 32768.0000 (32856.1537)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0006  max mem: 15572
[2025-01-10 21:54:29,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=234, lr=[3.565190408844115e-07, 3.565190408844115e-07, 5.093129155491593e-07, 5.093129155491593e-07, 7.275898793559419e-07, 7.275898793559419e-07, 1.0394141133656313e-06, 1.0394141133656313e-06, 1.4848773048080449e-06, 1.4848773048080449e-06, 2.121253292582921e-06, 2.121253292582921e-06, 3.0303618465470305e-06, 3.0303618465470305e-06, 4.3290883522100446e-06, 4.3290883522100446e-06, 6.1844119317286345e-06, 6.1844119317286345e-06, 8.834874188183765e-06, 8.834874188183765e-06, 1.2621248840262521e-05, 1.2621248840262521e-05, 1.8030355486089318e-05, 1.8030355486089318e-05, 2.5757650694413313e-05, 2.5757650694413313e-05, 3.679664384916188e-05, 3.679664384916188e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 21:54:29,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=45.47533294989076, CurrSamplesPerSec=61.856466480558495, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [1310/1404]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000000  loss: 3.7045 (3.6485)  loss_scale: 32768.0000 (32855.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0006  max mem: 15572
Epoch: [24]  [1320/1404]  eta: 0:00:49  lr: 0.000037  min_lr: 0.000000  loss: 3.7045 (3.6476)  loss_scale: 32768.0000 (32854.8191)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0007  max mem: 15572
Epoch: [24]  [1330/1404]  eta: 0:00:44  lr: 0.000037  min_lr: 0.000000  loss: 3.6391 (3.6479)  loss_scale: 32768.0000 (32854.1668)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0007  max mem: 15572
Epoch: [24]  [1340/1404]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000000  loss: 3.6391 (3.6478)  loss_scale: 32768.0000 (32853.5242)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0010  max mem: 15572
Epoch: [24]  [1350/1404]  eta: 0:00:32  lr: 0.000037  min_lr: 0.000000  loss: 3.5685 (3.6482)  loss_scale: 32768.0000 (32852.8912)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0011  max mem: 15572
Epoch: [24]  [1360/1404]  eta: 0:00:26  lr: 0.000037  min_lr: 0.000000  loss: 3.7056 (3.6470)  loss_scale: 32768.0000 (32852.2675)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0008  max mem: 15572
Epoch: [24]  [1370/1404]  eta: 0:00:20  lr: 0.000037  min_lr: 0.000000  loss: 3.7061 (3.6473)  loss_scale: 32768.0000 (32851.6528)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0006  max mem: 15572
[2025-01-10 21:55:14,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:55:14,578] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:55:14,675] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:55:14,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1380/1404]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000000  loss: 3.8826 (3.6474)  loss_scale: 32768.0000 (32874.7748)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0005  max mem: 15572
[2025-01-10 21:55:15,546] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35078
[2025-01-10 21:55:15,547] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:55:15,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35078
[2025-01-10 21:55:15,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:55:15,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [1390/1404]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 3.9053 (3.6473)  loss_scale: 32768.0000 (32897.5643)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0005  max mem: 15572
Epoch: [24]  [1400/1404]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000000  loss: 3.7898 (3.6477)  loss_scale: 32768.0000 (32896.6395)  weight_decay: 0.0500 (0.0500)  time: 0.4284  data: 0.0004  max mem: 15572
Epoch: [24]  [1403/1404]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 3.8058 (3.6481)  loss_scale: 32768.0000 (32896.3647)  weight_decay: 0.0500 (0.0500)  time: 0.4137  data: 0.0004  max mem: 15572
Epoch: [24] Total time: 0:13:52 (0.5930 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 3.8058 (3.6242)  loss_scale: 32768.0000 (32896.3647)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:08:43  loss: 1.6171 (1.6171)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 3.8527  data: 3.6691  max mem: 15572
Val:  [ 10/136]  eta: 0:01:38  loss: 2.3253 (2.1720)  acc1: 50.0000 (48.4848)  acc5: 77.7778 (77.2727)  time: 0.7820  data: 0.5723  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.3520 (2.2624)  acc1: 44.4444 (46.2963)  acc5: 77.7778 (77.2487)  time: 0.4116  data: 0.2071  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.1144 (2.0758)  acc1: 44.4444 (51.0753)  acc5: 83.3333 (79.5699)  time: 0.3391  data: 0.1375  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 1.7608 (2.0579)  acc1: 61.1111 (51.3550)  acc5: 83.3333 (79.9458)  time: 0.3654  data: 0.1539  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8693 (2.0636)  acc1: 50.0000 (50.9804)  acc5: 83.3333 (80.6100)  time: 0.3793  data: 0.1656  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.1579 (2.1589)  acc1: 38.8889 (47.8142)  acc5: 77.7778 (79.1439)  time: 0.3675  data: 0.1583  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.1424 (2.1216)  acc1: 44.4444 (48.8263)  acc5: 77.7778 (79.4210)  time: 0.3899  data: 0.1745  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8157 (2.1137)  acc1: 50.0000 (49.0398)  acc5: 88.8889 (79.7668)  time: 0.3967  data: 0.1827  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0861 (2.1281)  acc1: 38.8889 (48.0464)  acc5: 77.7778 (79.6093)  time: 0.3657  data: 0.1639  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.5091 (2.2116)  acc1: 33.3333 (46.2596)  acc5: 77.7778 (77.5578)  time: 0.3671  data: 0.1545  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.3657 (2.2132)  acc1: 38.8889 (46.4965)  acc5: 77.7778 (77.6276)  time: 0.3771  data: 0.1710  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8679 (2.1610)  acc1: 55.5556 (47.9798)  acc5: 83.3333 (78.3747)  time: 0.3419  data: 0.1632  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.4226 (2.1180)  acc1: 55.5556 (48.7701)  acc5: 88.8889 (78.9228)  time: 0.2300  data: 0.0758  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8188 (2.1149)  acc1: 50.0000 (49.0172)  acc5: 83.3333 (79.0336)  time: 0.1590  data: 0.0223  max mem: 15572
Val: Total time: 0:00:51 (0.3756 s / it)
* Acc@1 48.157 Acc@5 78.030 loss 2.167
Accuracy of the network on the 4883 val videos: 48.2%
[2025-01-10 21:56:15,698] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 21:56:15,700] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 21:56:15,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 21:56:15,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 21:56:18,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 21:56:18,195] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.16%
Epoch: [25]  [   0/1404]  eta: 3:17:39  lr: 0.000037  min_lr: 0.000000  loss: 4.0169 (4.0169)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4471  data: 7.9773  max mem: 15572
Epoch: [25]  [  10/1404]  eta: 0:31:02  lr: 0.000036  min_lr: 0.000000  loss: 3.7889 (3.6467)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3364  data: 0.7257  max mem: 15572
Epoch: [25]  [  20/1404]  eta: 0:22:32  lr: 0.000036  min_lr: 0.000000  loss: 3.7889 (3.7813)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0007  max mem: 15572
Epoch: [25]  [  30/1404]  eta: 0:19:04  lr: 0.000036  min_lr: 0.000000  loss: 3.8533 (3.8077)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0008  max mem: 15572
Epoch: [25]  [  40/1404]  eta: 0:17:08  lr: 0.000036  min_lr: 0.000000  loss: 3.9074 (3.8388)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0007  max mem: 15572
Epoch: [25]  [  50/1404]  eta: 0:16:18  lr: 0.000036  min_lr: 0.000000  loss: 3.7034 (3.7553)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0006  max mem: 15572
Epoch: [25]  [  60/1404]  eta: 0:15:34  lr: 0.000036  min_lr: 0.000000  loss: 3.4909 (3.7179)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0005  max mem: 15572
Epoch: [25]  [  70/1404]  eta: 0:15:10  lr: 0.000036  min_lr: 0.000000  loss: 3.6752 (3.7356)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0006  max mem: 15572
Epoch: [25]  [  80/1404]  eta: 0:14:40  lr: 0.000036  min_lr: 0.000000  loss: 3.6752 (3.7284)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0008  max mem: 15572
Epoch: [25]  [  90/1404]  eta: 0:14:14  lr: 0.000036  min_lr: 0.000000  loss: 3.6618 (3.7266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0011  max mem: 15572
Epoch: [25]  [ 100/1404]  eta: 0:14:05  lr: 0.000036  min_lr: 0.000000  loss: 3.7057 (3.7238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0010  max mem: 15572
[2025-01-10 21:57:28,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:57:28,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:57:28,425] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:57:28,425] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 110/1404]  eta: 0:13:57  lr: 0.000036  min_lr: 0.000000  loss: 3.7841 (3.7409)  loss_scale: 32768.0000 (33948.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0010  max mem: 15572
[2025-01-10 21:57:31,654] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35213
[2025-01-10 21:57:31,654] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:57:31,656] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35213
[2025-01-10 21:57:31,656] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:57:31,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 120/1404]  eta: 0:13:40  lr: 0.000036  min_lr: 0.000000  loss: 3.7107 (3.7260)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0009  max mem: 15572
Epoch: [25]  [ 130/1404]  eta: 0:13:33  lr: 0.000036  min_lr: 0.000000  loss: 3.6127 (3.7271)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0185  max mem: 15572
Epoch: [25]  [ 140/1404]  eta: 0:13:27  lr: 0.000036  min_lr: 0.000000  loss: 3.5631 (3.7128)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0186  max mem: 15572
Epoch: [25]  [ 150/1404]  eta: 0:13:14  lr: 0.000036  min_lr: 0.000000  loss: 3.7865 (3.7301)  loss_scale: 32768.0000 (34070.0397)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0352  max mem: 15572
Epoch: [25]  [ 160/1404]  eta: 0:13:05  lr: 0.000036  min_lr: 0.000000  loss: 3.8156 (3.7255)  loss_scale: 32768.0000 (33989.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0692  max mem: 15572
Epoch: [25]  [ 170/1404]  eta: 0:12:58  lr: 0.000036  min_lr: 0.000000  loss: 3.5359 (3.7131)  loss_scale: 32768.0000 (33917.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0511  max mem: 15572
Epoch: [25]  [ 180/1404]  eta: 0:12:50  lr: 0.000036  min_lr: 0.000000  loss: 3.4391 (3.6982)  loss_scale: 32768.0000 (33854.2320)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0549  max mem: 15572
Epoch: [25]  [ 190/1404]  eta: 0:12:37  lr: 0.000036  min_lr: 0.000000  loss: 3.1888 (3.6894)  loss_scale: 32768.0000 (33797.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0509  max mem: 15572
Epoch: [25]  [ 200/1404]  eta: 0:12:26  lr: 0.000036  min_lr: 0.000000  loss: 3.5707 (3.6843)  loss_scale: 32768.0000 (33746.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0356  max mem: 15572
Epoch: [25]  [ 210/1404]  eta: 0:12:19  lr: 0.000036  min_lr: 0.000000  loss: 3.5786 (3.6828)  loss_scale: 32768.0000 (33699.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0712  max mem: 15572
Epoch: [25]  [ 220/1404]  eta: 0:12:10  lr: 0.000036  min_lr: 0.000000  loss: 3.4920 (3.6708)  loss_scale: 32768.0000 (33657.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0914  max mem: 15572
Epoch: [25]  [ 230/1404]  eta: 0:12:03  lr: 0.000036  min_lr: 0.000000  loss: 3.6166 (3.6784)  loss_scale: 32768.0000 (33619.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0988  max mem: 15572
Epoch: [25]  [ 240/1404]  eta: 0:11:58  lr: 0.000036  min_lr: 0.000000  loss: 3.5995 (3.6647)  loss_scale: 32768.0000 (33583.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0559  max mem: 15572
[2025-01-10 21:58:48,114] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:58:48,115] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:58:48,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 21:58:48,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 21:58:51,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35349
[2025-01-10 21:58:51,743] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 21:58:51,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 21:58:51,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35349
[2025-01-10 21:58:51,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [ 250/1404]  eta: 0:11:48  lr: 0.000036  min_lr: 0.000000  loss: 3.3371 (3.6612)  loss_scale: 32768.0000 (34465.1474)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0006  max mem: 15572
Epoch: [25]  [ 260/1404]  eta: 0:11:42  lr: 0.000036  min_lr: 0.000000  loss: 3.2562 (3.6420)  loss_scale: 32768.0000 (34400.1226)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0006  max mem: 15572
Epoch: [25]  [ 270/1404]  eta: 0:11:38  lr: 0.000036  min_lr: 0.000000  loss: 3.2335 (3.6440)  loss_scale: 32768.0000 (34339.8967)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0552  max mem: 15572
Epoch: [25]  [ 280/1404]  eta: 0:11:30  lr: 0.000036  min_lr: 0.000000  loss: 3.6912 (3.6454)  loss_scale: 32768.0000 (34283.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0863  max mem: 15572
Epoch: [25]  [ 290/1404]  eta: 0:11:25  lr: 0.000036  min_lr: 0.000000  loss: 3.6673 (3.6419)  loss_scale: 32768.0000 (34231.8625)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0318  max mem: 15572
Epoch: [25]  [ 300/1404]  eta: 0:11:15  lr: 0.000036  min_lr: 0.000000  loss: 3.4890 (3.6331)  loss_scale: 32768.0000 (34183.2292)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0010  max mem: 15572
Epoch: [25]  [ 310/1404]  eta: 0:11:07  lr: 0.000036  min_lr: 0.000000  loss: 3.5548 (3.6337)  loss_scale: 32768.0000 (34137.7235)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0008  max mem: 15572
Epoch: [25]  [ 320/1404]  eta: 0:11:05  lr: 0.000036  min_lr: 0.000000  loss: 3.6700 (3.6321)  loss_scale: 32768.0000 (34095.0530)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.0009  max mem: 15572
Epoch: [25]  [ 330/1404]  eta: 0:10:56  lr: 0.000036  min_lr: 0.000000  loss: 3.6667 (3.6374)  loss_scale: 32768.0000 (34054.9607)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0010  max mem: 15572
Epoch: [25]  [ 340/1404]  eta: 0:10:50  lr: 0.000036  min_lr: 0.000000  loss: 3.6516 (3.6375)  loss_scale: 32768.0000 (34017.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0007  max mem: 15572
Epoch: [25]  [ 350/1404]  eta: 0:10:41  lr: 0.000035  min_lr: 0.000000  loss: 3.6323 (3.6352)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0007  max mem: 15572
Epoch: [25]  [ 360/1404]  eta: 0:10:36  lr: 0.000035  min_lr: 0.000000  loss: 3.6323 (3.6383)  loss_scale: 32768.0000 (33948.0111)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0007  max mem: 15572
Epoch: [25]  [ 370/1404]  eta: 0:10:29  lr: 0.000035  min_lr: 0.000000  loss: 3.9409 (3.6485)  loss_scale: 32768.0000 (33916.2049)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0006  max mem: 15572
[2025-01-10 22:00:09,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:00:09,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:00:09,941] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:00:09,941] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 380/1404]  eta: 0:10:24  lr: 0.000035  min_lr: 0.000000  loss: 3.7573 (3.6477)  loss_scale: 32768.0000 (34144.0840)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0007  max mem: 15572
[2025-01-10 22:00:16,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35490
[2025-01-10 22:00:16,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:00:16,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35490
[2025-01-10 22:00:16,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:00:16,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 390/1404]  eta: 0:10:16  lr: 0.000035  min_lr: 0.000000  loss: 3.6221 (3.6481)  loss_scale: 65536.0000 (34863.1407)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0008  max mem: 15572
Epoch: [25]  [ 400/1404]  eta: 0:10:08  lr: 0.000035  min_lr: 0.000000  loss: 3.6793 (3.6497)  loss_scale: 32768.0000 (34810.8928)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0007  max mem: 15572
Epoch: [25]  [ 410/1404]  eta: 0:10:00  lr: 0.000035  min_lr: 0.000000  loss: 3.6930 (3.6475)  loss_scale: 32768.0000 (34761.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0009  max mem: 15572
Epoch: [25]  [ 420/1404]  eta: 0:09:54  lr: 0.000035  min_lr: 0.000000  loss: 3.7031 (3.6463)  loss_scale: 32768.0000 (34713.8432)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0010  max mem: 15572
Epoch: [25]  [ 430/1404]  eta: 0:09:48  lr: 0.000035  min_lr: 0.000000  loss: 3.6155 (3.6438)  loss_scale: 32768.0000 (34668.6961)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0217  max mem: 15572
Epoch: [25]  [ 440/1404]  eta: 0:09:41  lr: 0.000035  min_lr: 0.000000  loss: 3.6155 (3.6429)  loss_scale: 32768.0000 (34625.5964)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.0217  max mem: 15572
Epoch: [25]  [ 450/1404]  eta: 0:09:34  lr: 0.000035  min_lr: 0.000000  loss: 3.6453 (3.6460)  loss_scale: 32768.0000 (34584.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0006  max mem: 15572
[2025-01-10 22:00:52,408] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35554
[2025-01-10 22:00:52,408] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 22:00:52,408] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-10 22:00:52,408] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35554
[2025-01-10 22:00:52,408] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [25]  [ 460/1404]  eta: 0:09:29  lr: 0.000035  min_lr: 0.000000  loss: 3.8302 (3.6438)  loss_scale: 32768.0000 (34296.2256)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0006  max mem: 15572
Epoch: [25]  [ 470/1404]  eta: 0:09:23  lr: 0.000035  min_lr: 0.000000  loss: 3.8325 (3.6444)  loss_scale: 16384.0000 (33915.9236)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0007  max mem: 15572
Epoch: [25]  [ 480/1404]  eta: 0:09:18  lr: 0.000035  min_lr: 0.000000  loss: 3.4157 (3.6457)  loss_scale: 16384.0000 (33551.4345)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.0006  max mem: 15572
Epoch: [25]  [ 490/1404]  eta: 0:09:11  lr: 0.000035  min_lr: 0.000000  loss: 3.4060 (3.6474)  loss_scale: 16384.0000 (33201.7923)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0008  max mem: 15572
Epoch: [25]  [ 500/1404]  eta: 0:09:06  lr: 0.000035  min_lr: 0.000000  loss: 3.7377 (3.6507)  loss_scale: 16384.0000 (32866.1078)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0007  max mem: 15572
Epoch: [25]  [ 510/1404]  eta: 0:08:58  lr: 0.000035  min_lr: 0.000000  loss: 3.8156 (3.6546)  loss_scale: 16384.0000 (32543.5616)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0004  max mem: 15572
Epoch: [25]  [ 520/1404]  eta: 0:08:51  lr: 0.000035  min_lr: 0.000000  loss: 3.7688 (3.6545)  loss_scale: 16384.0000 (32233.3973)  weight_decay: 0.0500 (0.0500)  time: 0.5093  data: 0.0005  max mem: 15572
Epoch: [25]  [ 530/1404]  eta: 0:08:46  lr: 0.000035  min_lr: 0.000000  loss: 3.7618 (3.6572)  loss_scale: 16384.0000 (31934.9153)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0006  max mem: 15572
Epoch: [25]  [ 540/1404]  eta: 0:08:39  lr: 0.000035  min_lr: 0.000000  loss: 3.7357 (3.6568)  loss_scale: 16384.0000 (31647.4677)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0009  max mem: 15572
Epoch: [25]  [ 550/1404]  eta: 0:08:34  lr: 0.000035  min_lr: 0.000000  loss: 3.4757 (3.6502)  loss_scale: 16384.0000 (31370.4537)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0009  max mem: 15572
Epoch: [25]  [ 560/1404]  eta: 0:08:27  lr: 0.000035  min_lr: 0.000000  loss: 3.5542 (3.6529)  loss_scale: 16384.0000 (31103.3155)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0006  max mem: 15572
Epoch: [25]  [ 570/1404]  eta: 0:08:20  lr: 0.000035  min_lr: 0.000000  loss: 3.4878 (3.6474)  loss_scale: 16384.0000 (30845.5342)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0007  max mem: 15572
Epoch: [25]  [ 580/1404]  eta: 0:08:15  lr: 0.000035  min_lr: 0.000000  loss: 3.4408 (3.6461)  loss_scale: 16384.0000 (30596.6265)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0008  max mem: 15572
[2025-01-10 22:02:09,255] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:02:09,255] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 22:02:09,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:02:09,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 590/1404]  eta: 0:08:11  lr: 0.000035  min_lr: 0.000000  loss: 3.6318 (3.6454)  loss_scale: 16384.0000 (30577.9222)  weight_decay: 0.0500 (0.0500)  time: 0.6815  data: 0.0006  max mem: 15572
Epoch: [25]  [ 600/1404]  eta: 0:08:04  lr: 0.000035  min_lr: 0.000000  loss: 3.6369 (3.6455)  loss_scale: 32768.0000 (30614.3627)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0007  max mem: 15572
Epoch: [25]  [ 610/1404]  eta: 0:07:57  lr: 0.000035  min_lr: 0.000000  loss: 3.5224 (3.6426)  loss_scale: 32768.0000 (30649.6105)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0007  max mem: 15572
Epoch: [25]  [ 620/1404]  eta: 0:07:50  lr: 0.000035  min_lr: 0.000000  loss: 3.2585 (3.6381)  loss_scale: 32768.0000 (30683.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0008  max mem: 15572
Epoch: [25]  [ 630/1404]  eta: 0:07:43  lr: 0.000035  min_lr: 0.000000  loss: 3.5784 (3.6407)  loss_scale: 32768.0000 (30716.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0011  max mem: 15572
Epoch: [25]  [ 640/1404]  eta: 0:07:37  lr: 0.000035  min_lr: 0.000000  loss: 3.5784 (3.6343)  loss_scale: 32768.0000 (30748.7551)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0012  max mem: 15572
Epoch: [25]  [ 650/1404]  eta: 0:07:31  lr: 0.000035  min_lr: 0.000000  loss: 3.7853 (3.6394)  loss_scale: 32768.0000 (30779.7727)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0009  max mem: 15572
Epoch: [25]  [ 660/1404]  eta: 0:07:24  lr: 0.000035  min_lr: 0.000000  loss: 3.8259 (3.6387)  loss_scale: 32768.0000 (30809.8517)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0008  max mem: 15572
Epoch: [25]  [ 670/1404]  eta: 0:07:18  lr: 0.000035  min_lr: 0.000000  loss: 3.4315 (3.6343)  loss_scale: 32768.0000 (30839.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.0009  max mem: 15572
Epoch: [25]  [ 680/1404]  eta: 0:07:12  lr: 0.000035  min_lr: 0.000000  loss: 3.3515 (3.6313)  loss_scale: 32768.0000 (30867.3598)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.0013  max mem: 15572
Epoch: [25]  [ 690/1404]  eta: 0:07:07  lr: 0.000034  min_lr: 0.000000  loss: 3.5865 (3.6345)  loss_scale: 32768.0000 (30894.8654)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0013  max mem: 15572
Epoch: [25]  [ 700/1404]  eta: 0:07:01  lr: 0.000034  min_lr: 0.000000  loss: 3.8652 (3.6376)  loss_scale: 32768.0000 (30921.5863)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0006  max mem: 15572
Epoch: [25]  [ 710/1404]  eta: 0:06:55  lr: 0.000034  min_lr: 0.000000  loss: 3.7301 (3.6348)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0005  max mem: 15572
[2025-01-10 22:03:24,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:03:24,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:03:24,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:03:24,218] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:03:24,730] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35812
[2025-01-10 22:03:24,731] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:03:24,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35812
[2025-01-10 22:03:24,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:03:24,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 720/1404]  eta: 0:06:48  lr: 0.000034  min_lr: 0.000000  loss: 3.5840 (3.6349)  loss_scale: 32768.0000 (31018.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0097  max mem: 15572
Epoch: [25]  [ 730/1404]  eta: 0:06:42  lr: 0.000034  min_lr: 0.000000  loss: 3.7716 (3.6385)  loss_scale: 32768.0000 (31042.1888)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0098  max mem: 15572
Epoch: [25]  [ 740/1404]  eta: 0:06:36  lr: 0.000034  min_lr: 0.000000  loss: 3.8238 (3.6401)  loss_scale: 32768.0000 (31065.4791)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0008  max mem: 15572
Epoch: [25]  [ 750/1404]  eta: 0:06:30  lr: 0.000034  min_lr: 0.000000  loss: 3.6700 (3.6399)  loss_scale: 32768.0000 (31088.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0008  max mem: 15572
Epoch: [25]  [ 760/1404]  eta: 0:06:24  lr: 0.000034  min_lr: 0.000000  loss: 3.4505 (3.6366)  loss_scale: 32768.0000 (31110.2234)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0007  max mem: 15572
Epoch: [25]  [ 770/1404]  eta: 0:06:18  lr: 0.000034  min_lr: 0.000000  loss: 3.4505 (3.6354)  loss_scale: 32768.0000 (31131.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0006  max mem: 15572
Epoch: [25]  [ 780/1404]  eta: 0:06:12  lr: 0.000034  min_lr: 0.000000  loss: 3.4628 (3.6339)  loss_scale: 32768.0000 (31152.6761)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0007  max mem: 15572
Epoch: [25]  [ 790/1404]  eta: 0:06:06  lr: 0.000034  min_lr: 0.000000  loss: 3.4319 (3.6309)  loss_scale: 32768.0000 (31173.0973)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0009  max mem: 15572
Epoch: [25]  [ 800/1404]  eta: 0:06:00  lr: 0.000034  min_lr: 0.000000  loss: 3.5716 (3.6313)  loss_scale: 32768.0000 (31193.0087)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0007  max mem: 15572
Epoch: [25]  [ 810/1404]  eta: 0:05:55  lr: 0.000034  min_lr: 0.000000  loss: 3.6157 (3.6307)  loss_scale: 32768.0000 (31212.4291)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0005  max mem: 15572
Epoch: [25]  [ 820/1404]  eta: 0:05:49  lr: 0.000034  min_lr: 0.000000  loss: 3.7382 (3.6304)  loss_scale: 32768.0000 (31231.3764)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0006  max mem: 15572
Epoch: [25]  [ 830/1404]  eta: 0:05:43  lr: 0.000034  min_lr: 0.000000  loss: 3.6195 (3.6280)  loss_scale: 32768.0000 (31249.8676)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0007  max mem: 15572
Epoch: [25]  [ 840/1404]  eta: 0:05:36  lr: 0.000034  min_lr: 0.000000  loss: 3.6478 (3.6309)  loss_scale: 32768.0000 (31267.9191)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0007  max mem: 15572
[2025-01-10 22:04:40,924] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:04:40,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:04:40,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:04:40,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:04:44,873] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35948
[2025-01-10 22:04:44,873] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:04:44,979] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35948
[2025-01-10 22:04:44,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:04:44,980] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 850/1404]  eta: 0:05:30  lr: 0.000034  min_lr: 0.000000  loss: 3.6703 (3.6303)  loss_scale: 32768.0000 (31555.0834)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0035  max mem: 15572
Epoch: [25]  [ 860/1404]  eta: 0:05:24  lr: 0.000034  min_lr: 0.000000  loss: 3.7254 (3.6316)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0039  max mem: 15572
Epoch: [25]  [ 870/1404]  eta: 0:05:18  lr: 0.000034  min_lr: 0.000000  loss: 3.5192 (3.6279)  loss_scale: 32768.0000 (31582.9346)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0154  max mem: 15572
Epoch: [25]  [ 880/1404]  eta: 0:05:12  lr: 0.000034  min_lr: 0.000000  loss: 3.0153 (3.6225)  loss_scale: 32768.0000 (31596.3859)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0150  max mem: 15572
Epoch: [25]  [ 890/1404]  eta: 0:05:06  lr: 0.000034  min_lr: 0.000000  loss: 3.3755 (3.6229)  loss_scale: 32768.0000 (31609.5354)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0170  max mem: 15572
[2025-01-10 22:05:15,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=241, lr=[3.284183656734508e-07, 3.284183656734508e-07, 4.6916909381921544e-07, 4.6916909381921544e-07, 6.702415625988792e-07, 6.702415625988792e-07, 9.574879465698275e-07, 9.574879465698275e-07, 1.3678399236711823e-06, 1.3678399236711823e-06, 1.9540570338159747e-06, 1.9540570338159747e-06, 2.7915100483085355e-06, 2.7915100483085355e-06, 3.9878714975836224e-06, 3.9878714975836224e-06, 5.696959282262318e-06, 5.696959282262318e-06, 8.13851326037474e-06, 8.13851326037474e-06, 1.1626447514821058e-05, 1.1626447514821058e-05, 1.6609210735458655e-05, 1.6609210735458655e-05, 2.372744390779808e-05, 2.372744390779808e-05, 3.389634843971155e-05, 3.389634843971155e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:05:15,278] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=45.43422538538816, CurrSamplesPerSec=46.176014488133205, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [ 900/1404]  eta: 0:05:00  lr: 0.000034  min_lr: 0.000000  loss: 3.4195 (3.6197)  loss_scale: 32768.0000 (31622.3929)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0636  max mem: 15572
Epoch: [25]  [ 910/1404]  eta: 0:04:54  lr: 0.000034  min_lr: 0.000000  loss: 3.4249 (3.6198)  loss_scale: 32768.0000 (31634.9682)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0474  max mem: 15572
Epoch: [25]  [ 920/1404]  eta: 0:04:48  lr: 0.000034  min_lr: 0.000000  loss: 3.5655 (3.6169)  loss_scale: 32768.0000 (31647.2704)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.0425  max mem: 15572
Epoch: [25]  [ 930/1404]  eta: 0:04:42  lr: 0.000034  min_lr: 0.000000  loss: 3.7222 (3.6181)  loss_scale: 32768.0000 (31659.3083)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0452  max mem: 15572
Epoch: [25]  [ 940/1404]  eta: 0:04:36  lr: 0.000034  min_lr: 0.000000  loss: 3.7222 (3.6181)  loss_scale: 32768.0000 (31671.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0141  max mem: 15572
Epoch: [25]  [ 950/1404]  eta: 0:04:30  lr: 0.000034  min_lr: 0.000000  loss: 3.6974 (3.6205)  loss_scale: 32768.0000 (31682.6246)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0524  max mem: 15572
Epoch: [25]  [ 960/1404]  eta: 0:04:24  lr: 0.000034  min_lr: 0.000000  loss: 3.6707 (3.6201)  loss_scale: 32768.0000 (31693.9188)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0743  max mem: 15572
Epoch: [25]  [ 970/1404]  eta: 0:04:18  lr: 0.000034  min_lr: 0.000000  loss: 3.6561 (3.6214)  loss_scale: 32768.0000 (31704.9804)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0454  max mem: 15572
[2025-01-10 22:06:02,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:06:02,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:06:02,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:06:02,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:06:04,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36080
[2025-01-10 22:06:04,099] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:06:04,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36080
[2025-01-10 22:06:04,099] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:06:04,099] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 980/1404]  eta: 0:04:13  lr: 0.000034  min_lr: 0.000000  loss: 3.4563 (3.6177)  loss_scale: 32768.0000 (31816.0245)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1295  max mem: 15572
Epoch: [25]  [ 990/1404]  eta: 0:04:06  lr: 0.000034  min_lr: 0.000000  loss: 3.4631 (3.6179)  loss_scale: 32768.0000 (31825.6307)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1173  max mem: 15572
Epoch: [25]  [1000/1404]  eta: 0:04:00  lr: 0.000034  min_lr: 0.000000  loss: 3.5603 (3.6172)  loss_scale: 32768.0000 (31835.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0007  max mem: 15572
Epoch: [25]  [1010/1404]  eta: 0:03:54  lr: 0.000034  min_lr: 0.000000  loss: 3.4962 (3.6167)  loss_scale: 32768.0000 (31844.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0007  max mem: 15572
Epoch: [25]  [1020/1404]  eta: 0:03:48  lr: 0.000034  min_lr: 0.000000  loss: 3.5224 (3.6164)  loss_scale: 32768.0000 (31853.3203)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0087  max mem: 15572
Epoch: [25]  [1030/1404]  eta: 0:03:42  lr: 0.000034  min_lr: 0.000000  loss: 3.5224 (3.6157)  loss_scale: 32768.0000 (31862.1920)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0238  max mem: 15572
Epoch: [25]  [1040/1404]  eta: 0:03:36  lr: 0.000033  min_lr: 0.000000  loss: 3.5132 (3.6154)  loss_scale: 32768.0000 (31870.8934)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0476  max mem: 15572
Epoch: [25]  [1050/1404]  eta: 0:03:30  lr: 0.000033  min_lr: 0.000000  loss: 3.4361 (3.6142)  loss_scale: 32768.0000 (31879.4291)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0326  max mem: 15572
Epoch: [25]  [1060/1404]  eta: 0:03:24  lr: 0.000033  min_lr: 0.000000  loss: 3.5794 (3.6151)  loss_scale: 32768.0000 (31887.8040)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0007  max mem: 15572
Epoch: [25]  [1070/1404]  eta: 0:03:18  lr: 0.000033  min_lr: 0.000000  loss: 3.6472 (3.6132)  loss_scale: 32768.0000 (31896.0224)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0010  max mem: 15572
Epoch: [25]  [1080/1404]  eta: 0:03:12  lr: 0.000033  min_lr: 0.000000  loss: 3.6292 (3.6133)  loss_scale: 32768.0000 (31904.0888)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0010  max mem: 15572
Epoch: [25]  [1090/1404]  eta: 0:03:07  lr: 0.000033  min_lr: 0.000000  loss: 3.5359 (3.6123)  loss_scale: 32768.0000 (31912.0073)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0007  max mem: 15572
Epoch: [25]  [1100/1404]  eta: 0:03:00  lr: 0.000033  min_lr: 0.000000  loss: 3.4063 (3.6113)  loss_scale: 32768.0000 (31919.7820)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0007  max mem: 15572
[2025-01-10 22:07:19,010] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:07:19,010] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:07:19,025] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:07:19,026] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:07:19,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36210
[2025-01-10 22:07:19,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:07:19,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36210
[2025-01-10 22:07:19,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:07:19,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1110/1404]  eta: 0:02:54  lr: 0.000033  min_lr: 0.000000  loss: 3.4479 (3.6116)  loss_scale: 32768.0000 (31956.9109)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0008  max mem: 15572
Epoch: [25]  [1120/1404]  eta: 0:02:48  lr: 0.000033  min_lr: 0.000000  loss: 3.6886 (3.6119)  loss_scale: 32768.0000 (31964.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0009  max mem: 15572
Epoch: [25]  [1130/1404]  eta: 0:02:42  lr: 0.000033  min_lr: 0.000000  loss: 3.6886 (3.6117)  loss_scale: 32768.0000 (31971.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0009  max mem: 15572
Epoch: [25]  [1140/1404]  eta: 0:02:36  lr: 0.000033  min_lr: 0.000000  loss: 3.7753 (3.6124)  loss_scale: 32768.0000 (31978.2366)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0009  max mem: 15572
Epoch: [25]  [1150/1404]  eta: 0:02:30  lr: 0.000033  min_lr: 0.000000  loss: 3.7627 (3.6120)  loss_scale: 32768.0000 (31985.0982)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0245  max mem: 15572
Epoch: [25]  [1160/1404]  eta: 0:02:25  lr: 0.000033  min_lr: 0.000000  loss: 3.5900 (3.6122)  loss_scale: 32768.0000 (31991.8415)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.0440  max mem: 15572
Epoch: [25]  [1170/1404]  eta: 0:02:19  lr: 0.000033  min_lr: 0.000000  loss: 3.6702 (3.6124)  loss_scale: 32768.0000 (31998.4697)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0203  max mem: 15572
Epoch: [25]  [1180/1404]  eta: 0:02:13  lr: 0.000033  min_lr: 0.000000  loss: 3.7884 (3.6138)  loss_scale: 32768.0000 (32004.9856)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0007  max mem: 15572
Epoch: [25]  [1190/1404]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000000  loss: 3.7884 (3.6149)  loss_scale: 32768.0000 (32011.3921)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0006  max mem: 15572
Epoch: [25]  [1200/1404]  eta: 0:02:01  lr: 0.000033  min_lr: 0.000000  loss: 3.7173 (3.6149)  loss_scale: 32768.0000 (32017.6919)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0006  max mem: 15572
Epoch: [25]  [1210/1404]  eta: 0:01:55  lr: 0.000033  min_lr: 0.000000  loss: 3.5627 (3.6143)  loss_scale: 32768.0000 (32023.8877)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0010  max mem: 15572
Epoch: [25]  [1220/1404]  eta: 0:01:49  lr: 0.000033  min_lr: 0.000000  loss: 3.4303 (3.6131)  loss_scale: 32768.0000 (32029.9820)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0011  max mem: 15572
Epoch: [25]  [1230/1404]  eta: 0:01:43  lr: 0.000033  min_lr: 0.000000  loss: 3.8552 (3.6129)  loss_scale: 32768.0000 (32035.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0007  max mem: 15572
[2025-01-10 22:08:36,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:08:36,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:08:36,898] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:08:36,899] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1240/1404]  eta: 0:01:37  lr: 0.000033  min_lr: 0.000000  loss: 3.2996 (3.6103)  loss_scale: 32768.0000 (32094.6849)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0009  max mem: 15572
[2025-01-10 22:08:39,884] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36345
[2025-01-10 22:08:39,884] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:08:39,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:08:39,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36345
[2025-01-10 22:08:39,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [1250/1404]  eta: 0:01:31  lr: 0.000033  min_lr: 0.000000  loss: 3.2996 (3.6085)  loss_scale: 32768.0000 (32204.8409)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0014  max mem: 15572
Epoch: [25]  [1260/1404]  eta: 0:01:25  lr: 0.000033  min_lr: 0.000000  loss: 3.6689 (3.6083)  loss_scale: 32768.0000 (32209.3069)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0013  max mem: 15572
Epoch: [25]  [1270/1404]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000000  loss: 3.7369 (3.6095)  loss_scale: 32768.0000 (32213.7026)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0010  max mem: 15572
Epoch: [25]  [1280/1404]  eta: 0:01:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8313 (3.6107)  loss_scale: 32768.0000 (32218.0297)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0011  max mem: 15572
Epoch: [25]  [1290/1404]  eta: 0:01:07  lr: 0.000033  min_lr: 0.000000  loss: 3.7079 (3.6095)  loss_scale: 32768.0000 (32222.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0009  max mem: 15572
Epoch: [25]  [1300/1404]  eta: 0:01:01  lr: 0.000033  min_lr: 0.000000  loss: 3.6846 (3.6111)  loss_scale: 32768.0000 (32226.4842)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0006  max mem: 15572
Epoch: [25]  [1310/1404]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000000  loss: 3.7182 (3.6118)  loss_scale: 32768.0000 (32230.6148)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0005  max mem: 15572
Epoch: [25]  [1320/1404]  eta: 0:00:49  lr: 0.000033  min_lr: 0.000000  loss: 3.9033 (3.6125)  loss_scale: 32768.0000 (32234.6828)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0006  max mem: 15572
Epoch: [25]  [1330/1404]  eta: 0:00:43  lr: 0.000033  min_lr: 0.000000  loss: 3.5825 (3.6114)  loss_scale: 32768.0000 (32238.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0007  max mem: 15572
Epoch: [25]  [1340/1404]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000000  loss: 3.7237 (3.6132)  loss_scale: 32768.0000 (32242.6368)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0007  max mem: 15572
Epoch: [25]  [1350/1404]  eta: 0:00:32  lr: 0.000033  min_lr: 0.000000  loss: 3.7433 (3.6122)  loss_scale: 32768.0000 (32246.5255)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0006  max mem: 15572
Epoch: [25]  [1360/1404]  eta: 0:00:26  lr: 0.000033  min_lr: 0.000000  loss: 3.6589 (3.6126)  loss_scale: 32768.0000 (32250.3571)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.0006  max mem: 15572
Epoch: [25]  [1370/1404]  eta: 0:00:20  lr: 0.000033  min_lr: 0.000000  loss: 3.6526 (3.6110)  loss_scale: 32768.0000 (32254.1327)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0007  max mem: 15572
[2025-01-10 22:09:55,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:09:55,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:09:55,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:09:55,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1380/1404]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000000  loss: 3.1874 (3.6086)  loss_scale: 32768.0000 (32423.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0007  max mem: 15572
[2025-01-10 22:09:59,866] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36483
[2025-01-10 22:09:59,866] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:09:59,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:09:59,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36483
[2025-01-10 22:09:59,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [1390/1404]  eta: 0:00:08  lr: 0.000032  min_lr: 0.000000  loss: 3.5189 (3.6102)  loss_scale: 32768.0000 (32473.5356)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0006  max mem: 15572
Epoch: [25]  [1400/1404]  eta: 0:00:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9170 (3.6111)  loss_scale: 32768.0000 (32475.6374)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0004  max mem: 15572
Epoch: [25]  [1403/1404]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000000  loss: 3.8482 (3.6115)  loss_scale: 32768.0000 (32476.2621)  weight_decay: 0.0500 (0.0500)  time: 0.4650  data: 0.0004  max mem: 15572
Epoch: [25] Total time: 0:13:51 (0.5920 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000000  loss: 3.8482 (3.6158)  loss_scale: 32768.0000 (32476.2621)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:39  loss: 1.5231 (1.5231)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.5853  data: 5.3195  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 2.1662 (2.1422)  acc1: 55.5556 (46.4646)  acc5: 77.7778 (78.2828)  time: 0.8264  data: 0.6144  max mem: 15572
Val:  [ 20/136]  eta: 0:01:11  loss: 2.3228 (2.2594)  acc1: 44.4444 (44.4444)  acc5: 77.7778 (76.4550)  time: 0.3707  data: 0.1516  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.2413 (2.1228)  acc1: 44.4444 (47.8495)  acc5: 77.7778 (78.1362)  time: 0.3066  data: 0.0802  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.7262 (2.0723)  acc1: 61.1111 (50.8130)  acc5: 83.3333 (78.7263)  time: 0.2951  data: 0.0769  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8687 (2.0583)  acc1: 55.5556 (51.0893)  acc5: 83.3333 (79.7386)  time: 0.3838  data: 0.1718  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 1.9706 (2.1338)  acc1: 44.4444 (48.5428)  acc5: 77.7778 (78.8707)  time: 0.4043  data: 0.1962  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 1.9179 (2.1012)  acc1: 50.0000 (49.5305)  acc5: 77.7778 (79.0297)  time: 0.3730  data: 0.1652  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9179 (2.1039)  acc1: 50.0000 (48.9026)  acc5: 83.3333 (79.4925)  time: 0.3575  data: 0.1504  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0681 (2.1133)  acc1: 44.4444 (48.5958)  acc5: 83.3333 (79.3040)  time: 0.3827  data: 0.1627  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2799 (2.1883)  acc1: 38.8889 (46.8097)  acc5: 77.7778 (77.7228)  time: 0.3731  data: 0.1342  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2484 (2.1792)  acc1: 38.8889 (47.1471)  acc5: 77.7778 (77.7277)  time: 0.3319  data: 0.1092  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8230 (2.1336)  acc1: 55.5556 (48.3471)  acc5: 83.3333 (78.4206)  time: 0.3340  data: 0.1316  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.7157 (2.0961)  acc1: 61.1111 (49.3215)  acc5: 88.8889 (78.9652)  time: 0.2559  data: 0.0800  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8230 (2.0912)  acc1: 55.5556 (49.5905)  acc5: 83.3333 (79.1155)  time: 0.1544  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.3723 s / it)
* Acc@1 48.976 Acc@5 78.235 loss 2.140
Accuracy of the network on the 4883 val videos: 49.0%
[2025-01-10 22:11:00,032] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 22:11:00,035] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 22:11:00,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 22:11:00,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 22:11:02,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 22:11:02,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.98%
Epoch: [26]  [   0/1404]  eta: 3:28:06  lr: 0.000032  min_lr: 0.000000  loss: 4.0667 (4.0667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8935  data: 6.7368  max mem: 15572
Epoch: [26]  [  10/1404]  eta: 0:30:19  lr: 0.000032  min_lr: 0.000000  loss: 3.6535 (3.5417)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3055  data: 0.6130  max mem: 15572
Epoch: [26]  [  20/1404]  eta: 0:22:51  lr: 0.000032  min_lr: 0.000000  loss: 3.6778 (3.6534)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0009  max mem: 15572
Epoch: [26]  [  30/1404]  eta: 0:19:25  lr: 0.000032  min_lr: 0.000000  loss: 3.6145 (3.5777)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0011  max mem: 15572
Epoch: [26]  [  40/1404]  eta: 0:17:55  lr: 0.000032  min_lr: 0.000000  loss: 3.6145 (3.6311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0008  max mem: 15572
Epoch: [26]  [  50/1404]  eta: 0:16:37  lr: 0.000032  min_lr: 0.000000  loss: 3.5695 (3.5911)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0006  max mem: 15572
Epoch: [26]  [  60/1404]  eta: 0:15:48  lr: 0.000032  min_lr: 0.000000  loss: 3.4869 (3.5877)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0161  max mem: 15572
Epoch: [26]  [  70/1404]  eta: 0:15:21  lr: 0.000032  min_lr: 0.000000  loss: 3.5037 (3.5953)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0585  max mem: 15572
Epoch: [26]  [  80/1404]  eta: 0:14:52  lr: 0.000032  min_lr: 0.000000  loss: 3.4816 (3.5863)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0741  max mem: 15572
Epoch: [26]  [  90/1404]  eta: 0:14:43  lr: 0.000032  min_lr: 0.000000  loss: 3.7048 (3.6101)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1117  max mem: 15572
Epoch: [26]  [ 100/1404]  eta: 0:14:20  lr: 0.000032  min_lr: 0.000000  loss: 3.7272 (3.6105)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0913  max mem: 15572
[2025-01-10 22:12:13,556] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:12:13,557] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:12:13,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:12:13,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:12:15,220] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36614
[2025-01-10 22:12:15,221] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:12:15,305] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36614
[2025-01-10 22:12:15,306] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:12:15,307] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 110/1404]  eta: 0:14:07  lr: 0.000032  min_lr: 0.000000  loss: 3.7272 (3.6153)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0554  max mem: 15572
Epoch: [26]  [ 120/1404]  eta: 0:13:56  lr: 0.000032  min_lr: 0.000000  loss: 3.5545 (3.6063)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0645  max mem: 15572
Epoch: [26]  [ 130/1404]  eta: 0:13:43  lr: 0.000032  min_lr: 0.000000  loss: 3.5545 (3.5892)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0430  max mem: 15572
Epoch: [26]  [ 140/1404]  eta: 0:13:27  lr: 0.000032  min_lr: 0.000000  loss: 3.4687 (3.5765)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0344  max mem: 15572
Epoch: [26]  [ 150/1404]  eta: 0:13:17  lr: 0.000032  min_lr: 0.000000  loss: 3.5723 (3.5876)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0544  max mem: 15572
Epoch: [26]  [ 160/1404]  eta: 0:13:09  lr: 0.000032  min_lr: 0.000000  loss: 3.7056 (3.5906)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0931  max mem: 15572
Epoch: [26]  [ 170/1404]  eta: 0:13:01  lr: 0.000032  min_lr: 0.000000  loss: 3.6439 (3.5926)  loss_scale: 32768.0000 (33151.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1117  max mem: 15572
Epoch: [26]  [ 180/1404]  eta: 0:12:54  lr: 0.000032  min_lr: 0.000000  loss: 3.3756 (3.5745)  loss_scale: 32768.0000 (33130.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1226  max mem: 15572
Epoch: [26]  [ 190/1404]  eta: 0:12:45  lr: 0.000032  min_lr: 0.000000  loss: 3.2419 (3.5688)  loss_scale: 32768.0000 (33111.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1140  max mem: 15572
Epoch: [26]  [ 200/1404]  eta: 0:12:42  lr: 0.000032  min_lr: 0.000000  loss: 3.4192 (3.5656)  loss_scale: 32768.0000 (33094.0498)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1393  max mem: 15572
Epoch: [26]  [ 210/1404]  eta: 0:12:29  lr: 0.000032  min_lr: 0.000000  loss: 3.5816 (3.5651)  loss_scale: 32768.0000 (33078.5972)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0866  max mem: 15572
Epoch: [26]  [ 220/1404]  eta: 0:12:18  lr: 0.000032  min_lr: 0.000000  loss: 3.6414 (3.5681)  loss_scale: 32768.0000 (33064.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0005  max mem: 15572
Epoch: [26]  [ 230/1404]  eta: 0:12:05  lr: 0.000032  min_lr: 0.000000  loss: 3.5554 (3.5563)  loss_scale: 32768.0000 (33051.7056)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0008  max mem: 15572
[2025-01-10 22:13:30,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:13:30,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:13:30,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:13:30,534] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 240/1404]  eta: 0:11:56  lr: 0.000032  min_lr: 0.000000  loss: 3.5554 (3.5591)  loss_scale: 32768.0000 (33311.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0211  max mem: 15572
[2025-01-10 22:13:33,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36749
[2025-01-10 22:13:33,474] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:13:33,474] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:13:33,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36749
[2025-01-10 22:13:33,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [26]  [ 250/1404]  eta: 0:11:48  lr: 0.000032  min_lr: 0.000000  loss: 3.7294 (3.5679)  loss_scale: 32768.0000 (33812.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0685  max mem: 15572
Epoch: [26]  [ 260/1404]  eta: 0:11:40  lr: 0.000032  min_lr: 0.000000  loss: 3.9321 (3.5790)  loss_scale: 32768.0000 (33772.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0778  max mem: 15572
Epoch: [26]  [ 270/1404]  eta: 0:11:37  lr: 0.000032  min_lr: 0.000000  loss: 3.7046 (3.5748)  loss_scale: 32768.0000 (33735.3210)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1226  max mem: 15572
Epoch: [26]  [ 280/1404]  eta: 0:11:33  lr: 0.000032  min_lr: 0.000000  loss: 3.2651 (3.5704)  loss_scale: 32768.0000 (33700.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.1905  max mem: 15572
Epoch: [26]  [ 290/1404]  eta: 0:11:27  lr: 0.000032  min_lr: 0.000000  loss: 3.2651 (3.5711)  loss_scale: 32768.0000 (33668.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1734  max mem: 15572
Epoch: [26]  [ 300/1404]  eta: 0:11:18  lr: 0.000032  min_lr: 0.000000  loss: 3.6208 (3.5698)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0875  max mem: 15572
Epoch: [26]  [ 310/1404]  eta: 0:11:15  lr: 0.000032  min_lr: 0.000000  loss: 3.4018 (3.5678)  loss_scale: 32768.0000 (33610.9068)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0855  max mem: 15572
Epoch: [26]  [ 320/1404]  eta: 0:11:05  lr: 0.000032  min_lr: 0.000000  loss: 3.5553 (3.5655)  loss_scale: 32768.0000 (33584.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0739  max mem: 15572
Epoch: [26]  [ 330/1404]  eta: 0:10:58  lr: 0.000032  min_lr: 0.000000  loss: 3.6021 (3.5624)  loss_scale: 32768.0000 (33559.9758)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0006  max mem: 15572
Epoch: [26]  [ 340/1404]  eta: 0:10:51  lr: 0.000031  min_lr: 0.000000  loss: 3.6555 (3.5660)  loss_scale: 32768.0000 (33536.7507)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0129  max mem: 15572
Epoch: [26]  [ 350/1404]  eta: 0:10:42  lr: 0.000031  min_lr: 0.000000  loss: 3.7951 (3.5701)  loss_scale: 32768.0000 (33514.8490)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0217  max mem: 15572
Epoch: [26]  [ 360/1404]  eta: 0:10:33  lr: 0.000031  min_lr: 0.000000  loss: 3.8686 (3.5747)  loss_scale: 32768.0000 (33494.1607)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0095  max mem: 15572
Epoch: [26]  [ 370/1404]  eta: 0:10:25  lr: 0.000031  min_lr: 0.000000  loss: 3.7875 (3.5780)  loss_scale: 32768.0000 (33474.5876)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0007  max mem: 15572
[2025-01-10 22:14:49,289] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:14:49,289] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:14:49,303] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:14:49,303] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:14:53,018] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36884
[2025-01-10 22:14:53,018] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:14:53,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36884
[2025-01-10 22:14:53,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:14:53,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 380/1404]  eta: 0:10:19  lr: 0.000031  min_lr: 0.000000  loss: 3.6721 (3.5809)  loss_scale: 32768.0000 (33972.0735)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0393  max mem: 15572
Epoch: [26]  [ 390/1404]  eta: 0:10:12  lr: 0.000031  min_lr: 0.000000  loss: 3.6721 (3.5821)  loss_scale: 32768.0000 (33941.2788)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0393  max mem: 15572
Epoch: [26]  [ 400/1404]  eta: 0:10:04  lr: 0.000031  min_lr: 0.000000  loss: 3.6322 (3.5839)  loss_scale: 32768.0000 (33912.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0043  max mem: 15572
Epoch: [26]  [ 410/1404]  eta: 0:09:59  lr: 0.000031  min_lr: 0.000000  loss: 3.5573 (3.5800)  loss_scale: 32768.0000 (33884.1849)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0656  max mem: 15572
Epoch: [26]  [ 420/1404]  eta: 0:09:51  lr: 0.000031  min_lr: 0.000000  loss: 3.5155 (3.5763)  loss_scale: 32768.0000 (33857.6722)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0889  max mem: 15572
Epoch: [26]  [ 430/1404]  eta: 0:09:47  lr: 0.000031  min_lr: 0.000000  loss: 3.7609 (3.5773)  loss_scale: 32768.0000 (33832.3898)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1123  max mem: 15572
Epoch: [26]  [ 440/1404]  eta: 0:09:42  lr: 0.000031  min_lr: 0.000000  loss: 3.7495 (3.5775)  loss_scale: 32768.0000 (33808.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1487  max mem: 15572
Epoch: [26]  [ 450/1404]  eta: 0:09:35  lr: 0.000031  min_lr: 0.000000  loss: 3.6944 (3.5792)  loss_scale: 32768.0000 (33785.1885)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0790  max mem: 15572
Epoch: [26]  [ 460/1404]  eta: 0:09:28  lr: 0.000031  min_lr: 0.000000  loss: 3.7325 (3.5879)  loss_scale: 32768.0000 (33763.1236)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0308  max mem: 15572
Epoch: [26]  [ 470/1404]  eta: 0:09:22  lr: 0.000031  min_lr: 0.000000  loss: 3.7091 (3.5854)  loss_scale: 32768.0000 (33741.9958)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0555  max mem: 15572
Epoch: [26]  [ 480/1404]  eta: 0:09:15  lr: 0.000031  min_lr: 0.000000  loss: 3.7277 (3.5885)  loss_scale: 32768.0000 (33721.7464)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0720  max mem: 15572
Epoch: [26]  [ 490/1404]  eta: 0:09:09  lr: 0.000031  min_lr: 0.000000  loss: 3.7547 (3.5898)  loss_scale: 32768.0000 (33702.3218)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0505  max mem: 15572
[2025-01-10 22:16:01,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=248, lr=[3.008333360706352e-07, 3.008333360706352e-07, 4.2976190867233605e-07, 4.2976190867233605e-07, 6.139455838176229e-07, 6.139455838176229e-07, 8.770651197394614e-07, 8.770651197394614e-07, 1.2529501710563735e-06, 1.2529501710563735e-06, 1.7899288157948193e-06, 1.7899288157948193e-06, 2.5570411654211706e-06, 2.5570411654211706e-06, 3.6529159506016727e-06, 3.6529159506016727e-06, 5.2184513580023895e-06, 5.2184513580023895e-06, 7.454930511431987e-06, 7.454930511431987e-06, 1.0649900730617123e-05, 1.0649900730617123e-05, 1.5214143900881605e-05, 1.5214143900881605e-05, 2.1734491286973724e-05, 2.1734491286973724e-05, 3.104927326710532e-05, 3.104927326710532e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:16:01,517] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=45.4544307160024, CurrSamplesPerSec=45.962498824264266, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [ 500/1404]  eta: 0:09:03  lr: 0.000031  min_lr: 0.000000  loss: 3.7012 (3.5917)  loss_scale: 32768.0000 (33683.6727)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0346  max mem: 15572
[2025-01-10 22:16:10,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:16:10,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:16:10,260] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:16:10,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 510/1404]  eta: 0:08:58  lr: 0.000031  min_lr: 0.000000  loss: 3.4267 (3.5882)  loss_scale: 32768.0000 (33794.0039)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0164  max mem: 15572
[2025-01-10 22:16:14,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37020
[2025-01-10 22:16:14,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:16:14,890] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37020
[2025-01-10 22:16:14,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:16:14,890] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 520/1404]  eta: 0:08:52  lr: 0.000031  min_lr: 0.000000  loss: 3.4503 (3.5886)  loss_scale: 32768.0000 (34088.7831)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0007  max mem: 15572
Epoch: [26]  [ 530/1404]  eta: 0:08:46  lr: 0.000031  min_lr: 0.000000  loss: 3.5955 (3.5902)  loss_scale: 32768.0000 (34063.9096)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0203  max mem: 15572
Epoch: [26]  [ 540/1404]  eta: 0:08:40  lr: 0.000031  min_lr: 0.000000  loss: 3.5487 (3.5896)  loss_scale: 32768.0000 (34039.9556)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0201  max mem: 15572
Epoch: [26]  [ 550/1404]  eta: 0:08:33  lr: 0.000031  min_lr: 0.000000  loss: 3.5487 (3.5897)  loss_scale: 32768.0000 (34016.8711)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0004  max mem: 15572
Epoch: [26]  [ 560/1404]  eta: 0:08:26  lr: 0.000031  min_lr: 0.000000  loss: 3.6577 (3.5923)  loss_scale: 32768.0000 (33994.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0108  max mem: 15572
Epoch: [26]  [ 570/1404]  eta: 0:08:19  lr: 0.000031  min_lr: 0.000000  loss: 3.7363 (3.5943)  loss_scale: 32768.0000 (33973.1278)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0109  max mem: 15572
Epoch: [26]  [ 580/1404]  eta: 0:08:14  lr: 0.000031  min_lr: 0.000000  loss: 3.7363 (3.5949)  loss_scale: 32768.0000 (33952.3855)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.0007  max mem: 15572
Epoch: [26]  [ 590/1404]  eta: 0:08:09  lr: 0.000031  min_lr: 0.000000  loss: 3.3398 (3.5884)  loss_scale: 32768.0000 (33932.3452)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.0208  max mem: 15572
Epoch: [26]  [ 600/1404]  eta: 0:08:03  lr: 0.000031  min_lr: 0.000000  loss: 3.3347 (3.5874)  loss_scale: 32768.0000 (33912.9717)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0506  max mem: 15572
Epoch: [26]  [ 610/1404]  eta: 0:07:58  lr: 0.000031  min_lr: 0.000000  loss: 3.6339 (3.5877)  loss_scale: 32768.0000 (33894.2324)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0997  max mem: 15572
Epoch: [26]  [ 620/1404]  eta: 0:07:51  lr: 0.000031  min_lr: 0.000000  loss: 3.7687 (3.5895)  loss_scale: 32768.0000 (33876.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0800  max mem: 15572
Epoch: [26]  [ 630/1404]  eta: 0:07:45  lr: 0.000031  min_lr: 0.000000  loss: 3.8538 (3.5946)  loss_scale: 32768.0000 (33858.5357)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0636  max mem: 15572
Epoch: [26]  [ 640/1404]  eta: 0:07:38  lr: 0.000031  min_lr: 0.000000  loss: 3.7026 (3.5954)  loss_scale: 32768.0000 (33841.5226)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0535  max mem: 15572
[2025-01-10 22:17:31,158] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:17:31,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:17:31,161] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:17:31,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:17:33,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37154
[2025-01-10 22:17:33,711] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:17:33,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37154
[2025-01-10 22:17:33,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:17:33,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 650/1404]  eta: 0:07:32  lr: 0.000031  min_lr: 0.000000  loss: 3.6030 (3.5962)  loss_scale: 32768.0000 (34076.7066)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0008  max mem: 15572
Epoch: [26]  [ 660/1404]  eta: 0:07:27  lr: 0.000031  min_lr: 0.000000  loss: 3.6030 (3.5960)  loss_scale: 32768.0000 (34056.9077)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.0009  max mem: 15572
Epoch: [26]  [ 670/1404]  eta: 0:07:20  lr: 0.000031  min_lr: 0.000000  loss: 3.7689 (3.6010)  loss_scale: 32768.0000 (34037.6990)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0007  max mem: 15572
Epoch: [26]  [ 680/1404]  eta: 0:07:14  lr: 0.000031  min_lr: 0.000000  loss: 3.7761 (3.6022)  loss_scale: 32768.0000 (34019.0543)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0127  max mem: 15572
Epoch: [26]  [ 690/1404]  eta: 0:07:08  lr: 0.000031  min_lr: 0.000000  loss: 3.5449 (3.5998)  loss_scale: 32768.0000 (34000.9493)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0128  max mem: 15572
Epoch: [26]  [ 700/1404]  eta: 0:07:01  lr: 0.000030  min_lr: 0.000000  loss: 3.4870 (3.5985)  loss_scale: 32768.0000 (33983.3609)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0295  max mem: 15572
Epoch: [26]  [ 710/1404]  eta: 0:06:55  lr: 0.000030  min_lr: 0.000000  loss: 3.4426 (3.5958)  loss_scale: 32768.0000 (33966.2672)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0636  max mem: 15572
Epoch: [26]  [ 720/1404]  eta: 0:06:49  lr: 0.000030  min_lr: 0.000000  loss: 3.6946 (3.5986)  loss_scale: 32768.0000 (33949.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0554  max mem: 15572
Epoch: [26]  [ 730/1404]  eta: 0:06:43  lr: 0.000030  min_lr: 0.000000  loss: 3.6946 (3.5969)  loss_scale: 32768.0000 (33933.4829)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0213  max mem: 15572
Epoch: [26]  [ 740/1404]  eta: 0:06:37  lr: 0.000030  min_lr: 0.000000  loss: 3.5864 (3.5976)  loss_scale: 32768.0000 (33917.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0009  max mem: 15572
Epoch: [26]  [ 750/1404]  eta: 0:06:31  lr: 0.000030  min_lr: 0.000000  loss: 3.6322 (3.5976)  loss_scale: 32768.0000 (33902.4447)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0008  max mem: 15572
Epoch: [26]  [ 760/1404]  eta: 0:06:25  lr: 0.000030  min_lr: 0.000000  loss: 3.3970 (3.5976)  loss_scale: 32768.0000 (33887.5375)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0009  max mem: 15572
Epoch: [26]  [ 770/1404]  eta: 0:06:18  lr: 0.000030  min_lr: 0.000000  loss: 3.3970 (3.5967)  loss_scale: 32768.0000 (33873.0169)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0009  max mem: 15572
[2025-01-10 22:18:49,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:18:49,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:18:49,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:18:49,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 780/1404]  eta: 0:06:12  lr: 0.000030  min_lr: 0.000000  loss: 3.6077 (3.5995)  loss_scale: 32768.0000 (33942.7810)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0296  max mem: 15572
[2025-01-10 22:18:51,710] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37288
[2025-01-10 22:18:51,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:18:51,710] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37288
[2025-01-10 22:18:51,711] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:18:51,711] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 790/1404]  eta: 0:06:07  lr: 0.000030  min_lr: 0.000000  loss: 3.8275 (3.6009)  loss_scale: 32768.0000 (34052.2073)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0880  max mem: 15572
Epoch: [26]  [ 800/1404]  eta: 0:06:01  lr: 0.000030  min_lr: 0.000000  loss: 3.6142 (3.6000)  loss_scale: 32768.0000 (34036.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1371  max mem: 15572
Epoch: [26]  [ 810/1404]  eta: 0:05:55  lr: 0.000030  min_lr: 0.000000  loss: 3.7045 (3.6025)  loss_scale: 32768.0000 (34020.5376)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1233  max mem: 15572
Epoch: [26]  [ 820/1404]  eta: 0:05:49  lr: 0.000030  min_lr: 0.000000  loss: 3.7743 (3.6042)  loss_scale: 32768.0000 (34005.2814)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1053  max mem: 15572
Epoch: [26]  [ 830/1404]  eta: 0:05:43  lr: 0.000030  min_lr: 0.000000  loss: 3.6279 (3.6053)  loss_scale: 32768.0000 (33990.3923)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1146  max mem: 15572
Epoch: [26]  [ 840/1404]  eta: 0:05:36  lr: 0.000030  min_lr: 0.000000  loss: 3.5321 (3.6051)  loss_scale: 32768.0000 (33975.8573)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0572  max mem: 15572
Epoch: [26]  [ 850/1404]  eta: 0:05:30  lr: 0.000030  min_lr: 0.000000  loss: 3.6068 (3.6046)  loss_scale: 32768.0000 (33961.6639)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0393  max mem: 15572
Epoch: [26]  [ 860/1404]  eta: 0:05:25  lr: 0.000030  min_lr: 0.000000  loss: 3.6689 (3.6060)  loss_scale: 32768.0000 (33947.8002)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1267  max mem: 15572
Epoch: [26]  [ 870/1404]  eta: 0:05:19  lr: 0.000030  min_lr: 0.000000  loss: 3.7119 (3.6062)  loss_scale: 32768.0000 (33934.2549)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1380  max mem: 15572
Epoch: [26]  [ 880/1404]  eta: 0:05:12  lr: 0.000030  min_lr: 0.000000  loss: 3.6231 (3.6049)  loss_scale: 32768.0000 (33921.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0572  max mem: 15572
Epoch: [26]  [ 890/1404]  eta: 0:05:07  lr: 0.000030  min_lr: 0.000000  loss: 3.6231 (3.6056)  loss_scale: 32768.0000 (33908.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0516  max mem: 15572
Epoch: [26]  [ 900/1404]  eta: 0:05:00  lr: 0.000030  min_lr: 0.000000  loss: 3.5533 (3.6046)  loss_scale: 32768.0000 (33895.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0425  max mem: 15572
Epoch: [26]  [ 910/1404]  eta: 0:04:54  lr: 0.000030  min_lr: 0.000000  loss: 3.3997 (3.6045)  loss_scale: 32768.0000 (33883.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0009  max mem: 15572
[2025-01-10 22:20:08,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:20:08,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:20:08,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:20:08,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 920/1404]  eta: 0:04:48  lr: 0.000030  min_lr: 0.000000  loss: 3.5651 (3.6036)  loss_scale: 32768.0000 (34155.5700)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0008  max mem: 15572
Epoch: [26]  [ 930/1404]  eta: 0:04:42  lr: 0.000030  min_lr: 0.000000  loss: 3.5651 (3.6034)  loss_scale: 65536.0000 (34492.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0885  max mem: 15572
[2025-01-10 22:20:19,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37436
[2025-01-10 22:20:19,880] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:20:19,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:20:19,935] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37436
[2025-01-10 22:20:19,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [26]  [ 940/1404]  eta: 0:04:36  lr: 0.000030  min_lr: 0.000000  loss: 3.5516 (3.6049)  loss_scale: 65536.0000 (34509.1265)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1174  max mem: 15572
Epoch: [26]  [ 950/1404]  eta: 0:04:30  lr: 0.000030  min_lr: 0.000000  loss: 3.4764 (3.6040)  loss_scale: 32768.0000 (34490.8181)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0535  max mem: 15572
Epoch: [26]  [ 960/1404]  eta: 0:04:24  lr: 0.000030  min_lr: 0.000000  loss: 3.6904 (3.6051)  loss_scale: 32768.0000 (34472.8907)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0704  max mem: 15572
Epoch: [26]  [ 970/1404]  eta: 0:04:19  lr: 0.000030  min_lr: 0.000000  loss: 3.7017 (3.6055)  loss_scale: 32768.0000 (34455.3326)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1306  max mem: 15572
Epoch: [26]  [ 980/1404]  eta: 0:04:13  lr: 0.000030  min_lr: 0.000000  loss: 3.6097 (3.6050)  loss_scale: 32768.0000 (34438.1325)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0940  max mem: 15572
Epoch: [26]  [ 990/1404]  eta: 0:04:07  lr: 0.000030  min_lr: 0.000000  loss: 3.2160 (3.6009)  loss_scale: 32768.0000 (34421.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0101  max mem: 15572
Epoch: [26]  [1000/1404]  eta: 0:04:01  lr: 0.000030  min_lr: 0.000000  loss: 3.2160 (3.6017)  loss_scale: 32768.0000 (34404.7632)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0009  max mem: 15572
Epoch: [26]  [1010/1404]  eta: 0:03:54  lr: 0.000030  min_lr: 0.000000  loss: 3.6092 (3.6012)  loss_scale: 32768.0000 (34388.5737)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0007  max mem: 15572
Epoch: [26]  [1020/1404]  eta: 0:03:49  lr: 0.000030  min_lr: 0.000000  loss: 3.6331 (3.6017)  loss_scale: 32768.0000 (34372.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0008  max mem: 15572
Epoch: [26]  [1030/1404]  eta: 0:03:43  lr: 0.000030  min_lr: 0.000000  loss: 3.7775 (3.6034)  loss_scale: 32768.0000 (34357.1368)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0011  max mem: 15572
Epoch: [26]  [1040/1404]  eta: 0:03:37  lr: 0.000030  min_lr: 0.000000  loss: 3.7642 (3.6035)  loss_scale: 32768.0000 (34341.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0013  max mem: 15572
Epoch: [26]  [1050/1404]  eta: 0:03:31  lr: 0.000029  min_lr: 0.000000  loss: 3.5919 (3.6026)  loss_scale: 32768.0000 (34326.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0012  max mem: 15572
Epoch: [26]  [1060/1404]  eta: 0:03:25  lr: 0.000029  min_lr: 0.000000  loss: 3.5579 (3.6011)  loss_scale: 32768.0000 (34312.2036)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0008  max mem: 15572
[2025-01-10 22:21:37,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:21:37,306] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:21:37,306] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:21:37,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:21:37,823] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37566
[2025-01-10 22:21:37,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:21:37,823] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:21:37,866] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37566
[2025-01-10 22:21:37,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [26]  [1070/1404]  eta: 0:03:19  lr: 0.000029  min_lr: 0.000000  loss: 3.5579 (3.6021)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0009  max mem: 15572
Epoch: [26]  [1080/1404]  eta: 0:03:13  lr: 0.000029  min_lr: 0.000000  loss: 3.8150 (3.6030)  loss_scale: 32768.0000 (34313.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0006  max mem: 15572
Epoch: [26]  [1090/1404]  eta: 0:03:07  lr: 0.000029  min_lr: 0.000000  loss: 3.8150 (3.6028)  loss_scale: 32768.0000 (34299.7764)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0007  max mem: 15572
Epoch: [26]  [1100/1404]  eta: 0:03:01  lr: 0.000029  min_lr: 0.000000  loss: 3.7393 (3.6036)  loss_scale: 32768.0000 (34285.8638)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0008  max mem: 15572
Epoch: [26]  [1110/1404]  eta: 0:02:55  lr: 0.000029  min_lr: 0.000000  loss: 3.6677 (3.6028)  loss_scale: 32768.0000 (34272.2016)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0007  max mem: 15572
Epoch: [26]  [1120/1404]  eta: 0:02:49  lr: 0.000029  min_lr: 0.000000  loss: 3.6762 (3.6029)  loss_scale: 32768.0000 (34258.7832)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0007  max mem: 15572
Epoch: [26]  [1130/1404]  eta: 0:02:43  lr: 0.000029  min_lr: 0.000000  loss: 3.8391 (3.6045)  loss_scale: 32768.0000 (34245.6021)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0007  max mem: 15572
Epoch: [26]  [1140/1404]  eta: 0:02:37  lr: 0.000029  min_lr: 0.000000  loss: 3.9164 (3.6069)  loss_scale: 32768.0000 (34232.6521)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0010  max mem: 15572
Epoch: [26]  [1150/1404]  eta: 0:02:31  lr: 0.000029  min_lr: 0.000000  loss: 3.8455 (3.6087)  loss_scale: 32768.0000 (34219.9270)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0009  max mem: 15572
Epoch: [26]  [1160/1404]  eta: 0:02:25  lr: 0.000029  min_lr: 0.000000  loss: 3.8190 (3.6081)  loss_scale: 32768.0000 (34207.4212)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0006  max mem: 15572
Epoch: [26]  [1170/1404]  eta: 0:02:19  lr: 0.000029  min_lr: 0.000000  loss: 3.6090 (3.6064)  loss_scale: 32768.0000 (34195.1289)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0007  max mem: 15572
Epoch: [26]  [1180/1404]  eta: 0:02:13  lr: 0.000029  min_lr: 0.000000  loss: 3.3787 (3.6051)  loss_scale: 32768.0000 (34183.0449)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0009  max mem: 15572
Epoch: [26]  [1190/1404]  eta: 0:02:07  lr: 0.000029  min_lr: 0.000000  loss: 3.4318 (3.6057)  loss_scale: 32768.0000 (34171.1637)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0009  max mem: 15572
[2025-01-10 22:22:53,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:22:53,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:22:53,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:22:53,542] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1200/1404]  eta: 0:02:01  lr: 0.000029  min_lr: 0.000000  loss: 3.7544 (3.6078)  loss_scale: 32768.0000 (34432.3197)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0009  max mem: 15572
Epoch: [26]  [1210/1404]  eta: 0:01:55  lr: 0.000029  min_lr: 0.000000  loss: 3.6669 (3.6066)  loss_scale: 65536.0000 (34689.1627)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0011  max mem: 15572
[2025-01-10 22:23:05,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37715
[2025-01-10 22:23:05,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:23:05,407] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:23:05,550] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37715
[2025-01-10 22:23:05,550] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [26]  [1220/1404]  eta: 0:01:49  lr: 0.000029  min_lr: 0.000000  loss: 3.5309 (3.6064)  loss_scale: 32768.0000 (34673.4283)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0009  max mem: 15572
Epoch: [26]  [1230/1404]  eta: 0:01:43  lr: 0.000029  min_lr: 0.000000  loss: 3.6730 (3.6086)  loss_scale: 32768.0000 (34657.9496)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0205  max mem: 15572
Epoch: [26]  [1240/1404]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000000  loss: 3.6630 (3.6061)  loss_scale: 32768.0000 (34642.7204)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0204  max mem: 15572
Epoch: [26]  [1250/1404]  eta: 0:01:31  lr: 0.000029  min_lr: 0.000000  loss: 3.1591 (3.6030)  loss_scale: 32768.0000 (34627.7346)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0010  max mem: 15572
Epoch: [26]  [1260/1404]  eta: 0:01:25  lr: 0.000029  min_lr: 0.000000  loss: 3.3896 (3.6042)  loss_scale: 32768.0000 (34612.9865)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0008  max mem: 15572
Epoch: [26]  [1270/1404]  eta: 0:01:19  lr: 0.000029  min_lr: 0.000000  loss: 3.7819 (3.6047)  loss_scale: 32768.0000 (34598.4705)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0006  max mem: 15572
Epoch: [26]  [1280/1404]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000000  loss: 3.3580 (3.6035)  loss_scale: 32768.0000 (34584.1811)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0011  max mem: 15572
Epoch: [26]  [1290/1404]  eta: 0:01:07  lr: 0.000029  min_lr: 0.000000  loss: 3.5321 (3.6040)  loss_scale: 32768.0000 (34570.1131)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0011  max mem: 15572
Epoch: [26]  [1300/1404]  eta: 0:01:01  lr: 0.000029  min_lr: 0.000000  loss: 3.5321 (3.6043)  loss_scale: 32768.0000 (34556.2613)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0010  max mem: 15572
Epoch: [26]  [1310/1404]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000000  loss: 3.5701 (3.6037)  loss_scale: 32768.0000 (34542.6209)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0010  max mem: 15572
Epoch: [26]  [1320/1404]  eta: 0:00:50  lr: 0.000029  min_lr: 0.000000  loss: 3.7422 (3.6045)  loss_scale: 32768.0000 (34529.1870)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0009  max mem: 15572
Epoch: [26]  [1330/1404]  eta: 0:00:44  lr: 0.000029  min_lr: 0.000000  loss: 3.4203 (3.6021)  loss_scale: 32768.0000 (34515.9549)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0009  max mem: 15572
[2025-01-10 22:24:21,615] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:24:21,616] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1340/1404]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000000  loss: 3.4804 (3.6028)  loss_scale: 32768.0000 (34527.3557)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0333  max mem: 15572
[2025-01-10 22:24:21,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:24:21,631] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:24:22,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37846
[2025-01-10 22:24:22,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:24:22,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37846
[2025-01-10 22:24:22,625] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:24:22,626] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1350/1404]  eta: 0:00:32  lr: 0.000029  min_lr: 0.000000  loss: 3.7398 (3.6037)  loss_scale: 32768.0000 (34538.5877)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0332  max mem: 15572
Epoch: [26]  [1360/1404]  eta: 0:00:26  lr: 0.000029  min_lr: 0.000000  loss: 3.6166 (3.6030)  loss_scale: 32768.0000 (34525.5783)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0010  max mem: 15572
Epoch: [26]  [1370/1404]  eta: 0:00:20  lr: 0.000029  min_lr: 0.000000  loss: 3.4736 (3.6021)  loss_scale: 32768.0000 (34512.7586)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0010  max mem: 15572
Epoch: [26]  [1380/1404]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000000  loss: 3.6997 (3.6038)  loss_scale: 32768.0000 (34500.1245)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0007  max mem: 15572
Epoch: [26]  [1390/1404]  eta: 0:00:08  lr: 0.000029  min_lr: 0.000000  loss: 3.9041 (3.6048)  loss_scale: 32768.0000 (34487.6722)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0108  max mem: 15572
Epoch: [26]  [1400/1404]  eta: 0:00:02  lr: 0.000029  min_lr: 0.000000  loss: 3.6976 (3.6030)  loss_scale: 32768.0000 (34475.3976)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0106  max mem: 15572
Epoch: [26]  [1403/1404]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 3.6561 (3.6028)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)  time: 0.4171  data: 0.0106  max mem: 15572
Epoch: [26] Total time: 0:13:53 (0.5936 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 3.6561 (3.5992)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:15:14  loss: 1.4400 (1.4400)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 6.7224  data: 6.4867  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 2.0357 (2.0850)  acc1: 55.5556 (50.5051)  acc5: 77.7778 (79.2929)  time: 0.7938  data: 0.5903  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.2535 (2.2360)  acc1: 44.4444 (45.5026)  acc5: 77.7778 (78.0423)  time: 0.2691  data: 0.0632  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.2197 (2.0873)  acc1: 50.0000 (50.0000)  acc5: 83.3333 (79.5699)  time: 0.3350  data: 0.1360  max mem: 15572
Val:  [ 40/136]  eta: 0:00:45  loss: 1.7375 (2.0516)  acc1: 61.1111 (51.8970)  acc5: 83.3333 (79.5393)  time: 0.3649  data: 0.1663  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8460 (2.0323)  acc1: 55.5556 (51.6340)  acc5: 83.3333 (80.7190)  time: 0.3598  data: 0.1448  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.2031 (2.1197)  acc1: 44.4444 (48.5428)  acc5: 83.3333 (79.5993)  time: 0.3619  data: 0.1441  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.1252 (2.0964)  acc1: 44.4444 (48.9828)  acc5: 77.7778 (79.6557)  time: 0.4027  data: 0.1823  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.9607 (2.0963)  acc1: 44.4444 (48.6283)  acc5: 88.8889 (80.1783)  time: 0.3721  data: 0.1425  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0990 (2.1046)  acc1: 44.4444 (48.2906)  acc5: 83.3333 (80.1587)  time: 0.3478  data: 0.1194  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2089 (2.1866)  acc1: 38.8889 (46.4796)  acc5: 77.7778 (77.9978)  time: 0.3725  data: 0.1684  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.3153 (2.1823)  acc1: 38.8889 (46.4464)  acc5: 72.2222 (77.9780)  time: 0.3735  data: 0.1796  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.9583 (2.1344)  acc1: 50.0000 (47.5666)  acc5: 83.3333 (78.6961)  time: 0.3342  data: 0.1444  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6757 (2.0941)  acc1: 55.5556 (48.5157)  acc5: 88.8889 (79.2621)  time: 0.2270  data: 0.0666  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8226 (2.0953)  acc1: 50.0000 (48.7305)  acc5: 83.3333 (79.3612)  time: 0.1827  data: 0.0393  max mem: 15572
Val: Total time: 0:00:50 (0.3713 s / it)
* Acc@1 48.423 Acc@5 78.133 loss 2.144
Accuracy of the network on the 4883 val videos: 48.4%
Max accuracy: 48.98%
Epoch: [27]  [   0/1404]  eta: 3:17:42  lr: 0.000029  min_lr: 0.000000  loss: 2.8964 (2.8964)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4488  data: 7.8668  max mem: 15572
Epoch: [27]  [  10/1404]  eta: 0:29:02  lr: 0.000028  min_lr: 0.000000  loss: 3.4323 (3.4683)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2500  data: 0.7159  max mem: 15572
Epoch: [27]  [  20/1404]  eta: 0:22:01  lr: 0.000028  min_lr: 0.000000  loss: 3.5251 (3.4919)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0225  max mem: 15572
Epoch: [27]  [  30/1404]  eta: 0:19:20  lr: 0.000028  min_lr: 0.000000  loss: 3.3532 (3.4674)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0225  max mem: 15572
Epoch: [27]  [  40/1404]  eta: 0:17:23  lr: 0.000028  min_lr: 0.000000  loss: 3.5936 (3.4641)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0011  max mem: 15572
Epoch: [27]  [  50/1404]  eta: 0:17:09  lr: 0.000028  min_lr: 0.000000  loss: 3.5936 (3.4714)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.0461  max mem: 15572
Epoch: [27]  [  60/1404]  eta: 0:16:02  lr: 0.000028  min_lr: 0.000000  loss: 3.4531 (3.4754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0458  max mem: 15572
[2025-01-10 22:26:33,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:26:33,500] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:26:33,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:26:33,500] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:26:34,547] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37977
[2025-01-10 22:26:34,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:26:34,556] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37977
[2025-01-10 22:26:34,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:26:34,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [  70/1404]  eta: 0:15:33  lr: 0.000028  min_lr: 0.000000  loss: 3.4659 (3.4916)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0519  max mem: 15572
Epoch: [27]  [  80/1404]  eta: 0:15:17  lr: 0.000028  min_lr: 0.000000  loss: 3.4659 (3.4726)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1104  max mem: 15572
Epoch: [27]  [  90/1404]  eta: 0:14:44  lr: 0.000028  min_lr: 0.000000  loss: 3.5227 (3.4966)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0593  max mem: 15572
[2025-01-10 22:26:48,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=256, lr=[2.738766601344082e-07, 2.738766601344082e-07, 3.9125237162058317e-07, 3.9125237162058317e-07, 5.58931959457976e-07, 5.58931959457976e-07, 7.984742277971087e-07, 7.984742277971087e-07, 1.1406774682815838e-06, 1.1406774682815838e-06, 1.6295392404022628e-06, 1.6295392404022628e-06, 2.3279132005746613e-06, 2.3279132005746613e-06, 3.3255902865352305e-06, 3.3255902865352305e-06, 4.750843266478901e-06, 4.750843266478901e-06, 6.786918952112716e-06, 6.786918952112716e-06, 9.695598503018165e-06, 9.695598503018165e-06, 1.3850855004311667e-05, 1.3850855004311667e-05, 1.978693572044524e-05, 1.978693572044524e-05, 2.8267051029207487e-05, 2.8267051029207487e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:26:48,133] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=45.44704264262889, CurrSamplesPerSec=55.527953928642354, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 100/1404]  eta: 0:14:27  lr: 0.000028  min_lr: 0.000000  loss: 3.3911 (3.4719)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0408  max mem: 15572
Epoch: [27]  [ 110/1404]  eta: 0:14:02  lr: 0.000028  min_lr: 0.000000  loss: 3.3800 (3.4748)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0408  max mem: 15572
Epoch: [27]  [ 120/1404]  eta: 0:14:02  lr: 0.000028  min_lr: 0.000000  loss: 3.4097 (3.4707)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0408  max mem: 15572
Epoch: [27]  [ 130/1404]  eta: 0:13:44  lr: 0.000028  min_lr: 0.000000  loss: 3.2928 (3.4606)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0406  max mem: 15572
Epoch: [27]  [ 140/1404]  eta: 0:13:40  lr: 0.000028  min_lr: 0.000000  loss: 3.4541 (3.4623)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0006  max mem: 15572
Epoch: [27]  [ 150/1404]  eta: 0:13:23  lr: 0.000028  min_lr: 0.000000  loss: 3.6871 (3.4539)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0007  max mem: 15572
Epoch: [27]  [ 160/1404]  eta: 0:13:10  lr: 0.000028  min_lr: 0.000000  loss: 3.4412 (3.4566)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0006  max mem: 15572
Epoch: [27]  [ 170/1404]  eta: 0:13:03  lr: 0.000028  min_lr: 0.000000  loss: 3.5065 (3.4656)  loss_scale: 32768.0000 (33151.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0007  max mem: 15572
Epoch: [27]  [ 180/1404]  eta: 0:12:57  lr: 0.000028  min_lr: 0.000000  loss: 3.5384 (3.4688)  loss_scale: 32768.0000 (33130.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0008  max mem: 15572
Epoch: [27]  [ 190/1404]  eta: 0:12:49  lr: 0.000028  min_lr: 0.000000  loss: 3.7018 (3.4758)  loss_scale: 32768.0000 (33111.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0007  max mem: 15572
[2025-01-10 22:27:51,572] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:27:51,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:27:51,582] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:27:51,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:27:52,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38108
[2025-01-10 22:27:52,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:27:52,668] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38108
[2025-01-10 22:27:52,669] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:27:52,669] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 200/1404]  eta: 0:12:36  lr: 0.000028  min_lr: 0.000000  loss: 3.7018 (3.4919)  loss_scale: 32768.0000 (33420.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0010  max mem: 15572
Epoch: [27]  [ 210/1404]  eta: 0:12:23  lr: 0.000028  min_lr: 0.000000  loss: 3.7838 (3.5045)  loss_scale: 32768.0000 (33389.1943)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0012  max mem: 15572
Epoch: [27]  [ 220/1404]  eta: 0:12:12  lr: 0.000028  min_lr: 0.000000  loss: 3.8855 (3.5209)  loss_scale: 32768.0000 (33361.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0009  max mem: 15572
Epoch: [27]  [ 230/1404]  eta: 0:12:04  lr: 0.000028  min_lr: 0.000000  loss: 3.8245 (3.5186)  loss_scale: 32768.0000 (33335.4113)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0009  max mem: 15572
Epoch: [27]  [ 240/1404]  eta: 0:11:57  lr: 0.000028  min_lr: 0.000000  loss: 3.6290 (3.5260)  loss_scale: 32768.0000 (33311.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0010  max mem: 15572
Epoch: [27]  [ 250/1404]  eta: 0:11:50  lr: 0.000028  min_lr: 0.000000  loss: 3.6703 (3.5421)  loss_scale: 32768.0000 (33290.1992)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0007  max mem: 15572
Epoch: [27]  [ 260/1404]  eta: 0:11:42  lr: 0.000028  min_lr: 0.000000  loss: 3.6342 (3.5475)  loss_scale: 32768.0000 (33270.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0006  max mem: 15572
Epoch: [27]  [ 270/1404]  eta: 0:11:38  lr: 0.000028  min_lr: 0.000000  loss: 3.4655 (3.5407)  loss_scale: 32768.0000 (33251.6605)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0010  max mem: 15572
Epoch: [27]  [ 280/1404]  eta: 0:11:31  lr: 0.000028  min_lr: 0.000000  loss: 3.4402 (3.5417)  loss_scale: 32768.0000 (33234.4484)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.0010  max mem: 15572
Epoch: [27]  [ 290/1404]  eta: 0:11:24  lr: 0.000028  min_lr: 0.000000  loss: 3.5095 (3.5437)  loss_scale: 32768.0000 (33218.4192)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0007  max mem: 15572
Epoch: [27]  [ 300/1404]  eta: 0:11:15  lr: 0.000028  min_lr: 0.000000  loss: 3.6848 (3.5485)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0007  max mem: 15572
Epoch: [27]  [ 310/1404]  eta: 0:11:08  lr: 0.000028  min_lr: 0.000000  loss: 3.7196 (3.5494)  loss_scale: 32768.0000 (33189.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0007  max mem: 15572
Epoch: [27]  [ 320/1404]  eta: 0:11:03  lr: 0.000028  min_lr: 0.000000  loss: 3.4122 (3.5411)  loss_scale: 32768.0000 (33176.3240)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0007  max mem: 15572
[2025-01-10 22:29:07,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:29:07,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:29:07,363] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:29:07,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 330/1404]  eta: 0:10:55  lr: 0.000028  min_lr: 0.000000  loss: 3.4192 (3.5479)  loss_scale: 32768.0000 (33361.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0007  max mem: 15572
[2025-01-10 22:29:09,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38240
[2025-01-10 22:29:09,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:29:09,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:29:09,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38240
[2025-01-10 22:29:09,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [ 340/1404]  eta: 0:10:47  lr: 0.000028  min_lr: 0.000000  loss: 3.5353 (3.5457)  loss_scale: 32768.0000 (33440.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0006  max mem: 15572
Epoch: [27]  [ 350/1404]  eta: 0:10:42  lr: 0.000028  min_lr: 0.000000  loss: 3.5770 (3.5546)  loss_scale: 32768.0000 (33421.4929)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0433  max mem: 15572
Epoch: [27]  [ 360/1404]  eta: 0:10:34  lr: 0.000028  min_lr: 0.000000  loss: 3.8070 (3.5566)  loss_scale: 32768.0000 (33403.3906)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0433  max mem: 15572
Epoch: [27]  [ 370/1404]  eta: 0:10:30  lr: 0.000028  min_lr: 0.000000  loss: 3.9510 (3.5671)  loss_scale: 32768.0000 (33386.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0007  max mem: 15572
Epoch: [27]  [ 380/1404]  eta: 0:10:23  lr: 0.000027  min_lr: 0.000000  loss: 3.8667 (3.5622)  loss_scale: 32768.0000 (33370.0367)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0007  max mem: 15572
Epoch: [27]  [ 390/1404]  eta: 0:10:17  lr: 0.000027  min_lr: 0.000000  loss: 3.4870 (3.5621)  loss_scale: 32768.0000 (33354.6394)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0006  max mem: 15572
Epoch: [27]  [ 400/1404]  eta: 0:10:10  lr: 0.000027  min_lr: 0.000000  loss: 3.3648 (3.5555)  loss_scale: 32768.0000 (33340.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0007  max mem: 15572
Epoch: [27]  [ 410/1404]  eta: 0:10:06  lr: 0.000027  min_lr: 0.000000  loss: 3.2508 (3.5518)  loss_scale: 32768.0000 (33326.0925)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0005  max mem: 15572
Epoch: [27]  [ 420/1404]  eta: 0:09:57  lr: 0.000027  min_lr: 0.000000  loss: 3.3271 (3.5498)  loss_scale: 32768.0000 (33312.8361)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0006  max mem: 15572
Epoch: [27]  [ 430/1404]  eta: 0:09:50  lr: 0.000027  min_lr: 0.000000  loss: 3.5512 (3.5520)  loss_scale: 32768.0000 (33300.1949)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0005  max mem: 15572
Epoch: [27]  [ 440/1404]  eta: 0:09:44  lr: 0.000027  min_lr: 0.000000  loss: 3.6775 (3.5529)  loss_scale: 32768.0000 (33288.1270)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0006  max mem: 15572
Epoch: [27]  [ 450/1404]  eta: 0:09:39  lr: 0.000027  min_lr: 0.000000  loss: 3.4128 (3.5519)  loss_scale: 32768.0000 (33276.5942)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0007  max mem: 15572
Epoch: [27]  [ 460/1404]  eta: 0:09:34  lr: 0.000027  min_lr: 0.000000  loss: 3.4128 (3.5508)  loss_scale: 32768.0000 (33265.5618)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0006  max mem: 15572
[2025-01-10 22:30:27,750] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:30:27,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:30:27,753] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:30:27,754] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:30:29,238] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38372
[2025-01-10 22:30:29,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:30:29,238] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38372
[2025-01-10 22:30:29,238] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:30:29,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 470/1404]  eta: 0:09:26  lr: 0.000027  min_lr: 0.000000  loss: 3.6379 (3.5527)  loss_scale: 32768.0000 (33463.7113)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0006  max mem: 15572
Epoch: [27]  [ 480/1404]  eta: 0:09:19  lr: 0.000027  min_lr: 0.000000  loss: 3.8083 (3.5582)  loss_scale: 32768.0000 (33449.2474)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0006  max mem: 15572
Epoch: [27]  [ 490/1404]  eta: 0:09:11  lr: 0.000027  min_lr: 0.000000  loss: 3.7830 (3.5598)  loss_scale: 32768.0000 (33435.3727)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0006  max mem: 15572
Epoch: [27]  [ 500/1404]  eta: 0:09:03  lr: 0.000027  min_lr: 0.000000  loss: 3.3360 (3.5557)  loss_scale: 32768.0000 (33422.0519)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0007  max mem: 15572
Epoch: [27]  [ 510/1404]  eta: 0:08:58  lr: 0.000027  min_lr: 0.000000  loss: 3.3006 (3.5498)  loss_scale: 32768.0000 (33409.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0007  max mem: 15572
Epoch: [27]  [ 520/1404]  eta: 0:08:51  lr: 0.000027  min_lr: 0.000000  loss: 3.6405 (3.5568)  loss_scale: 32768.0000 (33396.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0007  max mem: 15572
Epoch: [27]  [ 530/1404]  eta: 0:08:44  lr: 0.000027  min_lr: 0.000000  loss: 3.7776 (3.5570)  loss_scale: 32768.0000 (33385.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0009  max mem: 15572
Epoch: [27]  [ 540/1404]  eta: 0:08:39  lr: 0.000027  min_lr: 0.000000  loss: 3.4657 (3.5542)  loss_scale: 32768.0000 (33373.6932)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0008  max mem: 15572
Epoch: [27]  [ 550/1404]  eta: 0:08:33  lr: 0.000027  min_lr: 0.000000  loss: 3.4260 (3.5524)  loss_scale: 32768.0000 (33362.7005)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0007  max mem: 15572
Epoch: [27]  [ 560/1404]  eta: 0:08:26  lr: 0.000027  min_lr: 0.000000  loss: 3.5320 (3.5502)  loss_scale: 32768.0000 (33352.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0008  max mem: 15572
Epoch: [27]  [ 570/1404]  eta: 0:08:21  lr: 0.000027  min_lr: 0.000000  loss: 3.4556 (3.5501)  loss_scale: 32768.0000 (33341.8704)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0006  max mem: 15572
Epoch: [27]  [ 580/1404]  eta: 0:08:15  lr: 0.000027  min_lr: 0.000000  loss: 3.5735 (3.5524)  loss_scale: 32768.0000 (33331.9931)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0007  max mem: 15572
Epoch: [27]  [ 590/1404]  eta: 0:08:09  lr: 0.000027  min_lr: 0.000000  loss: 3.8046 (3.5553)  loss_scale: 32768.0000 (33322.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0008  max mem: 15572
[2025-01-10 22:31:43,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:31:43,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:31:43,445] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:31:43,445] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:31:45,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38504
[2025-01-10 22:31:45,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:31:45,132] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38504
[2025-01-10 22:31:45,133] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:31:45,133] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [ 600/1404]  eta: 0:08:02  lr: 0.000027  min_lr: 0.000000  loss: 3.8365 (3.5597)  loss_scale: 32768.0000 (33476.7920)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0008  max mem: 15572
Epoch: [27]  [ 610/1404]  eta: 0:07:55  lr: 0.000027  min_lr: 0.000000  loss: 3.8365 (3.5617)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0010  max mem: 15572
Epoch: [27]  [ 620/1404]  eta: 0:07:49  lr: 0.000027  min_lr: 0.000000  loss: 3.8260 (3.5621)  loss_scale: 32768.0000 (33453.9646)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0593  max mem: 15572
Epoch: [27]  [ 630/1404]  eta: 0:07:42  lr: 0.000027  min_lr: 0.000000  loss: 3.5306 (3.5587)  loss_scale: 32768.0000 (33443.0935)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0592  max mem: 15572
Epoch: [27]  [ 640/1404]  eta: 0:07:38  lr: 0.000027  min_lr: 0.000000  loss: 3.1904 (3.5536)  loss_scale: 32768.0000 (33432.5616)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0134  max mem: 15572
Epoch: [27]  [ 650/1404]  eta: 0:07:31  lr: 0.000027  min_lr: 0.000000  loss: 3.2506 (3.5513)  loss_scale: 32768.0000 (33422.3533)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.0135  max mem: 15572
Epoch: [27]  [ 660/1404]  eta: 0:07:28  lr: 0.000027  min_lr: 0.000000  loss: 3.4873 (3.5508)  loss_scale: 32768.0000 (33412.4539)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0783  max mem: 15572
Epoch: [27]  [ 670/1404]  eta: 0:07:20  lr: 0.000027  min_lr: 0.000000  loss: 3.4371 (3.5478)  loss_scale: 32768.0000 (33402.8495)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0781  max mem: 15572
Epoch: [27]  [ 680/1404]  eta: 0:07:14  lr: 0.000027  min_lr: 0.000000  loss: 3.3312 (3.5482)  loss_scale: 32768.0000 (33393.5272)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0038  max mem: 15572
Epoch: [27]  [ 690/1404]  eta: 0:07:07  lr: 0.000027  min_lr: 0.000000  loss: 3.5803 (3.5486)  loss_scale: 32768.0000 (33384.4747)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0037  max mem: 15572
Epoch: [27]  [ 700/1404]  eta: 0:07:00  lr: 0.000027  min_lr: 0.000000  loss: 3.5991 (3.5498)  loss_scale: 32768.0000 (33375.6805)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0007  max mem: 15572
Epoch: [27]  [ 710/1404]  eta: 0:06:53  lr: 0.000027  min_lr: 0.000000  loss: 3.7015 (3.5521)  loss_scale: 32768.0000 (33367.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0114  max mem: 15572
Epoch: [27]  [ 720/1404]  eta: 0:06:47  lr: 0.000027  min_lr: 0.000000  loss: 3.4704 (3.5491)  loss_scale: 32768.0000 (33358.8239)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0504  max mem: 15572
[2025-01-10 22:32:58,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:32:58,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:32:58,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:32:58,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:33:01,164] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38637
[2025-01-10 22:33:01,164] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:33:01,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:33:01,169] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38637
[2025-01-10 22:33:01,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [ 730/1404]  eta: 0:06:41  lr: 0.000027  min_lr: 0.000000  loss: 3.4704 (3.5482)  loss_scale: 32768.0000 (33530.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1026  max mem: 15572
Epoch: [27]  [ 740/1404]  eta: 0:06:36  lr: 0.000027  min_lr: 0.000000  loss: 3.5698 (3.5484)  loss_scale: 32768.0000 (33519.7625)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1435  max mem: 15572
Epoch: [27]  [ 750/1404]  eta: 0:06:29  lr: 0.000026  min_lr: 0.000000  loss: 3.5408 (3.5479)  loss_scale: 32768.0000 (33509.7523)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1109  max mem: 15572
Epoch: [27]  [ 760/1404]  eta: 0:06:23  lr: 0.000026  min_lr: 0.000000  loss: 3.5201 (3.5480)  loss_scale: 32768.0000 (33500.0053)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0411  max mem: 15572
Epoch: [27]  [ 770/1404]  eta: 0:06:18  lr: 0.000026  min_lr: 0.000000  loss: 3.6114 (3.5498)  loss_scale: 32768.0000 (33490.5110)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0766  max mem: 15572
Epoch: [27]  [ 780/1404]  eta: 0:06:11  lr: 0.000026  min_lr: 0.000000  loss: 3.4753 (3.5482)  loss_scale: 32768.0000 (33481.2599)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0665  max mem: 15572
Epoch: [27]  [ 790/1404]  eta: 0:06:06  lr: 0.000026  min_lr: 0.000000  loss: 3.4753 (3.5501)  loss_scale: 32768.0000 (33472.2427)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0679  max mem: 15572
Epoch: [27]  [ 800/1404]  eta: 0:06:00  lr: 0.000026  min_lr: 0.000000  loss: 3.6770 (3.5511)  loss_scale: 32768.0000 (33463.4507)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1484  max mem: 15572
Epoch: [27]  [ 810/1404]  eta: 0:05:54  lr: 0.000026  min_lr: 0.000000  loss: 3.6770 (3.5517)  loss_scale: 32768.0000 (33454.8755)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0812  max mem: 15572
Epoch: [27]  [ 820/1404]  eta: 0:05:48  lr: 0.000026  min_lr: 0.000000  loss: 3.6191 (3.5506)  loss_scale: 32768.0000 (33446.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0236  max mem: 15572
Epoch: [27]  [ 830/1404]  eta: 0:05:42  lr: 0.000026  min_lr: 0.000000  loss: 3.7154 (3.5542)  loss_scale: 32768.0000 (33438.3442)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0510  max mem: 15572
Epoch: [27]  [ 840/1404]  eta: 0:05:36  lr: 0.000026  min_lr: 0.000000  loss: 3.6963 (3.5546)  loss_scale: 32768.0000 (33430.3734)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0850  max mem: 15572
Epoch: [27]  [ 850/1404]  eta: 0:05:30  lr: 0.000026  min_lr: 0.000000  loss: 3.5843 (3.5547)  loss_scale: 32768.0000 (33422.5899)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0706  max mem: 15572
[2025-01-10 22:34:18,453] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:34:18,453] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:34:18,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:34:18,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 860/1404]  eta: 0:05:24  lr: 0.000026  min_lr: 0.000000  loss: 3.3393 (3.5516)  loss_scale: 32768.0000 (33529.1614)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0379  max mem: 15572
Epoch: [27]  [ 870/1404]  eta: 0:05:18  lr: 0.000026  min_lr: 0.000000  loss: 3.3189 (3.5507)  loss_scale: 65536.0000 (33896.6338)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0782  max mem: 15572
Epoch: [27]  [ 880/1404]  eta: 0:05:12  lr: 0.000026  min_lr: 0.000000  loss: 3.5055 (3.5513)  loss_scale: 65536.0000 (34255.7639)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0876  max mem: 15572
[2025-01-10 22:34:35,601] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38795
[2025-01-10 22:34:35,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:34:35,601] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:34:35,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38795
[2025-01-10 22:34:35,605] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [ 890/1404]  eta: 0:05:06  lr: 0.000026  min_lr: 0.000000  loss: 3.6184 (3.5502)  loss_scale: 65536.0000 (34459.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0591  max mem: 15572
Epoch: [27]  [ 900/1404]  eta: 0:05:00  lr: 0.000026  min_lr: 0.000000  loss: 3.6184 (3.5488)  loss_scale: 32768.0000 (34440.9501)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0821  max mem: 15572
Epoch: [27]  [ 910/1404]  eta: 0:04:54  lr: 0.000026  min_lr: 0.000000  loss: 3.3169 (3.5471)  loss_scale: 32768.0000 (34422.5862)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0891  max mem: 15572
Epoch: [27]  [ 920/1404]  eta: 0:04:48  lr: 0.000026  min_lr: 0.000000  loss: 3.1771 (3.5426)  loss_scale: 32768.0000 (34404.6211)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0326  max mem: 15572
Epoch: [27]  [ 930/1404]  eta: 0:04:42  lr: 0.000026  min_lr: 0.000000  loss: 3.1771 (3.5401)  loss_scale: 32768.0000 (34387.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0366  max mem: 15572
Epoch: [27]  [ 940/1404]  eta: 0:04:36  lr: 0.000026  min_lr: 0.000000  loss: 3.3027 (3.5380)  loss_scale: 32768.0000 (34369.8363)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0658  max mem: 15572
Epoch: [27]  [ 950/1404]  eta: 0:04:30  lr: 0.000026  min_lr: 0.000000  loss: 3.5715 (3.5361)  loss_scale: 32768.0000 (34352.9926)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0849  max mem: 15572
Epoch: [27]  [ 960/1404]  eta: 0:04:24  lr: 0.000026  min_lr: 0.000000  loss: 3.6330 (3.5375)  loss_scale: 32768.0000 (34336.4995)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0887  max mem: 15572
Epoch: [27]  [ 970/1404]  eta: 0:04:18  lr: 0.000026  min_lr: 0.000000  loss: 3.7734 (3.5392)  loss_scale: 32768.0000 (34320.3460)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1095  max mem: 15572
Epoch: [27]  [ 980/1404]  eta: 0:04:12  lr: 0.000026  min_lr: 0.000000  loss: 3.6647 (3.5399)  loss_scale: 32768.0000 (34304.5219)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1161  max mem: 15572
Epoch: [27]  [ 990/1404]  eta: 0:04:06  lr: 0.000026  min_lr: 0.000000  loss: 3.7161 (3.5428)  loss_scale: 32768.0000 (34289.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1007  max mem: 15572
Epoch: [27]  [1000/1404]  eta: 0:04:00  lr: 0.000026  min_lr: 0.000000  loss: 3.6321 (3.5444)  loss_scale: 32768.0000 (34273.8222)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1075  max mem: 15572
Epoch: [27]  [1010/1404]  eta: 0:03:55  lr: 0.000026  min_lr: 0.000000  loss: 3.6321 (3.5454)  loss_scale: 32768.0000 (34258.9278)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1322  max mem: 15572
[2025-01-10 22:35:52,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:35:52,849] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:35:52,885] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:35:52,886] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:35:53,994] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38926
[2025-01-10 22:35:53,995] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:35:54,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38926
[2025-01-10 22:35:54,009] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:35:54,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1020/1404]  eta: 0:03:48  lr: 0.000026  min_lr: 0.000000  loss: 3.6655 (3.5458)  loss_scale: 32768.0000 (34308.5132)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0856  max mem: 15572
Epoch: [27]  [1030/1404]  eta: 0:03:42  lr: 0.000026  min_lr: 0.000000  loss: 3.5535 (3.5456)  loss_scale: 32768.0000 (34293.5713)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0772  max mem: 15572
Epoch: [27]  [1040/1404]  eta: 0:03:37  lr: 0.000026  min_lr: 0.000000  loss: 3.3568 (3.5444)  loss_scale: 32768.0000 (34278.9164)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1642  max mem: 15572
Epoch: [27]  [1050/1404]  eta: 0:03:31  lr: 0.000026  min_lr: 0.000000  loss: 3.7144 (3.5486)  loss_scale: 32768.0000 (34264.5404)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1324  max mem: 15572
Epoch: [27]  [1060/1404]  eta: 0:03:24  lr: 0.000026  min_lr: 0.000000  loss: 3.5178 (3.5477)  loss_scale: 32768.0000 (34250.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0521  max mem: 15572
Epoch: [27]  [1070/1404]  eta: 0:03:18  lr: 0.000026  min_lr: 0.000000  loss: 3.4734 (3.5470)  loss_scale: 32768.0000 (34236.5938)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0364  max mem: 15572
Epoch: [27]  [1080/1404]  eta: 0:03:12  lr: 0.000026  min_lr: 0.000000  loss: 3.9014 (3.5494)  loss_scale: 32768.0000 (34223.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0751  max mem: 15572
Epoch: [27]  [1090/1404]  eta: 0:03:06  lr: 0.000026  min_lr: 0.000000  loss: 3.6873 (3.5505)  loss_scale: 32768.0000 (34209.6719)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0717  max mem: 15572
[2025-01-10 22:36:36,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=263, lr=[2.4765847856973254e-07, 2.4765847856973254e-07, 3.5379782652818937e-07, 3.5379782652818937e-07, 5.054254664688421e-07, 5.054254664688421e-07, 7.220363806697743e-07, 7.220363806697743e-07, 1.0314805438139633e-06, 1.0314805438139633e-06, 1.4735436340199477e-06, 1.4735436340199477e-06, 2.105062334314211e-06, 2.105062334314211e-06, 3.0072319061631594e-06, 3.0072319061631594e-06, 4.296045580233085e-06, 4.296045580233085e-06, 6.13720797176155e-06, 6.13720797176155e-06, 8.767439959659357e-06, 8.767439959659357e-06, 1.2524914228084798e-05, 1.2524914228084798e-05, 1.7892734611549713e-05, 1.7892734611549713e-05, 2.5561049445071018e-05, 2.5561049445071018e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:36:36,677] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=45.45876408827085, CurrSamplesPerSec=34.2274695521163, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [1100/1404]  eta: 0:03:00  lr: 0.000026  min_lr: 0.000000  loss: 3.5247 (3.5483)  loss_scale: 32768.0000 (34196.5777)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0434  max mem: 15572
Epoch: [27]  [1110/1404]  eta: 0:02:54  lr: 0.000026  min_lr: 0.000000  loss: 3.3173 (3.5456)  loss_scale: 32768.0000 (34183.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0507  max mem: 15572
Epoch: [27]  [1120/1404]  eta: 0:02:49  lr: 0.000025  min_lr: 0.000000  loss: 3.4920 (3.5455)  loss_scale: 32768.0000 (34171.0901)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1073  max mem: 15572
Epoch: [27]  [1130/1404]  eta: 0:02:43  lr: 0.000025  min_lr: 0.000000  loss: 3.8192 (3.5483)  loss_scale: 32768.0000 (34158.6844)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1029  max mem: 15572
Epoch: [27]  [1140/1404]  eta: 0:02:37  lr: 0.000025  min_lr: 0.000000  loss: 3.8972 (3.5501)  loss_scale: 32768.0000 (34146.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0292  max mem: 15572
[2025-01-10 22:37:09,752] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:37:09,752] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:37:09,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:37:09,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1150/1404]  eta: 0:02:31  lr: 0.000025  min_lr: 0.000000  loss: 3.6372 (3.5493)  loss_scale: 32768.0000 (34248.3962)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0246  max mem: 15572
Epoch: [27]  [1160/1404]  eta: 0:02:25  lr: 0.000025  min_lr: 0.000000  loss: 3.6358 (3.5504)  loss_scale: 65536.0000 (34517.8846)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0713  max mem: 15572
[2025-01-10 22:37:18,707] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39070
[2025-01-10 22:37:18,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:37:18,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:37:18,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39070
[2025-01-10 22:37:18,802] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [27]  [1170/1404]  eta: 0:02:19  lr: 0.000025  min_lr: 0.000000  loss: 3.6844 (3.5522)  loss_scale: 65536.0000 (34530.9240)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0479  max mem: 15572
Epoch: [27]  [1180/1404]  eta: 0:02:13  lr: 0.000025  min_lr: 0.000000  loss: 3.8232 (3.5549)  loss_scale: 32768.0000 (34515.9966)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0336  max mem: 15572
Epoch: [27]  [1190/1404]  eta: 0:02:07  lr: 0.000025  min_lr: 0.000000  loss: 3.7913 (3.5552)  loss_scale: 32768.0000 (34501.3199)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0817  max mem: 15572
Epoch: [27]  [1200/1404]  eta: 0:02:01  lr: 0.000025  min_lr: 0.000000  loss: 3.7871 (3.5576)  loss_scale: 32768.0000 (34486.8876)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0660  max mem: 15572
Epoch: [27]  [1210/1404]  eta: 0:01:55  lr: 0.000025  min_lr: 0.000000  loss: 3.6846 (3.5576)  loss_scale: 32768.0000 (34472.6936)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0296  max mem: 15572
Epoch: [27]  [1220/1404]  eta: 0:01:49  lr: 0.000025  min_lr: 0.000000  loss: 3.6471 (3.5570)  loss_scale: 32768.0000 (34458.7322)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1329  max mem: 15572
Epoch: [27]  [1230/1404]  eta: 0:01:43  lr: 0.000025  min_lr: 0.000000  loss: 3.3688 (3.5544)  loss_scale: 32768.0000 (34444.9976)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.1458  max mem: 15572
Epoch: [27]  [1240/1404]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000000  loss: 3.2938 (3.5546)  loss_scale: 32768.0000 (34431.4843)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0257  max mem: 15572
Epoch: [27]  [1250/1404]  eta: 0:01:31  lr: 0.000025  min_lr: 0.000000  loss: 3.5475 (3.5525)  loss_scale: 32768.0000 (34418.1871)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0008  max mem: 15572
Epoch: [27]  [1260/1404]  eta: 0:01:25  lr: 0.000025  min_lr: 0.000000  loss: 3.5161 (3.5531)  loss_scale: 32768.0000 (34405.1007)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0051  max mem: 15572
Epoch: [27]  [1270/1404]  eta: 0:01:19  lr: 0.000025  min_lr: 0.000000  loss: 3.7055 (3.5561)  loss_scale: 32768.0000 (34392.2203)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0327  max mem: 15572
Epoch: [27]  [1280/1404]  eta: 0:01:13  lr: 0.000025  min_lr: 0.000000  loss: 3.7129 (3.5561)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1473  max mem: 15572
Epoch: [27]  [1290/1404]  eta: 0:01:07  lr: 0.000025  min_lr: 0.000000  loss: 3.5610 (3.5578)  loss_scale: 32768.0000 (34367.0581)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1667  max mem: 15572
[2025-01-10 22:38:34,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:38:34,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:38:34,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:38:34,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1300/1404]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000000  loss: 3.6444 (3.5576)  loss_scale: 32768.0000 (34606.6349)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1055  max mem: 15572
Epoch: [27]  [1310/1404]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000000  loss: 3.5132 (3.5564)  loss_scale: 65536.0000 (34842.5568)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1092  max mem: 15572
[2025-01-10 22:38:49,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39224
[2025-01-10 22:38:49,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:38:49,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39224
[2025-01-10 22:38:49,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:38:49,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1320/1404]  eta: 0:00:49  lr: 0.000025  min_lr: 0.000000  loss: 3.5132 (3.5571)  loss_scale: 65536.0000 (34950.8796)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0749  max mem: 15572
Epoch: [27]  [1330/1404]  eta: 0:00:43  lr: 0.000025  min_lr: 0.000000  loss: 3.6556 (3.5588)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0241  max mem: 15572
Epoch: [27]  [1340/1404]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000000  loss: 3.6659 (3.5586)  loss_scale: 32768.0000 (34918.3236)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0008  max mem: 15572
Epoch: [27]  [1350/1404]  eta: 0:00:32  lr: 0.000025  min_lr: 0.000000  loss: 3.6874 (3.5607)  loss_scale: 32768.0000 (34902.4071)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0473  max mem: 15572
Epoch: [27]  [1360/1404]  eta: 0:00:26  lr: 0.000025  min_lr: 0.000000  loss: 3.7180 (3.5599)  loss_scale: 32768.0000 (34886.7245)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0472  max mem: 15572
Epoch: [27]  [1370/1404]  eta: 0:00:20  lr: 0.000025  min_lr: 0.000000  loss: 3.6679 (3.5596)  loss_scale: 32768.0000 (34871.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0544  max mem: 15572
Epoch: [27]  [1380/1404]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000000  loss: 3.7079 (3.5605)  loss_scale: 32768.0000 (34856.0406)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0544  max mem: 15572
Epoch: [27]  [1390/1404]  eta: 0:00:08  lr: 0.000025  min_lr: 0.000000  loss: 3.6200 (3.5600)  loss_scale: 32768.0000 (34841.0295)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0436  max mem: 15572
Epoch: [27]  [1400/1404]  eta: 0:00:02  lr: 0.000025  min_lr: 0.000000  loss: 3.7080 (3.5625)  loss_scale: 32768.0000 (34826.2327)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0433  max mem: 15572
Epoch: [27]  [1403/1404]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 3.7021 (3.5616)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)  time: 0.4528  data: 0.0004  max mem: 15572
Epoch: [27] Total time: 0:13:51 (0.5920 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 3.7021 (3.5838)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:01  loss: 1.5243 (1.5243)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 3.9849  data: 3.7849  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 2.1107 (2.0852)  acc1: 55.5556 (51.0101)  acc5: 77.7778 (79.2929)  time: 0.7870  data: 0.5885  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 2.2354 (2.2261)  acc1: 44.4444 (46.2963)  acc5: 72.2222 (76.7196)  time: 0.3976  data: 0.1926  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.1909 (2.1066)  acc1: 44.4444 (49.6416)  acc5: 83.3333 (78.4946)  time: 0.3261  data: 0.1161  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.7402 (2.0502)  acc1: 61.1111 (51.2195)  acc5: 83.3333 (79.9458)  time: 0.3156  data: 0.1055  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8657 (2.0519)  acc1: 50.0000 (51.4161)  acc5: 83.3333 (80.2832)  time: 0.3782  data: 0.1642  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.1476 (2.1397)  acc1: 44.4444 (48.5428)  acc5: 77.7778 (78.8707)  time: 0.4740  data: 0.2584  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.0008 (2.1087)  acc1: 44.4444 (49.2175)  acc5: 77.7778 (79.1862)  time: 0.4051  data: 0.2031  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8801 (2.1035)  acc1: 50.0000 (49.3141)  acc5: 83.3333 (79.6296)  time: 0.2952  data: 0.0933  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9634 (2.1110)  acc1: 44.4444 (48.7179)  acc5: 83.3333 (79.4872)  time: 0.3002  data: 0.0928  max mem: 15572
Val:  [100/136]  eta: 0:00:13  loss: 2.4548 (2.1817)  acc1: 38.8889 (47.0847)  acc5: 72.2222 (77.7228)  time: 0.2744  data: 0.0809  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2033 (2.1731)  acc1: 44.4444 (47.4975)  acc5: 72.2222 (77.5776)  time: 0.3433  data: 0.1442  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8502 (2.1240)  acc1: 55.5556 (48.6685)  acc5: 83.3333 (78.4665)  time: 0.3712  data: 0.1641  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6488 (2.0816)  acc1: 61.1111 (49.7031)  acc5: 88.8889 (79.0076)  time: 0.2189  data: 0.0399  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7520 (2.0810)  acc1: 55.5556 (49.7133)  acc5: 88.8889 (79.2383)  time: 0.1957  data: 0.0363  max mem: 15572
Val: Total time: 0:00:48 (0.3602 s / it)
* Acc@1 48.608 Acc@5 78.153 loss 2.132
Accuracy of the network on the 4883 val videos: 48.6%
Max accuracy: 48.98%
Epoch: [28]  [   0/1404]  eta: 3:06:02  lr: 0.000025  min_lr: 0.000000  loss: 3.2332 (3.2332)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9503  data: 7.4132  max mem: 15572
Epoch: [28]  [  10/1404]  eta: 0:29:38  lr: 0.000025  min_lr: 0.000000  loss: 3.5399 (3.6587)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2758  data: 0.6747  max mem: 15572
Epoch: [28]  [  20/1404]  eta: 0:22:06  lr: 0.000025  min_lr: 0.000000  loss: 3.5370 (3.4605)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0512  max mem: 15572
Epoch: [28]  [  30/1404]  eta: 0:19:43  lr: 0.000025  min_lr: 0.000000  loss: 3.1344 (3.4393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1330  max mem: 15572
Epoch: [28]  [  40/1404]  eta: 0:17:56  lr: 0.000025  min_lr: 0.000000  loss: 3.6057 (3.4830)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1122  max mem: 15572
[2025-01-10 22:41:01,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:41:01,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:41:01,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:41:01,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:41:06,203] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39362
[2025-01-10 22:41:06,203] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:41:06,204] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39362
[2025-01-10 22:41:06,205] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:41:06,205] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [  50/1404]  eta: 0:16:43  lr: 0.000025  min_lr: 0.000000  loss: 3.6225 (3.5019)  loss_scale: 32768.0000 (38550.5882)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0401  max mem: 15572
Epoch: [28]  [  60/1404]  eta: 0:16:10  lr: 0.000025  min_lr: 0.000000  loss: 3.5652 (3.5197)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0847  max mem: 15572
Epoch: [28]  [  70/1404]  eta: 0:15:35  lr: 0.000025  min_lr: 0.000000  loss: 3.7298 (3.5270)  loss_scale: 32768.0000 (36921.6901)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1033  max mem: 15572
Epoch: [28]  [  80/1404]  eta: 0:15:20  lr: 0.000025  min_lr: 0.000000  loss: 3.5193 (3.5261)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0809  max mem: 15572
Epoch: [28]  [  90/1404]  eta: 0:14:56  lr: 0.000024  min_lr: 0.000000  loss: 3.8010 (3.5649)  loss_scale: 32768.0000 (36008.7912)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0986  max mem: 15572
Epoch: [28]  [ 100/1404]  eta: 0:14:27  lr: 0.000024  min_lr: 0.000000  loss: 3.9451 (3.5903)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0469  max mem: 15572
Epoch: [28]  [ 110/1404]  eta: 0:14:18  lr: 0.000024  min_lr: 0.000000  loss: 3.7758 (3.5989)  loss_scale: 32768.0000 (35424.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0721  max mem: 15572
Epoch: [28]  [ 120/1404]  eta: 0:13:53  lr: 0.000024  min_lr: 0.000000  loss: 3.7224 (3.6002)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0721  max mem: 15572
Epoch: [28]  [ 130/1404]  eta: 0:13:40  lr: 0.000024  min_lr: 0.000000  loss: 3.6685 (3.5937)  loss_scale: 32768.0000 (35019.2366)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0348  max mem: 15572
Epoch: [28]  [ 140/1404]  eta: 0:13:23  lr: 0.000024  min_lr: 0.000000  loss: 3.7615 (3.6129)  loss_scale: 32768.0000 (34859.5745)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0634  max mem: 15572
Epoch: [28]  [ 150/1404]  eta: 0:13:20  lr: 0.000024  min_lr: 0.000000  loss: 3.9006 (3.6150)  loss_scale: 32768.0000 (34721.0596)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1177  max mem: 15572
Epoch: [28]  [ 160/1404]  eta: 0:13:04  lr: 0.000024  min_lr: 0.000000  loss: 3.6939 (3.6135)  loss_scale: 32768.0000 (34599.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0892  max mem: 15572
Epoch: [28]  [ 170/1404]  eta: 0:12:52  lr: 0.000024  min_lr: 0.000000  loss: 3.7702 (3.6218)  loss_scale: 32768.0000 (34492.6316)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0185  max mem: 15572
[2025-01-10 22:42:20,416] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:42:20,417] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:42:20,423] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:42:20,423] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 180/1404]  eta: 0:12:53  lr: 0.000024  min_lr: 0.000000  loss: 3.7496 (3.6193)  loss_scale: 32768.0000 (34759.4254)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1441  max mem: 15572
Epoch: [28]  [ 190/1404]  eta: 0:12:40  lr: 0.000024  min_lr: 0.000000  loss: 3.7575 (3.6234)  loss_scale: 65536.0000 (36370.7644)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1266  max mem: 15572
Epoch: [28]  [ 200/1404]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 3.8179 (3.6273)  loss_scale: 65536.0000 (37821.7711)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0878  max mem: 15572
[2025-01-10 22:42:37,676] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39518
[2025-01-10 22:42:37,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:42:37,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:42:37,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39518
[2025-01-10 22:42:37,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [ 210/1404]  eta: 0:12:27  lr: 0.000024  min_lr: 0.000000  loss: 3.7508 (3.6268)  loss_scale: 65536.0000 (38358.7488)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1327  max mem: 15572
Epoch: [28]  [ 220/1404]  eta: 0:12:15  lr: 0.000024  min_lr: 0.000000  loss: 3.6110 (3.6240)  loss_scale: 32768.0000 (38105.7738)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0600  max mem: 15572
Epoch: [28]  [ 230/1404]  eta: 0:12:08  lr: 0.000024  min_lr: 0.000000  loss: 3.5441 (3.6219)  loss_scale: 32768.0000 (37874.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0822  max mem: 15572
Epoch: [28]  [ 240/1404]  eta: 0:12:01  lr: 0.000024  min_lr: 0.000000  loss: 3.5461 (3.6158)  loss_scale: 32768.0000 (37662.8050)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1114  max mem: 15572
Epoch: [28]  [ 250/1404]  eta: 0:11:51  lr: 0.000024  min_lr: 0.000000  loss: 3.5461 (3.6043)  loss_scale: 32768.0000 (37467.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0720  max mem: 15572
Epoch: [28]  [ 260/1404]  eta: 0:11:45  lr: 0.000024  min_lr: 0.000000  loss: 3.4660 (3.6025)  loss_scale: 32768.0000 (37287.7241)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0760  max mem: 15572
Epoch: [28]  [ 270/1404]  eta: 0:11:39  lr: 0.000024  min_lr: 0.000000  loss: 3.7447 (3.6124)  loss_scale: 32768.0000 (37120.9446)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1032  max mem: 15572
Epoch: [28]  [ 280/1404]  eta: 0:11:30  lr: 0.000024  min_lr: 0.000000  loss: 3.8486 (3.6182)  loss_scale: 32768.0000 (36966.0356)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0585  max mem: 15572
Epoch: [28]  [ 290/1404]  eta: 0:11:24  lr: 0.000024  min_lr: 0.000000  loss: 3.8290 (3.6193)  loss_scale: 32768.0000 (36821.7732)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0563  max mem: 15572
Epoch: [28]  [ 300/1404]  eta: 0:11:15  lr: 0.000024  min_lr: 0.000000  loss: 3.5611 (3.6124)  loss_scale: 32768.0000 (36687.0963)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0535  max mem: 15572
Epoch: [28]  [ 310/1404]  eta: 0:11:07  lr: 0.000024  min_lr: 0.000000  loss: 3.7125 (3.6225)  loss_scale: 32768.0000 (36561.0804)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0402  max mem: 15572
Epoch: [28]  [ 320/1404]  eta: 0:11:03  lr: 0.000024  min_lr: 0.000000  loss: 3.8155 (3.6252)  loss_scale: 32768.0000 (36442.9159)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1188  max mem: 15572
Epoch: [28]  [ 330/1404]  eta: 0:10:57  lr: 0.000024  min_lr: 0.000000  loss: 3.5005 (3.6226)  loss_scale: 32768.0000 (36331.8912)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1422  max mem: 15572
[2025-01-10 22:43:53,699] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:43:53,700] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:43:53,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:43:53,728] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:43:54,772] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39649
[2025-01-10 22:43:54,772] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:43:54,797] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39649
[2025-01-10 22:43:54,797] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:43:54,797] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 340/1404]  eta: 0:10:48  lr: 0.000024  min_lr: 0.000000  loss: 3.8362 (3.6274)  loss_scale: 32768.0000 (36419.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0638  max mem: 15572
Epoch: [28]  [ 350/1404]  eta: 0:10:41  lr: 0.000024  min_lr: 0.000000  loss: 3.6831 (3.6221)  loss_scale: 32768.0000 (36315.5328)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0250  max mem: 15572
Epoch: [28]  [ 360/1404]  eta: 0:10:32  lr: 0.000024  min_lr: 0.000000  loss: 3.6464 (3.6216)  loss_scale: 32768.0000 (36217.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0252  max mem: 15572
Epoch: [28]  [ 370/1404]  eta: 0:10:28  lr: 0.000024  min_lr: 0.000000  loss: 3.6786 (3.6243)  loss_scale: 32768.0000 (36124.2911)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0999  max mem: 15572
Epoch: [28]  [ 380/1404]  eta: 0:10:21  lr: 0.000024  min_lr: 0.000000  loss: 3.7720 (3.6270)  loss_scale: 32768.0000 (36036.1995)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1132  max mem: 15572
Epoch: [28]  [ 390/1404]  eta: 0:10:13  lr: 0.000024  min_lr: 0.000000  loss: 3.6250 (3.6267)  loss_scale: 32768.0000 (35952.6138)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0343  max mem: 15572
Epoch: [28]  [ 400/1404]  eta: 0:10:06  lr: 0.000024  min_lr: 0.000000  loss: 3.4130 (3.6207)  loss_scale: 32768.0000 (35873.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0284  max mem: 15572
Epoch: [28]  [ 410/1404]  eta: 0:10:02  lr: 0.000024  min_lr: 0.000000  loss: 3.8963 (3.6284)  loss_scale: 32768.0000 (35797.6448)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0995  max mem: 15572
Epoch: [28]  [ 420/1404]  eta: 0:09:56  lr: 0.000024  min_lr: 0.000000  loss: 4.0121 (3.6318)  loss_scale: 32768.0000 (35725.6817)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.1519  max mem: 15572
Epoch: [28]  [ 430/1404]  eta: 0:09:50  lr: 0.000024  min_lr: 0.000000  loss: 3.6999 (3.6308)  loss_scale: 32768.0000 (35657.0580)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1119  max mem: 15572
Epoch: [28]  [ 440/1404]  eta: 0:09:43  lr: 0.000024  min_lr: 0.000000  loss: 3.6032 (3.6310)  loss_scale: 32768.0000 (35591.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0520  max mem: 15572
Epoch: [28]  [ 450/1404]  eta: 0:09:36  lr: 0.000024  min_lr: 0.000000  loss: 3.5397 (3.6308)  loss_scale: 32768.0000 (35528.9401)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0305  max mem: 15572
Epoch: [28]  [ 460/1404]  eta: 0:09:30  lr: 0.000024  min_lr: 0.000000  loss: 3.4835 (3.6234)  loss_scale: 32768.0000 (35469.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0637  max mem: 15572
[2025-01-10 22:45:10,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:45:10,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:45:10,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:45:10,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:45:11,006] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39780
[2025-01-10 22:45:11,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:45:11,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:45:11,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39780
[2025-01-10 22:45:11,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [ 470/1404]  eta: 0:09:22  lr: 0.000024  min_lr: 0.000000  loss: 3.3871 (3.6220)  loss_scale: 32768.0000 (35550.8450)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0559  max mem: 15572
Epoch: [28]  [ 480/1404]  eta: 0:09:17  lr: 0.000023  min_lr: 0.000000  loss: 3.5348 (3.6169)  loss_scale: 32768.0000 (35492.9896)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1094  max mem: 15572
Epoch: [28]  [ 490/1404]  eta: 0:09:09  lr: 0.000023  min_lr: 0.000000  loss: 3.6123 (3.6186)  loss_scale: 32768.0000 (35437.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0973  max mem: 15572
Epoch: [28]  [ 500/1404]  eta: 0:09:04  lr: 0.000023  min_lr: 0.000000  loss: 3.7307 (3.6196)  loss_scale: 32768.0000 (35384.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0585  max mem: 15572
Epoch: [28]  [ 510/1404]  eta: 0:08:57  lr: 0.000023  min_lr: 0.000000  loss: 3.7319 (3.6169)  loss_scale: 32768.0000 (35333.0098)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0731  max mem: 15572
Epoch: [28]  [ 520/1404]  eta: 0:08:51  lr: 0.000023  min_lr: 0.000000  loss: 3.5964 (3.6165)  loss_scale: 32768.0000 (35283.7774)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0435  max mem: 15572
Epoch: [28]  [ 530/1404]  eta: 0:08:45  lr: 0.000023  min_lr: 0.000000  loss: 3.4967 (3.6142)  loss_scale: 32768.0000 (35236.3992)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0599  max mem: 15572
Epoch: [28]  [ 540/1404]  eta: 0:08:39  lr: 0.000023  min_lr: 0.000000  loss: 3.4351 (3.6083)  loss_scale: 32768.0000 (35190.7726)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0639  max mem: 15572
Epoch: [28]  [ 550/1404]  eta: 0:08:32  lr: 0.000023  min_lr: 0.000000  loss: 3.6071 (3.6102)  loss_scale: 32768.0000 (35146.8022)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0232  max mem: 15572
Epoch: [28]  [ 560/1404]  eta: 0:08:25  lr: 0.000023  min_lr: 0.000000  loss: 3.6882 (3.6092)  loss_scale: 32768.0000 (35104.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0044  max mem: 15572
Epoch: [28]  [ 570/1404]  eta: 0:08:20  lr: 0.000023  min_lr: 0.000000  loss: 3.3953 (3.6036)  loss_scale: 32768.0000 (35063.4816)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0193  max mem: 15572
Epoch: [28]  [ 580/1404]  eta: 0:08:14  lr: 0.000023  min_lr: 0.000000  loss: 3.3699 (3.6040)  loss_scale: 32768.0000 (35023.9725)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0157  max mem: 15572
Epoch: [28]  [ 590/1404]  eta: 0:08:09  lr: 0.000023  min_lr: 0.000000  loss: 3.3174 (3.5978)  loss_scale: 32768.0000 (34985.8003)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0096  max mem: 15572
[2025-01-10 22:46:27,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:46:27,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:46:27,596] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:46:27,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 600/1404]  eta: 0:08:02  lr: 0.000023  min_lr: 0.000000  loss: 3.5165 (3.5994)  loss_scale: 32768.0000 (35166.9884)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0096  max mem: 15572
Epoch: [28]  [ 610/1404]  eta: 0:07:56  lr: 0.000023  min_lr: 0.000000  loss: 3.6876 (3.6006)  loss_scale: 65536.0000 (35664.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0008  max mem: 15572
[2025-01-10 22:46:36,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39924
[2025-01-10 22:46:36,429] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:46:36,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39924
[2025-01-10 22:46:36,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:46:36,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 620/1404]  eta: 0:07:50  lr: 0.000023  min_lr: 0.000000  loss: 3.4838 (3.5968)  loss_scale: 65536.0000 (35670.1578)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0009  max mem: 15572
Epoch: [28]  [ 630/1404]  eta: 0:07:44  lr: 0.000023  min_lr: 0.000000  loss: 3.7267 (3.6029)  loss_scale: 32768.0000 (35624.1648)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0008  max mem: 15572
Epoch: [28]  [ 640/1404]  eta: 0:07:37  lr: 0.000023  min_lr: 0.000000  loss: 3.8240 (3.6032)  loss_scale: 32768.0000 (35579.6069)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0008  max mem: 15572
Epoch: [28]  [ 650/1404]  eta: 0:07:31  lr: 0.000023  min_lr: 0.000000  loss: 3.4481 (3.6028)  loss_scale: 32768.0000 (35536.4178)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0008  max mem: 15572
Epoch: [28]  [ 660/1404]  eta: 0:07:25  lr: 0.000023  min_lr: 0.000000  loss: 3.5247 (3.6042)  loss_scale: 32768.0000 (35494.5356)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0124  max mem: 15572
Epoch: [28]  [ 670/1404]  eta: 0:07:19  lr: 0.000023  min_lr: 0.000000  loss: 3.7901 (3.6067)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0123  max mem: 15572
Epoch: [28]  [ 680/1404]  eta: 0:07:12  lr: 0.000023  min_lr: 0.000000  loss: 3.8876 (3.6117)  loss_scale: 32768.0000 (35414.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0217  max mem: 15572
[2025-01-10 22:47:19,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=270, lr=[2.2228591471062704e-07, 2.2228591471062704e-07, 3.175513067294672e-07, 3.175513067294672e-07, 4.5364472389923893e-07, 4.5364472389923893e-07, 6.480638912846271e-07, 6.480638912846271e-07, 9.258055589780387e-07, 9.258055589780387e-07, 1.3225793699686267e-06, 1.3225793699686267e-06, 1.8893990999551814e-06, 1.8893990999551814e-06, 2.699141571364545e-06, 2.699141571364545e-06, 3.855916530520778e-06, 3.855916530520778e-06, 5.508452186458256e-06, 5.508452186458256e-06, 7.86921740922608e-06, 7.86921740922608e-06, 1.1241739156037258e-05, 1.1241739156037258e-05, 1.605962736576751e-05, 1.605962736576751e-05, 2.2942324808239304e-05, 2.2942324808239304e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:47:19,988] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=45.5324034397796, CurrSamplesPerSec=44.40073148978852, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [ 690/1404]  eta: 0:07:06  lr: 0.000023  min_lr: 0.000000  loss: 3.8044 (3.6089)  loss_scale: 32768.0000 (35376.1621)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0258  max mem: 15572
Epoch: [28]  [ 700/1404]  eta: 0:07:01  lr: 0.000023  min_lr: 0.000000  loss: 3.5056 (3.6068)  loss_scale: 32768.0000 (35338.9558)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0379  max mem: 15572
Epoch: [28]  [ 710/1404]  eta: 0:06:54  lr: 0.000023  min_lr: 0.000000  loss: 3.3771 (3.6027)  loss_scale: 32768.0000 (35302.7961)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0337  max mem: 15572
Epoch: [28]  [ 720/1404]  eta: 0:06:48  lr: 0.000023  min_lr: 0.000000  loss: 3.2286 (3.5955)  loss_scale: 32768.0000 (35267.6394)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0509  max mem: 15572
Epoch: [28]  [ 730/1404]  eta: 0:06:42  lr: 0.000023  min_lr: 0.000000  loss: 3.2733 (3.5926)  loss_scale: 32768.0000 (35233.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0637  max mem: 15572
Epoch: [28]  [ 740/1404]  eta: 0:06:36  lr: 0.000023  min_lr: 0.000000  loss: 3.3398 (3.5894)  loss_scale: 32768.0000 (35200.1727)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0629  max mem: 15572
[2025-01-10 22:47:51,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:47:51,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:47:51,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:47:51,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:47:52,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40055
[2025-01-10 22:47:52,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:47:52,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:47:52,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40055
[2025-01-10 22:47:52,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [ 750/1404]  eta: 0:06:30  lr: 0.000023  min_lr: 0.000000  loss: 3.3643 (3.5899)  loss_scale: 32768.0000 (35255.0519)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1054  max mem: 15572
Epoch: [28]  [ 760/1404]  eta: 0:06:24  lr: 0.000023  min_lr: 0.000000  loss: 3.5469 (3.5895)  loss_scale: 32768.0000 (35222.3706)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0952  max mem: 15572
Epoch: [28]  [ 770/1404]  eta: 0:06:17  lr: 0.000023  min_lr: 0.000000  loss: 3.6270 (3.5900)  loss_scale: 32768.0000 (35190.5370)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0566  max mem: 15572
Epoch: [28]  [ 780/1404]  eta: 0:06:12  lr: 0.000023  min_lr: 0.000000  loss: 3.7865 (3.5924)  loss_scale: 32768.0000 (35159.5186)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1052  max mem: 15572
Epoch: [28]  [ 790/1404]  eta: 0:06:05  lr: 0.000023  min_lr: 0.000000  loss: 3.4653 (3.5889)  loss_scale: 32768.0000 (35129.2845)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0886  max mem: 15572
Epoch: [28]  [ 800/1404]  eta: 0:05:59  lr: 0.000023  min_lr: 0.000000  loss: 3.4629 (3.5902)  loss_scale: 32768.0000 (35099.8052)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0215  max mem: 15572
Epoch: [28]  [ 810/1404]  eta: 0:05:53  lr: 0.000023  min_lr: 0.000000  loss: 3.5765 (3.5887)  loss_scale: 32768.0000 (35071.0530)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0932  max mem: 15572
Epoch: [28]  [ 820/1404]  eta: 0:05:48  lr: 0.000023  min_lr: 0.000000  loss: 3.8018 (3.5903)  loss_scale: 32768.0000 (35043.0012)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1268  max mem: 15572
Epoch: [28]  [ 830/1404]  eta: 0:05:41  lr: 0.000023  min_lr: 0.000000  loss: 3.6891 (3.5904)  loss_scale: 32768.0000 (35015.6245)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0587  max mem: 15572
Epoch: [28]  [ 840/1404]  eta: 0:05:35  lr: 0.000023  min_lr: 0.000000  loss: 3.6862 (3.5906)  loss_scale: 32768.0000 (34988.8989)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0205  max mem: 15572
Epoch: [28]  [ 850/1404]  eta: 0:05:29  lr: 0.000023  min_lr: 0.000000  loss: 3.7078 (3.5903)  loss_scale: 32768.0000 (34962.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0223  max mem: 15572
Epoch: [28]  [ 860/1404]  eta: 0:05:24  lr: 0.000022  min_lr: 0.000000  loss: 3.7646 (3.5921)  loss_scale: 32768.0000 (34937.3101)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0062  max mem: 15572
Epoch: [28]  [ 870/1404]  eta: 0:05:17  lr: 0.000022  min_lr: 0.000000  loss: 3.6727 (3.5930)  loss_scale: 32768.0000 (34912.4041)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0008  max mem: 15572
[2025-01-10 22:49:08,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:49:08,463] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:49:08,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:49:08,463] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 880/1404]  eta: 0:05:12  lr: 0.000022  min_lr: 0.000000  loss: 3.3680 (3.5897)  loss_scale: 32768.0000 (35222.8104)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0007  max mem: 15572
Epoch: [28]  [ 890/1404]  eta: 0:05:06  lr: 0.000022  min_lr: 0.000000  loss: 3.3680 (3.5894)  loss_scale: 65536.0000 (35563.0258)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.0006  max mem: 15572
[2025-01-10 22:49:23,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40208
[2025-01-10 22:49:23,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:49:23,639] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40208
[2025-01-10 22:49:23,640] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:49:23,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 900/1404]  eta: 0:05:00  lr: 0.000022  min_lr: 0.000000  loss: 3.5971 (3.5884)  loss_scale: 65536.0000 (35713.8468)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0408  max mem: 15572
Epoch: [28]  [ 910/1404]  eta: 0:04:54  lr: 0.000022  min_lr: 0.000000  loss: 3.5971 (3.5902)  loss_scale: 32768.0000 (35681.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0661  max mem: 15572
Epoch: [28]  [ 920/1404]  eta: 0:04:49  lr: 0.000022  min_lr: 0.000000  loss: 3.5812 (3.5919)  loss_scale: 32768.0000 (35649.8762)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1373  max mem: 15572
Epoch: [28]  [ 930/1404]  eta: 0:04:42  lr: 0.000022  min_lr: 0.000000  loss: 3.4514 (3.5885)  loss_scale: 32768.0000 (35618.9216)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1120  max mem: 15572
Epoch: [28]  [ 940/1404]  eta: 0:04:37  lr: 0.000022  min_lr: 0.000000  loss: 3.2042 (3.5873)  loss_scale: 32768.0000 (35588.6249)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0546  max mem: 15572
Epoch: [28]  [ 950/1404]  eta: 0:04:30  lr: 0.000022  min_lr: 0.000000  loss: 3.7195 (3.5906)  loss_scale: 32768.0000 (35558.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0548  max mem: 15572
Epoch: [28]  [ 960/1404]  eta: 0:04:25  lr: 0.000022  min_lr: 0.000000  loss: 3.8866 (3.5924)  loss_scale: 32768.0000 (35529.9230)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0789  max mem: 15572
Epoch: [28]  [ 970/1404]  eta: 0:04:18  lr: 0.000022  min_lr: 0.000000  loss: 3.8274 (3.5944)  loss_scale: 32768.0000 (35501.4789)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.0924  max mem: 15572
Epoch: [28]  [ 980/1404]  eta: 0:04:12  lr: 0.000022  min_lr: 0.000000  loss: 3.9732 (3.5983)  loss_scale: 32768.0000 (35473.6147)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0349  max mem: 15572
Epoch: [28]  [ 990/1404]  eta: 0:04:07  lr: 0.000022  min_lr: 0.000000  loss: 3.8167 (3.5988)  loss_scale: 32768.0000 (35446.3128)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1006  max mem: 15572
Epoch: [28]  [1000/1404]  eta: 0:04:01  lr: 0.000022  min_lr: 0.000000  loss: 3.6375 (3.5991)  loss_scale: 32768.0000 (35419.5564)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1012  max mem: 15572
Epoch: [28]  [1010/1404]  eta: 0:03:55  lr: 0.000022  min_lr: 0.000000  loss: 3.6023 (3.5981)  loss_scale: 32768.0000 (35393.3294)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0574  max mem: 15572
Epoch: [28]  [1020/1404]  eta: 0:03:49  lr: 0.000022  min_lr: 0.000000  loss: 3.6117 (3.5983)  loss_scale: 32768.0000 (35367.6161)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0648  max mem: 15572
[2025-01-10 22:50:40,280] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:50:40,281] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:50:40,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:50:40,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:50:41,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40339
[2025-01-10 22:50:41,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:50:41,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:50:41,358] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40339
[2025-01-10 22:50:41,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [1030/1404]  eta: 0:03:43  lr: 0.000022  min_lr: 0.000000  loss: 3.6732 (3.5978)  loss_scale: 32768.0000 (35405.9670)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0851  max mem: 15572
Epoch: [28]  [1040/1404]  eta: 0:03:36  lr: 0.000022  min_lr: 0.000000  loss: 3.4151 (3.5968)  loss_scale: 32768.0000 (35380.6263)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0825  max mem: 15572
Epoch: [28]  [1050/1404]  eta: 0:03:31  lr: 0.000022  min_lr: 0.000000  loss: 3.4043 (3.5947)  loss_scale: 32768.0000 (35355.7678)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0926  max mem: 15572
Epoch: [28]  [1060/1404]  eta: 0:03:25  lr: 0.000022  min_lr: 0.000000  loss: 3.5811 (3.5962)  loss_scale: 32768.0000 (35331.3779)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1373  max mem: 15572
Epoch: [28]  [1070/1404]  eta: 0:03:19  lr: 0.000022  min_lr: 0.000000  loss: 3.6891 (3.5964)  loss_scale: 32768.0000 (35307.4435)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0934  max mem: 15572
Epoch: [28]  [1080/1404]  eta: 0:03:13  lr: 0.000022  min_lr: 0.000000  loss: 3.5776 (3.5960)  loss_scale: 32768.0000 (35283.9519)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0594  max mem: 15572
Epoch: [28]  [1090/1404]  eta: 0:03:07  lr: 0.000022  min_lr: 0.000000  loss: 3.5876 (3.5966)  loss_scale: 32768.0000 (35260.8909)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1228  max mem: 15572
Epoch: [28]  [1100/1404]  eta: 0:03:01  lr: 0.000022  min_lr: 0.000000  loss: 3.5437 (3.5949)  loss_scale: 32768.0000 (35238.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0859  max mem: 15572
Epoch: [28]  [1110/1404]  eta: 0:02:55  lr: 0.000022  min_lr: 0.000000  loss: 3.5437 (3.5955)  loss_scale: 32768.0000 (35216.0144)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0407  max mem: 15572
Epoch: [28]  [1120/1404]  eta: 0:02:49  lr: 0.000022  min_lr: 0.000000  loss: 3.5847 (3.5958)  loss_scale: 32768.0000 (35194.1766)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0408  max mem: 15572
Epoch: [28]  [1130/1404]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000000  loss: 3.5847 (3.5950)  loss_scale: 32768.0000 (35172.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0589  max mem: 15572
Epoch: [28]  [1140/1404]  eta: 0:02:37  lr: 0.000022  min_lr: 0.000000  loss: 3.5759 (3.5952)  loss_scale: 32768.0000 (35151.6494)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0721  max mem: 15572
Epoch: [28]  [1150/1404]  eta: 0:02:31  lr: 0.000022  min_lr: 0.000000  loss: 3.8389 (3.5976)  loss_scale: 32768.0000 (35130.9401)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0266  max mem: 15572
[2025-01-10 22:51:56,890] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:51:56,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:51:56,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:51:56,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:51:59,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40471
[2025-01-10 22:51:59,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:51:59,577] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:51:59,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40471
[2025-01-10 22:51:59,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [1160/1404]  eta: 0:02:25  lr: 0.000022  min_lr: 0.000000  loss: 3.7779 (3.5974)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0707  max mem: 15572
Epoch: [28]  [1170/1404]  eta: 0:02:19  lr: 0.000022  min_lr: 0.000000  loss: 3.6949 (3.5987)  loss_scale: 32768.0000 (35174.5312)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0893  max mem: 15572
Epoch: [28]  [1180/1404]  eta: 0:02:13  lr: 0.000022  min_lr: 0.000000  loss: 3.5781 (3.5981)  loss_scale: 32768.0000 (35154.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0520  max mem: 15572
Epoch: [28]  [1190/1404]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000000  loss: 3.5459 (3.5977)  loss_scale: 32768.0000 (35134.1192)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1001  max mem: 15572
Epoch: [28]  [1200/1404]  eta: 0:02:01  lr: 0.000022  min_lr: 0.000000  loss: 3.6519 (3.5983)  loss_scale: 32768.0000 (35114.4180)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0923  max mem: 15572
Epoch: [28]  [1210/1404]  eta: 0:01:55  lr: 0.000022  min_lr: 0.000000  loss: 3.7346 (3.6011)  loss_scale: 32768.0000 (35095.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0716  max mem: 15572
Epoch: [28]  [1220/1404]  eta: 0:01:49  lr: 0.000022  min_lr: 0.000000  loss: 3.6530 (3.6005)  loss_scale: 32768.0000 (35075.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0592  max mem: 15572
Epoch: [28]  [1230/1404]  eta: 0:01:43  lr: 0.000022  min_lr: 0.000000  loss: 3.4037 (3.5983)  loss_scale: 32768.0000 (35057.2348)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0006  max mem: 15572
Epoch: [28]  [1240/1404]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000000  loss: 3.2563 (3.5962)  loss_scale: 32768.0000 (35038.7881)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0010  max mem: 15572
Epoch: [28]  [1250/1404]  eta: 0:01:31  lr: 0.000022  min_lr: 0.000000  loss: 3.2563 (3.5946)  loss_scale: 32768.0000 (35020.6363)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0224  max mem: 15572
Epoch: [28]  [1260/1404]  eta: 0:01:25  lr: 0.000021  min_lr: 0.000000  loss: 3.5823 (3.5954)  loss_scale: 32768.0000 (35002.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0223  max mem: 15572
Epoch: [28]  [1270/1404]  eta: 0:01:19  lr: 0.000021  min_lr: 0.000000  loss: 3.6434 (3.5948)  loss_scale: 32768.0000 (34985.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0009  max mem: 15572
Epoch: [28]  [1280/1404]  eta: 0:01:13  lr: 0.000021  min_lr: 0.000000  loss: 3.6434 (3.5956)  loss_scale: 32768.0000 (34967.8813)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0676  max mem: 15572
[2025-01-10 22:53:14,914] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:53:14,914] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:53:14,928] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:53:14,928] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1290/1404]  eta: 0:01:07  lr: 0.000021  min_lr: 0.000000  loss: 3.7634 (3.5965)  loss_scale: 32768.0000 (35026.9868)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0676  max mem: 15572
[2025-01-10 22:53:18,210] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40606
[2025-01-10 22:53:18,210] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:53:18,213] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40606
[2025-01-10 22:53:18,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:53:18,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1300/1404]  eta: 0:01:01  lr: 0.000021  min_lr: 0.000000  loss: 3.6383 (3.5947)  loss_scale: 32768.0000 (35085.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0191  max mem: 15572
Epoch: [28]  [1310/1404]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 3.7605 (3.5961)  loss_scale: 32768.0000 (35067.5088)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1176  max mem: 15572
Epoch: [28]  [1320/1404]  eta: 0:00:49  lr: 0.000021  min_lr: 0.000000  loss: 3.7605 (3.5939)  loss_scale: 32768.0000 (35050.1014)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1171  max mem: 15572
Epoch: [28]  [1330/1404]  eta: 0:00:44  lr: 0.000021  min_lr: 0.000000  loss: 3.5140 (3.5944)  loss_scale: 32768.0000 (35032.9557)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1076  max mem: 15572
Epoch: [28]  [1340/1404]  eta: 0:00:38  lr: 0.000021  min_lr: 0.000000  loss: 3.6412 (3.5947)  loss_scale: 32768.0000 (35016.0656)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1032  max mem: 15572
Epoch: [28]  [1350/1404]  eta: 0:00:32  lr: 0.000021  min_lr: 0.000000  loss: 3.6024 (3.5950)  loss_scale: 32768.0000 (34999.4256)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1274  max mem: 15572
Epoch: [28]  [1360/1404]  eta: 0:00:26  lr: 0.000021  min_lr: 0.000000  loss: 3.7355 (3.5965)  loss_scale: 32768.0000 (34983.0301)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1637  max mem: 15572
Epoch: [28]  [1370/1404]  eta: 0:00:20  lr: 0.000021  min_lr: 0.000000  loss: 3.6954 (3.5956)  loss_scale: 32768.0000 (34966.8738)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0808  max mem: 15572
Epoch: [28]  [1380/1404]  eta: 0:00:14  lr: 0.000021  min_lr: 0.000000  loss: 3.4539 (3.5943)  loss_scale: 32768.0000 (34950.9515)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0567  max mem: 15572
Epoch: [28]  [1390/1404]  eta: 0:00:08  lr: 0.000021  min_lr: 0.000000  loss: 3.4387 (3.5943)  loss_scale: 32768.0000 (34935.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0262  max mem: 15572
Epoch: [28]  [1400/1404]  eta: 0:00:02  lr: 0.000021  min_lr: 0.000000  loss: 3.4020 (3.5930)  loss_scale: 32768.0000 (34919.7887)  weight_decay: 0.0500 (0.0500)  time: 0.4518  data: 0.0005  max mem: 15572
Epoch: [28]  [1403/1404]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 3.4538 (3.5933)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)  time: 0.4273  data: 0.0004  max mem: 15572
Epoch: [28] Total time: 0:13:52 (0.5928 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 3.4538 (3.5994)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:16:05  loss: 1.4793 (1.4793)  acc1: 61.1111 (61.1111)  acc5: 83.3333 (83.3333)  time: 7.0978  data: 6.9256  max mem: 15572
Val:  [ 10/136]  eta: 0:01:47  loss: 2.0263 (1.9921)  acc1: 50.0000 (50.0000)  acc5: 83.3333 (81.8182)  time: 0.8499  data: 0.6399  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 2.2979 (2.1826)  acc1: 44.4444 (45.5026)  acc5: 77.7778 (78.8360)  time: 0.2654  data: 0.0545  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.1298 (2.0706)  acc1: 44.4444 (48.9247)  acc5: 77.7778 (79.3907)  time: 0.2871  data: 0.0803  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.7769 (2.0161)  acc1: 61.1111 (51.2195)  acc5: 83.3333 (80.2168)  time: 0.3338  data: 0.1304  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 1.8104 (1.9973)  acc1: 55.5556 (51.8519)  acc5: 83.3333 (80.6100)  time: 0.4027  data: 0.1964  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.1176 (2.0957)  acc1: 44.4444 (48.8160)  acc5: 77.7778 (79.4171)  time: 0.3730  data: 0.1651  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 1.9944 (2.0689)  acc1: 50.0000 (49.9218)  acc5: 77.7778 (79.4210)  time: 0.3913  data: 0.1830  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8426 (2.0655)  acc1: 55.5556 (50.0686)  acc5: 83.3333 (79.8354)  time: 0.3731  data: 0.1537  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 1.9663 (2.0778)  acc1: 44.4444 (49.6337)  acc5: 83.3333 (79.7314)  time: 0.3454  data: 0.1272  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2208 (2.1600)  acc1: 38.8889 (47.4147)  acc5: 72.2222 (77.9978)  time: 0.3704  data: 0.1598  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.1065 (2.1487)  acc1: 44.4444 (48.0981)  acc5: 77.7778 (78.1782)  time: 0.3526  data: 0.1436  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.7339 (2.1017)  acc1: 55.5556 (49.3572)  acc5: 83.3333 (79.0174)  time: 0.3552  data: 0.1635  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5914 (2.0644)  acc1: 61.1111 (50.0848)  acc5: 88.8889 (79.5165)  time: 0.2528  data: 0.0911  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.8264 (2.0625)  acc1: 50.0000 (50.3276)  acc5: 83.3333 (79.6069)  time: 0.1518  data: 0.0034  max mem: 15572
Val: Total time: 0:00:50 (0.3738 s / it)
* Acc@1 49.263 Acc@5 78.604 loss 2.108
Accuracy of the network on the 4883 val videos: 49.3%
[2025-01-10 22:55:11,475] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 22:55:11,477] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 22:55:11,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 22:55:11,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 22:55:14,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 22:55:14,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 49.26%
Epoch: [29]  [   0/1404]  eta: 3:25:50  lr: 0.000021  min_lr: 0.000000  loss: 3.5810 (3.5810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7966  data: 8.2422  max mem: 15572
Epoch: [29]  [  10/1404]  eta: 0:30:52  lr: 0.000021  min_lr: 0.000000  loss: 3.8601 (3.8111)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3287  data: 0.7502  max mem: 15572
[2025-01-10 22:55:32,975] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:55:32,975] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:55:32,975] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:55:32,975] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [  20/1404]  eta: 0:22:58  lr: 0.000021  min_lr: 0.000000  loss: 3.7587 (3.6494)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0008  max mem: 15572
[2025-01-10 22:55:40,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40746
[2025-01-10 22:55:40,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:55:40,501] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [  30/1404]  eta: 0:19:34  lr: 0.000021  min_lr: 0.000000  loss: 3.4562 (3.6050)  loss_scale: 65536.0000 (44395.3548)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0008  max mem: 15572
[2025-01-10 22:55:40,516] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40746
[2025-01-10 22:55:40,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [  40/1404]  eta: 0:17:36  lr: 0.000021  min_lr: 0.000000  loss: 3.4114 (3.5366)  loss_scale: 32768.0000 (41559.4146)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0009  max mem: 15572
Epoch: [29]  [  50/1404]  eta: 0:16:29  lr: 0.000021  min_lr: 0.000000  loss: 3.4633 (3.5761)  loss_scale: 32768.0000 (39835.6078)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0008  max mem: 15572
Epoch: [29]  [  60/1404]  eta: 0:15:46  lr: 0.000021  min_lr: 0.000000  loss: 3.4633 (3.5344)  loss_scale: 32768.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0007  max mem: 15572
Epoch: [29]  [  70/1404]  eta: 0:15:24  lr: 0.000021  min_lr: 0.000000  loss: 3.3868 (3.5194)  loss_scale: 32768.0000 (37844.7324)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0007  max mem: 15572
Epoch: [29]  [  80/1404]  eta: 0:14:59  lr: 0.000021  min_lr: 0.000000  loss: 3.3868 (3.5094)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0007  max mem: 15572
Epoch: [29]  [  90/1404]  eta: 0:14:47  lr: 0.000021  min_lr: 0.000000  loss: 3.4669 (3.5234)  loss_scale: 32768.0000 (36728.9670)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0009  max mem: 15572
Epoch: [29]  [ 100/1404]  eta: 0:14:26  lr: 0.000021  min_lr: 0.000000  loss: 3.7127 (3.5379)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0130  max mem: 15572
Epoch: [29]  [ 110/1404]  eta: 0:14:06  lr: 0.000021  min_lr: 0.000000  loss: 3.4485 (3.5208)  loss_scale: 32768.0000 (36015.2793)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0342  max mem: 15572
Epoch: [29]  [ 120/1404]  eta: 0:13:42  lr: 0.000021  min_lr: 0.000000  loss: 3.4299 (3.5075)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0308  max mem: 15572
Epoch: [29]  [ 130/1404]  eta: 0:13:47  lr: 0.000021  min_lr: 0.000000  loss: 3.5272 (3.4969)  loss_scale: 32768.0000 (35519.5115)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.0097  max mem: 15572
Epoch: [29]  [ 140/1404]  eta: 0:13:33  lr: 0.000021  min_lr: 0.000000  loss: 3.5801 (3.4914)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0009  max mem: 15572
Epoch: [29]  [ 150/1404]  eta: 0:13:26  lr: 0.000021  min_lr: 0.000000  loss: 3.6270 (3.5121)  loss_scale: 32768.0000 (35155.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0008  max mem: 15572
[2025-01-10 22:56:55,834] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:56:55,834] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:56:55,840] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:56:55,840] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 160/1404]  eta: 0:13:10  lr: 0.000021  min_lr: 0.000000  loss: 3.7678 (3.5285)  loss_scale: 32768.0000 (35413.8634)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0007  max mem: 15572
[2025-01-10 22:56:57,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40878
[2025-01-10 22:56:57,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:56:57,940] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40878
[2025-01-10 22:56:57,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:56:57,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 170/1404]  eta: 0:12:58  lr: 0.000021  min_lr: 0.000000  loss: 3.5247 (3.5150)  loss_scale: 32768.0000 (35450.7602)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0009  max mem: 15572
Epoch: [29]  [ 180/1404]  eta: 0:12:45  lr: 0.000021  min_lr: 0.000000  loss: 3.2249 (3.5185)  loss_scale: 32768.0000 (35302.5414)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0010  max mem: 15572
Epoch: [29]  [ 190/1404]  eta: 0:12:32  lr: 0.000021  min_lr: 0.000000  loss: 3.6146 (3.5228)  loss_scale: 32768.0000 (35169.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0009  max mem: 15572
Epoch: [29]  [ 200/1404]  eta: 0:12:25  lr: 0.000021  min_lr: 0.000000  loss: 3.6905 (3.5257)  loss_scale: 32768.0000 (35050.3483)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0008  max mem: 15572
Epoch: [29]  [ 210/1404]  eta: 0:12:18  lr: 0.000021  min_lr: 0.000000  loss: 3.4898 (3.5191)  loss_scale: 32768.0000 (34942.1801)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0403  max mem: 15572
Epoch: [29]  [ 220/1404]  eta: 0:12:12  lr: 0.000021  min_lr: 0.000000  loss: 3.3657 (3.5099)  loss_scale: 32768.0000 (34843.8009)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.0994  max mem: 15572
Epoch: [29]  [ 230/1404]  eta: 0:12:04  lr: 0.000021  min_lr: 0.000000  loss: 3.3879 (3.5091)  loss_scale: 32768.0000 (34753.9394)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0838  max mem: 15572
Epoch: [29]  [ 240/1404]  eta: 0:11:59  lr: 0.000021  min_lr: 0.000000  loss: 3.6014 (3.5079)  loss_scale: 32768.0000 (34671.5353)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0874  max mem: 15572
Epoch: [29]  [ 250/1404]  eta: 0:11:49  lr: 0.000021  min_lr: 0.000000  loss: 3.5179 (3.5112)  loss_scale: 32768.0000 (34595.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0915  max mem: 15572
Epoch: [29]  [ 260/1404]  eta: 0:11:43  lr: 0.000020  min_lr: 0.000000  loss: 3.7291 (3.5200)  loss_scale: 32768.0000 (34525.6705)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1018  max mem: 15572
Epoch: [29]  [ 270/1404]  eta: 0:11:37  lr: 0.000020  min_lr: 0.000000  loss: 3.7279 (3.5205)  loss_scale: 32768.0000 (34460.8118)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1148  max mem: 15572
Epoch: [29]  [ 280/1404]  eta: 0:11:31  lr: 0.000020  min_lr: 0.000000  loss: 3.5837 (3.5237)  loss_scale: 32768.0000 (34400.5694)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1087  max mem: 15572
[2025-01-10 22:58:08,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=277, lr=[1.9786263683120527e-07, 1.9786263683120527e-07, 2.826609097588647e-07, 2.826609097588647e-07, 4.038012996555211e-07, 4.038012996555211e-07, 5.768589995078873e-07, 5.768589995078873e-07, 8.240842850112676e-07, 8.240842850112676e-07, 1.1772632643018108e-06, 1.1772632643018108e-06, 1.6818046632883012e-06, 1.6818046632883012e-06, 2.402578090411859e-06, 2.402578090411859e-06, 3.4322544148740844e-06, 3.4322544148740844e-06, 4.903220592677264e-06, 4.903220592677264e-06, 7.004600846681806e-06, 7.004600846681806e-06, 1.0006572638116867e-05, 1.0006572638116867e-05, 1.4295103768738381e-05, 1.4295103768738381e-05, 2.0421576812483404e-05, 2.0421576812483404e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 22:58:08,474] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=45.583255130554434, CurrSamplesPerSec=52.799828796941846, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [ 290/1404]  eta: 0:11:21  lr: 0.000020  min_lr: 0.000000  loss: 3.5704 (3.5197)  loss_scale: 32768.0000 (34344.4674)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0864  max mem: 15572
[2025-01-10 22:58:12,555] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:58:12,556] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:58:12,561] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:58:12,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 300/1404]  eta: 0:11:16  lr: 0.000020  min_lr: 0.000000  loss: 3.5479 (3.5279)  loss_scale: 32768.0000 (35380.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0730  max mem: 15572
Epoch: [29]  [ 310/1404]  eta: 0:11:11  lr: 0.000020  min_lr: 0.000000  loss: 3.5974 (3.5280)  loss_scale: 65536.0000 (36350.3537)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1211  max mem: 15572
[2025-01-10 22:58:25,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41027
[2025-01-10 22:58:25,495] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:58:25,495] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:58:25,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41027
[2025-01-10 22:58:25,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [ 320/1404]  eta: 0:11:04  lr: 0.000020  min_lr: 0.000000  loss: 3.6167 (3.5311)  loss_scale: 32768.0000 (36238.7539)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1069  max mem: 15572
Epoch: [29]  [ 330/1404]  eta: 0:10:58  lr: 0.000020  min_lr: 0.000000  loss: 3.6913 (3.5354)  loss_scale: 32768.0000 (36133.8973)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1071  max mem: 15572
Epoch: [29]  [ 340/1404]  eta: 0:10:52  lr: 0.000020  min_lr: 0.000000  loss: 3.4576 (3.5298)  loss_scale: 32768.0000 (36035.1906)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1000  max mem: 15572
Epoch: [29]  [ 350/1404]  eta: 0:10:48  lr: 0.000020  min_lr: 0.000000  loss: 3.4186 (3.5279)  loss_scale: 32768.0000 (35942.1083)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1409  max mem: 15572
Epoch: [29]  [ 360/1404]  eta: 0:10:38  lr: 0.000020  min_lr: 0.000000  loss: 3.5708 (3.5303)  loss_scale: 32768.0000 (35854.1828)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1083  max mem: 15572
Epoch: [29]  [ 370/1404]  eta: 0:10:30  lr: 0.000020  min_lr: 0.000000  loss: 3.5708 (3.5310)  loss_scale: 32768.0000 (35770.9973)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0122  max mem: 15572
Epoch: [29]  [ 380/1404]  eta: 0:10:22  lr: 0.000020  min_lr: 0.000000  loss: 3.8679 (3.5373)  loss_scale: 32768.0000 (35692.1785)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0385  max mem: 15572
Epoch: [29]  [ 390/1404]  eta: 0:10:20  lr: 0.000020  min_lr: 0.000000  loss: 3.6440 (3.5348)  loss_scale: 32768.0000 (35617.3913)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1494  max mem: 15572
Epoch: [29]  [ 400/1404]  eta: 0:10:11  lr: 0.000020  min_lr: 0.000000  loss: 3.6469 (3.5398)  loss_scale: 32768.0000 (35546.3342)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1234  max mem: 15572
Epoch: [29]  [ 410/1404]  eta: 0:10:03  lr: 0.000020  min_lr: 0.000000  loss: 3.8357 (3.5390)  loss_scale: 32768.0000 (35478.7348)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0008  max mem: 15572
Epoch: [29]  [ 420/1404]  eta: 0:09:56  lr: 0.000020  min_lr: 0.000000  loss: 3.6856 (3.5415)  loss_scale: 32768.0000 (35414.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0008  max mem: 15572
Epoch: [29]  [ 430/1404]  eta: 0:09:49  lr: 0.000020  min_lr: 0.000000  loss: 3.7778 (3.5507)  loss_scale: 32768.0000 (35352.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0342  max mem: 15572
[2025-01-10 22:59:40,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:59:40,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 22:59:40,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 22:59:40,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 440/1404]  eta: 0:09:42  lr: 0.000020  min_lr: 0.000000  loss: 3.6810 (3.5511)  loss_scale: 32768.0000 (35368.6349)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0478  max mem: 15572
[2025-01-10 22:59:45,836] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41165
[2025-01-10 22:59:45,837] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 22:59:45,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 22:59:45,968] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41165
[2025-01-10 22:59:45,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [ 450/1404]  eta: 0:09:35  lr: 0.000020  min_lr: 0.000000  loss: 3.7334 (3.5507)  loss_scale: 32768.0000 (35892.2217)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0351  max mem: 15572
Epoch: [29]  [ 460/1404]  eta: 0:09:29  lr: 0.000020  min_lr: 0.000000  loss: 3.5692 (3.5486)  loss_scale: 32768.0000 (35824.4512)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0585  max mem: 15572
Epoch: [29]  [ 470/1404]  eta: 0:09:22  lr: 0.000020  min_lr: 0.000000  loss: 3.4754 (3.5495)  loss_scale: 32768.0000 (35759.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0671  max mem: 15572
Epoch: [29]  [ 480/1404]  eta: 0:09:15  lr: 0.000020  min_lr: 0.000000  loss: 3.6496 (3.5530)  loss_scale: 32768.0000 (35697.3638)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0350  max mem: 15572
Epoch: [29]  [ 490/1404]  eta: 0:09:09  lr: 0.000020  min_lr: 0.000000  loss: 3.7095 (3.5549)  loss_scale: 32768.0000 (35637.7026)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0506  max mem: 15572
Epoch: [29]  [ 500/1404]  eta: 0:09:05  lr: 0.000020  min_lr: 0.000000  loss: 3.7095 (3.5514)  loss_scale: 32768.0000 (35580.4232)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0457  max mem: 15572
Epoch: [29]  [ 510/1404]  eta: 0:08:57  lr: 0.000020  min_lr: 0.000000  loss: 3.3458 (3.5503)  loss_scale: 32768.0000 (35525.3855)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0013  max mem: 15572
Epoch: [29]  [ 520/1404]  eta: 0:08:51  lr: 0.000020  min_lr: 0.000000  loss: 3.3334 (3.5442)  loss_scale: 32768.0000 (35472.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0012  max mem: 15572
Epoch: [29]  [ 530/1404]  eta: 0:08:44  lr: 0.000020  min_lr: 0.000000  loss: 3.5350 (3.5480)  loss_scale: 32768.0000 (35421.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0007  max mem: 15572
Epoch: [29]  [ 540/1404]  eta: 0:08:38  lr: 0.000020  min_lr: 0.000000  loss: 3.6894 (3.5488)  loss_scale: 32768.0000 (35372.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0496  max mem: 15572
Epoch: [29]  [ 550/1404]  eta: 0:08:32  lr: 0.000020  min_lr: 0.000000  loss: 3.6243 (3.5474)  loss_scale: 32768.0000 (35325.2123)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1088  max mem: 15572
Epoch: [29]  [ 560/1404]  eta: 0:08:25  lr: 0.000020  min_lr: 0.000000  loss: 3.6531 (3.5496)  loss_scale: 32768.0000 (35279.6292)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0603  max mem: 15572
Epoch: [29]  [ 570/1404]  eta: 0:08:19  lr: 0.000020  min_lr: 0.000000  loss: 3.8833 (3.5559)  loss_scale: 32768.0000 (35235.6427)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0411  max mem: 15572
[2025-01-10 23:01:01,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:01:01,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:01:01,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:01:01,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 580/1404]  eta: 0:08:15  lr: 0.000020  min_lr: 0.000000  loss: 3.8296 (3.5591)  loss_scale: 32768.0000 (35362.3683)  weight_decay: 0.0500 (0.0500)  time: 0.6884  data: 0.1105  max mem: 15572
[2025-01-10 23:01:09,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41305
[2025-01-10 23:01:09,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:01:09,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:01:09,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41305
[2025-01-10 23:01:09,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [ 590/1404]  eta: 0:08:10  lr: 0.000020  min_lr: 0.000000  loss: 3.6445 (3.5579)  loss_scale: 65536.0000 (35762.0305)  weight_decay: 0.0500 (0.0500)  time: 0.6902  data: 0.1366  max mem: 15572
Epoch: [29]  [ 600/1404]  eta: 0:08:03  lr: 0.000020  min_lr: 0.000000  loss: 3.4675 (3.5597)  loss_scale: 32768.0000 (35712.2130)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0667  max mem: 15572
Epoch: [29]  [ 610/1404]  eta: 0:07:56  lr: 0.000020  min_lr: 0.000000  loss: 3.6460 (3.5604)  loss_scale: 32768.0000 (35664.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0006  max mem: 15572
[2025-01-10 23:01:22,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41328
[2025-01-10 23:01:22,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 23:01:22,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-10 23:01:22,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41328
[2025-01-10 23:01:22,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [29]  [ 620/1404]  eta: 0:07:50  lr: 0.000020  min_lr: 0.000000  loss: 3.3749 (3.5563)  loss_scale: 32768.0000 (35379.9420)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0009  max mem: 15572
Epoch: [29]  [ 630/1404]  eta: 0:07:43  lr: 0.000020  min_lr: 0.000000  loss: 3.3749 (3.5525)  loss_scale: 16384.0000 (35078.8970)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0009  max mem: 15572
Epoch: [29]  [ 640/1404]  eta: 0:07:39  lr: 0.000020  min_lr: 0.000000  loss: 3.4460 (3.5514)  loss_scale: 16384.0000 (34787.2449)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.0972  max mem: 15572
Epoch: [29]  [ 650/1404]  eta: 0:07:32  lr: 0.000020  min_lr: 0.000000  loss: 3.6168 (3.5516)  loss_scale: 16384.0000 (34504.5530)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.1110  max mem: 15572
Epoch: [29]  [ 660/1404]  eta: 0:07:27  lr: 0.000019  min_lr: 0.000000  loss: 3.8691 (3.5579)  loss_scale: 16384.0000 (34230.4145)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1034  max mem: 15572
Epoch: [29]  [ 670/1404]  eta: 0:07:20  lr: 0.000019  min_lr: 0.000000  loss: 3.7304 (3.5558)  loss_scale: 16384.0000 (33964.4471)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0897  max mem: 15572
Epoch: [29]  [ 680/1404]  eta: 0:07:13  lr: 0.000019  min_lr: 0.000000  loss: 3.5742 (3.5573)  loss_scale: 16384.0000 (33706.2907)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0010  max mem: 15572
Epoch: [29]  [ 690/1404]  eta: 0:07:08  lr: 0.000019  min_lr: 0.000000  loss: 3.6865 (3.5597)  loss_scale: 16384.0000 (33455.6064)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0801  max mem: 15572
Epoch: [29]  [ 700/1404]  eta: 0:07:02  lr: 0.000019  min_lr: 0.000000  loss: 3.5295 (3.5583)  loss_scale: 16384.0000 (33212.0742)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1349  max mem: 15572
Epoch: [29]  [ 710/1404]  eta: 0:06:55  lr: 0.000019  min_lr: 0.000000  loss: 3.5295 (3.5580)  loss_scale: 16384.0000 (32975.3924)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0558  max mem: 15572
Epoch: [29]  [ 720/1404]  eta: 0:06:50  lr: 0.000019  min_lr: 0.000000  loss: 3.5045 (3.5562)  loss_scale: 16384.0000 (32745.2760)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0787  max mem: 15572
Epoch: [29]  [ 730/1404]  eta: 0:06:44  lr: 0.000019  min_lr: 0.000000  loss: 3.5929 (3.5595)  loss_scale: 16384.0000 (32521.4555)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.1288  max mem: 15572
Epoch: [29]  [ 740/1404]  eta: 0:06:38  lr: 0.000019  min_lr: 0.000000  loss: 3.7046 (3.5585)  loss_scale: 16384.0000 (32303.6761)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0617  max mem: 15572
[2025-01-10 23:02:39,050] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:02:39,050] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 23:02:39,132] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:02:39,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 750/1404]  eta: 0:06:32  lr: 0.000019  min_lr: 0.000000  loss: 3.4499 (3.5550)  loss_scale: 16384.0000 (32309.8589)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0877  max mem: 15572
Epoch: [29]  [ 760/1404]  eta: 0:06:25  lr: 0.000019  min_lr: 0.000000  loss: 3.4234 (3.5565)  loss_scale: 32768.0000 (32315.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0929  max mem: 15572
Epoch: [29]  [ 770/1404]  eta: 0:06:19  lr: 0.000019  min_lr: 0.000000  loss: 3.5338 (3.5563)  loss_scale: 32768.0000 (32321.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0402  max mem: 15572
Epoch: [29]  [ 780/1404]  eta: 0:06:14  lr: 0.000019  min_lr: 0.000000  loss: 3.2977 (3.5534)  loss_scale: 32768.0000 (32327.4571)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0700  max mem: 15572
Epoch: [29]  [ 790/1404]  eta: 0:06:07  lr: 0.000019  min_lr: 0.000000  loss: 3.4300 (3.5521)  loss_scale: 32768.0000 (32333.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0717  max mem: 15572
Epoch: [29]  [ 800/1404]  eta: 0:06:01  lr: 0.000019  min_lr: 0.000000  loss: 3.6433 (3.5541)  loss_scale: 32768.0000 (32338.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0693  max mem: 15572
Epoch: [29]  [ 810/1404]  eta: 0:05:55  lr: 0.000019  min_lr: 0.000000  loss: 3.6433 (3.5513)  loss_scale: 32768.0000 (32343.7534)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0822  max mem: 15572
Epoch: [29]  [ 820/1404]  eta: 0:05:48  lr: 0.000019  min_lr: 0.000000  loss: 3.3875 (3.5486)  loss_scale: 32768.0000 (32348.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0525  max mem: 15572
Epoch: [29]  [ 830/1404]  eta: 0:05:42  lr: 0.000019  min_lr: 0.000000  loss: 3.6481 (3.5500)  loss_scale: 32768.0000 (32353.9639)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0424  max mem: 15572
Epoch: [29]  [ 840/1404]  eta: 0:05:36  lr: 0.000019  min_lr: 0.000000  loss: 3.6701 (3.5488)  loss_scale: 32768.0000 (32358.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0936  max mem: 15572
Epoch: [29]  [ 850/1404]  eta: 0:05:31  lr: 0.000019  min_lr: 0.000000  loss: 3.6701 (3.5506)  loss_scale: 32768.0000 (32363.6945)  weight_decay: 0.0500 (0.0500)  time: 0.6632  data: 0.1679  max mem: 15572
Epoch: [29]  [ 860/1404]  eta: 0:05:25  lr: 0.000019  min_lr: 0.000000  loss: 3.6411 (3.5502)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1030  max mem: 15572
[2025-01-10 23:03:53,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:03:53,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:03:53,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:03:53,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 870/1404]  eta: 0:05:18  lr: 0.000019  min_lr: 0.000000  loss: 3.6338 (3.5518)  loss_scale: 32768.0000 (32448.2204)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0009  max mem: 15572
[2025-01-10 23:03:56,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41590
[2025-01-10 23:03:56,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:03:56,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:03:56,368] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41590
[2025-01-10 23:03:56,369] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [ 880/1404]  eta: 0:05:12  lr: 0.000019  min_lr: 0.000000  loss: 3.5784 (3.5519)  loss_scale: 32768.0000 (32563.4325)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0462  max mem: 15572
Epoch: [29]  [ 890/1404]  eta: 0:05:06  lr: 0.000019  min_lr: 0.000000  loss: 3.4398 (3.5491)  loss_scale: 32768.0000 (32565.7284)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0757  max mem: 15572
Epoch: [29]  [ 900/1404]  eta: 0:05:00  lr: 0.000019  min_lr: 0.000000  loss: 3.4398 (3.5485)  loss_scale: 32768.0000 (32567.9734)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0536  max mem: 15572
Epoch: [29]  [ 910/1404]  eta: 0:04:54  lr: 0.000019  min_lr: 0.000000  loss: 3.7336 (3.5503)  loss_scale: 32768.0000 (32570.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0563  max mem: 15572
Epoch: [29]  [ 920/1404]  eta: 0:04:48  lr: 0.000019  min_lr: 0.000000  loss: 3.6690 (3.5494)  loss_scale: 32768.0000 (32572.3170)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1048  max mem: 15572
Epoch: [29]  [ 930/1404]  eta: 0:04:42  lr: 0.000019  min_lr: 0.000000  loss: 3.4851 (3.5466)  loss_scale: 32768.0000 (32574.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1265  max mem: 15572
Epoch: [29]  [ 940/1404]  eta: 0:04:36  lr: 0.000019  min_lr: 0.000000  loss: 3.4358 (3.5464)  loss_scale: 32768.0000 (32576.4761)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1067  max mem: 15572
Epoch: [29]  [ 950/1404]  eta: 0:04:30  lr: 0.000019  min_lr: 0.000000  loss: 3.6717 (3.5475)  loss_scale: 32768.0000 (32578.4900)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0663  max mem: 15572
Epoch: [29]  [ 960/1404]  eta: 0:04:24  lr: 0.000019  min_lr: 0.000000  loss: 3.6717 (3.5489)  loss_scale: 32768.0000 (32580.4620)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0305  max mem: 15572
Epoch: [29]  [ 970/1404]  eta: 0:04:18  lr: 0.000019  min_lr: 0.000000  loss: 3.4774 (3.5454)  loss_scale: 32768.0000 (32582.3934)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0380  max mem: 15572
Epoch: [29]  [ 980/1404]  eta: 0:04:12  lr: 0.000019  min_lr: 0.000000  loss: 3.4657 (3.5465)  loss_scale: 32768.0000 (32584.2854)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0460  max mem: 15572
Epoch: [29]  [ 990/1404]  eta: 0:04:06  lr: 0.000019  min_lr: 0.000000  loss: 3.5826 (3.5431)  loss_scale: 32768.0000 (32586.1393)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1201  max mem: 15572
Epoch: [29]  [1000/1404]  eta: 0:04:00  lr: 0.000019  min_lr: 0.000000  loss: 3.3799 (3.5430)  loss_scale: 32768.0000 (32587.9560)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1168  max mem: 15572
[2025-01-10 23:05:12,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:05:12,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:05:12,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:05:12,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:05:13,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41721
[2025-01-10 23:05:13,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:05:13,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:05:13,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41721
[2025-01-10 23:05:13,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [1010/1404]  eta: 0:03:54  lr: 0.000019  min_lr: 0.000000  loss: 3.6183 (3.5438)  loss_scale: 32768.0000 (32654.5598)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0220  max mem: 15572
Epoch: [29]  [1020/1404]  eta: 0:03:49  lr: 0.000019  min_lr: 0.000000  loss: 3.6516 (3.5447)  loss_scale: 32768.0000 (32655.6709)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0013  max mem: 15572
Epoch: [29]  [1030/1404]  eta: 0:03:43  lr: 0.000019  min_lr: 0.000000  loss: 3.6966 (3.5453)  loss_scale: 32768.0000 (32656.7604)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0011  max mem: 15572
Epoch: [29]  [1040/1404]  eta: 0:03:37  lr: 0.000019  min_lr: 0.000000  loss: 3.6593 (3.5438)  loss_scale: 32768.0000 (32657.8290)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0009  max mem: 15572
Epoch: [29]  [1050/1404]  eta: 0:03:31  lr: 0.000019  min_lr: 0.000000  loss: 3.5142 (3.5426)  loss_scale: 32768.0000 (32658.8773)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0009  max mem: 15572
Epoch: [29]  [1060/1404]  eta: 0:03:25  lr: 0.000019  min_lr: 0.000000  loss: 3.3647 (3.5406)  loss_scale: 32768.0000 (32659.9057)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0008  max mem: 15572
Epoch: [29]  [1070/1404]  eta: 0:03:18  lr: 0.000019  min_lr: 0.000000  loss: 3.5816 (3.5414)  loss_scale: 32768.0000 (32660.9150)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0006  max mem: 15572
Epoch: [29]  [1080/1404]  eta: 0:03:13  lr: 0.000018  min_lr: 0.000000  loss: 3.7350 (3.5411)  loss_scale: 32768.0000 (32661.9056)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0005  max mem: 15572
Epoch: [29]  [1090/1404]  eta: 0:03:07  lr: 0.000018  min_lr: 0.000000  loss: 3.3762 (3.5392)  loss_scale: 32768.0000 (32662.8781)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0006  max mem: 15572
Epoch: [29]  [1100/1404]  eta: 0:03:01  lr: 0.000018  min_lr: 0.000000  loss: 3.5014 (3.5406)  loss_scale: 32768.0000 (32663.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0005  max mem: 15572
Epoch: [29]  [1110/1404]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000000  loss: 3.3505 (3.5381)  loss_scale: 32768.0000 (32664.7705)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0004  max mem: 15572
Epoch: [29]  [1120/1404]  eta: 0:02:49  lr: 0.000018  min_lr: 0.000000  loss: 3.1381 (3.5355)  loss_scale: 32768.0000 (32665.6913)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0005  max mem: 15572
Epoch: [29]  [1130/1404]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000000  loss: 3.1237 (3.5351)  loss_scale: 32768.0000 (32666.5959)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0006  max mem: 15572
[2025-01-10 23:06:29,828] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:06:29,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:06:29,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:06:29,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1140/1404]  eta: 0:02:36  lr: 0.000018  min_lr: 0.000000  loss: 3.5238 (3.5358)  loss_scale: 32768.0000 (32868.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0006  max mem: 15572
[2025-01-10 23:06:35,925] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41862
[2025-01-10 23:06:35,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:06:35,926] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41862
[2025-01-10 23:06:35,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:06:35,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1150/1404]  eta: 0:02:31  lr: 0.000018  min_lr: 0.000000  loss: 3.5238 (3.5354)  loss_scale: 65536.0000 (33009.9878)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0007  max mem: 15572
Epoch: [29]  [1160/1404]  eta: 0:02:25  lr: 0.000018  min_lr: 0.000000  loss: 3.5185 (3.5347)  loss_scale: 32768.0000 (33007.9035)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0008  max mem: 15572
Epoch: [29]  [1170/1404]  eta: 0:02:19  lr: 0.000018  min_lr: 0.000000  loss: 3.5185 (3.5344)  loss_scale: 32768.0000 (33005.8548)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0009  max mem: 15572
Epoch: [29]  [1180/1404]  eta: 0:02:12  lr: 0.000018  min_lr: 0.000000  loss: 3.5697 (3.5356)  loss_scale: 32768.0000 (33003.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5173  data: 0.0007  max mem: 15572
Epoch: [29]  [1190/1404]  eta: 0:02:06  lr: 0.000018  min_lr: 0.000000  loss: 3.7294 (3.5374)  loss_scale: 32768.0000 (33001.8606)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0062  max mem: 15572
Epoch: [29]  [1200/1404]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000000  loss: 3.7510 (3.5369)  loss_scale: 32768.0000 (32999.9134)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0065  max mem: 15572
Epoch: [29]  [1210/1404]  eta: 0:01:55  lr: 0.000018  min_lr: 0.000000  loss: 3.6914 (3.5385)  loss_scale: 32768.0000 (32997.9983)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0010  max mem: 15572
Epoch: [29]  [1220/1404]  eta: 0:01:49  lr: 0.000018  min_lr: 0.000000  loss: 3.5081 (3.5380)  loss_scale: 32768.0000 (32996.1147)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0267  max mem: 15572
Epoch: [29]  [1230/1404]  eta: 0:01:43  lr: 0.000018  min_lr: 0.000000  loss: 3.4629 (3.5383)  loss_scale: 32768.0000 (32994.2616)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0482  max mem: 15572
Epoch: [29]  [1240/1404]  eta: 0:01:37  lr: 0.000018  min_lr: 0.000000  loss: 3.6338 (3.5392)  loss_scale: 32768.0000 (32992.4384)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.0507  max mem: 15572
Epoch: [29]  [1250/1404]  eta: 0:01:31  lr: 0.000018  min_lr: 0.000000  loss: 3.6644 (3.5398)  loss_scale: 32768.0000 (32990.6443)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0467  max mem: 15572
Epoch: [29]  [1260/1404]  eta: 0:01:25  lr: 0.000018  min_lr: 0.000000  loss: 3.6148 (3.5402)  loss_scale: 32768.0000 (32988.8787)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0573  max mem: 15572
Epoch: [29]  [1270/1404]  eta: 0:01:19  lr: 0.000018  min_lr: 0.000000  loss: 3.6926 (3.5417)  loss_scale: 32768.0000 (32987.1408)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1249  max mem: 15572
[2025-01-10 23:07:52,926] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:07:52,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:07:52,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:07:52,947] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1280/1404]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 3.6981 (3.5420)  loss_scale: 32768.0000 (33138.9102)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1495  max mem: 15572
[2025-01-10 23:07:58,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=284, lr=[1.7448843457353946e-07, 1.7448843457353946e-07, 2.4926919224791356e-07, 2.4926919224791356e-07, 3.5609884606844795e-07, 3.5609884606844795e-07, 5.087126372406399e-07, 5.087126372406399e-07, 7.267323389152e-07, 7.267323389152e-07, 1.038189055593143e-06, 1.038189055593143e-06, 1.4831272222759185e-06, 1.4831272222759185e-06, 2.118753174679884e-06, 2.118753174679884e-06, 3.026790249542691e-06, 3.026790249542691e-06, 4.323986070775273e-06, 4.323986070775273e-06, 6.177122958250391e-06, 6.177122958250391e-06, 8.82446136892913e-06, 8.82446136892913e-06, 1.2606373384184473e-05, 1.2606373384184473e-05, 1.8009104834549248e-05, 1.8009104834549248e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 23:07:58,286] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=45.62706765437498, CurrSamplesPerSec=45.74550137763706, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 23:07:58,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42000
[2025-01-10 23:07:58,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:07:58,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:07:58,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42000
[2025-01-10 23:07:58,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [1290/1404]  eta: 0:01:07  lr: 0.000018  min_lr: 0.000000  loss: 3.6824 (3.5435)  loss_scale: 32768.0000 (33212.1828)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0873  max mem: 15572
Epoch: [29]  [1300/1404]  eta: 0:01:01  lr: 0.000018  min_lr: 0.000000  loss: 3.6698 (3.5436)  loss_scale: 32768.0000 (33208.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0652  max mem: 15572
Epoch: [29]  [1310/1404]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 3.3599 (3.5429)  loss_scale: 32768.0000 (33205.4066)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0828  max mem: 15572
Epoch: [29]  [1320/1404]  eta: 0:00:49  lr: 0.000018  min_lr: 0.000000  loss: 3.6868 (3.5434)  loss_scale: 32768.0000 (33202.0954)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0649  max mem: 15572
Epoch: [29]  [1330/1404]  eta: 0:00:43  lr: 0.000018  min_lr: 0.000000  loss: 3.6946 (3.5454)  loss_scale: 32768.0000 (33198.8340)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0920  max mem: 15572
Epoch: [29]  [1340/1404]  eta: 0:00:38  lr: 0.000018  min_lr: 0.000000  loss: 3.6243 (3.5464)  loss_scale: 32768.0000 (33195.6212)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1542  max mem: 15572
Epoch: [29]  [1350/1404]  eta: 0:00:32  lr: 0.000018  min_lr: 0.000000  loss: 3.5407 (3.5457)  loss_scale: 32768.0000 (33192.4560)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0864  max mem: 15572
Epoch: [29]  [1360/1404]  eta: 0:00:26  lr: 0.000018  min_lr: 0.000000  loss: 3.5272 (3.5461)  loss_scale: 32768.0000 (33189.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0006  max mem: 15572
Epoch: [29]  [1370/1404]  eta: 0:00:20  lr: 0.000018  min_lr: 0.000000  loss: 3.4170 (3.5446)  loss_scale: 32768.0000 (33186.2640)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0468  max mem: 15572
Epoch: [29]  [1380/1404]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 3.4517 (3.5449)  loss_scale: 32768.0000 (33183.2353)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1391  max mem: 15572
Epoch: [29]  [1390/1404]  eta: 0:00:08  lr: 0.000018  min_lr: 0.000000  loss: 3.6128 (3.5448)  loss_scale: 32768.0000 (33180.2502)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0930  max mem: 15572
Epoch: [29]  [1400/1404]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000000  loss: 3.6231 (3.5462)  loss_scale: 32768.0000 (33177.3076)  weight_decay: 0.0500 (0.0500)  time: 0.4400  data: 0.0004  max mem: 15572
Epoch: [29]  [1403/1404]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 3.6546 (3.5469)  loss_scale: 32768.0000 (33176.4330)  weight_decay: 0.0500 (0.0500)  time: 0.4127  data: 0.0003  max mem: 15572
Epoch: [29] Total time: 0:13:51 (0.5924 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 3.6546 (3.5686)  loss_scale: 32768.0000 (33176.4330)  weight_decay: 0.0500 (0.0500)
[2025-01-10 23:09:05,704] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-10 23:09:05,706] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-10 23:09:05,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-10 23:09:05,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
[2025-01-10 23:09:05,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-10 23:09:05,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/136]  eta: 0:07:55  loss: 1.4696 (1.4696)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 3.4950  data: 3.3115  max mem: 15572
Val:  [ 10/136]  eta: 0:01:28  loss: 2.0266 (1.9772)  acc1: 55.5556 (51.5152)  acc5: 77.7778 (80.8081)  time: 0.7029  data: 0.5234  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 2.2808 (2.1529)  acc1: 44.4444 (46.5608)  acc5: 77.7778 (78.8360)  time: 0.4073  data: 0.2204  max mem: 15572
Val:  [ 30/136]  eta: 0:00:47  loss: 2.0420 (2.0343)  acc1: 50.0000 (49.6416)  acc5: 83.3333 (79.9283)  time: 0.3101  data: 0.1093  max mem: 15572
Val:  [ 40/136]  eta: 0:00:41  loss: 1.7287 (2.0032)  acc1: 61.1111 (51.4905)  acc5: 83.3333 (80.2168)  time: 0.3081  data: 0.1005  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 1.9615 (2.0120)  acc1: 50.0000 (51.5251)  acc5: 83.3333 (80.7190)  time: 0.4084  data: 0.2050  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1469 (2.1231)  acc1: 44.4444 (48.6339)  acc5: 77.7778 (79.0528)  time: 0.4074  data: 0.2117  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.1453 (2.0897)  acc1: 50.0000 (49.3740)  acc5: 77.7778 (79.5775)  time: 0.3487  data: 0.1587  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 1.8053 (2.0793)  acc1: 50.0000 (49.6571)  acc5: 88.8889 (80.4527)  time: 0.3613  data: 0.1611  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9810 (2.0813)  acc1: 50.0000 (49.4505)  acc5: 83.3333 (80.4640)  time: 0.3698  data: 0.1613  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2318 (2.1578)  acc1: 38.8889 (47.5248)  acc5: 77.7778 (78.4929)  time: 0.3137  data: 0.1046  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2530 (2.1456)  acc1: 44.4444 (47.9479)  acc5: 77.7778 (78.7287)  time: 0.3230  data: 0.1120  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.7500 (2.0970)  acc1: 61.1111 (49.2654)  acc5: 83.3333 (79.4766)  time: 0.3546  data: 0.1337  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5820 (2.0561)  acc1: 66.6667 (50.1272)  acc5: 88.8889 (80.1103)  time: 0.3037  data: 0.1084  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.5851 (2.0517)  acc1: 55.5556 (50.4095)  acc5: 88.8889 (80.2211)  time: 0.2299  data: 0.0589  max mem: 15572
Val: Total time: 0:00:49 (0.3668 s / it)
* Acc@1 49.672 Acc@5 79.054 loss 2.098
Accuracy of the network on the 4883 val videos: 49.7%
[2025-01-10 23:09:56,389] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 23:09:56,391] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 23:09:56,391] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 23:09:56,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 23:09:58,854] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 23:09:58,855] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 49.67%
Epoch: [30]  [   0/1404]  eta: 2:48:31  lr: 0.000018  min_lr: 0.000000  loss: 3.9627 (3.9627)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2019  data: 6.6128  max mem: 15572
[2025-01-10 23:10:11,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:10:11,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:10:11,095] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:10:11,095] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [  10/1404]  eta: 0:26:56  lr: 0.000018  min_lr: 0.000000  loss: 3.5095 (3.4189)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.1597  data: 0.6018  max mem: 15572
Epoch: [30]  [  20/1404]  eta: 0:21:16  lr: 0.000018  min_lr: 0.000000  loss: 3.6549 (3.6613)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0193  max mem: 15572
[2025-01-10 23:10:19,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42142
[2025-01-10 23:10:19,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:10:19,266] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42142
[2025-01-10 23:10:19,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:10:19,266] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [  30/1404]  eta: 0:18:41  lr: 0.000018  min_lr: 0.000000  loss: 3.7552 (3.6273)  loss_scale: 65536.0000 (46509.4194)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0194  max mem: 15572
Epoch: [30]  [  40/1404]  eta: 0:17:22  lr: 0.000018  min_lr: 0.000000  loss: 3.4656 (3.6231)  loss_scale: 32768.0000 (43157.8537)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0008  max mem: 15572
Epoch: [30]  [  50/1404]  eta: 0:16:25  lr: 0.000018  min_lr: 0.000000  loss: 3.4656 (3.6156)  loss_scale: 32768.0000 (41120.6275)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0006  max mem: 15572
Epoch: [30]  [  60/1404]  eta: 0:15:48  lr: 0.000018  min_lr: 0.000000  loss: 3.6765 (3.6418)  loss_scale: 32768.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0007  max mem: 15572
Epoch: [30]  [  70/1404]  eta: 0:15:19  lr: 0.000018  min_lr: 0.000000  loss: 3.8811 (3.6524)  loss_scale: 32768.0000 (38767.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0007  max mem: 15572
Epoch: [30]  [  80/1404]  eta: 0:14:52  lr: 0.000018  min_lr: 0.000000  loss: 4.0024 (3.6701)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0007  max mem: 15572
Epoch: [30]  [  90/1404]  eta: 0:14:27  lr: 0.000018  min_lr: 0.000000  loss: 3.7805 (3.6571)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0007  max mem: 15572
Epoch: [30]  [ 100/1404]  eta: 0:14:15  lr: 0.000017  min_lr: 0.000000  loss: 3.6180 (3.6565)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0373  max mem: 15572
Epoch: [30]  [ 110/1404]  eta: 0:14:02  lr: 0.000017  min_lr: 0.000000  loss: 3.6180 (3.6499)  loss_scale: 32768.0000 (36605.6937)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0945  max mem: 15572
Epoch: [30]  [ 120/1404]  eta: 0:13:52  lr: 0.000017  min_lr: 0.000000  loss: 3.6673 (3.6437)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1218  max mem: 15572
Epoch: [30]  [ 130/1404]  eta: 0:13:48  lr: 0.000017  min_lr: 0.000000  loss: 3.5900 (3.6261)  loss_scale: 32768.0000 (36019.7863)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1626  max mem: 15572
Epoch: [30]  [ 140/1404]  eta: 0:13:37  lr: 0.000017  min_lr: 0.000000  loss: 3.6712 (3.6429)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1558  max mem: 15572
Epoch: [30]  [ 150/1404]  eta: 0:13:34  lr: 0.000017  min_lr: 0.000000  loss: 3.6820 (3.6213)  loss_scale: 32768.0000 (35589.0861)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1575  max mem: 15572
[2025-01-10 23:11:37,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:11:37,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:11:37,462] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:11:37,462] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:11:38,405] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42273
[2025-01-10 23:11:38,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:11:38,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42273
[2025-01-10 23:11:38,416] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:11:38,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 160/1404]  eta: 0:13:20  lr: 0.000017  min_lr: 0.000000  loss: 3.3585 (3.6111)  loss_scale: 32768.0000 (35820.9193)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1298  max mem: 15572
Epoch: [30]  [ 170/1404]  eta: 0:13:09  lr: 0.000017  min_lr: 0.000000  loss: 3.3585 (3.5989)  loss_scale: 32768.0000 (35642.3860)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0612  max mem: 15572
Epoch: [30]  [ 180/1404]  eta: 0:12:57  lr: 0.000017  min_lr: 0.000000  loss: 3.2874 (3.5908)  loss_scale: 32768.0000 (35483.5801)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.0570  max mem: 15572
Epoch: [30]  [ 190/1404]  eta: 0:12:47  lr: 0.000017  min_lr: 0.000000  loss: 3.6367 (3.5852)  loss_scale: 32768.0000 (35341.4031)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0663  max mem: 15572
Epoch: [30]  [ 200/1404]  eta: 0:12:41  lr: 0.000017  min_lr: 0.000000  loss: 3.6682 (3.5876)  loss_scale: 32768.0000 (35213.3731)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1145  max mem: 15572
Epoch: [30]  [ 210/1404]  eta: 0:12:28  lr: 0.000017  min_lr: 0.000000  loss: 3.6891 (3.5963)  loss_scale: 32768.0000 (35097.4787)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0740  max mem: 15572
Epoch: [30]  [ 220/1404]  eta: 0:12:19  lr: 0.000017  min_lr: 0.000000  loss: 3.5787 (3.5912)  loss_scale: 32768.0000 (34992.0724)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0007  max mem: 15572
Epoch: [30]  [ 230/1404]  eta: 0:12:10  lr: 0.000017  min_lr: 0.000000  loss: 3.3762 (3.5888)  loss_scale: 32768.0000 (34895.7922)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0107  max mem: 15572
Epoch: [30]  [ 240/1404]  eta: 0:12:03  lr: 0.000017  min_lr: 0.000000  loss: 3.3846 (3.5753)  loss_scale: 32768.0000 (34807.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0411  max mem: 15572
Epoch: [30]  [ 250/1404]  eta: 0:11:54  lr: 0.000017  min_lr: 0.000000  loss: 3.4854 (3.5724)  loss_scale: 32768.0000 (34726.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0477  max mem: 15572
Epoch: [30]  [ 260/1404]  eta: 0:11:49  lr: 0.000017  min_lr: 0.000000  loss: 3.4982 (3.5707)  loss_scale: 32768.0000 (34651.2184)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0733  max mem: 15572
Epoch: [30]  [ 270/1404]  eta: 0:11:38  lr: 0.000017  min_lr: 0.000000  loss: 3.3255 (3.5681)  loss_scale: 32768.0000 (34581.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0569  max mem: 15572
Epoch: [30]  [ 280/1404]  eta: 0:11:31  lr: 0.000017  min_lr: 0.000000  loss: 3.4668 (3.5753)  loss_scale: 32768.0000 (34517.1815)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0465  max mem: 15572
[2025-01-10 23:12:52,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:12:52,946] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:12:52,948] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:12:52,948] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:12:53,425] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42403
[2025-01-10 23:12:53,425] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:12:53,425] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:12:53,433] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42403
[2025-01-10 23:12:53,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [ 290/1404]  eta: 0:11:27  lr: 0.000017  min_lr: 0.000000  loss: 3.8038 (3.5797)  loss_scale: 32768.0000 (34569.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1290  max mem: 15572
Epoch: [30]  [ 300/1404]  eta: 0:11:19  lr: 0.000017  min_lr: 0.000000  loss: 3.5435 (3.5751)  loss_scale: 32768.0000 (34509.8206)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1103  max mem: 15572
Epoch: [30]  [ 310/1404]  eta: 0:11:14  lr: 0.000017  min_lr: 0.000000  loss: 3.5272 (3.5749)  loss_scale: 32768.0000 (34453.8135)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1123  max mem: 15572
Epoch: [30]  [ 320/1404]  eta: 0:11:08  lr: 0.000017  min_lr: 0.000000  loss: 3.5325 (3.5706)  loss_scale: 32768.0000 (34401.2960)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1314  max mem: 15572
Epoch: [30]  [ 330/1404]  eta: 0:10:58  lr: 0.000017  min_lr: 0.000000  loss: 3.6073 (3.5729)  loss_scale: 32768.0000 (34351.9517)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0470  max mem: 15572
Epoch: [30]  [ 340/1404]  eta: 0:10:51  lr: 0.000017  min_lr: 0.000000  loss: 3.6650 (3.5646)  loss_scale: 32768.0000 (34305.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0461  max mem: 15572
Epoch: [30]  [ 350/1404]  eta: 0:10:47  lr: 0.000017  min_lr: 0.000000  loss: 3.6377 (3.5682)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1305  max mem: 15572
Epoch: [30]  [ 360/1404]  eta: 0:10:38  lr: 0.000017  min_lr: 0.000000  loss: 3.5942 (3.5706)  loss_scale: 32768.0000 (34220.3213)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0995  max mem: 15572
Epoch: [30]  [ 370/1404]  eta: 0:10:33  lr: 0.000017  min_lr: 0.000000  loss: 3.5942 (3.5692)  loss_scale: 32768.0000 (34181.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0906  max mem: 15572
Epoch: [30]  [ 380/1404]  eta: 0:10:24  lr: 0.000017  min_lr: 0.000000  loss: 3.6502 (3.5736)  loss_scale: 32768.0000 (34144.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0871  max mem: 15572
Epoch: [30]  [ 390/1404]  eta: 0:10:18  lr: 0.000017  min_lr: 0.000000  loss: 3.5263 (3.5716)  loss_scale: 32768.0000 (34108.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0668  max mem: 15572
Epoch: [30]  [ 400/1404]  eta: 0:10:12  lr: 0.000017  min_lr: 0.000000  loss: 3.5861 (3.5757)  loss_scale: 32768.0000 (34075.4514)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0984  max mem: 15572
Epoch: [30]  [ 410/1404]  eta: 0:10:06  lr: 0.000017  min_lr: 0.000000  loss: 3.5861 (3.5752)  loss_scale: 32768.0000 (34043.6399)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1215  max mem: 15572
[2025-01-10 23:14:10,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:14:10,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:14:10,863] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:14:10,864] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:14:11,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42534
[2025-01-10 23:14:11,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42534
[2025-01-10 23:14:11,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:14:11,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:14:11,767] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 420/1404]  eta: 0:09:59  lr: 0.000017  min_lr: 0.000000  loss: 3.7031 (3.5784)  loss_scale: 32768.0000 (34169.0071)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1275  max mem: 15572
Epoch: [30]  [ 430/1404]  eta: 0:09:53  lr: 0.000017  min_lr: 0.000000  loss: 3.7639 (3.5783)  loss_scale: 32768.0000 (34136.5012)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0899  max mem: 15572
Epoch: [30]  [ 440/1404]  eta: 0:09:46  lr: 0.000017  min_lr: 0.000000  loss: 3.2596 (3.5686)  loss_scale: 32768.0000 (34105.4694)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0839  max mem: 15572
Epoch: [30]  [ 450/1404]  eta: 0:09:39  lr: 0.000017  min_lr: 0.000000  loss: 3.2596 (3.5697)  loss_scale: 32768.0000 (34075.8137)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0629  max mem: 15572
Epoch: [30]  [ 460/1404]  eta: 0:09:32  lr: 0.000017  min_lr: 0.000000  loss: 3.4858 (3.5658)  loss_scale: 32768.0000 (34047.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0469  max mem: 15572
Epoch: [30]  [ 470/1404]  eta: 0:09:26  lr: 0.000017  min_lr: 0.000000  loss: 3.4658 (3.5678)  loss_scale: 32768.0000 (34020.2803)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0698  max mem: 15572
Epoch: [30]  [ 480/1404]  eta: 0:09:21  lr: 0.000017  min_lr: 0.000000  loss: 3.7253 (3.5702)  loss_scale: 32768.0000 (33994.2453)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1024  max mem: 15572
Epoch: [30]  [ 490/1404]  eta: 0:09:13  lr: 0.000017  min_lr: 0.000000  loss: 3.4508 (3.5644)  loss_scale: 32768.0000 (33969.2709)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0747  max mem: 15572
Epoch: [30]  [ 500/1404]  eta: 0:09:07  lr: 0.000017  min_lr: 0.000000  loss: 3.4508 (3.5650)  loss_scale: 32768.0000 (33945.2934)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0489  max mem: 15572
Epoch: [30]  [ 510/1404]  eta: 0:09:01  lr: 0.000017  min_lr: 0.000000  loss: 3.4860 (3.5641)  loss_scale: 32768.0000 (33922.2544)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0889  max mem: 15572
Epoch: [30]  [ 520/1404]  eta: 0:08:53  lr: 0.000017  min_lr: 0.000000  loss: 3.7703 (3.5671)  loss_scale: 32768.0000 (33900.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0557  max mem: 15572
Epoch: [30]  [ 530/1404]  eta: 0:08:46  lr: 0.000017  min_lr: 0.000000  loss: 3.5983 (3.5673)  loss_scale: 32768.0000 (33878.7797)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0166  max mem: 15572
Epoch: [30]  [ 540/1404]  eta: 0:08:39  lr: 0.000016  min_lr: 0.000000  loss: 3.5274 (3.5651)  loss_scale: 32768.0000 (33858.2477)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0656  max mem: 15572
[2025-01-10 23:15:26,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:15:26,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:15:26,176] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:15:26,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:15:28,146] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42667
[2025-01-10 23:15:28,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:15:28,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42667
[2025-01-10 23:15:28,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:15:28,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 550/1404]  eta: 0:08:32  lr: 0.000016  min_lr: 0.000000  loss: 3.4145 (3.5608)  loss_scale: 32768.0000 (34076.3412)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0626  max mem: 15572
Epoch: [30]  [ 560/1404]  eta: 0:08:25  lr: 0.000016  min_lr: 0.000000  loss: 3.4254 (3.5624)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0517  max mem: 15572
Epoch: [30]  [ 570/1404]  eta: 0:08:22  lr: 0.000016  min_lr: 0.000000  loss: 3.4582 (3.5604)  loss_scale: 32768.0000 (34030.5149)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.1480  max mem: 15572
Epoch: [30]  [ 580/1404]  eta: 0:08:16  lr: 0.000016  min_lr: 0.000000  loss: 3.4101 (3.5580)  loss_scale: 32768.0000 (34008.7849)  weight_decay: 0.0500 (0.0500)  time: 0.6752  data: 0.1378  max mem: 15572
Epoch: [30]  [ 590/1404]  eta: 0:08:09  lr: 0.000016  min_lr: 0.000000  loss: 3.3511 (3.5546)  loss_scale: 32768.0000 (33987.7902)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0543  max mem: 15572
Epoch: [30]  [ 600/1404]  eta: 0:08:04  lr: 0.000016  min_lr: 0.000000  loss: 3.5689 (3.5562)  loss_scale: 32768.0000 (33967.4942)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0941  max mem: 15572
Epoch: [30]  [ 610/1404]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 3.5092 (3.5528)  loss_scale: 32768.0000 (33947.8625)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1532  max mem: 15572
Epoch: [30]  [ 620/1404]  eta: 0:07:51  lr: 0.000016  min_lr: 0.000000  loss: 3.6544 (3.5568)  loss_scale: 32768.0000 (33928.8631)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0988  max mem: 15572
Epoch: [30]  [ 630/1404]  eta: 0:07:45  lr: 0.000016  min_lr: 0.000000  loss: 3.6736 (3.5575)  loss_scale: 32768.0000 (33910.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0738  max mem: 15572
Epoch: [30]  [ 640/1404]  eta: 0:07:39  lr: 0.000016  min_lr: 0.000000  loss: 3.4940 (3.5564)  loss_scale: 32768.0000 (33892.6427)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0607  max mem: 15572
Epoch: [30]  [ 650/1404]  eta: 0:07:32  lr: 0.000016  min_lr: 0.000000  loss: 3.3912 (3.5534)  loss_scale: 32768.0000 (33875.3671)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0006  max mem: 15572
Epoch: [30]  [ 660/1404]  eta: 0:07:27  lr: 0.000016  min_lr: 0.000000  loss: 3.3912 (3.5501)  loss_scale: 32768.0000 (33858.6142)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0834  max mem: 15572
Epoch: [30]  [ 670/1404]  eta: 0:07:21  lr: 0.000016  min_lr: 0.000000  loss: 3.5833 (3.5514)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.1168  max mem: 15572
[2025-01-10 23:16:45,617] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:16:45,617] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:16:45,618] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:16:45,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:16:46,056] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42797
[2025-01-10 23:16:46,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:16:46,056] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:16:46,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42797
[2025-01-10 23:16:46,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [ 680/1404]  eta: 0:07:14  lr: 0.000016  min_lr: 0.000000  loss: 3.6817 (3.5537)  loss_scale: 32768.0000 (33874.7019)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0606  max mem: 15572
Epoch: [30]  [ 690/1404]  eta: 0:07:08  lr: 0.000016  min_lr: 0.000000  loss: 3.7901 (3.5538)  loss_scale: 32768.0000 (33858.6860)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0542  max mem: 15572
Epoch: [30]  [ 700/1404]  eta: 0:07:02  lr: 0.000016  min_lr: 0.000000  loss: 3.7328 (3.5559)  loss_scale: 32768.0000 (33843.1270)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0728  max mem: 15572
Epoch: [30]  [ 710/1404]  eta: 0:06:55  lr: 0.000016  min_lr: 0.000000  loss: 3.7355 (3.5573)  loss_scale: 32768.0000 (33828.0056)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0459  max mem: 15572
Epoch: [30]  [ 720/1404]  eta: 0:06:49  lr: 0.000016  min_lr: 0.000000  loss: 3.7355 (3.5592)  loss_scale: 32768.0000 (33813.3037)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0010  max mem: 15572
Epoch: [30]  [ 730/1404]  eta: 0:06:44  lr: 0.000016  min_lr: 0.000000  loss: 3.6877 (3.5611)  loss_scale: 32768.0000 (33799.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.0012  max mem: 15572
Epoch: [30]  [ 740/1404]  eta: 0:06:37  lr: 0.000016  min_lr: 0.000000  loss: 3.6134 (3.5608)  loss_scale: 32768.0000 (33785.0904)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0011  max mem: 15572
Epoch: [30]  [ 750/1404]  eta: 0:06:32  lr: 0.000016  min_lr: 0.000000  loss: 3.5439 (3.5613)  loss_scale: 32768.0000 (33771.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0659  max mem: 15572
Epoch: [30]  [ 760/1404]  eta: 0:06:25  lr: 0.000016  min_lr: 0.000000  loss: 3.6420 (3.5618)  loss_scale: 32768.0000 (33758.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.0659  max mem: 15572
Epoch: [30]  [ 770/1404]  eta: 0:06:20  lr: 0.000016  min_lr: 0.000000  loss: 3.7135 (3.5618)  loss_scale: 32768.0000 (33745.5149)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0834  max mem: 15572
Epoch: [30]  [ 780/1404]  eta: 0:06:13  lr: 0.000016  min_lr: 0.000000  loss: 3.4710 (3.5593)  loss_scale: 32768.0000 (33732.9987)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0929  max mem: 15572
Epoch: [30]  [ 790/1404]  eta: 0:06:07  lr: 0.000016  min_lr: 0.000000  loss: 3.5292 (3.5608)  loss_scale: 32768.0000 (33720.7990)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0652  max mem: 15572
Epoch: [30]  [ 800/1404]  eta: 0:06:01  lr: 0.000016  min_lr: 0.000000  loss: 3.6243 (3.5613)  loss_scale: 32768.0000 (33708.9039)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1010  max mem: 15572
[2025-01-10 23:18:01,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:18:01,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:18:01,993] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:18:01,993] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 810/1404]  eta: 0:05:55  lr: 0.000016  min_lr: 0.000000  loss: 3.6286 (3.5619)  loss_scale: 32768.0000 (33899.3243)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0463  max mem: 15572
[2025-01-10 23:18:10,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42940
[2025-01-10 23:18:10,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:18:10,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:18:10,127] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42940
[2025-01-10 23:18:10,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [ 820/1404]  eta: 0:05:49  lr: 0.000016  min_lr: 0.000000  loss: 3.3918 (3.5581)  loss_scale: 65536.0000 (34244.7552)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0588  max mem: 15572
Epoch: [30]  [ 830/1404]  eta: 0:05:42  lr: 0.000016  min_lr: 0.000000  loss: 3.1585 (3.5555)  loss_scale: 32768.0000 (34226.9844)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1043  max mem: 15572
Epoch: [30]  [ 840/1404]  eta: 0:05:36  lr: 0.000016  min_lr: 0.000000  loss: 3.2541 (3.5535)  loss_scale: 32768.0000 (34209.6361)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0531  max mem: 15572
Epoch: [30]  [ 850/1404]  eta: 0:05:31  lr: 0.000016  min_lr: 0.000000  loss: 3.4214 (3.5542)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1142  max mem: 15572
Epoch: [30]  [ 860/1404]  eta: 0:05:25  lr: 0.000016  min_lr: 0.000000  loss: 3.6066 (3.5543)  loss_scale: 32768.0000 (34176.1487)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1872  max mem: 15572
Epoch: [30]  [ 870/1404]  eta: 0:05:19  lr: 0.000016  min_lr: 0.000000  loss: 3.6344 (3.5548)  loss_scale: 32768.0000 (34159.9816)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1113  max mem: 15572
[2025-01-10 23:18:45,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=292, lr=[1.5225881122300058e-07, 1.5225881122300058e-07, 2.1751258746142944e-07, 2.1751258746142944e-07, 3.107322678020421e-07, 3.107322678020421e-07, 4.43903239717203e-07, 4.43903239717203e-07, 6.341474853102901e-07, 6.341474853102901e-07, 9.059249790147001e-07, 9.059249790147001e-07, 1.2941785414495716e-06, 1.2941785414495716e-06, 1.8488264877851026e-06, 1.8488264877851026e-06, 2.6411806968358607e-06, 2.6411806968358607e-06, 3.773115281194087e-06, 3.773115281194087e-06, 5.390164687420124e-06, 5.390164687420124e-06, 7.700235267743036e-06, 7.700235267743036e-06, 1.1000336096775766e-05, 1.1000336096775766e-05, 1.571476585253681e-05, 1.571476585253681e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 23:18:45,085] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=45.70668305827195, CurrSamplesPerSec=47.97693979486788, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [ 880/1404]  eta: 0:05:13  lr: 0.000016  min_lr: 0.000000  loss: 3.7311 (3.5548)  loss_scale: 32768.0000 (34144.1816)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0665  max mem: 15572
Epoch: [30]  [ 890/1404]  eta: 0:05:07  lr: 0.000016  min_lr: 0.000000  loss: 3.5430 (3.5548)  loss_scale: 32768.0000 (34128.7363)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1045  max mem: 15572
Epoch: [30]  [ 900/1404]  eta: 0:05:01  lr: 0.000016  min_lr: 0.000000  loss: 3.6788 (3.5552)  loss_scale: 32768.0000 (34113.6337)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1085  max mem: 15572
Epoch: [30]  [ 910/1404]  eta: 0:04:55  lr: 0.000016  min_lr: 0.000000  loss: 3.8922 (3.5593)  loss_scale: 32768.0000 (34098.8628)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0735  max mem: 15572
Epoch: [30]  [ 920/1404]  eta: 0:04:49  lr: 0.000016  min_lr: 0.000000  loss: 4.0052 (3.5610)  loss_scale: 32768.0000 (34084.4126)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0866  max mem: 15572
Epoch: [30]  [ 930/1404]  eta: 0:04:43  lr: 0.000016  min_lr: 0.000000  loss: 3.6154 (3.5624)  loss_scale: 32768.0000 (34070.2728)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.0527  max mem: 15572
Epoch: [30]  [ 940/1404]  eta: 0:04:37  lr: 0.000016  min_lr: 0.000000  loss: 3.4978 (3.5612)  loss_scale: 32768.0000 (34056.4336)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0632  max mem: 15572
[2025-01-10 23:19:26,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:19:26,732] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:19:26,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:19:26,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [ 950/1404]  eta: 0:04:31  lr: 0.000016  min_lr: 0.000000  loss: 3.6303 (3.5618)  loss_scale: 32768.0000 (34111.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.0631  max mem: 15572
[2025-01-10 23:19:29,855] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43075
[2025-01-10 23:19:29,855] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:19:29,876] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43075
[2025-01-10 23:19:29,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:19:29,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 960/1404]  eta: 0:04:25  lr: 0.000016  min_lr: 0.000000  loss: 3.7307 (3.5640)  loss_scale: 32768.0000 (34234.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0503  max mem: 15572
Epoch: [30]  [ 970/1404]  eta: 0:04:19  lr: 0.000016  min_lr: 0.000000  loss: 3.7739 (3.5647)  loss_scale: 32768.0000 (34219.1061)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0895  max mem: 15572
Epoch: [30]  [ 980/1404]  eta: 0:04:13  lr: 0.000015  min_lr: 0.000000  loss: 3.6681 (3.5651)  loss_scale: 32768.0000 (34204.3140)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1154  max mem: 15572
Epoch: [30]  [ 990/1404]  eta: 0:04:07  lr: 0.000015  min_lr: 0.000000  loss: 3.9080 (3.5678)  loss_scale: 32768.0000 (34189.8204)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0762  max mem: 15572
Epoch: [30]  [1000/1404]  eta: 0:04:01  lr: 0.000015  min_lr: 0.000000  loss: 3.6521 (3.5666)  loss_scale: 32768.0000 (34175.6164)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0673  max mem: 15572
Epoch: [30]  [1010/1404]  eta: 0:03:55  lr: 0.000015  min_lr: 0.000000  loss: 3.3915 (3.5624)  loss_scale: 32768.0000 (34161.6934)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1133  max mem: 15572
Epoch: [30]  [1020/1404]  eta: 0:03:48  lr: 0.000015  min_lr: 0.000000  loss: 3.4823 (3.5634)  loss_scale: 32768.0000 (34148.0431)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0467  max mem: 15572
Epoch: [30]  [1030/1404]  eta: 0:03:42  lr: 0.000015  min_lr: 0.000000  loss: 3.6113 (3.5632)  loss_scale: 32768.0000 (34134.6576)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0216  max mem: 15572
Epoch: [30]  [1040/1404]  eta: 0:03:37  lr: 0.000015  min_lr: 0.000000  loss: 3.4930 (3.5606)  loss_scale: 32768.0000 (34121.5293)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1162  max mem: 15572
Epoch: [30]  [1050/1404]  eta: 0:03:31  lr: 0.000015  min_lr: 0.000000  loss: 3.4490 (3.5578)  loss_scale: 32768.0000 (34108.6508)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1525  max mem: 15572
Epoch: [30]  [1060/1404]  eta: 0:03:25  lr: 0.000015  min_lr: 0.000000  loss: 3.5504 (3.5613)  loss_scale: 32768.0000 (34096.0151)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1189  max mem: 15572
Epoch: [30]  [1070/1404]  eta: 0:03:19  lr: 0.000015  min_lr: 0.000000  loss: 3.8569 (3.5633)  loss_scale: 32768.0000 (34083.6153)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1137  max mem: 15572
Epoch: [30]  [1080/1404]  eta: 0:03:13  lr: 0.000015  min_lr: 0.000000  loss: 3.8427 (3.5657)  loss_scale: 32768.0000 (34071.4450)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0950  max mem: 15572
[2025-01-10 23:20:45,732] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:20:45,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:20:45,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:20:45,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1090/1404]  eta: 0:03:07  lr: 0.000015  min_lr: 0.000000  loss: 3.8735 (3.5664)  loss_scale: 32768.0000 (34269.7415)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1102  max mem: 15572
[2025-01-10 23:20:51,868] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43214
[2025-01-10 23:20:51,868] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43214
[2025-01-10 23:20:51,868] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:20:51,868] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:20:51,868] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1100/1404]  eta: 0:03:01  lr: 0.000015  min_lr: 0.000000  loss: 3.7062 (3.5677)  loss_scale: 32768.0000 (34345.3878)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1508  max mem: 15572
Epoch: [30]  [1110/1404]  eta: 0:02:55  lr: 0.000015  min_lr: 0.000000  loss: 3.4395 (3.5673)  loss_scale: 32768.0000 (34331.1899)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1488  max mem: 15572
Epoch: [30]  [1120/1404]  eta: 0:02:49  lr: 0.000015  min_lr: 0.000000  loss: 3.5844 (3.5679)  loss_scale: 32768.0000 (34317.2453)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0659  max mem: 15572
Epoch: [30]  [1130/1404]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000000  loss: 3.6199 (3.5693)  loss_scale: 32768.0000 (34303.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0009  max mem: 15572
Epoch: [30]  [1140/1404]  eta: 0:02:37  lr: 0.000015  min_lr: 0.000000  loss: 3.7124 (3.5699)  loss_scale: 32768.0000 (34290.0894)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0010  max mem: 15572
Epoch: [30]  [1150/1404]  eta: 0:02:31  lr: 0.000015  min_lr: 0.000000  loss: 3.5846 (3.5701)  loss_scale: 32768.0000 (34276.8653)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0240  max mem: 15572
Epoch: [30]  [1160/1404]  eta: 0:02:25  lr: 0.000015  min_lr: 0.000000  loss: 3.6140 (3.5701)  loss_scale: 32768.0000 (34263.8691)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0597  max mem: 15572
Epoch: [30]  [1170/1404]  eta: 0:02:19  lr: 0.000015  min_lr: 0.000000  loss: 3.6809 (3.5714)  loss_scale: 32768.0000 (34251.0948)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0421  max mem: 15572
Epoch: [30]  [1180/1404]  eta: 0:02:13  lr: 0.000015  min_lr: 0.000000  loss: 3.5775 (3.5710)  loss_scale: 32768.0000 (34238.5368)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0670  max mem: 15572
Epoch: [30]  [1190/1404]  eta: 0:02:07  lr: 0.000015  min_lr: 0.000000  loss: 3.5435 (3.5690)  loss_scale: 32768.0000 (34226.1898)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0613  max mem: 15572
Epoch: [30]  [1200/1404]  eta: 0:02:01  lr: 0.000015  min_lr: 0.000000  loss: 3.3853 (3.5689)  loss_scale: 32768.0000 (34214.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0291  max mem: 15572
Epoch: [30]  [1210/1404]  eta: 0:01:55  lr: 0.000015  min_lr: 0.000000  loss: 3.6745 (3.5691)  loss_scale: 32768.0000 (34202.1073)  weight_decay: 0.0500 (0.0500)  time: 0.6845  data: 0.0290  max mem: 15572
Epoch: [30]  [1220/1404]  eta: 0:01:49  lr: 0.000015  min_lr: 0.000000  loss: 3.5888 (3.5698)  loss_scale: 32768.0000 (34190.3620)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.0006  max mem: 15572
[2025-01-10 23:22:08,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:22:08,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:22:08,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:22:08,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1230/1404]  eta: 0:01:43  lr: 0.000015  min_lr: 0.000000  loss: 3.6397 (3.5709)  loss_scale: 32768.0000 (34391.7595)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0007  max mem: 15572
[2025-01-10 23:22:19,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43360
[2025-01-10 23:22:19,060] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:22:19,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43360
[2025-01-10 23:22:19,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:22:19,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1240/1404]  eta: 0:01:37  lr: 0.000015  min_lr: 0.000000  loss: 3.6397 (3.5697)  loss_scale: 65536.0000 (34616.3159)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0007  max mem: 15572
Epoch: [30]  [1250/1404]  eta: 0:01:31  lr: 0.000015  min_lr: 0.000000  loss: 3.3265 (3.5691)  loss_scale: 32768.0000 (34601.5412)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0008  max mem: 15572
Epoch: [30]  [1260/1404]  eta: 0:01:25  lr: 0.000015  min_lr: 0.000000  loss: 3.7163 (3.5701)  loss_scale: 32768.0000 (34587.0008)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0007  max mem: 15572
Epoch: [30]  [1270/1404]  eta: 0:01:19  lr: 0.000015  min_lr: 0.000000  loss: 3.8041 (3.5715)  loss_scale: 32768.0000 (34572.6892)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0007  max mem: 15572
Epoch: [30]  [1280/1404]  eta: 0:01:13  lr: 0.000015  min_lr: 0.000000  loss: 3.7431 (3.5719)  loss_scale: 32768.0000 (34558.6011)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0007  max mem: 15572
Epoch: [30]  [1290/1404]  eta: 0:01:07  lr: 0.000015  min_lr: 0.000000  loss: 3.6578 (3.5728)  loss_scale: 32768.0000 (34544.7312)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0007  max mem: 15572
Epoch: [30]  [1300/1404]  eta: 0:01:01  lr: 0.000015  min_lr: 0.000000  loss: 3.7720 (3.5735)  loss_scale: 32768.0000 (34531.0746)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.0474  max mem: 15572
Epoch: [30]  [1310/1404]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 3.6767 (3.5745)  loss_scale: 32768.0000 (34517.6262)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.0955  max mem: 15572
Epoch: [30]  [1320/1404]  eta: 0:00:49  lr: 0.000015  min_lr: 0.000000  loss: 3.6366 (3.5756)  loss_scale: 32768.0000 (34504.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0626  max mem: 15572
Epoch: [30]  [1330/1404]  eta: 0:00:43  lr: 0.000015  min_lr: 0.000000  loss: 3.5825 (3.5765)  loss_scale: 32768.0000 (34491.3358)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0471  max mem: 15572
Epoch: [30]  [1340/1404]  eta: 0:00:38  lr: 0.000015  min_lr: 0.000000  loss: 3.8639 (3.5766)  loss_scale: 32768.0000 (34478.4847)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1348  max mem: 15572
Epoch: [30]  [1350/1404]  eta: 0:00:32  lr: 0.000015  min_lr: 0.000000  loss: 3.7399 (3.5763)  loss_scale: 32768.0000 (34465.8238)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1021  max mem: 15572
Epoch: [30]  [1360/1404]  eta: 0:00:26  lr: 0.000015  min_lr: 0.000000  loss: 3.4928 (3.5750)  loss_scale: 32768.0000 (34453.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0964  max mem: 15572
[2025-01-10 23:23:33,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:23:33,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:23:33,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:23:33,229] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1370/1404]  eta: 0:00:20  lr: 0.000015  min_lr: 0.000000  loss: 3.3341 (3.5749)  loss_scale: 32768.0000 (34488.8578)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1147  max mem: 15572
[2025-01-10 23:23:34,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43492
[2025-01-10 23:23:34,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:23:34,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:23:34,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43492
[2025-01-10 23:23:34,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [1380/1404]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 3.2751 (3.5733)  loss_scale: 32768.0000 (34500.1245)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0762  max mem: 15572
Epoch: [30]  [1390/1404]  eta: 0:00:08  lr: 0.000015  min_lr: 0.000000  loss: 3.4606 (3.5728)  loss_scale: 32768.0000 (34487.6722)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0582  max mem: 15572
Epoch: [30]  [1400/1404]  eta: 0:00:02  lr: 0.000015  min_lr: 0.000000  loss: 3.5717 (3.5728)  loss_scale: 32768.0000 (34475.3976)  weight_decay: 0.0500 (0.0500)  time: 0.4534  data: 0.0007  max mem: 15572
Epoch: [30]  [1403/1404]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 3.5484 (3.5732)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)  time: 0.4292  data: 0.0004  max mem: 15572
Epoch: [30] Total time: 0:13:51 (0.5923 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 3.5484 (3.5655)  loss_scale: 32768.0000 (34471.7493)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:48  loss: 1.4371 (1.4371)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 6.0955  data: 5.9056  max mem: 15572
Val:  [ 10/136]  eta: 0:01:44  loss: 1.9641 (1.9714)  acc1: 55.5556 (52.0202)  acc5: 83.3333 (81.8182)  time: 0.8257  data: 0.5976  max mem: 15572
Val:  [ 20/136]  eta: 0:01:11  loss: 2.3081 (2.1702)  acc1: 38.8889 (46.8254)  acc5: 83.3333 (80.1587)  time: 0.3389  data: 0.1214  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.2862 (2.0576)  acc1: 38.8889 (49.6416)  acc5: 83.3333 (80.4660)  time: 0.2992  data: 0.0949  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.7495 (2.0259)  acc1: 61.1111 (51.7615)  acc5: 83.3333 (80.7588)  time: 0.2902  data: 0.0907  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.7738 (2.0205)  acc1: 55.5556 (52.2876)  acc5: 83.3333 (81.3726)  time: 0.3827  data: 0.1831  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1462 (2.1127)  acc1: 44.4444 (49.0893)  acc5: 77.7778 (80.0546)  time: 0.3827  data: 0.1876  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.0070 (2.0787)  acc1: 44.4444 (49.9218)  acc5: 83.3333 (80.5164)  time: 0.3650  data: 0.1758  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8496 (2.0703)  acc1: 50.0000 (50.4115)  acc5: 88.8889 (81.0014)  time: 0.3779  data: 0.1806  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9527 (2.0813)  acc1: 44.4444 (49.6948)  acc5: 88.8889 (80.8913)  time: 0.3757  data: 0.1825  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3043 (2.1585)  acc1: 38.8889 (47.6898)  acc5: 72.2222 (79.0979)  time: 0.3938  data: 0.1989  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2618 (2.1489)  acc1: 44.4444 (48.3483)  acc5: 72.2222 (79.0791)  time: 0.4242  data: 0.2145  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.7921 (2.1012)  acc1: 61.1111 (49.5868)  acc5: 83.3333 (79.6602)  time: 0.3479  data: 0.1534  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6173 (2.0611)  acc1: 66.6667 (50.5089)  acc5: 88.8889 (80.2375)  time: 0.2056  data: 0.0487  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7286 (2.0605)  acc1: 55.5556 (50.5733)  acc5: 88.8889 (80.2621)  time: 0.1428  data: 0.0001  max mem: 15572
Val: Total time: 0:00:50 (0.3748 s / it)
* Acc@1 50.061 Acc@5 79.013 loss 2.105
Accuracy of the network on the 4883 val videos: 50.1%
[2025-01-10 23:24:41,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 23:24:41,458] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 23:24:41,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 23:24:41,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 23:24:43,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 23:24:43,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.06%
Epoch: [31]  [   0/1404]  eta: 3:12:14  lr: 0.000015  min_lr: 0.000000  loss: 4.0126 (4.0126)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2157  data: 7.7072  max mem: 15572
Epoch: [31]  [  10/1404]  eta: 0:29:38  lr: 0.000015  min_lr: 0.000000  loss: 3.4039 (3.4412)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2761  data: 0.7014  max mem: 15572
Epoch: [31]  [  20/1404]  eta: 0:21:23  lr: 0.000015  min_lr: 0.000000  loss: 3.4039 (3.4883)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0009  max mem: 15572
Epoch: [31]  [  30/1404]  eta: 0:18:17  lr: 0.000014  min_lr: 0.000000  loss: 3.4218 (3.5115)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0010  max mem: 15572
Epoch: [31]  [  40/1404]  eta: 0:16:55  lr: 0.000014  min_lr: 0.000000  loss: 3.3334 (3.4785)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0008  max mem: 15572
Epoch: [31]  [  50/1404]  eta: 0:16:31  lr: 0.000014  min_lr: 0.000000  loss: 3.2751 (3.4498)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0007  max mem: 15572
Epoch: [31]  [  60/1404]  eta: 0:16:10  lr: 0.000014  min_lr: 0.000000  loss: 3.3332 (3.4706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.0007  max mem: 15572
Epoch: [31]  [  70/1404]  eta: 0:15:43  lr: 0.000014  min_lr: 0.000000  loss: 3.7198 (3.4884)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.0008  max mem: 15572
Epoch: [31]  [  80/1404]  eta: 0:15:04  lr: 0.000014  min_lr: 0.000000  loss: 3.5553 (3.5149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0007  max mem: 15572
Epoch: [31]  [  90/1404]  eta: 0:14:32  lr: 0.000014  min_lr: 0.000000  loss: 3.5252 (3.5280)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0006  max mem: 15572
[2025-01-10 23:25:49,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:25:49,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:25:49,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:25:49,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 100/1404]  eta: 0:14:21  lr: 0.000014  min_lr: 0.000000  loss: 3.4459 (3.5120)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0007  max mem: 15572
[2025-01-10 23:25:52,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43626
[2025-01-10 23:25:52,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:25:52,348] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43626
[2025-01-10 23:25:52,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:25:52,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 110/1404]  eta: 0:14:02  lr: 0.000014  min_lr: 0.000000  loss: 3.4740 (3.5130)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0010  max mem: 15572
Epoch: [31]  [ 120/1404]  eta: 0:13:47  lr: 0.000014  min_lr: 0.000000  loss: 3.4849 (3.5188)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0012  max mem: 15572
Epoch: [31]  [ 130/1404]  eta: 0:13:36  lr: 0.000014  min_lr: 0.000000  loss: 3.6975 (3.5274)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0009  max mem: 15572
Epoch: [31]  [ 140/1404]  eta: 0:13:25  lr: 0.000014  min_lr: 0.000000  loss: 3.4710 (3.5181)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0008  max mem: 15572
Epoch: [31]  [ 150/1404]  eta: 0:13:25  lr: 0.000014  min_lr: 0.000000  loss: 3.5368 (3.5275)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0009  max mem: 15572
Epoch: [31]  [ 160/1404]  eta: 0:13:10  lr: 0.000014  min_lr: 0.000000  loss: 3.6857 (3.5394)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0009  max mem: 15572
Epoch: [31]  [ 170/1404]  eta: 0:12:55  lr: 0.000014  min_lr: 0.000000  loss: 3.5814 (3.5451)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0009  max mem: 15572
Epoch: [31]  [ 180/1404]  eta: 0:12:44  lr: 0.000014  min_lr: 0.000000  loss: 3.5241 (3.5426)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0010  max mem: 15572
Epoch: [31]  [ 190/1404]  eta: 0:12:40  lr: 0.000014  min_lr: 0.000000  loss: 3.5302 (3.5473)  loss_scale: 32768.0000 (33625.8010)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0009  max mem: 15572
Epoch: [31]  [ 200/1404]  eta: 0:12:30  lr: 0.000014  min_lr: 0.000000  loss: 3.8349 (3.5585)  loss_scale: 32768.0000 (33583.1244)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0010  max mem: 15572
Epoch: [31]  [ 210/1404]  eta: 0:12:18  lr: 0.000014  min_lr: 0.000000  loss: 3.5201 (3.5442)  loss_scale: 32768.0000 (33544.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0010  max mem: 15572
Epoch: [31]  [ 220/1404]  eta: 0:12:07  lr: 0.000014  min_lr: 0.000000  loss: 3.5611 (3.5480)  loss_scale: 32768.0000 (33509.3575)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0211  max mem: 15572
Epoch: [31]  [ 230/1404]  eta: 0:12:00  lr: 0.000014  min_lr: 0.000000  loss: 3.6009 (3.5463)  loss_scale: 32768.0000 (33477.2641)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0546  max mem: 15572
[2025-01-10 23:27:07,031] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:27:07,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:27:07,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:27:07,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:27:11,408] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43762
[2025-01-10 23:27:11,409] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:27:11,409] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:27:11,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43762
[2025-01-10 23:27:11,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 240/1404]  eta: 0:11:55  lr: 0.000014  min_lr: 0.000000  loss: 3.4377 (3.5424)  loss_scale: 32768.0000 (34399.6017)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0752  max mem: 15572
Epoch: [31]  [ 250/1404]  eta: 0:11:49  lr: 0.000014  min_lr: 0.000000  loss: 3.3812 (3.5340)  loss_scale: 32768.0000 (34334.5976)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0934  max mem: 15572
Epoch: [31]  [ 260/1404]  eta: 0:11:42  lr: 0.000014  min_lr: 0.000000  loss: 3.5371 (3.5351)  loss_scale: 32768.0000 (34274.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0992  max mem: 15572
Epoch: [31]  [ 270/1404]  eta: 0:11:34  lr: 0.000014  min_lr: 0.000000  loss: 3.6771 (3.5400)  loss_scale: 32768.0000 (34218.9815)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0850  max mem: 15572
Epoch: [31]  [ 280/1404]  eta: 0:11:29  lr: 0.000014  min_lr: 0.000000  loss: 3.6787 (3.5339)  loss_scale: 32768.0000 (34167.3452)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1207  max mem: 15572
Epoch: [31]  [ 290/1404]  eta: 0:11:23  lr: 0.000014  min_lr: 0.000000  loss: 3.4947 (3.5331)  loss_scale: 32768.0000 (34119.2577)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1364  max mem: 15572
Epoch: [31]  [ 300/1404]  eta: 0:11:16  lr: 0.000014  min_lr: 0.000000  loss: 3.7893 (3.5434)  loss_scale: 32768.0000 (34074.3654)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0966  max mem: 15572
Epoch: [31]  [ 310/1404]  eta: 0:11:10  lr: 0.000014  min_lr: 0.000000  loss: 3.7893 (3.5447)  loss_scale: 32768.0000 (34032.3601)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1082  max mem: 15572
Epoch: [31]  [ 320/1404]  eta: 0:11:03  lr: 0.000014  min_lr: 0.000000  loss: 3.3651 (3.5408)  loss_scale: 32768.0000 (33992.9720)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1340  max mem: 15572
Epoch: [31]  [ 330/1404]  eta: 0:10:56  lr: 0.000014  min_lr: 0.000000  loss: 3.4069 (3.5446)  loss_scale: 32768.0000 (33955.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1069  max mem: 15572
Epoch: [31]  [ 340/1404]  eta: 0:10:50  lr: 0.000014  min_lr: 0.000000  loss: 3.6286 (3.5436)  loss_scale: 32768.0000 (33921.1261)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1018  max mem: 15572
Epoch: [31]  [ 350/1404]  eta: 0:10:45  lr: 0.000014  min_lr: 0.000000  loss: 3.4923 (3.5396)  loss_scale: 32768.0000 (33888.2735)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1478  max mem: 15572
Epoch: [31]  [ 360/1404]  eta: 0:10:37  lr: 0.000014  min_lr: 0.000000  loss: 3.4649 (3.5422)  loss_scale: 32768.0000 (33857.2410)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.0845  max mem: 15572
[2025-01-10 23:28:29,142] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:28:29,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:28:29,208] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:28:29,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 370/1404]  eta: 0:10:31  lr: 0.000014  min_lr: 0.000000  loss: 3.3483 (3.5362)  loss_scale: 32768.0000 (34181.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0640  max mem: 15572
[2025-01-10 23:28:34,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43901
[2025-01-10 23:28:34,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:28:34,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:28:34,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43901
[2025-01-10 23:28:34,283] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 380/1404]  eta: 0:10:22  lr: 0.000014  min_lr: 0.000000  loss: 3.5488 (3.5382)  loss_scale: 32768.0000 (34660.1155)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0767  max mem: 15572
Epoch: [31]  [ 390/1404]  eta: 0:10:17  lr: 0.000014  min_lr: 0.000000  loss: 3.7209 (3.5426)  loss_scale: 32768.0000 (34611.7238)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0856  max mem: 15572
Epoch: [31]  [ 400/1404]  eta: 0:10:09  lr: 0.000014  min_lr: 0.000000  loss: 3.8170 (3.5471)  loss_scale: 32768.0000 (34565.7456)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0728  max mem: 15572
Epoch: [31]  [ 410/1404]  eta: 0:10:02  lr: 0.000014  min_lr: 0.000000  loss: 3.7847 (3.5498)  loss_scale: 32768.0000 (34522.0049)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0370  max mem: 15572
Epoch: [31]  [ 420/1404]  eta: 0:09:59  lr: 0.000014  min_lr: 0.000000  loss: 3.7195 (3.5487)  loss_scale: 32768.0000 (34480.3420)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1458  max mem: 15572
Epoch: [31]  [ 430/1404]  eta: 0:09:51  lr: 0.000014  min_lr: 0.000000  loss: 3.6190 (3.5517)  loss_scale: 32768.0000 (34440.6125)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1096  max mem: 15572
Epoch: [31]  [ 440/1404]  eta: 0:09:44  lr: 0.000014  min_lr: 0.000000  loss: 3.5988 (3.5532)  loss_scale: 32768.0000 (34402.6848)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0246  max mem: 15572
Epoch: [31]  [ 450/1404]  eta: 0:09:38  lr: 0.000014  min_lr: 0.000000  loss: 3.6136 (3.5547)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0756  max mem: 15572
Epoch: [31]  [ 460/1404]  eta: 0:09:32  lr: 0.000014  min_lr: 0.000000  loss: 3.6136 (3.5560)  loss_scale: 32768.0000 (34331.7657)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0807  max mem: 15572
Epoch: [31]  [ 470/1404]  eta: 0:09:24  lr: 0.000014  min_lr: 0.000000  loss: 3.4730 (3.5556)  loss_scale: 32768.0000 (34298.5648)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0296  max mem: 15572
[2025-01-10 23:29:33,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=299, lr=[1.3126459349697277e-07, 1.3126459349697277e-07, 1.875208478528183e-07, 1.875208478528183e-07, 2.678869255040261e-07, 2.678869255040261e-07, 3.826956078628945e-07, 3.826956078628945e-07, 5.467080112327065e-07, 5.467080112327065e-07, 7.810114446181522e-07, 7.810114446181522e-07, 1.115730635168789e-06, 1.115730635168789e-06, 1.5939009073839843e-06, 1.5939009073839843e-06, 2.2770012962628343e-06, 2.2770012962628343e-06, 3.2528589946611926e-06, 3.2528589946611926e-06, 4.6469414209445606e-06, 4.6469414209445606e-06, 6.6384877442065164e-06, 6.6384877442065164e-06, 9.483553920295024e-06, 9.483553920295024e-06, 1.3547934171850035e-05, 1.3547934171850035e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 23:29:33,308] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=45.758036084291035, CurrSamplesPerSec=46.009345999429584, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [ 480/1404]  eta: 0:09:20  lr: 0.000014  min_lr: 0.000000  loss: 3.7453 (3.5586)  loss_scale: 32768.0000 (34266.7443)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0864  max mem: 15572
Epoch: [31]  [ 490/1404]  eta: 0:09:11  lr: 0.000014  min_lr: 0.000000  loss: 3.6000 (3.5533)  loss_scale: 32768.0000 (34236.2200)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0864  max mem: 15572
Epoch: [31]  [ 500/1404]  eta: 0:09:04  lr: 0.000013  min_lr: 0.000000  loss: 3.3998 (3.5489)  loss_scale: 32768.0000 (34206.9142)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0154  max mem: 15572
[2025-01-10 23:29:48,924] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:29:48,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:29:48,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:29:48,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 510/1404]  eta: 0:08:56  lr: 0.000013  min_lr: 0.000000  loss: 3.7435 (3.5551)  loss_scale: 32768.0000 (34499.3816)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0279  max mem: 15572
Epoch: [31]  [ 520/1404]  eta: 0:08:52  lr: 0.000013  min_lr: 0.000000  loss: 3.7118 (3.5540)  loss_scale: 65536.0000 (35095.0940)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1105  max mem: 15572
[2025-01-10 23:29:58,713] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44045
[2025-01-10 23:29:58,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:29:58,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:29:58,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44045
[2025-01-10 23:29:58,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 530/1404]  eta: 0:08:45  lr: 0.000013  min_lr: 0.000000  loss: 3.5263 (3.5556)  loss_scale: 32768.0000 (35051.2693)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0981  max mem: 15572
Epoch: [31]  [ 540/1404]  eta: 0:08:37  lr: 0.000013  min_lr: 0.000000  loss: 3.6421 (3.5547)  loss_scale: 32768.0000 (35009.0647)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0009  max mem: 15572
Epoch: [31]  [ 550/1404]  eta: 0:08:30  lr: 0.000013  min_lr: 0.000000  loss: 3.6765 (3.5545)  loss_scale: 32768.0000 (34968.3920)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0008  max mem: 15572
Epoch: [31]  [ 560/1404]  eta: 0:08:25  lr: 0.000013  min_lr: 0.000000  loss: 3.6678 (3.5575)  loss_scale: 32768.0000 (34929.1693)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0897  max mem: 15572
Epoch: [31]  [ 570/1404]  eta: 0:08:18  lr: 0.000013  min_lr: 0.000000  loss: 3.6014 (3.5558)  loss_scale: 32768.0000 (34891.3205)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0897  max mem: 15572
Epoch: [31]  [ 580/1404]  eta: 0:08:11  lr: 0.000013  min_lr: 0.000000  loss: 3.6505 (3.5580)  loss_scale: 32768.0000 (34854.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0118  max mem: 15572
Epoch: [31]  [ 590/1404]  eta: 0:08:05  lr: 0.000013  min_lr: 0.000000  loss: 3.7071 (3.5599)  loss_scale: 32768.0000 (34819.4653)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0306  max mem: 15572
Epoch: [31]  [ 600/1404]  eta: 0:08:00  lr: 0.000013  min_lr: 0.000000  loss: 3.6605 (3.5617)  loss_scale: 32768.0000 (34785.3311)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0567  max mem: 15572
Epoch: [31]  [ 610/1404]  eta: 0:07:54  lr: 0.000013  min_lr: 0.000000  loss: 3.5782 (3.5613)  loss_scale: 32768.0000 (34752.3142)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0708  max mem: 15572
Epoch: [31]  [ 620/1404]  eta: 0:07:49  lr: 0.000013  min_lr: 0.000000  loss: 3.3820 (3.5604)  loss_scale: 32768.0000 (34720.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1072  max mem: 15572
Epoch: [31]  [ 630/1404]  eta: 0:07:43  lr: 0.000013  min_lr: 0.000000  loss: 3.5849 (3.5601)  loss_scale: 32768.0000 (34689.4200)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0881  max mem: 15572
Epoch: [31]  [ 640/1404]  eta: 0:07:37  lr: 0.000013  min_lr: 0.000000  loss: 3.6777 (3.5641)  loss_scale: 32768.0000 (34659.4446)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0902  max mem: 15572
[2025-01-10 23:31:14,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:31:14,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:31:14,054] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:31:14,054] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 650/1404]  eta: 0:07:31  lr: 0.000013  min_lr: 0.000000  loss: 3.5811 (3.5630)  loss_scale: 32768.0000 (34680.7250)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1059  max mem: 15572
Epoch: [31]  [ 660/1404]  eta: 0:07:26  lr: 0.000013  min_lr: 0.000000  loss: 3.5811 (3.5672)  loss_scale: 65536.0000 (35147.5219)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1142  max mem: 15572
[2025-01-10 23:31:23,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44189
[2025-01-10 23:31:23,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44189
[2025-01-10 23:31:23,803] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:31:23,803] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:31:23,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 670/1404]  eta: 0:07:19  lr: 0.000013  min_lr: 0.000000  loss: 3.8819 (3.5677)  loss_scale: 65536.0000 (35307.3979)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0980  max mem: 15572
Epoch: [31]  [ 680/1404]  eta: 0:07:13  lr: 0.000013  min_lr: 0.000000  loss: 3.8508 (3.5719)  loss_scale: 32768.0000 (35270.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0520  max mem: 15572
Epoch: [31]  [ 690/1404]  eta: 0:07:08  lr: 0.000013  min_lr: 0.000000  loss: 3.7070 (3.5699)  loss_scale: 32768.0000 (35233.8987)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0948  max mem: 15572
Epoch: [31]  [ 700/1404]  eta: 0:07:01  lr: 0.000013  min_lr: 0.000000  loss: 3.3550 (3.5683)  loss_scale: 32768.0000 (35198.7218)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0569  max mem: 15572
Epoch: [31]  [ 710/1404]  eta: 0:06:54  lr: 0.000013  min_lr: 0.000000  loss: 3.5933 (3.5699)  loss_scale: 32768.0000 (35164.5345)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0009  max mem: 15572
Epoch: [31]  [ 720/1404]  eta: 0:06:48  lr: 0.000013  min_lr: 0.000000  loss: 3.7756 (3.5716)  loss_scale: 32768.0000 (35131.2954)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0007  max mem: 15572
Epoch: [31]  [ 730/1404]  eta: 0:06:42  lr: 0.000013  min_lr: 0.000000  loss: 3.7099 (3.5734)  loss_scale: 32768.0000 (35098.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0449  max mem: 15572
Epoch: [31]  [ 740/1404]  eta: 0:06:36  lr: 0.000013  min_lr: 0.000000  loss: 3.6067 (3.5729)  loss_scale: 32768.0000 (35067.5088)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0773  max mem: 15572
Epoch: [31]  [ 750/1404]  eta: 0:06:29  lr: 0.000013  min_lr: 0.000000  loss: 3.6067 (3.5729)  loss_scale: 32768.0000 (35036.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0664  max mem: 15572
Epoch: [31]  [ 760/1404]  eta: 0:06:23  lr: 0.000013  min_lr: 0.000000  loss: 3.4910 (3.5689)  loss_scale: 32768.0000 (35007.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0542  max mem: 15572
Epoch: [31]  [ 770/1404]  eta: 0:06:18  lr: 0.000013  min_lr: 0.000000  loss: 3.3358 (3.5678)  loss_scale: 32768.0000 (34978.0337)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0706  max mem: 15572
Epoch: [31]  [ 780/1404]  eta: 0:06:12  lr: 0.000013  min_lr: 0.000000  loss: 3.4542 (3.5670)  loss_scale: 32768.0000 (34949.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0504  max mem: 15572
Epoch: [31]  [ 790/1404]  eta: 0:06:06  lr: 0.000013  min_lr: 0.000000  loss: 3.6362 (3.5688)  loss_scale: 32768.0000 (34922.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0006  max mem: 15572
[2025-01-10 23:32:39,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:32:39,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:32:39,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:32:39,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:32:41,164] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44321
[2025-01-10 23:32:41,164] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:32:41,164] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:32:41,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44321
[2025-01-10 23:32:41,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 800/1404]  eta: 0:06:00  lr: 0.000013  min_lr: 0.000000  loss: 3.6481 (3.5675)  loss_scale: 32768.0000 (35017.9875)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0006  max mem: 15572
Epoch: [31]  [ 810/1404]  eta: 0:05:54  lr: 0.000013  min_lr: 0.000000  loss: 3.5078 (3.5685)  loss_scale: 32768.0000 (34990.2441)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0008  max mem: 15572
Epoch: [31]  [ 820/1404]  eta: 0:05:47  lr: 0.000013  min_lr: 0.000000  loss: 3.5001 (3.5659)  loss_scale: 32768.0000 (34963.1766)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0236  max mem: 15572
Epoch: [31]  [ 830/1404]  eta: 0:05:41  lr: 0.000013  min_lr: 0.000000  loss: 3.6036 (3.5705)  loss_scale: 32768.0000 (34936.7605)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0524  max mem: 15572
Epoch: [31]  [ 840/1404]  eta: 0:05:36  lr: 0.000013  min_lr: 0.000000  loss: 3.9182 (3.5714)  loss_scale: 32768.0000 (34910.9727)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0295  max mem: 15572
Epoch: [31]  [ 850/1404]  eta: 0:05:30  lr: 0.000013  min_lr: 0.000000  loss: 3.6694 (3.5723)  loss_scale: 32768.0000 (34885.7908)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.0007  max mem: 15572
Epoch: [31]  [ 860/1404]  eta: 0:05:24  lr: 0.000013  min_lr: 0.000000  loss: 3.5787 (3.5727)  loss_scale: 32768.0000 (34861.1940)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0007  max mem: 15572
Epoch: [31]  [ 870/1404]  eta: 0:05:18  lr: 0.000013  min_lr: 0.000000  loss: 3.6341 (3.5728)  loss_scale: 32768.0000 (34837.1619)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0007  max mem: 15572
Epoch: [31]  [ 880/1404]  eta: 0:05:12  lr: 0.000013  min_lr: 0.000000  loss: 3.6872 (3.5719)  loss_scale: 32768.0000 (34813.6754)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0009  max mem: 15572
Epoch: [31]  [ 890/1404]  eta: 0:05:06  lr: 0.000013  min_lr: 0.000000  loss: 3.3829 (3.5688)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0008  max mem: 15572
Epoch: [31]  [ 900/1404]  eta: 0:05:00  lr: 0.000013  min_lr: 0.000000  loss: 3.3279 (3.5678)  loss_scale: 32768.0000 (34768.2664)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0005  max mem: 15572
Epoch: [31]  [ 910/1404]  eta: 0:04:54  lr: 0.000013  min_lr: 0.000000  loss: 3.5542 (3.5680)  loss_scale: 32768.0000 (34746.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0006  max mem: 15572
Epoch: [31]  [ 920/1404]  eta: 0:04:47  lr: 0.000013  min_lr: 0.000000  loss: 3.6156 (3.5670)  loss_scale: 32768.0000 (34724.8295)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0007  max mem: 15572
[2025-01-10 23:33:55,401] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:33:55,401] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:33:55,401] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:33:55,402] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:33:55,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44451
[2025-01-10 23:33:55,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:33:55,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:33:55,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44451
[2025-01-10 23:33:55,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 930/1404]  eta: 0:04:41  lr: 0.000013  min_lr: 0.000000  loss: 3.6156 (3.5653)  loss_scale: 32768.0000 (34739.0075)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0007  max mem: 15572
Epoch: [31]  [ 940/1404]  eta: 0:04:35  lr: 0.000013  min_lr: 0.000000  loss: 3.5753 (3.5660)  loss_scale: 32768.0000 (34718.0616)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0007  max mem: 15572
Epoch: [31]  [ 950/1404]  eta: 0:04:29  lr: 0.000013  min_lr: 0.000000  loss: 3.5365 (3.5658)  loss_scale: 32768.0000 (34697.5563)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0146  max mem: 15572
Epoch: [31]  [ 960/1404]  eta: 0:04:23  lr: 0.000013  min_lr: 0.000000  loss: 3.4664 (3.5648)  loss_scale: 32768.0000 (34677.4776)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0148  max mem: 15572
Epoch: [31]  [ 970/1404]  eta: 0:04:18  lr: 0.000013  min_lr: 0.000000  loss: 3.5965 (3.5662)  loss_scale: 32768.0000 (34657.8126)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0010  max mem: 15572
Epoch: [31]  [ 980/1404]  eta: 0:04:12  lr: 0.000013  min_lr: 0.000000  loss: 3.5756 (3.5658)  loss_scale: 32768.0000 (34638.5484)  weight_decay: 0.0500 (0.0500)  time: 0.6632  data: 0.0011  max mem: 15572
Epoch: [31]  [ 990/1404]  eta: 0:04:06  lr: 0.000012  min_lr: 0.000000  loss: 3.6108 (3.5657)  loss_scale: 32768.0000 (34619.6731)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0009  max mem: 15572
Epoch: [31]  [1000/1404]  eta: 0:04:00  lr: 0.000012  min_lr: 0.000000  loss: 3.4428 (3.5645)  loss_scale: 32768.0000 (34601.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0010  max mem: 15572
Epoch: [31]  [1010/1404]  eta: 0:03:54  lr: 0.000012  min_lr: 0.000000  loss: 3.3125 (3.5615)  loss_scale: 32768.0000 (34583.0425)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0010  max mem: 15572
Epoch: [31]  [1020/1404]  eta: 0:03:48  lr: 0.000012  min_lr: 0.000000  loss: 3.6447 (3.5649)  loss_scale: 32768.0000 (34565.2654)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0007  max mem: 15572
Epoch: [31]  [1030/1404]  eta: 0:03:42  lr: 0.000012  min_lr: 0.000000  loss: 3.6447 (3.5638)  loss_scale: 32768.0000 (34547.8332)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0008  max mem: 15572
Epoch: [31]  [1040/1404]  eta: 0:03:36  lr: 0.000012  min_lr: 0.000000  loss: 3.3316 (3.5617)  loss_scale: 32768.0000 (34530.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0007  max mem: 15572
Epoch: [31]  [1050/1404]  eta: 0:03:30  lr: 0.000012  min_lr: 0.000000  loss: 3.3635 (3.5617)  loss_scale: 32768.0000 (34513.9638)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0007  max mem: 15572
[2025-01-10 23:35:12,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:35:12,717] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:35:12,719] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:35:12,719] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1060/1404]  eta: 0:03:24  lr: 0.000012  min_lr: 0.000000  loss: 3.6863 (3.5617)  loss_scale: 32768.0000 (34651.9284)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0011  max mem: 15572
[2025-01-10 23:35:15,607] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44585
[2025-01-10 23:35:15,607] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:35:15,608] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:35:15,623] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44585
[2025-01-10 23:35:15,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [1070/1404]  eta: 0:03:18  lr: 0.000012  min_lr: 0.000000  loss: 3.4797 (3.5619)  loss_scale: 32768.0000 (34634.3380)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0276  max mem: 15572
Epoch: [31]  [1080/1404]  eta: 0:03:12  lr: 0.000012  min_lr: 0.000000  loss: 3.6130 (3.5628)  loss_scale: 32768.0000 (34617.0731)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0272  max mem: 15572
Epoch: [31]  [1090/1404]  eta: 0:03:06  lr: 0.000012  min_lr: 0.000000  loss: 3.6800 (3.5635)  loss_scale: 32768.0000 (34600.1247)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0139  max mem: 15572
Epoch: [31]  [1100/1404]  eta: 0:03:00  lr: 0.000012  min_lr: 0.000000  loss: 3.6119 (3.5635)  loss_scale: 32768.0000 (34583.4841)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0234  max mem: 15572
Epoch: [31]  [1110/1404]  eta: 0:02:54  lr: 0.000012  min_lr: 0.000000  loss: 3.5576 (3.5638)  loss_scale: 32768.0000 (34567.1431)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0101  max mem: 15572
Epoch: [31]  [1120/1404]  eta: 0:02:48  lr: 0.000012  min_lr: 0.000000  loss: 3.6124 (3.5665)  loss_scale: 32768.0000 (34551.0937)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.0006  max mem: 15572
Epoch: [31]  [1130/1404]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 3.8148 (3.5669)  loss_scale: 32768.0000 (34535.3280)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0008  max mem: 15572
Epoch: [31]  [1140/1404]  eta: 0:02:37  lr: 0.000012  min_lr: 0.000000  loss: 3.7587 (3.5671)  loss_scale: 32768.0000 (34519.8387)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0008  max mem: 15572
Epoch: [31]  [1150/1404]  eta: 0:02:31  lr: 0.000012  min_lr: 0.000000  loss: 3.5935 (3.5658)  loss_scale: 32768.0000 (34504.6186)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0007  max mem: 15572
Epoch: [31]  [1160/1404]  eta: 0:02:24  lr: 0.000012  min_lr: 0.000000  loss: 3.4494 (3.5663)  loss_scale: 32768.0000 (34489.6606)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0044  max mem: 15572
Epoch: [31]  [1170/1404]  eta: 0:02:19  lr: 0.000012  min_lr: 0.000000  loss: 3.4494 (3.5654)  loss_scale: 32768.0000 (34474.9582)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0044  max mem: 15572
Epoch: [31]  [1180/1404]  eta: 0:02:13  lr: 0.000012  min_lr: 0.000000  loss: 3.4528 (3.5656)  loss_scale: 32768.0000 (34460.5047)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0006  max mem: 15572
[2025-01-10 23:36:32,564] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:36:32,564] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:36:32,580] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:36:32,581] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1190/1404]  eta: 0:02:07  lr: 0.000012  min_lr: 0.000000  loss: 3.6557 (3.5666)  loss_scale: 32768.0000 (34473.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0407  max mem: 15572
[2025-01-10 23:36:38,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44724
[2025-01-10 23:36:38,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44724
[2025-01-10 23:36:38,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:36:38,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:36:38,822] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1200/1404]  eta: 0:02:01  lr: 0.000012  min_lr: 0.000000  loss: 3.7510 (3.5682)  loss_scale: 32768.0000 (34705.1590)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1109  max mem: 15572
Epoch: [31]  [1210/1404]  eta: 0:01:55  lr: 0.000012  min_lr: 0.000000  loss: 3.7510 (3.5688)  loss_scale: 32768.0000 (34689.1627)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1037  max mem: 15572
Epoch: [31]  [1220/1404]  eta: 0:01:49  lr: 0.000012  min_lr: 0.000000  loss: 3.5604 (3.5682)  loss_scale: 32768.0000 (34673.4283)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0686  max mem: 15572
Epoch: [31]  [1230/1404]  eta: 0:01:43  lr: 0.000012  min_lr: 0.000000  loss: 3.6291 (3.5680)  loss_scale: 32768.0000 (34657.9496)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0504  max mem: 15572
Epoch: [31]  [1240/1404]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 3.5251 (3.5682)  loss_scale: 32768.0000 (34642.7204)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0156  max mem: 15572
Epoch: [31]  [1250/1404]  eta: 0:01:31  lr: 0.000012  min_lr: 0.000000  loss: 3.7053 (3.5683)  loss_scale: 32768.0000 (34627.7346)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0386  max mem: 15572
Epoch: [31]  [1260/1404]  eta: 0:01:25  lr: 0.000012  min_lr: 0.000000  loss: 3.7205 (3.5683)  loss_scale: 32768.0000 (34612.9865)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.0385  max mem: 15572
Epoch: [31]  [1270/1404]  eta: 0:01:19  lr: 0.000012  min_lr: 0.000000  loss: 3.7205 (3.5699)  loss_scale: 32768.0000 (34598.4705)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0008  max mem: 15572
Epoch: [31]  [1280/1404]  eta: 0:01:13  lr: 0.000012  min_lr: 0.000000  loss: 3.7272 (3.5705)  loss_scale: 32768.0000 (34584.1811)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0009  max mem: 15572
Epoch: [31]  [1290/1404]  eta: 0:01:07  lr: 0.000012  min_lr: 0.000000  loss: 3.5413 (3.5693)  loss_scale: 32768.0000 (34570.1131)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0009  max mem: 15572
Epoch: [31]  [1300/1404]  eta: 0:01:01  lr: 0.000012  min_lr: 0.000000  loss: 3.3517 (3.5674)  loss_scale: 32768.0000 (34556.2613)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0009  max mem: 15572
Epoch: [31]  [1310/1404]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 3.4564 (3.5689)  loss_scale: 32768.0000 (34542.6209)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0386  max mem: 15572
Epoch: [31]  [1320/1404]  eta: 0:00:49  lr: 0.000012  min_lr: 0.000000  loss: 3.4129 (3.5667)  loss_scale: 32768.0000 (34529.1870)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1072  max mem: 15572
[2025-01-10 23:37:55,280] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:37:55,280] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:37:55,280] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:37:55,281] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1330/1404]  eta: 0:00:43  lr: 0.000012  min_lr: 0.000000  loss: 3.3872 (3.5665)  loss_scale: 32768.0000 (34565.1931)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0696  max mem: 15572
Epoch: [31]  [1340/1404]  eta: 0:00:38  lr: 0.000012  min_lr: 0.000000  loss: 3.3938 (3.5659)  loss_scale: 65536.0000 (34796.1462)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0010  max mem: 15572
[2025-01-10 23:38:07,402] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44874
[2025-01-10 23:38:07,402] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:38:07,403] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1350/1404]  eta: 0:00:32  lr: 0.000012  min_lr: 0.000000  loss: 3.3884 (3.5651)  loss_scale: 65536.0000 (34999.4256)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0008  max mem: 15572
[2025-01-10 23:38:07,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44874
[2025-01-10 23:38:07,490] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [1360/1404]  eta: 0:00:26  lr: 0.000012  min_lr: 0.000000  loss: 3.5140 (3.5644)  loss_scale: 32768.0000 (34983.0301)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0009  max mem: 15572
Epoch: [31]  [1370/1404]  eta: 0:00:20  lr: 0.000012  min_lr: 0.000000  loss: 3.5115 (3.5643)  loss_scale: 32768.0000 (34966.8738)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0009  max mem: 15572
Epoch: [31]  [1380/1404]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 3.4660 (3.5637)  loss_scale: 32768.0000 (34950.9515)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0273  max mem: 15572
Epoch: [31]  [1390/1404]  eta: 0:00:08  lr: 0.000012  min_lr: 0.000000  loss: 3.4184 (3.5639)  loss_scale: 32768.0000 (34935.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0502  max mem: 15572
Epoch: [31]  [1400/1404]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000000  loss: 3.6401 (3.5644)  loss_scale: 32768.0000 (34919.7887)  weight_decay: 0.0500 (0.0500)  time: 0.4670  data: 0.0233  max mem: 15572
Epoch: [31]  [1403/1404]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 3.6401 (3.5633)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)  time: 0.4404  data: 0.0232  max mem: 15572
Epoch: [31] Total time: 0:13:50 (0.5919 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 3.6401 (3.5686)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:11:46  loss: 1.4275 (1.4275)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.1933  data: 4.9861  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 1.9824 (1.9715)  acc1: 61.1111 (52.0202)  acc5: 83.3333 (81.8182)  time: 0.7894  data: 0.5813  max mem: 15572
Val:  [ 20/136]  eta: 0:01:00  loss: 2.3189 (2.1603)  acc1: 38.8889 (46.0317)  acc5: 77.7778 (79.3651)  time: 0.2892  data: 0.0885  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.1175 (2.0446)  acc1: 38.8889 (50.0000)  acc5: 83.3333 (80.1075)  time: 0.2843  data: 0.0836  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 1.7613 (2.0096)  acc1: 61.1111 (52.1680)  acc5: 83.3333 (81.0298)  time: 0.3498  data: 0.1487  max mem: 15572
Val:  [ 50/136]  eta: 0:00:36  loss: 1.7652 (2.0157)  acc1: 55.5556 (52.6144)  acc5: 83.3333 (81.2636)  time: 0.3765  data: 0.1722  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1409 (2.1030)  acc1: 50.0000 (50.0000)  acc5: 77.7778 (79.8725)  time: 0.3987  data: 0.1829  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 1.8340 (2.0669)  acc1: 50.0000 (50.5477)  acc5: 83.3333 (80.5164)  time: 0.3951  data: 0.1728  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8340 (2.0617)  acc1: 50.0000 (50.3429)  acc5: 88.8889 (81.0014)  time: 0.3765  data: 0.1461  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.0235 (2.0721)  acc1: 44.4444 (49.8169)  acc5: 77.7778 (80.5861)  time: 0.3643  data: 0.1465  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.4113 (2.1503)  acc1: 44.4444 (47.9098)  acc5: 72.2222 (78.6029)  time: 0.3272  data: 0.0984  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.1582 (2.1402)  acc1: 50.0000 (48.3984)  acc5: 77.7778 (78.7287)  time: 0.3266  data: 0.0938  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8767 (2.0947)  acc1: 55.5556 (49.4490)  acc5: 83.3333 (79.3388)  time: 0.3983  data: 0.1865  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5698 (2.0517)  acc1: 61.1111 (50.6361)  acc5: 88.8889 (80.0254)  time: 0.3050  data: 0.1271  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7070 (2.0497)  acc1: 61.1111 (50.8190)  acc5: 88.8889 (80.1392)  time: 0.1726  data: 0.0155  max mem: 15572
Val: Total time: 0:00:50 (0.3719 s / it)
* Acc@1 50.061 Acc@5 78.829 loss 2.094
Accuracy of the network on the 4883 val videos: 50.1%
Max accuracy: 50.06%
Epoch: [32]  [   0/1404]  eta: 2:47:57  lr: 0.000012  min_lr: 0.000000  loss: 3.7360 (3.7360)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1780  data: 6.7032  max mem: 15572
Epoch: [32]  [  10/1404]  eta: 0:28:03  lr: 0.000012  min_lr: 0.000000  loss: 3.6877 (3.4802)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2076  data: 0.6101  max mem: 15572
Epoch: [32]  [  20/1404]  eta: 0:21:57  lr: 0.000012  min_lr: 0.000000  loss: 3.5784 (3.5636)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.0445  max mem: 15572
Epoch: [32]  [  30/1404]  eta: 0:18:53  lr: 0.000012  min_lr: 0.000000  loss: 3.6987 (3.5754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0625  max mem: 15572
Epoch: [32]  [  40/1404]  eta: 0:17:20  lr: 0.000012  min_lr: 0.000000  loss: 3.7246 (3.5971)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0187  max mem: 15572
Epoch: [32]  [  50/1404]  eta: 0:16:06  lr: 0.000012  min_lr: 0.000000  loss: 3.7467 (3.5874)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0192  max mem: 15572
Epoch: [32]  [  60/1404]  eta: 0:15:31  lr: 0.000012  min_lr: 0.000000  loss: 3.5769 (3.5712)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0353  max mem: 15572
Epoch: [32]  [  70/1404]  eta: 0:14:53  lr: 0.000012  min_lr: 0.000000  loss: 3.5975 (3.5947)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0407  max mem: 15572
[2025-01-10 23:40:13,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=306, lr=[1.1159156044128313e-07, 1.1159156044128313e-07, 1.5941651491611877e-07, 1.5941651491611877e-07, 2.2773787845159827e-07, 2.2773787845159827e-07, 3.253398263594261e-07, 3.253398263594261e-07, 4.6477118051346586e-07, 4.6477118051346586e-07, 6.639588293049513e-07, 6.639588293049513e-07, 9.485126132927875e-07, 9.485126132927875e-07, 1.3550180189896967e-06, 1.3550180189896967e-06, 1.9357400271281383e-06, 1.9357400271281383e-06, 2.7653428958973406e-06, 2.7653428958973406e-06, 3.950489851281915e-06, 3.950489851281915e-06, 5.643556930402736e-06, 5.643556930402736e-06, 8.062224186289624e-06, 8.062224186289624e-06, 1.1517463123270892e-05, 1.1517463123270892e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 23:40:13,713] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=45.760050410536266, CurrSamplesPerSec=54.45662871165007, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-10 23:40:16,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:40:16,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:40:16,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:40:16,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [  80/1404]  eta: 0:14:39  lr: 0.000011  min_lr: 0.000000  loss: 3.7694 (3.6020)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.0706  max mem: 15572
Epoch: [32]  [  90/1404]  eta: 0:14:15  lr: 0.000011  min_lr: 0.000000  loss: 3.5637 (3.6020)  loss_scale: 65536.0000 (38529.4066)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0759  max mem: 15572
[2025-01-10 23:40:26,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45020
[2025-01-10 23:40:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:40:26,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:40:26,574] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45020
[2025-01-10 23:40:26,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [ 100/1404]  eta: 0:14:01  lr: 0.000011  min_lr: 0.000000  loss: 3.5637 (3.5972)  loss_scale: 65536.0000 (38283.4059)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0643  max mem: 15572
Epoch: [32]  [ 110/1404]  eta: 0:13:48  lr: 0.000011  min_lr: 0.000000  loss: 3.7089 (3.5913)  loss_scale: 32768.0000 (37786.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0641  max mem: 15572
Epoch: [32]  [ 120/1404]  eta: 0:13:54  lr: 0.000011  min_lr: 0.000000  loss: 3.7089 (3.6013)  loss_scale: 32768.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0877  max mem: 15572
Epoch: [32]  [ 130/1404]  eta: 0:13:39  lr: 0.000011  min_lr: 0.000000  loss: 3.4610 (3.5921)  loss_scale: 32768.0000 (37020.3359)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.0587  max mem: 15572
Epoch: [32]  [ 140/1404]  eta: 0:13:29  lr: 0.000011  min_lr: 0.000000  loss: 3.3979 (3.5810)  loss_scale: 32768.0000 (36718.7518)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0434  max mem: 15572
Epoch: [32]  [ 150/1404]  eta: 0:13:20  lr: 0.000011  min_lr: 0.000000  loss: 3.6211 (3.5880)  loss_scale: 32768.0000 (36457.1126)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1058  max mem: 15572
Epoch: [32]  [ 160/1404]  eta: 0:13:07  lr: 0.000011  min_lr: 0.000000  loss: 3.7341 (3.5985)  loss_scale: 32768.0000 (36227.9752)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0951  max mem: 15572
Epoch: [32]  [ 170/1404]  eta: 0:13:01  lr: 0.000011  min_lr: 0.000000  loss: 3.7393 (3.6062)  loss_scale: 32768.0000 (36025.6374)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1107  max mem: 15572
Epoch: [32]  [ 180/1404]  eta: 0:12:52  lr: 0.000011  min_lr: 0.000000  loss: 3.7393 (3.5986)  loss_scale: 32768.0000 (35845.6575)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1261  max mem: 15572
Epoch: [32]  [ 190/1404]  eta: 0:12:45  lr: 0.000011  min_lr: 0.000000  loss: 3.4991 (3.5921)  loss_scale: 32768.0000 (35684.5236)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0934  max mem: 15572
Epoch: [32]  [ 200/1404]  eta: 0:12:33  lr: 0.000011  min_lr: 0.000000  loss: 3.4991 (3.5868)  loss_scale: 32768.0000 (35539.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0695  max mem: 15572
Epoch: [32]  [ 210/1404]  eta: 0:12:21  lr: 0.000011  min_lr: 0.000000  loss: 3.4261 (3.5849)  loss_scale: 32768.0000 (35408.0758)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0242  max mem: 15572
Epoch: [32]  [ 220/1404]  eta: 0:12:18  lr: 0.000011  min_lr: 0.000000  loss: 3.7118 (3.5936)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0712  max mem: 15572
[2025-01-10 23:41:43,951] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:41:43,951] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:41:44,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:41:44,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 230/1404]  eta: 0:12:07  lr: 0.000011  min_lr: 0.000000  loss: 3.6737 (3.5834)  loss_scale: 32768.0000 (36598.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0709  max mem: 15572
[2025-01-10 23:41:53,761] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45166
[2025-01-10 23:41:53,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:41:53,766] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45166
[2025-01-10 23:41:53,767] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:41:53,767] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 240/1404]  eta: 0:11:59  lr: 0.000011  min_lr: 0.000000  loss: 3.3676 (3.5787)  loss_scale: 65536.0000 (37390.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0402  max mem: 15572
Epoch: [32]  [ 250/1404]  eta: 0:11:55  lr: 0.000011  min_lr: 0.000000  loss: 3.6192 (3.5873)  loss_scale: 32768.0000 (37206.6932)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0730  max mem: 15572
Epoch: [32]  [ 260/1404]  eta: 0:11:45  lr: 0.000011  min_lr: 0.000000  loss: 3.6576 (3.5908)  loss_scale: 32768.0000 (37036.6284)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0425  max mem: 15572
Epoch: [32]  [ 270/1404]  eta: 0:11:39  lr: 0.000011  min_lr: 0.000000  loss: 3.5872 (3.5818)  loss_scale: 32768.0000 (36879.1144)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0278  max mem: 15572
Epoch: [32]  [ 280/1404]  eta: 0:11:35  lr: 0.000011  min_lr: 0.000000  loss: 3.5193 (3.5798)  loss_scale: 32768.0000 (36732.8114)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0973  max mem: 15572
Epoch: [32]  [ 290/1404]  eta: 0:11:24  lr: 0.000011  min_lr: 0.000000  loss: 3.5193 (3.5720)  loss_scale: 32768.0000 (36596.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0942  max mem: 15572
Epoch: [32]  [ 300/1404]  eta: 0:11:17  lr: 0.000011  min_lr: 0.000000  loss: 3.4642 (3.5727)  loss_scale: 32768.0000 (36469.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0410  max mem: 15572
Epoch: [32]  [ 310/1404]  eta: 0:11:08  lr: 0.000011  min_lr: 0.000000  loss: 3.6565 (3.5771)  loss_scale: 32768.0000 (36350.3537)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0262  max mem: 15572
Epoch: [32]  [ 320/1404]  eta: 0:11:03  lr: 0.000011  min_lr: 0.000000  loss: 3.5645 (3.5718)  loss_scale: 32768.0000 (36238.7539)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0367  max mem: 15572
Epoch: [32]  [ 330/1404]  eta: 0:10:54  lr: 0.000011  min_lr: 0.000000  loss: 3.4820 (3.5674)  loss_scale: 32768.0000 (36133.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0474  max mem: 15572
Epoch: [32]  [ 340/1404]  eta: 0:10:48  lr: 0.000011  min_lr: 0.000000  loss: 3.6289 (3.5693)  loss_scale: 32768.0000 (36035.1906)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0665  max mem: 15572
Epoch: [32]  [ 350/1404]  eta: 0:10:41  lr: 0.000011  min_lr: 0.000000  loss: 3.5622 (3.5633)  loss_scale: 32768.0000 (35942.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0871  max mem: 15572
Epoch: [32]  [ 360/1404]  eta: 0:10:34  lr: 0.000011  min_lr: 0.000000  loss: 3.4598 (3.5634)  loss_scale: 32768.0000 (35854.1828)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.0581  max mem: 15572
[2025-01-10 23:43:09,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:43:09,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:43:09,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:43:09,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:43:10,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45296
[2025-01-10 23:43:10,474] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:43:10,474] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:43:10,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45296
[2025-01-10 23:43:10,497] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [ 370/1404]  eta: 0:10:28  lr: 0.000011  min_lr: 0.000000  loss: 3.6206 (3.5686)  loss_scale: 32768.0000 (35859.3208)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0928  max mem: 15572
Epoch: [32]  [ 380/1404]  eta: 0:10:21  lr: 0.000011  min_lr: 0.000000  loss: 3.7657 (3.5720)  loss_scale: 32768.0000 (35778.1837)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0970  max mem: 15572
Epoch: [32]  [ 390/1404]  eta: 0:10:12  lr: 0.000011  min_lr: 0.000000  loss: 3.5915 (3.5724)  loss_scale: 32768.0000 (35701.1969)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0309  max mem: 15572
Epoch: [32]  [ 400/1404]  eta: 0:10:06  lr: 0.000011  min_lr: 0.000000  loss: 3.4224 (3.5624)  loss_scale: 32768.0000 (35628.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0332  max mem: 15572
Epoch: [32]  [ 410/1404]  eta: 0:10:01  lr: 0.000011  min_lr: 0.000000  loss: 3.1759 (3.5599)  loss_scale: 32768.0000 (35558.4623)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0713  max mem: 15572
Epoch: [32]  [ 420/1404]  eta: 0:09:53  lr: 0.000011  min_lr: 0.000000  loss: 3.4899 (3.5578)  loss_scale: 32768.0000 (35492.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0386  max mem: 15572
Epoch: [32]  [ 430/1404]  eta: 0:09:46  lr: 0.000011  min_lr: 0.000000  loss: 3.2769 (3.5532)  loss_scale: 32768.0000 (35428.9745)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0006  max mem: 15572
Epoch: [32]  [ 440/1404]  eta: 0:09:39  lr: 0.000011  min_lr: 0.000000  loss: 3.3773 (3.5541)  loss_scale: 32768.0000 (35368.6349)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0160  max mem: 15572
Epoch: [32]  [ 450/1404]  eta: 0:09:34  lr: 0.000011  min_lr: 0.000000  loss: 3.3773 (3.5460)  loss_scale: 32768.0000 (35310.9712)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0569  max mem: 15572
Epoch: [32]  [ 460/1404]  eta: 0:09:28  lr: 0.000011  min_lr: 0.000000  loss: 3.4237 (3.5511)  loss_scale: 32768.0000 (35255.8091)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1086  max mem: 15572
Epoch: [32]  [ 470/1404]  eta: 0:09:22  lr: 0.000011  min_lr: 0.000000  loss: 3.7901 (3.5525)  loss_scale: 32768.0000 (35202.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1156  max mem: 15572
Epoch: [32]  [ 480/1404]  eta: 0:09:15  lr: 0.000011  min_lr: 0.000000  loss: 3.7009 (3.5533)  loss_scale: 32768.0000 (35152.3659)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0611  max mem: 15572
Epoch: [32]  [ 490/1404]  eta: 0:09:11  lr: 0.000011  min_lr: 0.000000  loss: 3.5697 (3.5522)  loss_scale: 32768.0000 (35103.8045)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0131  max mem: 15572
[2025-01-10 23:44:25,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:44:25,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:44:25,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:44:25,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 500/1404]  eta: 0:09:04  lr: 0.000011  min_lr: 0.000000  loss: 3.5404 (3.5546)  loss_scale: 32768.0000 (35318.8024)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.0007  max mem: 15572
[2025-01-10 23:44:29,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45431
[2025-01-10 23:44:29,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:44:29,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45431
[2025-01-10 23:44:29,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:44:29,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 510/1404]  eta: 0:08:58  lr: 0.000011  min_lr: 0.000000  loss: 3.7271 (3.5578)  loss_scale: 32768.0000 (35397.1350)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0010  max mem: 15572
Epoch: [32]  [ 520/1404]  eta: 0:08:51  lr: 0.000011  min_lr: 0.000000  loss: 3.6748 (3.5544)  loss_scale: 32768.0000 (35346.6718)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0012  max mem: 15572
Epoch: [32]  [ 530/1404]  eta: 0:08:45  lr: 0.000011  min_lr: 0.000000  loss: 3.4820 (3.5543)  loss_scale: 32768.0000 (35298.1092)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0216  max mem: 15572
Epoch: [32]  [ 540/1404]  eta: 0:08:39  lr: 0.000011  min_lr: 0.000000  loss: 3.4820 (3.5550)  loss_scale: 32768.0000 (35251.3420)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0214  max mem: 15572
Epoch: [32]  [ 550/1404]  eta: 0:08:34  lr: 0.000011  min_lr: 0.000000  loss: 3.4255 (3.5564)  loss_scale: 32768.0000 (35206.2722)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0490  max mem: 15572
Epoch: [32]  [ 560/1404]  eta: 0:08:28  lr: 0.000011  min_lr: 0.000000  loss: 3.7327 (3.5618)  loss_scale: 32768.0000 (35162.8093)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0679  max mem: 15572
Epoch: [32]  [ 570/1404]  eta: 0:08:22  lr: 0.000011  min_lr: 0.000000  loss: 3.6892 (3.5568)  loss_scale: 32768.0000 (35120.8687)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0887  max mem: 15572
Epoch: [32]  [ 580/1404]  eta: 0:08:16  lr: 0.000011  min_lr: 0.000000  loss: 3.6082 (3.5561)  loss_scale: 32768.0000 (35080.3718)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1323  max mem: 15572
Epoch: [32]  [ 590/1404]  eta: 0:08:10  lr: 0.000011  min_lr: 0.000000  loss: 3.6290 (3.5565)  loss_scale: 32768.0000 (35041.2453)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0948  max mem: 15572
Epoch: [32]  [ 600/1404]  eta: 0:08:04  lr: 0.000011  min_lr: 0.000000  loss: 3.4782 (3.5570)  loss_scale: 32768.0000 (35003.4210)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0906  max mem: 15572
Epoch: [32]  [ 610/1404]  eta: 0:07:57  lr: 0.000010  min_lr: 0.000000  loss: 3.3388 (3.5505)  loss_scale: 32768.0000 (34966.8347)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0650  max mem: 15572
Epoch: [32]  [ 620/1404]  eta: 0:07:52  lr: 0.000010  min_lr: 0.000000  loss: 3.3832 (3.5501)  loss_scale: 32768.0000 (34931.4267)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0933  max mem: 15572
Epoch: [32]  [ 630/1404]  eta: 0:07:46  lr: 0.000010  min_lr: 0.000000  loss: 3.8089 (3.5526)  loss_scale: 32768.0000 (34897.1410)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1171  max mem: 15572
[2025-01-10 23:45:46,880] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:45:46,881] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:45:46,928] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:45:46,929] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 640/1404]  eta: 0:07:38  lr: 0.000010  min_lr: 0.000000  loss: 3.7116 (3.5516)  loss_scale: 32768.0000 (35324.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0306  max mem: 15572
[2025-01-10 23:45:56,898] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45576
[2025-01-10 23:45:56,898] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:45:56,898] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:45:56,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45576
[2025-01-10 23:45:56,900] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [ 650/1404]  eta: 0:07:34  lr: 0.000010  min_lr: 0.000000  loss: 3.3183 (3.5493)  loss_scale: 65536.0000 (35637.0876)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1000  max mem: 15572
Epoch: [32]  [ 660/1404]  eta: 0:07:27  lr: 0.000010  min_lr: 0.000000  loss: 3.4333 (3.5496)  loss_scale: 32768.0000 (35593.6823)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1078  max mem: 15572
Epoch: [32]  [ 670/1404]  eta: 0:07:20  lr: 0.000010  min_lr: 0.000000  loss: 3.6259 (3.5501)  loss_scale: 32768.0000 (35551.5708)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0389  max mem: 15572
Epoch: [32]  [ 680/1404]  eta: 0:07:14  lr: 0.000010  min_lr: 0.000000  loss: 3.5131 (3.5474)  loss_scale: 32768.0000 (35510.6960)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0627  max mem: 15572
Epoch: [32]  [ 690/1404]  eta: 0:07:08  lr: 0.000010  min_lr: 0.000000  loss: 3.6445 (3.5514)  loss_scale: 32768.0000 (35471.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0444  max mem: 15572
Epoch: [32]  [ 700/1404]  eta: 0:07:02  lr: 0.000010  min_lr: 0.000000  loss: 3.5658 (3.5463)  loss_scale: 32768.0000 (35432.4451)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0444  max mem: 15572
Epoch: [32]  [ 710/1404]  eta: 0:06:55  lr: 0.000010  min_lr: 0.000000  loss: 3.2425 (3.5455)  loss_scale: 32768.0000 (35394.9705)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0667  max mem: 15572
Epoch: [32]  [ 720/1404]  eta: 0:06:49  lr: 0.000010  min_lr: 0.000000  loss: 3.5348 (3.5443)  loss_scale: 32768.0000 (35358.5354)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0673  max mem: 15572
Epoch: [32]  [ 730/1404]  eta: 0:06:43  lr: 0.000010  min_lr: 0.000000  loss: 3.3921 (3.5434)  loss_scale: 32768.0000 (35323.0971)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1005  max mem: 15572
Epoch: [32]  [ 740/1404]  eta: 0:06:37  lr: 0.000010  min_lr: 0.000000  loss: 3.9206 (3.5479)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0952  max mem: 15572
Epoch: [32]  [ 750/1404]  eta: 0:06:30  lr: 0.000010  min_lr: 0.000000  loss: 3.9206 (3.5494)  loss_scale: 32768.0000 (35255.0519)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0336  max mem: 15572
Epoch: [32]  [ 760/1404]  eta: 0:06:24  lr: 0.000010  min_lr: 0.000000  loss: 3.8347 (3.5539)  loss_scale: 32768.0000 (35222.3706)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0180  max mem: 15572
Epoch: [32]  [ 770/1404]  eta: 0:06:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8396 (3.5547)  loss_scale: 32768.0000 (35190.5370)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1047  max mem: 15572
[2025-01-10 23:47:10,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:47:10,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:47:10,354] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:47:10,354] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 780/1404]  eta: 0:06:12  lr: 0.000010  min_lr: 0.000000  loss: 3.4995 (3.5534)  loss_scale: 32768.0000 (35327.3444)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0935  max mem: 15572
[2025-01-10 23:47:16,700] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45715
[2025-01-10 23:47:16,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:47:16,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45715
[2025-01-10 23:47:16,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:47:16,726] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 790/1404]  eta: 0:06:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5896 (3.5538)  loss_scale: 32768.0000 (35543.5449)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0682  max mem: 15572
Epoch: [32]  [ 800/1404]  eta: 0:06:00  lr: 0.000010  min_lr: 0.000000  loss: 3.6857 (3.5561)  loss_scale: 32768.0000 (35508.8939)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0761  max mem: 15572
Epoch: [32]  [ 810/1404]  eta: 0:05:54  lr: 0.000010  min_lr: 0.000000  loss: 3.5569 (3.5557)  loss_scale: 32768.0000 (35475.0974)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0087  max mem: 15572
Epoch: [32]  [ 820/1404]  eta: 0:05:48  lr: 0.000010  min_lr: 0.000000  loss: 3.5738 (3.5570)  loss_scale: 32768.0000 (35442.1242)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0317  max mem: 15572
Epoch: [32]  [ 830/1404]  eta: 0:05:42  lr: 0.000010  min_lr: 0.000000  loss: 3.5888 (3.5540)  loss_scale: 32768.0000 (35409.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0318  max mem: 15572
Epoch: [32]  [ 840/1404]  eta: 0:05:36  lr: 0.000010  min_lr: 0.000000  loss: 3.4405 (3.5535)  loss_scale: 32768.0000 (35378.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0141  max mem: 15572
Epoch: [32]  [ 850/1404]  eta: 0:05:31  lr: 0.000010  min_lr: 0.000000  loss: 3.5657 (3.5551)  loss_scale: 32768.0000 (35347.8543)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.0139  max mem: 15572
Epoch: [32]  [ 860/1404]  eta: 0:05:24  lr: 0.000010  min_lr: 0.000000  loss: 3.7233 (3.5569)  loss_scale: 32768.0000 (35317.8908)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0007  max mem: 15572
Epoch: [32]  [ 870/1404]  eta: 0:05:18  lr: 0.000010  min_lr: 0.000000  loss: 3.5443 (3.5533)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0007  max mem: 15572
Epoch: [32]  [ 880/1404]  eta: 0:05:12  lr: 0.000010  min_lr: 0.000000  loss: 3.3994 (3.5509)  loss_scale: 32768.0000 (35260.0045)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0010  max mem: 15572
Epoch: [32]  [ 890/1404]  eta: 0:05:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5364 (3.5511)  loss_scale: 32768.0000 (35232.0359)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0009  max mem: 15572
Epoch: [32]  [ 900/1404]  eta: 0:04:59  lr: 0.000010  min_lr: 0.000000  loss: 3.7170 (3.5529)  loss_scale: 32768.0000 (35204.6881)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0007  max mem: 15572
[2025-01-10 23:48:23,268] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45830
[2025-01-10 23:48:23,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 23:48:23,271] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45830
[2025-01-10 23:48:23,272] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-10 23:48:23,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 910/1404]  eta: 0:04:54  lr: 0.000010  min_lr: 0.000000  loss: 3.7993 (3.5530)  loss_scale: 32768.0000 (35016.0790)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0010  max mem: 15572
Epoch: [32]  [ 920/1404]  eta: 0:04:48  lr: 0.000010  min_lr: 0.000000  loss: 3.5206 (3.5525)  loss_scale: 16384.0000 (34813.7763)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0109  max mem: 15572
Epoch: [32]  [ 930/1404]  eta: 0:04:42  lr: 0.000010  min_lr: 0.000000  loss: 3.4766 (3.5518)  loss_scale: 16384.0000 (34615.8195)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.0108  max mem: 15572
Epoch: [32]  [ 940/1404]  eta: 0:04:36  lr: 0.000010  min_lr: 0.000000  loss: 3.4409 (3.5481)  loss_scale: 16384.0000 (34422.0701)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0008  max mem: 15572
Epoch: [32]  [ 950/1404]  eta: 0:04:30  lr: 0.000010  min_lr: 0.000000  loss: 3.2533 (3.5504)  loss_scale: 16384.0000 (34232.3954)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0008  max mem: 15572
Epoch: [32]  [ 960/1404]  eta: 0:04:24  lr: 0.000010  min_lr: 0.000000  loss: 3.4401 (3.5494)  loss_scale: 16384.0000 (34046.6681)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0008  max mem: 15572
Epoch: [32]  [ 970/1404]  eta: 0:04:18  lr: 0.000010  min_lr: 0.000000  loss: 3.4473 (3.5510)  loss_scale: 16384.0000 (33864.7662)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.0009  max mem: 15572
Epoch: [32]  [ 980/1404]  eta: 0:04:12  lr: 0.000010  min_lr: 0.000000  loss: 3.5851 (3.5504)  loss_scale: 16384.0000 (33686.5729)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0360  max mem: 15572
Epoch: [32]  [ 990/1404]  eta: 0:04:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5851 (3.5500)  loss_scale: 16384.0000 (33511.9758)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0383  max mem: 15572
Epoch: [32]  [1000/1404]  eta: 0:04:00  lr: 0.000010  min_lr: 0.000000  loss: 3.5412 (3.5497)  loss_scale: 16384.0000 (33340.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0031  max mem: 15572
Epoch: [32]  [1010/1404]  eta: 0:03:54  lr: 0.000010  min_lr: 0.000000  loss: 3.7194 (3.5521)  loss_scale: 16384.0000 (33173.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0006  max mem: 15572
Epoch: [32]  [1020/1404]  eta: 0:03:48  lr: 0.000010  min_lr: 0.000000  loss: 3.7496 (3.5538)  loss_scale: 16384.0000 (33008.7052)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0009  max mem: 15572
Epoch: [32]  [1030/1404]  eta: 0:03:42  lr: 0.000010  min_lr: 0.000000  loss: 3.5470 (3.5536)  loss_scale: 16384.0000 (32847.4568)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0009  max mem: 15572
[2025-01-10 23:49:39,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:49:39,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-10 23:49:39,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:49:39,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [1040/1404]  eta: 0:03:36  lr: 0.000010  min_lr: 0.000000  loss: 3.5766 (3.5541)  loss_scale: 16384.0000 (32846.6936)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0009  max mem: 15572
Epoch: [32]  [1050/1404]  eta: 0:03:30  lr: 0.000010  min_lr: 0.000000  loss: 3.6505 (3.5551)  loss_scale: 32768.0000 (32845.9448)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0009  max mem: 15572
Epoch: [32]  [1060/1404]  eta: 0:03:24  lr: 0.000010  min_lr: 0.000000  loss: 3.6006 (3.5526)  loss_scale: 32768.0000 (32845.2102)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0009  max mem: 15572
Epoch: [32]  [1070/1404]  eta: 0:03:18  lr: 0.000010  min_lr: 0.000000  loss: 3.6006 (3.5544)  loss_scale: 32768.0000 (32844.4893)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0009  max mem: 15572
[2025-01-10 23:50:03,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=313, lr=[9.332009295061709e-08, 9.332009295061709e-08, 1.3331441850088156e-07, 1.3331441850088156e-07, 1.904491692869737e-07, 1.904491692869737e-07, 2.7207024183853387e-07, 2.7207024183853387e-07, 3.886717740550484e-07, 3.886717740550484e-07, 5.55245391507212e-07, 5.55245391507212e-07, 7.9320770215316e-07, 7.9320770215316e-07, 1.1331538602188002e-06, 1.1331538602188002e-06, 1.6187912288840002e-06, 1.6187912288840002e-06, 2.3125588984057147e-06, 2.3125588984057147e-06, 3.303655569151021e-06, 3.303655569151021e-06, 4.719507955930031e-06, 4.719507955930031e-06, 6.742154222757187e-06, 6.742154222757187e-06, 9.631648889653125e-06, 9.631648889653125e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-10 23:50:03,972] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=45.78138464361737, CurrSamplesPerSec=62.39148538440847, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [1080/1404]  eta: 0:03:12  lr: 0.000010  min_lr: 0.000000  loss: 3.6630 (3.5542)  loss_scale: 32768.0000 (32843.7817)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0007  max mem: 15572
Epoch: [32]  [1090/1404]  eta: 0:03:06  lr: 0.000010  min_lr: 0.000000  loss: 3.5964 (3.5543)  loss_scale: 32768.0000 (32843.0871)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0008  max mem: 15572
Epoch: [32]  [1100/1404]  eta: 0:03:00  lr: 0.000010  min_lr: 0.000000  loss: 3.5580 (3.5543)  loss_scale: 32768.0000 (32842.4051)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0009  max mem: 15572
Epoch: [32]  [1110/1404]  eta: 0:02:54  lr: 0.000010  min_lr: 0.000000  loss: 3.6094 (3.5536)  loss_scale: 32768.0000 (32841.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0008  max mem: 15572
Epoch: [32]  [1120/1404]  eta: 0:02:48  lr: 0.000010  min_lr: 0.000000  loss: 3.3876 (3.5516)  loss_scale: 32768.0000 (32841.0776)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0009  max mem: 15572
Epoch: [32]  [1130/1404]  eta: 0:02:42  lr: 0.000010  min_lr: 0.000000  loss: 3.3520 (3.5503)  loss_scale: 32768.0000 (32840.4315)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0011  max mem: 15572
Epoch: [32]  [1140/1404]  eta: 0:02:37  lr: 0.000010  min_lr: 0.000000  loss: 3.4183 (3.5495)  loss_scale: 32768.0000 (32839.7967)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0012  max mem: 15572
Epoch: [32]  [1150/1404]  eta: 0:02:31  lr: 0.000009  min_lr: 0.000000  loss: 3.5560 (3.5502)  loss_scale: 32768.0000 (32839.1729)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.0010  max mem: 15572
[2025-01-10 23:50:55,632] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:50:55,633] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:50:55,635] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:50:55,635] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1160/1404]  eta: 0:02:25  lr: 0.000009  min_lr: 0.000000  loss: 3.6985 (3.5506)  loss_scale: 32768.0000 (32895.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0009  max mem: 15572
Epoch: [32]  [1170/1404]  eta: 0:02:19  lr: 0.000009  min_lr: 0.000000  loss: 3.7928 (3.5500)  loss_scale: 65536.0000 (33173.7523)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0008  max mem: 15572
[2025-01-10 23:51:02,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46099
[2025-01-10 23:51:02,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:51:02,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46099
[2025-01-10 23:51:02,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:51:02,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1180/1404]  eta: 0:02:13  lr: 0.000009  min_lr: 0.000000  loss: 3.6439 (3.5508)  loss_scale: 32768.0000 (33170.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0008  max mem: 15572
Epoch: [32]  [1190/1404]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000000  loss: 3.7626 (3.5532)  loss_scale: 32768.0000 (33166.9387)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0008  max mem: 15572
Epoch: [32]  [1200/1404]  eta: 0:02:01  lr: 0.000009  min_lr: 0.000000  loss: 3.8092 (3.5532)  loss_scale: 32768.0000 (33163.6170)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0009  max mem: 15572
Epoch: [32]  [1210/1404]  eta: 0:01:55  lr: 0.000009  min_lr: 0.000000  loss: 3.4470 (3.5528)  loss_scale: 32768.0000 (33160.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.0075  max mem: 15572
Epoch: [32]  [1220/1404]  eta: 0:01:49  lr: 0.000009  min_lr: 0.000000  loss: 3.6779 (3.5539)  loss_scale: 32768.0000 (33157.1368)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0078  max mem: 15572
Epoch: [32]  [1230/1404]  eta: 0:01:43  lr: 0.000009  min_lr: 0.000000  loss: 3.6876 (3.5535)  loss_scale: 32768.0000 (33153.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0012  max mem: 15572
Epoch: [32]  [1240/1404]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000000  loss: 3.5641 (3.5535)  loss_scale: 32768.0000 (33150.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0008  max mem: 15572
Epoch: [32]  [1250/1404]  eta: 0:01:31  lr: 0.000009  min_lr: 0.000000  loss: 3.5641 (3.5535)  loss_scale: 32768.0000 (33147.8050)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0201  max mem: 15572
Epoch: [32]  [1260/1404]  eta: 0:01:25  lr: 0.000009  min_lr: 0.000000  loss: 3.6193 (3.5548)  loss_scale: 32768.0000 (33144.7930)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0579  max mem: 15572
Epoch: [32]  [1270/1404]  eta: 0:01:19  lr: 0.000009  min_lr: 0.000000  loss: 3.7728 (3.5554)  loss_scale: 32768.0000 (33141.8285)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0734  max mem: 15572
Epoch: [32]  [1280/1404]  eta: 0:01:13  lr: 0.000009  min_lr: 0.000000  loss: 3.5069 (3.5551)  loss_scale: 32768.0000 (33138.9102)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0866  max mem: 15572
Epoch: [32]  [1290/1404]  eta: 0:01:07  lr: 0.000009  min_lr: 0.000000  loss: 3.5892 (3.5551)  loss_scale: 32768.0000 (33136.0372)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1213  max mem: 15572
[2025-01-10 23:52:18,021] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:52:18,021] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:52:18,021] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:52:18,021] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1300/1404]  eta: 0:01:01  lr: 0.000009  min_lr: 0.000000  loss: 3.6947 (3.5561)  loss_scale: 32768.0000 (33158.3951)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0828  max mem: 15572
[2025-01-10 23:52:22,826] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46235
[2025-01-10 23:52:22,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:52:22,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:52:22,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46235
[2025-01-10 23:52:22,828] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [1310/1404]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 3.5381 (3.5542)  loss_scale: 32768.0000 (33305.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0784  max mem: 15572
Epoch: [32]  [1320/1404]  eta: 0:00:49  lr: 0.000009  min_lr: 0.000000  loss: 3.4999 (3.5538)  loss_scale: 32768.0000 (33301.3172)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1175  max mem: 15572
Epoch: [32]  [1330/1404]  eta: 0:00:43  lr: 0.000009  min_lr: 0.000000  loss: 3.6363 (3.5554)  loss_scale: 32768.0000 (33297.3103)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1238  max mem: 15572
Epoch: [32]  [1340/1404]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 3.5805 (3.5544)  loss_scale: 32768.0000 (33293.3632)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0723  max mem: 15572
Epoch: [32]  [1350/1404]  eta: 0:00:32  lr: 0.000009  min_lr: 0.000000  loss: 3.3961 (3.5538)  loss_scale: 32768.0000 (33289.4745)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0822  max mem: 15572
Epoch: [32]  [1360/1404]  eta: 0:00:26  lr: 0.000009  min_lr: 0.000000  loss: 3.7065 (3.5558)  loss_scale: 32768.0000 (33285.6429)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1103  max mem: 15572
Epoch: [32]  [1370/1404]  eta: 0:00:20  lr: 0.000009  min_lr: 0.000000  loss: 3.8100 (3.5568)  loss_scale: 32768.0000 (33281.8673)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0624  max mem: 15572
Epoch: [32]  [1380/1404]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 3.6131 (3.5542)  loss_scale: 32768.0000 (33278.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0343  max mem: 15572
Epoch: [32]  [1390/1404]  eta: 0:00:08  lr: 0.000009  min_lr: 0.000000  loss: 3.5736 (3.5553)  loss_scale: 32768.0000 (33274.4788)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0014  max mem: 15572
Epoch: [32]  [1400/1404]  eta: 0:00:02  lr: 0.000009  min_lr: 0.000000  loss: 3.6882 (3.5558)  loss_scale: 32768.0000 (33270.8637)  weight_decay: 0.0500 (0.0500)  time: 0.4495  data: 0.0011  max mem: 15572
Epoch: [32]  [1403/1404]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 3.5736 (3.5558)  loss_scale: 32768.0000 (33269.7892)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0009  max mem: 15572
Epoch: [32] Total time: 0:13:50 (0.5912 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 3.5736 (3.5573)  loss_scale: 32768.0000 (33269.7892)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:22  loss: 1.4126 (1.4126)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.1381  data: 3.9410  max mem: 15572
Val:  [ 10/136]  eta: 0:01:41  loss: 1.9440 (1.9475)  acc1: 55.5556 (53.0303)  acc5: 83.3333 (83.3333)  time: 0.8050  data: 0.6004  max mem: 15572
Val:  [ 20/136]  eta: 0:01:03  loss: 2.2311 (2.1260)  acc1: 44.4444 (47.8836)  acc5: 77.7778 (80.4233)  time: 0.3678  data: 0.1669  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.2217 (2.0243)  acc1: 44.4444 (51.0753)  acc5: 83.3333 (81.3620)  time: 0.3138  data: 0.1160  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.7918 (1.9979)  acc1: 61.1111 (52.9810)  acc5: 83.3333 (81.9783)  time: 0.3810  data: 0.1727  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8552 (1.9955)  acc1: 55.5556 (53.1590)  acc5: 83.3333 (82.4619)  time: 0.3885  data: 0.1792  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1421 (2.0931)  acc1: 44.4444 (49.7268)  acc5: 77.7778 (80.8743)  time: 0.3659  data: 0.1626  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.0449 (2.0638)  acc1: 44.4444 (50.3912)  acc5: 77.7778 (80.8294)  time: 0.3611  data: 0.1560  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.7998 (2.0530)  acc1: 50.0000 (50.6859)  acc5: 88.8889 (81.2757)  time: 0.3646  data: 0.1584  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.0096 (2.0631)  acc1: 50.0000 (50.1221)  acc5: 83.3333 (81.1355)  time: 0.3638  data: 0.1579  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3253 (2.1355)  acc1: 38.8889 (48.5149)  acc5: 72.2222 (79.2079)  time: 0.3831  data: 0.1883  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2956 (2.1259)  acc1: 44.4444 (48.8488)  acc5: 72.2222 (79.0290)  time: 0.3928  data: 0.1969  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8219 (2.0765)  acc1: 55.5556 (50.0000)  acc5: 83.3333 (79.7980)  time: 0.3499  data: 0.1428  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5121 (2.0345)  acc1: 61.1111 (51.1450)  acc5: 88.8889 (80.4919)  time: 0.2277  data: 0.0527  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.6310 (2.0338)  acc1: 61.1111 (51.3104)  acc5: 88.8889 (80.5897)  time: 0.1954  data: 0.0477  max mem: 15572
Val: Total time: 0:00:50 (0.3728 s / it)
* Acc@1 50.491 Acc@5 79.279 loss 2.079
Accuracy of the network on the 4883 val videos: 50.5%
[2025-01-10 23:54:06,423] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-10 23:54:06,425] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-10 23:54:06,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-10 23:54:06,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-10 23:54:08,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-10 23:54:08,860] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.49%
Epoch: [33]  [   0/1404]  eta: 3:06:24  lr: 0.000009  min_lr: 0.000000  loss: 3.8667 (3.8667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9664  data: 5.7276  max mem: 15572
Epoch: [33]  [  10/1404]  eta: 0:27:34  lr: 0.000009  min_lr: 0.000000  loss: 3.9373 (3.8462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1866  data: 0.5211  max mem: 15572
Epoch: [33]  [  20/1404]  eta: 0:20:32  lr: 0.000009  min_lr: 0.000000  loss: 3.6621 (3.5975)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0390  max mem: 15572
Epoch: [33]  [  30/1404]  eta: 0:18:13  lr: 0.000009  min_lr: 0.000000  loss: 3.6621 (3.5817)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0916  max mem: 15572
[2025-01-10 23:54:35,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:54:35,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:54:35,224] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:54:35,224] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [  40/1404]  eta: 0:16:49  lr: 0.000009  min_lr: 0.000000  loss: 3.7458 (3.6624)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0834  max mem: 15572
[2025-01-10 23:54:39,709] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46373
[2025-01-10 23:54:39,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:54:39,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:54:39,840] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46373
[2025-01-10 23:54:39,841] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [  50/1404]  eta: 0:16:23  lr: 0.000009  min_lr: 0.000000  loss: 3.6422 (3.6178)  loss_scale: 32768.0000 (38550.5882)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1162  max mem: 15572
Epoch: [33]  [  60/1404]  eta: 0:15:48  lr: 0.000009  min_lr: 0.000000  loss: 3.4130 (3.6303)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0861  max mem: 15572
Epoch: [33]  [  70/1404]  eta: 0:15:22  lr: 0.000009  min_lr: 0.000000  loss: 3.6722 (3.6375)  loss_scale: 32768.0000 (36921.6901)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0008  max mem: 15572
Epoch: [33]  [  80/1404]  eta: 0:14:56  lr: 0.000009  min_lr: 0.000000  loss: 3.6722 (3.6385)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0093  max mem: 15572
Epoch: [33]  [  90/1404]  eta: 0:14:35  lr: 0.000009  min_lr: 0.000000  loss: 3.7036 (3.6374)  loss_scale: 32768.0000 (36008.7912)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0121  max mem: 15572
Epoch: [33]  [ 100/1404]  eta: 0:14:20  lr: 0.000009  min_lr: 0.000000  loss: 3.6959 (3.6178)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0602  max mem: 15572
Epoch: [33]  [ 110/1404]  eta: 0:13:59  lr: 0.000009  min_lr: 0.000000  loss: 3.3571 (3.5704)  loss_scale: 32768.0000 (35424.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0733  max mem: 15572
Epoch: [33]  [ 120/1404]  eta: 0:13:44  lr: 0.000009  min_lr: 0.000000  loss: 3.4697 (3.5764)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0305  max mem: 15572
Epoch: [33]  [ 130/1404]  eta: 0:13:31  lr: 0.000009  min_lr: 0.000000  loss: 3.7171 (3.5809)  loss_scale: 32768.0000 (35019.2366)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0149  max mem: 15572
Epoch: [33]  [ 140/1404]  eta: 0:13:29  lr: 0.000009  min_lr: 0.000000  loss: 3.5814 (3.5807)  loss_scale: 32768.0000 (34859.5745)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0164  max mem: 15572
Epoch: [33]  [ 150/1404]  eta: 0:13:13  lr: 0.000009  min_lr: 0.000000  loss: 3.5362 (3.5679)  loss_scale: 32768.0000 (34721.0596)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0166  max mem: 15572
Epoch: [33]  [ 160/1404]  eta: 0:13:03  lr: 0.000009  min_lr: 0.000000  loss: 3.2832 (3.5632)  loss_scale: 32768.0000 (34599.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0016  max mem: 15572
[2025-01-10 23:55:56,390] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:55:56,390] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:55:56,514] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:55:56,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 170/1404]  eta: 0:12:56  lr: 0.000009  min_lr: 0.000000  loss: 3.4890 (3.5655)  loss_scale: 32768.0000 (34684.2573)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0012  max mem: 15572
[2025-01-10 23:56:01,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46511
[2025-01-10 23:56:01,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:56:01,438] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46511
[2025-01-10 23:56:01,438] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:56:01,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 180/1404]  eta: 0:12:50  lr: 0.000009  min_lr: 0.000000  loss: 3.3982 (3.5497)  loss_scale: 32768.0000 (36026.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.0008  max mem: 15572
Epoch: [33]  [ 190/1404]  eta: 0:12:41  lr: 0.000009  min_lr: 0.000000  loss: 3.1725 (3.5444)  loss_scale: 32768.0000 (35856.0838)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0009  max mem: 15572
Epoch: [33]  [ 200/1404]  eta: 0:12:29  lr: 0.000009  min_lr: 0.000000  loss: 3.6413 (3.5499)  loss_scale: 32768.0000 (35702.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0008  max mem: 15572
Epoch: [33]  [ 210/1404]  eta: 0:12:21  lr: 0.000009  min_lr: 0.000000  loss: 3.5154 (3.5437)  loss_scale: 32768.0000 (35563.3744)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0077  max mem: 15572
Epoch: [33]  [ 220/1404]  eta: 0:12:12  lr: 0.000009  min_lr: 0.000000  loss: 3.6850 (3.5540)  loss_scale: 32768.0000 (35436.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0246  max mem: 15572
Epoch: [33]  [ 230/1404]  eta: 0:12:03  lr: 0.000009  min_lr: 0.000000  loss: 3.6850 (3.5497)  loss_scale: 32768.0000 (35321.3506)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0469  max mem: 15572
Epoch: [33]  [ 240/1404]  eta: 0:11:58  lr: 0.000009  min_lr: 0.000000  loss: 3.4456 (3.5457)  loss_scale: 32768.0000 (35215.4025)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0651  max mem: 15572
Epoch: [33]  [ 250/1404]  eta: 0:11:49  lr: 0.000009  min_lr: 0.000000  loss: 3.6555 (3.5606)  loss_scale: 32768.0000 (35117.8964)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0642  max mem: 15572
Epoch: [33]  [ 260/1404]  eta: 0:11:43  lr: 0.000009  min_lr: 0.000000  loss: 3.6555 (3.5547)  loss_scale: 32768.0000 (35027.8621)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0912  max mem: 15572
Epoch: [33]  [ 270/1404]  eta: 0:11:39  lr: 0.000009  min_lr: 0.000000  loss: 3.4860 (3.5520)  loss_scale: 32768.0000 (34944.4723)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1000  max mem: 15572
Epoch: [33]  [ 280/1404]  eta: 0:11:31  lr: 0.000009  min_lr: 0.000000  loss: 3.4923 (3.5523)  loss_scale: 32768.0000 (34867.0178)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0772  max mem: 15572
Epoch: [33]  [ 290/1404]  eta: 0:11:22  lr: 0.000009  min_lr: 0.000000  loss: 3.1714 (3.5343)  loss_scale: 32768.0000 (34794.8866)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0399  max mem: 15572
Epoch: [33]  [ 300/1404]  eta: 0:11:16  lr: 0.000009  min_lr: 0.000000  loss: 3.2398 (3.5317)  loss_scale: 32768.0000 (34727.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0396  max mem: 15572
[2025-01-10 23:57:18,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:57:18,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:57:18,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:57:18,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:57:19,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46642
[2025-01-10 23:57:19,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:57:19,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:57:19,516] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46642
[2025-01-10 23:57:19,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [ 310/1404]  eta: 0:11:10  lr: 0.000008  min_lr: 0.000000  loss: 3.6582 (3.5431)  loss_scale: 32768.0000 (34875.2669)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0927  max mem: 15572
Epoch: [33]  [ 320/1404]  eta: 0:11:00  lr: 0.000008  min_lr: 0.000000  loss: 3.7224 (3.5397)  loss_scale: 32768.0000 (34809.6199)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0538  max mem: 15572
Epoch: [33]  [ 330/1404]  eta: 0:10:55  lr: 0.000008  min_lr: 0.000000  loss: 3.7158 (3.5396)  loss_scale: 32768.0000 (34747.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0576  max mem: 15572
Epoch: [33]  [ 340/1404]  eta: 0:10:46  lr: 0.000008  min_lr: 0.000000  loss: 3.7158 (3.5422)  loss_scale: 32768.0000 (34689.8768)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.0576  max mem: 15572
Epoch: [33]  [ 350/1404]  eta: 0:10:41  lr: 0.000008  min_lr: 0.000000  loss: 3.6120 (3.5424)  loss_scale: 32768.0000 (34635.1225)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0455  max mem: 15572
Epoch: [33]  [ 360/1404]  eta: 0:10:36  lr: 0.000008  min_lr: 0.000000  loss: 3.3720 (3.5427)  loss_scale: 32768.0000 (34583.4017)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0458  max mem: 15572
Epoch: [33]  [ 370/1404]  eta: 0:10:30  lr: 0.000008  min_lr: 0.000000  loss: 3.4579 (3.5385)  loss_scale: 32768.0000 (34534.4690)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0010  max mem: 15572
Epoch: [33]  [ 380/1404]  eta: 0:10:22  lr: 0.000008  min_lr: 0.000000  loss: 3.4579 (3.5370)  loss_scale: 32768.0000 (34488.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0008  max mem: 15572
Epoch: [33]  [ 390/1404]  eta: 0:10:16  lr: 0.000008  min_lr: 0.000000  loss: 3.5990 (3.5393)  loss_scale: 32768.0000 (34444.1125)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0008  max mem: 15572
Epoch: [33]  [ 400/1404]  eta: 0:10:11  lr: 0.000008  min_lr: 0.000000  loss: 3.3609 (3.5301)  loss_scale: 32768.0000 (34402.3142)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0009  max mem: 15572
Epoch: [33]  [ 410/1404]  eta: 0:10:03  lr: 0.000008  min_lr: 0.000000  loss: 3.2671 (3.5274)  loss_scale: 32768.0000 (34362.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0012  max mem: 15572
Epoch: [33]  [ 420/1404]  eta: 0:09:57  lr: 0.000008  min_lr: 0.000000  loss: 3.2522 (3.5235)  loss_scale: 32768.0000 (34324.6746)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0012  max mem: 15572
Epoch: [33]  [ 430/1404]  eta: 0:09:49  lr: 0.000008  min_lr: 0.000000  loss: 3.4247 (3.5265)  loss_scale: 32768.0000 (34288.5568)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0008  max mem: 15572
[2025-01-10 23:58:35,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:58:35,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:58:35,479] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:58:35,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 440/1404]  eta: 0:09:43  lr: 0.000008  min_lr: 0.000000  loss: 3.6783 (3.5308)  loss_scale: 32768.0000 (34402.6848)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0011  max mem: 15572
[2025-01-10 23:58:39,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46779
[2025-01-10 23:58:39,451] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:58:39,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-10 23:58:39,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46779
[2025-01-10 23:58:39,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [ 450/1404]  eta: 0:09:36  lr: 0.000008  min_lr: 0.000000  loss: 3.6917 (3.5346)  loss_scale: 32768.0000 (34802.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0012  max mem: 15572
Epoch: [33]  [ 460/1404]  eta: 0:09:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6917 (3.5375)  loss_scale: 32768.0000 (34758.2473)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0007  max mem: 15572
Epoch: [33]  [ 470/1404]  eta: 0:09:25  lr: 0.000008  min_lr: 0.000000  loss: 3.7131 (3.5418)  loss_scale: 32768.0000 (34715.9915)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0007  max mem: 15572
Epoch: [33]  [ 480/1404]  eta: 0:09:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7131 (3.5408)  loss_scale: 32768.0000 (34675.4927)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0008  max mem: 15572
Epoch: [33]  [ 490/1404]  eta: 0:09:10  lr: 0.000008  min_lr: 0.000000  loss: 3.7011 (3.5474)  loss_scale: 32768.0000 (34636.6436)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0007  max mem: 15572
Epoch: [33]  [ 500/1404]  eta: 0:09:02  lr: 0.000008  min_lr: 0.000000  loss: 3.7396 (3.5510)  loss_scale: 32768.0000 (34599.3453)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0006  max mem: 15572
Epoch: [33]  [ 510/1404]  eta: 0:08:57  lr: 0.000008  min_lr: 0.000000  loss: 3.7396 (3.5558)  loss_scale: 32768.0000 (34563.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0005  max mem: 15572
Epoch: [33]  [ 520/1404]  eta: 0:08:50  lr: 0.000008  min_lr: 0.000000  loss: 3.5105 (3.5513)  loss_scale: 32768.0000 (34529.0441)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0319  max mem: 15572
Epoch: [33]  [ 530/1404]  eta: 0:08:44  lr: 0.000008  min_lr: 0.000000  loss: 3.4885 (3.5508)  loss_scale: 32768.0000 (34495.8795)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0320  max mem: 15572
Epoch: [33]  [ 540/1404]  eta: 0:08:38  lr: 0.000008  min_lr: 0.000000  loss: 3.6422 (3.5508)  loss_scale: 32768.0000 (34463.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0310  max mem: 15572
Epoch: [33]  [ 550/1404]  eta: 0:08:34  lr: 0.000008  min_lr: 0.000000  loss: 3.5101 (3.5459)  loss_scale: 32768.0000 (34433.1615)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0466  max mem: 15572
Epoch: [33]  [ 560/1404]  eta: 0:08:28  lr: 0.000008  min_lr: 0.000000  loss: 3.4596 (3.5459)  loss_scale: 32768.0000 (34403.4795)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.0844  max mem: 15572
Epoch: [33]  [ 570/1404]  eta: 0:08:21  lr: 0.000008  min_lr: 0.000000  loss: 3.3312 (3.5409)  loss_scale: 32768.0000 (34374.8371)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0690  max mem: 15572
[2025-01-10 23:59:55,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:59:55,315] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-10 23:59:55,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-10 23:59:55,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 580/1404]  eta: 0:08:14  lr: 0.000008  min_lr: 0.000000  loss: 3.1549 (3.5355)  loss_scale: 32768.0000 (34629.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0009  max mem: 15572
[2025-01-10 23:59:58,575] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46913
[2025-01-10 23:59:58,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:59:58,575] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46913
[2025-01-10 23:59:58,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-10 23:59:58,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 590/1404]  eta: 0:08:08  lr: 0.000008  min_lr: 0.000000  loss: 3.3437 (3.5362)  loss_scale: 32768.0000 (34597.6853)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0006  max mem: 15572
Epoch: [33]  [ 600/1404]  eta: 0:08:04  lr: 0.000008  min_lr: 0.000000  loss: 3.5101 (3.5377)  loss_scale: 32768.0000 (34567.2413)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.1285  max mem: 15572
Epoch: [33]  [ 610/1404]  eta: 0:07:58  lr: 0.000008  min_lr: 0.000000  loss: 3.8238 (3.5421)  loss_scale: 32768.0000 (34537.7938)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.1606  max mem: 15572
Epoch: [33]  [ 620/1404]  eta: 0:07:51  lr: 0.000008  min_lr: 0.000000  loss: 3.7100 (3.5443)  loss_scale: 32768.0000 (34509.2947)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0712  max mem: 15572
Epoch: [33]  [ 630/1404]  eta: 0:07:45  lr: 0.000008  min_lr: 0.000000  loss: 3.5886 (3.5463)  loss_scale: 32768.0000 (34481.6989)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0596  max mem: 15572
Epoch: [33]  [ 640/1404]  eta: 0:07:39  lr: 0.000008  min_lr: 0.000000  loss: 3.6139 (3.5476)  loss_scale: 32768.0000 (34454.9641)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0467  max mem: 15572
Epoch: [33]  [ 650/1404]  eta: 0:07:33  lr: 0.000008  min_lr: 0.000000  loss: 3.4884 (3.5476)  loss_scale: 32768.0000 (34429.0507)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0534  max mem: 15572
Epoch: [33]  [ 660/1404]  eta: 0:07:27  lr: 0.000008  min_lr: 0.000000  loss: 3.3894 (3.5496)  loss_scale: 32768.0000 (34403.9213)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0842  max mem: 15572
[2025-01-11 00:00:50,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=320, lr=[7.652484534492424e-08, 7.652484534492424e-08, 1.0932120763560607e-07, 1.0932120763560607e-07, 1.5617315376515155e-07, 1.5617315376515155e-07, 2.2310450537878794e-07, 2.2310450537878794e-07, 3.187207219696971e-07, 3.187207219696971e-07, 4.553153170995673e-07, 4.553153170995673e-07, 6.504504529993819e-07, 6.504504529993819e-07, 9.292149328562599e-07, 9.292149328562599e-07, 1.3274499040803713e-06, 1.3274499040803713e-06, 1.896357005829102e-06, 1.896357005829102e-06, 2.7090814368987172e-06, 2.7090814368987172e-06, 3.870116338426739e-06, 3.870116338426739e-06, 5.528737626323913e-06, 5.528737626323913e-06, 7.898196609034162e-06, 7.898196609034162e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:00:50,333] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=45.786054624848376, CurrSamplesPerSec=46.02926919381913, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [ 670/1404]  eta: 0:07:21  lr: 0.000008  min_lr: 0.000000  loss: 3.6693 (3.5520)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1010  max mem: 15572
Epoch: [33]  [ 680/1404]  eta: 0:07:15  lr: 0.000008  min_lr: 0.000000  loss: 3.6410 (3.5539)  loss_scale: 32768.0000 (34355.8767)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0775  max mem: 15572
Epoch: [33]  [ 690/1404]  eta: 0:07:08  lr: 0.000008  min_lr: 0.000000  loss: 3.6871 (3.5576)  loss_scale: 32768.0000 (34332.8973)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0335  max mem: 15572
Epoch: [33]  [ 700/1404]  eta: 0:07:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7036 (3.5568)  loss_scale: 32768.0000 (34310.5735)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0008  max mem: 15572
[2025-01-11 00:01:14,625] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:01:14,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:01:14,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:01:14,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 710/1404]  eta: 0:06:55  lr: 0.000008  min_lr: 0.000000  loss: 3.6961 (3.5565)  loss_scale: 32768.0000 (34334.9648)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0065  max mem: 15572
Epoch: [33]  [ 720/1404]  eta: 0:06:50  lr: 0.000008  min_lr: 0.000000  loss: 3.6012 (3.5567)  loss_scale: 65536.0000 (34767.7115)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0855  max mem: 15572
[2025-01-11 00:01:23,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47056
[2025-01-11 00:01:23,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:01:23,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:01:23,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47056
[2025-01-11 00:01:23,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [ 730/1404]  eta: 0:06:44  lr: 0.000008  min_lr: 0.000000  loss: 3.6002 (3.5548)  loss_scale: 65536.0000 (34874.8345)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.1606  max mem: 15572
Epoch: [33]  [ 740/1404]  eta: 0:06:38  lr: 0.000008  min_lr: 0.000000  loss: 3.6037 (3.5557)  loss_scale: 32768.0000 (34846.4022)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0960  max mem: 15572
Epoch: [33]  [ 750/1404]  eta: 0:06:31  lr: 0.000008  min_lr: 0.000000  loss: 3.6090 (3.5554)  loss_scale: 32768.0000 (34818.7270)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0452  max mem: 15572
Epoch: [33]  [ 760/1404]  eta: 0:06:25  lr: 0.000008  min_lr: 0.000000  loss: 3.5996 (3.5552)  loss_scale: 32768.0000 (34791.7792)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0516  max mem: 15572
Epoch: [33]  [ 770/1404]  eta: 0:06:19  lr: 0.000008  min_lr: 0.000000  loss: 3.6749 (3.5554)  loss_scale: 32768.0000 (34765.5305)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0909  max mem: 15572
Epoch: [33]  [ 780/1404]  eta: 0:06:13  lr: 0.000008  min_lr: 0.000000  loss: 3.6749 (3.5566)  loss_scale: 32768.0000 (34739.9539)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0952  max mem: 15572
Epoch: [33]  [ 790/1404]  eta: 0:06:06  lr: 0.000008  min_lr: 0.000000  loss: 3.6108 (3.5539)  loss_scale: 32768.0000 (34715.0240)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0344  max mem: 15572
Epoch: [33]  [ 800/1404]  eta: 0:06:01  lr: 0.000008  min_lr: 0.000000  loss: 3.4192 (3.5515)  loss_scale: 32768.0000 (34690.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0576  max mem: 15572
Epoch: [33]  [ 810/1404]  eta: 0:05:54  lr: 0.000008  min_lr: 0.000000  loss: 3.4192 (3.5522)  loss_scale: 32768.0000 (34667.0086)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0712  max mem: 15572
Epoch: [33]  [ 820/1404]  eta: 0:05:48  lr: 0.000008  min_lr: 0.000000  loss: 3.4662 (3.5510)  loss_scale: 32768.0000 (34643.8782)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0319  max mem: 15572
Epoch: [33]  [ 830/1404]  eta: 0:05:42  lr: 0.000008  min_lr: 0.000000  loss: 3.5461 (3.5543)  loss_scale: 32768.0000 (34621.3045)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0636  max mem: 15572
Epoch: [33]  [ 840/1404]  eta: 0:05:36  lr: 0.000008  min_lr: 0.000000  loss: 3.5809 (3.5524)  loss_scale: 32768.0000 (34599.2675)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0938  max mem: 15572
Epoch: [33]  [ 850/1404]  eta: 0:05:31  lr: 0.000008  min_lr: 0.000000  loss: 3.4005 (3.5542)  loss_scale: 32768.0000 (34577.7485)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.0560  max mem: 15572
[2025-01-11 00:02:39,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:02:39,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:02:39,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:02:39,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:02:43,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47192
[2025-01-11 00:02:43,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:02:43,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47192
[2025-01-11 00:02:43,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:02:43,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 860/1404]  eta: 0:05:25  lr: 0.000008  min_lr: 0.000000  loss: 3.2778 (3.5508)  loss_scale: 32768.0000 (34823.1359)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0168  max mem: 15572
Epoch: [33]  [ 870/1404]  eta: 0:05:19  lr: 0.000008  min_lr: 0.000000  loss: 3.2581 (3.5503)  loss_scale: 32768.0000 (34799.5408)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0047  max mem: 15572
Epoch: [33]  [ 880/1404]  eta: 0:05:13  lr: 0.000008  min_lr: 0.000000  loss: 3.6319 (3.5486)  loss_scale: 32768.0000 (34776.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0045  max mem: 15572
Epoch: [33]  [ 890/1404]  eta: 0:05:07  lr: 0.000008  min_lr: 0.000000  loss: 3.7699 (3.5516)  loss_scale: 32768.0000 (34753.9394)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0203  max mem: 15572
Epoch: [33]  [ 900/1404]  eta: 0:05:00  lr: 0.000008  min_lr: 0.000000  loss: 3.7699 (3.5496)  loss_scale: 32768.0000 (34731.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0443  max mem: 15572
Epoch: [33]  [ 910/1404]  eta: 0:04:54  lr: 0.000008  min_lr: 0.000000  loss: 3.5744 (3.5503)  loss_scale: 32768.0000 (34710.3403)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0249  max mem: 15572
Epoch: [33]  [ 920/1404]  eta: 0:04:48  lr: 0.000007  min_lr: 0.000000  loss: 3.7192 (3.5506)  loss_scale: 32768.0000 (34689.2508)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.0389  max mem: 15572
Epoch: [33]  [ 930/1404]  eta: 0:04:43  lr: 0.000007  min_lr: 0.000000  loss: 3.7192 (3.5518)  loss_scale: 32768.0000 (34668.6144)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0601  max mem: 15572
Epoch: [33]  [ 940/1404]  eta: 0:04:37  lr: 0.000007  min_lr: 0.000000  loss: 3.6849 (3.5508)  loss_scale: 32768.0000 (34648.4166)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0220  max mem: 15572
Epoch: [33]  [ 950/1404]  eta: 0:04:31  lr: 0.000007  min_lr: 0.000000  loss: 3.6849 (3.5508)  loss_scale: 32768.0000 (34628.6435)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0009  max mem: 15572
Epoch: [33]  [ 960/1404]  eta: 0:04:25  lr: 0.000007  min_lr: 0.000000  loss: 3.6964 (3.5523)  loss_scale: 32768.0000 (34609.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0010  max mem: 15572
Epoch: [33]  [ 970/1404]  eta: 0:04:19  lr: 0.000007  min_lr: 0.000000  loss: 3.6498 (3.5498)  loss_scale: 32768.0000 (34590.3193)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0007  max mem: 15572
Epoch: [33]  [ 980/1404]  eta: 0:04:13  lr: 0.000007  min_lr: 0.000000  loss: 3.3723 (3.5502)  loss_scale: 32768.0000 (34571.7431)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.0006  max mem: 15572
[2025-01-11 00:04:00,759] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:04:00,760] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:04:00,760] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:04:00,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:04:01,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47322
[2025-01-11 00:04:01,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:04:01,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 990/1404]  eta: 0:04:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5655 (3.5492)  loss_scale: 32768.0000 (34586.6075)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0006  max mem: 15572
[2025-01-11 00:04:01,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47322
[2025-01-11 00:04:01,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [33]  [1000/1404]  eta: 0:04:00  lr: 0.000007  min_lr: 0.000000  loss: 3.5692 (3.5500)  loss_scale: 32768.0000 (34568.4396)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0006  max mem: 15572
Epoch: [33]  [1010/1404]  eta: 0:03:55  lr: 0.000007  min_lr: 0.000000  loss: 3.7214 (3.5505)  loss_scale: 32768.0000 (34550.6311)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0007  max mem: 15572
Epoch: [33]  [1020/1404]  eta: 0:03:48  lr: 0.000007  min_lr: 0.000000  loss: 3.5094 (3.5498)  loss_scale: 32768.0000 (34533.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.0006  max mem: 15572
Epoch: [33]  [1030/1404]  eta: 0:03:42  lr: 0.000007  min_lr: 0.000000  loss: 3.4994 (3.5499)  loss_scale: 32768.0000 (34516.0504)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0006  max mem: 15572
Epoch: [33]  [1040/1404]  eta: 0:03:37  lr: 0.000007  min_lr: 0.000000  loss: 3.4706 (3.5489)  loss_scale: 32768.0000 (34499.2584)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.0005  max mem: 15572
Epoch: [33]  [1050/1404]  eta: 0:03:31  lr: 0.000007  min_lr: 0.000000  loss: 3.6286 (3.5492)  loss_scale: 32768.0000 (34482.7859)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0008  max mem: 15572
Epoch: [33]  [1060/1404]  eta: 0:03:24  lr: 0.000007  min_lr: 0.000000  loss: 3.5215 (3.5471)  loss_scale: 32768.0000 (34466.6239)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0010  max mem: 15572
Epoch: [33]  [1070/1404]  eta: 0:03:18  lr: 0.000007  min_lr: 0.000000  loss: 3.4572 (3.5464)  loss_scale: 32768.0000 (34450.7638)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0010  max mem: 15572
Epoch: [33]  [1080/1404]  eta: 0:03:12  lr: 0.000007  min_lr: 0.000000  loss: 3.6037 (3.5482)  loss_scale: 32768.0000 (34435.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0144  max mem: 15572
Epoch: [33]  [1090/1404]  eta: 0:03:07  lr: 0.000007  min_lr: 0.000000  loss: 3.6610 (3.5488)  loss_scale: 32768.0000 (34419.9157)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0252  max mem: 15572
Epoch: [33]  [1100/1404]  eta: 0:03:01  lr: 0.000007  min_lr: 0.000000  loss: 3.7493 (3.5499)  loss_scale: 32768.0000 (34404.9119)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0117  max mem: 15572
Epoch: [33]  [1110/1404]  eta: 0:02:55  lr: 0.000007  min_lr: 0.000000  loss: 3.6018 (3.5506)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.0006  max mem: 15572
[2025-01-11 00:05:17,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:05:17,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:05:17,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:05:17,465] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1120/1404]  eta: 0:02:49  lr: 0.000007  min_lr: 0.000000  loss: 3.5439 (3.5476)  loss_scale: 32768.0000 (34434.1695)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.0007  max mem: 15572
[2025-01-11 00:05:22,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47461
[2025-01-11 00:05:22,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:05:22,993] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47461
[2025-01-11 00:05:22,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:05:22,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1130/1404]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000000  loss: 3.3051 (3.5465)  loss_scale: 32768.0000 (34651.2184)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0008  max mem: 15572
Epoch: [33]  [1140/1404]  eta: 0:02:37  lr: 0.000007  min_lr: 0.000000  loss: 3.6373 (3.5485)  loss_scale: 32768.0000 (34634.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0010  max mem: 15572
Epoch: [33]  [1150/1404]  eta: 0:02:31  lr: 0.000007  min_lr: 0.000000  loss: 3.6373 (3.5479)  loss_scale: 32768.0000 (34618.4952)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0009  max mem: 15572
Epoch: [33]  [1160/1404]  eta: 0:02:25  lr: 0.000007  min_lr: 0.000000  loss: 3.3403 (3.5473)  loss_scale: 32768.0000 (34602.5564)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0007  max mem: 15572
Epoch: [33]  [1170/1404]  eta: 0:02:19  lr: 0.000007  min_lr: 0.000000  loss: 3.4345 (3.5466)  loss_scale: 32768.0000 (34586.8898)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0007  max mem: 15572
Epoch: [33]  [1180/1404]  eta: 0:02:13  lr: 0.000007  min_lr: 0.000000  loss: 3.4460 (3.5458)  loss_scale: 32768.0000 (34571.4886)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0007  max mem: 15572
Epoch: [33]  [1190/1404]  eta: 0:02:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5378 (3.5452)  loss_scale: 32768.0000 (34556.3459)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0007  max mem: 15572
Epoch: [33]  [1200/1404]  eta: 0:02:01  lr: 0.000007  min_lr: 0.000000  loss: 3.5735 (3.5440)  loss_scale: 32768.0000 (34541.4555)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0008  max mem: 15572
Epoch: [33]  [1210/1404]  eta: 0:01:55  lr: 0.000007  min_lr: 0.000000  loss: 3.6095 (3.5447)  loss_scale: 32768.0000 (34526.8109)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0012  max mem: 15572
Epoch: [33]  [1220/1404]  eta: 0:01:49  lr: 0.000007  min_lr: 0.000000  loss: 3.4069 (3.5421)  loss_scale: 32768.0000 (34512.4062)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0011  max mem: 15572
Epoch: [33]  [1230/1404]  eta: 0:01:43  lr: 0.000007  min_lr: 0.000000  loss: 3.5053 (3.5439)  loss_scale: 32768.0000 (34498.2356)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0007  max mem: 15572
Epoch: [33]  [1240/1404]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000000  loss: 3.6101 (3.5436)  loss_scale: 32768.0000 (34484.2933)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0008  max mem: 15572
Epoch: [33]  [1250/1404]  eta: 0:01:31  lr: 0.000007  min_lr: 0.000000  loss: 3.5689 (3.5430)  loss_scale: 32768.0000 (34470.5739)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0009  max mem: 15572
[2025-01-11 00:06:39,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:06:39,106] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:06:39,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:06:39,109] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1260/1404]  eta: 0:01:25  lr: 0.000007  min_lr: 0.000000  loss: 3.6470 (3.5432)  loss_scale: 32768.0000 (34535.0293)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0008  max mem: 15572
Epoch: [33]  [1270/1404]  eta: 0:01:19  lr: 0.000007  min_lr: 0.000000  loss: 3.6695 (3.5435)  loss_scale: 65536.0000 (34778.9394)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0008  max mem: 15572
Epoch: [33]  [1280/1404]  eta: 0:01:13  lr: 0.000007  min_lr: 0.000000  loss: 3.6103 (3.5446)  loss_scale: 65536.0000 (35019.0414)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0009  max mem: 15572
[2025-01-11 00:06:52,938] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47614
[2025-01-11 00:06:52,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:06:52,942] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47614
[2025-01-11 00:06:52,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:06:52,943] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1290/1404]  eta: 0:01:07  lr: 0.000007  min_lr: 0.000000  loss: 3.6103 (3.5455)  loss_scale: 65536.0000 (35026.9868)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0009  max mem: 15572
Epoch: [33]  [1300/1404]  eta: 0:01:01  lr: 0.000007  min_lr: 0.000000  loss: 3.6887 (3.5458)  loss_scale: 32768.0000 (35009.6234)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0007  max mem: 15572
Epoch: [33]  [1310/1404]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 3.7791 (3.5475)  loss_scale: 32768.0000 (34992.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0007  max mem: 15572
Epoch: [33]  [1320/1404]  eta: 0:00:50  lr: 0.000007  min_lr: 0.000000  loss: 3.7123 (3.5472)  loss_scale: 32768.0000 (34975.6851)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0007  max mem: 15572
Epoch: [33]  [1330/1404]  eta: 0:00:44  lr: 0.000007  min_lr: 0.000000  loss: 3.5217 (3.5471)  loss_scale: 32768.0000 (34959.0984)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0006  max mem: 15572
Epoch: [33]  [1340/1404]  eta: 0:00:38  lr: 0.000007  min_lr: 0.000000  loss: 3.5594 (3.5471)  loss_scale: 32768.0000 (34942.7591)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0007  max mem: 15572
Epoch: [33]  [1350/1404]  eta: 0:00:32  lr: 0.000007  min_lr: 0.000000  loss: 3.5594 (3.5469)  loss_scale: 32768.0000 (34926.6617)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0008  max mem: 15572
Epoch: [33]  [1360/1404]  eta: 0:00:26  lr: 0.000007  min_lr: 0.000000  loss: 3.5627 (3.5491)  loss_scale: 32768.0000 (34910.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0009  max mem: 15572
Epoch: [33]  [1370/1404]  eta: 0:00:20  lr: 0.000007  min_lr: 0.000000  loss: 3.5627 (3.5494)  loss_scale: 32768.0000 (34895.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0010  max mem: 15572
Epoch: [33]  [1380/1404]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 3.5543 (3.5499)  loss_scale: 32768.0000 (34879.7683)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0017  max mem: 15572
Epoch: [33]  [1390/1404]  eta: 0:00:08  lr: 0.000007  min_lr: 0.000000  loss: 3.6462 (3.5504)  loss_scale: 32768.0000 (34864.5866)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0016  max mem: 15572
Epoch: [33]  [1400/1404]  eta: 0:00:02  lr: 0.000007  min_lr: 0.000000  loss: 3.5303 (3.5494)  loss_scale: 32768.0000 (34849.6217)  weight_decay: 0.0500 (0.0500)  time: 0.4750  data: 0.0006  max mem: 15572
Epoch: [33]  [1403/1404]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 3.5303 (3.5497)  loss_scale: 32768.0000 (34845.1738)  weight_decay: 0.0500 (0.0500)  time: 0.4494  data: 0.0005  max mem: 15572
Epoch: [33] Total time: 0:13:52 (0.5930 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 3.5303 (3.5456)  loss_scale: 32768.0000 (34845.1738)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:31  loss: 1.4053 (1.4053)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.6454  data: 4.4730  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 1.9438 (1.9946)  acc1: 55.5556 (49.4949)  acc5: 77.7778 (81.3131)  time: 0.8141  data: 0.6084  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 2.3845 (2.1911)  acc1: 44.4444 (46.5608)  acc5: 77.7778 (78.3069)  time: 0.4049  data: 0.1914  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.2562 (2.0579)  acc1: 44.4444 (50.1792)  acc5: 77.7778 (79.5699)  time: 0.2924  data: 0.0809  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.7470 (2.0107)  acc1: 61.1111 (52.4390)  acc5: 83.3333 (80.4878)  time: 0.2907  data: 0.0911  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.7470 (2.0117)  acc1: 55.5556 (51.9608)  acc5: 83.3333 (80.9368)  time: 0.3986  data: 0.1847  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.0464 (2.0990)  acc1: 44.4444 (49.0893)  acc5: 77.7778 (79.9636)  time: 0.3946  data: 0.1629  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 1.9481 (2.0640)  acc1: 50.0000 (50.0000)  acc5: 77.7778 (80.4382)  time: 0.3857  data: 0.1638  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.7754 (2.0524)  acc1: 50.0000 (50.1372)  acc5: 88.8889 (80.9328)  time: 0.3946  data: 0.1805  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0094 (2.0602)  acc1: 50.0000 (49.9390)  acc5: 83.3333 (80.8303)  time: 0.3580  data: 0.1478  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3378 (2.1361)  acc1: 44.4444 (48.1298)  acc5: 72.2222 (78.7129)  time: 0.3377  data: 0.1378  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.1977 (2.1284)  acc1: 44.4444 (48.3984)  acc5: 72.2222 (78.9790)  time: 0.3499  data: 0.1618  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8808 (2.0840)  acc1: 55.5556 (49.4031)  acc5: 83.3333 (79.5684)  time: 0.3530  data: 0.1651  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6796 (2.0464)  acc1: 61.1111 (50.3393)  acc5: 88.8889 (80.1103)  time: 0.2479  data: 0.0790  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7235 (2.0436)  acc1: 55.5556 (50.4914)  acc5: 83.3333 (80.2211)  time: 0.1973  data: 0.0460  max mem: 15572
Val: Total time: 0:00:50 (0.3709 s / it)
* Acc@1 49.959 Acc@5 79.054 loss 2.087
Accuracy of the network on the 4883 val videos: 50.0%
Max accuracy: 50.49%
Epoch: [34]  [   0/1404]  eta: 3:17:21  lr: 0.000007  min_lr: 0.000000  loss: 3.1883 (3.1883)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4341  data: 8.0116  max mem: 15572
[2025-01-11 00:09:04,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:09:04,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:09:04,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:09:04,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [  10/1404]  eta: 0:31:28  lr: 0.000007  min_lr: 0.000000  loss: 3.5879 (3.5138)  loss_scale: 32768.0000 (44683.6364)  weight_decay: 0.0500 (0.0500)  time: 1.3545  data: 0.8214  max mem: 15572
[2025-01-11 00:09:07,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47747
[2025-01-11 00:09:07,193] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:09:07,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47747
[2025-01-11 00:09:07,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:09:07,195] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [  20/1404]  eta: 0:22:01  lr: 0.000007  min_lr: 0.000000  loss: 3.6481 (3.5927)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0517  max mem: 15572
Epoch: [34]  [  30/1404]  eta: 0:18:38  lr: 0.000007  min_lr: 0.000000  loss: 3.6403 (3.5752)  loss_scale: 32768.0000 (36996.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0009  max mem: 15572
Epoch: [34]  [  40/1404]  eta: 0:17:12  lr: 0.000007  min_lr: 0.000000  loss: 3.6025 (3.6069)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0390  max mem: 15572
Epoch: [34]  [  50/1404]  eta: 0:16:43  lr: 0.000007  min_lr: 0.000000  loss: 3.6239 (3.5842)  loss_scale: 32768.0000 (35338.0392)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1246  max mem: 15572
Epoch: [34]  [  60/1404]  eta: 0:16:06  lr: 0.000007  min_lr: 0.000000  loss: 3.6583 (3.6174)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1349  max mem: 15572
Epoch: [34]  [  70/1404]  eta: 0:15:35  lr: 0.000007  min_lr: 0.000000  loss: 3.6437 (3.5996)  loss_scale: 32768.0000 (34614.0845)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0923  max mem: 15572
Epoch: [34]  [  80/1404]  eta: 0:15:19  lr: 0.000007  min_lr: 0.000000  loss: 3.3741 (3.5619)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1110  max mem: 15572
Epoch: [34]  [  90/1404]  eta: 0:14:58  lr: 0.000007  min_lr: 0.000000  loss: 3.5273 (3.5794)  loss_scale: 32768.0000 (34208.3516)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1072  max mem: 15572
Epoch: [34]  [ 100/1404]  eta: 0:14:38  lr: 0.000007  min_lr: 0.000000  loss: 3.6228 (3.5613)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0972  max mem: 15572
Epoch: [34]  [ 110/1404]  eta: 0:14:11  lr: 0.000007  min_lr: 0.000000  loss: 3.6051 (3.5610)  loss_scale: 32768.0000 (33948.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0579  max mem: 15572
Epoch: [34]  [ 120/1404]  eta: 0:13:56  lr: 0.000007  min_lr: 0.000000  loss: 3.6573 (3.5606)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0341  max mem: 15572
Epoch: [34]  [ 130/1404]  eta: 0:13:48  lr: 0.000007  min_lr: 0.000000  loss: 3.6422 (3.5618)  loss_scale: 32768.0000 (33768.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0946  max mem: 15572
[2025-01-11 00:10:22,419] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:10:22,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:10:22,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:10:22,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 140/1404]  eta: 0:13:31  lr: 0.000007  min_lr: 0.000000  loss: 3.6384 (3.5782)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0708  max mem: 15572
Epoch: [34]  [ 150/1404]  eta: 0:13:26  lr: 0.000006  min_lr: 0.000000  loss: 3.6497 (3.5706)  loss_scale: 65536.0000 (36023.0993)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0777  max mem: 15572
Epoch: [34]  [ 160/1404]  eta: 0:13:18  lr: 0.000006  min_lr: 0.000000  loss: 3.6497 (3.5681)  loss_scale: 65536.0000 (37856.1988)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1129  max mem: 15572
Epoch: [34]  [ 170/1404]  eta: 0:13:02  lr: 0.000006  min_lr: 0.000000  loss: 3.5908 (3.5740)  loss_scale: 65536.0000 (39474.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0457  max mem: 15572
[2025-01-11 00:10:44,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47915
[2025-01-11 00:10:44,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:10:44,566] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47915
[2025-01-11 00:10:44,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:10:44,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 180/1404]  eta: 0:12:55  lr: 0.000006  min_lr: 0.000000  loss: 3.5908 (3.5736)  loss_scale: 65536.0000 (40552.6630)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0792  max mem: 15572
Epoch: [34]  [ 190/1404]  eta: 0:12:40  lr: 0.000006  min_lr: 0.000000  loss: 3.4651 (3.5636)  loss_scale: 32768.0000 (40145.0890)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0942  max mem: 15572
Epoch: [34]  [ 200/1404]  eta: 0:12:33  lr: 0.000006  min_lr: 0.000000  loss: 3.6102 (3.5641)  loss_scale: 32768.0000 (39778.0697)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0578  max mem: 15572
Epoch: [34]  [ 210/1404]  eta: 0:12:23  lr: 0.000006  min_lr: 0.000000  loss: 3.5590 (3.5624)  loss_scale: 32768.0000 (39445.8389)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0643  max mem: 15572
Epoch: [34]  [ 220/1404]  eta: 0:12:14  lr: 0.000006  min_lr: 0.000000  loss: 3.4604 (3.5604)  loss_scale: 32768.0000 (39143.6742)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0608  max mem: 15572
Epoch: [34]  [ 230/1404]  eta: 0:12:04  lr: 0.000006  min_lr: 0.000000  loss: 3.4205 (3.5574)  loss_scale: 32768.0000 (38867.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0624  max mem: 15572
Epoch: [34]  [ 240/1404]  eta: 0:11:58  lr: 0.000006  min_lr: 0.000000  loss: 3.4205 (3.5511)  loss_scale: 32768.0000 (38614.5726)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.0547  max mem: 15572
Epoch: [34]  [ 250/1404]  eta: 0:11:51  lr: 0.000006  min_lr: 0.000000  loss: 3.4609 (3.5530)  loss_scale: 32768.0000 (38381.6414)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0707  max mem: 15572
Epoch: [34]  [ 260/1404]  eta: 0:11:40  lr: 0.000006  min_lr: 0.000000  loss: 3.7781 (3.5576)  loss_scale: 32768.0000 (38166.5594)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0400  max mem: 15572
[2025-01-11 00:11:33,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=327, lr=[6.127444034370026e-08, 6.127444034370026e-08, 8.753491477671467e-08, 8.753491477671467e-08, 1.2504987825244955e-07, 1.2504987825244955e-07, 1.7864268321778507e-07, 1.7864268321778507e-07, 2.552038331682644e-07, 2.552038331682644e-07, 3.6457690452609205e-07, 3.6457690452609205e-07, 5.208241493229887e-07, 5.208241493229887e-07, 7.44034499032841e-07, 7.44034499032841e-07, 1.0629064271897728e-06, 1.0629064271897728e-06, 1.5184377531282471e-06, 1.5184377531282471e-06, 2.16919679018321e-06, 2.16919679018321e-06, 3.098852557404586e-06, 3.098852557404586e-06, 4.426932224863695e-06, 4.426932224863695e-06, 6.324188892662422e-06, 6.324188892662422e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:11:33,384] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=45.79068982588206, CurrSamplesPerSec=48.47864861402608, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [ 270/1404]  eta: 0:11:32  lr: 0.000006  min_lr: 0.000000  loss: 3.6985 (3.5629)  loss_scale: 32768.0000 (37967.3506)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0229  max mem: 15572
Epoch: [34]  [ 280/1404]  eta: 0:11:25  lr: 0.000006  min_lr: 0.000000  loss: 3.7157 (3.5757)  loss_scale: 32768.0000 (37782.3203)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0736  max mem: 15572
Epoch: [34]  [ 290/1404]  eta: 0:11:24  lr: 0.000006  min_lr: 0.000000  loss: 3.6369 (3.5701)  loss_scale: 32768.0000 (37610.0069)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.1553  max mem: 15572
Epoch: [34]  [ 300/1404]  eta: 0:11:13  lr: 0.000006  min_lr: 0.000000  loss: 3.3780 (3.5714)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1047  max mem: 15572
[2025-01-11 00:11:59,603] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:11:59,604] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:11:59,618] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:11:59,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 310/1404]  eta: 0:11:07  lr: 0.000006  min_lr: 0.000000  loss: 3.4450 (3.5709)  loss_scale: 32768.0000 (37614.7138)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0544  max mem: 15572
[2025-01-11 00:12:04,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48052
[2025-01-11 00:12:04,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:12:04,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48052
[2025-01-11 00:12:04,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:12:04,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 320/1404]  eta: 0:10:59  lr: 0.000006  min_lr: 0.000000  loss: 3.5116 (3.5703)  loss_scale: 32768.0000 (37974.1308)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0898  max mem: 15572
Epoch: [34]  [ 330/1404]  eta: 0:10:51  lr: 0.000006  min_lr: 0.000000  loss: 3.5512 (3.5712)  loss_scale: 32768.0000 (37816.8459)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0364  max mem: 15572
Epoch: [34]  [ 340/1404]  eta: 0:10:44  lr: 0.000006  min_lr: 0.000000  loss: 3.6144 (3.5707)  loss_scale: 32768.0000 (37668.7859)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0378  max mem: 15572
Epoch: [34]  [ 350/1404]  eta: 0:10:35  lr: 0.000006  min_lr: 0.000000  loss: 3.4775 (3.5693)  loss_scale: 32768.0000 (37529.1624)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0493  max mem: 15572
Epoch: [34]  [ 360/1404]  eta: 0:10:29  lr: 0.000006  min_lr: 0.000000  loss: 3.4318 (3.5585)  loss_scale: 32768.0000 (37397.2742)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0466  max mem: 15572
Epoch: [34]  [ 370/1404]  eta: 0:10:25  lr: 0.000006  min_lr: 0.000000  loss: 3.3098 (3.5551)  loss_scale: 32768.0000 (37272.4960)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.1250  max mem: 15572
Epoch: [34]  [ 380/1404]  eta: 0:10:18  lr: 0.000006  min_lr: 0.000000  loss: 3.5726 (3.5552)  loss_scale: 32768.0000 (37154.2677)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.0906  max mem: 15572
Epoch: [34]  [ 390/1404]  eta: 0:10:13  lr: 0.000006  min_lr: 0.000000  loss: 3.6439 (3.5587)  loss_scale: 32768.0000 (37042.0870)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0222  max mem: 15572
Epoch: [34]  [ 400/1404]  eta: 0:10:08  lr: 0.000006  min_lr: 0.000000  loss: 3.6371 (3.5571)  loss_scale: 32768.0000 (36935.5012)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.0223  max mem: 15572
Epoch: [34]  [ 410/1404]  eta: 0:10:01  lr: 0.000006  min_lr: 0.000000  loss: 3.6371 (3.5580)  loss_scale: 32768.0000 (36834.1022)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0006  max mem: 15572
Epoch: [34]  [ 420/1404]  eta: 0:09:55  lr: 0.000006  min_lr: 0.000000  loss: 3.7157 (3.5595)  loss_scale: 32768.0000 (36737.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0007  max mem: 15572
Epoch: [34]  [ 430/1404]  eta: 0:09:48  lr: 0.000006  min_lr: 0.000000  loss: 3.5477 (3.5584)  loss_scale: 32768.0000 (36645.4200)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0008  max mem: 15572
Epoch: [34]  [ 440/1404]  eta: 0:09:43  lr: 0.000006  min_lr: 0.000000  loss: 3.5189 (3.5538)  loss_scale: 32768.0000 (36557.4966)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0008  max mem: 15572
[2025-01-11 00:13:21,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:13:21,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:13:21,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:13:21,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 450/1404]  eta: 0:09:35  lr: 0.000006  min_lr: 0.000000  loss: 3.5189 (3.5542)  loss_scale: 32768.0000 (36909.4102)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0050  max mem: 15572
[2025-01-11 00:13:30,328] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48195
[2025-01-11 00:13:30,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:13:30,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:13:30,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48195
[2025-01-11 00:13:30,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [34]  [ 460/1404]  eta: 0:09:31  lr: 0.000006  min_lr: 0.000000  loss: 3.4856 (3.5458)  loss_scale: 65536.0000 (37388.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0235  max mem: 15572
Epoch: [34]  [ 470/1404]  eta: 0:09:26  lr: 0.000006  min_lr: 0.000000  loss: 3.3368 (3.5410)  loss_scale: 32768.0000 (37290.1231)  weight_decay: 0.0500 (0.0500)  time: 0.6863  data: 0.0628  max mem: 15572
Epoch: [34]  [ 480/1404]  eta: 0:09:19  lr: 0.000006  min_lr: 0.000000  loss: 3.4693 (3.5428)  loss_scale: 32768.0000 (37196.1081)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0443  max mem: 15572
Epoch: [34]  [ 490/1404]  eta: 0:09:14  lr: 0.000006  min_lr: 0.000000  loss: 3.7514 (3.5475)  loss_scale: 32768.0000 (37105.9226)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0782  max mem: 15572
Epoch: [34]  [ 500/1404]  eta: 0:09:06  lr: 0.000006  min_lr: 0.000000  loss: 3.7166 (3.5484)  loss_scale: 32768.0000 (37019.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0779  max mem: 15572
Epoch: [34]  [ 510/1404]  eta: 0:08:58  lr: 0.000006  min_lr: 0.000000  loss: 3.5256 (3.5492)  loss_scale: 32768.0000 (36936.1409)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0007  max mem: 15572
Epoch: [34]  [ 520/1404]  eta: 0:08:53  lr: 0.000006  min_lr: 0.000000  loss: 3.4505 (3.5456)  loss_scale: 32768.0000 (36856.1382)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0665  max mem: 15572
Epoch: [34]  [ 530/1404]  eta: 0:08:46  lr: 0.000006  min_lr: 0.000000  loss: 3.5345 (3.5469)  loss_scale: 32768.0000 (36779.1488)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0837  max mem: 15572
Epoch: [34]  [ 540/1404]  eta: 0:08:41  lr: 0.000006  min_lr: 0.000000  loss: 3.6559 (3.5487)  loss_scale: 32768.0000 (36705.0055)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0759  max mem: 15572
Epoch: [34]  [ 550/1404]  eta: 0:08:34  lr: 0.000006  min_lr: 0.000000  loss: 3.8670 (3.5531)  loss_scale: 32768.0000 (36633.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0854  max mem: 15572
Epoch: [34]  [ 560/1404]  eta: 0:08:27  lr: 0.000006  min_lr: 0.000000  loss: 3.7421 (3.5544)  loss_scale: 32768.0000 (36564.6488)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0393  max mem: 15572
Epoch: [34]  [ 570/1404]  eta: 0:08:19  lr: 0.000006  min_lr: 0.000000  loss: 3.3138 (3.5478)  loss_scale: 32768.0000 (36498.1576)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0172  max mem: 15572
Epoch: [34]  [ 580/1404]  eta: 0:08:14  lr: 0.000006  min_lr: 0.000000  loss: 3.2503 (3.5462)  loss_scale: 32768.0000 (36433.9552)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0881  max mem: 15572
[2025-01-11 00:14:45,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:14:45,371] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:14:45,375] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:14:45,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 590/1404]  eta: 0:08:09  lr: 0.000006  min_lr: 0.000000  loss: 3.6512 (3.5462)  loss_scale: 32768.0000 (36538.2606)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1306  max mem: 15572
[2025-01-11 00:14:51,756] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48334
[2025-01-11 00:14:51,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:14:51,767] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48334
[2025-01-11 00:14:51,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:14:51,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 600/1404]  eta: 0:08:02  lr: 0.000006  min_lr: 0.000000  loss: 3.6315 (3.5454)  loss_scale: 32768.0000 (36857.1847)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0479  max mem: 15572
Epoch: [34]  [ 610/1404]  eta: 0:07:55  lr: 0.000006  min_lr: 0.000000  loss: 3.5630 (3.5461)  loss_scale: 32768.0000 (36790.2586)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0010  max mem: 15572
Epoch: [34]  [ 620/1404]  eta: 0:07:48  lr: 0.000006  min_lr: 0.000000  loss: 3.6162 (3.5459)  loss_scale: 32768.0000 (36725.4879)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0010  max mem: 15572
Epoch: [34]  [ 630/1404]  eta: 0:07:43  lr: 0.000006  min_lr: 0.000000  loss: 3.6000 (3.5473)  loss_scale: 32768.0000 (36662.7702)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.0556  max mem: 15572
Epoch: [34]  [ 640/1404]  eta: 0:07:36  lr: 0.000006  min_lr: 0.000000  loss: 3.7569 (3.5515)  loss_scale: 32768.0000 (36602.0094)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0650  max mem: 15572
Epoch: [34]  [ 650/1404]  eta: 0:07:30  lr: 0.000006  min_lr: 0.000000  loss: 3.6034 (3.5505)  loss_scale: 32768.0000 (36543.1152)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0616  max mem: 15572
Epoch: [34]  [ 660/1404]  eta: 0:07:25  lr: 0.000006  min_lr: 0.000000  loss: 3.5717 (3.5541)  loss_scale: 32768.0000 (36486.0030)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1178  max mem: 15572
Epoch: [34]  [ 670/1404]  eta: 0:07:20  lr: 0.000006  min_lr: 0.000000  loss: 3.7361 (3.5551)  loss_scale: 32768.0000 (36430.5931)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.1301  max mem: 15572
Epoch: [34]  [ 680/1404]  eta: 0:07:14  lr: 0.000006  min_lr: 0.000000  loss: 3.6081 (3.5527)  loss_scale: 32768.0000 (36376.8106)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.1116  max mem: 15572
Epoch: [34]  [ 690/1404]  eta: 0:07:07  lr: 0.000006  min_lr: 0.000000  loss: 3.6081 (3.5519)  loss_scale: 32768.0000 (36324.5847)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0479  max mem: 15572
Epoch: [34]  [ 700/1404]  eta: 0:07:01  lr: 0.000006  min_lr: 0.000000  loss: 3.8391 (3.5553)  loss_scale: 32768.0000 (36273.8488)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0006  max mem: 15572
Epoch: [34]  [ 710/1404]  eta: 0:06:54  lr: 0.000006  min_lr: 0.000000  loss: 3.8028 (3.5566)  loss_scale: 32768.0000 (36224.5401)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0172  max mem: 15572
Epoch: [34]  [ 720/1404]  eta: 0:06:48  lr: 0.000006  min_lr: 0.000000  loss: 3.6497 (3.5577)  loss_scale: 32768.0000 (36176.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1075  max mem: 15572
[2025-01-11 00:16:06,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:16:06,870] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:16:06,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:16:06,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 730/1404]  eta: 0:06:42  lr: 0.000006  min_lr: 0.000000  loss: 3.5907 (3.5552)  loss_scale: 32768.0000 (36309.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0909  max mem: 15572
[2025-01-11 00:16:10,001] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48469
[2025-01-11 00:16:10,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:16:10,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:16:10,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48469
[2025-01-11 00:16:10,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [34]  [ 740/1404]  eta: 0:06:36  lr: 0.000006  min_lr: 0.000000  loss: 3.5086 (3.5556)  loss_scale: 32768.0000 (36349.9271)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0572  max mem: 15572
Epoch: [34]  [ 750/1404]  eta: 0:06:30  lr: 0.000006  min_lr: 0.000000  loss: 3.5785 (3.5556)  loss_scale: 32768.0000 (36302.2317)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0959  max mem: 15572
Epoch: [34]  [ 760/1404]  eta: 0:06:24  lr: 0.000006  min_lr: 0.000000  loss: 3.8099 (3.5564)  loss_scale: 32768.0000 (36255.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0884  max mem: 15572
Epoch: [34]  [ 770/1404]  eta: 0:06:18  lr: 0.000006  min_lr: 0.000000  loss: 3.7204 (3.5580)  loss_scale: 32768.0000 (36210.5525)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0759  max mem: 15572
Epoch: [34]  [ 780/1404]  eta: 0:06:12  lr: 0.000006  min_lr: 0.000000  loss: 3.7978 (3.5583)  loss_scale: 32768.0000 (36166.4738)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0725  max mem: 15572
Epoch: [34]  [ 790/1404]  eta: 0:06:07  lr: 0.000006  min_lr: 0.000000  loss: 3.3829 (3.5555)  loss_scale: 32768.0000 (36123.5095)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.0630  max mem: 15572
Epoch: [34]  [ 800/1404]  eta: 0:06:01  lr: 0.000006  min_lr: 0.000000  loss: 3.3829 (3.5541)  loss_scale: 32768.0000 (36081.6180)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0174  max mem: 15572
Epoch: [34]  [ 810/1404]  eta: 0:05:55  lr: 0.000006  min_lr: 0.000000  loss: 3.2807 (3.5503)  loss_scale: 32768.0000 (36040.7596)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0008  max mem: 15572
Epoch: [34]  [ 820/1404]  eta: 0:05:48  lr: 0.000006  min_lr: 0.000000  loss: 3.5355 (3.5506)  loss_scale: 32768.0000 (36000.8965)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0008  max mem: 15572
Epoch: [34]  [ 830/1404]  eta: 0:05:42  lr: 0.000006  min_lr: 0.000000  loss: 3.6634 (3.5517)  loss_scale: 32768.0000 (35961.9928)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0008  max mem: 15572
Epoch: [34]  [ 840/1404]  eta: 0:05:36  lr: 0.000005  min_lr: 0.000000  loss: 3.5954 (3.5509)  loss_scale: 32768.0000 (35924.0143)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0363  max mem: 15572
Epoch: [34]  [ 850/1404]  eta: 0:05:30  lr: 0.000005  min_lr: 0.000000  loss: 3.4811 (3.5501)  loss_scale: 32768.0000 (35886.9283)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0897  max mem: 15572
Epoch: [34]  [ 860/1404]  eta: 0:05:25  lr: 0.000005  min_lr: 0.000000  loss: 3.6043 (3.5516)  loss_scale: 32768.0000 (35850.7038)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1399  max mem: 15572
[2025-01-11 00:17:28,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:17:28,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:17:28,073] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:17:28,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 870/1404]  eta: 0:05:19  lr: 0.000005  min_lr: 0.000000  loss: 3.6648 (3.5525)  loss_scale: 32768.0000 (36153.9013)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.1005  max mem: 15572
[2025-01-11 00:17:36,155] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48612
[2025-01-11 00:17:36,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:17:36,156] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48612
[2025-01-11 00:17:36,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:17:36,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 880/1404]  eta: 0:05:13  lr: 0.000005  min_lr: 0.000000  loss: 3.5850 (3.5519)  loss_scale: 65536.0000 (36301.4393)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0544  max mem: 15572
Epoch: [34]  [ 890/1404]  eta: 0:05:07  lr: 0.000005  min_lr: 0.000000  loss: 3.3923 (3.5517)  loss_scale: 32768.0000 (36261.7823)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0403  max mem: 15572
Epoch: [34]  [ 900/1404]  eta: 0:05:01  lr: 0.000005  min_lr: 0.000000  loss: 3.3923 (3.5507)  loss_scale: 32768.0000 (36223.0055)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0008  max mem: 15572
Epoch: [34]  [ 910/1404]  eta: 0:04:55  lr: 0.000005  min_lr: 0.000000  loss: 3.4461 (3.5498)  loss_scale: 32768.0000 (36185.0801)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0006  max mem: 15572
Epoch: [34]  [ 920/1404]  eta: 0:04:49  lr: 0.000005  min_lr: 0.000000  loss: 3.5783 (3.5500)  loss_scale: 32768.0000 (36147.9783)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0005  max mem: 15572
Epoch: [34]  [ 930/1404]  eta: 0:04:42  lr: 0.000005  min_lr: 0.000000  loss: 3.4982 (3.5496)  loss_scale: 32768.0000 (36111.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0006  max mem: 15572
Epoch: [34]  [ 940/1404]  eta: 0:04:37  lr: 0.000005  min_lr: 0.000000  loss: 3.6382 (3.5526)  loss_scale: 32768.0000 (36076.1403)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0007  max mem: 15572
Epoch: [34]  [ 950/1404]  eta: 0:04:31  lr: 0.000005  min_lr: 0.000000  loss: 3.6352 (3.5528)  loss_scale: 32768.0000 (36041.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0007  max mem: 15572
Epoch: [34]  [ 960/1404]  eta: 0:04:24  lr: 0.000005  min_lr: 0.000000  loss: 3.4509 (3.5525)  loss_scale: 32768.0000 (36007.2924)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0006  max mem: 15572
Epoch: [34]  [ 970/1404]  eta: 0:04:18  lr: 0.000005  min_lr: 0.000000  loss: 3.4388 (3.5523)  loss_scale: 32768.0000 (35973.9320)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0007  max mem: 15572
Epoch: [34]  [ 980/1404]  eta: 0:04:12  lr: 0.000005  min_lr: 0.000000  loss: 3.1839 (3.5485)  loss_scale: 32768.0000 (35941.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0007  max mem: 15572
Epoch: [34]  [ 990/1404]  eta: 0:04:06  lr: 0.000005  min_lr: 0.000000  loss: 3.4176 (3.5471)  loss_scale: 32768.0000 (35909.2311)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0007  max mem: 15572
Epoch: [34]  [1000/1404]  eta: 0:04:00  lr: 0.000005  min_lr: 0.000000  loss: 3.4685 (3.5475)  loss_scale: 32768.0000 (35877.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0008  max mem: 15572
[2025-01-11 00:18:50,682] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:18:50,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:18:50,690] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:18:50,690] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1010/1404]  eta: 0:03:54  lr: 0.000005  min_lr: 0.000000  loss: 3.4685 (3.5468)  loss_scale: 32768.0000 (36041.5589)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.0008  max mem: 15572
[2025-01-11 00:18:56,425] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48749
[2025-01-11 00:18:56,426] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:18:56,426] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:18:56,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48749
[2025-01-11 00:18:56,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [34]  [1020/1404]  eta: 0:03:48  lr: 0.000005  min_lr: 0.000000  loss: 3.5114 (3.5482)  loss_scale: 32768.0000 (36073.6846)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0006  max mem: 15572
Epoch: [34]  [1030/1404]  eta: 0:03:42  lr: 0.000005  min_lr: 0.000000  loss: 3.7860 (3.5484)  loss_scale: 32768.0000 (36041.6217)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0008  max mem: 15572
Epoch: [34]  [1040/1404]  eta: 0:03:36  lr: 0.000005  min_lr: 0.000000  loss: 3.5582 (3.5480)  loss_scale: 32768.0000 (36010.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0009  max mem: 15572
Epoch: [34]  [1050/1404]  eta: 0:03:30  lr: 0.000005  min_lr: 0.000000  loss: 3.3572 (3.5461)  loss_scale: 32768.0000 (35979.3264)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0008  max mem: 15572
Epoch: [34]  [1060/1404]  eta: 0:03:24  lr: 0.000005  min_lr: 0.000000  loss: 3.4435 (3.5451)  loss_scale: 32768.0000 (35949.0594)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0010  max mem: 15572
Epoch: [34]  [1070/1404]  eta: 0:03:18  lr: 0.000005  min_lr: 0.000000  loss: 3.5078 (3.5450)  loss_scale: 32768.0000 (35919.3576)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0012  max mem: 15572
Epoch: [34]  [1080/1404]  eta: 0:03:12  lr: 0.000005  min_lr: 0.000000  loss: 3.5078 (3.5440)  loss_scale: 32768.0000 (35890.2054)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0008  max mem: 15572
Epoch: [34]  [1090/1404]  eta: 0:03:06  lr: 0.000005  min_lr: 0.000000  loss: 3.5198 (3.5426)  loss_scale: 32768.0000 (35861.5875)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.0006  max mem: 15572
Epoch: [34]  [1100/1404]  eta: 0:03:00  lr: 0.000005  min_lr: 0.000000  loss: 3.5198 (3.5415)  loss_scale: 32768.0000 (35833.4896)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0008  max mem: 15572
Epoch: [34]  [1110/1404]  eta: 0:02:54  lr: 0.000005  min_lr: 0.000000  loss: 3.5682 (3.5411)  loss_scale: 32768.0000 (35805.8974)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0008  max mem: 15572
Epoch: [34]  [1120/1404]  eta: 0:02:48  lr: 0.000005  min_lr: 0.000000  loss: 3.3564 (3.5392)  loss_scale: 32768.0000 (35778.7975)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0073  max mem: 15572
Epoch: [34]  [1130/1404]  eta: 0:02:42  lr: 0.000005  min_lr: 0.000000  loss: 3.5173 (3.5415)  loss_scale: 32768.0000 (35752.1768)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0073  max mem: 15572
Epoch: [34]  [1140/1404]  eta: 0:02:36  lr: 0.000005  min_lr: 0.000000  loss: 3.6564 (3.5417)  loss_scale: 32768.0000 (35726.0228)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0006  max mem: 15572
[2025-01-11 00:20:11,215] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:20:11,215] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:20:11,224] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:20:11,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:20:15,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48884
[2025-01-11 00:20:15,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:20:15,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48884
[2025-01-11 00:20:15,225] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:20:15,225] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1150/1404]  eta: 0:02:30  lr: 0.000005  min_lr: 0.000000  loss: 3.6843 (3.5453)  loss_scale: 32768.0000 (35871.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0007  max mem: 15572
Epoch: [34]  [1160/1404]  eta: 0:02:24  lr: 0.000005  min_lr: 0.000000  loss: 3.8477 (3.5465)  loss_scale: 32768.0000 (35844.4100)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0009  max mem: 15572
Epoch: [34]  [1170/1404]  eta: 0:02:19  lr: 0.000005  min_lr: 0.000000  loss: 3.6383 (3.5480)  loss_scale: 32768.0000 (35818.1383)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0012  max mem: 15572
Epoch: [34]  [1180/1404]  eta: 0:02:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7473 (3.5510)  loss_scale: 32768.0000 (35792.3116)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0011  max mem: 15572
Epoch: [34]  [1190/1404]  eta: 0:02:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7939 (3.5515)  loss_scale: 32768.0000 (35766.9186)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.0246  max mem: 15572
Epoch: [34]  [1200/1404]  eta: 0:02:01  lr: 0.000005  min_lr: 0.000000  loss: 3.6160 (3.5519)  loss_scale: 32768.0000 (35741.9484)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.0244  max mem: 15572
Epoch: [34]  [1210/1404]  eta: 0:01:55  lr: 0.000005  min_lr: 0.000000  loss: 3.7789 (3.5539)  loss_scale: 32768.0000 (35717.3906)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0222  max mem: 15572
Epoch: [34]  [1220/1404]  eta: 0:01:49  lr: 0.000005  min_lr: 0.000000  loss: 3.5707 (3.5519)  loss_scale: 32768.0000 (35693.2351)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0633  max mem: 15572
Epoch: [34]  [1230/1404]  eta: 0:01:43  lr: 0.000005  min_lr: 0.000000  loss: 3.4177 (3.5503)  loss_scale: 32768.0000 (35669.4720)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0418  max mem: 15572
Epoch: [34]  [1240/1404]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000000  loss: 3.4945 (3.5504)  loss_scale: 32768.0000 (35646.0919)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0369  max mem: 15572
Epoch: [34]  [1250/1404]  eta: 0:01:31  lr: 0.000005  min_lr: 0.000000  loss: 3.5579 (3.5522)  loss_scale: 32768.0000 (35623.0855)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0844  max mem: 15572
Epoch: [34]  [1260/1404]  eta: 0:01:25  lr: 0.000005  min_lr: 0.000000  loss: 3.7127 (3.5509)  loss_scale: 32768.0000 (35600.4441)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0751  max mem: 15572
[2025-01-11 00:21:23,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=334, lr=[4.7631188684432676e-08, 4.7631188684432676e-08, 6.804455526347526e-08, 6.804455526347526e-08, 9.720650751925038e-08, 9.720650751925038e-08, 1.3886643931321484e-07, 1.3886643931321484e-07, 1.9838062759030693e-07, 1.9838062759030693e-07, 2.8340089655758133e-07, 2.8340089655758133e-07, 4.048584236536876e-07, 4.048584236536876e-07, 5.783691766481252e-07, 5.783691766481252e-07, 8.262416809258931e-07, 8.262416809258931e-07, 1.180345258465562e-06, 1.180345258465562e-06, 1.6862075120936596e-06, 1.6862075120936596e-06, 2.4088678744195143e-06, 2.4088678744195143e-06, 3.441239820599306e-06, 3.441239820599306e-06, 4.916056886570438e-06, 4.916056886570438e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:21:23,844] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=45.79577802439278, CurrSamplesPerSec=51.9615251596838, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [1270/1404]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000000  loss: 3.3557 (3.5492)  loss_scale: 32768.0000 (35578.1589)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1276  max mem: 15572
[2025-01-11 00:21:33,085] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:21:33,085] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:21:33,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:21:33,092] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1280/1404]  eta: 0:01:13  lr: 0.000005  min_lr: 0.000000  loss: 3.6493 (3.5510)  loss_scale: 32768.0000 (35658.5418)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1008  max mem: 15572
[2025-01-11 00:21:36,220] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49019
[2025-01-11 00:21:36,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:21:36,308] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49019
[2025-01-11 00:21:36,308] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:21:36,308] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1290/1404]  eta: 0:01:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7509 (3.5513)  loss_scale: 32768.0000 (35686.9156)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0272  max mem: 15572
Epoch: [34]  [1300/1404]  eta: 0:01:01  lr: 0.000005  min_lr: 0.000000  loss: 3.4949 (3.5493)  loss_scale: 32768.0000 (35664.4796)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0277  max mem: 15572
Epoch: [34]  [1310/1404]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 3.3597 (3.5483)  loss_scale: 32768.0000 (35642.3860)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0014  max mem: 15572
Epoch: [34]  [1320/1404]  eta: 0:00:49  lr: 0.000005  min_lr: 0.000000  loss: 3.4341 (3.5487)  loss_scale: 32768.0000 (35620.6268)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0008  max mem: 15572
Epoch: [34]  [1330/1404]  eta: 0:00:44  lr: 0.000005  min_lr: 0.000000  loss: 3.6146 (3.5496)  loss_scale: 32768.0000 (35599.1946)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0008  max mem: 15572
Epoch: [34]  [1340/1404]  eta: 0:00:38  lr: 0.000005  min_lr: 0.000000  loss: 3.4327 (3.5483)  loss_scale: 32768.0000 (35578.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0010  max mem: 15572
Epoch: [34]  [1350/1404]  eta: 0:00:32  lr: 0.000005  min_lr: 0.000000  loss: 3.1526 (3.5467)  loss_scale: 32768.0000 (35557.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0008  max mem: 15572
Epoch: [34]  [1360/1404]  eta: 0:00:26  lr: 0.000005  min_lr: 0.000000  loss: 3.5072 (3.5474)  loss_scale: 32768.0000 (35536.7877)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0006  max mem: 15572
Epoch: [34]  [1370/1404]  eta: 0:00:20  lr: 0.000005  min_lr: 0.000000  loss: 3.6822 (3.5477)  loss_scale: 32768.0000 (35516.5923)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0065  max mem: 15572
Epoch: [34]  [1380/1404]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 3.6054 (3.5479)  loss_scale: 32768.0000 (35496.6894)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0066  max mem: 15572
Epoch: [34]  [1390/1404]  eta: 0:00:08  lr: 0.000005  min_lr: 0.000000  loss: 3.5661 (3.5473)  loss_scale: 32768.0000 (35477.0726)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0008  max mem: 15572
Epoch: [34]  [1400/1404]  eta: 0:00:02  lr: 0.000005  min_lr: 0.000000  loss: 3.5490 (3.5478)  loss_scale: 32768.0000 (35457.7359)  weight_decay: 0.0500 (0.0500)  time: 0.4578  data: 0.0004  max mem: 15572
Epoch: [34]  [1403/1404]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 3.5490 (3.5482)  loss_scale: 32768.0000 (35451.9886)  weight_decay: 0.0500 (0.0500)  time: 0.4352  data: 0.0003  max mem: 15572
Epoch: [34] Total time: 0:13:52 (0.5927 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 3.5490 (3.5382)  loss_scale: 32768.0000 (35451.9886)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:01  loss: 1.4241 (1.4241)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 5.3016  data: 4.9950  max mem: 15572
Val:  [ 10/136]  eta: 0:01:30  loss: 1.9847 (1.9519)  acc1: 61.1111 (54.5455)  acc5: 83.3333 (82.3232)  time: 0.7196  data: 0.4892  max mem: 15572
Val:  [ 20/136]  eta: 0:00:58  loss: 2.2670 (2.1312)  acc1: 44.4444 (49.4709)  acc5: 77.7778 (80.1587)  time: 0.2668  data: 0.0418  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.1370 (2.0205)  acc1: 44.4444 (52.3297)  acc5: 83.3333 (80.8244)  time: 0.3321  data: 0.1151  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.8176 (1.9932)  acc1: 61.1111 (53.5230)  acc5: 83.3333 (81.0298)  time: 0.3872  data: 0.1767  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.8252 (1.9966)  acc1: 55.5556 (53.7037)  acc5: 83.3333 (81.2636)  time: 0.4092  data: 0.1970  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1749 (2.0894)  acc1: 44.4444 (50.4554)  acc5: 83.3333 (80.0546)  time: 0.3885  data: 0.1855  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 1.9454 (2.0543)  acc1: 50.0000 (51.1737)  acc5: 83.3333 (80.3599)  time: 0.3687  data: 0.1560  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8418 (2.0497)  acc1: 50.0000 (51.0974)  acc5: 88.8889 (81.0014)  time: 0.3580  data: 0.1361  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.0257 (2.0595)  acc1: 50.0000 (50.7937)  acc5: 83.3333 (80.7692)  time: 0.3256  data: 0.1156  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3249 (2.1355)  acc1: 44.4444 (48.8999)  acc5: 72.2222 (78.9329)  time: 0.3195  data: 0.1112  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2423 (2.1275)  acc1: 44.4444 (49.0490)  acc5: 77.7778 (78.8789)  time: 0.3404  data: 0.1314  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8482 (2.0813)  acc1: 55.5556 (50.0918)  acc5: 83.3333 (79.4307)  time: 0.3719  data: 0.1668  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5686 (2.0419)  acc1: 55.5556 (50.8482)  acc5: 88.8889 (80.0254)  time: 0.3059  data: 0.1342  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7447 (2.0405)  acc1: 50.0000 (50.9828)  acc5: 83.3333 (80.1392)  time: 0.2048  data: 0.0506  max mem: 15572
Val: Total time: 0:00:50 (0.3705 s / it)
* Acc@1 50.041 Acc@5 79.218 loss 2.085
Accuracy of the network on the 4883 val videos: 50.0%
Max accuracy: 50.49%
Epoch: [35]  [   0/1404]  eta: 3:18:46  lr: 0.000005  min_lr: 0.000000  loss: 3.0026 (3.0026)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4946  data: 5.1658  max mem: 15572
[2025-01-11 00:23:47,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:23:47,486] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:23:47,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:23:47,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [  10/1404]  eta: 0:29:18  lr: 0.000005  min_lr: 0.000000  loss: 3.2643 (3.3691)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 1.2616  data: 0.4704  max mem: 15572
Epoch: [35]  [  20/1404]  eta: 0:21:17  lr: 0.000005  min_lr: 0.000000  loss: 3.7967 (3.5000)  loss_scale: 65536.0000 (53052.9524)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0079  max mem: 15572
[2025-01-11 00:23:55,381] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49161
[2025-01-11 00:23:55,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:23:55,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:23:55,421] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49161
[2025-01-11 00:23:55,422] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [  30/1404]  eta: 0:18:38  lr: 0.000005  min_lr: 0.000000  loss: 3.8573 (3.5939)  loss_scale: 32768.0000 (46509.4194)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0078  max mem: 15572
Epoch: [35]  [  40/1404]  eta: 0:17:29  lr: 0.000005  min_lr: 0.000000  loss: 3.7768 (3.6093)  loss_scale: 32768.0000 (43157.8537)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0006  max mem: 15572
Epoch: [35]  [  50/1404]  eta: 0:16:39  lr: 0.000005  min_lr: 0.000000  loss: 3.6120 (3.5937)  loss_scale: 32768.0000 (41120.6275)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0011  max mem: 15572
Epoch: [35]  [  60/1404]  eta: 0:15:43  lr: 0.000005  min_lr: 0.000000  loss: 3.5442 (3.5790)  loss_scale: 32768.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0013  max mem: 15572
Epoch: [35]  [  70/1404]  eta: 0:15:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7318 (3.5800)  loss_scale: 32768.0000 (38767.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0009  max mem: 15572
Epoch: [35]  [  80/1404]  eta: 0:14:57  lr: 0.000005  min_lr: 0.000000  loss: 3.7558 (3.5996)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0009  max mem: 15572
Epoch: [35]  [  90/1404]  eta: 0:14:28  lr: 0.000005  min_lr: 0.000000  loss: 3.5745 (3.6012)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0009  max mem: 15572
Epoch: [35]  [ 100/1404]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 3.4984 (3.5691)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0008  max mem: 15572
Epoch: [35]  [ 110/1404]  eta: 0:13:55  lr: 0.000005  min_lr: 0.000000  loss: 3.1072 (3.5314)  loss_scale: 32768.0000 (36605.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0458  max mem: 15572
Epoch: [35]  [ 120/1404]  eta: 0:13:47  lr: 0.000005  min_lr: 0.000000  loss: 3.5197 (3.5406)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0541  max mem: 15572
Epoch: [35]  [ 130/1404]  eta: 0:13:32  lr: 0.000005  min_lr: 0.000000  loss: 3.6486 (3.5523)  loss_scale: 32768.0000 (36019.7863)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0461  max mem: 15572
Epoch: [35]  [ 140/1404]  eta: 0:13:17  lr: 0.000005  min_lr: 0.000000  loss: 3.7073 (3.5651)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0589  max mem: 15572
[2025-01-11 00:25:09,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:25:09,897] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:25:09,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:25:09,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 150/1404]  eta: 0:13:10  lr: 0.000005  min_lr: 0.000000  loss: 3.6685 (3.5478)  loss_scale: 32768.0000 (35806.0927)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0820  max mem: 15572
[2025-01-11 00:25:13,007] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49294
[2025-01-11 00:25:13,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:25:13,008] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:25:13,009] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49294
[2025-01-11 00:25:13,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [ 160/1404]  eta: 0:13:00  lr: 0.000005  min_lr: 0.000000  loss: 3.4811 (3.5506)  loss_scale: 32768.0000 (36227.9752)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0922  max mem: 15572
Epoch: [35]  [ 170/1404]  eta: 0:12:54  lr: 0.000005  min_lr: 0.000000  loss: 3.4864 (3.5494)  loss_scale: 32768.0000 (36025.6374)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0322  max mem: 15572
Epoch: [35]  [ 180/1404]  eta: 0:12:38  lr: 0.000005  min_lr: 0.000000  loss: 3.4101 (3.5488)  loss_scale: 32768.0000 (35845.6575)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0006  max mem: 15572
Epoch: [35]  [ 190/1404]  eta: 0:12:35  lr: 0.000004  min_lr: 0.000000  loss: 3.4101 (3.5383)  loss_scale: 32768.0000 (35684.5236)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0416  max mem: 15572
Epoch: [35]  [ 200/1404]  eta: 0:12:30  lr: 0.000004  min_lr: 0.000000  loss: 3.4208 (3.5402)  loss_scale: 32768.0000 (35539.4229)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.0624  max mem: 15572
Epoch: [35]  [ 210/1404]  eta: 0:12:16  lr: 0.000004  min_lr: 0.000000  loss: 3.4499 (3.5371)  loss_scale: 32768.0000 (35408.0758)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.0216  max mem: 15572
Epoch: [35]  [ 220/1404]  eta: 0:12:12  lr: 0.000004  min_lr: 0.000000  loss: 3.5929 (3.5524)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0084  max mem: 15572
Epoch: [35]  [ 230/1404]  eta: 0:12:06  lr: 0.000004  min_lr: 0.000000  loss: 3.7569 (3.5598)  loss_scale: 32768.0000 (35179.4978)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0083  max mem: 15572
Epoch: [35]  [ 240/1404]  eta: 0:11:58  lr: 0.000004  min_lr: 0.000000  loss: 3.8550 (3.5662)  loss_scale: 32768.0000 (35079.4357)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0007  max mem: 15572
Epoch: [35]  [ 250/1404]  eta: 0:11:52  lr: 0.000004  min_lr: 0.000000  loss: 3.8999 (3.5723)  loss_scale: 32768.0000 (34987.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0009  max mem: 15572
Epoch: [35]  [ 260/1404]  eta: 0:11:48  lr: 0.000004  min_lr: 0.000000  loss: 3.8338 (3.5862)  loss_scale: 32768.0000 (34902.3142)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0009  max mem: 15572
Epoch: [35]  [ 270/1404]  eta: 0:11:38  lr: 0.000004  min_lr: 0.000000  loss: 3.7220 (3.5818)  loss_scale: 32768.0000 (34823.5572)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0012  max mem: 15572
Epoch: [35]  [ 280/1404]  eta: 0:11:32  lr: 0.000004  min_lr: 0.000000  loss: 3.5885 (3.5836)  loss_scale: 32768.0000 (34750.4057)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0012  max mem: 15572
[2025-01-11 00:26:30,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:26:30,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:26:30,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:26:30,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:26:31,708] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49426
[2025-01-11 00:26:31,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:26:31,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:26:31,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49426
[2025-01-11 00:26:31,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [ 290/1404]  eta: 0:11:25  lr: 0.000004  min_lr: 0.000000  loss: 3.4966 (3.5754)  loss_scale: 32768.0000 (35020.0962)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0008  max mem: 15572
Epoch: [35]  [ 300/1404]  eta: 0:11:19  lr: 0.000004  min_lr: 0.000000  loss: 3.4689 (3.5771)  loss_scale: 32768.0000 (34945.2757)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0007  max mem: 15572
Epoch: [35]  [ 310/1404]  eta: 0:11:15  lr: 0.000004  min_lr: 0.000000  loss: 3.6377 (3.5763)  loss_scale: 32768.0000 (34875.2669)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0011  max mem: 15572
Epoch: [35]  [ 320/1404]  eta: 0:11:05  lr: 0.000004  min_lr: 0.000000  loss: 3.8351 (3.5850)  loss_scale: 32768.0000 (34809.6199)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0012  max mem: 15572
Epoch: [35]  [ 330/1404]  eta: 0:10:57  lr: 0.000004  min_lr: 0.000000  loss: 3.5158 (3.5743)  loss_scale: 32768.0000 (34747.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0010  max mem: 15572
Epoch: [35]  [ 340/1404]  eta: 0:10:52  lr: 0.000004  min_lr: 0.000000  loss: 3.1381 (3.5654)  loss_scale: 32768.0000 (34689.8768)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0012  max mem: 15572
Epoch: [35]  [ 350/1404]  eta: 0:10:43  lr: 0.000004  min_lr: 0.000000  loss: 3.5143 (3.5689)  loss_scale: 32768.0000 (34635.1225)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0009  max mem: 15572
Epoch: [35]  [ 360/1404]  eta: 0:10:34  lr: 0.000004  min_lr: 0.000000  loss: 3.6430 (3.5662)  loss_scale: 32768.0000 (34583.4017)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0009  max mem: 15572
Epoch: [35]  [ 370/1404]  eta: 0:10:29  lr: 0.000004  min_lr: 0.000000  loss: 3.6430 (3.5694)  loss_scale: 32768.0000 (34534.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.0009  max mem: 15572
Epoch: [35]  [ 380/1404]  eta: 0:10:21  lr: 0.000004  min_lr: 0.000000  loss: 3.7179 (3.5762)  loss_scale: 32768.0000 (34488.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0007  max mem: 15572
Epoch: [35]  [ 390/1404]  eta: 0:10:17  lr: 0.000004  min_lr: 0.000000  loss: 3.4227 (3.5693)  loss_scale: 32768.0000 (34444.1125)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0006  max mem: 15572
Epoch: [35]  [ 400/1404]  eta: 0:10:09  lr: 0.000004  min_lr: 0.000000  loss: 3.3618 (3.5701)  loss_scale: 32768.0000 (34402.3142)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0006  max mem: 15572
Epoch: [35]  [ 410/1404]  eta: 0:10:03  lr: 0.000004  min_lr: 0.000000  loss: 3.4718 (3.5664)  loss_scale: 32768.0000 (34362.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0007  max mem: 15572
[2025-01-11 00:27:47,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:27:47,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:27:47,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:27:47,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 420/1404]  eta: 0:09:55  lr: 0.000004  min_lr: 0.000000  loss: 3.2833 (3.5623)  loss_scale: 32768.0000 (34791.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0007  max mem: 15572
[2025-01-11 00:27:52,646] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49565
[2025-01-11 00:27:52,646] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49565
[2025-01-11 00:27:52,646] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:27:52,646] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:27:52,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 430/1404]  eta: 0:09:47  lr: 0.000004  min_lr: 0.000000  loss: 3.2785 (3.5633)  loss_scale: 32768.0000 (35048.8353)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0010  max mem: 15572
Epoch: [35]  [ 440/1404]  eta: 0:09:41  lr: 0.000004  min_lr: 0.000000  loss: 3.4704 (3.5610)  loss_scale: 32768.0000 (34997.1156)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0010  max mem: 15572
Epoch: [35]  [ 450/1404]  eta: 0:09:35  lr: 0.000004  min_lr: 0.000000  loss: 3.6671 (3.5650)  loss_scale: 32768.0000 (34947.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0008  max mem: 15572
Epoch: [35]  [ 460/1404]  eta: 0:09:29  lr: 0.000004  min_lr: 0.000000  loss: 3.5802 (3.5611)  loss_scale: 32768.0000 (34900.4078)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0007  max mem: 15572
Epoch: [35]  [ 470/1404]  eta: 0:09:23  lr: 0.000004  min_lr: 0.000000  loss: 3.5332 (3.5617)  loss_scale: 32768.0000 (34855.1338)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.0007  max mem: 15572
Epoch: [35]  [ 480/1404]  eta: 0:09:17  lr: 0.000004  min_lr: 0.000000  loss: 3.5332 (3.5587)  loss_scale: 32768.0000 (34811.7422)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0008  max mem: 15572
Epoch: [35]  [ 490/1404]  eta: 0:09:11  lr: 0.000004  min_lr: 0.000000  loss: 3.3808 (3.5571)  loss_scale: 32768.0000 (34770.1181)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0007  max mem: 15572
Epoch: [35]  [ 500/1404]  eta: 0:09:06  lr: 0.000004  min_lr: 0.000000  loss: 3.6844 (3.5589)  loss_scale: 32768.0000 (34730.1557)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.0006  max mem: 15572
Epoch: [35]  [ 510/1404]  eta: 0:09:01  lr: 0.000004  min_lr: 0.000000  loss: 3.7486 (3.5629)  loss_scale: 32768.0000 (34691.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0005  max mem: 15572
Epoch: [35]  [ 520/1404]  eta: 0:08:53  lr: 0.000004  min_lr: 0.000000  loss: 3.6428 (3.5603)  loss_scale: 32768.0000 (34654.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0007  max mem: 15572
Epoch: [35]  [ 530/1404]  eta: 0:08:48  lr: 0.000004  min_lr: 0.000000  loss: 3.4820 (3.5581)  loss_scale: 32768.0000 (34619.2994)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0007  max mem: 15572
Epoch: [35]  [ 540/1404]  eta: 0:08:41  lr: 0.000004  min_lr: 0.000000  loss: 3.4820 (3.5571)  loss_scale: 32768.0000 (34585.0795)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0008  max mem: 15572
Epoch: [35]  [ 550/1404]  eta: 0:08:35  lr: 0.000004  min_lr: 0.000000  loss: 3.6219 (3.5597)  loss_scale: 32768.0000 (34552.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.0010  max mem: 15572
[2025-01-11 00:29:09,976] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:29:09,976] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:29:10,002] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:29:10,002] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 560/1404]  eta: 0:08:28  lr: 0.000004  min_lr: 0.000000  loss: 3.6219 (3.5587)  loss_scale: 32768.0000 (34929.1693)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0008  max mem: 15572
Epoch: [35]  [ 570/1404]  eta: 0:08:22  lr: 0.000004  min_lr: 0.000000  loss: 3.1650 (3.5502)  loss_scale: 65536.0000 (35465.1909)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0007  max mem: 15572
Epoch: [35]  [ 580/1404]  eta: 0:08:15  lr: 0.000004  min_lr: 0.000000  loss: 3.2239 (3.5519)  loss_scale: 65536.0000 (35982.7608)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0007  max mem: 15572
[2025-01-11 00:29:27,146] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49725
[2025-01-11 00:29:27,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:29:27,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49725
[2025-01-11 00:29:27,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:29:27,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 590/1404]  eta: 0:08:08  lr: 0.000004  min_lr: 0.000000  loss: 3.6291 (3.5504)  loss_scale: 65536.0000 (36150.1455)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0007  max mem: 15572
Epoch: [35]  [ 600/1404]  eta: 0:08:01  lr: 0.000004  min_lr: 0.000000  loss: 3.6156 (3.5545)  loss_scale: 32768.0000 (36093.8702)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0007  max mem: 15572
Epoch: [35]  [ 610/1404]  eta: 0:07:55  lr: 0.000004  min_lr: 0.000000  loss: 3.5754 (3.5538)  loss_scale: 32768.0000 (36039.4370)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0007  max mem: 15572
Epoch: [35]  [ 620/1404]  eta: 0:07:50  lr: 0.000004  min_lr: 0.000000  loss: 3.3598 (3.5490)  loss_scale: 32768.0000 (35986.7568)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0006  max mem: 15572
Epoch: [35]  [ 630/1404]  eta: 0:07:44  lr: 0.000004  min_lr: 0.000000  loss: 3.3598 (3.5494)  loss_scale: 32768.0000 (35935.7464)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0009  max mem: 15572
Epoch: [35]  [ 640/1404]  eta: 0:07:39  lr: 0.000004  min_lr: 0.000000  loss: 3.4273 (3.5471)  loss_scale: 32768.0000 (35886.3276)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0008  max mem: 15572
Epoch: [35]  [ 650/1404]  eta: 0:07:33  lr: 0.000004  min_lr: 0.000000  loss: 3.4389 (3.5480)  loss_scale: 32768.0000 (35838.4270)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0005  max mem: 15572
Epoch: [35]  [ 660/1404]  eta: 0:07:27  lr: 0.000004  min_lr: 0.000000  loss: 3.4779 (3.5468)  loss_scale: 32768.0000 (35791.9758)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0006  max mem: 15572
Epoch: [35]  [ 670/1404]  eta: 0:07:21  lr: 0.000004  min_lr: 0.000000  loss: 3.6059 (3.5475)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0006  max mem: 15572
Epoch: [35]  [ 680/1404]  eta: 0:07:14  lr: 0.000004  min_lr: 0.000000  loss: 3.7248 (3.5517)  loss_scale: 32768.0000 (35703.1659)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0007  max mem: 15572
Epoch: [35]  [ 690/1404]  eta: 0:07:07  lr: 0.000004  min_lr: 0.000000  loss: 3.6388 (3.5530)  loss_scale: 32768.0000 (35660.6889)  weight_decay: 0.0500 (0.0500)  time: 0.4928  data: 0.0006  max mem: 15572
Epoch: [35]  [ 700/1404]  eta: 0:07:00  lr: 0.000004  min_lr: 0.000000  loss: 3.4780 (3.5535)  loss_scale: 32768.0000 (35619.4237)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0004  max mem: 15572
Epoch: [35]  [ 710/1404]  eta: 0:06:54  lr: 0.000004  min_lr: 0.000000  loss: 3.6459 (3.5548)  loss_scale: 32768.0000 (35579.3193)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0144  max mem: 15572
[2025-01-11 00:30:42,466] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:30:42,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:30:42,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:30:42,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:30:42,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49855
[2025-01-11 00:30:42,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:30:42,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49855
[2025-01-11 00:30:42,978] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:30:42,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 720/1404]  eta: 0:06:48  lr: 0.000004  min_lr: 0.000000  loss: 3.6191 (3.5537)  loss_scale: 32768.0000 (35585.7753)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0145  max mem: 15572
Epoch: [35]  [ 730/1404]  eta: 0:06:43  lr: 0.000004  min_lr: 0.000000  loss: 3.5923 (3.5562)  loss_scale: 32768.0000 (35547.2285)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0950  max mem: 15572
Epoch: [35]  [ 740/1404]  eta: 0:06:37  lr: 0.000004  min_lr: 0.000000  loss: 3.5849 (3.5568)  loss_scale: 32768.0000 (35509.7220)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1470  max mem: 15572
Epoch: [35]  [ 750/1404]  eta: 0:06:32  lr: 0.000004  min_lr: 0.000000  loss: 3.5684 (3.5579)  loss_scale: 32768.0000 (35473.2144)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.1292  max mem: 15572
Epoch: [35]  [ 760/1404]  eta: 0:06:25  lr: 0.000004  min_lr: 0.000000  loss: 3.7670 (3.5620)  loss_scale: 32768.0000 (35437.6662)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1128  max mem: 15572
Epoch: [35]  [ 770/1404]  eta: 0:06:20  lr: 0.000004  min_lr: 0.000000  loss: 3.6882 (3.5615)  loss_scale: 32768.0000 (35403.0402)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0882  max mem: 15572
Epoch: [35]  [ 780/1404]  eta: 0:06:13  lr: 0.000004  min_lr: 0.000000  loss: 3.6410 (3.5629)  loss_scale: 32768.0000 (35369.3009)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.0624  max mem: 15572
Epoch: [35]  [ 790/1404]  eta: 0:06:08  lr: 0.000004  min_lr: 0.000000  loss: 3.7456 (3.5658)  loss_scale: 32768.0000 (35336.4147)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0872  max mem: 15572
Epoch: [35]  [ 800/1404]  eta: 0:06:01  lr: 0.000004  min_lr: 0.000000  loss: 3.5872 (3.5677)  loss_scale: 32768.0000 (35304.3496)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0777  max mem: 15572
Epoch: [35]  [ 810/1404]  eta: 0:05:55  lr: 0.000004  min_lr: 0.000000  loss: 3.6244 (3.5680)  loss_scale: 32768.0000 (35273.0752)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0750  max mem: 15572
Epoch: [35]  [ 820/1404]  eta: 0:05:49  lr: 0.000004  min_lr: 0.000000  loss: 3.6895 (3.5660)  loss_scale: 32768.0000 (35242.5627)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0834  max mem: 15572
Epoch: [35]  [ 830/1404]  eta: 0:05:43  lr: 0.000004  min_lr: 0.000000  loss: 3.0969 (3.5619)  loss_scale: 32768.0000 (35212.7846)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0094  max mem: 15572
Epoch: [35]  [ 840/1404]  eta: 0:05:37  lr: 0.000004  min_lr: 0.000000  loss: 3.4876 (3.5655)  loss_scale: 32768.0000 (35183.7146)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0389  max mem: 15572
[2025-01-11 00:31:59,879] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:31:59,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:31:59,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:31:59,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:32:00,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49986
[2025-01-11 00:32:00,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:32:00,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:32:00,801] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49986
[2025-01-11 00:32:00,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [ 850/1404]  eta: 0:05:30  lr: 0.000004  min_lr: 0.000000  loss: 3.7082 (3.5661)  loss_scale: 32768.0000 (35232.3384)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0388  max mem: 15572
[2025-01-11 00:32:08,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=342, lr=[3.565083453080395e-08, 3.565083453080395e-08, 5.092976361543422e-08, 5.092976361543422e-08, 7.275680516490604e-08, 7.275680516490604e-08, 1.0393829309272291e-07, 1.0393829309272291e-07, 1.4848327584674703e-07, 1.4848327584674703e-07, 2.1211896549535291e-07, 2.1211896549535291e-07, 3.030270935647899e-07, 3.030270935647899e-07, 4.328958479496999e-07, 4.328958479496999e-07, 6.184226399281426e-07, 6.184226399281426e-07, 8.834609141830612e-07, 8.834609141830612e-07, 1.2620870202615158e-06, 1.2620870202615158e-06, 1.8029814575164513e-06, 1.8029814575164513e-06, 2.5756877964520737e-06, 2.5756877964520737e-06, 3.679553994931534e-06, 3.679553994931534e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:32:08,506] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=45.76964898529937, CurrSamplesPerSec=49.597823403828656, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [ 860/1404]  eta: 0:05:24  lr: 0.000004  min_lr: 0.000000  loss: 3.4637 (3.5623)  loss_scale: 32768.0000 (35203.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0533  max mem: 15572
Epoch: [35]  [ 870/1404]  eta: 0:05:18  lr: 0.000004  min_lr: 0.000000  loss: 3.2907 (3.5589)  loss_scale: 32768.0000 (35175.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1149  max mem: 15572
Epoch: [35]  [ 880/1404]  eta: 0:05:13  lr: 0.000004  min_lr: 0.000000  loss: 3.2907 (3.5548)  loss_scale: 32768.0000 (35148.4222)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1319  max mem: 15572
Epoch: [35]  [ 890/1404]  eta: 0:05:06  lr: 0.000004  min_lr: 0.000000  loss: 3.2297 (3.5540)  loss_scale: 32768.0000 (35121.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0703  max mem: 15572
Epoch: [35]  [ 900/1404]  eta: 0:05:00  lr: 0.000004  min_lr: 0.000000  loss: 3.2297 (3.5491)  loss_scale: 32768.0000 (35095.5827)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0006  max mem: 15572
Epoch: [35]  [ 910/1404]  eta: 0:04:54  lr: 0.000004  min_lr: 0.000000  loss: 3.2510 (3.5489)  loss_scale: 32768.0000 (35070.0329)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0316  max mem: 15572
Epoch: [35]  [ 920/1404]  eta: 0:04:48  lr: 0.000004  min_lr: 0.000000  loss: 3.5275 (3.5504)  loss_scale: 32768.0000 (35045.0380)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0317  max mem: 15572
Epoch: [35]  [ 930/1404]  eta: 0:04:42  lr: 0.000004  min_lr: 0.000000  loss: 3.7738 (3.5534)  loss_scale: 32768.0000 (35020.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0205  max mem: 15572
Epoch: [35]  [ 940/1404]  eta: 0:04:36  lr: 0.000004  min_lr: 0.000000  loss: 3.7538 (3.5535)  loss_scale: 32768.0000 (34996.6419)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0205  max mem: 15572
Epoch: [35]  [ 950/1404]  eta: 0:04:30  lr: 0.000004  min_lr: 0.000000  loss: 3.4359 (3.5526)  loss_scale: 32768.0000 (34973.2072)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0007  max mem: 15572
Epoch: [35]  [ 960/1404]  eta: 0:04:24  lr: 0.000004  min_lr: 0.000000  loss: 3.4359 (3.5519)  loss_scale: 32768.0000 (34950.2601)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.0006  max mem: 15572
Epoch: [35]  [ 970/1404]  eta: 0:04:18  lr: 0.000004  min_lr: 0.000000  loss: 3.4039 (3.5477)  loss_scale: 32768.0000 (34927.7858)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0006  max mem: 15572
[2025-01-11 00:33:17,208] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:33:17,208] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:33:17,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:33:17,227] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 980/1404]  eta: 0:04:12  lr: 0.000004  min_lr: 0.000000  loss: 3.4341 (3.5494)  loss_scale: 32768.0000 (35106.1855)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0006  max mem: 15572
[2025-01-11 00:33:22,627] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50124
[2025-01-11 00:33:22,628] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:33:22,641] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50124
[2025-01-11 00:33:22,642] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:33:22,642] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 990/1404]  eta: 0:04:06  lr: 0.000004  min_lr: 0.000000  loss: 3.4234 (3.5457)  loss_scale: 32768.0000 (35181.7881)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.0006  max mem: 15572
Epoch: [35]  [1000/1404]  eta: 0:04:00  lr: 0.000004  min_lr: 0.000000  loss: 3.3177 (3.5460)  loss_scale: 32768.0000 (35157.6743)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0007  max mem: 15572
Epoch: [35]  [1010/1404]  eta: 0:03:55  lr: 0.000004  min_lr: 0.000000  loss: 3.5363 (3.5447)  loss_scale: 32768.0000 (35134.0376)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.0006  max mem: 15572
Epoch: [35]  [1020/1404]  eta: 0:03:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6183 (3.5452)  loss_scale: 32768.0000 (35110.8639)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0007  max mem: 15572
Epoch: [35]  [1030/1404]  eta: 0:03:42  lr: 0.000003  min_lr: 0.000000  loss: 3.5281 (3.5459)  loss_scale: 32768.0000 (35088.1397)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0009  max mem: 15572
Epoch: [35]  [1040/1404]  eta: 0:03:36  lr: 0.000003  min_lr: 0.000000  loss: 3.4969 (3.5454)  loss_scale: 32768.0000 (35065.8521)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0010  max mem: 15572
Epoch: [35]  [1050/1404]  eta: 0:03:30  lr: 0.000003  min_lr: 0.000000  loss: 3.5978 (3.5459)  loss_scale: 32768.0000 (35043.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0008  max mem: 15572
Epoch: [35]  [1060/1404]  eta: 0:03:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5063 (3.5440)  loss_scale: 32768.0000 (35022.5372)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0553  max mem: 15572
Epoch: [35]  [1070/1404]  eta: 0:03:18  lr: 0.000003  min_lr: 0.000000  loss: 3.4944 (3.5447)  loss_scale: 32768.0000 (35001.4865)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0554  max mem: 15572
Epoch: [35]  [1080/1404]  eta: 0:03:12  lr: 0.000003  min_lr: 0.000000  loss: 3.6077 (3.5444)  loss_scale: 32768.0000 (34980.8252)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.0008  max mem: 15572
Epoch: [35]  [1090/1404]  eta: 0:03:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5421 (3.5441)  loss_scale: 32768.0000 (34960.5426)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0007  max mem: 15572
Epoch: [35]  [1100/1404]  eta: 0:03:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5059 (3.5437)  loss_scale: 32768.0000 (34940.6285)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.0006  max mem: 15572
Epoch: [35]  [1110/1404]  eta: 0:02:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5810 (3.5443)  loss_scale: 32768.0000 (34921.0729)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0007  max mem: 15572
[2025-01-11 00:34:39,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:34:39,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:34:39,147] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:34:39,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1120/1404]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5786 (3.5440)  loss_scale: 32768.0000 (35135.7145)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0007  max mem: 15572
Epoch: [35]  [1130/1404]  eta: 0:02:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4833 (3.5440)  loss_scale: 65536.0000 (35404.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0007  max mem: 15572
Epoch: [35]  [1140/1404]  eta: 0:02:36  lr: 0.000003  min_lr: 0.000000  loss: 3.6105 (3.5443)  loss_scale: 65536.0000 (35668.5855)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0008  max mem: 15572
[2025-01-11 00:34:56,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50286
[2025-01-11 00:34:56,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:34:56,780] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50286
[2025-01-11 00:34:56,780] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:34:56,781] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1150/1404]  eta: 0:02:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7873 (3.5464)  loss_scale: 65536.0000 (35785.7307)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0009  max mem: 15572
Epoch: [35]  [1160/1404]  eta: 0:02:24  lr: 0.000003  min_lr: 0.000000  loss: 3.7873 (3.5460)  loss_scale: 32768.0000 (35759.7382)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0009  max mem: 15572
Epoch: [35]  [1170/1404]  eta: 0:02:19  lr: 0.000003  min_lr: 0.000000  loss: 3.3678 (3.5445)  loss_scale: 32768.0000 (35734.1896)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0010  max mem: 15572
Epoch: [35]  [1180/1404]  eta: 0:02:13  lr: 0.000003  min_lr: 0.000000  loss: 3.3002 (3.5428)  loss_scale: 32768.0000 (35709.0737)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0008  max mem: 15572
Epoch: [35]  [1190/1404]  eta: 0:02:06  lr: 0.000003  min_lr: 0.000000  loss: 3.3045 (3.5416)  loss_scale: 32768.0000 (35684.3795)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0008  max mem: 15572
Epoch: [35]  [1200/1404]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 3.5931 (3.5427)  loss_scale: 32768.0000 (35660.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0188  max mem: 15572
Epoch: [35]  [1210/1404]  eta: 0:01:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5542 (3.5429)  loss_scale: 32768.0000 (35636.2147)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0189  max mem: 15572
Epoch: [35]  [1220/1404]  eta: 0:01:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5210 (3.5410)  loss_scale: 32768.0000 (35612.7240)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0008  max mem: 15572
Epoch: [35]  [1230/1404]  eta: 0:01:43  lr: 0.000003  min_lr: 0.000000  loss: 3.3145 (3.5405)  loss_scale: 32768.0000 (35589.6149)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0005  max mem: 15572
Epoch: [35]  [1240/1404]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5387 (3.5414)  loss_scale: 32768.0000 (35566.8783)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0006  max mem: 15572
Epoch: [35]  [1250/1404]  eta: 0:01:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4099 (3.5410)  loss_scale: 32768.0000 (35544.5052)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0005  max mem: 15572
Epoch: [35]  [1260/1404]  eta: 0:01:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5140 (3.5442)  loss_scale: 32768.0000 (35522.4869)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0007  max mem: 15572
Epoch: [35]  [1270/1404]  eta: 0:01:19  lr: 0.000003  min_lr: 0.000000  loss: 3.9487 (3.5466)  loss_scale: 32768.0000 (35500.8151)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0009  max mem: 15572
[2025-01-11 00:36:15,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:36:15,164] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:36:15,165] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:36:15,165] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1280/1404]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 3.8160 (3.5479)  loss_scale: 32768.0000 (35632.9617)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0007  max mem: 15572
[2025-01-11 00:36:20,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50425
[2025-01-11 00:36:20,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:36:20,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50425
[2025-01-11 00:36:20,627] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:36:20,628] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1290/1404]  eta: 0:01:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6416 (3.5484)  loss_scale: 32768.0000 (35712.2974)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0008  max mem: 15572
Epoch: [35]  [1300/1404]  eta: 0:01:01  lr: 0.000003  min_lr: 0.000000  loss: 3.7526 (3.5510)  loss_scale: 32768.0000 (35689.6664)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0009  max mem: 15572
Epoch: [35]  [1310/1404]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7965 (3.5512)  loss_scale: 32768.0000 (35667.3806)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0008  max mem: 15572
Epoch: [35]  [1320/1404]  eta: 0:00:49  lr: 0.000003  min_lr: 0.000000  loss: 3.7449 (3.5533)  loss_scale: 32768.0000 (35645.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0008  max mem: 15572
Epoch: [35]  [1330/1404]  eta: 0:00:44  lr: 0.000003  min_lr: 0.000000  loss: 3.8691 (3.5556)  loss_scale: 32768.0000 (35623.8137)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0561  max mem: 15572
Epoch: [35]  [1340/1404]  eta: 0:00:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6425 (3.5550)  loss_scale: 32768.0000 (35602.5175)  weight_decay: 0.0500 (0.0500)  time: 0.6976  data: 0.1498  max mem: 15572
Epoch: [35]  [1350/1404]  eta: 0:00:32  lr: 0.000003  min_lr: 0.000000  loss: 3.2278 (3.5529)  loss_scale: 32768.0000 (35581.5366)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0943  max mem: 15572
Epoch: [35]  [1360/1404]  eta: 0:00:26  lr: 0.000003  min_lr: 0.000000  loss: 3.4408 (3.5541)  loss_scale: 32768.0000 (35560.8641)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0010  max mem: 15572
Epoch: [35]  [1370/1404]  eta: 0:00:20  lr: 0.000003  min_lr: 0.000000  loss: 3.6478 (3.5539)  loss_scale: 32768.0000 (35540.4931)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0011  max mem: 15572
Epoch: [35]  [1380/1404]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.5551)  loss_scale: 32768.0000 (35520.4171)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0327  max mem: 15572
Epoch: [35]  [1390/1404]  eta: 0:00:08  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.5550)  loss_scale: 32768.0000 (35500.6298)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0872  max mem: 15572
Epoch: [35]  [1400/1404]  eta: 0:00:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7083 (3.5565)  loss_scale: 32768.0000 (35481.1249)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0549  max mem: 15572
Epoch: [35]  [1403/1404]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 3.7083 (3.5560)  loss_scale: 32768.0000 (35475.3276)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0549  max mem: 15572
Epoch: [35] Total time: 0:13:52 (0.5928 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 3.7083 (3.5419)  loss_scale: 32768.0000 (35475.3276)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:49  loss: 1.3953 (1.3953)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.3326  data: 4.1579  max mem: 15572
Val:  [ 10/136]  eta: 0:01:52  loss: 2.0008 (1.9472)  acc1: 61.1111 (52.0202)  acc5: 83.3333 (82.3232)  time: 0.8925  data: 0.6835  max mem: 15572
Val:  [ 20/136]  eta: 0:01:14  loss: 2.2599 (2.1436)  acc1: 44.4444 (47.8836)  acc5: 77.7778 (78.8360)  time: 0.4570  data: 0.2402  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 2.1594 (2.0259)  acc1: 50.0000 (51.2545)  acc5: 83.3333 (80.4660)  time: 0.3090  data: 0.0939  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 1.6856 (1.9906)  acc1: 61.1111 (52.8455)  acc5: 83.3333 (81.3008)  time: 0.3320  data: 0.1193  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 1.8615 (1.9901)  acc1: 55.5556 (53.3769)  acc5: 83.3333 (81.9172)  time: 0.3834  data: 0.1709  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 2.0969 (2.0814)  acc1: 50.0000 (50.0911)  acc5: 77.7778 (80.3279)  time: 0.3927  data: 0.1718  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 1.9285 (2.0487)  acc1: 50.0000 (50.7825)  acc5: 77.7778 (80.6729)  time: 0.3955  data: 0.1806  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 1.8326 (2.0484)  acc1: 50.0000 (50.4801)  acc5: 88.8889 (81.1385)  time: 0.3990  data: 0.1980  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0163 (2.0549)  acc1: 50.0000 (50.4274)  acc5: 83.3333 (80.9524)  time: 0.3919  data: 0.1803  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 2.3236 (2.1306)  acc1: 38.8889 (48.4598)  acc5: 72.2222 (78.9879)  time: 0.3408  data: 0.1259  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2646 (2.1226)  acc1: 50.0000 (49.0991)  acc5: 72.2222 (78.8789)  time: 0.3711  data: 0.1711  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8921 (2.0783)  acc1: 61.1111 (49.9541)  acc5: 83.3333 (79.3848)  time: 0.3059  data: 0.1246  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6013 (2.0375)  acc1: 61.1111 (51.0178)  acc5: 88.8889 (80.0679)  time: 0.1710  data: 0.0150  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7276 (2.0341)  acc1: 55.5556 (51.3104)  acc5: 88.8889 (80.1802)  time: 0.1576  data: 0.0149  max mem: 15572
Val: Total time: 0:00:51 (0.3758 s / it)
* Acc@1 50.655 Acc@5 79.320 loss 2.079
Accuracy of the network on the 4883 val videos: 50.7%
[2025-01-11 00:38:18,187] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-11 00:38:18,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-11 00:38:18,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-11 00:38:18,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-11 00:38:20,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-11 00:38:20,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.66%
Epoch: [36]  [   0/1404]  eta: 2:48:43  lr: 0.000003  min_lr: 0.000000  loss: 4.3309 (4.3309)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2106  data: 6.0028  max mem: 15572
[2025-01-11 00:38:35,803] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:38:35,803] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:38:35,804] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:38:35,805] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [  10/1404]  eta: 0:31:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6883 (3.8274)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 1.3740  data: 0.6079  max mem: 15572
Epoch: [36]  [  20/1404]  eta: 0:22:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6510 (3.7166)  loss_scale: 65536.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0345  max mem: 15572
[2025-01-11 00:38:45,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50573
[2025-01-11 00:38:45,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:38:45,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:38:45,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50573
[2025-01-11 00:38:45,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [  30/1404]  eta: 0:18:55  lr: 0.000003  min_lr: 0.000000  loss: 3.6741 (3.7337)  loss_scale: 65536.0000 (52851.6129)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0012  max mem: 15572
Epoch: [36]  [  40/1404]  eta: 0:17:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7145 (3.6833)  loss_scale: 32768.0000 (47953.1707)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0014  max mem: 15572
Epoch: [36]  [  50/1404]  eta: 0:16:00  lr: 0.000003  min_lr: 0.000000  loss: 3.6687 (3.6890)  loss_scale: 32768.0000 (44975.6863)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0011  max mem: 15572
Epoch: [36]  [  60/1404]  eta: 0:15:25  lr: 0.000003  min_lr: 0.000000  loss: 3.4028 (3.6272)  loss_scale: 32768.0000 (42974.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0011  max mem: 15572
Epoch: [36]  [  70/1404]  eta: 0:14:59  lr: 0.000003  min_lr: 0.000000  loss: 3.3482 (3.6120)  loss_scale: 32768.0000 (41536.9014)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0009  max mem: 15572
Epoch: [36]  [  80/1404]  eta: 0:14:51  lr: 0.000003  min_lr: 0.000000  loss: 3.3482 (3.5548)  loss_scale: 32768.0000 (40454.3210)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0220  max mem: 15572
Epoch: [36]  [  90/1404]  eta: 0:14:32  lr: 0.000003  min_lr: 0.000000  loss: 3.0568 (3.5201)  loss_scale: 32768.0000 (39609.6703)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0524  max mem: 15572
Epoch: [36]  [ 100/1404]  eta: 0:14:30  lr: 0.000003  min_lr: 0.000000  loss: 3.4286 (3.5507)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1025  max mem: 15572
Epoch: [36]  [ 110/1404]  eta: 0:14:09  lr: 0.000003  min_lr: 0.000000  loss: 3.6211 (3.5265)  loss_scale: 32768.0000 (38376.9369)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.0917  max mem: 15572
Epoch: [36]  [ 120/1404]  eta: 0:13:54  lr: 0.000003  min_lr: 0.000000  loss: 3.5248 (3.5423)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0204  max mem: 15572
Epoch: [36]  [ 130/1404]  eta: 0:13:46  lr: 0.000003  min_lr: 0.000000  loss: 3.8169 (3.5643)  loss_scale: 32768.0000 (37520.6107)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0006  max mem: 15572
Epoch: [36]  [ 140/1404]  eta: 0:13:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5202 (3.5623)  loss_scale: 32768.0000 (37183.5461)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0005  max mem: 15572
Epoch: [36]  [ 150/1404]  eta: 0:13:25  lr: 0.000003  min_lr: 0.000000  loss: 3.6342 (3.5856)  loss_scale: 32768.0000 (36891.1258)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0008  max mem: 15572
[2025-01-11 00:40:02,141] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:40:02,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:40:02,151] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:40:02,151] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 160/1404]  eta: 0:13:11  lr: 0.000003  min_lr: 0.000000  loss: 3.8257 (3.5858)  loss_scale: 32768.0000 (37245.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0010  max mem: 15572
[2025-01-11 00:40:06,003] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50706
[2025-01-11 00:40:06,003] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:40:06,036] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50706
[2025-01-11 00:40:06,037] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:40:06,037] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 170/1404]  eta: 0:13:06  lr: 0.000003  min_lr: 0.000000  loss: 3.5779 (3.5911)  loss_scale: 32768.0000 (37175.3918)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0008  max mem: 15572
Epoch: [36]  [ 180/1404]  eta: 0:12:48  lr: 0.000003  min_lr: 0.000000  loss: 3.6023 (3.5955)  loss_scale: 32768.0000 (36931.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0006  max mem: 15572
Epoch: [36]  [ 190/1404]  eta: 0:12:42  lr: 0.000003  min_lr: 0.000000  loss: 3.7668 (3.6069)  loss_scale: 32768.0000 (36713.8848)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0005  max mem: 15572
Epoch: [36]  [ 200/1404]  eta: 0:12:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7442 (3.6066)  loss_scale: 32768.0000 (36517.5721)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0005  max mem: 15572
Epoch: [36]  [ 210/1404]  eta: 0:12:22  lr: 0.000003  min_lr: 0.000000  loss: 3.5574 (3.5996)  loss_scale: 32768.0000 (36339.8673)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0006  max mem: 15572
Epoch: [36]  [ 220/1404]  eta: 0:12:17  lr: 0.000003  min_lr: 0.000000  loss: 3.4354 (3.5924)  loss_scale: 32768.0000 (36178.2443)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0006  max mem: 15572
Epoch: [36]  [ 230/1404]  eta: 0:12:07  lr: 0.000003  min_lr: 0.000000  loss: 3.4997 (3.5878)  loss_scale: 32768.0000 (36030.6147)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0006  max mem: 15572
Epoch: [36]  [ 240/1404]  eta: 0:11:55  lr: 0.000003  min_lr: 0.000000  loss: 3.4491 (3.5723)  loss_scale: 32768.0000 (35895.2365)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0007  max mem: 15572
Epoch: [36]  [ 250/1404]  eta: 0:11:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5045 (3.5694)  loss_scale: 32768.0000 (35770.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0008  max mem: 15572
Epoch: [36]  [ 260/1404]  eta: 0:11:38  lr: 0.000003  min_lr: 0.000000  loss: 3.5238 (3.5627)  loss_scale: 32768.0000 (35655.6015)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0008  max mem: 15572
Epoch: [36]  [ 270/1404]  eta: 0:11:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6249 (3.5680)  loss_scale: 32768.0000 (35549.0480)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0034  max mem: 15572
Epoch: [36]  [ 280/1404]  eta: 0:11:26  lr: 0.000003  min_lr: 0.000000  loss: 3.5850 (3.5566)  loss_scale: 32768.0000 (35450.0783)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0110  max mem: 15572
Epoch: [36]  [ 290/1404]  eta: 0:11:20  lr: 0.000003  min_lr: 0.000000  loss: 3.5237 (3.5623)  loss_scale: 32768.0000 (35357.9107)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0138  max mem: 15572
[2025-01-11 00:41:20,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:41:20,087] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:41:20,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:41:20,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:41:22,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50839
[2025-01-11 00:41:22,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:41:22,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:41:22,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50839
[2025-01-11 00:41:22,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [ 300/1404]  eta: 0:11:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7888 (3.5609)  loss_scale: 32768.0000 (35707.3223)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.0063  max mem: 15572
Epoch: [36]  [ 310/1404]  eta: 0:11:09  lr: 0.000003  min_lr: 0.000000  loss: 3.7696 (3.5661)  loss_scale: 32768.0000 (35612.8103)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0010  max mem: 15572
Epoch: [36]  [ 320/1404]  eta: 0:11:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7063 (3.5650)  loss_scale: 32768.0000 (35524.1869)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0008  max mem: 15572
Epoch: [36]  [ 330/1404]  eta: 0:10:53  lr: 0.000003  min_lr: 0.000000  loss: 3.6638 (3.5686)  loss_scale: 32768.0000 (35440.9184)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0008  max mem: 15572
Epoch: [36]  [ 340/1404]  eta: 0:10:46  lr: 0.000003  min_lr: 0.000000  loss: 3.7676 (3.5737)  loss_scale: 32768.0000 (35362.5337)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0009  max mem: 15572
Epoch: [36]  [ 350/1404]  eta: 0:10:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6458 (3.5732)  loss_scale: 32768.0000 (35288.6154)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0007  max mem: 15572
Epoch: [36]  [ 360/1404]  eta: 0:10:34  lr: 0.000003  min_lr: 0.000000  loss: 3.5910 (3.5713)  loss_scale: 32768.0000 (35218.7922)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0006  max mem: 15572
Epoch: [36]  [ 370/1404]  eta: 0:10:28  lr: 0.000003  min_lr: 0.000000  loss: 3.5910 (3.5759)  loss_scale: 32768.0000 (35152.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0010  max mem: 15572
Epoch: [36]  [ 380/1404]  eta: 0:10:23  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.5806)  loss_scale: 32768.0000 (35090.1417)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0010  max mem: 15572
Epoch: [36]  [ 390/1404]  eta: 0:10:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.5781)  loss_scale: 32768.0000 (35030.7519)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0008  max mem: 15572
Epoch: [36]  [ 400/1404]  eta: 0:10:09  lr: 0.000003  min_lr: 0.000000  loss: 3.4641 (3.5782)  loss_scale: 32768.0000 (34974.3242)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.0007  max mem: 15572
Epoch: [36]  [ 410/1404]  eta: 0:10:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7782 (3.5790)  loss_scale: 32768.0000 (34920.6423)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0007  max mem: 15572
Epoch: [36]  [ 420/1404]  eta: 0:09:56  lr: 0.000003  min_lr: 0.000000  loss: 3.6409 (3.5814)  loss_scale: 32768.0000 (34869.5107)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0007  max mem: 15572
[2025-01-11 00:42:38,294] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:42:38,295] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:42:38,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:42:38,297] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 430/1404]  eta: 0:09:48  lr: 0.000003  min_lr: 0.000000  loss: 3.5656 (3.5828)  loss_scale: 32768.0000 (35352.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0007  max mem: 15572
[2025-01-11 00:42:45,460] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50981
[2025-01-11 00:42:45,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:42:45,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50981
[2025-01-11 00:42:45,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:42:45,490] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 440/1404]  eta: 0:09:41  lr: 0.000003  min_lr: 0.000000  loss: 3.5656 (3.5790)  loss_scale: 65536.0000 (35740.1542)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0007  max mem: 15572
Epoch: [36]  [ 450/1404]  eta: 0:09:37  lr: 0.000003  min_lr: 0.000000  loss: 3.2486 (3.5709)  loss_scale: 32768.0000 (35674.2528)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0007  max mem: 15572
[2025-01-11 00:42:56,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=349, lr=[2.5382327710875498e-08, 2.5382327710875498e-08, 3.626046815839357e-08, 3.626046815839357e-08, 5.1800668797705105e-08, 5.1800668797705105e-08, 7.400095542529302e-08, 7.400095542529302e-08, 1.0571565060756145e-07, 1.0571565060756145e-07, 1.510223580108021e-07, 1.510223580108021e-07, 2.1574622572971727e-07, 2.1574622572971727e-07, 3.0820889389959615e-07, 3.0820889389959615e-07, 4.402984198565659e-07, 4.402984198565659e-07, 6.289977426522371e-07, 6.289977426522371e-07, 8.985682037889101e-07, 8.985682037889101e-07, 1.283668862555586e-06, 1.283668862555586e-06, 1.8338126607936943e-06, 1.8338126607936943e-06, 2.6197323725624208e-06, 2.6197323725624208e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:42:56,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=45.73467367233274, CurrSamplesPerSec=59.263519746893124, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [ 460/1404]  eta: 0:09:29  lr: 0.000003  min_lr: 0.000000  loss: 3.4851 (3.5739)  loss_scale: 32768.0000 (35611.2104)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0006  max mem: 15572
Epoch: [36]  [ 470/1404]  eta: 0:09:23  lr: 0.000003  min_lr: 0.000000  loss: 3.5526 (3.5688)  loss_scale: 32768.0000 (35550.8450)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0005  max mem: 15572
Epoch: [36]  [ 480/1404]  eta: 0:09:17  lr: 0.000003  min_lr: 0.000000  loss: 3.3613 (3.5670)  loss_scale: 32768.0000 (35492.9896)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.0008  max mem: 15572
Epoch: [36]  [ 490/1404]  eta: 0:09:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4188 (3.5622)  loss_scale: 32768.0000 (35437.4908)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0008  max mem: 15572
Epoch: [36]  [ 500/1404]  eta: 0:09:05  lr: 0.000003  min_lr: 0.000000  loss: 3.5433 (3.5669)  loss_scale: 32768.0000 (35384.2076)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0006  max mem: 15572
Epoch: [36]  [ 510/1404]  eta: 0:08:58  lr: 0.000003  min_lr: 0.000000  loss: 3.7004 (3.5668)  loss_scale: 32768.0000 (35333.0098)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0006  max mem: 15572
Epoch: [36]  [ 520/1404]  eta: 0:08:53  lr: 0.000003  min_lr: 0.000000  loss: 3.5819 (3.5658)  loss_scale: 32768.0000 (35283.7774)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0008  max mem: 15572
Epoch: [36]  [ 530/1404]  eta: 0:08:47  lr: 0.000003  min_lr: 0.000000  loss: 3.4548 (3.5609)  loss_scale: 32768.0000 (35236.3992)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.0008  max mem: 15572
Epoch: [36]  [ 540/1404]  eta: 0:08:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5587 (3.5631)  loss_scale: 32768.0000 (35190.7726)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0006  max mem: 15572
Epoch: [36]  [ 550/1404]  eta: 0:08:32  lr: 0.000003  min_lr: 0.000000  loss: 3.7217 (3.5669)  loss_scale: 32768.0000 (35146.8022)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0006  max mem: 15572
Epoch: [36]  [ 560/1404]  eta: 0:08:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6331 (3.5674)  loss_scale: 32768.0000 (35104.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.0006  max mem: 15572
[2025-01-11 00:44:00,988] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:44:00,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:44:01,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:44:01,027] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:44:02,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51112
[2025-01-11 00:44:02,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:44:02,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51112
[2025-01-11 00:44:02,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:44:02,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 570/1404]  eta: 0:08:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5722 (3.5668)  loss_scale: 32768.0000 (35178.2557)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0033  max mem: 15572
Epoch: [36]  [ 580/1404]  eta: 0:08:14  lr: 0.000002  min_lr: 0.000000  loss: 3.5855 (3.5674)  loss_scale: 32768.0000 (35136.7711)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0035  max mem: 15572
Epoch: [36]  [ 590/1404]  eta: 0:08:08  lr: 0.000002  min_lr: 0.000000  loss: 3.4458 (3.5622)  loss_scale: 32768.0000 (35096.6904)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0010  max mem: 15572
Epoch: [36]  [ 600/1404]  eta: 0:08:02  lr: 0.000002  min_lr: 0.000000  loss: 3.4759 (3.5619)  loss_scale: 32768.0000 (35057.9434)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0010  max mem: 15572
Epoch: [36]  [ 610/1404]  eta: 0:07:56  lr: 0.000002  min_lr: 0.000000  loss: 3.5491 (3.5605)  loss_scale: 32768.0000 (35020.4648)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0009  max mem: 15572
Epoch: [36]  [ 620/1404]  eta: 0:07:50  lr: 0.000002  min_lr: 0.000000  loss: 3.6467 (3.5626)  loss_scale: 32768.0000 (34984.1932)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0402  max mem: 15572
Epoch: [36]  [ 630/1404]  eta: 0:07:44  lr: 0.000002  min_lr: 0.000000  loss: 3.6021 (3.5610)  loss_scale: 32768.0000 (34949.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0606  max mem: 15572
Epoch: [36]  [ 640/1404]  eta: 0:07:37  lr: 0.000002  min_lr: 0.000000  loss: 3.6021 (3.5609)  loss_scale: 32768.0000 (34915.0452)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0493  max mem: 15572
Epoch: [36]  [ 650/1404]  eta: 0:07:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7152 (3.5629)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0544  max mem: 15572
Epoch: [36]  [ 660/1404]  eta: 0:07:27  lr: 0.000002  min_lr: 0.000000  loss: 3.7359 (3.5617)  loss_scale: 32768.0000 (34850.0817)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.1256  max mem: 15572
Epoch: [36]  [ 670/1404]  eta: 0:07:20  lr: 0.000002  min_lr: 0.000000  loss: 3.3831 (3.5582)  loss_scale: 32768.0000 (34819.0522)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1001  max mem: 15572
Epoch: [36]  [ 680/1404]  eta: 0:07:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4682 (3.5572)  loss_scale: 32768.0000 (34788.9339)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0006  max mem: 15572
Epoch: [36]  [ 690/1404]  eta: 0:07:06  lr: 0.000002  min_lr: 0.000000  loss: 3.4186 (3.5516)  loss_scale: 32768.0000 (34759.6874)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0006  max mem: 15572
[2025-01-11 00:45:19,830] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:45:19,830] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:45:19,842] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:45:19,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 700/1404]  eta: 0:07:01  lr: 0.000002  min_lr: 0.000000  loss: 3.1257 (3.5476)  loss_scale: 32768.0000 (34918.2539)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1100  max mem: 15572
[2025-01-11 00:45:24,026] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51249
[2025-01-11 00:45:24,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:45:24,040] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51249
[2025-01-11 00:45:24,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:45:24,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 710/1404]  eta: 0:06:55  lr: 0.000002  min_lr: 0.000000  loss: 3.1380 (3.5449)  loss_scale: 32768.0000 (35072.3601)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1246  max mem: 15572
Epoch: [36]  [ 720/1404]  eta: 0:06:50  lr: 0.000002  min_lr: 0.000000  loss: 3.4398 (3.5432)  loss_scale: 32768.0000 (35040.3994)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0894  max mem: 15572
Epoch: [36]  [ 730/1404]  eta: 0:06:43  lr: 0.000002  min_lr: 0.000000  loss: 3.5018 (3.5422)  loss_scale: 32768.0000 (35009.3133)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1266  max mem: 15572
Epoch: [36]  [ 740/1404]  eta: 0:06:37  lr: 0.000002  min_lr: 0.000000  loss: 3.5018 (3.5403)  loss_scale: 32768.0000 (34979.0661)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0964  max mem: 15572
Epoch: [36]  [ 750/1404]  eta: 0:06:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6764 (3.5458)  loss_scale: 32768.0000 (34949.6245)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0448  max mem: 15572
Epoch: [36]  [ 760/1404]  eta: 0:06:24  lr: 0.000002  min_lr: 0.000000  loss: 3.9028 (3.5463)  loss_scale: 32768.0000 (34920.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0427  max mem: 15572
Epoch: [36]  [ 770/1404]  eta: 0:06:17  lr: 0.000002  min_lr: 0.000000  loss: 3.5549 (3.5458)  loss_scale: 32768.0000 (34893.0324)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0515  max mem: 15572
Epoch: [36]  [ 780/1404]  eta: 0:06:11  lr: 0.000002  min_lr: 0.000000  loss: 3.5095 (3.5439)  loss_scale: 32768.0000 (34865.8233)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0566  max mem: 15572
Epoch: [36]  [ 790/1404]  eta: 0:06:06  lr: 0.000002  min_lr: 0.000000  loss: 3.3785 (3.5438)  loss_scale: 32768.0000 (34839.3021)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1242  max mem: 15572
Epoch: [36]  [ 800/1404]  eta: 0:06:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8477 (3.5466)  loss_scale: 32768.0000 (34813.4432)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1300  max mem: 15572
Epoch: [36]  [ 810/1404]  eta: 0:05:54  lr: 0.000002  min_lr: 0.000000  loss: 3.6195 (3.5460)  loss_scale: 32768.0000 (34788.2219)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0757  max mem: 15572
Epoch: [36]  [ 820/1404]  eta: 0:05:48  lr: 0.000002  min_lr: 0.000000  loss: 3.6317 (3.5483)  loss_scale: 32768.0000 (34763.6151)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.0531  max mem: 15572
Epoch: [36]  [ 830/1404]  eta: 0:05:43  lr: 0.000002  min_lr: 0.000000  loss: 3.7923 (3.5478)  loss_scale: 32768.0000 (34739.6005)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1488  max mem: 15572
[2025-01-11 00:46:40,230] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:46:40,231] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:46:40,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:46:40,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 840/1404]  eta: 0:05:36  lr: 0.000002  min_lr: 0.000000  loss: 3.5046 (3.5466)  loss_scale: 32768.0000 (34988.8989)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1184  max mem: 15572
[2025-01-11 00:46:43,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51385
[2025-01-11 00:46:43,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:46:43,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:46:43,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51385
[2025-01-11 00:46:43,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [ 850/1404]  eta: 0:05:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5702 (3.5492)  loss_scale: 32768.0000 (34962.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0829  max mem: 15572
Epoch: [36]  [ 860/1404]  eta: 0:05:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6921 (3.5492)  loss_scale: 32768.0000 (34937.3101)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1308  max mem: 15572
Epoch: [36]  [ 870/1404]  eta: 0:05:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5492 (3.5494)  loss_scale: 32768.0000 (34912.4041)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0814  max mem: 15572
Epoch: [36]  [ 880/1404]  eta: 0:05:12  lr: 0.000002  min_lr: 0.000000  loss: 3.6059 (3.5517)  loss_scale: 32768.0000 (34888.0636)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0910  max mem: 15572
Epoch: [36]  [ 890/1404]  eta: 0:05:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5948 (3.5502)  loss_scale: 32768.0000 (34864.2694)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0582  max mem: 15572
Epoch: [36]  [ 900/1404]  eta: 0:05:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5804 (3.5524)  loss_scale: 32768.0000 (34841.0033)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0486  max mem: 15572
Epoch: [36]  [ 910/1404]  eta: 0:04:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5804 (3.5510)  loss_scale: 32768.0000 (34818.2481)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0622  max mem: 15572
Epoch: [36]  [ 920/1404]  eta: 0:04:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3206 (3.5491)  loss_scale: 32768.0000 (34795.9870)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0470  max mem: 15572
Epoch: [36]  [ 930/1404]  eta: 0:04:41  lr: 0.000002  min_lr: 0.000000  loss: 3.5725 (3.5500)  loss_scale: 32768.0000 (34774.2041)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0335  max mem: 15572
Epoch: [36]  [ 940/1404]  eta: 0:04:36  lr: 0.000002  min_lr: 0.000000  loss: 3.3603 (3.5466)  loss_scale: 32768.0000 (34752.8842)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0789  max mem: 15572
Epoch: [36]  [ 950/1404]  eta: 0:04:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5382 (3.5483)  loss_scale: 32768.0000 (34732.0126)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.1237  max mem: 15572
Epoch: [36]  [ 960/1404]  eta: 0:04:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6154 (3.5481)  loss_scale: 32768.0000 (34711.5754)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0961  max mem: 15572
[2025-01-11 00:48:00,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:48:00,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:48:00,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:48:00,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 970/1404]  eta: 0:04:19  lr: 0.000002  min_lr: 0.000000  loss: 3.5125 (3.5469)  loss_scale: 32768.0000 (34725.3059)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1075  max mem: 15572
Epoch: [36]  [ 980/1404]  eta: 0:04:12  lr: 0.000002  min_lr: 0.000000  loss: 3.4879 (3.5464)  loss_scale: 65536.0000 (35039.3802)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0569  max mem: 15572
Epoch: [36]  [ 990/1404]  eta: 0:04:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5538 (3.5479)  loss_scale: 65536.0000 (35347.1160)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0788  max mem: 15572
[2025-01-11 00:48:14,025] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51537
[2025-01-11 00:48:14,025] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:48:14,025] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:48:14,039] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51537
[2025-01-11 00:48:14,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [1000/1404]  eta: 0:04:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6550 (3.5487)  loss_scale: 65536.0000 (35386.8212)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0964  max mem: 15572
Epoch: [36]  [1010/1404]  eta: 0:03:54  lr: 0.000002  min_lr: 0.000000  loss: 3.6550 (3.5474)  loss_scale: 32768.0000 (35360.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.0715  max mem: 15572
Epoch: [36]  [1020/1404]  eta: 0:03:48  lr: 0.000002  min_lr: 0.000000  loss: 3.5818 (3.5465)  loss_scale: 32768.0000 (35335.5220)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0647  max mem: 15572
Epoch: [36]  [1030/1404]  eta: 0:03:42  lr: 0.000002  min_lr: 0.000000  loss: 3.3806 (3.5436)  loss_scale: 32768.0000 (35310.6188)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0290  max mem: 15572
Epoch: [36]  [1040/1404]  eta: 0:03:36  lr: 0.000002  min_lr: 0.000000  loss: 3.3960 (3.5443)  loss_scale: 32768.0000 (35286.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0302  max mem: 15572
Epoch: [36]  [1050/1404]  eta: 0:03:30  lr: 0.000002  min_lr: 0.000000  loss: 3.4028 (3.5439)  loss_scale: 32768.0000 (35262.2341)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0380  max mem: 15572
Epoch: [36]  [1060/1404]  eta: 0:03:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6538 (3.5441)  loss_scale: 32768.0000 (35238.7257)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0515  max mem: 15572
Epoch: [36]  [1070/1404]  eta: 0:03:18  lr: 0.000002  min_lr: 0.000000  loss: 3.5111 (3.5439)  loss_scale: 32768.0000 (35215.6564)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.0327  max mem: 15572
Epoch: [36]  [1080/1404]  eta: 0:03:12  lr: 0.000002  min_lr: 0.000000  loss: 3.5111 (3.5428)  loss_scale: 32768.0000 (35193.0139)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0466  max mem: 15572
Epoch: [36]  [1090/1404]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 3.5690 (3.5433)  loss_scale: 32768.0000 (35170.7864)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0979  max mem: 15572
Epoch: [36]  [1100/1404]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000000  loss: 3.7431 (3.5456)  loss_scale: 32768.0000 (35148.9628)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1101  max mem: 15572
Epoch: [36]  [1110/1404]  eta: 0:02:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5887 (3.5439)  loss_scale: 32768.0000 (35127.5320)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1093  max mem: 15572
Epoch: [36]  [1120/1404]  eta: 0:02:48  lr: 0.000002  min_lr: 0.000000  loss: 3.3746 (3.5444)  loss_scale: 32768.0000 (35106.4835)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1046  max mem: 15572
[2025-01-11 00:49:29,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:49:29,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:49:29,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:49:29,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:49:32,107] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51671
[2025-01-11 00:49:32,107] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:49:32,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:49:32,111] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51671
[2025-01-11 00:49:32,111] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [36]  [1130/1404]  eta: 0:02:42  lr: 0.000002  min_lr: 0.000000  loss: 3.6365 (3.5456)  loss_scale: 32768.0000 (35230.6702)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0779  max mem: 15572
Epoch: [36]  [1140/1404]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000000  loss: 3.5933 (3.5441)  loss_scale: 32768.0000 (35209.0868)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0760  max mem: 15572
Epoch: [36]  [1150/1404]  eta: 0:02:31  lr: 0.000002  min_lr: 0.000000  loss: 3.5626 (3.5444)  loss_scale: 32768.0000 (35187.8784)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1357  max mem: 15572
Epoch: [36]  [1160/1404]  eta: 0:02:25  lr: 0.000002  min_lr: 0.000000  loss: 3.4116 (3.5428)  loss_scale: 32768.0000 (35167.0353)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0910  max mem: 15572
Epoch: [36]  [1170/1404]  eta: 0:02:19  lr: 0.000002  min_lr: 0.000000  loss: 3.4093 (3.5418)  loss_scale: 32768.0000 (35146.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0381  max mem: 15572
Epoch: [36]  [1180/1404]  eta: 0:02:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4366 (3.5418)  loss_scale: 32768.0000 (35126.4081)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0714  max mem: 15572
Epoch: [36]  [1190/1404]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5944 (3.5419)  loss_scale: 32768.0000 (35106.6062)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0530  max mem: 15572
Epoch: [36]  [1200/1404]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 3.6967 (3.5431)  loss_scale: 32768.0000 (35087.1341)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0501  max mem: 15572
Epoch: [36]  [1210/1404]  eta: 0:01:55  lr: 0.000002  min_lr: 0.000000  loss: 3.6915 (3.5430)  loss_scale: 32768.0000 (35067.9835)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0760  max mem: 15572
Epoch: [36]  [1220/1404]  eta: 0:01:49  lr: 0.000002  min_lr: 0.000000  loss: 3.4595 (3.5405)  loss_scale: 32768.0000 (35049.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0779  max mem: 15572
Epoch: [36]  [1230/1404]  eta: 0:01:43  lr: 0.000002  min_lr: 0.000000  loss: 3.4375 (3.5407)  loss_scale: 32768.0000 (35030.6158)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0830  max mem: 15572
Epoch: [36]  [1240/1404]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 3.5298 (3.5389)  loss_scale: 32768.0000 (35012.3836)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1199  max mem: 15572
Epoch: [36]  [1250/1404]  eta: 0:01:31  lr: 0.000002  min_lr: 0.000000  loss: 3.5298 (3.5382)  loss_scale: 32768.0000 (34994.4428)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1118  max mem: 15572
[2025-01-11 00:50:48,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:50:48,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:50:48,288] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:50:48,288] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1260/1404]  eta: 0:01:25  lr: 0.000002  min_lr: 0.000000  loss: 3.5644 (3.5380)  loss_scale: 32768.0000 (35106.7153)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0662  max mem: 15572
Epoch: [36]  [1270/1404]  eta: 0:01:19  lr: 0.000002  min_lr: 0.000000  loss: 3.5073 (3.5383)  loss_scale: 65536.0000 (35346.1275)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0553  max mem: 15572
Epoch: [36]  [1280/1404]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 3.5979 (3.5396)  loss_scale: 65536.0000 (35581.8017)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0590  max mem: 15572
[2025-01-11 00:51:04,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51827
[2025-01-11 00:51:04,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51827
[2025-01-11 00:51:04,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:51:04,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:51:04,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1290/1404]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5979 (3.5385)  loss_scale: 65536.0000 (35610.7699)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0593  max mem: 15572
Epoch: [36]  [1300/1404]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 3.5951 (3.5409)  loss_scale: 32768.0000 (35588.9193)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0505  max mem: 15572
Epoch: [36]  [1310/1404]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7159 (3.5413)  loss_scale: 32768.0000 (35567.4020)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0566  max mem: 15572
Epoch: [36]  [1320/1404]  eta: 0:00:49  lr: 0.000002  min_lr: 0.000000  loss: 3.6287 (3.5408)  loss_scale: 32768.0000 (35546.2104)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0378  max mem: 15572
Epoch: [36]  [1330/1404]  eta: 0:00:43  lr: 0.000002  min_lr: 0.000000  loss: 3.4270 (3.5401)  loss_scale: 32768.0000 (35525.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0004  max mem: 15572
Epoch: [36]  [1340/1404]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 3.4270 (3.5396)  loss_scale: 32768.0000 (35504.7755)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0410  max mem: 15572
Epoch: [36]  [1350/1404]  eta: 0:00:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7049 (3.5415)  loss_scale: 32768.0000 (35484.5181)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.0410  max mem: 15572
Epoch: [36]  [1360/1404]  eta: 0:00:26  lr: 0.000002  min_lr: 0.000000  loss: 3.7920 (3.5416)  loss_scale: 32768.0000 (35464.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0009  max mem: 15572
Epoch: [36]  [1370/1404]  eta: 0:00:20  lr: 0.000002  min_lr: 0.000000  loss: 3.7920 (3.5438)  loss_scale: 32768.0000 (35444.8899)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.0009  max mem: 15572
Epoch: [36]  [1380/1404]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6793 (3.5433)  loss_scale: 32768.0000 (35425.5062)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0008  max mem: 15572
Epoch: [36]  [1390/1404]  eta: 0:00:08  lr: 0.000002  min_lr: 0.000000  loss: 3.5787 (3.5442)  loss_scale: 32768.0000 (35406.4012)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0007  max mem: 15572
Epoch: [36]  [1400/1404]  eta: 0:00:02  lr: 0.000002  min_lr: 0.000000  loss: 3.6315 (3.5449)  loss_scale: 32768.0000 (35387.5689)  weight_decay: 0.0500 (0.0500)  time: 0.4398  data: 0.0003  max mem: 15572
Epoch: [36]  [1403/1404]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5274 (3.5447)  loss_scale: 32768.0000 (35381.9715)  weight_decay: 0.0500 (0.0500)  time: 0.4132  data: 0.0003  max mem: 15572
Epoch: [36] Total time: 0:13:51 (0.5922 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 3.5274 (3.5475)  loss_scale: 32768.0000 (35381.9715)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:43  loss: 1.3711 (1.3711)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.7352  data: 4.5436  max mem: 15572
Val:  [ 10/136]  eta: 0:01:46  loss: 1.9114 (1.9418)  acc1: 55.5556 (52.0202)  acc5: 83.3333 (82.3232)  time: 0.8460  data: 0.6341  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.2858 (2.1327)  acc1: 38.8889 (46.2963)  acc5: 83.3333 (80.1587)  time: 0.3665  data: 0.1565  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 2.1773 (2.0230)  acc1: 44.4444 (50.3584)  acc5: 83.3333 (80.4660)  time: 0.2985  data: 0.1065  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.7788 (1.9981)  acc1: 61.1111 (52.0325)  acc5: 83.3333 (81.0298)  time: 0.3241  data: 0.1354  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 1.8783 (2.0032)  acc1: 55.5556 (52.1786)  acc5: 83.3333 (81.5904)  time: 0.3348  data: 0.1341  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.1521 (2.0982)  acc1: 44.4444 (48.9071)  acc5: 77.7778 (80.0546)  time: 0.3709  data: 0.1745  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.0111 (2.0713)  acc1: 50.0000 (49.6088)  acc5: 77.7778 (80.1252)  time: 0.3818  data: 0.1837  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 1.8759 (2.0605)  acc1: 50.0000 (49.8628)  acc5: 83.3333 (80.6584)  time: 0.3445  data: 0.1208  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9856 (2.0692)  acc1: 50.0000 (49.7558)  acc5: 77.7778 (80.2808)  time: 0.3777  data: 0.1327  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2952 (2.1422)  acc1: 44.4444 (48.0198)  acc5: 72.2222 (78.4929)  time: 0.3801  data: 0.1499  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.1828 (2.1299)  acc1: 44.4444 (48.6987)  acc5: 77.7778 (78.4785)  time: 0.3716  data: 0.1591  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8293 (2.0839)  acc1: 55.5556 (49.6786)  acc5: 83.3333 (79.0634)  time: 0.3735  data: 0.1661  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6331 (2.0446)  acc1: 61.1111 (50.8058)  acc5: 88.8889 (79.6862)  time: 0.2426  data: 0.0655  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7244 (2.0415)  acc1: 55.5556 (50.9419)  acc5: 88.8889 (79.8526)  time: 0.1596  data: 0.0002  max mem: 15572
Val: Total time: 0:00:50 (0.3698 s / it)
* Acc@1 50.061 Acc@5 79.156 loss 2.082
Accuracy of the network on the 4883 val videos: 50.1%
Max accuracy: 50.66%
Epoch: [37]  [   0/1404]  eta: 3:27:55  lr: 0.000002  min_lr: 0.000000  loss: 3.8401 (3.8401)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8854  data: 8.3735  max mem: 15572
[2025-01-11 00:53:15,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:53:15,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:53:15,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:53:15,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:53:16,600] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51958
[2025-01-11 00:53:16,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:53:16,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [  10/1404]  eta: 0:29:36  lr: 0.000002  min_lr: 0.000000  loss: 3.7286 (3.6329)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  time: 1.2744  data: 0.7618  max mem: 15572
[2025-01-11 00:53:16,674] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51958
[2025-01-11 00:53:16,675] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [  20/1404]  eta: 0:21:10  lr: 0.000002  min_lr: 0.000000  loss: 3.6711 (3.6015)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0008  max mem: 15572
Epoch: [37]  [  30/1404]  eta: 0:18:10  lr: 0.000002  min_lr: 0.000000  loss: 3.6711 (3.6504)  loss_scale: 32768.0000 (34882.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0141  max mem: 15572
Epoch: [37]  [  40/1404]  eta: 0:16:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5131 (3.5726)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0349  max mem: 15572
Epoch: [37]  [  50/1404]  eta: 0:16:04  lr: 0.000002  min_lr: 0.000000  loss: 3.3840 (3.5839)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0747  max mem: 15572
[2025-01-11 00:53:39,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=356, lr=[1.686762371584848e-08, 1.686762371584848e-08, 2.4096605308354973e-08, 2.4096605308354973e-08, 3.442372186907854e-08, 3.442372186907854e-08, 4.917674552725506e-08, 4.917674552725506e-08, 7.025249361036437e-08, 7.025249361036437e-08, 1.0036070515766339e-07, 1.0036070515766339e-07, 1.4337243593951914e-07, 1.4337243593951914e-07, 2.048177656278845e-07, 2.048177656278845e-07, 2.92596808039835e-07, 2.92596808039835e-07, 4.179954400569072e-07, 4.179954400569072e-07, 5.971363429384388e-07, 5.971363429384388e-07, 8.530519184834842e-07, 8.530519184834842e-07, 1.2186455978335488e-06, 1.2186455978335488e-06, 1.7409222826193556e-06, 1.7409222826193556e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 00:53:39,389] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=45.77756615223566, CurrSamplesPerSec=58.230262895301756, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [  60/1404]  eta: 0:15:23  lr: 0.000002  min_lr: 0.000000  loss: 3.6336 (3.5535)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0894  max mem: 15572
Epoch: [37]  [  70/1404]  eta: 0:15:15  lr: 0.000002  min_lr: 0.000000  loss: 3.6336 (3.5637)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1304  max mem: 15572
Epoch: [37]  [  80/1404]  eta: 0:15:01  lr: 0.000002  min_lr: 0.000000  loss: 3.5764 (3.5482)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.1613  max mem: 15572
Epoch: [37]  [  90/1404]  eta: 0:14:59  lr: 0.000002  min_lr: 0.000000  loss: 3.4382 (3.5270)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.1725  max mem: 15572
Epoch: [37]  [ 100/1404]  eta: 0:14:31  lr: 0.000002  min_lr: 0.000000  loss: 3.6003 (3.5197)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1061  max mem: 15572
Epoch: [37]  [ 110/1404]  eta: 0:14:25  lr: 0.000002  min_lr: 0.000000  loss: 3.6069 (3.5147)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0895  max mem: 15572
Epoch: [37]  [ 120/1404]  eta: 0:14:04  lr: 0.000002  min_lr: 0.000000  loss: 3.5307 (3.4902)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1025  max mem: 15572
Epoch: [37]  [ 130/1404]  eta: 0:13:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6441 (3.5180)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0967  max mem: 15572
[2025-01-11 00:54:32,998] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:54:32,998] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:54:33,101] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:54:33,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 140/1404]  eta: 0:13:34  lr: 0.000002  min_lr: 0.000000  loss: 3.7295 (3.5073)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0835  max mem: 15572
[2025-01-11 00:54:33,993] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52089
[2025-01-11 00:54:33,993] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52089
[2025-01-11 00:54:33,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:54:33,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:54:33,994] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 150/1404]  eta: 0:13:26  lr: 0.000002  min_lr: 0.000000  loss: 3.8703 (3.5245)  loss_scale: 32768.0000 (33636.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0671  max mem: 15572
Epoch: [37]  [ 160/1404]  eta: 0:13:17  lr: 0.000002  min_lr: 0.000000  loss: 3.7080 (3.5144)  loss_scale: 32768.0000 (33582.1118)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1308  max mem: 15572
Epoch: [37]  [ 170/1404]  eta: 0:13:01  lr: 0.000002  min_lr: 0.000000  loss: 3.3690 (3.5259)  loss_scale: 32768.0000 (33534.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0674  max mem: 15572
Epoch: [37]  [ 180/1404]  eta: 0:12:55  lr: 0.000002  min_lr: 0.000000  loss: 3.6991 (3.5391)  loss_scale: 32768.0000 (33492.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0464  max mem: 15572
Epoch: [37]  [ 190/1404]  eta: 0:12:42  lr: 0.000002  min_lr: 0.000000  loss: 3.6991 (3.5516)  loss_scale: 32768.0000 (33454.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0433  max mem: 15572
Epoch: [37]  [ 200/1404]  eta: 0:12:28  lr: 0.000002  min_lr: 0.000000  loss: 3.7893 (3.5633)  loss_scale: 32768.0000 (33420.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0007  max mem: 15572
Epoch: [37]  [ 210/1404]  eta: 0:12:20  lr: 0.000002  min_lr: 0.000000  loss: 3.7581 (3.5713)  loss_scale: 32768.0000 (33389.1943)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0373  max mem: 15572
Epoch: [37]  [ 220/1404]  eta: 0:12:13  lr: 0.000002  min_lr: 0.000000  loss: 3.4851 (3.5711)  loss_scale: 32768.0000 (33361.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0952  max mem: 15572
Epoch: [37]  [ 230/1404]  eta: 0:12:08  lr: 0.000002  min_lr: 0.000000  loss: 3.4851 (3.5747)  loss_scale: 32768.0000 (33335.4113)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1377  max mem: 15572
Epoch: [37]  [ 240/1404]  eta: 0:12:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6699 (3.5758)  loss_scale: 32768.0000 (33311.8672)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1334  max mem: 15572
Epoch: [37]  [ 250/1404]  eta: 0:11:54  lr: 0.000002  min_lr: 0.000000  loss: 3.5716 (3.5760)  loss_scale: 32768.0000 (33290.1992)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1227  max mem: 15572
Epoch: [37]  [ 260/1404]  eta: 0:11:47  lr: 0.000002  min_lr: 0.000000  loss: 3.5086 (3.5711)  loss_scale: 32768.0000 (33270.1916)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1348  max mem: 15572
[2025-01-11 00:55:49,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:55:49,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 270/1404]  eta: 0:11:39  lr: 0.000002  min_lr: 0.000000  loss: 3.5683 (3.5716)  loss_scale: 32768.0000 (33372.5756)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1167  max mem: 15572
[2025-01-11 00:55:49,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:55:49,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:55:50,315] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52219
[2025-01-11 00:55:50,315] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:55:50,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52219
[2025-01-11 00:55:50,341] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:55:50,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 280/1404]  eta: 0:11:31  lr: 0.000002  min_lr: 0.000000  loss: 3.8322 (3.5821)  loss_scale: 32768.0000 (33351.0605)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0902  max mem: 15572
Epoch: [37]  [ 290/1404]  eta: 0:11:27  lr: 0.000002  min_lr: 0.000000  loss: 3.7920 (3.5814)  loss_scale: 32768.0000 (33331.0241)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1113  max mem: 15572
Epoch: [37]  [ 300/1404]  eta: 0:11:16  lr: 0.000002  min_lr: 0.000000  loss: 3.6582 (3.5815)  loss_scale: 32768.0000 (33312.3189)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0719  max mem: 15572
Epoch: [37]  [ 310/1404]  eta: 0:11:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6010 (3.5786)  loss_scale: 32768.0000 (33294.8167)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0006  max mem: 15572
Epoch: [37]  [ 320/1404]  eta: 0:11:00  lr: 0.000002  min_lr: 0.000000  loss: 3.5555 (3.5746)  loss_scale: 32768.0000 (33278.4050)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0506  max mem: 15572
Epoch: [37]  [ 330/1404]  eta: 0:10:51  lr: 0.000002  min_lr: 0.000000  loss: 3.1589 (3.5630)  loss_scale: 32768.0000 (33262.9849)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0507  max mem: 15572
Epoch: [37]  [ 340/1404]  eta: 0:10:47  lr: 0.000002  min_lr: 0.000000  loss: 3.3495 (3.5638)  loss_scale: 32768.0000 (33248.4692)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0805  max mem: 15572
Epoch: [37]  [ 350/1404]  eta: 0:10:40  lr: 0.000002  min_lr: 0.000000  loss: 3.4987 (3.5598)  loss_scale: 32768.0000 (33234.7806)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1157  max mem: 15572
Epoch: [37]  [ 360/1404]  eta: 0:10:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7614 (3.5682)  loss_scale: 32768.0000 (33221.8504)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0567  max mem: 15572
Epoch: [37]  [ 370/1404]  eta: 0:10:26  lr: 0.000001  min_lr: 0.000000  loss: 3.8340 (3.5679)  loss_scale: 32768.0000 (33209.6173)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0708  max mem: 15572
Epoch: [37]  [ 380/1404]  eta: 0:10:18  lr: 0.000001  min_lr: 0.000000  loss: 3.6281 (3.5708)  loss_scale: 32768.0000 (33198.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0661  max mem: 15572
Epoch: [37]  [ 390/1404]  eta: 0:10:13  lr: 0.000001  min_lr: 0.000000  loss: 3.6815 (3.5701)  loss_scale: 32768.0000 (33187.0281)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0827  max mem: 15572
[2025-01-11 00:57:06,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:57:06,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:57:06,646] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:57:06,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 400/1404]  eta: 0:10:10  lr: 0.000001  min_lr: 0.000000  loss: 3.4531 (3.5658)  loss_scale: 32768.0000 (33258.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.1526  max mem: 15572
[2025-01-11 00:57:10,121] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52353
[2025-01-11 00:57:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:57:10,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 00:57:10,122] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52353
[2025-01-11 00:57:10,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [ 410/1404]  eta: 0:10:04  lr: 0.000001  min_lr: 0.000000  loss: 3.3180 (3.5594)  loss_scale: 32768.0000 (33565.2749)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.1324  max mem: 15572
Epoch: [37]  [ 420/1404]  eta: 0:09:56  lr: 0.000001  min_lr: 0.000000  loss: 3.5315 (3.5653)  loss_scale: 32768.0000 (33546.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0588  max mem: 15572
Epoch: [37]  [ 430/1404]  eta: 0:09:49  lr: 0.000001  min_lr: 0.000000  loss: 3.9119 (3.5683)  loss_scale: 32768.0000 (33528.2784)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0447  max mem: 15572
Epoch: [37]  [ 440/1404]  eta: 0:09:43  lr: 0.000001  min_lr: 0.000000  loss: 3.6289 (3.5716)  loss_scale: 32768.0000 (33511.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0746  max mem: 15572
Epoch: [37]  [ 450/1404]  eta: 0:09:37  lr: 0.000001  min_lr: 0.000000  loss: 3.6064 (3.5679)  loss_scale: 32768.0000 (33494.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0874  max mem: 15572
Epoch: [37]  [ 460/1404]  eta: 0:09:31  lr: 0.000001  min_lr: 0.000000  loss: 3.6064 (3.5709)  loss_scale: 32768.0000 (33478.8026)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0988  max mem: 15572
Epoch: [37]  [ 470/1404]  eta: 0:09:24  lr: 0.000001  min_lr: 0.000000  loss: 3.7677 (3.5757)  loss_scale: 32768.0000 (33463.7113)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0751  max mem: 15572
Epoch: [37]  [ 480/1404]  eta: 0:09:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6413 (3.5724)  loss_scale: 32768.0000 (33449.2474)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.0549  max mem: 15572
Epoch: [37]  [ 490/1404]  eta: 0:09:10  lr: 0.000001  min_lr: 0.000000  loss: 3.5260 (3.5747)  loss_scale: 32768.0000 (33435.3727)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0492  max mem: 15572
Epoch: [37]  [ 500/1404]  eta: 0:09:02  lr: 0.000001  min_lr: 0.000000  loss: 3.5476 (3.5723)  loss_scale: 32768.0000 (33422.0519)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0229  max mem: 15572
Epoch: [37]  [ 510/1404]  eta: 0:08:57  lr: 0.000001  min_lr: 0.000000  loss: 3.4470 (3.5699)  loss_scale: 32768.0000 (33409.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0704  max mem: 15572
Epoch: [37]  [ 520/1404]  eta: 0:08:51  lr: 0.000001  min_lr: 0.000000  loss: 3.3290 (3.5705)  loss_scale: 32768.0000 (33396.9443)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0733  max mem: 15572
Epoch: [37]  [ 530/1404]  eta: 0:08:44  lr: 0.000001  min_lr: 0.000000  loss: 3.6172 (3.5719)  loss_scale: 32768.0000 (33385.0998)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0162  max mem: 15572
[2025-01-11 00:58:25,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:58:25,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:58:25,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:58:25,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 540/1404]  eta: 0:08:40  lr: 0.000001  min_lr: 0.000000  loss: 3.6172 (3.5702)  loss_scale: 32768.0000 (33797.6784)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.0734  max mem: 15572
[2025-01-11 00:58:32,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52496
[2025-01-11 00:58:32,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52496
[2025-01-11 00:58:32,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:58:32,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:58:32,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 550/1404]  eta: 0:08:34  lr: 0.000001  min_lr: 0.000000  loss: 3.4417 (3.5689)  loss_scale: 65536.0000 (34195.2813)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0678  max mem: 15572
Epoch: [37]  [ 560/1404]  eta: 0:08:30  lr: 0.000001  min_lr: 0.000000  loss: 3.4417 (3.5655)  loss_scale: 32768.0000 (34169.8396)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0008  max mem: 15572
Epoch: [37]  [ 570/1404]  eta: 0:08:22  lr: 0.000001  min_lr: 0.000000  loss: 3.5109 (3.5666)  loss_scale: 32768.0000 (34145.2890)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0006  max mem: 15572
Epoch: [37]  [ 580/1404]  eta: 0:08:15  lr: 0.000001  min_lr: 0.000000  loss: 3.6736 (3.5649)  loss_scale: 32768.0000 (34121.5835)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0008  max mem: 15572
Epoch: [37]  [ 590/1404]  eta: 0:08:09  lr: 0.000001  min_lr: 0.000000  loss: 3.6736 (3.5677)  loss_scale: 32768.0000 (34098.6802)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0008  max mem: 15572
Epoch: [37]  [ 600/1404]  eta: 0:08:02  lr: 0.000001  min_lr: 0.000000  loss: 3.7983 (3.5722)  loss_scale: 32768.0000 (34076.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0007  max mem: 15572
Epoch: [37]  [ 610/1404]  eta: 0:07:56  lr: 0.000001  min_lr: 0.000000  loss: 3.7928 (3.5725)  loss_scale: 32768.0000 (34055.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0009  max mem: 15572
Epoch: [37]  [ 620/1404]  eta: 0:07:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7065 (3.5719)  loss_scale: 32768.0000 (34034.3961)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0010  max mem: 15572
Epoch: [37]  [ 630/1404]  eta: 0:07:44  lr: 0.000001  min_lr: 0.000000  loss: 3.7512 (3.5733)  loss_scale: 32768.0000 (34014.3265)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0009  max mem: 15572
Epoch: [37]  [ 640/1404]  eta: 0:07:37  lr: 0.000001  min_lr: 0.000000  loss: 3.8251 (3.5766)  loss_scale: 32768.0000 (33994.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.0009  max mem: 15572
Epoch: [37]  [ 650/1404]  eta: 0:07:32  lr: 0.000001  min_lr: 0.000000  loss: 3.7003 (3.5733)  loss_scale: 32768.0000 (33976.0369)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.0193  max mem: 15572
Epoch: [37]  [ 660/1404]  eta: 0:07:26  lr: 0.000001  min_lr: 0.000000  loss: 3.5984 (3.5749)  loss_scale: 32768.0000 (33957.7610)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.0194  max mem: 15572
Epoch: [37]  [ 670/1404]  eta: 0:07:21  lr: 0.000001  min_lr: 0.000000  loss: 3.6394 (3.5747)  loss_scale: 32768.0000 (33940.0298)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0008  max mem: 15572
[2025-01-11 00:59:49,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:59:49,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:59:49,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 00:59:49,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 00:59:51,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52628
[2025-01-11 00:59:51,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52628
[2025-01-11 00:59:51,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:59:51,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 00:59:51,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 680/1404]  eta: 0:07:14  lr: 0.000001  min_lr: 0.000000  loss: 3.4472 (3.5705)  loss_scale: 32768.0000 (34067.1718)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0007  max mem: 15572
Epoch: [37]  [ 690/1404]  eta: 0:07:07  lr: 0.000001  min_lr: 0.000000  loss: 3.3399 (3.5704)  loss_scale: 32768.0000 (34048.3705)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0006  max mem: 15572
Epoch: [37]  [ 700/1404]  eta: 0:07:01  lr: 0.000001  min_lr: 0.000000  loss: 3.3860 (3.5680)  loss_scale: 32768.0000 (34030.1056)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0259  max mem: 15572
Epoch: [37]  [ 710/1404]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 3.2808 (3.5645)  loss_scale: 32768.0000 (34012.3544)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0798  max mem: 15572
Epoch: [37]  [ 720/1404]  eta: 0:06:49  lr: 0.000001  min_lr: 0.000000  loss: 3.2808 (3.5612)  loss_scale: 32768.0000 (33995.0957)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0877  max mem: 15572
Epoch: [37]  [ 730/1404]  eta: 0:06:42  lr: 0.000001  min_lr: 0.000000  loss: 3.3013 (3.5568)  loss_scale: 32768.0000 (33978.3092)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.0535  max mem: 15572
Epoch: [37]  [ 740/1404]  eta: 0:06:36  lr: 0.000001  min_lr: 0.000000  loss: 3.5951 (3.5552)  loss_scale: 32768.0000 (33961.9757)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0698  max mem: 15572
Epoch: [37]  [ 750/1404]  eta: 0:06:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7199 (3.5585)  loss_scale: 32768.0000 (33946.0772)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1104  max mem: 15572
Epoch: [37]  [ 760/1404]  eta: 0:06:24  lr: 0.000001  min_lr: 0.000000  loss: 3.6524 (3.5570)  loss_scale: 32768.0000 (33930.5966)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0669  max mem: 15572
Epoch: [37]  [ 770/1404]  eta: 0:06:18  lr: 0.000001  min_lr: 0.000000  loss: 3.5823 (3.5594)  loss_scale: 32768.0000 (33915.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0484  max mem: 15572
Epoch: [37]  [ 780/1404]  eta: 0:06:12  lr: 0.000001  min_lr: 0.000000  loss: 3.6375 (3.5603)  loss_scale: 32768.0000 (33900.8246)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.0942  max mem: 15572
Epoch: [37]  [ 790/1404]  eta: 0:06:06  lr: 0.000001  min_lr: 0.000000  loss: 3.4615 (3.5598)  loss_scale: 32768.0000 (33886.5032)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0787  max mem: 15572
Epoch: [37]  [ 800/1404]  eta: 0:05:59  lr: 0.000001  min_lr: 0.000000  loss: 3.5286 (3.5584)  loss_scale: 32768.0000 (33872.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0339  max mem: 15572
[2025-01-11 01:01:06,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:01:06,090] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:01:06,107] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:01:06,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 810/1404]  eta: 0:05:54  lr: 0.000001  min_lr: 0.000000  loss: 3.7383 (3.5600)  loss_scale: 32768.0000 (33939.7287)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0079  max mem: 15572
[2025-01-11 01:01:12,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52766
[2025-01-11 01:01:12,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:01:12,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:01:12,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52766
[2025-01-11 01:01:12,677] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [ 820/1404]  eta: 0:05:49  lr: 0.000001  min_lr: 0.000000  loss: 3.6568 (3.5587)  loss_scale: 32768.0000 (34204.8429)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.0432  max mem: 15572
Epoch: [37]  [ 830/1404]  eta: 0:05:42  lr: 0.000001  min_lr: 0.000000  loss: 3.3894 (3.5548)  loss_scale: 32768.0000 (34187.5523)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0432  max mem: 15572
Epoch: [37]  [ 840/1404]  eta: 0:05:36  lr: 0.000001  min_lr: 0.000000  loss: 3.0855 (3.5516)  loss_scale: 32768.0000 (34170.6730)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0173  max mem: 15572
Epoch: [37]  [ 850/1404]  eta: 0:05:30  lr: 0.000001  min_lr: 0.000000  loss: 3.4358 (3.5505)  loss_scale: 32768.0000 (34154.1904)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0597  max mem: 15572
Epoch: [37]  [ 860/1404]  eta: 0:05:25  lr: 0.000001  min_lr: 0.000000  loss: 3.4358 (3.5483)  loss_scale: 32768.0000 (34138.0906)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1105  max mem: 15572
Epoch: [37]  [ 870/1404]  eta: 0:05:18  lr: 0.000001  min_lr: 0.000000  loss: 3.4116 (3.5462)  loss_scale: 32768.0000 (34122.3605)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0791  max mem: 15572
Epoch: [37]  [ 880/1404]  eta: 0:05:12  lr: 0.000001  min_lr: 0.000000  loss: 3.4578 (3.5459)  loss_scale: 32768.0000 (34106.9875)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0174  max mem: 15572
Epoch: [37]  [ 890/1404]  eta: 0:05:06  lr: 0.000001  min_lr: 0.000000  loss: 3.3999 (3.5450)  loss_scale: 32768.0000 (34091.9596)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0539  max mem: 15572
Epoch: [37]  [ 900/1404]  eta: 0:05:00  lr: 0.000001  min_lr: 0.000000  loss: 3.1903 (3.5414)  loss_scale: 32768.0000 (34077.2653)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0532  max mem: 15572
Epoch: [37]  [ 910/1404]  eta: 0:04:53  lr: 0.000001  min_lr: 0.000000  loss: 3.3843 (3.5428)  loss_scale: 32768.0000 (34062.8935)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0176  max mem: 15572
Epoch: [37]  [ 920/1404]  eta: 0:04:47  lr: 0.000001  min_lr: 0.000000  loss: 3.4669 (3.5430)  loss_scale: 32768.0000 (34048.8339)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0411  max mem: 15572
Epoch: [37]  [ 930/1404]  eta: 0:04:41  lr: 0.000001  min_lr: 0.000000  loss: 3.6162 (3.5429)  loss_scale: 32768.0000 (34035.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0545  max mem: 15572
Epoch: [37]  [ 940/1404]  eta: 0:04:36  lr: 0.000001  min_lr: 0.000000  loss: 3.7599 (3.5447)  loss_scale: 32768.0000 (34021.6111)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1264  max mem: 15572
[2025-01-11 01:02:27,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:02:27,508] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:02:27,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:02:27,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 950/1404]  eta: 0:04:30  lr: 0.000001  min_lr: 0.000000  loss: 3.6134 (3.5430)  loss_scale: 32768.0000 (34146.2545)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1117  max mem: 15572
[2025-01-11 01:02:33,872] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52906
[2025-01-11 01:02:33,872] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:02:33,873] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:02:34,018] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52906
[2025-01-11 01:02:34,018] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [ 960/1404]  eta: 0:04:24  lr: 0.000001  min_lr: 0.000000  loss: 3.2787 (3.5409)  loss_scale: 65536.0000 (34370.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0115  max mem: 15572
Epoch: [37]  [ 970/1404]  eta: 0:04:18  lr: 0.000001  min_lr: 0.000000  loss: 3.3966 (3.5422)  loss_scale: 32768.0000 (34354.0927)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0012  max mem: 15572
Epoch: [37]  [ 980/1404]  eta: 0:04:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7296 (3.5436)  loss_scale: 32768.0000 (34337.9246)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0666  max mem: 15572
Epoch: [37]  [ 990/1404]  eta: 0:04:06  lr: 0.000001  min_lr: 0.000000  loss: 3.6365 (3.5445)  loss_scale: 32768.0000 (34322.0827)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1244  max mem: 15572
Epoch: [37]  [1000/1404]  eta: 0:04:00  lr: 0.000001  min_lr: 0.000000  loss: 3.5142 (3.5445)  loss_scale: 32768.0000 (34306.5574)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0585  max mem: 15572
Epoch: [37]  [1010/1404]  eta: 0:03:54  lr: 0.000001  min_lr: 0.000000  loss: 3.5142 (3.5423)  loss_scale: 32768.0000 (34291.3393)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0005  max mem: 15572
Epoch: [37]  [1020/1404]  eta: 0:03:48  lr: 0.000001  min_lr: 0.000000  loss: 3.3824 (3.5416)  loss_scale: 32768.0000 (34276.4192)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.0006  max mem: 15572
Epoch: [37]  [1030/1404]  eta: 0:03:43  lr: 0.000001  min_lr: 0.000000  loss: 3.6080 (3.5427)  loss_scale: 32768.0000 (34261.7886)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0008  max mem: 15572
Epoch: [37]  [1040/1404]  eta: 0:03:37  lr: 0.000001  min_lr: 0.000000  loss: 3.5725 (3.5425)  loss_scale: 32768.0000 (34247.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0007  max mem: 15572
Epoch: [37]  [1050/1404]  eta: 0:03:31  lr: 0.000001  min_lr: 0.000000  loss: 3.6064 (3.5435)  loss_scale: 32768.0000 (34233.3625)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0007  max mem: 15572
[2025-01-11 01:03:30,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=53000, skipped=363, lr=[1.0141512276577659e-08, 1.0141512276577659e-08, 1.4487874680825229e-08, 1.4487874680825229e-08, 2.069696382975033e-08, 2.069696382975033e-08, 2.9567091185357615e-08, 2.9567091185357615e-08, 4.223870169336803e-08, 4.223870169336803e-08, 6.034100241909718e-08, 6.034100241909718e-08, 8.620143202728168e-08, 8.620143202728168e-08, 1.231449028961167e-07, 1.231449028961167e-07, 1.759212898515953e-07, 1.759212898515953e-07, 2.513161283594219e-07, 2.513161283594219e-07, 3.5902304051345984e-07, 3.5902304051345984e-07, 5.128900578763712e-07, 5.128900578763712e-07, 7.327000826805304e-07, 7.327000826805304e-07, 1.0467144038293292e-06, 1.0467144038293292e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 01:03:30,186] [INFO] [timer.py:260:stop] epoch=0/micro_step=53000/global_step=53000, RunningAvgSamplesPerSec=45.813664696945985, CurrSamplesPerSec=50.49758305792835, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [1060/1404]  eta: 0:03:25  lr: 0.000001  min_lr: 0.000000  loss: 3.6276 (3.5438)  loss_scale: 32768.0000 (34219.5514)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0009  max mem: 15572
Epoch: [37]  [1070/1404]  eta: 0:03:18  lr: 0.000001  min_lr: 0.000000  loss: 3.4717 (3.5436)  loss_scale: 32768.0000 (34205.9981)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0008  max mem: 15572
Epoch: [37]  [1080/1404]  eta: 0:03:13  lr: 0.000001  min_lr: 0.000000  loss: 3.5126 (3.5440)  loss_scale: 32768.0000 (34192.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0005  max mem: 15572
[2025-01-11 01:03:51,763] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:03:51,763] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:03:51,792] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:03:51,794] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1090/1404]  eta: 0:03:07  lr: 0.000001  min_lr: 0.000000  loss: 3.6530 (3.5448)  loss_scale: 32768.0000 (34299.7764)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0006  max mem: 15572
[2025-01-11 01:03:53,720] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53039
[2025-01-11 01:03:53,720] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:03:53,720] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:03:53,780] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53039
[2025-01-11 01:03:53,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [1100/1404]  eta: 0:03:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7415 (3.5438)  loss_scale: 32768.0000 (34285.8638)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0007  max mem: 15572
Epoch: [37]  [1110/1404]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 3.5370 (3.5442)  loss_scale: 32768.0000 (34272.2016)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0007  max mem: 15572
Epoch: [37]  [1120/1404]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 3.5370 (3.5446)  loss_scale: 32768.0000 (34258.7832)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0007  max mem: 15572
Epoch: [37]  [1130/1404]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 3.5199 (3.5438)  loss_scale: 32768.0000 (34245.6021)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0008  max mem: 15572
Epoch: [37]  [1140/1404]  eta: 0:02:37  lr: 0.000001  min_lr: 0.000000  loss: 3.6058 (3.5445)  loss_scale: 32768.0000 (34232.6521)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0009  max mem: 15572
Epoch: [37]  [1150/1404]  eta: 0:02:31  lr: 0.000001  min_lr: 0.000000  loss: 3.6695 (3.5445)  loss_scale: 32768.0000 (34219.9270)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0007  max mem: 15572
Epoch: [37]  [1160/1404]  eta: 0:02:25  lr: 0.000001  min_lr: 0.000000  loss: 3.7026 (3.5459)  loss_scale: 32768.0000 (34207.4212)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0008  max mem: 15572
Epoch: [37]  [1170/1404]  eta: 0:02:19  lr: 0.000001  min_lr: 0.000000  loss: 3.8025 (3.5479)  loss_scale: 32768.0000 (34195.1289)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0007  max mem: 15572
Epoch: [37]  [1180/1404]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 3.7641 (3.5461)  loss_scale: 32768.0000 (34183.0449)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0010  max mem: 15572
Epoch: [37]  [1190/1404]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 3.4297 (3.5463)  loss_scale: 32768.0000 (34171.1637)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0011  max mem: 15572
Epoch: [37]  [1200/1404]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5923 (3.5494)  loss_scale: 32768.0000 (34159.4804)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0011  max mem: 15572
Epoch: [37]  [1210/1404]  eta: 0:01:55  lr: 0.000001  min_lr: 0.000000  loss: 3.6293 (3.5504)  loss_scale: 32768.0000 (34147.9901)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0010  max mem: 15572
[2025-01-11 01:05:09,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:05:09,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:05:09,651] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:05:09,651] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1220/1404]  eta: 0:01:49  lr: 0.000001  min_lr: 0.000000  loss: 3.5347 (3.5504)  loss_scale: 32768.0000 (34163.5250)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0008  max mem: 15572
[2025-01-11 01:05:14,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53177
[2025-01-11 01:05:14,743] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:05:14,749] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53177
[2025-01-11 01:05:14,749] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:05:14,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1230/1404]  eta: 0:01:43  lr: 0.000001  min_lr: 0.000000  loss: 3.3465 (3.5485)  loss_scale: 32768.0000 (34365.1405)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.0007  max mem: 15572
Epoch: [37]  [1240/1404]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 3.5769 (3.5503)  loss_scale: 32768.0000 (34352.2707)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0008  max mem: 15572
Epoch: [37]  [1250/1404]  eta: 0:01:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7397 (3.5492)  loss_scale: 32768.0000 (34339.6067)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0010  max mem: 15572
Epoch: [37]  [1260/1404]  eta: 0:01:25  lr: 0.000001  min_lr: 0.000000  loss: 3.2922 (3.5470)  loss_scale: 32768.0000 (34327.1435)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0009  max mem: 15572
Epoch: [37]  [1270/1404]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000000  loss: 3.1716 (3.5449)  loss_scale: 32768.0000 (34314.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0009  max mem: 15572
Epoch: [37]  [1280/1404]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 3.5325 (3.5454)  loss_scale: 32768.0000 (34302.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0009  max mem: 15572
Epoch: [37]  [1290/1404]  eta: 0:01:07  lr: 0.000001  min_lr: 0.000000  loss: 3.5325 (3.5444)  loss_scale: 32768.0000 (34290.9125)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0008  max mem: 15572
Epoch: [37]  [1300/1404]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 3.4512 (3.5435)  loss_scale: 32768.0000 (34279.2068)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0009  max mem: 15572
Epoch: [37]  [1310/1404]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 3.5442 (3.5426)  loss_scale: 32768.0000 (34267.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0010  max mem: 15572
Epoch: [37]  [1320/1404]  eta: 0:00:49  lr: 0.000001  min_lr: 0.000000  loss: 3.2128 (3.5398)  loss_scale: 32768.0000 (34256.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0010  max mem: 15572
Epoch: [37]  [1330/1404]  eta: 0:00:43  lr: 0.000001  min_lr: 0.000000  loss: 3.3883 (3.5407)  loss_scale: 32768.0000 (34245.1450)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0009  max mem: 15572
Epoch: [37]  [1340/1404]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 3.5601 (3.5406)  loss_scale: 32768.0000 (34234.1298)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0007  max mem: 15572
Epoch: [37]  [1350/1404]  eta: 0:00:32  lr: 0.000001  min_lr: 0.000000  loss: 3.5418 (3.5412)  loss_scale: 32768.0000 (34223.2776)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0006  max mem: 15572
[2025-01-11 01:06:30,698] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:06:30,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:06:30,735] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:06:30,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:06:31,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53308
[2025-01-11 01:06:31,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:06:31,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:06:31,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53308
[2025-01-11 01:06:31,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [1360/1404]  eta: 0:00:26  lr: 0.000001  min_lr: 0.000000  loss: 3.6292 (3.5406)  loss_scale: 32768.0000 (34260.7377)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0007  max mem: 15572
Epoch: [37]  [1370/1404]  eta: 0:00:20  lr: 0.000001  min_lr: 0.000000  loss: 3.4965 (3.5392)  loss_scale: 32768.0000 (34249.8497)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0007  max mem: 15572
Epoch: [37]  [1380/1404]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 3.5787 (3.5392)  loss_scale: 32768.0000 (34239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.0006  max mem: 15572
Epoch: [37]  [1390/1404]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000000  loss: 3.6077 (3.5379)  loss_scale: 32768.0000 (34228.5435)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0008  max mem: 15572
Epoch: [37]  [1400/1404]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000000  loss: 3.5576 (3.5368)  loss_scale: 32768.0000 (34218.1185)  weight_decay: 0.0500 (0.0500)  time: 0.4295  data: 0.0005  max mem: 15572
Epoch: [37]  [1403/1404]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 3.5531 (3.5359)  loss_scale: 32768.0000 (34215.0199)  weight_decay: 0.0500 (0.0500)  time: 0.4158  data: 0.0004  max mem: 15572
Epoch: [37] Total time: 0:13:52 (0.5929 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 3.5531 (3.5356)  loss_scale: 32768.0000 (34215.0199)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:58  loss: 1.4311 (1.4311)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.3998  data: 4.2213  max mem: 15572
Val:  [ 10/136]  eta: 0:01:43  loss: 1.8758 (1.9643)  acc1: 55.5556 (54.5455)  acc5: 83.3333 (82.3232)  time: 0.8206  data: 0.6233  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.2446 (2.1381)  acc1: 50.0000 (48.4127)  acc5: 83.3333 (80.4233)  time: 0.3881  data: 0.1781  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 2.1730 (2.0292)  acc1: 50.0000 (51.0753)  acc5: 83.3333 (81.0036)  time: 0.2752  data: 0.0704  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.8320 (1.9981)  acc1: 61.1111 (52.4390)  acc5: 83.3333 (81.4363)  time: 0.3177  data: 0.0982  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 1.8320 (1.9933)  acc1: 55.5556 (53.1590)  acc5: 83.3333 (81.9172)  time: 0.3952  data: 0.1694  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.0521 (2.0895)  acc1: 50.0000 (49.6357)  acc5: 77.7778 (80.6011)  time: 0.3939  data: 0.1604  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.0297 (2.0607)  acc1: 44.4444 (50.2347)  acc5: 77.7778 (80.7512)  time: 0.3769  data: 0.1403  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 1.8484 (2.0520)  acc1: 50.0000 (50.5487)  acc5: 88.8889 (81.1385)  time: 0.3937  data: 0.1663  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.0046 (2.0623)  acc1: 44.4444 (50.1832)  acc5: 83.3333 (80.8303)  time: 0.3877  data: 0.1532  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3028 (2.1394)  acc1: 38.8889 (48.1298)  acc5: 72.2222 (78.9879)  time: 0.3670  data: 0.1387  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.1152 (2.1289)  acc1: 44.4444 (48.5485)  acc5: 77.7778 (79.2292)  time: 0.3729  data: 0.1428  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8042 (2.0827)  acc1: 55.5556 (49.6327)  acc5: 83.3333 (79.7062)  time: 0.3545  data: 0.1561  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6399 (2.0401)  acc1: 61.1111 (50.6361)  acc5: 88.8889 (80.3223)  time: 0.2454  data: 0.0900  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7551 (2.0382)  acc1: 50.0000 (50.9009)  acc5: 88.8889 (80.4259)  time: 0.2039  data: 0.0622  max mem: 15572
Val: Total time: 0:00:50 (0.3745 s / it)
* Acc@1 50.082 Acc@5 79.382 loss 2.082
Accuracy of the network on the 4883 val videos: 50.1%
Max accuracy: 50.66%
Epoch: [38]  [   0/1404]  eta: 3:33:07  lr: 0.000001  min_lr: 0.000000  loss: 3.4227 (3.4227)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1079  data: 5.8150  max mem: 15572
Epoch: [38]  [  10/1404]  eta: 0:29:54  lr: 0.000001  min_lr: 0.000000  loss: 3.6544 (3.6136)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2874  data: 0.5292  max mem: 15572
Epoch: [38]  [  20/1404]  eta: 0:21:11  lr: 0.000001  min_lr: 0.000000  loss: 3.6674 (3.5786)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0055  max mem: 15572
Epoch: [38]  [  30/1404]  eta: 0:18:13  lr: 0.000001  min_lr: 0.000000  loss: 3.4270 (3.4753)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0056  max mem: 15572
Epoch: [38]  [  40/1404]  eta: 0:17:12  lr: 0.000001  min_lr: 0.000000  loss: 3.3167 (3.4776)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0619  max mem: 15572
Epoch: [38]  [  50/1404]  eta: 0:15:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6244 (3.5263)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0622  max mem: 15572
Epoch: [38]  [  60/1404]  eta: 0:15:25  lr: 0.000001  min_lr: 0.000000  loss: 3.6244 (3.5201)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0441  max mem: 15572
Epoch: [38]  [  70/1404]  eta: 0:14:54  lr: 0.000001  min_lr: 0.000000  loss: 3.5904 (3.5213)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0577  max mem: 15572
Epoch: [38]  [  80/1404]  eta: 0:14:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6784 (3.5272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0493  max mem: 15572
[2025-01-11 01:08:43,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:08:43,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:08:43,125] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:08:43,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [  90/1404]  eta: 0:14:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6784 (3.5329)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0353  max mem: 15572
Epoch: [38]  [ 100/1404]  eta: 0:14:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6266 (3.5409)  loss_scale: 65536.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0006  max mem: 15572
[2025-01-11 01:08:55,299] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53457
[2025-01-11 01:08:55,299] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:08:55,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53457
[2025-01-11 01:08:55,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:08:55,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 110/1404]  eta: 0:14:11  lr: 0.000001  min_lr: 0.000000  loss: 3.6125 (3.5396)  loss_scale: 65536.0000 (38672.1441)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0007  max mem: 15572
Epoch: [38]  [ 120/1404]  eta: 0:14:04  lr: 0.000001  min_lr: 0.000000  loss: 3.3577 (3.5310)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0016  max mem: 15572
Epoch: [38]  [ 130/1404]  eta: 0:13:46  lr: 0.000001  min_lr: 0.000000  loss: 3.3779 (3.5177)  loss_scale: 32768.0000 (37770.7481)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0014  max mem: 15572
Epoch: [38]  [ 140/1404]  eta: 0:13:37  lr: 0.000001  min_lr: 0.000000  loss: 3.4881 (3.5412)  loss_scale: 32768.0000 (37415.9433)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0007  max mem: 15572
Epoch: [38]  [ 150/1404]  eta: 0:13:24  lr: 0.000001  min_lr: 0.000000  loss: 3.6745 (3.5459)  loss_scale: 32768.0000 (37108.1325)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0008  max mem: 15572
Epoch: [38]  [ 160/1404]  eta: 0:13:18  lr: 0.000001  min_lr: 0.000000  loss: 3.5463 (3.5430)  loss_scale: 32768.0000 (36838.5590)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0008  max mem: 15572
Epoch: [38]  [ 170/1404]  eta: 0:13:06  lr: 0.000001  min_lr: 0.000000  loss: 3.4462 (3.5388)  loss_scale: 32768.0000 (36600.5146)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0010  max mem: 15572
Epoch: [38]  [ 180/1404]  eta: 0:12:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6306 (3.5474)  loss_scale: 32768.0000 (36388.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0010  max mem: 15572
Epoch: [38]  [ 190/1404]  eta: 0:12:49  lr: 0.000001  min_lr: 0.000000  loss: 3.6545 (3.5497)  loss_scale: 32768.0000 (36199.2042)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0009  max mem: 15572
Epoch: [38]  [ 200/1404]  eta: 0:12:42  lr: 0.000001  min_lr: 0.000000  loss: 3.5334 (3.5526)  loss_scale: 32768.0000 (36028.4975)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0009  max mem: 15572
Epoch: [38]  [ 210/1404]  eta: 0:12:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6507 (3.5491)  loss_scale: 32768.0000 (35873.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0007  max mem: 15572
Epoch: [38]  [ 220/1404]  eta: 0:12:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6507 (3.5480)  loss_scale: 32768.0000 (35733.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0007  max mem: 15572
Epoch: [38]  [ 230/1404]  eta: 0:12:11  lr: 0.000001  min_lr: 0.000000  loss: 3.5228 (3.5474)  loss_scale: 32768.0000 (35605.0563)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0008  max mem: 15572
[2025-01-11 01:10:11,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:10:11,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:10:11,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:10:11,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 240/1404]  eta: 0:12:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7962 (3.5609)  loss_scale: 32768.0000 (36439.1037)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0007  max mem: 15572
Epoch: [38]  [ 250/1404]  eta: 0:11:55  lr: 0.000001  min_lr: 0.000000  loss: 3.7156 (3.5555)  loss_scale: 65536.0000 (37598.3426)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0006  max mem: 15572
[2025-01-11 01:10:26,575] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53611
[2025-01-11 01:10:26,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:10:26,606] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53611
[2025-01-11 01:10:26,606] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:10:26,606] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 260/1404]  eta: 0:11:48  lr: 0.000001  min_lr: 0.000000  loss: 3.3575 (3.5534)  loss_scale: 65536.0000 (38417.6552)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0006  max mem: 15572
Epoch: [38]  [ 270/1404]  eta: 0:11:45  lr: 0.000001  min_lr: 0.000000  loss: 3.3575 (3.5408)  loss_scale: 32768.0000 (38209.1808)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.0007  max mem: 15572
Epoch: [38]  [ 280/1404]  eta: 0:11:34  lr: 0.000001  min_lr: 0.000000  loss: 3.3177 (3.5318)  loss_scale: 32768.0000 (38015.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0009  max mem: 15572
Epoch: [38]  [ 290/1404]  eta: 0:11:25  lr: 0.000001  min_lr: 0.000000  loss: 3.3214 (3.5227)  loss_scale: 32768.0000 (37835.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0007  max mem: 15572
Epoch: [38]  [ 300/1404]  eta: 0:11:18  lr: 0.000001  min_lr: 0.000000  loss: 3.4341 (3.5306)  loss_scale: 32768.0000 (37666.8704)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0005  max mem: 15572
Epoch: [38]  [ 310/1404]  eta: 0:11:13  lr: 0.000001  min_lr: 0.000000  loss: 3.7752 (3.5180)  loss_scale: 32768.0000 (37509.3505)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0007  max mem: 15572
Epoch: [38]  [ 320/1404]  eta: 0:11:03  lr: 0.000001  min_lr: 0.000000  loss: 3.1259 (3.5093)  loss_scale: 32768.0000 (37361.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0008  max mem: 15572
Epoch: [38]  [ 330/1404]  eta: 0:10:58  lr: 0.000001  min_lr: 0.000000  loss: 3.2643 (3.5036)  loss_scale: 32768.0000 (37222.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0006  max mem: 15572
Epoch: [38]  [ 340/1404]  eta: 0:10:49  lr: 0.000001  min_lr: 0.000000  loss: 3.4689 (3.5087)  loss_scale: 32768.0000 (37092.2229)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0007  max mem: 15572
Epoch: [38]  [ 350/1404]  eta: 0:10:42  lr: 0.000001  min_lr: 0.000000  loss: 3.6441 (3.5119)  loss_scale: 32768.0000 (36969.0256)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0008  max mem: 15572
Epoch: [38]  [ 360/1404]  eta: 0:10:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6551 (3.5138)  loss_scale: 32768.0000 (36852.6537)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0007  max mem: 15572
Epoch: [38]  [ 370/1404]  eta: 0:10:28  lr: 0.000001  min_lr: 0.000000  loss: 3.4056 (3.5123)  loss_scale: 32768.0000 (36742.5553)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0005  max mem: 15572
Epoch: [38]  [ 380/1404]  eta: 0:10:22  lr: 0.000001  min_lr: 0.000000  loss: 3.4802 (3.5139)  loss_scale: 32768.0000 (36638.2362)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0006  max mem: 15572
[2025-01-11 01:11:42,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:11:42,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:11:42,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:11:42,377] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 390/1404]  eta: 0:10:16  lr: 0.000001  min_lr: 0.000000  loss: 3.6546 (3.5168)  loss_scale: 32768.0000 (36790.6701)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0008  max mem: 15572
[2025-01-11 01:11:47,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53749
[2025-01-11 01:11:47,958] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:11:47,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53749
[2025-01-11 01:11:47,958] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:11:47,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 400/1404]  eta: 0:10:09  lr: 0.000001  min_lr: 0.000000  loss: 3.6546 (3.5134)  loss_scale: 32768.0000 (37180.6484)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0008  max mem: 15572
Epoch: [38]  [ 410/1404]  eta: 0:10:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5870 (3.5157)  loss_scale: 32768.0000 (37073.2847)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0008  max mem: 15572
Epoch: [38]  [ 420/1404]  eta: 0:09:53  lr: 0.000001  min_lr: 0.000000  loss: 3.5983 (3.5131)  loss_scale: 32768.0000 (36971.0214)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0009  max mem: 15572
Epoch: [38]  [ 430/1404]  eta: 0:09:48  lr: 0.000001  min_lr: 0.000000  loss: 3.5221 (3.5088)  loss_scale: 32768.0000 (36873.5035)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
Epoch: [38]  [ 440/1404]  eta: 0:09:44  lr: 0.000001  min_lr: 0.000000  loss: 3.5221 (3.5061)  loss_scale: 32768.0000 (36780.4082)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.0006  max mem: 15572
Epoch: [38]  [ 450/1404]  eta: 0:09:38  lr: 0.000001  min_lr: 0.000000  loss: 3.3732 (3.5079)  loss_scale: 32768.0000 (36691.4412)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0005  max mem: 15572
Epoch: [38]  [ 460/1404]  eta: 0:09:31  lr: 0.000001  min_lr: 0.000000  loss: 3.5811 (3.5094)  loss_scale: 32768.0000 (36606.3341)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0006  max mem: 15572
Epoch: [38]  [ 470/1404]  eta: 0:09:25  lr: 0.000001  min_lr: 0.000000  loss: 3.5554 (3.5084)  loss_scale: 32768.0000 (36524.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.0006  max mem: 15572
Epoch: [38]  [ 480/1404]  eta: 0:09:18  lr: 0.000001  min_lr: 0.000000  loss: 3.5643 (3.5105)  loss_scale: 32768.0000 (36446.7360)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0006  max mem: 15572
Epoch: [38]  [ 490/1404]  eta: 0:09:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8323 (3.5179)  loss_scale: 32768.0000 (36371.8126)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0007  max mem: 15572
Epoch: [38]  [ 500/1404]  eta: 0:09:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7047 (3.5185)  loss_scale: 32768.0000 (36299.8802)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0008  max mem: 15572
Epoch: [38]  [ 510/1404]  eta: 0:08:59  lr: 0.000001  min_lr: 0.000000  loss: 3.6880 (3.5173)  loss_scale: 32768.0000 (36230.7632)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0009  max mem: 15572
Epoch: [38]  [ 520/1404]  eta: 0:08:53  lr: 0.000001  min_lr: 0.000000  loss: 3.7693 (3.5185)  loss_scale: 32768.0000 (36164.2994)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0011  max mem: 15572
[2025-01-11 01:13:04,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:13:04,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:13:04,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:13:04,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 530/1404]  eta: 0:08:47  lr: 0.000001  min_lr: 0.000000  loss: 3.7411 (3.5221)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0010  max mem: 15572
[2025-01-11 01:13:13,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53892
[2025-01-11 01:13:13,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:13:13,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 540/1404]  eta: 0:08:42  lr: 0.000001  min_lr: 0.000000  loss: 3.6359 (3.5232)  loss_scale: 65536.0000 (36886.7135)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0007  max mem: 15572
[2025-01-11 01:13:13,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53892
[2025-01-11 01:13:13,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [38]  [ 550/1404]  eta: 0:08:34  lr: 0.000001  min_lr: 0.000000  loss: 3.5155 (3.5234)  loss_scale: 32768.0000 (36811.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0005  max mem: 15572
Epoch: [38]  [ 560/1404]  eta: 0:08:27  lr: 0.000001  min_lr: 0.000000  loss: 3.5155 (3.5221)  loss_scale: 32768.0000 (36739.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0006  max mem: 15572
Epoch: [38]  [ 570/1404]  eta: 0:08:20  lr: 0.000001  min_lr: 0.000000  loss: 3.2771 (3.5175)  loss_scale: 32768.0000 (36670.3187)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0009  max mem: 15572
Epoch: [38]  [ 580/1404]  eta: 0:08:14  lr: 0.000001  min_lr: 0.000000  loss: 3.4506 (3.5191)  loss_scale: 32768.0000 (36603.1532)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0009  max mem: 15572
Epoch: [38]  [ 590/1404]  eta: 0:08:07  lr: 0.000001  min_lr: 0.000000  loss: 3.8934 (3.5261)  loss_scale: 32768.0000 (36538.2606)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0205  max mem: 15572
Epoch: [38]  [ 600/1404]  eta: 0:08:02  lr: 0.000001  min_lr: 0.000000  loss: 3.8627 (3.5261)  loss_scale: 32768.0000 (36475.5275)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0203  max mem: 15572
Epoch: [38]  [ 610/1404]  eta: 0:07:57  lr: 0.000001  min_lr: 0.000000  loss: 3.3204 (3.5240)  loss_scale: 32768.0000 (36414.8478)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0005  max mem: 15572
Epoch: [38]  [ 620/1404]  eta: 0:07:50  lr: 0.000001  min_lr: 0.000000  loss: 3.4276 (3.5217)  loss_scale: 32768.0000 (36356.1224)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0005  max mem: 15572
Epoch: [38]  [ 630/1404]  eta: 0:07:45  lr: 0.000001  min_lr: 0.000000  loss: 3.4140 (3.5183)  loss_scale: 32768.0000 (36299.2583)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0006  max mem: 15572
Epoch: [38]  [ 640/1404]  eta: 0:07:38  lr: 0.000001  min_lr: 0.000000  loss: 3.4337 (3.5201)  loss_scale: 32768.0000 (36244.1685)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0008  max mem: 15572
[2025-01-11 01:14:15,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=370, lr=[5.231475218245322e-09, 5.231475218245322e-09, 7.473536026064748e-09, 7.473536026064748e-09, 1.0676480037235355e-08, 1.0676480037235355e-08, 1.525211433890765e-08, 1.525211433890765e-08, 2.1788734769868072e-08, 2.1788734769868072e-08, 3.112676395695439e-08, 3.112676395695439e-08, 4.4466805652791985e-08, 4.4466805652791985e-08, 6.352400807541714e-08, 6.352400807541714e-08, 9.074858296488161e-08, 9.074858296488161e-08, 1.2964083280697376e-07, 1.2964083280697376e-07, 1.852011897242482e-07, 1.852011897242482e-07, 2.6457312817749746e-07, 2.6457312817749746e-07, 3.779616116821393e-07, 3.779616116821393e-07, 5.399451595459133e-07, 5.399451595459133e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 01:14:15,559] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=45.76268576263236, CurrSamplesPerSec=50.23732741777313, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [ 650/1404]  eta: 0:07:32  lr: 0.000001  min_lr: 0.000000  loss: 3.4789 (3.5187)  loss_scale: 32768.0000 (36190.7711)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0007  max mem: 15572
Epoch: [38]  [ 660/1404]  eta: 0:07:25  lr: 0.000001  min_lr: 0.000000  loss: 3.3986 (3.5177)  loss_scale: 32768.0000 (36138.9894)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0006  max mem: 15572
[2025-01-11 01:14:27,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:14:27,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:14:27,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:14:27,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 670/1404]  eta: 0:07:20  lr: 0.000001  min_lr: 0.000000  loss: 3.4755 (3.5164)  loss_scale: 32768.0000 (36186.4203)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0007  max mem: 15572
[2025-01-11 01:14:28,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54023
[2025-01-11 01:14:28,882] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:14:28,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:14:28,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54023
[2025-01-11 01:14:28,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [38]  [ 680/1404]  eta: 0:07:13  lr: 0.000001  min_lr: 0.000000  loss: 3.7848 (3.5226)  loss_scale: 32768.0000 (36136.2232)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0008  max mem: 15572
Epoch: [38]  [ 690/1404]  eta: 0:07:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7657 (3.5242)  loss_scale: 32768.0000 (36087.4790)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0007  max mem: 15572
Epoch: [38]  [ 700/1404]  eta: 0:07:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5592 (3.5224)  loss_scale: 32768.0000 (36040.1255)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0007  max mem: 15572
Epoch: [38]  [ 710/1404]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 3.5592 (3.5242)  loss_scale: 32768.0000 (35994.1041)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.0008  max mem: 15572
Epoch: [38]  [ 720/1404]  eta: 0:06:48  lr: 0.000001  min_lr: 0.000000  loss: 3.5685 (3.5228)  loss_scale: 32768.0000 (35949.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0007  max mem: 15572
Epoch: [38]  [ 730/1404]  eta: 0:06:42  lr: 0.000001  min_lr: 0.000000  loss: 3.6256 (3.5257)  loss_scale: 32768.0000 (35905.8386)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0009  max mem: 15572
Epoch: [38]  [ 740/1404]  eta: 0:06:36  lr: 0.000001  min_lr: 0.000000  loss: 3.7480 (3.5281)  loss_scale: 32768.0000 (35863.4926)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0009  max mem: 15572
Epoch: [38]  [ 750/1404]  eta: 0:06:31  lr: 0.000000  min_lr: 0.000000  loss: 3.4892 (3.5241)  loss_scale: 32768.0000 (35822.2743)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.0675  max mem: 15572
Epoch: [38]  [ 760/1404]  eta: 0:06:26  lr: 0.000000  min_lr: 0.000000  loss: 3.3700 (3.5241)  loss_scale: 32768.0000 (35782.1393)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.1534  max mem: 15572
Epoch: [38]  [ 770/1404]  eta: 0:06:19  lr: 0.000000  min_lr: 0.000000  loss: 3.2613 (3.5181)  loss_scale: 32768.0000 (35743.0454)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0992  max mem: 15572
Epoch: [38]  [ 780/1404]  eta: 0:06:14  lr: 0.000000  min_lr: 0.000000  loss: 3.2613 (3.5154)  loss_scale: 32768.0000 (35704.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0628  max mem: 15572
Epoch: [38]  [ 790/1404]  eta: 0:06:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6538 (3.5169)  loss_scale: 32768.0000 (35667.8230)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0502  max mem: 15572
[2025-01-11 01:15:45,476] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:15:45,477] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 800/1404]  eta: 0:06:01  lr: 0.000000  min_lr: 0.000000  loss: 3.5975 (3.5163)  loss_scale: 32768.0000 (35672.5293)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0396  max mem: 15572
[2025-01-11 01:15:45,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:15:45,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:15:50,215] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54160
[2025-01-11 01:15:50,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:15:50,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54160
[2025-01-11 01:15:50,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:15:50,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 810/1404]  eta: 0:05:55  lr: 0.000000  min_lr: 0.000000  loss: 3.3088 (3.5156)  loss_scale: 32768.0000 (35919.5462)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0723  max mem: 15572
Epoch: [38]  [ 820/1404]  eta: 0:05:49  lr: 0.000000  min_lr: 0.000000  loss: 3.4796 (3.5153)  loss_scale: 32768.0000 (35881.1596)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0334  max mem: 15572
Epoch: [38]  [ 830/1404]  eta: 0:05:43  lr: 0.000000  min_lr: 0.000000  loss: 3.5374 (3.5150)  loss_scale: 32768.0000 (35843.6968)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0009  max mem: 15572
Epoch: [38]  [ 840/1404]  eta: 0:05:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5796 (3.5175)  loss_scale: 32768.0000 (35807.1249)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0009  max mem: 15572
Epoch: [38]  [ 850/1404]  eta: 0:05:31  lr: 0.000000  min_lr: 0.000000  loss: 3.6713 (3.5181)  loss_scale: 32768.0000 (35771.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0007  max mem: 15572
Epoch: [38]  [ 860/1404]  eta: 0:05:25  lr: 0.000000  min_lr: 0.000000  loss: 3.3768 (3.5161)  loss_scale: 32768.0000 (35736.5296)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0007  max mem: 15572
Epoch: [38]  [ 870/1404]  eta: 0:05:19  lr: 0.000000  min_lr: 0.000000  loss: 3.5836 (3.5184)  loss_scale: 32768.0000 (35702.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0007  max mem: 15572
Epoch: [38]  [ 880/1404]  eta: 0:05:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5333 (3.5189)  loss_scale: 32768.0000 (35669.1396)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0008  max mem: 15572
Epoch: [38]  [ 890/1404]  eta: 0:05:07  lr: 0.000000  min_lr: 0.000000  loss: 3.5006 (3.5203)  loss_scale: 32768.0000 (35636.5791)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0007  max mem: 15572
Epoch: [38]  [ 900/1404]  eta: 0:05:01  lr: 0.000000  min_lr: 0.000000  loss: 3.5846 (3.5225)  loss_scale: 32768.0000 (35604.7414)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0005  max mem: 15572
Epoch: [38]  [ 910/1404]  eta: 0:04:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7261 (3.5235)  loss_scale: 32768.0000 (35573.6026)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0006  max mem: 15572
Epoch: [38]  [ 920/1404]  eta: 0:04:48  lr: 0.000000  min_lr: 0.000000  loss: 3.7740 (3.5253)  loss_scale: 32768.0000 (35543.1401)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0008  max mem: 15572
Epoch: [38]  [ 930/1404]  eta: 0:04:42  lr: 0.000000  min_lr: 0.000000  loss: 3.6887 (3.5261)  loss_scale: 32768.0000 (35513.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0007  max mem: 15572
[2025-01-11 01:17:05,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:17:05,415] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:17:05,417] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:17:05,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:17:05,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54290
[2025-01-11 01:17:05,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:17:06,033] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54290
[2025-01-11 01:17:06,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:17:06,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 940/1404]  eta: 0:04:36  lr: 0.000000  min_lr: 0.000000  loss: 3.6175 (3.5272)  loss_scale: 32768.0000 (35518.9798)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.0006  max mem: 15572
Epoch: [38]  [ 950/1404]  eta: 0:04:30  lr: 0.000000  min_lr: 0.000000  loss: 3.4993 (3.5268)  loss_scale: 32768.0000 (35490.0526)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0007  max mem: 15572
Epoch: [38]  [ 960/1404]  eta: 0:04:24  lr: 0.000000  min_lr: 0.000000  loss: 3.6486 (3.5278)  loss_scale: 32768.0000 (35461.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.0008  max mem: 15572
Epoch: [38]  [ 970/1404]  eta: 0:04:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6486 (3.5265)  loss_scale: 32768.0000 (35433.9856)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0008  max mem: 15572
Epoch: [38]  [ 980/1404]  eta: 0:04:12  lr: 0.000000  min_lr: 0.000000  loss: 3.4899 (3.5263)  loss_scale: 32768.0000 (35406.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0007  max mem: 15572
Epoch: [38]  [ 990/1404]  eta: 0:04:06  lr: 0.000000  min_lr: 0.000000  loss: 3.4942 (3.5255)  loss_scale: 32768.0000 (35380.1816)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0006  max mem: 15572
Epoch: [38]  [1000/1404]  eta: 0:04:00  lr: 0.000000  min_lr: 0.000000  loss: 3.2158 (3.5234)  loss_scale: 32768.0000 (35354.0859)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0006  max mem: 15572
Epoch: [38]  [1010/1404]  eta: 0:03:55  lr: 0.000000  min_lr: 0.000000  loss: 3.4605 (3.5225)  loss_scale: 32768.0000 (35328.5064)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0009  max mem: 15572
Epoch: [38]  [1020/1404]  eta: 0:03:49  lr: 0.000000  min_lr: 0.000000  loss: 3.4605 (3.5197)  loss_scale: 32768.0000 (35303.4280)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0009  max mem: 15572
Epoch: [38]  [1030/1404]  eta: 0:03:43  lr: 0.000000  min_lr: 0.000000  loss: 3.5827 (3.5216)  loss_scale: 32768.0000 (35278.8361)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0008  max mem: 15572
Epoch: [38]  [1040/1404]  eta: 0:03:37  lr: 0.000000  min_lr: 0.000000  loss: 3.6607 (3.5228)  loss_scale: 32768.0000 (35254.7166)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0009  max mem: 15572
Epoch: [38]  [1050/1404]  eta: 0:03:31  lr: 0.000000  min_lr: 0.000000  loss: 3.2073 (3.5185)  loss_scale: 32768.0000 (35231.0561)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0007  max mem: 15572
Epoch: [38]  [1060/1404]  eta: 0:03:25  lr: 0.000000  min_lr: 0.000000  loss: 3.0396 (3.5157)  loss_scale: 32768.0000 (35207.8417)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0006  max mem: 15572
[2025-01-11 01:18:23,795] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:18:23,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:18:23,797] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:18:23,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1070/1404]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 3.2936 (3.5153)  loss_scale: 32768.0000 (35307.4435)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0006  max mem: 15572
[2025-01-11 01:18:26,838] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54425
[2025-01-11 01:18:26,839] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:18:26,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54425
[2025-01-11 01:18:26,888] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:18:26,888] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1080/1404]  eta: 0:03:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5457 (3.5165)  loss_scale: 32768.0000 (35344.5772)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0006  max mem: 15572
Epoch: [38]  [1090/1404]  eta: 0:03:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6575 (3.5175)  loss_scale: 32768.0000 (35320.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0007  max mem: 15572
Epoch: [38]  [1100/1404]  eta: 0:03:01  lr: 0.000000  min_lr: 0.000000  loss: 3.2771 (3.5149)  loss_scale: 32768.0000 (35297.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0008  max mem: 15572
Epoch: [38]  [1110/1404]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 3.4512 (3.5161)  loss_scale: 32768.0000 (35275.0027)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0007  max mem: 15572
Epoch: [38]  [1120/1404]  eta: 0:02:49  lr: 0.000000  min_lr: 0.000000  loss: 3.7018 (3.5158)  loss_scale: 32768.0000 (35252.6387)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0008  max mem: 15572
Epoch: [38]  [1130/1404]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 3.7018 (3.5185)  loss_scale: 32768.0000 (35230.6702)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0008  max mem: 15572
Epoch: [38]  [1140/1404]  eta: 0:02:37  lr: 0.000000  min_lr: 0.000000  loss: 3.3399 (3.5161)  loss_scale: 32768.0000 (35209.0868)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0006  max mem: 15572
Epoch: [38]  [1150/1404]  eta: 0:02:31  lr: 0.000000  min_lr: 0.000000  loss: 3.4139 (3.5181)  loss_scale: 32768.0000 (35187.8784)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.0007  max mem: 15572
Epoch: [38]  [1160/1404]  eta: 0:02:25  lr: 0.000000  min_lr: 0.000000  loss: 3.6643 (3.5176)  loss_scale: 32768.0000 (35167.0353)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0006  max mem: 15572
Epoch: [38]  [1170/1404]  eta: 0:02:19  lr: 0.000000  min_lr: 0.000000  loss: 3.5498 (3.5172)  loss_scale: 32768.0000 (35146.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0007  max mem: 15572
Epoch: [38]  [1180/1404]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5095 (3.5159)  loss_scale: 32768.0000 (35126.4081)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0009  max mem: 15572
Epoch: [38]  [1190/1404]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 3.4982 (3.5162)  loss_scale: 32768.0000 (35106.6062)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0006  max mem: 15572
Epoch: [38]  [1200/1404]  eta: 0:02:01  lr: 0.000000  min_lr: 0.000000  loss: 3.6117 (3.5173)  loss_scale: 32768.0000 (35087.1341)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.0009  max mem: 15572
[2025-01-11 01:19:41,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:19:41,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:19:41,399] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:19:41,400] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1210/1404]  eta: 0:01:55  lr: 0.000000  min_lr: 0.000000  loss: 3.6117 (3.5172)  loss_scale: 32768.0000 (35311.5111)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0010  max mem: 15572
[2025-01-11 01:19:49,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54568
[2025-01-11 01:19:49,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:19:49,385] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:19:49,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54568
[2025-01-11 01:19:49,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [38]  [1220/1404]  eta: 0:01:49  lr: 0.000000  min_lr: 0.000000  loss: 3.4947 (3.5165)  loss_scale: 65536.0000 (35424.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0009  max mem: 15572
Epoch: [38]  [1230/1404]  eta: 0:01:43  lr: 0.000000  min_lr: 0.000000  loss: 3.5216 (3.5171)  loss_scale: 32768.0000 (35403.2819)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0008  max mem: 15572
Epoch: [38]  [1240/1404]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5607 (3.5160)  loss_scale: 32768.0000 (35382.0467)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0007  max mem: 15572
Epoch: [38]  [1250/1404]  eta: 0:01:31  lr: 0.000000  min_lr: 0.000000  loss: 3.5808 (3.5170)  loss_scale: 32768.0000 (35361.1511)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0011  max mem: 15572
Epoch: [38]  [1260/1404]  eta: 0:01:25  lr: 0.000000  min_lr: 0.000000  loss: 3.4940 (3.5148)  loss_scale: 32768.0000 (35340.5868)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0063  max mem: 15572
Epoch: [38]  [1270/1404]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 3.2999 (3.5138)  loss_scale: 32768.0000 (35320.3462)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.0059  max mem: 15572
Epoch: [38]  [1280/1404]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 3.4408 (3.5131)  loss_scale: 32768.0000 (35300.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0011  max mem: 15572
Epoch: [38]  [1290/1404]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 3.5128 (3.5137)  loss_scale: 32768.0000 (35280.8056)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0009  max mem: 15572
Epoch: [38]  [1300/1404]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 3.4135 (3.5129)  loss_scale: 32768.0000 (35261.4912)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0008  max mem: 15572
Epoch: [38]  [1310/1404]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 3.2924 (3.5108)  loss_scale: 32768.0000 (35242.4714)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0256  max mem: 15572
Epoch: [38]  [1320/1404]  eta: 0:00:49  lr: 0.000000  min_lr: 0.000000  loss: 3.2816 (3.5101)  loss_scale: 32768.0000 (35223.7396)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0261  max mem: 15572
Epoch: [38]  [1330/1404]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6055 (3.5110)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.0012  max mem: 15572
Epoch: [38]  [1340/1404]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 3.6551 (3.5111)  loss_scale: 32768.0000 (35187.1141)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0007  max mem: 15572
[2025-01-11 01:21:08,166] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:21:08,166] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:21:08,166] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:21:08,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:21:09,137] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54699
[2025-01-11 01:21:09,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:21:09,166] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54699
[2025-01-11 01:21:09,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:21:09,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1350/1404]  eta: 0:00:32  lr: 0.000000  min_lr: 0.000000  loss: 3.2818 (3.5085)  loss_scale: 32768.0000 (35217.7172)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0036  max mem: 15572
Epoch: [38]  [1360/1404]  eta: 0:00:26  lr: 0.000000  min_lr: 0.000000  loss: 3.2818 (3.5062)  loss_scale: 32768.0000 (35199.7179)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0036  max mem: 15572
Epoch: [38]  [1370/1404]  eta: 0:00:20  lr: 0.000000  min_lr: 0.000000  loss: 3.5561 (3.5072)  loss_scale: 32768.0000 (35181.9810)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0006  max mem: 15572
Epoch: [38]  [1380/1404]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 3.5879 (3.5068)  loss_scale: 32768.0000 (35164.5011)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0004  max mem: 15572
Epoch: [38]  [1390/1404]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 3.3722 (3.5052)  loss_scale: 32768.0000 (35147.2725)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.0004  max mem: 15572
Epoch: [38]  [1400/1404]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 3.5199 (3.5063)  loss_scale: 32768.0000 (35130.2898)  weight_decay: 0.0500 (0.0500)  time: 0.5144  data: 0.0003  max mem: 15572
Epoch: [38]  [1403/1404]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5199 (3.5062)  loss_scale: 32768.0000 (35125.2422)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0003  max mem: 15572
Epoch: [38] Total time: 0:13:51 (0.5922 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.5199 (3.5151)  loss_scale: 32768.0000 (35125.2422)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:24  loss: 1.4249 (1.4249)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.1492  data: 3.9805  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 1.9259 (1.9531)  acc1: 55.5556 (52.0202)  acc5: 83.3333 (81.8182)  time: 0.8154  data: 0.6039  max mem: 15572
Val:  [ 20/136]  eta: 0:01:04  loss: 2.2911 (2.1457)  acc1: 44.4444 (47.3545)  acc5: 83.3333 (79.1005)  time: 0.3778  data: 0.1662  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.1631 (2.0363)  acc1: 44.4444 (50.8961)  acc5: 83.3333 (80.1075)  time: 0.3348  data: 0.1267  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.7984 (2.0040)  acc1: 61.1111 (52.5745)  acc5: 83.3333 (80.8943)  time: 0.3465  data: 0.1371  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 1.7984 (1.9944)  acc1: 61.1111 (53.2680)  acc5: 83.3333 (81.8083)  time: 0.3531  data: 0.1303  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.0452 (2.0870)  acc1: 44.4444 (49.8179)  acc5: 83.3333 (80.6011)  time: 0.3920  data: 0.1749  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 1.9352 (2.0535)  acc1: 44.4444 (50.4695)  acc5: 83.3333 (80.8294)  time: 0.3618  data: 0.1569  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 1.8101 (2.0449)  acc1: 50.0000 (50.4801)  acc5: 88.8889 (81.3443)  time: 0.3330  data: 0.1122  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9841 (2.0539)  acc1: 50.0000 (50.1221)  acc5: 83.3333 (81.3187)  time: 0.3374  data: 0.1276  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.2415 (2.1325)  acc1: 38.8889 (47.9098)  acc5: 77.7778 (79.3179)  time: 0.3423  data: 0.1479  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2050 (2.1223)  acc1: 38.8889 (48.2482)  acc5: 72.2222 (79.3293)  time: 0.3521  data: 0.1541  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8127 (2.0740)  acc1: 55.5556 (49.4949)  acc5: 83.3333 (80.0735)  time: 0.3675  data: 0.1750  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.6367 (2.0355)  acc1: 55.5556 (50.3393)  acc5: 88.8889 (80.6616)  time: 0.2697  data: 0.0971  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.6665 (2.0312)  acc1: 55.5556 (50.5733)  acc5: 83.3333 (80.7944)  time: 0.2126  data: 0.0595  max mem: 15572
Val: Total time: 0:00:49 (0.3676 s / it)
* Acc@1 50.123 Acc@5 79.259 loss 2.080
Accuracy of the network on the 4883 val videos: 50.1%
Max accuracy: 50.66%
Epoch: [39]  [   0/1404]  eta: 3:17:55  lr: 0.000000  min_lr: 0.000000  loss: 3.5094 (3.5094)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4585  data: 7.9900  max mem: 15572
Epoch: [39]  [  10/1404]  eta: 0:29:16  lr: 0.000000  min_lr: 0.000000  loss: 3.5134 (3.4231)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2599  data: 0.7598  max mem: 15572
Epoch: [39]  [  20/1404]  eta: 0:21:47  lr: 0.000000  min_lr: 0.000000  loss: 3.5134 (3.4332)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0722  max mem: 15572
Epoch: [39]  [  30/1404]  eta: 0:19:14  lr: 0.000000  min_lr: 0.000000  loss: 3.6462 (3.4579)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1037  max mem: 15572
Epoch: [39]  [  40/1404]  eta: 0:17:39  lr: 0.000000  min_lr: 0.000000  loss: 3.6462 (3.4750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0884  max mem: 15572
Epoch: [39]  [  50/1404]  eta: 0:17:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5099 (3.4607)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1311  max mem: 15572
Epoch: [39]  [  60/1404]  eta: 0:16:24  lr: 0.000000  min_lr: 0.000000  loss: 3.4552 (3.4704)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1570  max mem: 15572
Epoch: [39]  [  70/1404]  eta: 0:15:39  lr: 0.000000  min_lr: 0.000000  loss: 3.4400 (3.4447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0649  max mem: 15572
[2025-01-11 01:23:18,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:23:18,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:23:18,854] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:23:18,855] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [  80/1404]  eta: 0:14:56  lr: 0.000000  min_lr: 0.000000  loss: 3.4849 (3.4672)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0009  max mem: 15572
Epoch: [39]  [  90/1404]  eta: 0:14:46  lr: 0.000000  min_lr: 0.000000  loss: 3.7525 (3.4776)  loss_scale: 65536.0000 (39609.6703)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0618  max mem: 15572
[2025-01-11 01:23:34,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54855
[2025-01-11 01:23:34,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:23:34,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54855
[2025-01-11 01:23:34,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:23:34,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 100/1404]  eta: 0:14:22  lr: 0.000000  min_lr: 0.000000  loss: 3.6072 (3.4859)  loss_scale: 65536.0000 (41527.7624)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0759  max mem: 15572
Epoch: [39]  [ 110/1404]  eta: 0:14:11  lr: 0.000000  min_lr: 0.000000  loss: 3.4848 (3.4904)  loss_scale: 32768.0000 (40738.5946)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0638  max mem: 15572
Epoch: [39]  [ 120/1404]  eta: 0:14:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7158 (3.5120)  loss_scale: 32768.0000 (40079.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1021  max mem: 15572
Epoch: [39]  [ 130/1404]  eta: 0:13:39  lr: 0.000000  min_lr: 0.000000  loss: 3.8129 (3.5266)  loss_scale: 32768.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0528  max mem: 15572
Epoch: [39]  [ 140/1404]  eta: 0:13:26  lr: 0.000000  min_lr: 0.000000  loss: 3.8423 (3.5502)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0445  max mem: 15572
Epoch: [39]  [ 150/1404]  eta: 0:13:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7791 (3.5540)  loss_scale: 32768.0000 (38627.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1024  max mem: 15572
Epoch: [39]  [ 160/1404]  eta: 0:13:08  lr: 0.000000  min_lr: 0.000000  loss: 3.5092 (3.5539)  loss_scale: 32768.0000 (38263.2547)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1277  max mem: 15572
Epoch: [39]  [ 170/1404]  eta: 0:13:03  lr: 0.000000  min_lr: 0.000000  loss: 3.3531 (3.5426)  loss_scale: 32768.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1536  max mem: 15572
Epoch: [39]  [ 180/1404]  eta: 0:12:54  lr: 0.000000  min_lr: 0.000000  loss: 3.4578 (3.5537)  loss_scale: 32768.0000 (37656.0442)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1132  max mem: 15572
Epoch: [39]  [ 190/1404]  eta: 0:12:48  lr: 0.000000  min_lr: 0.000000  loss: 3.4423 (3.5421)  loss_scale: 32768.0000 (37400.1257)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0781  max mem: 15572
Epoch: [39]  [ 200/1404]  eta: 0:12:40  lr: 0.000000  min_lr: 0.000000  loss: 3.3078 (3.5367)  loss_scale: 32768.0000 (37169.6716)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1032  max mem: 15572
Epoch: [39]  [ 210/1404]  eta: 0:12:27  lr: 0.000000  min_lr: 0.000000  loss: 3.4653 (3.5317)  loss_scale: 32768.0000 (36961.0616)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0691  max mem: 15572
Epoch: [39]  [ 220/1404]  eta: 0:12:18  lr: 0.000000  min_lr: 0.000000  loss: 3.3578 (3.5274)  loss_scale: 32768.0000 (36771.3303)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0404  max mem: 15572
[2025-01-11 01:24:52,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:24:52,238] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:24:52,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:24:52,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 230/1404]  eta: 0:12:18  lr: 0.000000  min_lr: 0.000000  loss: 3.4968 (3.5352)  loss_scale: 32768.0000 (37023.5844)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1443  max mem: 15572
[2025-01-11 01:24:53,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54987
[2025-01-11 01:24:53,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54987
[2025-01-11 01:24:53,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:24:53,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:24:53,847] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 240/1404]  eta: 0:12:06  lr: 0.000000  min_lr: 0.000000  loss: 3.4381 (3.5236)  loss_scale: 32768.0000 (36847.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1191  max mem: 15572
[2025-01-11 01:25:00,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=55000, skipped=378, lr=[2.157574173979932e-09, 2.157574173979932e-09, 3.082248819971332e-09, 3.082248819971332e-09, 4.403212599959046e-09, 4.403212599959046e-09, 6.290303714227209e-09, 6.290303714227209e-09, 8.986148163181727e-09, 8.986148163181727e-09, 1.283735451883104e-08, 1.283735451883104e-08, 1.8339077884044344e-08, 1.8339077884044344e-08, 2.6198682691491923e-08, 2.6198682691491923e-08, 3.7426689559274175e-08, 3.7426689559274175e-08, 5.3466699370391684e-08, 5.3466699370391684e-08, 7.638099910055955e-08, 7.638099910055955e-08, 1.0911571300079936e-07, 1.0911571300079936e-07, 1.5587959000114197e-07, 1.5587959000114197e-07, 2.2268512857305995e-07, 2.2268512857305995e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 01:25:00,028] [INFO] [timer.py:260:stop] epoch=0/micro_step=55000/global_step=55000, RunningAvgSamplesPerSec=45.75542950016057, CurrSamplesPerSec=41.82326861450521, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [ 250/1404]  eta: 0:11:54  lr: 0.000000  min_lr: 0.000000  loss: 3.5398 (3.5368)  loss_scale: 32768.0000 (36684.4940)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0006  max mem: 15572
Epoch: [39]  [ 260/1404]  eta: 0:11:48  lr: 0.000000  min_lr: 0.000000  loss: 3.6959 (3.5404)  loss_scale: 32768.0000 (36534.4368)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0430  max mem: 15572
Epoch: [39]  [ 270/1404]  eta: 0:11:38  lr: 0.000000  min_lr: 0.000000  loss: 3.5892 (3.5420)  loss_scale: 32768.0000 (36395.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0519  max mem: 15572
Epoch: [39]  [ 280/1404]  eta: 0:11:33  lr: 0.000000  min_lr: 0.000000  loss: 3.4067 (3.5426)  loss_scale: 32768.0000 (36266.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0830  max mem: 15572
Epoch: [39]  [ 290/1404]  eta: 0:11:23  lr: 0.000000  min_lr: 0.000000  loss: 3.4067 (3.5398)  loss_scale: 32768.0000 (36146.1443)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0742  max mem: 15572
Epoch: [39]  [ 300/1404]  eta: 0:11:13  lr: 0.000000  min_lr: 0.000000  loss: 3.6833 (3.5465)  loss_scale: 32768.0000 (36033.9136)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0010  max mem: 15572
Epoch: [39]  [ 310/1404]  eta: 0:11:05  lr: 0.000000  min_lr: 0.000000  loss: 3.4429 (3.5376)  loss_scale: 32768.0000 (35928.9003)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0301  max mem: 15572
Epoch: [39]  [ 320/1404]  eta: 0:10:57  lr: 0.000000  min_lr: 0.000000  loss: 3.4429 (3.5425)  loss_scale: 32768.0000 (35830.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0534  max mem: 15572
Epoch: [39]  [ 330/1404]  eta: 0:10:51  lr: 0.000000  min_lr: 0.000000  loss: 3.5666 (3.5359)  loss_scale: 32768.0000 (35737.9094)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0244  max mem: 15572
Epoch: [39]  [ 340/1404]  eta: 0:10:46  lr: 0.000000  min_lr: 0.000000  loss: 3.4536 (3.5351)  loss_scale: 32768.0000 (35650.8152)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0009  max mem: 15572
Epoch: [39]  [ 350/1404]  eta: 0:10:38  lr: 0.000000  min_lr: 0.000000  loss: 3.5484 (3.5383)  loss_scale: 32768.0000 (35568.6838)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.0008  max mem: 15572
[2025-01-11 01:26:06,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:26:06,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:26:06,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:26:06,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 360/1404]  eta: 0:10:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7077 (3.5316)  loss_scale: 32768.0000 (35581.8726)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0008  max mem: 15572
[2025-01-11 01:26:07,189] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55117
[2025-01-11 01:26:07,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:26:07,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:26:07,306] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55117
[2025-01-11 01:26:07,307] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [39]  [ 370/1404]  eta: 0:10:27  lr: 0.000000  min_lr: 0.000000  loss: 3.4511 (3.5324)  loss_scale: 32768.0000 (35506.0270)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.0319  max mem: 15572
Epoch: [39]  [ 380/1404]  eta: 0:10:20  lr: 0.000000  min_lr: 0.000000  loss: 3.4511 (3.5302)  loss_scale: 32768.0000 (35434.1627)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0497  max mem: 15572
Epoch: [39]  [ 390/1404]  eta: 0:10:15  lr: 0.000000  min_lr: 0.000000  loss: 3.6345 (3.5315)  loss_scale: 32768.0000 (35365.9744)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0185  max mem: 15572
Epoch: [39]  [ 400/1404]  eta: 0:10:08  lr: 0.000000  min_lr: 0.000000  loss: 3.5866 (3.5303)  loss_scale: 32768.0000 (35301.1870)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0008  max mem: 15572
Epoch: [39]  [ 410/1404]  eta: 0:10:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5650 (3.5283)  loss_scale: 32768.0000 (35239.5523)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0007  max mem: 15572
Epoch: [39]  [ 420/1404]  eta: 0:09:55  lr: 0.000000  min_lr: 0.000000  loss: 3.5449 (3.5266)  loss_scale: 32768.0000 (35180.8456)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0007  max mem: 15572
Epoch: [39]  [ 430/1404]  eta: 0:09:47  lr: 0.000000  min_lr: 0.000000  loss: 3.4453 (3.5229)  loss_scale: 32768.0000 (35124.8631)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0006  max mem: 15572
Epoch: [39]  [ 440/1404]  eta: 0:09:40  lr: 0.000000  min_lr: 0.000000  loss: 3.4683 (3.5256)  loss_scale: 32768.0000 (35071.4195)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0376  max mem: 15572
Epoch: [39]  [ 450/1404]  eta: 0:09:33  lr: 0.000000  min_lr: 0.000000  loss: 3.9349 (3.5305)  loss_scale: 32768.0000 (35020.3459)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0727  max mem: 15572
Epoch: [39]  [ 460/1404]  eta: 0:09:28  lr: 0.000000  min_lr: 0.000000  loss: 3.7527 (3.5292)  loss_scale: 32768.0000 (34971.4881)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0356  max mem: 15572
Epoch: [39]  [ 470/1404]  eta: 0:09:22  lr: 0.000000  min_lr: 0.000000  loss: 3.4325 (3.5314)  loss_scale: 32768.0000 (34924.7049)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0458  max mem: 15572
Epoch: [39]  [ 480/1404]  eta: 0:09:16  lr: 0.000000  min_lr: 0.000000  loss: 3.5281 (3.5350)  loss_scale: 32768.0000 (34879.8669)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0932  max mem: 15572
[2025-01-11 01:27:24,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:27:24,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:27:24,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:27:24,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 490/1404]  eta: 0:09:11  lr: 0.000000  min_lr: 0.000000  loss: 3.4337 (3.5298)  loss_scale: 32768.0000 (34903.5927)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.1335  max mem: 15572
[2025-01-11 01:27:25,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55248
[2025-01-11 01:27:25,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55248
[2025-01-11 01:27:25,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:27:25,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:27:25,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 500/1404]  eta: 0:09:04  lr: 0.000000  min_lr: 0.000000  loss: 3.5459 (3.5321)  loss_scale: 32768.0000 (34926.3713)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1229  max mem: 15572
Epoch: [39]  [ 510/1404]  eta: 0:08:58  lr: 0.000000  min_lr: 0.000000  loss: 3.5459 (3.5281)  loss_scale: 32768.0000 (34884.1331)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0780  max mem: 15572
Epoch: [39]  [ 520/1404]  eta: 0:08:54  lr: 0.000000  min_lr: 0.000000  loss: 3.3492 (3.5272)  loss_scale: 32768.0000 (34843.5163)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1282  max mem: 15572
Epoch: [39]  [ 530/1404]  eta: 0:08:48  lr: 0.000000  min_lr: 0.000000  loss: 3.4024 (3.5266)  loss_scale: 32768.0000 (34804.4294)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.1416  max mem: 15572
Epoch: [39]  [ 540/1404]  eta: 0:08:42  lr: 0.000000  min_lr: 0.000000  loss: 3.4669 (3.5215)  loss_scale: 32768.0000 (34766.7874)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1266  max mem: 15572
Epoch: [39]  [ 550/1404]  eta: 0:08:35  lr: 0.000000  min_lr: 0.000000  loss: 3.4669 (3.5196)  loss_scale: 32768.0000 (34730.5118)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0872  max mem: 15572
Epoch: [39]  [ 560/1404]  eta: 0:08:29  lr: 0.000000  min_lr: 0.000000  loss: 3.5209 (3.5177)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0796  max mem: 15572
Epoch: [39]  [ 570/1404]  eta: 0:08:22  lr: 0.000000  min_lr: 0.000000  loss: 3.5944 (3.5196)  loss_scale: 32768.0000 (34661.7723)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.0737  max mem: 15572
Epoch: [39]  [ 580/1404]  eta: 0:08:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7271 (3.5219)  loss_scale: 32768.0000 (34629.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0292  max mem: 15572
Epoch: [39]  [ 590/1404]  eta: 0:08:11  lr: 0.000000  min_lr: 0.000000  loss: 3.8009 (3.5278)  loss_scale: 32768.0000 (34597.6853)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0871  max mem: 15572
Epoch: [39]  [ 600/1404]  eta: 0:08:03  lr: 0.000000  min_lr: 0.000000  loss: 3.5958 (3.5288)  loss_scale: 32768.0000 (34567.2413)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0739  max mem: 15572
Epoch: [39]  [ 610/1404]  eta: 0:07:56  lr: 0.000000  min_lr: 0.000000  loss: 3.6001 (3.5307)  loss_scale: 32768.0000 (34537.7938)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0075  max mem: 15572
Epoch: [39]  [ 620/1404]  eta: 0:07:51  lr: 0.000000  min_lr: 0.000000  loss: 3.7756 (3.5351)  loss_scale: 32768.0000 (34509.2947)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.0611  max mem: 15572
[2025-01-11 01:28:42,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:28:42,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:28:42,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:28:42,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 630/1404]  eta: 0:07:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6139 (3.5323)  loss_scale: 32768.0000 (35001.0016)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.0611  max mem: 15572
Epoch: [39]  [ 640/1404]  eta: 0:07:37  lr: 0.000000  min_lr: 0.000000  loss: 3.4930 (3.5322)  loss_scale: 65536.0000 (35477.3666)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0009  max mem: 15572
[2025-01-11 01:28:52,919] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55398
[2025-01-11 01:28:52,919] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:28:52,919] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:28:52,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55398
[2025-01-11 01:28:52,925] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [39]  [ 650/1404]  eta: 0:07:30  lr: 0.000000  min_lr: 0.000000  loss: 3.7253 (3.5329)  loss_scale: 65536.0000 (35486.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0057  max mem: 15572
Epoch: [39]  [ 660/1404]  eta: 0:07:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7253 (3.5341)  loss_scale: 32768.0000 (35444.9622)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0056  max mem: 15572
Epoch: [39]  [ 670/1404]  eta: 0:07:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6630 (3.5348)  loss_scale: 32768.0000 (35405.0671)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0350  max mem: 15572
Epoch: [39]  [ 680/1404]  eta: 0:07:12  lr: 0.000000  min_lr: 0.000000  loss: 3.5553 (3.5330)  loss_scale: 32768.0000 (35366.3436)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0780  max mem: 15572
Epoch: [39]  [ 690/1404]  eta: 0:07:06  lr: 0.000000  min_lr: 0.000000  loss: 3.4217 (3.5326)  loss_scale: 32768.0000 (35328.7410)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0889  max mem: 15572
Epoch: [39]  [ 700/1404]  eta: 0:07:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5717 (3.5340)  loss_scale: 32768.0000 (35292.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1117  max mem: 15572
Epoch: [39]  [ 710/1404]  eta: 0:06:54  lr: 0.000000  min_lr: 0.000000  loss: 3.5955 (3.5333)  loss_scale: 32768.0000 (35256.7089)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0664  max mem: 15572
Epoch: [39]  [ 720/1404]  eta: 0:06:48  lr: 0.000000  min_lr: 0.000000  loss: 3.4906 (3.5335)  loss_scale: 32768.0000 (35222.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0005  max mem: 15572
Epoch: [39]  [ 730/1404]  eta: 0:06:42  lr: 0.000000  min_lr: 0.000000  loss: 3.5237 (3.5331)  loss_scale: 32768.0000 (35188.6183)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0203  max mem: 15572
Epoch: [39]  [ 740/1404]  eta: 0:06:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5237 (3.5324)  loss_scale: 32768.0000 (35155.9514)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.0713  max mem: 15572
Epoch: [39]  [ 750/1404]  eta: 0:06:30  lr: 0.000000  min_lr: 0.000000  loss: 3.5719 (3.5333)  loss_scale: 32768.0000 (35124.1545)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0516  max mem: 15572
Epoch: [39]  [ 760/1404]  eta: 0:06:23  lr: 0.000000  min_lr: 0.000000  loss: 3.4748 (3.5314)  loss_scale: 32768.0000 (35093.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0007  max mem: 15572
Epoch: [39]  [ 770/1404]  eta: 0:06:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6076 (3.5319)  loss_scale: 32768.0000 (35063.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0716  max mem: 15572
[2025-01-11 01:30:09,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:30:09,251] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:30:09,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:30:09,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 780/1404]  eta: 0:06:12  lr: 0.000000  min_lr: 0.000000  loss: 3.8196 (3.5352)  loss_scale: 32768.0000 (35453.2138)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1486  max mem: 15572
Epoch: [39]  [ 790/1404]  eta: 0:06:06  lr: 0.000000  min_lr: 0.000000  loss: 3.5983 (3.5322)  loss_scale: 65536.0000 (35833.5272)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0777  max mem: 15572
[2025-01-11 01:30:24,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55554
[2025-01-11 01:30:24,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:30:24,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:30:24,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55554
[2025-01-11 01:30:24,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [39]  [ 800/1404]  eta: 0:06:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5333 (3.5336)  loss_scale: 65536.0000 (36081.6180)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0008  max mem: 15572
Epoch: [39]  [ 810/1404]  eta: 0:05:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7513 (3.5351)  loss_scale: 32768.0000 (36040.7596)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1374  max mem: 15572
Epoch: [39]  [ 820/1404]  eta: 0:05:48  lr: 0.000000  min_lr: 0.000000  loss: 3.4514 (3.5318)  loss_scale: 32768.0000 (36000.8965)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.1374  max mem: 15572
Epoch: [39]  [ 830/1404]  eta: 0:05:42  lr: 0.000000  min_lr: 0.000000  loss: 3.5283 (3.5347)  loss_scale: 32768.0000 (35961.9928)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.0543  max mem: 15572
Epoch: [39]  [ 840/1404]  eta: 0:05:36  lr: 0.000000  min_lr: 0.000000  loss: 3.6398 (3.5352)  loss_scale: 32768.0000 (35924.0143)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0624  max mem: 15572
Epoch: [39]  [ 850/1404]  eta: 0:05:30  lr: 0.000000  min_lr: 0.000000  loss: 3.6832 (3.5378)  loss_scale: 32768.0000 (35886.9283)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0786  max mem: 15572
Epoch: [39]  [ 860/1404]  eta: 0:05:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7203 (3.5372)  loss_scale: 32768.0000 (35850.7038)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0704  max mem: 15572
Epoch: [39]  [ 870/1404]  eta: 0:05:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6441 (3.5389)  loss_scale: 32768.0000 (35815.3111)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0318  max mem: 15572
Epoch: [39]  [ 880/1404]  eta: 0:05:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7906 (3.5419)  loss_scale: 32768.0000 (35780.7219)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0519  max mem: 15572
Epoch: [39]  [ 890/1404]  eta: 0:05:06  lr: 0.000000  min_lr: 0.000000  loss: 3.5866 (3.5380)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0301  max mem: 15572
Epoch: [39]  [ 900/1404]  eta: 0:05:00  lr: 0.000000  min_lr: 0.000000  loss: 3.4463 (3.5381)  loss_scale: 32768.0000 (35713.8468)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0808  max mem: 15572
Epoch: [39]  [ 910/1404]  eta: 0:04:55  lr: 0.000000  min_lr: 0.000000  loss: 3.4630 (3.5383)  loss_scale: 32768.0000 (35681.5104)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.1148  max mem: 15572
Epoch: [39]  [ 920/1404]  eta: 0:04:48  lr: 0.000000  min_lr: 0.000000  loss: 3.4583 (3.5365)  loss_scale: 32768.0000 (35649.8762)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.0440  max mem: 15572
[2025-01-11 01:31:41,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:31:41,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:31:41,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:31:41,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 930/1404]  eta: 0:04:42  lr: 0.000000  min_lr: 0.000000  loss: 3.6532 (3.5366)  loss_scale: 32768.0000 (35759.7078)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0010  max mem: 15572
Epoch: [39]  [ 940/1404]  eta: 0:04:36  lr: 0.000000  min_lr: 0.000000  loss: 3.7706 (3.5386)  loss_scale: 65536.0000 (36076.1403)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0400  max mem: 15572
Epoch: [39]  [ 950/1404]  eta: 0:04:30  lr: 0.000000  min_lr: 0.000000  loss: 3.7053 (3.5372)  loss_scale: 65536.0000 (36385.9180)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0920  max mem: 15572
Epoch: [39]  [ 960/1404]  eta: 0:04:24  lr: 0.000000  min_lr: 0.000000  loss: 3.3807 (3.5351)  loss_scale: 65536.0000 (36689.2487)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0531  max mem: 15572
[2025-01-11 01:32:02,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55718
[2025-01-11 01:32:02,149] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:32:02,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-11 01:32:02,175] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55718
[2025-01-11 01:32:02,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [39]  [ 970/1404]  eta: 0:04:18  lr: 0.000000  min_lr: 0.000000  loss: 3.3807 (3.5342)  loss_scale: 65536.0000 (36682.6117)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.0384  max mem: 15572
Epoch: [39]  [ 980/1404]  eta: 0:04:12  lr: 0.000000  min_lr: 0.000000  loss: 3.4566 (3.5316)  loss_scale: 32768.0000 (36642.7074)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1141  max mem: 15572
Epoch: [39]  [ 990/1404]  eta: 0:04:06  lr: 0.000000  min_lr: 0.000000  loss: 3.4174 (3.5305)  loss_scale: 32768.0000 (36603.6085)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.0965  max mem: 15572
Epoch: [39]  [1000/1404]  eta: 0:04:01  lr: 0.000000  min_lr: 0.000000  loss: 3.6896 (3.5315)  loss_scale: 32768.0000 (36565.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0206  max mem: 15572
Epoch: [39]  [1010/1404]  eta: 0:03:54  lr: 0.000000  min_lr: 0.000000  loss: 3.6673 (3.5307)  loss_scale: 32768.0000 (36527.7310)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0004  max mem: 15572
Epoch: [39]  [1020/1404]  eta: 0:03:49  lr: 0.000000  min_lr: 0.000000  loss: 3.6225 (3.5290)  loss_scale: 32768.0000 (36490.9070)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0290  max mem: 15572
[2025-01-11 01:32:38,633] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55777
[2025-01-11 01:32:38,634] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-11 01:32:38,634] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-11 01:32:38,634] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55777
[2025-01-11 01:32:38,634] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [39]  [1030/1404]  eta: 0:03:43  lr: 0.000000  min_lr: 0.000000  loss: 3.4654 (3.5278)  loss_scale: 16384.0000 (36295.8836)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.0292  max mem: 15572
Epoch: [39]  [1040/1404]  eta: 0:03:37  lr: 0.000000  min_lr: 0.000000  loss: 3.3627 (3.5269)  loss_scale: 16384.0000 (36104.6071)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0007  max mem: 15572
Epoch: [39]  [1050/1404]  eta: 0:03:31  lr: 0.000000  min_lr: 0.000000  loss: 3.3905 (3.5284)  loss_scale: 16384.0000 (35916.9705)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.0008  max mem: 15572
Epoch: [39]  [1060/1404]  eta: 0:03:25  lr: 0.000000  min_lr: 0.000000  loss: 3.3932 (3.5276)  loss_scale: 16384.0000 (35732.8709)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0010  max mem: 15572
Epoch: [39]  [1070/1404]  eta: 0:03:19  lr: 0.000000  min_lr: 0.000000  loss: 3.1587 (3.5242)  loss_scale: 16384.0000 (35552.2092)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0009  max mem: 15572
Epoch: [39]  [1080/1404]  eta: 0:03:13  lr: 0.000000  min_lr: 0.000000  loss: 3.4948 (3.5241)  loss_scale: 16384.0000 (35374.8899)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0007  max mem: 15572
Epoch: [39]  [1090/1404]  eta: 0:03:07  lr: 0.000000  min_lr: 0.000000  loss: 3.4948 (3.5233)  loss_scale: 16384.0000 (35200.8213)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0007  max mem: 15572
Epoch: [39]  [1100/1404]  eta: 0:03:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7193 (3.5247)  loss_scale: 16384.0000 (35029.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0007  max mem: 15572
Epoch: [39]  [1110/1404]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7968 (3.5274)  loss_scale: 16384.0000 (34862.0846)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0008  max mem: 15572
Epoch: [39]  [1120/1404]  eta: 0:02:49  lr: 0.000000  min_lr: 0.000000  loss: 3.6184 (3.5271)  loss_scale: 16384.0000 (34697.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0008  max mem: 15572
Epoch: [39]  [1130/1404]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 3.6053 (3.5265)  loss_scale: 16384.0000 (34535.3280)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.0005  max mem: 15572
Epoch: [39]  [1140/1404]  eta: 0:02:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5755 (3.5274)  loss_scale: 16384.0000 (34376.2454)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0006  max mem: 15572
[2025-01-11 01:33:53,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:33:53,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-11 01:33:53,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:33:53,535] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [1150/1404]  eta: 0:02:31  lr: 0.000000  min_lr: 0.000000  loss: 3.4914 (3.5262)  loss_scale: 16384.0000 (34234.1616)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0008  max mem: 15572
Epoch: [39]  [1160/1404]  eta: 0:02:25  lr: 0.000000  min_lr: 0.000000  loss: 3.4419 (3.5271)  loss_scale: 32768.0000 (34221.5332)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0008  max mem: 15572
Epoch: [39]  [1170/1404]  eta: 0:02:19  lr: 0.000000  min_lr: 0.000000  loss: 3.4419 (3.5265)  loss_scale: 32768.0000 (34209.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0008  max mem: 15572
Epoch: [39]  [1180/1404]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5073 (3.5263)  loss_scale: 32768.0000 (34196.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0122  max mem: 15572
Epoch: [39]  [1190/1404]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6023 (3.5268)  loss_scale: 32768.0000 (34184.9202)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0585  max mem: 15572
Epoch: [39]  [1200/1404]  eta: 0:02:01  lr: 0.000000  min_lr: 0.000000  loss: 3.6023 (3.5286)  loss_scale: 32768.0000 (34173.1224)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0993  max mem: 15572
Epoch: [39]  [1210/1404]  eta: 0:01:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7673 (3.5291)  loss_scale: 32768.0000 (34161.5194)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0704  max mem: 15572
Epoch: [39]  [1220/1404]  eta: 0:01:49  lr: 0.000000  min_lr: 0.000000  loss: 3.7404 (3.5299)  loss_scale: 32768.0000 (34150.1065)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0242  max mem: 15572
Epoch: [39]  [1230/1404]  eta: 0:01:43  lr: 0.000000  min_lr: 0.000000  loss: 3.6217 (3.5299)  loss_scale: 32768.0000 (34138.8790)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0068  max mem: 15572
Epoch: [39]  [1240/1404]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5991 (3.5301)  loss_scale: 32768.0000 (34127.8324)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0813  max mem: 15572
[2025-01-11 01:34:48,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=384, lr=[9.323686162031562e-10, 9.323686162031562e-10, 1.3319551660045092e-09, 1.3319551660045092e-09, 1.9027930942921564e-09, 1.9027930942921564e-09, 2.7182758489887946e-09, 2.7182758489887946e-09, 3.883251212841135e-09, 3.883251212841135e-09, 5.547501732630193e-09, 5.547501732630193e-09, 7.925002475185992e-09, 7.925002475185992e-09, 1.132143210740856e-08, 1.132143210740856e-08, 1.6173474439155085e-08, 1.6173474439155085e-08, 2.310496348450727e-08, 2.310496348450727e-08, 3.300709069215324e-08, 3.300709069215324e-08, 4.715298670307606e-08, 4.715298670307606e-08, 6.736140957582295e-08, 6.736140957582295e-08, 9.623058510831851e-08, 9.623058510831851e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-11 01:34:48,325] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=45.77387095911452, CurrSamplesPerSec=49.73748401594551, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [1250/1404]  eta: 0:01:31  lr: 0.000000  min_lr: 0.000000  loss: 3.4698 (3.5299)  loss_scale: 32768.0000 (34116.9624)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0868  max mem: 15572
Epoch: [39]  [1260/1404]  eta: 0:01:25  lr: 0.000000  min_lr: 0.000000  loss: 3.6422 (3.5316)  loss_scale: 32768.0000 (34106.2649)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.0823  max mem: 15572
Epoch: [39]  [1270/1404]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 3.5990 (3.5315)  loss_scale: 32768.0000 (34095.7356)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1378  max mem: 15572
[2025-01-11 01:35:08,816] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:35:08,816] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-11 01:35:08,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-11 01:35:08,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1280/1404]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5242 (3.5327)  loss_scale: 32768.0000 (34162.1109)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0740  max mem: 15572
Epoch: [39]  [1290/1404]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6758 (3.5331)  loss_scale: 65536.0000 (34405.1309)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0129  max mem: 15572
[2025-01-11 01:35:18,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56051
[2025-01-11 01:35:18,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:35:18,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56051
[2025-01-11 01:35:18,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-11 01:35:18,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1300/1404]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 3.5978 (3.5332)  loss_scale: 65536.0000 (34493.2944)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.0007  max mem: 15572
Epoch: [39]  [1310/1404]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 3.5451 (3.5328)  loss_scale: 32768.0000 (34480.1342)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.0005  max mem: 15572
Epoch: [39]  [1320/1404]  eta: 0:00:49  lr: 0.000000  min_lr: 0.000000  loss: 3.4795 (3.5332)  loss_scale: 32768.0000 (34467.1734)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0034  max mem: 15572
Epoch: [39]  [1330/1404]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6326 (3.5338)  loss_scale: 32768.0000 (34454.4072)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.0035  max mem: 15572
Epoch: [39]  [1340/1404]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 3.5261 (3.5329)  loss_scale: 32768.0000 (34441.8315)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0008  max mem: 15572
Epoch: [39]  [1350/1404]  eta: 0:00:32  lr: 0.000000  min_lr: 0.000000  loss: 3.5261 (3.5337)  loss_scale: 32768.0000 (34429.4419)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0008  max mem: 15572
Epoch: [39]  [1360/1404]  eta: 0:00:26  lr: 0.000000  min_lr: 0.000000  loss: 3.6478 (3.5348)  loss_scale: 32768.0000 (34417.2344)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0009  max mem: 15572
Epoch: [39]  [1370/1404]  eta: 0:00:20  lr: 0.000000  min_lr: 0.000000  loss: 3.5476 (3.5322)  loss_scale: 32768.0000 (34405.2050)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.0008  max mem: 15572
Epoch: [39]  [1380/1404]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 3.5380 (3.5325)  loss_scale: 32768.0000 (34393.3497)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.0007  max mem: 15572
Epoch: [39]  [1390/1404]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 3.4893 (3.5322)  loss_scale: 32768.0000 (34381.6650)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0006  max mem: 15572
Epoch: [39]  [1400/1404]  eta: 0:00:02  lr: 0.000000  min_lr: 0.000000  loss: 3.4364 (3.5330)  loss_scale: 32768.0000 (34370.1470)  weight_decay: 0.0500 (0.0500)  time: 0.4608  data: 0.0004  max mem: 15572
Epoch: [39]  [1403/1404]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5471 (3.5330)  loss_scale: 32768.0000 (34366.7236)  weight_decay: 0.0500 (0.0500)  time: 0.4457  data: 0.0004  max mem: 15572
Epoch: [39] Total time: 0:13:52 (0.5926 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.5471 (3.5326)  loss_scale: 32768.0000 (34366.7236)  weight_decay: 0.0500 (0.0500)
[2025-01-11 01:36:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2025-01-11 01:36:20,037] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-39/mp_rank_00_model_states.pt
[2025-01-11 01:36:20,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-39/mp_rank_00_model_states.pt...
[2025-01-11 01:36:20,037] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2025-01-11 01:36:20,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_videomae_pretrain_small_patch16_224_frame_16x2_tube_mask_ratio_0.9_e2400_curriculum/checkpoint-39/mp_rank_00_model_states.pt.
[2025-01-11 01:36:20,351] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [  0/136]  eta: 0:11:07  loss: 1.4547 (1.4547)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 4.9077  data: 4.7190  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 1.9779 (1.9789)  acc1: 55.5556 (53.5354)  acc5: 77.7778 (79.7980)  time: 0.7908  data: 0.6069  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 2.3248 (2.1440)  acc1: 50.0000 (49.2063)  acc5: 77.7778 (78.5714)  time: 0.3837  data: 0.1833  max mem: 15572
Val:  [ 30/136]  eta: 0:00:50  loss: 2.2127 (2.0392)  acc1: 50.0000 (51.4337)  acc5: 83.3333 (79.7491)  time: 0.3104  data: 0.0860  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 1.8093 (2.0069)  acc1: 61.1111 (53.2520)  acc5: 83.3333 (80.3523)  time: 0.2979  data: 0.0738  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 1.8243 (1.9962)  acc1: 61.1111 (54.0305)  acc5: 83.3333 (81.1547)  time: 0.3552  data: 0.1525  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.0253 (2.0895)  acc1: 50.0000 (50.8197)  acc5: 77.7778 (80.0546)  time: 0.3699  data: 0.1666  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 1.9967 (2.0603)  acc1: 50.0000 (51.3302)  acc5: 77.7778 (80.3599)  time: 0.3883  data: 0.1737  max mem: 15572
Val:  [ 80/136]  eta: 0:00:22  loss: 1.7853 (2.0523)  acc1: 50.0000 (51.3032)  acc5: 88.8889 (80.7956)  time: 0.3368  data: 0.1365  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 1.9610 (2.0606)  acc1: 50.0000 (50.7937)  acc5: 77.7778 (80.5861)  time: 0.3374  data: 0.1301  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.3132 (2.1404)  acc1: 44.4444 (48.6249)  acc5: 77.7778 (78.8229)  time: 0.3646  data: 0.1554  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.2505 (2.1310)  acc1: 44.4444 (49.2492)  acc5: 72.2222 (78.8288)  time: 0.3683  data: 0.1627  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 1.8755 (2.0862)  acc1: 61.1111 (50.2296)  acc5: 83.3333 (79.4307)  time: 0.3869  data: 0.1659  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.5863 (2.0467)  acc1: 55.5556 (50.8906)  acc5: 88.8889 (80.1103)  time: 0.2811  data: 0.0913  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 1.7524 (2.0432)  acc1: 50.0000 (51.0238)  acc5: 88.8889 (80.2621)  time: 0.1769  data: 0.0128  max mem: 15572
Val: Total time: 0:00:50 (0.3707 s / it)
* Acc@1 50.328 Acc@5 79.095 loss 2.082
Accuracy of the network on the 4883 val videos: 50.3%
Max accuracy: 50.66%
Test:  [   0/1221]  eta: 1:50:10  loss: 0.5095 (0.5095)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 5.4138  data: 5.2833  max mem: 15572
Test:  [  10/1221]  eta: 0:15:16  loss: 2.0974 (1.8977)  acc1: 58.3333 (53.0303)  acc5: 83.3333 (81.8182)  time: 0.7566  data: 0.5964  max mem: 15572
Test:  [  20/1221]  eta: 0:11:02  loss: 2.0974 (2.0077)  acc1: 58.3333 (49.2064)  acc5: 83.3333 (80.9524)  time: 0.3089  data: 0.1519  max mem: 15572
Test:  [  30/1221]  eta: 0:09:18  loss: 2.6606 (2.2815)  acc1: 25.0000 (41.9355)  acc5: 75.0000 (74.7312)  time: 0.3102  data: 0.1590  max mem: 15572
Test:  [  40/1221]  eta: 0:07:46  loss: 2.2575 (2.1963)  acc1: 33.3333 (44.5122)  acc5: 75.0000 (76.4228)  time: 0.2295  data: 0.0809  max mem: 15572
Test:  [  50/1221]  eta: 0:07:34  loss: 1.9028 (2.1856)  acc1: 50.0000 (45.0980)  acc5: 83.3333 (76.4706)  time: 0.2634  data: 0.1219  max mem: 15572
Test:  [  60/1221]  eta: 0:07:06  loss: 2.1065 (2.1635)  acc1: 50.0000 (45.7650)  acc5: 83.3333 (77.3224)  time: 0.3105  data: 0.1577  max mem: 15572
Test:  [  70/1221]  eta: 0:06:37  loss: 2.1065 (2.1512)  acc1: 41.6667 (46.1268)  acc5: 75.0000 (77.5822)  time: 0.2352  data: 0.0687  max mem: 15572
Test:  [  80/1221]  eta: 0:06:23  loss: 2.4449 (2.2170)  acc1: 33.3333 (44.3416)  acc5: 75.0000 (77.0576)  time: 0.2420  data: 0.0880  max mem: 15572
Test:  [  90/1221]  eta: 0:06:11  loss: 2.5317 (2.2341)  acc1: 33.3333 (43.9560)  acc5: 75.0000 (76.5568)  time: 0.2693  data: 0.1240  max mem: 15572
Test:  [ 100/1221]  eta: 0:06:01  loss: 2.0569 (2.1803)  acc1: 50.0000 (45.7921)  acc5: 83.3333 (77.3102)  time: 0.2674  data: 0.1154  max mem: 15572
Test:  [ 110/1221]  eta: 0:05:50  loss: 2.0041 (2.2083)  acc1: 50.0000 (45.2703)  acc5: 83.3333 (76.8769)  time: 0.2545  data: 0.0987  max mem: 15572
Test:  [ 120/1221]  eta: 0:05:42  loss: 2.0629 (2.1837)  acc1: 50.0000 (46.0055)  acc5: 83.3333 (77.6171)  time: 0.2555  data: 0.1002  max mem: 15572
Test:  [ 130/1221]  eta: 0:05:37  loss: 1.8096 (2.1821)  acc1: 50.0000 (45.9924)  acc5: 83.3333 (77.5445)  time: 0.2760  data: 0.1246  max mem: 15572
Test:  [ 140/1221]  eta: 0:05:31  loss: 2.4797 (2.2371)  acc1: 33.3333 (44.4444)  acc5: 75.0000 (76.4184)  time: 0.2792  data: 0.1273  max mem: 15572
Test:  [ 150/1221]  eta: 0:05:28  loss: 2.4797 (2.2646)  acc1: 33.3333 (44.0949)  acc5: 75.0000 (76.1038)  time: 0.2897  data: 0.1330  max mem: 15572
Test:  [ 160/1221]  eta: 0:05:21  loss: 2.2073 (2.2579)  acc1: 50.0000 (44.3582)  acc5: 83.3333 (76.3458)  time: 0.2729  data: 0.1076  max mem: 15572
Test:  [ 170/1221]  eta: 0:05:19  loss: 1.7945 (2.2333)  acc1: 50.0000 (45.3704)  acc5: 83.3333 (76.7057)  time: 0.2810  data: 0.1181  max mem: 15572
Test:  [ 180/1221]  eta: 0:05:14  loss: 1.4740 (2.2029)  acc1: 66.6667 (46.2247)  acc5: 91.6667 (77.3481)  time: 0.2990  data: 0.1543  max mem: 15572
Test:  [ 190/1221]  eta: 0:05:12  loss: 1.9144 (2.2024)  acc1: 50.0000 (46.0297)  acc5: 83.3333 (77.3560)  time: 0.2981  data: 0.1505  max mem: 15572
Test:  [ 200/1221]  eta: 0:05:07  loss: 1.7005 (2.1560)  acc1: 58.3333 (47.5539)  acc5: 83.3333 (77.9436)  time: 0.2958  data: 0.1332  max mem: 15572
Test:  [ 210/1221]  eta: 0:05:04  loss: 1.7367 (2.1585)  acc1: 58.3333 (47.3934)  acc5: 83.3333 (77.7251)  time: 0.2863  data: 0.1254  max mem: 15572
Test:  [ 220/1221]  eta: 0:04:59  loss: 2.1471 (2.1574)  acc1: 50.0000 (47.4736)  acc5: 75.0000 (77.8281)  time: 0.2779  data: 0.1327  max mem: 15572
Test:  [ 230/1221]  eta: 0:04:57  loss: 2.1988 (2.1726)  acc1: 33.3333 (46.8615)  acc5: 83.3333 (77.7056)  time: 0.2829  data: 0.1356  max mem: 15572
Test:  [ 240/1221]  eta: 0:04:52  loss: 2.2389 (2.1622)  acc1: 33.3333 (47.0263)  acc5: 91.6667 (78.2158)  time: 0.2807  data: 0.1286  max mem: 15572
Test:  [ 250/1221]  eta: 0:04:49  loss: 1.5695 (2.1413)  acc1: 66.6667 (47.7756)  acc5: 91.6667 (78.4529)  time: 0.2817  data: 0.1205  max mem: 15572
Test:  [ 260/1221]  eta: 0:04:43  loss: 1.5120 (2.1221)  acc1: 66.6667 (48.3078)  acc5: 83.3333 (78.6079)  time: 0.2628  data: 0.0969  max mem: 15572
Test:  [ 270/1221]  eta: 0:04:41  loss: 1.8306 (2.1243)  acc1: 58.3333 (48.4932)  acc5: 83.3333 (78.6285)  time: 0.2733  data: 0.1070  max mem: 15572
Test:  [ 280/1221]  eta: 0:04:38  loss: 2.0609 (2.1396)  acc1: 50.0000 (48.1020)  acc5: 83.3333 (78.4104)  time: 0.3046  data: 0.1446  max mem: 15572
Test:  [ 290/1221]  eta: 0:04:36  loss: 2.0960 (2.1462)  acc1: 50.0000 (48.1386)  acc5: 83.3333 (78.2932)  time: 0.2987  data: 0.1469  max mem: 15572
Test:  [ 300/1221]  eta: 0:04:32  loss: 2.1807 (2.1512)  acc1: 50.0000 (48.1451)  acc5: 75.0000 (78.1285)  time: 0.2964  data: 0.1266  max mem: 15572
Test:  [ 310/1221]  eta: 0:04:28  loss: 2.1211 (2.1455)  acc1: 50.0000 (48.3119)  acc5: 75.0000 (78.0547)  time: 0.2652  data: 0.0990  max mem: 15572
Test:  [ 320/1221]  eta: 0:04:26  loss: 2.1211 (2.1442)  acc1: 41.6667 (48.1568)  acc5: 83.3333 (78.1672)  time: 0.3006  data: 0.1486  max mem: 15572
Test:  [ 330/1221]  eta: 0:04:22  loss: 2.2505 (2.1497)  acc1: 33.3333 (47.9356)  acc5: 75.0000 (77.8701)  time: 0.2954  data: 0.1388  max mem: 15572
Test:  [ 340/1221]  eta: 0:04:19  loss: 2.2283 (2.1511)  acc1: 41.6667 (47.7028)  acc5: 66.6667 (77.8592)  time: 0.2726  data: 0.1121  max mem: 15572
Test:  [ 350/1221]  eta: 0:04:17  loss: 2.1969 (2.1630)  acc1: 41.6667 (47.2460)  acc5: 75.0000 (77.6591)  time: 0.3181  data: 0.1522  max mem: 15572
Test:  [ 360/1221]  eta: 0:04:12  loss: 2.3529 (2.1681)  acc1: 33.3333 (47.2761)  acc5: 75.0000 (77.5392)  time: 0.2681  data: 0.1112  max mem: 15572
Test:  [ 370/1221]  eta: 0:04:08  loss: 2.0522 (2.1682)  acc1: 41.6667 (47.2597)  acc5: 75.0000 (77.5831)  time: 0.2230  data: 0.0818  max mem: 15572
Test:  [ 380/1221]  eta: 0:04:06  loss: 1.6301 (2.1467)  acc1: 58.3333 (47.8565)  acc5: 83.3333 (77.8871)  time: 0.2914  data: 0.1492  max mem: 15572
Test:  [ 390/1221]  eta: 0:04:02  loss: 1.7479 (2.1500)  acc1: 50.0000 (47.7195)  acc5: 83.3333 (77.8772)  time: 0.2963  data: 0.1572  max mem: 15572
Test:  [ 400/1221]  eta: 0:03:59  loss: 1.7937 (2.1284)  acc1: 50.0000 (48.2959)  acc5: 83.3333 (78.1588)  time: 0.2705  data: 0.1245  max mem: 15572
Test:  [ 410/1221]  eta: 0:03:57  loss: 1.3071 (2.1184)  acc1: 75.0000 (48.5199)  acc5: 91.6667 (78.3252)  time: 0.3081  data: 0.1556  max mem: 15572
Test:  [ 420/1221]  eta: 0:03:53  loss: 1.6514 (2.1080)  acc1: 58.3333 (48.8915)  acc5: 91.6667 (78.5234)  time: 0.2900  data: 0.1419  max mem: 15572
Test:  [ 430/1221]  eta: 0:03:51  loss: 1.9916 (2.1153)  acc1: 58.3333 (48.5499)  acc5: 75.0000 (78.3836)  time: 0.2777  data: 0.1303  max mem: 15572
Test:  [ 440/1221]  eta: 0:03:48  loss: 2.5174 (2.1231)  acc1: 33.3333 (48.2993)  acc5: 75.0000 (78.2880)  time: 0.3060  data: 0.1553  max mem: 15572
Test:  [ 450/1221]  eta: 0:03:44  loss: 2.0359 (2.1184)  acc1: 41.6667 (48.5033)  acc5: 75.0000 (78.2890)  time: 0.2760  data: 0.1216  max mem: 15572
Test:  [ 460/1221]  eta: 0:03:42  loss: 1.6396 (2.1093)  acc1: 58.3333 (48.8069)  acc5: 83.3333 (78.3984)  time: 0.2735  data: 0.1275  max mem: 15572
Test:  [ 470/1221]  eta: 0:03:38  loss: 1.8932 (2.1110)  acc1: 50.0000 (48.7969)  acc5: 75.0000 (78.3970)  time: 0.2898  data: 0.1420  max mem: 15572
Test:  [ 480/1221]  eta: 0:03:36  loss: 2.2159 (2.1140)  acc1: 50.0000 (48.7699)  acc5: 75.0000 (78.3784)  time: 0.3044  data: 0.1564  max mem: 15572
Test:  [ 490/1221]  eta: 0:03:33  loss: 2.4436 (2.1299)  acc1: 33.3333 (48.3367)  acc5: 66.6667 (78.0720)  time: 0.2882  data: 0.1513  max mem: 15572
Test:  [ 500/1221]  eta: 0:03:30  loss: 2.8186 (2.1346)  acc1: 25.0000 (48.1703)  acc5: 66.6667 (78.0106)  time: 0.2726  data: 0.1185  max mem: 15572
Test:  [ 510/1221]  eta: 0:03:27  loss: 2.2230 (2.1377)  acc1: 41.6667 (48.1898)  acc5: 75.0000 (77.9354)  time: 0.2985  data: 0.1355  max mem: 15572
Test:  [ 520/1221]  eta: 0:03:23  loss: 1.9730 (2.1364)  acc1: 50.0000 (48.1926)  acc5: 83.3333 (77.9910)  time: 0.2718  data: 0.1169  max mem: 15572
Test:  [ 530/1221]  eta: 0:03:21  loss: 1.8005 (2.1348)  acc1: 50.0000 (48.2580)  acc5: 83.3333 (78.0132)  time: 0.2736  data: 0.1232  max mem: 15572
Test:  [ 540/1221]  eta: 0:03:18  loss: 1.8745 (2.1365)  acc1: 41.6667 (48.2594)  acc5: 83.3333 (77.9575)  time: 0.2918  data: 0.1454  max mem: 15572
Test:  [ 550/1221]  eta: 0:03:15  loss: 2.4717 (2.1546)  acc1: 25.0000 (47.7768)  acc5: 75.0000 (77.5711)  time: 0.3113  data: 0.1704  max mem: 15572
Test:  [ 560/1221]  eta: 0:03:12  loss: 2.4717 (2.1556)  acc1: 33.3333 (47.7867)  acc5: 75.0000 (77.5401)  time: 0.3069  data: 0.1605  max mem: 15572
Test:  [ 570/1221]  eta: 0:03:09  loss: 1.7769 (2.1574)  acc1: 50.0000 (47.7233)  acc5: 83.3333 (77.5540)  time: 0.2531  data: 0.0999  max mem: 15572
Test:  [ 580/1221]  eta: 0:03:06  loss: 1.7639 (2.1482)  acc1: 58.3333 (48.0350)  acc5: 83.3333 (77.6535)  time: 0.2857  data: 0.1207  max mem: 15572
Test:  [ 590/1221]  eta: 0:03:03  loss: 1.6306 (2.1451)  acc1: 58.3333 (48.0400)  acc5: 83.3333 (77.7778)  time: 0.2876  data: 0.1200  max mem: 15572
Test:  [ 600/1221]  eta: 0:03:00  loss: 1.7272 (2.1371)  acc1: 50.0000 (48.1975)  acc5: 83.3333 (77.8841)  time: 0.2696  data: 0.1000  max mem: 15572
Test:  [ 610/1221]  eta: 0:02:57  loss: 1.6433 (2.1297)  acc1: 58.3333 (48.4179)  acc5: 83.3333 (78.0142)  time: 0.2970  data: 0.1100  max mem: 15572
Test:  [ 620/1221]  eta: 0:02:54  loss: 1.9756 (2.1335)  acc1: 50.0000 (48.3092)  acc5: 83.3333 (77.9522)  time: 0.2759  data: 0.0952  max mem: 15572
Test:  [ 630/1221]  eta: 0:02:51  loss: 2.3894 (2.1371)  acc1: 41.6667 (48.2171)  acc5: 75.0000 (77.9054)  time: 0.2608  data: 0.0922  max mem: 15572
Test:  [ 640/1221]  eta: 0:02:48  loss: 2.5327 (2.1460)  acc1: 33.3333 (47.9329)  acc5: 75.0000 (77.7951)  time: 0.2670  data: 0.1074  max mem: 15572
Test:  [ 650/1221]  eta: 0:02:44  loss: 2.3540 (2.1425)  acc1: 41.6667 (48.1183)  acc5: 83.3333 (77.9314)  time: 0.2655  data: 0.1076  max mem: 15572
Test:  [ 660/1221]  eta: 0:02:42  loss: 1.6879 (2.1400)  acc1: 58.3333 (48.1594)  acc5: 83.3333 (77.9375)  time: 0.2934  data: 0.1386  max mem: 15572
Test:  [ 670/1221]  eta: 0:02:39  loss: 1.9383 (2.1404)  acc1: 50.0000 (48.2240)  acc5: 75.0000 (77.8813)  time: 0.2859  data: 0.1370  max mem: 15572
Test:  [ 680/1221]  eta: 0:02:36  loss: 2.1776 (2.1436)  acc1: 50.0000 (48.2134)  acc5: 75.0000 (77.8634)  time: 0.2645  data: 0.1211  max mem: 15572
Test:  [ 690/1221]  eta: 0:02:33  loss: 2.3578 (2.1544)  acc1: 41.6667 (47.9981)  acc5: 75.0000 (77.6411)  time: 0.2975  data: 0.1491  max mem: 15572
Test:  [ 700/1221]  eta: 0:02:30  loss: 2.4472 (2.1617)  acc1: 33.3333 (47.8602)  acc5: 75.0000 (77.5321)  time: 0.2753  data: 0.1263  max mem: 15572
Test:  [ 710/1221]  eta: 0:02:27  loss: 2.0184 (2.1559)  acc1: 50.0000 (48.0661)  acc5: 75.0000 (77.5785)  time: 0.2774  data: 0.1190  max mem: 15572
Test:  [ 720/1221]  eta: 0:02:24  loss: 2.0184 (2.1615)  acc1: 50.0000 (47.9080)  acc5: 75.0000 (77.4965)  time: 0.3060  data: 0.1329  max mem: 15572
Test:  [ 730/1221]  eta: 0:02:21  loss: 2.5128 (2.1643)  acc1: 33.3333 (47.8112)  acc5: 66.6667 (77.4168)  time: 0.2900  data: 0.1211  max mem: 15572
Test:  [ 740/1221]  eta: 0:02:18  loss: 2.4131 (2.1656)  acc1: 41.6667 (47.8408)  acc5: 75.0000 (77.3954)  time: 0.2939  data: 0.1366  max mem: 15572
Test:  [ 750/1221]  eta: 0:02:16  loss: 2.4371 (2.1730)  acc1: 41.6667 (47.6032)  acc5: 75.0000 (77.2526)  time: 0.2914  data: 0.1431  max mem: 15572
Test:  [ 760/1221]  eta: 0:02:13  loss: 2.4403 (2.1785)  acc1: 33.3333 (47.4485)  acc5: 75.0000 (77.1573)  time: 0.2809  data: 0.1334  max mem: 15572
Test:  [ 770/1221]  eta: 0:02:10  loss: 2.2476 (2.1815)  acc1: 41.6667 (47.4168)  acc5: 75.0000 (77.0968)  time: 0.3054  data: 0.1553  max mem: 15572
Test:  [ 780/1221]  eta: 0:02:07  loss: 2.2476 (2.1807)  acc1: 41.6667 (47.5245)  acc5: 75.0000 (77.0807)  time: 0.3140  data: 0.1550  max mem: 15572
Test:  [ 790/1221]  eta: 0:02:04  loss: 1.9174 (2.1744)  acc1: 50.0000 (47.6928)  acc5: 75.0000 (77.1703)  time: 0.2899  data: 0.1381  max mem: 15572
Test:  [ 800/1221]  eta: 0:02:01  loss: 1.9670 (2.1746)  acc1: 50.0000 (47.7112)  acc5: 83.3333 (77.1223)  time: 0.2709  data: 0.1146  max mem: 15572
Test:  [ 810/1221]  eta: 0:01:58  loss: 1.6868 (2.1660)  acc1: 50.0000 (47.9449)  acc5: 83.3333 (77.2503)  time: 0.2578  data: 0.1053  max mem: 15572
Test:  [ 820/1221]  eta: 0:01:55  loss: 1.6107 (2.1637)  acc1: 66.6667 (47.9801)  acc5: 91.6667 (77.3346)  time: 0.2849  data: 0.1423  max mem: 15572
Test:  [ 830/1221]  eta: 0:01:52  loss: 1.7683 (2.1603)  acc1: 58.3333 (48.0846)  acc5: 83.3333 (77.3967)  time: 0.2793  data: 0.1223  max mem: 15572
Test:  [ 840/1221]  eta: 0:01:49  loss: 1.7683 (2.1613)  acc1: 50.0000 (47.9786)  acc5: 83.3333 (77.4178)  time: 0.2721  data: 0.1162  max mem: 15572
Test:  [ 850/1221]  eta: 0:01:47  loss: 1.8975 (2.1589)  acc1: 41.6667 (48.0024)  acc5: 83.3333 (77.4775)  time: 0.3060  data: 0.1559  max mem: 15572
Test:  [ 860/1221]  eta: 0:01:44  loss: 1.7896 (2.1545)  acc1: 50.0000 (48.1707)  acc5: 83.3333 (77.5068)  time: 0.3131  data: 0.1528  max mem: 15572
Test:  [ 870/1221]  eta: 0:01:41  loss: 1.5866 (2.1501)  acc1: 66.6667 (48.3352)  acc5: 83.3333 (77.5450)  time: 0.2637  data: 0.1038  max mem: 15572
Test:  [ 880/1221]  eta: 0:01:38  loss: 1.8939 (2.1472)  acc1: 58.3333 (48.4109)  acc5: 83.3333 (77.6296)  time: 0.2375  data: 0.0895  max mem: 15572
Test:  [ 890/1221]  eta: 0:01:35  loss: 2.0911 (2.1480)  acc1: 50.0000 (48.3726)  acc5: 83.3333 (77.6843)  time: 0.3132  data: 0.1544  max mem: 15572
Test:  [ 900/1221]  eta: 0:01:32  loss: 2.3228 (2.1529)  acc1: 33.3333 (48.2334)  acc5: 75.0000 (77.5620)  time: 0.3101  data: 0.1405  max mem: 15572
Test:  [ 910/1221]  eta: 0:01:29  loss: 2.2829 (2.1514)  acc1: 33.3333 (48.2162)  acc5: 75.0000 (77.6436)  time: 0.2662  data: 0.0968  max mem: 15572
Test:  [ 920/1221]  eta: 0:01:26  loss: 1.8851 (2.1495)  acc1: 50.0000 (48.2990)  acc5: 83.3333 (77.6421)  time: 0.2904  data: 0.1230  max mem: 15572
Test:  [ 930/1221]  eta: 0:01:23  loss: 1.8851 (2.1489)  acc1: 50.0000 (48.2546)  acc5: 83.3333 (77.6942)  time: 0.3054  data: 0.1435  max mem: 15572
Test:  [ 940/1221]  eta: 0:01:20  loss: 1.8290 (2.1455)  acc1: 50.0000 (48.3528)  acc5: 91.6667 (77.8162)  time: 0.2699  data: 0.1111  max mem: 15572
Test:  [ 950/1221]  eta: 0:01:17  loss: 1.8361 (2.1468)  acc1: 50.0000 (48.2737)  acc5: 83.3333 (77.7778)  time: 0.2504  data: 0.0988  max mem: 15572
Test:  [ 960/1221]  eta: 0:01:15  loss: 2.2445 (2.1524)  acc1: 41.6667 (48.1183)  acc5: 75.0000 (77.6708)  time: 0.2809  data: 0.1284  max mem: 15572
Test:  [ 970/1221]  eta: 0:01:12  loss: 2.2445 (2.1539)  acc1: 41.6667 (48.0690)  acc5: 83.3333 (77.6262)  time: 0.2732  data: 0.1116  max mem: 15572
Test:  [ 980/1221]  eta: 0:01:09  loss: 1.9460 (2.1535)  acc1: 50.0000 (48.1142)  acc5: 83.3333 (77.6419)  time: 0.2806  data: 0.1192  max mem: 15572
Test:  [ 990/1221]  eta: 0:01:06  loss: 1.7089 (2.1445)  acc1: 58.3333 (48.3687)  acc5: 91.6667 (77.7918)  time: 0.3086  data: 0.1471  max mem: 15572
Test:  [1000/1221]  eta: 0:01:03  loss: 1.7089 (2.1460)  acc1: 50.0000 (48.2517)  acc5: 91.6667 (77.8055)  time: 0.2973  data: 0.1284  max mem: 15572
Test:  [1010/1221]  eta: 0:01:00  loss: 1.8700 (2.1364)  acc1: 50.0000 (48.5328)  acc5: 83.3333 (77.9426)  time: 0.2764  data: 0.1123  max mem: 15572
Test:  [1020/1221]  eta: 0:00:57  loss: 1.6145 (2.1351)  acc1: 66.6667 (48.5798)  acc5: 91.6667 (77.9628)  time: 0.2595  data: 0.1085  max mem: 15572
Test:  [1030/1221]  eta: 0:00:54  loss: 2.2050 (2.1362)  acc1: 50.0000 (48.5936)  acc5: 75.0000 (77.9583)  time: 0.2812  data: 0.1318  max mem: 15572
Test:  [1040/1221]  eta: 0:00:52  loss: 2.3422 (2.1396)  acc1: 41.6667 (48.4870)  acc5: 75.0000 (77.9139)  time: 0.2967  data: 0.1413  max mem: 15572
Test:  [1050/1221]  eta: 0:00:49  loss: 2.4268 (2.1445)  acc1: 33.3333 (48.3508)  acc5: 75.0000 (77.8703)  time: 0.3490  data: 0.1845  max mem: 15572
Test:  [1060/1221]  eta: 0:00:46  loss: 2.1842 (2.1413)  acc1: 41.6667 (48.4134)  acc5: 83.3333 (77.8982)  time: 0.2932  data: 0.1259  max mem: 15572
Test:  [1070/1221]  eta: 0:00:43  loss: 1.6618 (2.1378)  acc1: 66.6667 (48.5450)  acc5: 75.0000 (77.9101)  time: 0.2735  data: 0.1122  max mem: 15572
Test:  [1080/1221]  eta: 0:00:40  loss: 1.9531 (2.1399)  acc1: 50.0000 (48.4659)  acc5: 75.0000 (77.8986)  time: 0.3249  data: 0.1680  max mem: 15572
Test:  [1090/1221]  eta: 0:00:37  loss: 2.2611 (2.1429)  acc1: 41.6667 (48.3960)  acc5: 75.0000 (77.8567)  time: 0.2360  data: 0.0781  max mem: 15572
Test:  [1100/1221]  eta: 0:00:34  loss: 2.4166 (2.1495)  acc1: 33.3333 (48.2137)  acc5: 75.0000 (77.7097)  time: 0.2404  data: 0.0861  max mem: 15572
Test:  [1110/1221]  eta: 0:00:31  loss: 2.6171 (2.1520)  acc1: 33.3333 (48.1923)  acc5: 66.6667 (77.6778)  time: 0.2633  data: 0.1148  max mem: 15572
Test:  [1120/1221]  eta: 0:00:29  loss: 2.6171 (2.1536)  acc1: 41.6667 (48.1936)  acc5: 75.0000 (77.6167)  time: 0.2771  data: 0.1319  max mem: 15572
Test:  [1130/1221]  eta: 0:00:26  loss: 2.1109 (2.1537)  acc1: 41.6667 (48.1874)  acc5: 75.0000 (77.6452)  time: 0.2909  data: 0.1442  max mem: 15572
Test:  [1140/1221]  eta: 0:00:23  loss: 2.3653 (2.1559)  acc1: 50.0000 (48.1814)  acc5: 75.0000 (77.6001)  time: 0.2799  data: 0.1253  max mem: 15572
Test:  [1150/1221]  eta: 0:00:20  loss: 2.3665 (2.1568)  acc1: 50.0000 (48.1755)  acc5: 66.6667 (77.5340)  time: 0.2934  data: 0.1350  max mem: 15572
Test:  [1160/1221]  eta: 0:00:17  loss: 2.4779 (2.1634)  acc1: 33.3333 (47.9687)  acc5: 66.6667 (77.3974)  time: 0.2783  data: 0.1280  max mem: 15572
Test:  [1170/1221]  eta: 0:00:14  loss: 2.0186 (2.1625)  acc1: 41.6667 (47.9718)  acc5: 75.0000 (77.4125)  time: 0.2424  data: 0.0959  max mem: 15572
Test:  [1180/1221]  eta: 0:00:11  loss: 2.0186 (2.1644)  acc1: 41.6667 (47.9678)  acc5: 83.3333 (77.3638)  time: 0.2657  data: 0.1187  max mem: 15572
Test:  [1190/1221]  eta: 0:00:08  loss: 2.2898 (2.1619)  acc1: 41.6667 (48.0619)  acc5: 75.0000 (77.3860)  time: 0.3070  data: 0.1543  max mem: 15572
Test:  [1200/1221]  eta: 0:00:06  loss: 2.0150 (2.1606)  acc1: 50.0000 (48.0849)  acc5: 75.0000 (77.4285)  time: 0.3106  data: 0.1579  max mem: 15572
Test:  [1210/1221]  eta: 0:00:03  loss: 2.0099 (2.1578)  acc1: 50.0000 (48.2108)  acc5: 83.3333 (77.4635)  time: 0.2160  data: 0.0870  max mem: 15572
Test:  [1220/1221]  eta: 0:00:00  loss: 1.4387 (2.1537)  acc1: 66.6667 (48.3514)  acc5: 88.8889 (77.5411)  time: 0.1065  data: 0.0001  max mem: 15572
Test: Total time: 0:05:47 (0.2846 s / it)
* Acc@1 48.392 Acc@5 77.719 loss 2.155
Start merging results...
Reading individual output files
Computing final results
4883
Accuracy of the network on the 29298 test videos: Top-1: 51.57%, Top-5: 80.40%
Training time 10:05:37
