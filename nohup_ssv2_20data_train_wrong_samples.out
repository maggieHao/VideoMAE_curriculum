/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 21:21:50,325] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f96943566b0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 21:21:54,027] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 21:21:54,027] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 21:21:54,071] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.052875518798828125 seconds
[2025-01-12 21:21:54,500] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 21:21:54,500] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 21:21:54,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 21:21:54,503] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 21:21:54,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 21:21:54,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 21:21:54,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 21:21:54,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:21:54,513] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 21:21:54,513] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f966379add0>
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 21:21:54,514] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 21:21:54,515] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 21:21:54,516] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 21:21:54,516] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 21:21:54,516] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 21:21:54,516] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 21:21:54,516] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4193686 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 4193651 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-12 21:22:02,197] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ffb25842680>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-12 21:22:05,958] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-12 21:22:05,958] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-12 21:22:06,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.057604074478149414 seconds
[2025-01-12 21:22:06,392] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-12 21:22:06,392] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-12 21:22:06,394] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-12 21:22:06,395] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-12 21:22:06,402] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-12 21:22:06,402] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-12 21:22:06,402] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-12 21:22:06,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:22:06,402] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-12 21:22:06,402] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-12 21:22:06,402] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-12 21:22:06,402] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-12 21:22:06,402] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ffb24efedd0>
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-12 21:22:06,403] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-12 21:22:06,404] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-12 21:22:06,404] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
Epoch: [0]  [   0/2809]  eta: 12:12:04  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 15.6371  data: 6.6377  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:23:42  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.7945  data: 0.6040  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:53:50  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4343  data: 0.0007  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:42:50  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4465  data: 0.0008  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:37:48  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4631  data: 0.0010  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:35:04  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0279  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:33:41  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0969  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:32:41  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1451  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:32:02  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1656  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:31:21  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1492  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:30:45  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1166  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:30:07  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1137  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:29:11  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0739  max mem: 15572
[2025-01-12 21:23:29,168] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:23:29,169] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:28:26  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.4550  data: 0.0180  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:27:55  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0242  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:28:05  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1614  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:28:16  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.7301  data: 0.2675  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:28:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6795  data: 0.2242  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:28:08  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6557  data: 0.1923  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:28:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.7245  data: 0.2136  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:28:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1591)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.7141  data: 0.2060  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:28:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.2022  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:28:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1590)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.7357  data: 0.2437  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:28:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1576 (5.1590)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.1982  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:28:13  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6835  data: 0.2082  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:28:16  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1588)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.7644  data: 0.2736  max mem: 15572
[2025-01-12 21:24:55,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:24:55,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:27:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1318  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:27:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1587)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.4034  data: 0.0004  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:26:51  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.4351  data: 0.0004  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:26:28  lr: 0.000001  min_lr: 0.000000  loss: 5.1567 (5.1586)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.4530  data: 0.0005  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:26:08  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.4562  data: 0.0068  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:25:59  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1584)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0823  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:25:47  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1267  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:25:52  lr: 0.000001  min_lr: 0.000000  loss: 5.1555 (5.1583)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.2185  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:25:44  lr: 0.000001  min_lr: 0.000000  loss: 5.1554 (5.1582)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.2351  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:25:43  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1581)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1996  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:25:33  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1580)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1938  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:25:26  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1579)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1419  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:25:19  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1578)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1521  max mem: 15572
[2025-01-12 21:26:07,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:26:07,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:25:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1577)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1036  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:24:58  lr: 0.000001  min_lr: 0.000000  loss: 5.1520 (5.1576)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0900  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:24:47  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1575)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1113  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:24:36  lr: 0.000001  min_lr: 0.000000  loss: 5.1532 (5.1573)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0927  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:24:36  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1572)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1567  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:24:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1571)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1315  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:24:19  lr: 0.000002  min_lr: 0.000000  loss: 5.1515 (5.1569)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1426  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:24:13  lr: 0.000002  min_lr: 0.000000  loss: 5.1496 (5.1568)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.1947  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:24:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1490 (5.1566)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0917  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:23:53  lr: 0.000002  min_lr: 0.000000  loss: 5.1484 (5.1564)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0585  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1474 (5.1562)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1560  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:41  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1740  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:23:28  lr: 0.000002  min_lr: 0.000000  loss: 5.1456 (5.1558)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0540  max mem: 15572
[2025-01-12 21:27:21,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:27:21,015] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:23:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1441 (5.1556)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0500  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:23:16  lr: 0.000002  min_lr: 0.000000  loss: 5.1440 (5.1553)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1372  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:23:07  lr: 0.000002  min_lr: 0.000000  loss: 5.1419 (5.1551)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1454  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:23:00  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1040  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:54  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1547)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1234  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:52  lr: 0.000002  min_lr: 0.000000  loss: 5.1395 (5.1545)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.2059  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:45  lr: 0.000002  min_lr: 0.000000  loss: 5.1396 (5.1543)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.1936  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:34  lr: 0.000002  min_lr: 0.000000  loss: 5.1376 (5.1540)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0742  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:22:29  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1537)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0938  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:22:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1535)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.1933  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:22:18  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1531)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1673  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:22:13  lr: 0.000002  min_lr: 0.000000  loss: 5.1308 (5.1529)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1417  max mem: 15572
[2025-01-12 21:28:39,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:28:39,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:22:06  lr: 0.000002  min_lr: 0.000000  loss: 5.1382 (5.1527)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1488  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1335 (5.1523)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0637  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1286 (5.1520)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0891  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1264 (5.1516)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1533  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:42  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1512)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.2079  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:21:33  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1509)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1783  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:21:24  lr: 0.000002  min_lr: 0.000000  loss: 5.1347 (5.1506)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0626  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:21:16  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1503)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0798  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:21:11  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1499)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1426  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:21:03  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1495)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1021  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:53  lr: 0.000002  min_lr: 0.000000  loss: 5.1246 (5.1492)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.4974  data: 0.0303  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:50  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1487)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1259  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1483)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1246  max mem: 15572
[2025-01-12 21:29:51,760] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:29:51,760] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:35  lr: 0.000003  min_lr: 0.000000  loss: 5.1169 (5.1479)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0947  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:30  lr: 0.000003  min_lr: 0.000000  loss: 5.1170 (5.1475)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1706  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:20:24  lr: 0.000003  min_lr: 0.000000  loss: 5.1164 (5.1472)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1639  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:20:20  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1467)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1924  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:20:15  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1463)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.2162  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:20:10  lr: 0.000003  min_lr: 0.000000  loss: 5.1146 (5.1460)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1674  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:20:02  lr: 0.000003  min_lr: 0.000000  loss: 5.1068 (5.1455)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1034  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:55  lr: 0.000003  min_lr: 0.000000  loss: 5.1024 (5.1450)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0760  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:47  lr: 0.000003  min_lr: 0.000000  loss: 5.1057 (5.1446)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0689  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:40  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1442)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0862  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:33  lr: 0.000003  min_lr: 0.000000  loss: 5.1044 (5.1439)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1347  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:28  lr: 0.000003  min_lr: 0.000000  loss: 5.1025 (5.1433)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1533  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:19:20  lr: 0.000003  min_lr: 0.000000  loss: 5.0929 (5.1429)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.0816  max mem: 15572
[2025-01-12 21:31:08,706] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:31:08,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:19:16  lr: 0.000003  min_lr: 0.000000  loss: 5.1061 (5.1425)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1050  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:19:08  lr: 0.000003  min_lr: 0.000000  loss: 5.1046 (5.1421)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1259  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:19:02  lr: 0.000003  min_lr: 0.000000  loss: 5.1040 (5.1418)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1033  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:55  lr: 0.000003  min_lr: 0.000000  loss: 5.1048 (5.1414)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1274  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:48  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1410)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0863  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:42  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1407)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1233  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:35  lr: 0.000003  min_lr: 0.000000  loss: 5.1051 (5.1402)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1139  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:29  lr: 0.000003  min_lr: 0.000000  loss: 5.1051 (5.1399)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1169  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:24  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1394)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1719  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:18:19  lr: 0.000003  min_lr: 0.000000  loss: 5.0866 (5.1387)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1680  max mem: 15572
[2025-01-12 21:32:10,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:32:10,353] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=27.360279641828157, CurrSamplesPerSec=24.063545185679576, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:18:12  lr: 0.000003  min_lr: 0.000000  loss: 5.0870 (5.1383)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1172  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:18:04  lr: 0.000003  min_lr: 0.000000  loss: 5.0940 (5.1380)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0362  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:17:57  lr: 0.000003  min_lr: 0.000000  loss: 5.0888 (5.1375)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0656  max mem: 15572
[2025-01-12 21:32:23,992] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:32:23,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:50  lr: 0.000003  min_lr: 0.000000  loss: 5.0857 (5.1370)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.1057  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:45  lr: 0.000003  min_lr: 0.000000  loss: 5.0919 (5.1368)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1403  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:38  lr: 0.000004  min_lr: 0.000000  loss: 5.1028 (5.1364)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1514  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:32  lr: 0.000004  min_lr: 0.000000  loss: 5.0846 (5.1359)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1328  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:26  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1354)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1508  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:19  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1348)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1088  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:17:13  lr: 0.000004  min_lr: 0.000000  loss: 5.0771 (5.1344)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0999  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:17:07  lr: 0.000004  min_lr: 0.000000  loss: 5.0976 (5.1341)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1150  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:17:02  lr: 0.000004  min_lr: 0.000000  loss: 5.0843 (5.1335)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1613  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:16:56  lr: 0.000004  min_lr: 0.000000  loss: 5.0730 (5.1331)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1815  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:50  lr: 0.000004  min_lr: 0.000000  loss: 5.0699 (5.1326)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1545  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:42  lr: 0.000004  min_lr: 0.000000  loss: 5.0726 (5.1321)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0909  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1317)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0713  max mem: 15572
[2025-01-12 21:33:39,336] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:33:39,337] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1313)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1552  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:23  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1307)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1287  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:16:17  lr: 0.000004  min_lr: 0.000000  loss: 5.0701 (5.1303)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1431  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:16:10  lr: 0.000004  min_lr: 0.000000  loss: 5.0956 (5.1299)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1354  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:16:02  lr: 0.000004  min_lr: 0.000000  loss: 5.0828 (5.1295)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0346  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:15:57  lr: 0.000004  min_lr: 0.000000  loss: 5.0916 (5.1291)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0823  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:51  lr: 0.000004  min_lr: 0.000000  loss: 5.0774 (5.1286)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1539  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:45  lr: 0.000004  min_lr: 0.000000  loss: 5.0682 (5.1283)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1837  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:39  lr: 0.000004  min_lr: 0.000000  loss: 5.0855 (5.1280)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1598  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:33  lr: 0.000004  min_lr: 0.000000  loss: 5.0822 (5.1276)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1250  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:25  lr: 0.000004  min_lr: 0.000000  loss: 5.0856 (5.1273)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0869  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:19  lr: 0.000004  min_lr: 0.000000  loss: 5.0768 (5.1269)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0818  max mem: 15572
[2025-01-12 21:34:53,170] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:34:53,171] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:15:14  lr: 0.000004  min_lr: 0.000000  loss: 5.0672 (5.1266)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1646  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:15:07  lr: 0.000004  min_lr: 0.000000  loss: 5.0922 (5.1264)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1555  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:15:01  lr: 0.000004  min_lr: 0.000000  loss: 5.0757 (5.1259)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1215  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:14:55  lr: 0.000004  min_lr: 0.000000  loss: 5.0489 (5.1253)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1097  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:49  lr: 0.000004  min_lr: 0.000000  loss: 5.0517 (5.1248)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1160  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:43  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1244)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1433  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:38  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1239)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1613  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:31  lr: 0.000005  min_lr: 0.000000  loss: 5.0654 (5.1235)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1094  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:24  lr: 0.000005  min_lr: 0.000000  loss: 5.0695 (5.1231)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5027  data: 0.0455  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:17  lr: 0.000005  min_lr: 0.000000  loss: 5.0580 (5.1226)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0795  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:11  lr: 0.000005  min_lr: 0.000000  loss: 5.0384 (5.1220)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0750  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1216)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0908  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:13:59  lr: 0.000005  min_lr: 0.000000  loss: 5.0647 (5.1213)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1596  max mem: 15572
[2025-01-12 21:36:07,586] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:36:07,587] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:13:53  lr: 0.000005  min_lr: 0.000000  loss: 5.0692 (5.1209)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1832  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:47  lr: 0.000005  min_lr: 0.000000  loss: 5.0708 (5.1204)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1272  max mem: 15572
[2025-01-12 21:36:19,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-12 21:36:19,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-12 21:36:19,282] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:40  lr: 0.000005  min_lr: 0.000000  loss: 5.0685 (5.1199)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0874  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:34  lr: 0.000005  min_lr: 0.000000  loss: 5.0702 (5.1197)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1334  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:28  lr: 0.000005  min_lr: 0.000000  loss: 5.0620 (5.1192)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1437  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:23  lr: 0.000005  min_lr: 0.000000  loss: 5.0597 (5.1188)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1821  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:17  lr: 0.000005  min_lr: 0.000000  loss: 5.0780 (5.1186)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1560  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:11  lr: 0.000005  min_lr: 0.000000  loss: 5.0553 (5.1182)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0987  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:13:05  lr: 0.000005  min_lr: 0.000000  loss: 5.0651 (5.1178)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1121  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:12:58  lr: 0.000005  min_lr: 0.000000  loss: 5.0768 (5.1175)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1001  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:12:53  lr: 0.000005  min_lr: 0.000000  loss: 5.0635 (5.1170)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1179  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:46  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1166)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1004  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:41  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1163)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1255  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:34  lr: 0.000005  min_lr: 0.000000  loss: 5.0472 (5.1159)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1522  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:29  lr: 0.000005  min_lr: 0.000000  loss: 5.0354 (5.1154)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1480  max mem: 15572
[2025-01-12 21:37:34,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:37:34,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:22  lr: 0.000005  min_lr: 0.000000  loss: 5.0520 (5.1152)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1231  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:16  lr: 0.000005  min_lr: 0.000000  loss: 5.0554 (5.1148)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1192  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:10  lr: 0.000005  min_lr: 0.000000  loss: 5.0238 (5.1142)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1307  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:12:05  lr: 0.000005  min_lr: 0.000000  loss: 5.0186 (5.1136)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1499  max mem: 15572
[2025-01-12 21:37:57,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1599
[2025-01-12 21:37:57,912] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:37:57,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1600/2809]  eta: 0:11:58  lr: 0.000005  min_lr: 0.000000  loss: 5.0195 (5.1131)  loss_scale: 262144.0000 (41824.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1038  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:52  lr: 0.000005  min_lr: 0.000000  loss: 5.0479 (5.1127)  loss_scale: 131072.0000 (42378.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0810  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:46  lr: 0.000005  min_lr: 0.000000  loss: 5.0527 (5.1123)  loss_scale: 131072.0000 (42925.8779)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1540  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:40  lr: 0.000005  min_lr: 0.000000  loss: 5.0335 (5.1119)  loss_scale: 131072.0000 (43466.3200)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1513  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:34  lr: 0.000005  min_lr: 0.000000  loss: 5.0410 (5.1115)  loss_scale: 131072.0000 (44000.1755)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1712  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:28  lr: 0.000006  min_lr: 0.000000  loss: 5.0617 (5.1111)  loss_scale: 131072.0000 (44527.5639)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1881  max mem: 15572
Epoch: [0]  [1660/2809]  eta: 0:11:22  lr: 0.000006  min_lr: 0.000000  loss: 5.0134 (5.1104)  loss_scale: 131072.0000 (45048.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0953  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:16  lr: 0.000006  min_lr: 0.000000  loss: 5.0045 (5.1100)  loss_scale: 131072.0000 (45563.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0821  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:10  lr: 0.000006  min_lr: 0.000000  loss: 5.0359 (5.1098)  loss_scale: 131072.0000 (46072.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1389  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:11:03  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1095)  loss_scale: 131072.0000 (46574.7416)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1159  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:10:58  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1094)  loss_scale: 131072.0000 (47071.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1878  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:52  lr: 0.000006  min_lr: 0.000000  loss: 5.0553 (5.1089)  loss_scale: 131072.0000 (47562.4360)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.2030  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:46  lr: 0.000006  min_lr: 0.000000  loss: 5.0238 (5.1084)  loss_scale: 131072.0000 (48047.6746)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0863  max mem: 15572
[2025-01-12 21:39:13,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:39:13,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1730/2809]  eta: 0:10:39  lr: 0.000006  min_lr: 0.000000  loss: 5.0297 (5.1080)  loss_scale: 131072.0000 (48754.4679)  weight_decay: 0.0500 (0.0500)  time: 0.4954  data: 0.0375  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:33  lr: 0.000006  min_lr: 0.000000  loss: 5.0292 (5.1075)  loss_scale: 262144.0000 (49980.1401)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0773  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:28  lr: 0.000006  min_lr: 0.000000  loss: 5.0127 (5.1070)  loss_scale: 262144.0000 (51191.8127)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1758  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:22  lr: 0.000006  min_lr: 0.000000  loss: 5.0273 (5.1067)  loss_scale: 262144.0000 (52389.7240)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1503  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0398 (5.1062)  loss_scale: 262144.0000 (53574.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0271  max mem: 15572
[2025-01-12 21:39:39,653] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1776
[2025-01-12 21:39:39,654] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:39:39,655] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1780/2809]  eta: 0:10:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0520 (5.1059)  loss_scale: 262144.0000 (54377.2173)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0041  max mem: 15572
Epoch: [0]  [1790/2809]  eta: 0:10:03  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1056)  loss_scale: 131072.0000 (54805.4405)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0640  max mem: 15572
Epoch: [0]  [1800/2809]  eta: 0:09:57  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1054)  loss_scale: 131072.0000 (55228.9084)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1104  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:51  lr: 0.000006  min_lr: 0.000000  loss: 5.0073 (5.1048)  loss_scale: 131072.0000 (55647.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1125  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:45  lr: 0.000006  min_lr: 0.000000  loss: 5.0470 (5.1046)  loss_scale: 131072.0000 (56061.8913)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1334  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:39  lr: 0.000006  min_lr: 0.000000  loss: 5.0751 (5.1045)  loss_scale: 131072.0000 (56471.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1183  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:32  lr: 0.000006  min_lr: 0.000000  loss: 5.0669 (5.1042)  loss_scale: 131072.0000 (56876.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0761  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:26  lr: 0.000006  min_lr: 0.000000  loss: 5.0295 (5.1039)  loss_scale: 131072.0000 (57277.6143)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0678  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:20  lr: 0.000006  min_lr: 0.000000  loss: 5.0140 (5.1034)  loss_scale: 131072.0000 (57674.1451)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1071  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0332 (5.1030)  loss_scale: 131072.0000 (58066.4372)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1441  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0503 (5.1027)  loss_scale: 131072.0000 (58454.5582)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1650  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:03  lr: 0.000006  min_lr: 0.000000  loss: 5.0270 (5.1022)  loss_scale: 131072.0000 (58838.5743)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1909  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:08:57  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1021)  loss_scale: 131072.0000 (59218.5502)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1464  max mem: 15572
[2025-01-12 21:40:54,177] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:40:54,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 21:40:58,153] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1909
[2025-01-12 21:40:58,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:40:58,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1910/2809]  eta: 0:08:52  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1017)  loss_scale: 131072.0000 (59868.9021)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1525  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:45  lr: 0.000006  min_lr: 0.000000  loss: 5.0367 (5.1014)  loss_scale: 131072.0000 (60239.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1372  max mem: 15572
Epoch: [0]  [1930/2809]  eta: 0:08:39  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1008)  loss_scale: 131072.0000 (60606.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0986  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:33  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1004)  loss_scale: 131072.0000 (60969.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1352  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:28  lr: 0.000007  min_lr: 0.000000  loss: 5.0086 (5.1000)  loss_scale: 131072.0000 (61328.7299)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1466  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:21  lr: 0.000007  min_lr: 0.000000  loss: 4.9853 (5.0996)  loss_scale: 131072.0000 (61684.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0811  max mem: 15572
Epoch: [0]  [1970/2809]  eta: 0:08:16  lr: 0.000007  min_lr: 0.000000  loss: 4.9785 (5.0991)  loss_scale: 131072.0000 (62036.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1170  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:10  lr: 0.000007  min_lr: 0.000000  loss: 5.0137 (5.0989)  loss_scale: 131072.0000 (62384.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1887  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:04  lr: 0.000007  min_lr: 0.000000  loss: 5.0183 (5.0985)  loss_scale: 131072.0000 (62729.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1623  max mem: 15572
[2025-01-12 21:41:51,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:41:51,413] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=27.57144853317993, CurrSamplesPerSec=29.002407472499613, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:07:58  lr: 0.000007  min_lr: 0.000000  loss: 5.0114 (5.0981)  loss_scale: 131072.0000 (63071.4403)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1758  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:52  lr: 0.000007  min_lr: 0.000000  loss: 4.9854 (5.0975)  loss_scale: 131072.0000 (63409.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0858  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:46  lr: 0.000007  min_lr: 0.000000  loss: 4.9888 (5.0973)  loss_scale: 131072.0000 (63744.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0677  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:40  lr: 0.000007  min_lr: 0.000000  loss: 5.0313 (5.0971)  loss_scale: 131072.0000 (64075.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0884  max mem: 15572
[2025-01-12 21:42:13,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:42:13,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2040/2809]  eta: 0:07:34  lr: 0.000007  min_lr: 0.000000  loss: 4.9939 (5.0964)  loss_scale: 131072.0000 (64596.7898)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1328  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:28  lr: 0.000007  min_lr: 0.000000  loss: 4.9703 (5.0960)  loss_scale: 262144.0000 (65559.9649)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1807  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:22  lr: 0.000007  min_lr: 0.000000  loss: 5.0084 (5.0957)  loss_scale: 262144.0000 (66513.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1179  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:16  lr: 0.000007  min_lr: 0.000000  loss: 5.0252 (5.0954)  loss_scale: 262144.0000 (67458.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1269  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:10  lr: 0.000007  min_lr: 0.000000  loss: 5.0044 (5.0949)  loss_scale: 262144.0000 (68393.9491)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1574  max mem: 15572
[2025-01-12 21:42:40,289] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2086
[2025-01-12 21:42:40,289] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:42:40,289] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2090/2809]  eta: 0:07:04  lr: 0.000007  min_lr: 0.000000  loss: 4.9915 (5.0944)  loss_scale: 262144.0000 (69007.1200)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1023  max mem: 15572
Epoch: [0]  [2100/2809]  eta: 0:06:58  lr: 0.000007  min_lr: 0.000000  loss: 5.0105 (5.0942)  loss_scale: 131072.0000 (69302.5264)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0821  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:52  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0937)  loss_scale: 131072.0000 (69595.1341)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1273  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:46  lr: 0.000007  min_lr: 0.000000  loss: 4.9933 (5.0933)  loss_scale: 131072.0000 (69884.9826)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1423  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:41  lr: 0.000007  min_lr: 0.000000  loss: 4.9997 (5.0930)  loss_scale: 131072.0000 (70172.1107)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1504  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:35  lr: 0.000007  min_lr: 0.000000  loss: 5.0188 (5.0929)  loss_scale: 131072.0000 (70456.5567)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1571  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:29  lr: 0.000007  min_lr: 0.000000  loss: 5.0121 (5.0926)  loss_scale: 131072.0000 (70738.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0815  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:23  lr: 0.000007  min_lr: 0.000000  loss: 5.0001 (5.0922)  loss_scale: 131072.0000 (71017.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0665  max mem: 15572
Epoch: [0]  [2170/2809]  eta: 0:06:17  lr: 0.000007  min_lr: 0.000000  loss: 5.0220 (5.0919)  loss_scale: 131072.0000 (71294.1723)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1386  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:11  lr: 0.000007  min_lr: 0.000000  loss: 5.0319 (5.0918)  loss_scale: 131072.0000 (71568.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1321  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:05  lr: 0.000007  min_lr: 0.000000  loss: 5.0153 (5.0913)  loss_scale: 131072.0000 (71839.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1445  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:05:59  lr: 0.000007  min_lr: 0.000000  loss: 5.0022 (5.0910)  loss_scale: 131072.0000 (72108.9541)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1548  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:53  lr: 0.000007  min_lr: 0.000000  loss: 5.0012 (5.0904)  loss_scale: 131072.0000 (72375.6346)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1134  max mem: 15572
[2025-01-12 21:43:54,998] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:43:54,998] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2220/2809]  eta: 0:05:47  lr: 0.000007  min_lr: 0.000000  loss: 4.9886 (5.0900)  loss_scale: 131072.0000 (72994.0027)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0844  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:41  lr: 0.000007  min_lr: 0.000000  loss: 4.9899 (5.0894)  loss_scale: 262144.0000 (73841.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0912  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:35  lr: 0.000007  min_lr: 0.000000  loss: 5.0007 (5.0892)  loss_scale: 262144.0000 (74682.0884)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1408  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:29  lr: 0.000008  min_lr: 0.000000  loss: 5.0239 (5.0888)  loss_scale: 262144.0000 (75514.8823)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1631  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:23  lr: 0.000008  min_lr: 0.000000  loss: 5.0211 (5.0886)  loss_scale: 262144.0000 (76340.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1035  max mem: 15572
[2025-01-12 21:44:22,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2263
[2025-01-12 21:44:22,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:44:22,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2270/2809]  eta: 0:05:18  lr: 0.000008  min_lr: 0.000000  loss: 4.9858 (5.0880)  loss_scale: 262144.0000 (76696.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1198  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:12  lr: 0.000008  min_lr: 0.000000  loss: 4.9672 (5.0876)  loss_scale: 131072.0000 (76935.1267)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1405  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:05  lr: 0.000008  min_lr: 0.000000  loss: 5.0192 (5.0872)  loss_scale: 131072.0000 (77171.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0623  max mem: 15572
Epoch: [0]  [2300/2809]  eta: 0:05:00  lr: 0.000008  min_lr: 0.000000  loss: 4.9886 (5.0867)  loss_scale: 131072.0000 (77405.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0725  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:54  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0864)  loss_scale: 131072.0000 (77637.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1039  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:48  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0861)  loss_scale: 131072.0000 (77868.1189)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0813  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:42  lr: 0.000008  min_lr: 0.000000  loss: 4.9869 (5.0857)  loss_scale: 131072.0000 (78096.3638)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0890  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:36  lr: 0.000008  min_lr: 0.000000  loss: 5.0125 (5.0856)  loss_scale: 131072.0000 (78322.6587)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.1777  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:30  lr: 0.000008  min_lr: 0.000000  loss: 5.0449 (5.0854)  loss_scale: 131072.0000 (78547.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1404  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:24  lr: 0.000008  min_lr: 0.000000  loss: 5.0186 (5.0851)  loss_scale: 131072.0000 (78769.4977)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1362  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:18  lr: 0.000008  min_lr: 0.000000  loss: 5.0500 (5.0852)  loss_scale: 131072.0000 (78990.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1333  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:12  lr: 0.000008  min_lr: 0.000000  loss: 5.0313 (5.0848)  loss_scale: 131072.0000 (79208.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0471  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:06  lr: 0.000008  min_lr: 0.000000  loss: 5.0198 (5.0846)  loss_scale: 131072.0000 (79425.7399)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1138  max mem: 15572
[2025-01-12 21:45:38,659] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:45:38,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-12 21:45:41,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-12 21:45:41,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:45:41,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:00  lr: 0.000008  min_lr: 0.000000  loss: 5.0043 (5.0842)  loss_scale: 131072.0000 (80022.9771)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1719  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:55  lr: 0.000008  min_lr: 0.000000  loss: 4.9684 (5.0835)  loss_scale: 131072.0000 (80234.7109)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1646  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:49  lr: 0.000008  min_lr: 0.000000  loss: 4.9712 (5.0834)  loss_scale: 131072.0000 (80444.6956)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1241  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:43  lr: 0.000008  min_lr: 0.000000  loss: 5.0326 (5.0833)  loss_scale: 131072.0000 (80652.9527)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.1893  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:37  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0831)  loss_scale: 131072.0000 (80859.5035)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1730  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:31  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0828)  loss_scale: 131072.0000 (81064.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1023  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:25  lr: 0.000008  min_lr: 0.000000  loss: 5.0174 (5.0826)  loss_scale: 131072.0000 (81267.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1075  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:19  lr: 0.000008  min_lr: 0.000000  loss: 5.0133 (5.0823)  loss_scale: 131072.0000 (81469.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1082  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:13  lr: 0.000008  min_lr: 0.000000  loss: 4.9948 (5.0819)  loss_scale: 131072.0000 (81669.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1204  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:07  lr: 0.000008  min_lr: 0.000000  loss: 4.9702 (5.0815)  loss_scale: 131072.0000 (81867.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0964  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:02  lr: 0.000008  min_lr: 0.000000  loss: 5.0013 (5.0812)  loss_scale: 131072.0000 (82064.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1054  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:56  lr: 0.000008  min_lr: 0.000000  loss: 4.9884 (5.0809)  loss_scale: 131072.0000 (82259.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1235  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:50  lr: 0.000008  min_lr: 0.000000  loss: 4.9615 (5.0804)  loss_scale: 131072.0000 (82452.9187)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1727  max mem: 15572
[2025-01-12 21:46:58,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:46:58,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:44  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0801)  loss_scale: 131072.0000 (82800.3730)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1510  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0214 (5.0798)  loss_scale: 262144.0000 (83506.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1252  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:32  lr: 0.000009  min_lr: 0.000000  loss: 5.0318 (5.0798)  loss_scale: 262144.0000 (84206.4383)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1373  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:26  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0794)  loss_scale: 262144.0000 (84901.2355)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0848  max mem: 15572
[2025-01-12 21:47:19,989] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-12 21:47:19,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:47:19,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:20  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0792)  loss_scale: 262144.0000 (85284.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1044  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:14  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0788)  loss_scale: 131072.0000 (85462.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1101  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:08  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0786)  loss_scale: 131072.0000 (85638.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1331  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 4.9907 (5.0780)  loss_scale: 131072.0000 (85812.8535)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1650  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:57  lr: 0.000009  min_lr: 0.000000  loss: 4.9891 (5.0776)  loss_scale: 131072.0000 (85986.1938)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1625  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 5.0390 (5.0776)  loss_scale: 131072.0000 (86158.2114)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.1850  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000000  loss: 5.0639 (5.0774)  loss_scale: 131072.0000 (86328.9213)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1362  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 4.9870 (5.0772)  loss_scale: 131072.0000 (86498.3385)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1142  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.0771)  loss_scale: 131072.0000 (86666.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0871  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:27  lr: 0.000009  min_lr: 0.000000  loss: 5.0035 (5.0768)  loss_scale: 131072.0000 (86833.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0766  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:21  lr: 0.000009  min_lr: 0.000000  loss: 4.9875 (5.0765)  loss_scale: 131072.0000 (86998.9787)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1169  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 4.9701 (5.0761)  loss_scale: 131072.0000 (87163.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0821  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:10  lr: 0.000009  min_lr: 0.000000  loss: 4.9704 (5.0759)  loss_scale: 131072.0000 (87326.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1641  max mem: 15572
[2025-01-12 21:48:36,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:48:36,454] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:04  lr: 0.000009  min_lr: 0.000000  loss: 5.0239 (5.0758)  loss_scale: 131072.0000 (87828.1881)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.1715  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0756)  loss_scale: 262144.0000 (88471.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0498  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0047 (5.0755)  loss_scale: 262144.0000 (89109.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0463  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 4.9830 (5.0752)  loss_scale: 262144.0000 (89743.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0509  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9566 (5.0748)  loss_scale: 262144.0000 (90372.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0895  max mem: 15572
[2025-01-12 21:49:06,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-12 21:49:06,479] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:49:06,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000000  loss: 4.9600 (5.0745)  loss_scale: 262144.0000 (90948.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0968  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0024 (5.0742)  loss_scale: 131072.0000 (91094.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1058  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9535 (5.0738)  loss_scale: 131072.0000 (91238.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.0984  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0012 (5.0736)  loss_scale: 131072.0000 (91381.5980)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0631  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9633 (5.0731)  loss_scale: 131072.0000 (91523.8065)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.1845  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9705 (5.0731)  loss_scale: 131072.0000 (91664.9996)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1749  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)  time: 0.4447  data: 0.0389  max mem: 15572
Epoch: [0] Total time: 0:27:32 (0.5883 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0420 (5.0730)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 619
Indices to remove: tensor([   64,    69,   182,   268,   363,   463,   587,   659,   663,   799,
         1083,  1158,  1244,  1337,  1582,  1911,  1971,  1987,  2201,  2226,
         2252,  2353,  2396,  2430,  2567,  2645,  2669,  2701,  2731,  2743,
         2807,  3124,  3453,  3494,  3528,  4043,  4496,  4511,  4609,  4710,
         4807,  4815,  4935,  5321,  5354,  5468,  5506,  5742,  5813,  5957,
         6027,  6055,  6137,  6189,  6322,  6742,  6792,  6863,  7026,  7275,
         7285,  7557,  7728,  7864,  7982,  8051,  8590,  8600,  8606,  8622,
         8643,  8828,  8830,  8866,  8895,  8902,  8961,  8997,  9021,  9033,
         9060,  9064,  9085,  9116,  9192,  9308,  9320,  9325,  9348,  9370,
         9385,  9388,  9394,  9400,  9411,  9425,  9450,  9451,  9457,  9468,
         9475,  9482,  9486,  9495,  9506,  9510,  9515,  9518,  9519,  9525,
         9527,  9528,  9531,  9549,  9570,  9576,  9580,  9581,  9588,  9589,
         9590,  9591,  9596,  9599,  9600,  9601,  9602,  9616,  9625,  9637,
         9644,  9648,  9653,  9665,  9672,  9676,  9697,  9709,  9719,  9729,
         9739,  9741,  9744,  9756,  9763,  9764,  9770,  9773,  9788,  9805,
         9812,  9815,  9816,  9825,  9826,  9827,  9830,  9836,  9844,  9852,
         9857,  9862,  9865,  9866,  9881,  9894,  9907,  9908,  9915,  9917,
         9923,  9937,  9938, 10116, 10229, 10352, 10637, 10793, 10860, 10995,
        11389, 11443, 11463, 11946, 12647, 13208, 13389, 13437, 13676, 13770,
        14010, 14150, 14245, 14514, 14734, 14761, 14775, 14786, 14855, 15150,
        15226, 15256, 15343, 15405, 15483, 15523, 15580, 15695, 15951, 16086,
        16376, 16474, 16996, 17333, 17405, 17433, 17454, 17484, 17522, 17526,
        17568, 17579, 17635, 17695, 17719, 18015, 18124, 18224, 18496, 18520,
        18527, 18796, 18909, 18940, 18985, 19148, 19273, 19319, 19333, 19436,
        19464, 19567, 19570, 19801, 20151, 20157, 20201, 20240, 20463, 20502,
        20615, 20618, 20622, 20649, 20873, 20875, 20899, 20902, 20939, 21058,
        21062, 21067, 21072, 21073, 21077, 21078, 21079, 21080, 21081, 21084,
        21085, 21086, 21088, 21091, 21093, 21098, 21099, 21100, 21103, 21105,
        21109, 21116, 21118, 21119, 21123, 21124, 21125, 21127, 21129, 21133,
        21137, 21138, 21141, 21142, 21149, 21151, 21155, 21156, 21157, 21158,
        21160, 21161, 21166, 21167, 21168, 21169, 21170, 21173, 21176, 21178,
        21180, 21183, 21184, 21186, 21187, 21188, 21189, 21192, 21197, 21198,
        21202, 21203, 21204, 21205, 21210, 21211, 21213, 21214, 21215, 21217,
        21218, 21219, 21220, 21223, 21226, 21228, 21233, 21235, 21240, 21241,
        21243, 21246, 21247, 21249, 21251, 21254, 21257, 21258, 21263, 21265,
        21268, 21269, 21271, 21274, 21275, 21276, 21280, 21285, 21286, 21288,
        21289, 21291, 21294, 21300, 21301, 21306, 21310, 21320, 21321, 21324,
        21328, 21330, 21332, 21333, 21335, 21337, 21342, 21343, 21344, 21345,
        21346, 21347, 21351, 21353, 21354, 21357, 21358, 21362, 21363, 21364,
        21366, 21368, 21369, 21370, 21375, 21376, 21377, 21378, 21380, 21381,
        21382, 21384, 21385, 21386, 21390, 21392, 21393, 21397, 21401, 21407,
        21408, 21409, 21410, 21412, 21414, 21415, 21421, 21422, 21424, 21425,
        21426, 21427, 21431, 21434, 21435, 21440, 21442, 21444, 21446, 21449,
        21451, 21453, 21454, 21455, 21456, 21457, 21464, 21466, 21467, 21469,
        21471, 21473, 21474, 21475, 21476, 21480, 21481, 21483, 21484, 21492,
        21495, 21497, 21499, 21500, 21503, 21510, 21512, 21514, 21518, 21519,
        21520, 21522, 21524, 21526, 21529, 21530, 21531, 21533, 21534, 21537,
        21544, 21545, 21547, 21549, 21551, 21553, 21554, 21557, 21559, 21561,
        21564, 21568, 21569, 21571, 21572, 21575, 21576, 21577, 21582, 21583,
        21585, 21588, 21589, 21590, 21592, 21593, 21597, 21600, 21601, 21602,
        21603, 21605, 21612, 21614, 21615, 21616, 21617, 21618, 21620, 21621,
        21622, 21624, 21625, 21626, 21630, 21631, 21632, 21633, 21635, 21636,
        21637, 21640, 21642, 21645, 21650, 21651, 21653, 21654, 21656, 21659,
        21661, 21664, 21665, 21670, 21677, 21679, 21681, 21683, 21686, 21687,
        21691, 21696, 21697, 21698, 21703, 21705, 21706, 21709, 21710, 21725,
        21752, 22312, 22404, 22775, 22927, 23013, 23253, 23442, 23529, 23561,
        23569, 23582, 24104, 24344, 24851, 25209, 25477, 25592, 25645, 25797,
        25963, 26132, 26337, 26519, 26765, 26958, 26977, 27175, 27202, 27411,
        27487, 27843, 27881, 27943, 28091, 28230, 28239, 28402, 28416, 28715,
        29167, 29517, 29688, 29805, 29970, 30078, 30080, 30333, 30463, 30577,
        30768, 30917, 31099, 31213, 31246, 31353, 31488, 31530, 31563, 31604,
        31624, 31651, 31879, 32254, 32419, 32980, 32982, 33184, 33412],
       device='cuda:0')
length of data loader train is: 2757
num_training_steps_per_epoch is: 2757
Change step level LR scheduler!
Set warmup steps = 13785
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:23:06  loss: 4.9922 (4.9922)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.0963  data: 4.7844  max mem: 15572
Val:  [ 10/272]  eta: 0:03:07  loss: 5.1506 (5.0917)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.7139  data: 0.4909  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 5.1836 (5.0610)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3132  data: 0.0941  max mem: 15572
Val:  [ 30/272]  eta: 0:01:52  loss: 5.1020 (5.0353)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3268  data: 0.1156  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 4.8739 (4.9999)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3085  data: 0.1060  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 4.8283 (5.0215)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3002  data: 0.1007  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 4.8307 (5.0150)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2871  data: 0.0829  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 4.8320 (4.9990)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.3350  data: 0.1215  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 4.9648 (4.9579)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.1221)  time: 0.2895  data: 0.0924  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 5.0204 (4.9828)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.1197)  time: 0.2592  data: 0.0621  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 5.1758 (5.0141)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.3157)  time: 0.3595  data: 0.1490  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 5.2500 (5.0352)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.6567)  time: 0.3544  data: 0.1481  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 5.1777 (5.0578)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.1065)  time: 0.3057  data: 0.1122  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 5.1120 (5.0564)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.5912)  time: 0.2964  data: 0.1002  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 4.8459 (5.0422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.0623)  time: 0.3044  data: 0.1017  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.8459 (5.0327)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.4621)  time: 0.3429  data: 0.1345  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.6328 (5.0026)  acc1: 0.0000 (1.8634)  acc5: 0.0000 (9.7999)  time: 0.3829  data: 0.1709  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.6250 (5.0105)  acc1: 0.0000 (1.9493)  acc5: 0.0000 (9.4217)  time: 0.3380  data: 0.1397  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 5.1155 (5.0132)  acc1: 0.0000 (1.8416)  acc5: 0.0000 (8.9012)  time: 0.2909  data: 0.1055  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 5.1745 (5.0205)  acc1: 0.0000 (1.7452)  acc5: 0.0000 (8.4351)  time: 0.3182  data: 0.1340  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 5.0760 (5.0293)  acc1: 0.0000 (1.6584)  acc5: 0.0000 (8.0155)  time: 0.3281  data: 0.1347  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0757 (5.0345)  acc1: 0.0000 (1.5798)  acc5: 0.0000 (7.6356)  time: 0.3139  data: 0.1196  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.8477 (5.0207)  acc1: 0.0000 (1.5083)  acc5: 0.0000 (7.2901)  time: 0.3253  data: 0.1255  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7385 (5.0101)  acc1: 0.0000 (1.4430)  acc5: 0.0000 (6.9745)  time: 0.3421  data: 0.1407  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.7630 (5.0087)  acc1: 0.0000 (1.3831)  acc5: 0.0000 (6.6851)  time: 0.3820  data: 0.1947  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2500 (5.0253)  acc1: 0.0000 (1.3280)  acc5: 0.0000 (6.4188)  time: 0.3137  data: 0.1224  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1847 (5.0217)  acc1: 0.0000 (1.2771)  acc5: 0.0000 (6.1728)  time: 0.2872  data: 0.0950  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0894 (5.0235)  acc1: 0.0000 (1.2300)  acc5: 0.0000 (5.9451)  time: 0.2593  data: 0.0947  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0948 (5.0255)  acc1: 0.0000 (1.2288)  acc5: 0.0000 (5.9390)  time: 0.2544  data: 0.0947  max mem: 15572
Val: Total time: 0:01:30 (0.3323 s / it)
* Acc@1 1.229 Acc@5 5.939 loss 5.025
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-12 21:51:10,125] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-12 21:51:10,129] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 21:51:10,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 21:51:13,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 21:51:13,017] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/2757]  eta: 5:34:48  lr: 0.000009  min_lr: 0.000000  loss: 4.9763 (4.9763)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2863  data: 6.7103  max mem: 15572
Epoch: [1]  [  10/2757]  eta: 0:55:39  lr: 0.000009  min_lr: 0.000000  loss: 5.0577 (5.0558)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2155  data: 0.7783  max mem: 15572
Epoch: [1]  [  20/2757]  eta: 0:41:11  lr: 0.000009  min_lr: 0.000000  loss: 5.0166 (5.0293)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1566  max mem: 15572
Epoch: [1]  [  30/2757]  eta: 0:34:31  lr: 0.000009  min_lr: 0.000000  loss: 4.9697 (5.0131)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0809  max mem: 15572
Epoch: [1]  [  40/2757]  eta: 0:33:21  lr: 0.000010  min_lr: 0.000000  loss: 4.9548 (5.0082)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1147  max mem: 15572
Epoch: [1]  [  50/2757]  eta: 0:31:45  lr: 0.000010  min_lr: 0.000000  loss: 4.9907 (5.0082)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1574  max mem: 15572
Epoch: [1]  [  60/2757]  eta: 0:30:53  lr: 0.000010  min_lr: 0.000000  loss: 4.9824 (5.0017)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1079  max mem: 15572
[2025-01-12 21:52:01,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:52:01,123] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2757]  eta: 0:30:19  lr: 0.000010  min_lr: 0.000000  loss: 4.9608 (4.9969)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1159  max mem: 15572
Epoch: [1]  [  80/2757]  eta: 0:29:48  lr: 0.000010  min_lr: 0.000000  loss: 4.9980 (4.9982)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1282  max mem: 15572
[2025-01-12 21:52:13,425] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2899
[2025-01-12 21:52:13,426] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:52:13,426] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [  90/2757]  eta: 0:29:29  lr: 0.000010  min_lr: 0.000000  loss: 5.0413 (4.9999)  loss_scale: 262144.0000 (159879.0330)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1404  max mem: 15572
Epoch: [1]  [ 100/2757]  eta: 0:29:15  lr: 0.000010  min_lr: 0.000000  loss: 4.9944 (4.9997)  loss_scale: 131072.0000 (157026.8515)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1464  max mem: 15572
Epoch: [1]  [ 110/2757]  eta: 0:28:29  lr: 0.000010  min_lr: 0.000000  loss: 4.9666 (4.9985)  loss_scale: 131072.0000 (154688.5766)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0784  max mem: 15572
Epoch: [1]  [ 120/2757]  eta: 0:27:58  lr: 0.000010  min_lr: 0.000000  loss: 4.9959 (4.9989)  loss_scale: 131072.0000 (152736.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0561  max mem: 15572
Epoch: [1]  [ 130/2757]  eta: 0:28:04  lr: 0.000010  min_lr: 0.000000  loss: 5.0436 (5.0021)  loss_scale: 131072.0000 (151082.9924)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1464  max mem: 15572
Epoch: [1]  [ 140/2757]  eta: 0:27:59  lr: 0.000010  min_lr: 0.000000  loss: 5.0551 (5.0022)  loss_scale: 131072.0000 (149663.7730)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.1970  max mem: 15572
Epoch: [1]  [ 150/2757]  eta: 0:27:12  lr: 0.000010  min_lr: 0.000000  loss: 4.9553 (4.9995)  loss_scale: 131072.0000 (148432.5298)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0953  max mem: 15572
Epoch: [1]  [ 160/2757]  eta: 0:26:46  lr: 0.000010  min_lr: 0.000000  loss: 4.9459 (4.9957)  loss_scale: 131072.0000 (147354.2360)  weight_decay: 0.0500 (0.0500)  time: 0.4558  data: 0.0007  max mem: 15572
Epoch: [1]  [ 170/2757]  eta: 0:26:31  lr: 0.000010  min_lr: 0.000000  loss: 4.9748 (4.9981)  loss_scale: 131072.0000 (146402.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0355  max mem: 15572
Epoch: [1]  [ 180/2757]  eta: 0:26:40  lr: 0.000010  min_lr: 0.000000  loss: 5.0344 (5.0019)  loss_scale: 131072.0000 (145555.0939)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1524  max mem: 15572
[2025-01-12 21:53:12,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=10, lr=[9.710032249894423e-08, 9.710032249894423e-08, 1.387147464270632e-07, 1.387147464270632e-07, 1.9816392346723315e-07, 1.9816392346723315e-07, 2.8309131923890453e-07, 2.8309131923890453e-07, 4.0441617034129223e-07, 4.0441617034129223e-07, 5.77737386201846e-07, 5.77737386201846e-07, 8.253391231454944e-07, 8.253391231454944e-07, 1.1790558902078493e-06, 1.1790558902078493e-06, 1.6843655574397846e-06, 1.6843655574397846e-06, 2.4062365106282643e-06, 2.4062365106282643e-06, 3.4374807294689483e-06, 3.4374807294689483e-06, 4.9106867563842125e-06, 4.9106867563842125e-06, 7.01526679483459e-06, 7.01526679483459e-06, 1.0021809706906558e-05, 1.0021809706906558e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 21:53:12,871] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=27.538705185205252, CurrSamplesPerSec=22.79683400254095, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2757]  eta: 0:26:49  lr: 0.000010  min_lr: 0.000000  loss: 5.0246 (5.0029)  loss_scale: 131072.0000 (144796.8168)  weight_decay: 0.0500 (0.0500)  time: 0.7295  data: 0.2426  max mem: 15572
Epoch: [1]  [ 200/2757]  eta: 0:26:58  lr: 0.000010  min_lr: 0.000000  loss: 4.9440 (5.0006)  loss_scale: 131072.0000 (144113.9900)  weight_decay: 0.0500 (0.0500)  time: 0.7429  data: 0.2621  max mem: 15572
Epoch: [1]  [ 210/2757]  eta: 0:26:52  lr: 0.000010  min_lr: 0.000000  loss: 4.9440 (5.0007)  loss_scale: 131072.0000 (143495.8863)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.2142  max mem: 15572
[2025-01-12 21:53:33,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:53:33,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 220/2757]  eta: 0:26:57  lr: 0.000010  min_lr: 0.000000  loss: 5.0172 (5.0034)  loss_scale: 131072.0000 (144119.8914)  weight_decay: 0.0500 (0.0500)  time: 0.6850  data: 0.2083  max mem: 15572
[2025-01-12 21:53:39,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3035
[2025-01-12 21:53:39,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:53:39,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 230/2757]  eta: 0:26:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0131 (5.0018)  loss_scale: 131072.0000 (146392.1039)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.2450  max mem: 15572
Epoch: [1]  [ 240/2757]  eta: 0:26:52  lr: 0.000010  min_lr: 0.000000  loss: 4.9922 (5.0026)  loss_scale: 131072.0000 (145756.4149)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.2117  max mem: 15572
Epoch: [1]  [ 250/2757]  eta: 0:26:38  lr: 0.000010  min_lr: 0.000000  loss: 4.9841 (5.0015)  loss_scale: 131072.0000 (145171.3785)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1349  max mem: 15572
Epoch: [1]  [ 260/2757]  eta: 0:26:35  lr: 0.000010  min_lr: 0.000000  loss: 4.9670 (5.0004)  loss_scale: 131072.0000 (144631.1724)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1522  max mem: 15572
Epoch: [1]  [ 270/2757]  eta: 0:26:35  lr: 0.000010  min_lr: 0.000000  loss: 4.9598 (4.9994)  loss_scale: 131072.0000 (144130.8339)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.2301  max mem: 15572
Epoch: [1]  [ 280/2757]  eta: 0:26:43  lr: 0.000010  min_lr: 0.000000  loss: 4.9551 (4.9984)  loss_scale: 131072.0000 (143666.1068)  weight_decay: 0.0500 (0.0500)  time: 0.7548  data: 0.2811  max mem: 15572
Epoch: [1]  [ 290/2757]  eta: 0:26:15  lr: 0.000010  min_lr: 0.000000  loss: 4.9052 (4.9956)  loss_scale: 131072.0000 (143233.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1662  max mem: 15572
Epoch: [1]  [ 300/2757]  eta: 0:25:51  lr: 0.000010  min_lr: 0.000000  loss: 4.9384 (4.9947)  loss_scale: 131072.0000 (142829.2890)  weight_decay: 0.0500 (0.0500)  time: 0.4081  data: 0.0005  max mem: 15572
Epoch: [1]  [ 310/2757]  eta: 0:25:31  lr: 0.000010  min_lr: 0.000000  loss: 4.9577 (4.9947)  loss_scale: 131072.0000 (142451.2412)  weight_decay: 0.0500 (0.0500)  time: 0.4385  data: 0.0009  max mem: 15572
Epoch: [1]  [ 320/2757]  eta: 0:25:09  lr: 0.000010  min_lr: 0.000000  loss: 4.9620 (4.9942)  loss_scale: 131072.0000 (142096.7477)  weight_decay: 0.0500 (0.0500)  time: 0.4386  data: 0.0008  max mem: 15572
Epoch: [1]  [ 330/2757]  eta: 0:24:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9801 (4.9943)  loss_scale: 131072.0000 (141763.6737)  weight_decay: 0.0500 (0.0500)  time: 0.4257  data: 0.0007  max mem: 15572
Epoch: [1]  [ 340/2757]  eta: 0:24:38  lr: 0.000011  min_lr: 0.000000  loss: 4.9804 (4.9940)  loss_scale: 131072.0000 (141450.1349)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0446  max mem: 15572
Epoch: [1]  [ 350/2757]  eta: 0:24:27  lr: 0.000011  min_lr: 0.000000  loss: 4.9733 (4.9940)  loss_scale: 131072.0000 (141154.4615)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0745  max mem: 15572
[2025-01-12 21:54:49,735] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:54:49,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 360/2757]  eta: 0:24:20  lr: 0.000011  min_lr: 0.000000  loss: 5.0030 (4.9950)  loss_scale: 131072.0000 (143053.6510)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1013  max mem: 15572
Epoch: [1]  [ 370/2757]  eta: 0:24:14  lr: 0.000011  min_lr: 0.000000  loss: 5.0543 (4.9975)  loss_scale: 262144.0000 (146263.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1717  max mem: 15572
Epoch: [1]  [ 380/2757]  eta: 0:24:15  lr: 0.000011  min_lr: 0.000000  loss: 5.0818 (4.9976)  loss_scale: 262144.0000 (149305.1129)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.2289  max mem: 15572
Epoch: [1]  [ 390/2757]  eta: 0:24:10  lr: 0.000011  min_lr: 0.000000  loss: 5.0076 (4.9978)  loss_scale: 262144.0000 (152191.0179)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.2102  max mem: 15572
Epoch: [1]  [ 400/2757]  eta: 0:24:01  lr: 0.000011  min_lr: 0.000000  loss: 4.9987 (4.9975)  loss_scale: 262144.0000 (154932.9875)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1357  max mem: 15572
Epoch: [1]  [ 410/2757]  eta: 0:23:47  lr: 0.000011  min_lr: 0.000000  loss: 4.9970 (4.9975)  loss_scale: 262144.0000 (157541.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0755  max mem: 15572
Epoch: [1]  [ 420/2757]  eta: 0:23:34  lr: 0.000011  min_lr: 0.000000  loss: 5.0184 (4.9981)  loss_scale: 262144.0000 (160026.1473)  weight_decay: 0.0500 (0.0500)  time: 0.4751  data: 0.0513  max mem: 15572
Epoch: [1]  [ 430/2757]  eta: 0:23:31  lr: 0.000011  min_lr: 0.000000  loss: 5.0313 (4.9982)  loss_scale: 262144.0000 (162395.4710)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1498  max mem: 15572
Epoch: [1]  [ 440/2757]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 5.0118 (4.9985)  loss_scale: 262144.0000 (164657.3424)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1497  max mem: 15572
Epoch: [1]  [ 450/2757]  eta: 0:23:14  lr: 0.000011  min_lr: 0.000000  loss: 5.0118 (4.9987)  loss_scale: 262144.0000 (166818.9091)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1122  max mem: 15572
Epoch: [1]  [ 460/2757]  eta: 0:23:09  lr: 0.000011  min_lr: 0.000000  loss: 4.9940 (4.9982)  loss_scale: 262144.0000 (168886.6985)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1719  max mem: 15572
Epoch: [1]  [ 470/2757]  eta: 0:23:03  lr: 0.000011  min_lr: 0.000000  loss: 4.9550 (4.9977)  loss_scale: 262144.0000 (170866.6837)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1764  max mem: 15572
[2025-01-12 21:56:04,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3288
[2025-01-12 21:56:04,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:56:04,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 480/2757]  eta: 0:22:59  lr: 0.000011  min_lr: 0.000000  loss: 4.9570 (4.9983)  loss_scale: 262144.0000 (172219.3430)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1800  max mem: 15572
Epoch: [1]  [ 490/2757]  eta: 0:22:50  lr: 0.000011  min_lr: 0.000000  loss: 4.9606 (4.9976)  loss_scale: 131072.0000 (171381.3116)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1488  max mem: 15572
Epoch: [1]  [ 500/2757]  eta: 0:22:43  lr: 0.000011  min_lr: 0.000000  loss: 4.9678 (4.9984)  loss_scale: 131072.0000 (170576.7345)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1273  max mem: 15572
Epoch: [1]  [ 510/2757]  eta: 0:22:37  lr: 0.000011  min_lr: 0.000000  loss: 5.0162 (4.9989)  loss_scale: 131072.0000 (169803.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1514  max mem: 15572
Epoch: [1]  [ 520/2757]  eta: 0:22:29  lr: 0.000011  min_lr: 0.000000  loss: 5.0243 (5.0001)  loss_scale: 131072.0000 (169060.2380)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1240  max mem: 15572
Epoch: [1]  [ 530/2757]  eta: 0:22:22  lr: 0.000011  min_lr: 0.000000  loss: 5.0132 (4.9992)  loss_scale: 131072.0000 (168344.8286)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0956  max mem: 15572
Epoch: [1]  [ 540/2757]  eta: 0:22:17  lr: 0.000011  min_lr: 0.000000  loss: 5.0117 (4.9998)  loss_scale: 131072.0000 (167655.8669)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1433  max mem: 15572
Epoch: [1]  [ 550/2757]  eta: 0:22:11  lr: 0.000011  min_lr: 0.000000  loss: 5.0685 (5.0018)  loss_scale: 131072.0000 (166991.9129)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1544  max mem: 15572
Epoch: [1]  [ 560/2757]  eta: 0:22:03  lr: 0.000011  min_lr: 0.000000  loss: 5.0619 (5.0022)  loss_scale: 131072.0000 (166351.6292)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1238  max mem: 15572
Epoch: [1]  [ 570/2757]  eta: 0:21:56  lr: 0.000011  min_lr: 0.000000  loss: 5.0101 (5.0029)  loss_scale: 131072.0000 (165733.7723)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1170  max mem: 15572
Epoch: [1]  [ 580/2757]  eta: 0:21:52  lr: 0.000011  min_lr: 0.000000  loss: 5.0466 (5.0043)  loss_scale: 131072.0000 (165137.1842)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1604  max mem: 15572
Epoch: [1]  [ 590/2757]  eta: 0:21:46  lr: 0.000011  min_lr: 0.000000  loss: 5.0048 (5.0045)  loss_scale: 131072.0000 (164560.7851)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1776  max mem: 15572
Epoch: [1]  [ 600/2757]  eta: 0:21:40  lr: 0.000011  min_lr: 0.000000  loss: 4.9677 (5.0036)  loss_scale: 131072.0000 (164003.5674)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1518  max mem: 15572
[2025-01-12 21:57:20,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:57:20,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 610/2757]  eta: 0:21:32  lr: 0.000011  min_lr: 0.000000  loss: 4.9647 (5.0039)  loss_scale: 131072.0000 (164108.1506)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1257  max mem: 15572
Epoch: [1]  [ 620/2757]  eta: 0:21:22  lr: 0.000011  min_lr: 0.000000  loss: 4.9886 (5.0033)  loss_scale: 262144.0000 (165686.8277)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0762  max mem: 15572
Epoch: [1]  [ 630/2757]  eta: 0:21:14  lr: 0.000012  min_lr: 0.000000  loss: 5.0010 (5.0039)  loss_scale: 262144.0000 (167215.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0764  max mem: 15572
Epoch: [1]  [ 640/2757]  eta: 0:21:07  lr: 0.000012  min_lr: 0.000000  loss: 5.0269 (5.0047)  loss_scale: 262144.0000 (168696.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0876  max mem: 15572
Epoch: [1]  [ 650/2757]  eta: 0:21:01  lr: 0.000012  min_lr: 0.000000  loss: 5.0416 (5.0055)  loss_scale: 262144.0000 (170131.8587)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1040  max mem: 15572
Epoch: [1]  [ 660/2757]  eta: 0:20:55  lr: 0.000012  min_lr: 0.000000  loss: 5.0201 (5.0057)  loss_scale: 262144.0000 (171523.8729)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1417  max mem: 15572
Epoch: [1]  [ 670/2757]  eta: 0:20:54  lr: 0.000012  min_lr: 0.000000  loss: 4.9801 (5.0053)  loss_scale: 262144.0000 (172874.3964)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.2067  max mem: 15572
Epoch: [1]  [ 680/2757]  eta: 0:20:46  lr: 0.000012  min_lr: 0.000000  loss: 4.9785 (5.0060)  loss_scale: 262144.0000 (174185.2570)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1703  max mem: 15572
Epoch: [1]  [ 690/2757]  eta: 0:20:39  lr: 0.000012  min_lr: 0.000000  loss: 4.9593 (5.0052)  loss_scale: 262144.0000 (175458.1766)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1041  max mem: 15572
Epoch: [1]  [ 700/2757]  eta: 0:20:34  lr: 0.000012  min_lr: 0.000000  loss: 4.9704 (5.0054)  loss_scale: 262144.0000 (176694.7789)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1542  max mem: 15572
Epoch: [1]  [ 710/2757]  eta: 0:20:30  lr: 0.000012  min_lr: 0.000000  loss: 4.9959 (5.0054)  loss_scale: 262144.0000 (177896.5963)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.1967  max mem: 15572
Epoch: [1]  [ 720/2757]  eta: 0:20:25  lr: 0.000012  min_lr: 0.000000  loss: 4.9787 (5.0053)  loss_scale: 262144.0000 (179065.0763)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1868  max mem: 15572
Epoch: [1]  [ 730/2757]  eta: 0:20:16  lr: 0.000012  min_lr: 0.000000  loss: 5.0178 (5.0056)  loss_scale: 262144.0000 (180201.5869)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1141  max mem: 15572
[2025-01-12 21:58:36,778] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 21:58:36,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
[2025-01-12 21:58:37,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3546
[2025-01-12 21:58:37,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
[2025-01-12 21:58:37,302] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
Epoch: [1]  [ 740/2757]  eta: 0:20:11  lr: 0.000012  min_lr: 0.000000  loss: 5.0212 (5.0058)  loss_scale: 262144.0000 (181661.1930)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1351  max mem: 15572
Epoch: [1]  [ 750/2757]  eta: 0:20:05  lr: 0.000012  min_lr: 0.000000  loss: 4.9520 (5.0049)  loss_scale: 262144.0000 (182732.8682)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1659  max mem: 15572
[2025-01-12 21:58:50,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3569
[2025-01-12 21:58:50,528] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 21:58:50,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 760/2757]  eta: 0:19:59  lr: 0.000012  min_lr: 0.000000  loss: 5.0204 (5.0053)  loss_scale: 262144.0000 (183604.1419)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1506  max mem: 15572
Epoch: [1]  [ 770/2757]  eta: 0:19:52  lr: 0.000012  min_lr: 0.000000  loss: 5.0501 (5.0059)  loss_scale: 131072.0000 (182922.7912)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1363  max mem: 15572
Epoch: [1]  [ 780/2757]  eta: 0:19:46  lr: 0.000012  min_lr: 0.000000  loss: 5.0661 (5.0066)  loss_scale: 131072.0000 (182258.8886)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1342  max mem: 15572
Epoch: [1]  [ 790/2757]  eta: 0:19:41  lr: 0.000012  min_lr: 0.000000  loss: 4.9994 (5.0062)  loss_scale: 131072.0000 (181611.7724)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1836  max mem: 15572
Epoch: [1]  [ 800/2757]  eta: 0:19:35  lr: 0.000012  min_lr: 0.000000  loss: 4.9756 (5.0065)  loss_scale: 131072.0000 (180980.8140)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1636  max mem: 15572
Epoch: [1]  [ 810/2757]  eta: 0:19:30  lr: 0.000012  min_lr: 0.000000  loss: 5.0220 (5.0071)  loss_scale: 131072.0000 (180365.4155)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1491  max mem: 15572
Epoch: [1]  [ 820/2757]  eta: 0:19:25  lr: 0.000012  min_lr: 0.000000  loss: 5.0601 (5.0082)  loss_scale: 131072.0000 (179765.0085)  weight_decay: 0.0500 (0.0500)  time: 0.6539  data: 0.1974  max mem: 15572
Epoch: [1]  [ 830/2757]  eta: 0:19:19  lr: 0.000012  min_lr: 0.000000  loss: 5.0660 (5.0088)  loss_scale: 131072.0000 (179179.0517)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1760  max mem: 15572
Epoch: [1]  [ 840/2757]  eta: 0:19:13  lr: 0.000012  min_lr: 0.000000  loss: 5.0486 (5.0089)  loss_scale: 131072.0000 (178607.0297)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1453  max mem: 15572
Epoch: [1]  [ 850/2757]  eta: 0:19:10  lr: 0.000012  min_lr: 0.000000  loss: 5.0392 (5.0090)  loss_scale: 131072.0000 (178048.4512)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.2310  max mem: 15572
Epoch: [1]  [ 860/2757]  eta: 0:19:03  lr: 0.000012  min_lr: 0.000000  loss: 5.0125 (5.0089)  loss_scale: 131072.0000 (177502.8479)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.2066  max mem: 15572
Epoch: [1]  [ 870/2757]  eta: 0:18:57  lr: 0.000012  min_lr: 0.000000  loss: 5.0125 (5.0087)  loss_scale: 131072.0000 (176969.7727)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1303  max mem: 15572
Epoch: [1]  [ 880/2757]  eta: 0:18:48  lr: 0.000012  min_lr: 0.000000  loss: 5.0051 (5.0085)  loss_scale: 131072.0000 (176448.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0884  max mem: 15572
[2025-01-12 22:00:08,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:00:08,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 890/2757]  eta: 0:18:41  lr: 0.000012  min_lr: 0.000000  loss: 5.0051 (5.0095)  loss_scale: 131072.0000 (176233.7329)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0767  max mem: 15572
[2025-01-12 22:00:11,686] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3705
[2025-01-12 22:00:11,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 22:00:11,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 900/2757]  eta: 0:18:35  lr: 0.000012  min_lr: 0.000000  loss: 4.9633 (5.0088)  loss_scale: 131072.0000 (176459.8624)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1112  max mem: 15572
Epoch: [1]  [ 910/2757]  eta: 0:18:29  lr: 0.000012  min_lr: 0.000000  loss: 4.9755 (5.0085)  loss_scale: 131072.0000 (175961.6422)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1306  max mem: 15572
Epoch: [1]  [ 920/2757]  eta: 0:18:25  lr: 0.000013  min_lr: 0.000000  loss: 4.9838 (5.0086)  loss_scale: 131072.0000 (175474.2410)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1919  max mem: 15572
Epoch: [1]  [ 930/2757]  eta: 0:18:20  lr: 0.000013  min_lr: 0.000000  loss: 4.9838 (5.0092)  loss_scale: 131072.0000 (174997.3104)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.2348  max mem: 15572
Epoch: [1]  [ 940/2757]  eta: 0:18:13  lr: 0.000013  min_lr: 0.000000  loss: 5.0389 (5.0094)  loss_scale: 131072.0000 (174530.5165)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1823  max mem: 15572
Epoch: [1]  [ 950/2757]  eta: 0:18:09  lr: 0.000013  min_lr: 0.000000  loss: 5.0513 (5.0097)  loss_scale: 131072.0000 (174073.5394)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1703  max mem: 15572
Epoch: [1]  [ 960/2757]  eta: 0:18:00  lr: 0.000013  min_lr: 0.000000  loss: 5.0497 (5.0096)  loss_scale: 131072.0000 (173626.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1109  max mem: 15572
Epoch: [1]  [ 970/2757]  eta: 0:17:53  lr: 0.000013  min_lr: 0.000000  loss: 4.9743 (5.0093)  loss_scale: 131072.0000 (173187.8229)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0582  max mem: 15572
Epoch: [1]  [ 980/2757]  eta: 0:17:47  lr: 0.000013  min_lr: 0.000000  loss: 4.9902 (5.0093)  loss_scale: 131072.0000 (172758.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1467  max mem: 15572
Epoch: [1]  [ 990/2757]  eta: 0:17:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0689 (5.0102)  loss_scale: 131072.0000 (172337.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0892  max mem: 15572
Epoch: [1]  [1000/2757]  eta: 0:17:32  lr: 0.000013  min_lr: 0.000000  loss: 5.0443 (5.0103)  loss_scale: 131072.0000 (171925.6104)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0418  max mem: 15572
Epoch: [1]  [1010/2757]  eta: 0:17:25  lr: 0.000013  min_lr: 0.000000  loss: 4.9842 (5.0099)  loss_scale: 131072.0000 (171521.5193)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0546  max mem: 15572
Epoch: [1]  [1020/2757]  eta: 0:17:19  lr: 0.000013  min_lr: 0.000000  loss: 4.9879 (5.0103)  loss_scale: 131072.0000 (171125.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0952  max mem: 15572
[2025-01-12 22:01:27,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:01:27,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [1030/2757]  eta: 0:17:14  lr: 0.000013  min_lr: 0.000000  loss: 4.9723 (5.0095)  loss_scale: 131072.0000 (171499.6392)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1545  max mem: 15572
Epoch: [1]  [1040/2757]  eta: 0:17:06  lr: 0.000013  min_lr: 0.000000  loss: 4.9317 (5.0092)  loss_scale: 262144.0000 (172370.3823)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1068  max mem: 15572
Epoch: [1]  [1050/2757]  eta: 0:16:58  lr: 0.000013  min_lr: 0.000000  loss: 4.9422 (5.0088)  loss_scale: 262144.0000 (173224.5557)  weight_decay: 0.0500 (0.0500)  time: 0.4904  data: 0.0349  max mem: 15572
Epoch: [1]  [1060/2757]  eta: 0:16:53  lr: 0.000013  min_lr: 0.000000  loss: 4.9884 (5.0093)  loss_scale: 262144.0000 (174062.6277)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1226  max mem: 15572
Epoch: [1]  [1070/2757]  eta: 0:16:48  lr: 0.000013  min_lr: 0.000000  loss: 5.0089 (5.0092)  loss_scale: 262144.0000 (174885.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.2161  max mem: 15572
Epoch: [1]  [1080/2757]  eta: 0:16:42  lr: 0.000013  min_lr: 0.000000  loss: 4.9937 (5.0091)  loss_scale: 262144.0000 (175692.2553)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1715  max mem: 15572
Epoch: [1]  [1090/2757]  eta: 0:16:37  lr: 0.000013  min_lr: 0.000000  loss: 5.0148 (5.0095)  loss_scale: 262144.0000 (176484.6636)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1761  max mem: 15572
[2025-01-12 22:02:06,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3900
[2025-01-12 22:02:06,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 22:02:06,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1100/2757]  eta: 0:16:29  lr: 0.000013  min_lr: 0.000000  loss: 4.9750 (5.0089)  loss_scale: 131072.0000 (176072.1962)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1147  max mem: 15572
Epoch: [1]  [1110/2757]  eta: 0:16:24  lr: 0.000013  min_lr: 0.000000  loss: 5.0049 (5.0094)  loss_scale: 131072.0000 (175667.1539)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1183  max mem: 15572
Epoch: [1]  [1120/2757]  eta: 0:16:17  lr: 0.000013  min_lr: 0.000000  loss: 5.0241 (5.0097)  loss_scale: 131072.0000 (175269.3381)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1324  max mem: 15572
Epoch: [1]  [1130/2757]  eta: 0:16:11  lr: 0.000013  min_lr: 0.000000  loss: 5.0618 (5.0099)  loss_scale: 131072.0000 (174878.5570)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0903  max mem: 15572
Epoch: [1]  [1140/2757]  eta: 0:16:05  lr: 0.000013  min_lr: 0.000000  loss: 4.9791 (5.0094)  loss_scale: 131072.0000 (174494.6258)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1234  max mem: 15572
Epoch: [1]  [1150/2757]  eta: 0:15:58  lr: 0.000013  min_lr: 0.000000  loss: 4.9224 (5.0089)  loss_scale: 131072.0000 (174117.3658)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0913  max mem: 15572
Epoch: [1]  [1160/2757]  eta: 0:15:52  lr: 0.000013  min_lr: 0.000000  loss: 4.9703 (5.0094)  loss_scale: 131072.0000 (173746.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1059  max mem: 15572
Epoch: [1]  [1170/2757]  eta: 0:15:46  lr: 0.000013  min_lr: 0.000000  loss: 5.0081 (5.0090)  loss_scale: 131072.0000 (173382.1759)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1533  max mem: 15572
Epoch: [1]  [1180/2757]  eta: 0:15:39  lr: 0.000013  min_lr: 0.000000  loss: 4.9821 (5.0093)  loss_scale: 131072.0000 (173023.9187)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1046  max mem: 15572
[2025-01-12 22:03:02,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=16, lr=[1.300491933842324e-07, 1.300491933842324e-07, 1.8578456197747487e-07, 1.8578456197747487e-07, 2.654065171106784e-07, 2.654065171106784e-07, 3.791521673009692e-07, 3.791521673009692e-07, 5.416459532870989e-07, 5.416459532870989e-07, 7.737799332672842e-07, 7.737799332672842e-07, 1.105399904667549e-06, 1.105399904667549e-06, 1.5791427209536414e-06, 1.5791427209536414e-06, 2.2559181727909163e-06, 2.2559181727909163e-06, 3.2227402468441663e-06, 3.2227402468441663e-06, 4.603914638348809e-06, 4.603914638348809e-06, 6.577020911926871e-06, 6.577020911926871e-06, 9.39574415989553e-06, 9.39574415989553e-06, 1.3422491656993615e-05, 1.3422491656993615e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:03:02,865] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=27.664847000557476, CurrSamplesPerSec=28.74663410382604, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2757]  eta: 0:15:33  lr: 0.000013  min_lr: 0.000000  loss: 4.8701 (5.0082)  loss_scale: 131072.0000 (172671.6776)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1069  max mem: 15572
Epoch: [1]  [1200/2757]  eta: 0:15:27  lr: 0.000013  min_lr: 0.000000  loss: 4.8433 (5.0076)  loss_scale: 131072.0000 (172325.3022)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1305  max mem: 15572
Epoch: [1]  [1210/2757]  eta: 0:15:20  lr: 0.000013  min_lr: 0.000000  loss: 4.9558 (5.0076)  loss_scale: 131072.0000 (171984.6474)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1099  max mem: 15572
[2025-01-12 22:03:20,734] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:03:20,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [1220/2757]  eta: 0:15:15  lr: 0.000014  min_lr: 0.000000  loss: 4.9553 (5.0072)  loss_scale: 131072.0000 (171756.9206)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1647  max mem: 15572
[2025-01-12 22:03:21,666] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4031
[2025-01-12 22:03:21,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-12 22:03:21,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1230/2757]  eta: 0:15:09  lr: 0.000014  min_lr: 0.000000  loss: 4.9553 (5.0071)  loss_scale: 131072.0000 (171532.8936)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1619  max mem: 15572
Epoch: [1]  [1240/2757]  eta: 0:15:03  lr: 0.000014  min_lr: 0.000000  loss: 5.0563 (5.0079)  loss_scale: 131072.0000 (171206.8590)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1212  max mem: 15572
Epoch: [1]  [1250/2757]  eta: 0:14:57  lr: 0.000014  min_lr: 0.000000  loss: 5.0079 (5.0074)  loss_scale: 131072.0000 (170886.0368)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1688  max mem: 15572
Epoch: [1]  [1260/2757]  eta: 0:14:51  lr: 0.000014  min_lr: 0.000000  loss: 5.0015 (5.0076)  loss_scale: 131072.0000 (170570.3029)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1620  max mem: 15572
Epoch: [1]  [1270/2757]  eta: 0:14:45  lr: 0.000014  min_lr: 0.000000  loss: 5.0387 (5.0079)  loss_scale: 131072.0000 (170259.5374)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1213  max mem: 15572
Epoch: [1]  [1280/2757]  eta: 0:14:40  lr: 0.000014  min_lr: 0.000000  loss: 5.0499 (5.0083)  loss_scale: 131072.0000 (169953.6237)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1548  max mem: 15572
Epoch: [1]  [1290/2757]  eta: 0:14:34  lr: 0.000014  min_lr: 0.000000  loss: 5.0499 (5.0085)  loss_scale: 131072.0000 (169652.4493)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1607  max mem: 15572
Epoch: [1]  [1300/2757]  eta: 0:14:28  lr: 0.000014  min_lr: 0.000000  loss: 4.9486 (5.0078)  loss_scale: 131072.0000 (169355.9047)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1645  max mem: 15572
[2025-01-12 22:04:09,910] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4111
[2025-01-12 22:04:09,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:04:09,911] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1310/2757]  eta: 0:14:21  lr: 0.000014  min_lr: 0.000000  loss: 4.9659 (5.0076)  loss_scale: 131072.0000 (168613.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1377  max mem: 15572
Epoch: [1]  [1320/2757]  eta: 0:14:16  lr: 0.000014  min_lr: 0.000000  loss: 4.9952 (5.0081)  loss_scale: 65536.0000 (167833.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1337  max mem: 15572
Epoch: [1]  [1330/2757]  eta: 0:14:11  lr: 0.000014  min_lr: 0.000000  loss: 4.9952 (5.0084)  loss_scale: 65536.0000 (167065.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.2108  max mem: 15572
Epoch: [1]  [1340/2757]  eta: 0:14:04  lr: 0.000014  min_lr: 0.000000  loss: 4.9915 (5.0080)  loss_scale: 65536.0000 (166307.9851)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1716  max mem: 15572
Epoch: [1]  [1350/2757]  eta: 0:13:58  lr: 0.000014  min_lr: 0.000000  loss: 4.9399 (5.0078)  loss_scale: 65536.0000 (165562.0785)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1104  max mem: 15572
Epoch: [1]  [1360/2757]  eta: 0:13:52  lr: 0.000014  min_lr: 0.000000  loss: 4.9395 (5.0076)  loss_scale: 65536.0000 (164827.1330)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1336  max mem: 15572
Epoch: [1]  [1370/2757]  eta: 0:13:46  lr: 0.000014  min_lr: 0.000000  loss: 4.9607 (5.0073)  loss_scale: 65536.0000 (164102.9088)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1304  max mem: 15572
Epoch: [1]  [1380/2757]  eta: 0:13:41  lr: 0.000014  min_lr: 0.000000  loss: 4.9585 (5.0068)  loss_scale: 65536.0000 (163389.1731)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1860  max mem: 15572
Epoch: [1]  [1390/2757]  eta: 0:13:34  lr: 0.000014  min_lr: 0.000000  loss: 4.9144 (5.0068)  loss_scale: 65536.0000 (162685.6995)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1608  max mem: 15572
Epoch: [1]  [1400/2757]  eta: 0:13:28  lr: 0.000014  min_lr: 0.000000  loss: 4.9506 (5.0065)  loss_scale: 65536.0000 (161992.2684)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1199  max mem: 15572
Epoch: [1]  [1410/2757]  eta: 0:13:23  lr: 0.000014  min_lr: 0.000000  loss: 4.9506 (5.0066)  loss_scale: 65536.0000 (161308.6662)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1675  max mem: 15572
Epoch: [1]  [1420/2757]  eta: 0:13:17  lr: 0.000014  min_lr: 0.000000  loss: 4.9600 (5.0068)  loss_scale: 65536.0000 (160634.6854)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1441  max mem: 15572
Epoch: [1]  [1430/2757]  eta: 0:13:11  lr: 0.000014  min_lr: 0.000000  loss: 5.0157 (5.0069)  loss_scale: 65536.0000 (159970.1244)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1351  max mem: 15572
[2025-01-12 22:05:27,501] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:05:27,502] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:05:28,016] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4241
[2025-01-12 22:05:28,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:05:28,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1440/2757]  eta: 0:13:04  lr: 0.000014  min_lr: 0.000000  loss: 4.9816 (5.0069)  loss_scale: 65536.0000 (159360.2665)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1000  max mem: 15572
Epoch: [1]  [1450/2757]  eta: 0:12:58  lr: 0.000014  min_lr: 0.000000  loss: 4.9816 (5.0069)  loss_scale: 65536.0000 (158713.6485)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1044  max mem: 15572
Epoch: [1]  [1460/2757]  eta: 0:12:52  lr: 0.000014  min_lr: 0.000000  loss: 4.9683 (5.0067)  loss_scale: 65536.0000 (158075.8823)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1128  max mem: 15572
Epoch: [1]  [1470/2757]  eta: 0:12:46  lr: 0.000014  min_lr: 0.000000  loss: 4.9806 (5.0069)  loss_scale: 65536.0000 (157446.7872)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1374  max mem: 15572
Epoch: [1]  [1480/2757]  eta: 0:12:40  lr: 0.000014  min_lr: 0.000000  loss: 4.9983 (5.0067)  loss_scale: 65536.0000 (156826.1877)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1250  max mem: 15572
Epoch: [1]  [1490/2757]  eta: 0:12:34  lr: 0.000014  min_lr: 0.000000  loss: 4.9844 (5.0066)  loss_scale: 65536.0000 (156213.9128)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1171  max mem: 15572
Epoch: [1]  [1500/2757]  eta: 0:12:28  lr: 0.000014  min_lr: 0.000000  loss: 4.9540 (5.0064)  loss_scale: 65536.0000 (155609.7961)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1352  max mem: 15572
Epoch: [1]  [1510/2757]  eta: 0:12:21  lr: 0.000015  min_lr: 0.000000  loss: 4.9479 (5.0062)  loss_scale: 65536.0000 (155013.6757)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0925  max mem: 15572
Epoch: [1]  [1520/2757]  eta: 0:12:16  lr: 0.000015  min_lr: 0.000000  loss: 4.9701 (5.0060)  loss_scale: 65536.0000 (154425.3938)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1225  max mem: 15572
Epoch: [1]  [1530/2757]  eta: 0:12:10  lr: 0.000015  min_lr: 0.000000  loss: 4.9777 (5.0061)  loss_scale: 65536.0000 (153844.7969)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1698  max mem: 15572
Epoch: [1]  [1540/2757]  eta: 0:12:04  lr: 0.000015  min_lr: 0.000000  loss: 4.9865 (5.0062)  loss_scale: 65536.0000 (153271.7352)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1731  max mem: 15572
Epoch: [1]  [1550/2757]  eta: 0:11:58  lr: 0.000015  min_lr: 0.000000  loss: 5.0310 (5.0063)  loss_scale: 65536.0000 (152706.0632)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1413  max mem: 15572
Epoch: [1]  [1560/2757]  eta: 0:11:52  lr: 0.000015  min_lr: 0.000000  loss: 5.0330 (5.0062)  loss_scale: 65536.0000 (152147.6387)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0899  max mem: 15572
[2025-01-12 22:06:43,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:06:43,110] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:06:43,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4371
[2025-01-12 22:06:43,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:06:43,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1570/2757]  eta: 0:11:46  lr: 0.000015  min_lr: 0.000000  loss: 4.9472 (5.0057)  loss_scale: 65536.0000 (151638.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0990  max mem: 15572
Epoch: [1]  [1580/2757]  eta: 0:11:40  lr: 0.000015  min_lr: 0.000000  loss: 4.9117 (5.0053)  loss_scale: 65536.0000 (151093.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1508  max mem: 15572
Epoch: [1]  [1590/2757]  eta: 0:11:34  lr: 0.000015  min_lr: 0.000000  loss: 4.9766 (5.0052)  loss_scale: 65536.0000 (150555.6757)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1254  max mem: 15572
Epoch: [1]  [1600/2757]  eta: 0:11:28  lr: 0.000015  min_lr: 0.000000  loss: 4.9974 (5.0052)  loss_scale: 65536.0000 (150024.6346)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1730  max mem: 15572
Epoch: [1]  [1610/2757]  eta: 0:11:22  lr: 0.000015  min_lr: 0.000000  loss: 5.0162 (5.0053)  loss_scale: 65536.0000 (149500.1862)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1963  max mem: 15572
Epoch: [1]  [1620/2757]  eta: 0:11:17  lr: 0.000015  min_lr: 0.000000  loss: 5.0172 (5.0052)  loss_scale: 65536.0000 (148982.2085)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1480  max mem: 15572
Epoch: [1]  [1630/2757]  eta: 0:11:10  lr: 0.000015  min_lr: 0.000000  loss: 5.0369 (5.0054)  loss_scale: 65536.0000 (148470.5825)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1315  max mem: 15572
Epoch: [1]  [1640/2757]  eta: 0:11:05  lr: 0.000015  min_lr: 0.000000  loss: 4.9468 (5.0050)  loss_scale: 65536.0000 (147965.1920)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1452  max mem: 15572
Epoch: [1]  [1650/2757]  eta: 0:10:58  lr: 0.000015  min_lr: 0.000000  loss: 4.9441 (5.0050)  loss_scale: 65536.0000 (147465.9237)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1170  max mem: 15572
Epoch: [1]  [1660/2757]  eta: 0:10:52  lr: 0.000015  min_lr: 0.000000  loss: 4.9993 (5.0052)  loss_scale: 65536.0000 (146972.6671)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0896  max mem: 15572
Epoch: [1]  [1670/2757]  eta: 0:10:47  lr: 0.000015  min_lr: 0.000000  loss: 5.0224 (5.0051)  loss_scale: 65536.0000 (146485.3142)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1901  max mem: 15572
Epoch: [1]  [1680/2757]  eta: 0:10:41  lr: 0.000015  min_lr: 0.000000  loss: 5.0313 (5.0055)  loss_scale: 65536.0000 (146003.7597)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1708  max mem: 15572
[2025-01-12 22:08:00,788] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4497
[2025-01-12 22:08:00,788] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:08:00,788] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1690/2757]  eta: 0:10:36  lr: 0.000015  min_lr: 0.000000  loss: 5.0243 (5.0054)  loss_scale: 65536.0000 (145469.7670)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1723  max mem: 15572
Epoch: [1]  [1700/2757]  eta: 0:10:30  lr: 0.000015  min_lr: 0.000000  loss: 4.9534 (5.0054)  loss_scale: 32768.0000 (144807.2052)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.2045  max mem: 15572
Epoch: [1]  [1710/2757]  eta: 0:10:24  lr: 0.000015  min_lr: 0.000000  loss: 4.9976 (5.0056)  loss_scale: 32768.0000 (144152.3881)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1854  max mem: 15572
Epoch: [1]  [1720/2757]  eta: 0:10:18  lr: 0.000015  min_lr: 0.000000  loss: 4.9971 (5.0054)  loss_scale: 32768.0000 (143505.1807)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1497  max mem: 15572
Epoch: [1]  [1730/2757]  eta: 0:10:12  lr: 0.000015  min_lr: 0.000000  loss: 4.9724 (5.0052)  loss_scale: 32768.0000 (142865.4512)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1024  max mem: 15572
Epoch: [1]  [1740/2757]  eta: 0:10:06  lr: 0.000015  min_lr: 0.000000  loss: 4.9724 (5.0050)  loss_scale: 32768.0000 (142233.0706)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1298  max mem: 15572
Epoch: [1]  [1750/2757]  eta: 0:10:00  lr: 0.000015  min_lr: 0.000000  loss: 4.9560 (5.0049)  loss_scale: 32768.0000 (141607.9132)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1267  max mem: 15572
Epoch: [1]  [1760/2757]  eta: 0:09:54  lr: 0.000015  min_lr: 0.000000  loss: 4.9117 (5.0045)  loss_scale: 32768.0000 (140989.8558)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1312  max mem: 15572
Epoch: [1]  [1770/2757]  eta: 0:09:47  lr: 0.000015  min_lr: 0.000000  loss: 4.9475 (5.0045)  loss_scale: 32768.0000 (140378.7781)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1158  max mem: 15572
Epoch: [1]  [1780/2757]  eta: 0:09:41  lr: 0.000015  min_lr: 0.000000  loss: 4.9475 (5.0040)  loss_scale: 32768.0000 (139774.5626)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0861  max mem: 15572
Epoch: [1]  [1790/2757]  eta: 0:09:35  lr: 0.000015  min_lr: 0.000000  loss: 4.9278 (5.0041)  loss_scale: 32768.0000 (139177.0944)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0803  max mem: 15572
Epoch: [1]  [1800/2757]  eta: 0:09:29  lr: 0.000015  min_lr: 0.000000  loss: 4.9579 (5.0041)  loss_scale: 32768.0000 (138586.2610)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1036  max mem: 15572
Epoch: [1]  [1810/2757]  eta: 0:09:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9300 (5.0039)  loss_scale: 32768.0000 (138001.9525)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1336  max mem: 15572
[2025-01-12 22:09:15,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:09:15,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1820/2757]  eta: 0:09:17  lr: 0.000016  min_lr: 0.000000  loss: 4.9300 (5.0037)  loss_scale: 32768.0000 (137496.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1315  max mem: 15572
Epoch: [1]  [1830/2757]  eta: 0:09:11  lr: 0.000016  min_lr: 0.000000  loss: 4.9599 (5.0040)  loss_scale: 65536.0000 (137103.0300)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1389  max mem: 15572
Epoch: [1]  [1840/2757]  eta: 0:09:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9599 (5.0036)  loss_scale: 65536.0000 (136714.2901)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1262  max mem: 15572
Epoch: [1]  [1850/2757]  eta: 0:08:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9366 (5.0036)  loss_scale: 65536.0000 (136329.7504)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1137  max mem: 15572
Epoch: [1]  [1860/2757]  eta: 0:08:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9796 (5.0036)  loss_scale: 65536.0000 (135949.3434)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1022  max mem: 15572
Epoch: [1]  [1870/2757]  eta: 0:08:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9965 (5.0037)  loss_scale: 65536.0000 (135573.0027)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1521  max mem: 15572
Epoch: [1]  [1880/2757]  eta: 0:08:41  lr: 0.000016  min_lr: 0.000000  loss: 5.0213 (5.0037)  loss_scale: 65536.0000 (135200.6635)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1792  max mem: 15572
Epoch: [1]  [1890/2757]  eta: 0:08:35  lr: 0.000016  min_lr: 0.000000  loss: 4.9534 (5.0033)  loss_scale: 65536.0000 (134832.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1423  max mem: 15572
Epoch: [1]  [1900/2757]  eta: 0:08:29  lr: 0.000016  min_lr: 0.000000  loss: 4.9399 (5.0031)  loss_scale: 65536.0000 (134467.7370)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1250  max mem: 15572
Epoch: [1]  [1910/2757]  eta: 0:08:23  lr: 0.000016  min_lr: 0.000000  loss: 5.0035 (5.0030)  loss_scale: 65536.0000 (134107.0267)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0860  max mem: 15572
Epoch: [1]  [1920/2757]  eta: 0:08:17  lr: 0.000016  min_lr: 0.000000  loss: 5.0035 (5.0030)  loss_scale: 65536.0000 (133750.0718)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1061  max mem: 15572
Epoch: [1]  [1930/2757]  eta: 0:08:11  lr: 0.000016  min_lr: 0.000000  loss: 4.9060 (5.0026)  loss_scale: 65536.0000 (133396.8141)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1585  max mem: 15572
Epoch: [1]  [1940/2757]  eta: 0:08:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9465 (5.0027)  loss_scale: 65536.0000 (133047.1963)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1614  max mem: 15572
[2025-01-12 22:10:30,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:10:30,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:10:33,506] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4758
[2025-01-12 22:10:33,507] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:10:33,507] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1950/2757]  eta: 0:07:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9789 (5.0025)  loss_scale: 65536.0000 (132835.5264)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1467  max mem: 15572
Epoch: [1]  [1960/2757]  eta: 0:07:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9362 (5.0024)  loss_scale: 65536.0000 (132492.3366)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1378  max mem: 15572
Epoch: [1]  [1970/2757]  eta: 0:07:48  lr: 0.000016  min_lr: 0.000000  loss: 4.9789 (5.0025)  loss_scale: 65536.0000 (132152.6291)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1720  max mem: 15572
Epoch: [1]  [1980/2757]  eta: 0:07:42  lr: 0.000016  min_lr: 0.000000  loss: 5.0177 (5.0025)  loss_scale: 65536.0000 (131816.3513)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1399  max mem: 15572
Epoch: [1]  [1990/2757]  eta: 0:07:36  lr: 0.000016  min_lr: 0.000000  loss: 5.0177 (5.0025)  loss_scale: 65536.0000 (131483.4515)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0755  max mem: 15572
Epoch: [1]  [2000/2757]  eta: 0:07:30  lr: 0.000016  min_lr: 0.000000  loss: 4.9999 (5.0025)  loss_scale: 65536.0000 (131153.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1059  max mem: 15572
Epoch: [1]  [2010/2757]  eta: 0:07:24  lr: 0.000016  min_lr: 0.000000  loss: 4.9562 (5.0023)  loss_scale: 65536.0000 (130827.5843)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1132  max mem: 15572
Epoch: [1]  [2020/2757]  eta: 0:07:18  lr: 0.000016  min_lr: 0.000000  loss: 4.9342 (5.0022)  loss_scale: 65536.0000 (130504.5186)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1224  max mem: 15572
Epoch: [1]  [2030/2757]  eta: 0:07:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0120 (5.0021)  loss_scale: 65536.0000 (130184.6342)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0750  max mem: 15572
Epoch: [1]  [2040/2757]  eta: 0:07:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9927 (5.0022)  loss_scale: 65536.0000 (129867.8844)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0808  max mem: 15572
Epoch: [1]  [2050/2757]  eta: 0:06:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9555 (5.0019)  loss_scale: 65536.0000 (129554.2233)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1612  max mem: 15572
Epoch: [1]  [2060/2757]  eta: 0:06:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9555 (5.0018)  loss_scale: 65536.0000 (129243.6060)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.0955  max mem: 15572
[2025-01-12 22:11:43,043] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4878
[2025-01-12 22:11:43,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:11:43,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2070/2757]  eta: 0:06:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9691 (5.0015)  loss_scale: 65536.0000 (128904.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0990  max mem: 15572
Epoch: [1]  [2080/2757]  eta: 0:06:42  lr: 0.000016  min_lr: 0.000000  loss: 4.9606 (5.0014)  loss_scale: 32768.0000 (128442.3719)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1886  max mem: 15572
Epoch: [1]  [2090/2757]  eta: 0:06:36  lr: 0.000016  min_lr: 0.000000  loss: 4.9747 (5.0012)  loss_scale: 32768.0000 (127984.8187)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1716  max mem: 15572
Epoch: [1]  [2100/2757]  eta: 0:06:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9763 (5.0012)  loss_scale: 32768.0000 (127531.6211)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1695  max mem: 15572
Epoch: [1]  [2110/2757]  eta: 0:06:24  lr: 0.000017  min_lr: 0.000000  loss: 4.9763 (5.0015)  loss_scale: 32768.0000 (127082.7172)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1690  max mem: 15572
Epoch: [1]  [2120/2757]  eta: 0:06:18  lr: 0.000017  min_lr: 0.000000  loss: 5.0306 (5.0016)  loss_scale: 32768.0000 (126638.0462)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1326  max mem: 15572
Epoch: [1]  [2130/2757]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9694 (5.0015)  loss_scale: 32768.0000 (126197.5486)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.2154  max mem: 15572
Epoch: [1]  [2140/2757]  eta: 0:06:06  lr: 0.000017  min_lr: 0.000000  loss: 5.0001 (5.0016)  loss_scale: 32768.0000 (125761.1658)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1618  max mem: 15572
Epoch: [1]  [2150/2757]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 5.0358 (5.0017)  loss_scale: 32768.0000 (125328.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1046  max mem: 15572
Epoch: [1]  [2160/2757]  eta: 0:05:54  lr: 0.000017  min_lr: 0.000000  loss: 4.9501 (5.0015)  loss_scale: 32768.0000 (124900.5164)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1524  max mem: 15572
Epoch: [1]  [2170/2757]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 4.9501 (5.0014)  loss_scale: 32768.0000 (124476.1382)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0979  max mem: 15572
Epoch: [1]  [2180/2757]  eta: 0:05:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9653 (5.0013)  loss_scale: 32768.0000 (124055.6515)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1572  max mem: 15572
[2025-01-12 22:12:56,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=23, lr=[1.6299806426952057e-07, 1.6299806426952057e-07, 2.3285437752788657e-07, 2.3285437752788657e-07, 3.326491107541237e-07, 3.326491107541237e-07, 4.752130153630339e-07, 4.752130153630339e-07, 6.788757362329056e-07, 6.788757362329056e-07, 9.698224803327224e-07, 9.698224803327224e-07, 1.3854606861896035e-06, 1.3854606861896035e-06, 1.979229551699434e-06, 1.979229551699434e-06, 2.827470788142048e-06, 2.827470788142048e-06, 4.039243983060069e-06, 4.039243983060069e-06, 5.77034854722867e-06, 5.77034854722867e-06, 8.24335506746953e-06, 8.24335506746953e-06, 1.1776221524956472e-05, 1.1776221524956472e-05, 1.6823173607080675e-05, 1.6823173607080675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:12:56,599] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=27.70197373847942, CurrSamplesPerSec=31.650784986712512, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2757]  eta: 0:05:37  lr: 0.000017  min_lr: 0.000000  loss: 4.9606 (5.0014)  loss_scale: 32768.0000 (123639.0032)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.1911  max mem: 15572
[2025-01-12 22:13:00,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:13:00,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2200/2757]  eta: 0:05:31  lr: 0.000017  min_lr: 0.000000  loss: 5.0154 (5.0015)  loss_scale: 32768.0000 (123270.8042)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1185  max mem: 15572
Epoch: [1]  [2210/2757]  eta: 0:05:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9875 (5.0012)  loss_scale: 65536.0000 (123009.6789)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1280  max mem: 15572
Epoch: [1]  [2220/2757]  eta: 0:05:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9621 (5.0011)  loss_scale: 65536.0000 (122750.9050)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1727  max mem: 15572
Epoch: [1]  [2230/2757]  eta: 0:05:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9891 (5.0010)  loss_scale: 65536.0000 (122494.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0917  max mem: 15572
Epoch: [1]  [2240/2757]  eta: 0:05:06  lr: 0.000017  min_lr: 0.000000  loss: 4.9356 (5.0006)  loss_scale: 65536.0000 (122240.2856)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0371  max mem: 15572
Epoch: [1]  [2250/2757]  eta: 0:05:01  lr: 0.000017  min_lr: 0.000000  loss: 4.9439 (5.0004)  loss_scale: 65536.0000 (121988.3785)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0932  max mem: 15572
Epoch: [1]  [2260/2757]  eta: 0:04:55  lr: 0.000017  min_lr: 0.000000  loss: 4.9320 (5.0000)  loss_scale: 65536.0000 (121738.6997)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1086  max mem: 15572
Epoch: [1]  [2270/2757]  eta: 0:04:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9340 (4.9997)  loss_scale: 65536.0000 (121491.2197)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1707  max mem: 15572
Epoch: [1]  [2280/2757]  eta: 0:04:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9663 (4.9996)  loss_scale: 65536.0000 (121245.9097)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1355  max mem: 15572
Epoch: [1]  [2290/2757]  eta: 0:04:37  lr: 0.000017  min_lr: 0.000000  loss: 4.9271 (4.9993)  loss_scale: 65536.0000 (121002.7412)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0625  max mem: 15572
Epoch: [1]  [2300/2757]  eta: 0:04:31  lr: 0.000017  min_lr: 0.000000  loss: 4.9146 (4.9991)  loss_scale: 65536.0000 (120761.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0830  max mem: 15572
Epoch: [1]  [2310/2757]  eta: 0:04:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9074 (4.9987)  loss_scale: 65536.0000 (120522.7174)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0841  max mem: 15572
Epoch: [1]  [2320/2757]  eta: 0:04:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9042 (4.9984)  loss_scale: 65536.0000 (120285.8078)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0607  max mem: 15572
[2025-01-12 22:14:14,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:14:14,459] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:14:15,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5137
[2025-01-12 22:14:15,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:14:15,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2330/2757]  eta: 0:04:13  lr: 0.000017  min_lr: 0.000000  loss: 4.9072 (4.9982)  loss_scale: 65536.0000 (120107.1609)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0971  max mem: 15572
Epoch: [1]  [2340/2757]  eta: 0:04:07  lr: 0.000017  min_lr: 0.000000  loss: 4.9276 (4.9981)  loss_scale: 65536.0000 (119874.0504)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1193  max mem: 15572
Epoch: [1]  [2350/2757]  eta: 0:04:01  lr: 0.000017  min_lr: 0.000000  loss: 5.0196 (4.9983)  loss_scale: 65536.0000 (119642.9230)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0895  max mem: 15572
Epoch: [1]  [2360/2757]  eta: 0:03:55  lr: 0.000017  min_lr: 0.000000  loss: 5.0005 (4.9983)  loss_scale: 65536.0000 (119413.7535)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0874  max mem: 15572
Epoch: [1]  [2370/2757]  eta: 0:03:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9258 (4.9980)  loss_scale: 65536.0000 (119186.5171)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0917  max mem: 15572
Epoch: [1]  [2380/2757]  eta: 0:03:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9258 (4.9979)  loss_scale: 65536.0000 (118961.1894)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1095  max mem: 15572
Epoch: [1]  [2390/2757]  eta: 0:03:37  lr: 0.000018  min_lr: 0.000000  loss: 4.9520 (4.9978)  loss_scale: 65536.0000 (118737.7465)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1594  max mem: 15572
Epoch: [1]  [2400/2757]  eta: 0:03:31  lr: 0.000018  min_lr: 0.000000  loss: 4.9626 (4.9976)  loss_scale: 65536.0000 (118516.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1312  max mem: 15572
Epoch: [1]  [2410/2757]  eta: 0:03:25  lr: 0.000018  min_lr: 0.000000  loss: 4.9298 (4.9973)  loss_scale: 65536.0000 (118296.4214)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0887  max mem: 15572
Epoch: [1]  [2420/2757]  eta: 0:03:19  lr: 0.000018  min_lr: 0.000000  loss: 4.9183 (4.9972)  loss_scale: 65536.0000 (118078.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1248  max mem: 15572
Epoch: [1]  [2430/2757]  eta: 0:03:13  lr: 0.000018  min_lr: 0.000000  loss: 4.9801 (4.9973)  loss_scale: 65536.0000 (117862.3579)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1658  max mem: 15572
Epoch: [1]  [2440/2757]  eta: 0:03:07  lr: 0.000018  min_lr: 0.000000  loss: 5.0071 (4.9972)  loss_scale: 65536.0000 (117647.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1783  max mem: 15572
Epoch: [1]  [2450/2757]  eta: 0:03:01  lr: 0.000018  min_lr: 0.000000  loss: 4.9142 (4.9970)  loss_scale: 65536.0000 (117435.3782)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1289  max mem: 15572
[2025-01-12 22:15:30,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:15:30,292] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:15:31,119] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5268
[2025-01-12 22:15:31,119] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:15:31,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2460/2757]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000000  loss: 4.9668 (4.9971)  loss_scale: 65536.0000 (117277.7505)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1427  max mem: 15572
Epoch: [1]  [2470/2757]  eta: 0:02:49  lr: 0.000018  min_lr: 0.000000  loss: 4.9566 (4.9969)  loss_scale: 65536.0000 (117068.3545)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1584  max mem: 15572
Epoch: [1]  [2480/2757]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9235 (4.9965)  loss_scale: 65536.0000 (116860.6465)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1845  max mem: 15572
Epoch: [1]  [2490/2757]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9352 (4.9962)  loss_scale: 65536.0000 (116654.6062)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1746  max mem: 15572
Epoch: [1]  [2500/2757]  eta: 0:02:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9503 (4.9963)  loss_scale: 65536.0000 (116450.2135)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1231  max mem: 15572
Epoch: [1]  [2510/2757]  eta: 0:02:26  lr: 0.000018  min_lr: 0.000000  loss: 5.0270 (4.9966)  loss_scale: 65536.0000 (116247.4488)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1665  max mem: 15572
Epoch: [1]  [2520/2757]  eta: 0:02:20  lr: 0.000018  min_lr: 0.000000  loss: 5.0135 (4.9966)  loss_scale: 65536.0000 (116046.2927)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1488  max mem: 15572
Epoch: [1]  [2530/2757]  eta: 0:02:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9944 (4.9964)  loss_scale: 65536.0000 (115846.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1266  max mem: 15572
Epoch: [1]  [2540/2757]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 5.0073 (4.9965)  loss_scale: 65536.0000 (115648.7304)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1021  max mem: 15572
Epoch: [1]  [2550/2757]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 5.0091 (4.9965)  loss_scale: 65536.0000 (115452.2869)  weight_decay: 0.0500 (0.0500)  time: 0.4917  data: 0.0367  max mem: 15572
Epoch: [1]  [2560/2757]  eta: 0:01:56  lr: 0.000018  min_lr: 0.000000  loss: 5.0244 (4.9966)  loss_scale: 65536.0000 (115257.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0939  max mem: 15572
Epoch: [1]  [2570/2757]  eta: 0:01:50  lr: 0.000018  min_lr: 0.000000  loss: 5.0304 (4.9968)  loss_scale: 65536.0000 (115063.9844)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.1879  max mem: 15572
Epoch: [1]  [2580/2757]  eta: 0:01:44  lr: 0.000018  min_lr: 0.000000  loss: 4.9806 (4.9966)  loss_scale: 65536.0000 (114872.0899)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1688  max mem: 15572
[2025-01-12 22:16:47,751] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:16:47,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:16:48,166] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5398
[2025-01-12 22:16:48,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:16:48,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2590/2757]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9474 (4.9963)  loss_scale: 65536.0000 (114706.9703)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1685  max mem: 15572
Epoch: [1]  [2600/2757]  eta: 0:01:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9032 (4.9960)  loss_scale: 65536.0000 (114517.9239)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1939  max mem: 15572
Epoch: [1]  [2610/2757]  eta: 0:01:27  lr: 0.000018  min_lr: 0.000000  loss: 4.8851 (4.9957)  loss_scale: 65536.0000 (114330.3255)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.2347  max mem: 15572
Epoch: [1]  [2620/2757]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9095 (4.9955)  loss_scale: 65536.0000 (114144.1587)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.2266  max mem: 15572
Epoch: [1]  [2630/2757]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 4.9679 (4.9955)  loss_scale: 65536.0000 (113959.4071)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1452  max mem: 15572
Epoch: [1]  [2640/2757]  eta: 0:01:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9852 (4.9954)  loss_scale: 65536.0000 (113776.0545)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1240  max mem: 15572
Epoch: [1]  [2650/2757]  eta: 0:01:03  lr: 0.000018  min_lr: 0.000000  loss: 4.9861 (4.9953)  loss_scale: 65536.0000 (113594.0853)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1348  max mem: 15572
Epoch: [1]  [2660/2757]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9777 (4.9952)  loss_scale: 65536.0000 (113413.4837)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2277  max mem: 15572
[2025-01-12 22:17:33,860] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5471
[2025-01-12 22:17:33,860] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:17:33,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2670/2757]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 4.9376 (4.9949)  loss_scale: 65536.0000 (113123.8218)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.1792  max mem: 15572
Epoch: [1]  [2680/2757]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 4.9725 (4.9950)  loss_scale: 32768.0000 (112824.0985)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0847  max mem: 15572
Epoch: [1]  [2690/2757]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000000  loss: 4.9698 (4.9948)  loss_scale: 32768.0000 (112526.6027)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0931  max mem: 15572
Epoch: [1]  [2700/2757]  eta: 0:00:33  lr: 0.000019  min_lr: 0.000000  loss: 4.9558 (4.9949)  loss_scale: 32768.0000 (112231.3099)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0814  max mem: 15572
Epoch: [1]  [2710/2757]  eta: 0:00:27  lr: 0.000019  min_lr: 0.000000  loss: 4.9558 (4.9947)  loss_scale: 32768.0000 (111938.1955)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0537  max mem: 15572
Epoch: [1]  [2720/2757]  eta: 0:00:21  lr: 0.000019  min_lr: 0.000000  loss: 4.9237 (4.9945)  loss_scale: 32768.0000 (111647.2356)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0764  max mem: 15572
Epoch: [1]  [2730/2757]  eta: 0:00:15  lr: 0.000019  min_lr: 0.000000  loss: 4.9602 (4.9943)  loss_scale: 32768.0000 (111358.4064)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.0946  max mem: 15572
Epoch: [1]  [2740/2757]  eta: 0:00:10  lr: 0.000019  min_lr: 0.000000  loss: 4.9616 (4.9941)  loss_scale: 32768.0000 (111071.6848)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1101  max mem: 15572
Epoch: [1]  [2750/2757]  eta: 0:00:04  lr: 0.000019  min_lr: 0.000000  loss: 4.9486 (4.9938)  loss_scale: 32768.0000 (110787.0476)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0779  max mem: 15572
Epoch: [1]  [2756/2757]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.8956 (4.9937)  loss_scale: 32768.0000 (110617.2564)  weight_decay: 0.0500 (0.0500)  time: 0.4126  data: 0.0006  max mem: 15572
Epoch: [1] Total time: 0:27:11 (0.5917 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.8956 (4.9937)  loss_scale: 32768.0000 (110617.2564)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 822
Indices to remove: tensor([   89,   356,   465,   734,   785,   936,   945,  1007,  1019,  1095,
         1130,  1140,  1188,  1195,  1211,  1294,  1377,  1383,  1387,  1416,
         1449,  1844,  1888,  2037,  2162,  2170,  2330,  2461,  2537,  2553,
         2579,  2632,  2828,  2858,  2898,  2903,  2910,  3056,  3221,  3381,
         3403,  3419,  3468,  3526,  3586,  3591,  3600,  3611,  3613,  3617,
         3636,  3642,  3659,  3692,  3694,  3698,  3739,  3764,  3772,  3822,
         3870,  3891,  3909,  3930,  3936,  4070,  4152,  4164,  4225,  4514,
         4731,  5013,  5043,  5211,  5235,  5259,  5291,  5292,  5299,  5324,
         5440,  5479,  5712,  5722,  5733,  5808,  5855,  6037,  6165,  6690,
         6714,  6736,  6818,  6830,  6902,  7057,  7309,  7580,  7915,  8078,
         8137,  8368,  8465,  8470,  8589,  8594,  8595,  8611,  8618,  8630,
         8638,  8641,  8648,  8654,  8666,  8681,  8689,  8698,  8699,  8725,
         8744,  8745,  8750,  8759,  8760,  8761,  8762,  8765,  8779,  8790,
         8804,  8820,  8822,  8844,  8846,  8851,  8856,  8863,  8864,  8876,
         8878,  8886,  8900,  8907,  8931,  8935,  8956,  8963,  8978,  8987,
         8990,  8991,  8993,  8995,  9007,  9027,  9028,  9029,  9045,  9061,
         9065,  9091,  9102,  9110,  9111,  9114,  9123,  9294,  9307,  9309,
         9310,  9312,  9316,  9318,  9322,  9323,  9327,  9328,  9330,  9337,
         9339,  9341,  9342,  9343,  9344,  9345,  9346,  9347,  9351,  9352,
         9354,  9355,  9358,  9362,  9363,  9364,  9371,  9372,  9373,  9375,
         9376,  9379,  9381,  9386,  9389,  9390,  9391,  9393,  9397,  9399,
         9409,  9410,  9412,  9413,  9415,  9418,  9419,  9421,  9426,  9429,
         9431,  9433,  9435,  9436,  9439,  9447,  9448,  9449,  9452,  9454,
         9459,  9461,  9463,  9464,  9470,  9474,  9477,  9484,  9488,  9491,
         9492,  9499,  9501,  9502,  9503,  9508,  9509,  9513,  9514,  9516,
         9522,  9523,  9530,  9532,  9534,  9535,  9537,  9543,  9550,  9552,
         9559,  9561,  9566,  9567,  9575,  9578,  9584,  9585,  9597,  9605,
         9607,  9610,  9612,  9618,  9620,  9622,  9623,  9627,  9630,  9631,
         9636,  9638,  9639,  9642,  9643,  9660,  9669,  9671,  9681,  9682,
         9684,  9687,  9693,  9696,  9698,  9701,  9703,  9704,  9712,  9713,
         9714,  9716,  9718,  9722,  9726,  9732,  9742,  9743,  9749,  9751,
         9754,  9759,  9761,  9762,  9765,  9769,  9771,  9772,  9775,  9776,
         9779,  9780,  9782,  9784,  9786,  9791,  9799,  9800,  9801,  9804,
         9809,  9811,  9813,  9819,  9820,  9823,  9824,  9829,  9832,  9833,
         9835,  9838,  9839,  9842,  9843,  9846,  9848,  9854,  9855,  9861,
         9864,  9867,  9868,  9869,  9870,  9871,  9872,  9875,  9876,  9877,
         9884,  9885,  9888,  9890,  9893,  9895,  9897,  9899,  9904,  9910,
         9914,  9916,  9918,  9920,  9921,  9926,  9927,  9929,  9931,  9932,
         9935,  9936, 10018, 10276, 10509, 10548, 10638, 10820, 10821, 11036,
        11084, 11276, 11332, 11333, 11339, 11357, 11366, 11369, 11376, 11385,
        11386, 11388, 11391, 11396, 11399, 11400, 11403, 11404, 11412, 11420,
        11422, 11423, 11428, 11444, 11445, 11449, 11450, 11452, 11457, 11466,
        11476, 11478, 11481, 11484, 11492, 11493, 11496, 11498, 11507, 11511,
        11513, 11514, 11517, 11519, 11524, 11528, 11532, 11551, 11554, 11562,
        11566, 11575, 11583, 11584, 11592, 11596, 11598, 11604, 11606, 11609,
        11615, 11628, 11633, 11634, 11647, 11651, 11662, 11672, 11676, 11682,
        11686, 11688, 11698, 11704, 11707, 11711, 11723, 11726, 11733, 11739,
        11742, 11759, 11924, 12069, 12233, 12307, 12356, 12514, 12621, 12795,
        13064, 13133, 13201, 13226, 13326, 13418, 13532, 13537, 13599, 13760,
        13867, 13933, 14086, 14315, 14524, 14586, 14982, 15046, 15052, 15224,
        15241, 15276, 15368, 15389, 15453, 15488, 15564, 15741, 16000, 16292,
        16325, 16414, 16424, 16531, 16542, 16703, 16710, 16714, 16719, 16721,
        16723, 16725, 16730, 16733, 16741, 16744, 16749, 16751, 16752, 16753,
        16755, 16762, 16767, 16773, 16778, 16783, 16786, 16791, 16804, 16807,
        16808, 16810, 16811, 16813, 16818, 16819, 16820, 16823, 16835, 16836,
        16844, 16845, 16846, 16848, 16851, 16861, 16863, 16868, 16872, 16878,
        16884, 16885, 16887, 16892, 16902, 16903, 16904, 16906, 16914, 16915,
        16917, 16919, 16921, 16924, 16925, 16929, 16933, 16939, 16940, 16942,
        16947, 16957, 16959, 16962, 16964, 16980, 16988, 16989, 16990, 16994,
        16995, 17001, 17010, 17018, 17023, 17032, 17034, 17042, 17044, 17054,
        17060, 17062, 17064, 17066, 17068, 17069, 17071, 17072, 17074, 17075,
        17076, 17077, 17079, 17080, 17081, 17082, 17083, 17084, 17086, 17087,
        17090, 17091, 17092, 17093, 17103, 17105, 17110, 17113, 17115, 17116,
        17117, 17121, 17127, 17128, 17130, 17131, 17132, 17136, 17139, 17140,
        17141, 17147, 17149, 17155, 17156, 17158, 17161, 17162, 17163, 17168,
        17170, 17172, 17182, 17183, 17185, 17189, 17191, 17192, 17195, 17200,
        17204, 17207, 17215, 17217, 17219, 17223, 17226, 17227, 17229, 17230,
        17234, 17240, 17241, 17244, 17245, 17249, 17252, 17255, 17258, 17261,
        17264, 17268, 17270, 17277, 17278, 17286, 17288, 17380, 17386, 17389,
        17410, 17465, 17475, 17481, 17491, 17497, 17535, 17539, 17546, 17549,
        17559, 17585, 17624, 17652, 17664, 17668, 17669, 17670, 17672, 17676,
        17682, 17696, 17706, 17715, 17741, 17761, 17771, 17810, 17831, 17868,
        18078, 18299, 18343, 18667, 18693, 18753, 18918, 19066, 19220, 19882,
        20131, 20251, 20338, 20404, 20470, 20556, 20710, 20735, 20814, 20832,
        20908, 20948, 21011, 21201, 21206, 21413, 21976, 22053, 22196, 22251,
        22373, 22528, 22625, 22639, 22698, 22921, 22990, 23236, 23456, 23464,
        23706, 23708, 23772, 23802, 23848, 24125, 24236, 24681, 24737, 25153,
        25299, 25449, 25579, 25784, 25787, 25885, 25935, 26228, 26312, 26836,
        27091, 27097, 27107, 27270, 27345, 27385, 27428, 27504, 27853, 27869,
        27886, 28071, 28375, 28561, 28604, 28647, 28785, 29036, 29228, 29268,
        29320, 29366, 29459, 29520, 29899, 29958, 30435, 30636, 31367, 31450,
        31575, 31583, 31897, 31900, 32072, 32322, 32709, 32989, 32999, 33083,
        33291, 33407], device='cuda:0')
length of data loader train is: 2689
num_training_steps_per_epoch is: 2689
Change step level LR scheduler!
Set warmup steps = 13445
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:28:41  loss: 5.0156 (5.0156)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 6.3278  data: 6.0936  max mem: 15572
Val:  [ 10/272]  eta: 0:03:47  loss: 5.1161 (5.0154)  acc1: 0.0000 (6.0606)  acc5: 0.0000 (19.1919)  time: 0.8666  data: 0.6806  max mem: 15572
Val:  [ 20/272]  eta: 0:02:15  loss: 5.0115 (4.8878)  acc1: 0.0000 (7.4074)  acc5: 0.0000 (18.7831)  time: 0.2501  data: 0.0699  max mem: 15572
Val:  [ 30/272]  eta: 0:01:56  loss: 4.8783 (4.8666)  acc1: 0.0000 (5.1971)  acc5: 0.0000 (18.6380)  time: 0.2716  data: 0.0941  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 4.5074 (4.7596)  acc1: 0.0000 (8.6721)  acc5: 27.7778 (29.9458)  time: 0.2976  data: 0.1098  max mem: 15572
Val:  [ 50/272]  eta: 0:01:33  loss: 4.5299 (4.7987)  acc1: 0.0000 (6.9717)  acc5: 16.6667 (24.9455)  time: 0.3291  data: 0.1337  max mem: 15572
Val:  [ 60/272]  eta: 0:01:28  loss: 4.7122 (4.8144)  acc1: 0.0000 (5.8288)  acc5: 0.0000 (20.8561)  time: 0.4066  data: 0.2137  max mem: 15572
Val:  [ 70/272]  eta: 0:01:21  loss: 4.7270 (4.8018)  acc1: 0.0000 (8.8419)  acc5: 0.0000 (21.9875)  time: 0.3604  data: 0.1689  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 4.9064 (4.7873)  acc1: 0.0000 (7.7503)  acc5: 0.0000 (23.4568)  time: 0.3089  data: 0.1252  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 4.9184 (4.8234)  acc1: 0.0000 (6.8987)  acc5: 0.0000 (20.8791)  time: 0.2919  data: 0.1021  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 4.9616 (4.8451)  acc1: 0.0000 (6.3256)  acc5: 0.0000 (20.4070)  time: 0.2933  data: 0.0933  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.9616 (4.8738)  acc1: 0.0000 (5.7558)  acc5: 0.0000 (18.5686)  time: 0.3062  data: 0.1077  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 5.1732 (4.8992)  acc1: 0.0000 (5.2801)  acc5: 0.0000 (17.0340)  time: 0.2839  data: 0.0805  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 5.0519 (4.8876)  acc1: 0.0000 (6.6582)  acc5: 0.0000 (19.0416)  time: 0.2760  data: 0.0692  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.7682 (4.8809)  acc1: 0.0000 (6.3436)  acc5: 0.0000 (19.5429)  time: 0.3100  data: 0.1138  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.7789 (4.8789)  acc1: 0.0000 (5.9235)  acc5: 0.0000 (18.2487)  time: 0.3247  data: 0.1336  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 4.6541 (4.8672)  acc1: 0.0000 (5.5556)  acc5: 0.0000 (17.5293)  time: 0.3232  data: 0.1315  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.6549 (4.8816)  acc1: 0.0000 (5.2307)  acc5: 0.0000 (16.5042)  time: 0.3282  data: 0.1292  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 5.0102 (4.8822)  acc1: 0.0000 (4.9417)  acc5: 0.0000 (15.5924)  time: 0.3073  data: 0.0946  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.8984 (4.8873)  acc1: 0.0000 (4.6830)  acc5: 0.0000 (14.7760)  time: 0.3151  data: 0.1020  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 5.1048 (4.8988)  acc1: 0.0000 (4.4500)  acc5: 0.0000 (14.0409)  time: 0.3408  data: 0.1448  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.1283 (4.9079)  acc1: 0.0000 (4.2391)  acc5: 0.0000 (13.4281)  time: 0.3169  data: 0.1342  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.9286 (4.8991)  acc1: 0.0000 (4.1981)  acc5: 0.0000 (13.3987)  time: 0.3282  data: 0.1346  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7203 (4.8923)  acc1: 0.0000 (4.0164)  acc5: 0.0000 (12.8187)  time: 0.3615  data: 0.1520  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.7222 (4.8914)  acc1: 0.0000 (3.8497)  acc5: 0.0000 (12.5403)  time: 0.3394  data: 0.1296  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.1923 (4.9101)  acc1: 0.0000 (3.6963)  acc5: 0.0000 (12.0407)  time: 0.2972  data: 0.1000  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.8713 (4.9025)  acc1: 0.0000 (3.5547)  acc5: 0.0000 (11.6645)  time: 0.2999  data: 0.1128  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.7387 (4.9020)  acc1: 0.0000 (3.4235)  acc5: 0.0000 (12.3206)  time: 0.2381  data: 0.0704  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.7450 (4.9045)  acc1: 0.0000 (3.4200)  acc5: 0.0000 (12.3080)  time: 0.2293  data: 0.0704  max mem: 15572
Val: Total time: 0:01:30 (0.3319 s / it)
* Acc@1 3.420 Acc@5 12.308 loss 4.905
Accuracy of the network on the 4883 val videos: 3.4%
[2025-01-12 22:19:55,234] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 22:19:55,238] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 22:19:55,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 22:19:58,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 22:19:58,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 3.42%
Epoch: [2]  [   0/2689]  eta: 7:01:54  lr: 0.000019  min_lr: 0.000000  loss: 5.0057 (5.0057)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4141  data: 8.9224  max mem: 15572
Epoch: [2]  [  10/2689]  eta: 1:00:22  lr: 0.000019  min_lr: 0.000000  loss: 5.0057 (5.0115)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3522  data: 0.8957  max mem: 15572
Epoch: [2]  [  20/2689]  eta: 0:41:44  lr: 0.000019  min_lr: 0.000000  loss: 4.9537 (4.9664)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0471  max mem: 15572
Epoch: [2]  [  30/2689]  eta: 0:36:03  lr: 0.000019  min_lr: 0.000000  loss: 4.9130 (4.9445)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0537  max mem: 15572
[2025-01-12 22:20:25,444] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:20:25,444] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [  40/2689]  eta: 0:34:15  lr: 0.000019  min_lr: 0.000000  loss: 4.9295 (4.9562)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1668  max mem: 15572
Epoch: [2]  [  50/2689]  eta: 0:32:47  lr: 0.000019  min_lr: 0.000000  loss: 4.9478 (4.9489)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1982  max mem: 15572
Epoch: [2]  [  60/2689]  eta: 0:31:57  lr: 0.000019  min_lr: 0.000000  loss: 5.0048 (4.9600)  loss_scale: 65536.0000 (47271.8689)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1818  max mem: 15572
Epoch: [2]  [  70/2689]  eta: 0:31:03  lr: 0.000019  min_lr: 0.000000  loss: 5.0006 (4.9596)  loss_scale: 65536.0000 (49844.2817)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1822  max mem: 15572
Epoch: [2]  [  80/2689]  eta: 0:30:49  lr: 0.000019  min_lr: 0.000000  loss: 4.9225 (4.9617)  loss_scale: 65536.0000 (51781.5309)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.2073  max mem: 15572
Epoch: [2]  [  90/2689]  eta: 0:30:20  lr: 0.000019  min_lr: 0.000000  loss: 4.9069 (4.9591)  loss_scale: 65536.0000 (53293.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.2201  max mem: 15572
Epoch: [2]  [ 100/2689]  eta: 0:29:49  lr: 0.000019  min_lr: 0.000000  loss: 4.9246 (4.9620)  loss_scale: 65536.0000 (54505.1881)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1839  max mem: 15572
Epoch: [2]  [ 110/2689]  eta: 0:29:21  lr: 0.000019  min_lr: 0.000000  loss: 5.0082 (4.9693)  loss_scale: 65536.0000 (55498.9550)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1502  max mem: 15572
Epoch: [2]  [ 120/2689]  eta: 0:28:36  lr: 0.000019  min_lr: 0.000000  loss: 5.0143 (4.9641)  loss_scale: 65536.0000 (56328.4628)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0935  max mem: 15572
Epoch: [2]  [ 130/2689]  eta: 0:28:07  lr: 0.000019  min_lr: 0.000000  loss: 4.9198 (4.9657)  loss_scale: 65536.0000 (57031.3282)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0920  max mem: 15572
Epoch: [2]  [ 140/2689]  eta: 0:27:46  lr: 0.000019  min_lr: 0.000000  loss: 4.9500 (4.9646)  loss_scale: 65536.0000 (57634.4965)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1273  max mem: 15572
[2025-01-12 22:21:32,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5712
[2025-01-12 22:21:32,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:21:32,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 150/2689]  eta: 0:27:33  lr: 0.000019  min_lr: 0.000000  loss: 4.9085 (4.9595)  loss_scale: 65536.0000 (57072.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1423  max mem: 15572
Epoch: [2]  [ 160/2689]  eta: 0:27:17  lr: 0.000019  min_lr: 0.000000  loss: 4.9187 (4.9605)  loss_scale: 32768.0000 (55563.1304)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1532  max mem: 15572
Epoch: [2]  [ 170/2689]  eta: 0:26:55  lr: 0.000019  min_lr: 0.000000  loss: 4.9548 (4.9599)  loss_scale: 32768.0000 (54230.0819)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0959  max mem: 15572
Epoch: [2]  [ 180/2689]  eta: 0:26:41  lr: 0.000019  min_lr: 0.000000  loss: 4.9217 (4.9586)  loss_scale: 32768.0000 (53044.3315)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0736  max mem: 15572
Epoch: [2]  [ 190/2689]  eta: 0:26:03  lr: 0.000019  min_lr: 0.000000  loss: 4.9295 (4.9616)  loss_scale: 32768.0000 (51982.7435)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0535  max mem: 15572
Epoch: [2]  [ 200/2689]  eta: 0:25:37  lr: 0.000019  min_lr: 0.000000  loss: 4.9832 (4.9616)  loss_scale: 32768.0000 (51026.7861)  weight_decay: 0.0500 (0.0500)  time: 0.4318  data: 0.0005  max mem: 15572
Epoch: [2]  [ 210/2689]  eta: 0:25:15  lr: 0.000019  min_lr: 0.000000  loss: 4.9149 (4.9569)  loss_scale: 32768.0000 (50161.4408)  weight_decay: 0.0500 (0.0500)  time: 0.4750  data: 0.0008  max mem: 15572
Epoch: [2]  [ 220/2689]  eta: 0:24:53  lr: 0.000020  min_lr: 0.000000  loss: 4.9149 (4.9571)  loss_scale: 32768.0000 (49374.4072)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0010  max mem: 15572
Epoch: [2]  [ 230/2689]  eta: 0:24:37  lr: 0.000020  min_lr: 0.000000  loss: 4.9810 (4.9571)  loss_scale: 32768.0000 (48655.5152)  weight_decay: 0.0500 (0.0500)  time: 0.4897  data: 0.0283  max mem: 15572
Epoch: [2]  [ 240/2689]  eta: 0:24:42  lr: 0.000020  min_lr: 0.000000  loss: 4.9401 (4.9567)  loss_scale: 32768.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1522  max mem: 15572
Epoch: [2]  [ 250/2689]  eta: 0:24:52  lr: 0.000020  min_lr: 0.000000  loss: 4.9200 (4.9565)  loss_scale: 32768.0000 (47389.5777)  weight_decay: 0.0500 (0.0500)  time: 0.7396  data: 0.2735  max mem: 15572
Epoch: [2]  [ 260/2689]  eta: 0:25:00  lr: 0.000020  min_lr: 0.000000  loss: 4.8977 (4.9547)  loss_scale: 32768.0000 (46829.3640)  weight_decay: 0.0500 (0.0500)  time: 0.7664  data: 0.2739  max mem: 15572
Epoch: [2]  [ 270/2689]  eta: 0:25:06  lr: 0.000020  min_lr: 0.000000  loss: 4.9140 (4.9546)  loss_scale: 32768.0000 (46310.4945)  weight_decay: 0.0500 (0.0500)  time: 0.7599  data: 0.2645  max mem: 15572
[2025-01-12 22:22:50,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:22:50,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 280/2689]  eta: 0:24:59  lr: 0.000020  min_lr: 0.000000  loss: 4.8915 (4.9526)  loss_scale: 32768.0000 (46528.2278)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.2171  max mem: 15572
Epoch: [2]  [ 290/2689]  eta: 0:24:56  lr: 0.000020  min_lr: 0.000000  loss: 4.9592 (4.9540)  loss_scale: 65536.0000 (47181.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1675  max mem: 15572
Epoch: [2]  [ 300/2689]  eta: 0:24:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9730 (4.9547)  loss_scale: 65536.0000 (47791.2027)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1390  max mem: 15572
Epoch: [2]  [ 310/2689]  eta: 0:24:47  lr: 0.000020  min_lr: 0.000000  loss: 4.9213 (4.9525)  loss_scale: 65536.0000 (48361.7749)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.1770  max mem: 15572
Epoch: [2]  [ 320/2689]  eta: 0:24:48  lr: 0.000020  min_lr: 0.000000  loss: 4.9419 (4.9529)  loss_scale: 65536.0000 (48896.7975)  weight_decay: 0.0500 (0.0500)  time: 0.7302  data: 0.2563  max mem: 15572
Epoch: [2]  [ 330/2689]  eta: 0:24:44  lr: 0.000020  min_lr: 0.000000  loss: 4.9426 (4.9519)  loss_scale: 65536.0000 (49399.4924)  weight_decay: 0.0500 (0.0500)  time: 0.6935  data: 0.2490  max mem: 15572
Epoch: [2]  [ 340/2689]  eta: 0:24:23  lr: 0.000020  min_lr: 0.000000  loss: 4.9414 (4.9518)  loss_scale: 65536.0000 (49872.7038)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1209  max mem: 15572
Epoch: [2]  [ 350/2689]  eta: 0:24:05  lr: 0.000020  min_lr: 0.000000  loss: 4.9593 (4.9518)  loss_scale: 65536.0000 (50318.9516)  weight_decay: 0.0500 (0.0500)  time: 0.4280  data: 0.0007  max mem: 15572
Epoch: [2]  [ 360/2689]  eta: 0:23:49  lr: 0.000020  min_lr: 0.000000  loss: 4.8971 (4.9498)  loss_scale: 65536.0000 (50740.4765)  weight_decay: 0.0500 (0.0500)  time: 0.4580  data: 0.0008  max mem: 15572
Epoch: [2]  [ 370/2689]  eta: 0:23:33  lr: 0.000020  min_lr: 0.000000  loss: 4.8402 (4.9487)  loss_scale: 65536.0000 (51139.2776)  weight_decay: 0.0500 (0.0500)  time: 0.4612  data: 0.0007  max mem: 15572
Epoch: [2]  [ 380/2689]  eta: 0:23:30  lr: 0.000020  min_lr: 0.000000  loss: 4.9749 (4.9493)  loss_scale: 65536.0000 (51517.1444)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0915  max mem: 15572
Epoch: [2]  [ 390/2689]  eta: 0:23:18  lr: 0.000020  min_lr: 0.000000  loss: 4.9749 (4.9493)  loss_scale: 65536.0000 (51875.6829)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1391  max mem: 15572
[2025-01-12 22:23:58,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5960
[2025-01-12 22:23:58,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:23:58,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 400/2689]  eta: 0:23:08  lr: 0.000020  min_lr: 0.000000  loss: 4.9244 (4.9486)  loss_scale: 65536.0000 (51644.3292)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.1037  max mem: 15572
Epoch: [2]  [ 410/2689]  eta: 0:23:02  lr: 0.000020  min_lr: 0.000000  loss: 4.9244 (4.9475)  loss_scale: 32768.0000 (51185.0511)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1407  max mem: 15572
Epoch: [2]  [ 420/2689]  eta: 0:22:54  lr: 0.000020  min_lr: 0.000000  loss: 4.9191 (4.9479)  loss_scale: 32768.0000 (50747.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1402  max mem: 15572
Epoch: [2]  [ 430/2689]  eta: 0:22:49  lr: 0.000020  min_lr: 0.000000  loss: 4.8890 (4.9455)  loss_scale: 32768.0000 (50330.4316)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1559  max mem: 15572
[2025-01-12 22:24:21,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=29, lr=[1.9630807869974867e-07, 1.9630807869974867e-07, 2.8044011242821245e-07, 2.8044011242821245e-07, 4.006287320403035e-07, 4.006287320403035e-07, 5.723267600575764e-07, 5.723267600575764e-07, 8.176096572251093e-07, 8.176096572251093e-07, 1.1680137960358705e-06, 1.1680137960358705e-06, 1.6685911371941007e-06, 1.6685911371941007e-06, 2.3837016245630013e-06, 2.3837016245630013e-06, 3.4052880350900016e-06, 3.4052880350900016e-06, 4.864697192985718e-06, 4.864697192985718e-06, 6.949567418551025e-06, 6.949567418551025e-06, 9.927953455072893e-06, 9.927953455072893e-06, 1.4182790650104135e-05, 1.4182790650104135e-05, 2.0261129500148765e-05, 2.0261129500148765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:24:21,416] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=27.735024077335645, CurrSamplesPerSec=29.04094622032892, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 440/2689]  eta: 0:22:45  lr: 0.000020  min_lr: 0.000000  loss: 4.8716 (4.9456)  loss_scale: 32768.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1912  max mem: 15572
Epoch: [2]  [ 450/2689]  eta: 0:22:40  lr: 0.000020  min_lr: 0.000000  loss: 4.9547 (4.9463)  loss_scale: 32768.0000 (49551.6098)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1805  max mem: 15572
Epoch: [2]  [ 460/2689]  eta: 0:22:32  lr: 0.000020  min_lr: 0.000000  loss: 4.9693 (4.9466)  loss_scale: 32768.0000 (49187.5401)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1380  max mem: 15572
Epoch: [2]  [ 470/2689]  eta: 0:22:31  lr: 0.000020  min_lr: 0.000000  loss: 4.9436 (4.9454)  loss_scale: 32768.0000 (48838.9299)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1836  max mem: 15572
Epoch: [2]  [ 480/2689]  eta: 0:22:25  lr: 0.000020  min_lr: 0.000000  loss: 4.8742 (4.9434)  loss_scale: 32768.0000 (48504.8150)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.2240  max mem: 15572
Epoch: [2]  [ 490/2689]  eta: 0:22:20  lr: 0.000020  min_lr: 0.000000  loss: 4.9438 (4.9445)  loss_scale: 32768.0000 (48184.3096)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1631  max mem: 15572
Epoch: [2]  [ 500/2689]  eta: 0:22:10  lr: 0.000020  min_lr: 0.000000  loss: 4.9776 (4.9449)  loss_scale: 32768.0000 (47876.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1105  max mem: 15572
Epoch: [2]  [ 510/2689]  eta: 0:22:06  lr: 0.000021  min_lr: 0.000000  loss: 4.8815 (4.9432)  loss_scale: 32768.0000 (47580.9315)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1415  max mem: 15572
Epoch: [2]  [ 520/2689]  eta: 0:21:56  lr: 0.000021  min_lr: 0.000000  loss: 4.8815 (4.9434)  loss_scale: 32768.0000 (47296.6142)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1351  max mem: 15572
[2025-01-12 22:25:15,822] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:25:15,823] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 530/2689]  eta: 0:21:50  lr: 0.000021  min_lr: 0.000000  loss: 4.9356 (4.9427)  loss_scale: 32768.0000 (47516.6855)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1116  max mem: 15572
Epoch: [2]  [ 540/2689]  eta: 0:21:43  lr: 0.000021  min_lr: 0.000000  loss: 4.9246 (4.9425)  loss_scale: 65536.0000 (47849.7597)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1478  max mem: 15572
Epoch: [2]  [ 550/2689]  eta: 0:21:38  lr: 0.000021  min_lr: 0.000000  loss: 4.8962 (4.9416)  loss_scale: 65536.0000 (48170.7441)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1702  max mem: 15572
Epoch: [2]  [ 560/2689]  eta: 0:21:30  lr: 0.000021  min_lr: 0.000000  loss: 4.8904 (4.9415)  loss_scale: 65536.0000 (48480.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1555  max mem: 15572
Epoch: [2]  [ 570/2689]  eta: 0:21:24  lr: 0.000021  min_lr: 0.000000  loss: 4.9054 (4.9409)  loss_scale: 65536.0000 (48778.9842)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1287  max mem: 15572
Epoch: [2]  [ 580/2689]  eta: 0:21:20  lr: 0.000021  min_lr: 0.000000  loss: 4.9335 (4.9416)  loss_scale: 65536.0000 (49067.4010)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1822  max mem: 15572
Epoch: [2]  [ 590/2689]  eta: 0:21:12  lr: 0.000021  min_lr: 0.000000  loss: 4.9301 (4.9411)  loss_scale: 65536.0000 (49346.0575)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1296  max mem: 15572
Epoch: [2]  [ 600/2689]  eta: 0:21:09  lr: 0.000021  min_lr: 0.000000  loss: 4.9259 (4.9412)  loss_scale: 65536.0000 (49615.4409)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1406  max mem: 15572
Epoch: [2]  [ 610/2689]  eta: 0:21:04  lr: 0.000021  min_lr: 0.000000  loss: 4.9181 (4.9410)  loss_scale: 65536.0000 (49876.0065)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.2101  max mem: 15572
Epoch: [2]  [ 620/2689]  eta: 0:20:55  lr: 0.000021  min_lr: 0.000000  loss: 4.8963 (4.9403)  loss_scale: 65536.0000 (50128.1804)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1390  max mem: 15572
Epoch: [2]  [ 630/2689]  eta: 0:20:51  lr: 0.000021  min_lr: 0.000000  loss: 4.9077 (4.9405)  loss_scale: 65536.0000 (50372.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1498  max mem: 15572
Epoch: [2]  [ 640/2689]  eta: 0:20:43  lr: 0.000021  min_lr: 0.000000  loss: 4.9498 (4.9407)  loss_scale: 65536.0000 (50608.9236)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1469  max mem: 15572
[2025-01-12 22:26:30,972] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6214
[2025-01-12 22:26:30,972] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:26:30,972] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 650/2689]  eta: 0:20:41  lr: 0.000021  min_lr: 0.000000  loss: 4.8337 (4.9386)  loss_scale: 65536.0000 (50687.2135)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1793  max mem: 15572
Epoch: [2]  [ 660/2689]  eta: 0:20:32  lr: 0.000021  min_lr: 0.000000  loss: 4.8271 (4.9377)  loss_scale: 32768.0000 (50416.1210)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1918  max mem: 15572
Epoch: [2]  [ 670/2689]  eta: 0:20:24  lr: 0.000021  min_lr: 0.000000  loss: 4.8995 (4.9378)  loss_scale: 32768.0000 (50153.1088)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0967  max mem: 15572
Epoch: [2]  [ 680/2689]  eta: 0:20:18  lr: 0.000021  min_lr: 0.000000  loss: 4.9293 (4.9373)  loss_scale: 32768.0000 (49897.8209)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1236  max mem: 15572
Epoch: [2]  [ 690/2689]  eta: 0:20:13  lr: 0.000021  min_lr: 0.000000  loss: 4.8396 (4.9359)  loss_scale: 32768.0000 (49649.9219)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1824  max mem: 15572
Epoch: [2]  [ 700/2689]  eta: 0:20:07  lr: 0.000021  min_lr: 0.000000  loss: 4.8396 (4.9355)  loss_scale: 32768.0000 (49409.0956)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1685  max mem: 15572
Epoch: [2]  [ 710/2689]  eta: 0:19:59  lr: 0.000021  min_lr: 0.000000  loss: 4.8847 (4.9351)  loss_scale: 32768.0000 (49175.0436)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1080  max mem: 15572
Epoch: [2]  [ 720/2689]  eta: 0:19:52  lr: 0.000021  min_lr: 0.000000  loss: 4.8847 (4.9344)  loss_scale: 32768.0000 (48947.4840)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0908  max mem: 15572
Epoch: [2]  [ 730/2689]  eta: 0:19:48  lr: 0.000021  min_lr: 0.000000  loss: 4.9296 (4.9346)  loss_scale: 32768.0000 (48726.1505)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1479  max mem: 15572
Epoch: [2]  [ 740/2689]  eta: 0:19:43  lr: 0.000021  min_lr: 0.000000  loss: 4.9060 (4.9338)  loss_scale: 32768.0000 (48510.7908)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.1875  max mem: 15572
Epoch: [2]  [ 750/2689]  eta: 0:19:37  lr: 0.000021  min_lr: 0.000000  loss: 4.9073 (4.9343)  loss_scale: 32768.0000 (48301.1664)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1864  max mem: 15572
Epoch: [2]  [ 760/2689]  eta: 0:19:28  lr: 0.000021  min_lr: 0.000000  loss: 4.9073 (4.9332)  loss_scale: 32768.0000 (48097.0512)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1401  max mem: 15572
Epoch: [2]  [ 770/2689]  eta: 0:19:22  lr: 0.000021  min_lr: 0.000000  loss: 4.8521 (4.9323)  loss_scale: 32768.0000 (47898.2309)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1115  max mem: 15572
[2025-01-12 22:27:48,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:27:48,292] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 780/2689]  eta: 0:19:15  lr: 0.000021  min_lr: 0.000000  loss: 4.8440 (4.9315)  loss_scale: 32768.0000 (47872.3278)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1488  max mem: 15572
Epoch: [2]  [ 790/2689]  eta: 0:19:08  lr: 0.000022  min_lr: 0.000000  loss: 4.8390 (4.9305)  loss_scale: 65536.0000 (48095.6359)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1466  max mem: 15572
[2025-01-12 22:27:58,628] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6360
[2025-01-12 22:27:58,628] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:27:58,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 800/2689]  eta: 0:19:02  lr: 0.000022  min_lr: 0.000000  loss: 4.8221 (4.9295)  loss_scale: 65536.0000 (48027.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1489  max mem: 15572
Epoch: [2]  [ 810/2689]  eta: 0:18:55  lr: 0.000022  min_lr: 0.000000  loss: 4.8946 (4.9291)  loss_scale: 32768.0000 (47838.8557)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1491  max mem: 15572
Epoch: [2]  [ 820/2689]  eta: 0:18:48  lr: 0.000022  min_lr: 0.000000  loss: 4.9095 (4.9288)  loss_scale: 32768.0000 (47655.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1267  max mem: 15572
Epoch: [2]  [ 830/2689]  eta: 0:18:42  lr: 0.000022  min_lr: 0.000000  loss: 4.9102 (4.9285)  loss_scale: 32768.0000 (47476.1396)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1163  max mem: 15572
Epoch: [2]  [ 840/2689]  eta: 0:18:35  lr: 0.000022  min_lr: 0.000000  loss: 4.9295 (4.9286)  loss_scale: 32768.0000 (47301.2509)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0818  max mem: 15572
Epoch: [2]  [ 850/2689]  eta: 0:18:28  lr: 0.000022  min_lr: 0.000000  loss: 4.9278 (4.9283)  loss_scale: 32768.0000 (47130.4724)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0812  max mem: 15572
Epoch: [2]  [ 860/2689]  eta: 0:18:23  lr: 0.000022  min_lr: 0.000000  loss: 4.9106 (4.9285)  loss_scale: 32768.0000 (46963.6609)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1584  max mem: 15572
Epoch: [2]  [ 870/2689]  eta: 0:18:17  lr: 0.000022  min_lr: 0.000000  loss: 4.9106 (4.9288)  loss_scale: 32768.0000 (46800.6797)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1791  max mem: 15572
Epoch: [2]  [ 880/2689]  eta: 0:18:09  lr: 0.000022  min_lr: 0.000000  loss: 4.9251 (4.9294)  loss_scale: 32768.0000 (46641.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1073  max mem: 15572
Epoch: [2]  [ 890/2689]  eta: 0:18:02  lr: 0.000022  min_lr: 0.000000  loss: 4.9899 (4.9297)  loss_scale: 32768.0000 (46485.6925)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0781  max mem: 15572
Epoch: [2]  [ 900/2689]  eta: 0:17:55  lr: 0.000022  min_lr: 0.000000  loss: 4.8865 (4.9289)  loss_scale: 32768.0000 (46333.4428)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0776  max mem: 15572
Epoch: [2]  [ 910/2689]  eta: 0:17:49  lr: 0.000022  min_lr: 0.000000  loss: 4.8648 (4.9282)  loss_scale: 32768.0000 (46184.5357)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1027  max mem: 15572
Epoch: [2]  [ 920/2689]  eta: 0:17:43  lr: 0.000022  min_lr: 0.000000  loss: 4.8696 (4.9280)  loss_scale: 32768.0000 (46038.8621)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1323  max mem: 15572
[2025-01-12 22:29:13,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:29:13,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 930/2689]  eta: 0:17:36  lr: 0.000022  min_lr: 0.000000  loss: 4.8037 (4.9267)  loss_scale: 32768.0000 (46177.8904)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1193  max mem: 15572
Epoch: [2]  [ 940/2689]  eta: 0:17:28  lr: 0.000022  min_lr: 0.000000  loss: 4.8487 (4.9266)  loss_scale: 65536.0000 (46383.6089)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1086  max mem: 15572
Epoch: [2]  [ 950/2689]  eta: 0:17:23  lr: 0.000022  min_lr: 0.000000  loss: 4.8932 (4.9260)  loss_scale: 65536.0000 (46585.0011)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1190  max mem: 15572
Epoch: [2]  [ 960/2689]  eta: 0:17:16  lr: 0.000022  min_lr: 0.000000  loss: 4.8545 (4.9255)  loss_scale: 65536.0000 (46782.2019)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1180  max mem: 15572
Epoch: [2]  [ 970/2689]  eta: 0:17:11  lr: 0.000022  min_lr: 0.000000  loss: 4.8232 (4.9246)  loss_scale: 65536.0000 (46975.3409)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1329  max mem: 15572
Epoch: [2]  [ 980/2689]  eta: 0:17:05  lr: 0.000022  min_lr: 0.000000  loss: 4.8033 (4.9239)  loss_scale: 65536.0000 (47164.5423)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1731  max mem: 15572
Epoch: [2]  [ 990/2689]  eta: 0:16:59  lr: 0.000022  min_lr: 0.000000  loss: 4.8153 (4.9229)  loss_scale: 65536.0000 (47349.9253)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1587  max mem: 15572
Epoch: [2]  [1000/2689]  eta: 0:16:55  lr: 0.000022  min_lr: 0.000000  loss: 4.8651 (4.9229)  loss_scale: 65536.0000 (47531.6044)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1546  max mem: 15572
Epoch: [2]  [1010/2689]  eta: 0:16:46  lr: 0.000022  min_lr: 0.000000  loss: 5.0089 (4.9241)  loss_scale: 65536.0000 (47709.6894)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0852  max mem: 15572
Epoch: [2]  [1020/2689]  eta: 0:16:39  lr: 0.000022  min_lr: 0.000000  loss: 4.9715 (4.9239)  loss_scale: 65536.0000 (47884.2860)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0295  max mem: 15572
Epoch: [2]  [1030/2689]  eta: 0:16:35  lr: 0.000022  min_lr: 0.000000  loss: 4.8797 (4.9228)  loss_scale: 65536.0000 (48055.4956)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1576  max mem: 15572
Epoch: [2]  [1040/2689]  eta: 0:16:27  lr: 0.000022  min_lr: 0.000000  loss: 4.8797 (4.9223)  loss_scale: 65536.0000 (48223.4159)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1290  max mem: 15572
Epoch: [2]  [1050/2689]  eta: 0:16:20  lr: 0.000022  min_lr: 0.000000  loss: 4.8820 (4.9216)  loss_scale: 65536.0000 (48388.1408)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0375  max mem: 15572
[2025-01-12 22:30:27,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:30:27,576] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1060/2689]  eta: 0:16:13  lr: 0.000022  min_lr: 0.000000  loss: 4.8735 (4.9215)  loss_scale: 65536.0000 (49167.4420)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0653  max mem: 15572
[2025-01-12 22:30:34,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6630
[2025-01-12 22:30:34,800] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:30:34,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1070/2689]  eta: 0:16:08  lr: 0.000022  min_lr: 0.000000  loss: 4.8702 (4.9213)  loss_scale: 131072.0000 (49503.8506)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1193  max mem: 15572
Epoch: [2]  [1080/2689]  eta: 0:16:00  lr: 0.000023  min_lr: 0.000000  loss: 4.8820 (4.9214)  loss_scale: 65536.0000 (49652.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0970  max mem: 15572
Epoch: [2]  [1090/2689]  eta: 0:15:55  lr: 0.000023  min_lr: 0.000000  loss: 4.9011 (4.9213)  loss_scale: 65536.0000 (49797.7489)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1298  max mem: 15572
Epoch: [2]  [1100/2689]  eta: 0:15:50  lr: 0.000023  min_lr: 0.000000  loss: 4.8609 (4.9208)  loss_scale: 65536.0000 (49940.6939)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.2193  max mem: 15572
Epoch: [2]  [1110/2689]  eta: 0:15:42  lr: 0.000023  min_lr: 0.000000  loss: 4.8467 (4.9198)  loss_scale: 65536.0000 (50081.0657)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1299  max mem: 15572
Epoch: [2]  [1120/2689]  eta: 0:15:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8735 (4.9200)  loss_scale: 65536.0000 (50218.9331)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1293  max mem: 15572
Epoch: [2]  [1130/2689]  eta: 0:15:31  lr: 0.000023  min_lr: 0.000000  loss: 4.8735 (4.9196)  loss_scale: 65536.0000 (50354.3625)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1858  max mem: 15572
Epoch: [2]  [1140/2689]  eta: 0:15:25  lr: 0.000023  min_lr: 0.000000  loss: 4.8759 (4.9197)  loss_scale: 65536.0000 (50487.4181)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1371  max mem: 15572
[2025-01-12 22:31:21,493] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6709
[2025-01-12 22:31:21,494] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:31:21,494] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1150/2689]  eta: 0:15:19  lr: 0.000023  min_lr: 0.000000  loss: 4.8759 (4.9193)  loss_scale: 65536.0000 (50390.4083)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1393  max mem: 15572
Epoch: [2]  [1160/2689]  eta: 0:15:13  lr: 0.000023  min_lr: 0.000000  loss: 4.8874 (4.9192)  loss_scale: 32768.0000 (50238.6219)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1530  max mem: 15572
Epoch: [2]  [1170/2689]  eta: 0:15:07  lr: 0.000023  min_lr: 0.000000  loss: 4.9075 (4.9186)  loss_scale: 32768.0000 (50089.4278)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1369  max mem: 15572
Epoch: [2]  [1180/2689]  eta: 0:15:01  lr: 0.000023  min_lr: 0.000000  loss: 4.8662 (4.9184)  loss_scale: 32768.0000 (49942.7604)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1257  max mem: 15572
Epoch: [2]  [1190/2689]  eta: 0:14:56  lr: 0.000023  min_lr: 0.000000  loss: 4.8095 (4.9171)  loss_scale: 32768.0000 (49798.5558)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1578  max mem: 15572
Epoch: [2]  [1200/2689]  eta: 0:14:49  lr: 0.000023  min_lr: 0.000000  loss: 4.8126 (4.9170)  loss_scale: 32768.0000 (49656.7527)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1603  max mem: 15572
Epoch: [2]  [1210/2689]  eta: 0:14:43  lr: 0.000023  min_lr: 0.000000  loss: 4.9031 (4.9168)  loss_scale: 32768.0000 (49517.2915)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0953  max mem: 15572
Epoch: [2]  [1220/2689]  eta: 0:14:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8436 (4.9161)  loss_scale: 32768.0000 (49380.1147)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1552  max mem: 15572
Epoch: [2]  [1230/2689]  eta: 0:14:31  lr: 0.000023  min_lr: 0.000000  loss: 4.8436 (4.9160)  loss_scale: 32768.0000 (49245.1665)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1382  max mem: 15572
Epoch: [2]  [1240/2689]  eta: 0:14:25  lr: 0.000023  min_lr: 0.000000  loss: 4.9243 (4.9158)  loss_scale: 32768.0000 (49112.3932)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1122  max mem: 15572
Epoch: [2]  [1250/2689]  eta: 0:14:17  lr: 0.000023  min_lr: 0.000000  loss: 4.9857 (4.9168)  loss_scale: 32768.0000 (48981.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0909  max mem: 15572
Epoch: [2]  [1260/2689]  eta: 0:14:12  lr: 0.000023  min_lr: 0.000000  loss: 4.8964 (4.9157)  loss_scale: 32768.0000 (48853.1642)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0921  max mem: 15572
Epoch: [2]  [1270/2689]  eta: 0:14:05  lr: 0.000023  min_lr: 0.000000  loss: 4.8030 (4.9151)  loss_scale: 32768.0000 (48726.6090)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1155  max mem: 15572
[2025-01-12 22:32:36,827] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:32:36,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1280/2689]  eta: 0:14:00  lr: 0.000023  min_lr: 0.000000  loss: 4.8035 (4.9146)  loss_scale: 32768.0000 (48832.2498)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1444  max mem: 15572
Epoch: [2]  [1290/2689]  eta: 0:13:53  lr: 0.000023  min_lr: 0.000000  loss: 4.8355 (4.9143)  loss_scale: 65536.0000 (48961.6359)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1668  max mem: 15572
Epoch: [2]  [1300/2689]  eta: 0:13:47  lr: 0.000023  min_lr: 0.000000  loss: 4.8923 (4.9142)  loss_scale: 65536.0000 (49089.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0965  max mem: 15572
Epoch: [2]  [1310/2689]  eta: 0:13:42  lr: 0.000023  min_lr: 0.000000  loss: 4.9017 (4.9139)  loss_scale: 65536.0000 (49214.4867)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1377  max mem: 15572
Epoch: [2]  [1320/2689]  eta: 0:13:36  lr: 0.000023  min_lr: 0.000000  loss: 4.7805 (4.9130)  loss_scale: 65536.0000 (49338.0409)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1656  max mem: 15572
Epoch: [2]  [1330/2689]  eta: 0:13:30  lr: 0.000023  min_lr: 0.000000  loss: 4.7810 (4.9124)  loss_scale: 65536.0000 (49459.7385)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1637  max mem: 15572
Epoch: [2]  [1340/2689]  eta: 0:13:24  lr: 0.000023  min_lr: 0.000000  loss: 4.9091 (4.9122)  loss_scale: 65536.0000 (49579.6212)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1420  max mem: 15572
Epoch: [2]  [1350/2689]  eta: 0:13:18  lr: 0.000023  min_lr: 0.000000  loss: 4.9193 (4.9119)  loss_scale: 65536.0000 (49697.7291)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1439  max mem: 15572
Epoch: [2]  [1360/2689]  eta: 0:13:12  lr: 0.000023  min_lr: 0.000000  loss: 4.8492 (4.9112)  loss_scale: 65536.0000 (49814.1014)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1821  max mem: 15572
Epoch: [2]  [1370/2689]  eta: 0:13:06  lr: 0.000024  min_lr: 0.000000  loss: 4.8458 (4.9106)  loss_scale: 65536.0000 (49928.7761)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1627  max mem: 15572
Epoch: [2]  [1380/2689]  eta: 0:13:00  lr: 0.000024  min_lr: 0.000000  loss: 4.8939 (4.9104)  loss_scale: 65536.0000 (50041.7900)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1478  max mem: 15572
Epoch: [2]  [1390/2689]  eta: 0:12:55  lr: 0.000024  min_lr: 0.000000  loss: 4.8939 (4.9100)  loss_scale: 65536.0000 (50153.1790)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1626  max mem: 15572
[2025-01-12 22:33:55,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:33:55,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1400/2689]  eta: 0:12:49  lr: 0.000024  min_lr: 0.000000  loss: 4.7466 (4.9089)  loss_scale: 65536.0000 (50309.7559)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.1804  max mem: 15572
[2025-01-12 22:33:56,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6967
[2025-01-12 22:33:56,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:33:56,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1410/2689]  eta: 0:12:43  lr: 0.000024  min_lr: 0.000000  loss: 4.8014 (4.9088)  loss_scale: 65536.0000 (50417.6669)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1601  max mem: 15572
Epoch: [2]  [1420/2689]  eta: 0:12:37  lr: 0.000024  min_lr: 0.000000  loss: 4.8875 (4.9090)  loss_scale: 65536.0000 (50524.0591)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1308  max mem: 15572
Epoch: [2]  [1430/2689]  eta: 0:12:31  lr: 0.000024  min_lr: 0.000000  loss: 4.8574 (4.9081)  loss_scale: 65536.0000 (50628.9644)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1016  max mem: 15572
[2025-01-12 22:34:14,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=34, lr=[2.300902295687469e-07, 2.300902295687469e-07, 3.2870032795535273e-07, 3.2870032795535273e-07, 4.695718970790754e-07, 4.695718970790754e-07, 6.708169958272506e-07, 6.708169958272506e-07, 9.583099940389295e-07, 9.583099940389295e-07, 1.3690142771984708e-06, 1.3690142771984708e-06, 1.955734681712101e-06, 1.955734681712101e-06, 2.793906688160145e-06, 2.793906688160145e-06, 3.991295268800207e-06, 3.991295268800207e-06, 5.7018503840002965e-06, 5.7018503840002965e-06, 8.14550054857185e-06, 8.14550054857185e-06, 1.1636429355102646e-05, 1.1636429355102646e-05, 1.6623470507289495e-05, 1.6623470507289495e-05, 2.3747815010413566e-05, 2.3747815010413566e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:34:14,984] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=27.7591282621628, CurrSamplesPerSec=30.24894796245479, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1440/2689]  eta: 0:12:24  lr: 0.000024  min_lr: 0.000000  loss: 4.7702 (4.9070)  loss_scale: 65536.0000 (50732.4136)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0939  max mem: 15572
Epoch: [2]  [1450/2689]  eta: 0:12:17  lr: 0.000024  min_lr: 0.000000  loss: 4.7555 (4.9062)  loss_scale: 65536.0000 (50834.4369)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0703  max mem: 15572
Epoch: [2]  [1460/2689]  eta: 0:12:11  lr: 0.000024  min_lr: 0.000000  loss: 4.8180 (4.9060)  loss_scale: 65536.0000 (50935.0637)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0939  max mem: 15572
Epoch: [2]  [1470/2689]  eta: 0:12:05  lr: 0.000024  min_lr: 0.000000  loss: 4.8557 (4.9056)  loss_scale: 65536.0000 (51034.3222)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1214  max mem: 15572
Epoch: [2]  [1480/2689]  eta: 0:12:00  lr: 0.000024  min_lr: 0.000000  loss: 4.8081 (4.9050)  loss_scale: 65536.0000 (51132.2404)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1687  max mem: 15572
Epoch: [2]  [1490/2689]  eta: 0:11:53  lr: 0.000024  min_lr: 0.000000  loss: 4.8081 (4.9046)  loss_scale: 65536.0000 (51228.8451)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1503  max mem: 15572
Epoch: [2]  [1500/2689]  eta: 0:11:47  lr: 0.000024  min_lr: 0.000000  loss: 4.8446 (4.9042)  loss_scale: 65536.0000 (51324.1626)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0700  max mem: 15572
Epoch: [2]  [1510/2689]  eta: 0:11:42  lr: 0.000024  min_lr: 0.000000  loss: 4.7527 (4.9037)  loss_scale: 65536.0000 (51418.2184)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1536  max mem: 15572
Epoch: [2]  [1520/2689]  eta: 0:11:36  lr: 0.000024  min_lr: 0.000000  loss: 4.7767 (4.9029)  loss_scale: 65536.0000 (51511.0375)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1542  max mem: 15572
[2025-01-12 22:35:09,488] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:35:09,488] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1530/2689]  eta: 0:11:29  lr: 0.000024  min_lr: 0.000000  loss: 4.8698 (4.9034)  loss_scale: 65536.0000 (51645.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0540  max mem: 15572
[2025-01-12 22:35:15,958] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7106
[2025-01-12 22:35:15,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:35:15,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1540/2689]  eta: 0:11:23  lr: 0.000024  min_lr: 0.000000  loss: 4.8931 (4.9027)  loss_scale: 65536.0000 (52118.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1061  max mem: 15572
Epoch: [2]  [1550/2689]  eta: 0:11:17  lr: 0.000024  min_lr: 0.000000  loss: 4.8888 (4.9028)  loss_scale: 65536.0000 (52204.8536)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1400  max mem: 15572
Epoch: [2]  [1560/2689]  eta: 0:11:11  lr: 0.000024  min_lr: 0.000000  loss: 4.8888 (4.9025)  loss_scale: 65536.0000 (52290.2550)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0993  max mem: 15572
Epoch: [2]  [1570/2689]  eta: 0:11:05  lr: 0.000024  min_lr: 0.000000  loss: 4.8688 (4.9020)  loss_scale: 65536.0000 (52374.5691)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1268  max mem: 15572
Epoch: [2]  [1580/2689]  eta: 0:10:59  lr: 0.000024  min_lr: 0.000000  loss: 4.7924 (4.9011)  loss_scale: 65536.0000 (52457.8166)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1364  max mem: 15572
Epoch: [2]  [1590/2689]  eta: 0:10:53  lr: 0.000024  min_lr: 0.000000  loss: 4.8285 (4.9012)  loss_scale: 65536.0000 (52540.0176)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1537  max mem: 15572
Epoch: [2]  [1600/2689]  eta: 0:10:47  lr: 0.000024  min_lr: 0.000000  loss: 4.9150 (4.9008)  loss_scale: 65536.0000 (52621.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1378  max mem: 15572
Epoch: [2]  [1610/2689]  eta: 0:10:40  lr: 0.000024  min_lr: 0.000000  loss: 4.8950 (4.9008)  loss_scale: 65536.0000 (52701.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0894  max mem: 15572
Epoch: [2]  [1620/2689]  eta: 0:10:35  lr: 0.000024  min_lr: 0.000000  loss: 4.8764 (4.9003)  loss_scale: 65536.0000 (52780.5355)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1076  max mem: 15572
Epoch: [2]  [1630/2689]  eta: 0:10:29  lr: 0.000024  min_lr: 0.000000  loss: 4.8651 (4.9002)  loss_scale: 65536.0000 (52858.7419)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1544  max mem: 15572
Epoch: [2]  [1640/2689]  eta: 0:10:22  lr: 0.000024  min_lr: 0.000000  loss: 4.8651 (4.9000)  loss_scale: 65536.0000 (52935.9951)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1279  max mem: 15572
Epoch: [2]  [1650/2689]  eta: 0:10:16  lr: 0.000025  min_lr: 0.000000  loss: 4.8639 (4.8997)  loss_scale: 65536.0000 (53012.3125)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0919  max mem: 15572
Epoch: [2]  [1660/2689]  eta: 0:10:11  lr: 0.000025  min_lr: 0.000000  loss: 4.8908 (4.8995)  loss_scale: 65536.0000 (53087.7110)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1689  max mem: 15572
[2025-01-12 22:36:30,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:36:30,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1670/2689]  eta: 0:10:05  lr: 0.000025  min_lr: 0.000000  loss: 4.8925 (4.8996)  loss_scale: 65536.0000 (53240.6463)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1601  max mem: 15572
[2025-01-12 22:36:35,896] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7244
[2025-01-12 22:36:35,897] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:36:35,897] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1680/2689]  eta: 0:09:59  lr: 0.000025  min_lr: 0.000000  loss: 4.8895 (4.8990)  loss_scale: 65536.0000 (53586.6936)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0845  max mem: 15572
Epoch: [2]  [1690/2689]  eta: 0:09:53  lr: 0.000025  min_lr: 0.000000  loss: 4.8131 (4.8985)  loss_scale: 65536.0000 (53657.3578)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1587  max mem: 15572
Epoch: [2]  [1700/2689]  eta: 0:09:47  lr: 0.000025  min_lr: 0.000000  loss: 4.7886 (4.8977)  loss_scale: 65536.0000 (53727.1911)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1889  max mem: 15572
Epoch: [2]  [1710/2689]  eta: 0:09:41  lr: 0.000025  min_lr: 0.000000  loss: 4.8060 (4.8974)  loss_scale: 65536.0000 (53796.2081)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1565  max mem: 15572
Epoch: [2]  [1720/2689]  eta: 0:09:35  lr: 0.000025  min_lr: 0.000000  loss: 4.8272 (4.8968)  loss_scale: 65536.0000 (53864.4230)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1672  max mem: 15572
Epoch: [2]  [1730/2689]  eta: 0:09:29  lr: 0.000025  min_lr: 0.000000  loss: 4.8526 (4.8967)  loss_scale: 65536.0000 (53931.8498)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1194  max mem: 15572
Epoch: [2]  [1740/2689]  eta: 0:09:23  lr: 0.000025  min_lr: 0.000000  loss: 4.8435 (4.8963)  loss_scale: 65536.0000 (53998.5020)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0944  max mem: 15572
Epoch: [2]  [1750/2689]  eta: 0:09:17  lr: 0.000025  min_lr: 0.000000  loss: 4.8107 (4.8958)  loss_scale: 65536.0000 (54064.3929)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1319  max mem: 15572
Epoch: [2]  [1760/2689]  eta: 0:09:12  lr: 0.000025  min_lr: 0.000000  loss: 4.8562 (4.8955)  loss_scale: 65536.0000 (54129.5355)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1573  max mem: 15572
Epoch: [2]  [1770/2689]  eta: 0:09:06  lr: 0.000025  min_lr: 0.000000  loss: 4.8084 (4.8951)  loss_scale: 65536.0000 (54193.9424)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1658  max mem: 15572
Epoch: [2]  [1780/2689]  eta: 0:09:00  lr: 0.000025  min_lr: 0.000000  loss: 4.8047 (4.8951)  loss_scale: 65536.0000 (54257.6261)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1490  max mem: 15572
Epoch: [2]  [1790/2689]  eta: 0:08:54  lr: 0.000025  min_lr: 0.000000  loss: 4.8439 (4.8949)  loss_scale: 65536.0000 (54320.5985)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1520  max mem: 15572
Epoch: [2]  [1800/2689]  eta: 0:08:48  lr: 0.000025  min_lr: 0.000000  loss: 4.8864 (4.8948)  loss_scale: 65536.0000 (54382.8717)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1623  max mem: 15572
[2025-01-12 22:37:55,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:37:55,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1810/2689]  eta: 0:08:43  lr: 0.000025  min_lr: 0.000000  loss: 4.8954 (4.8944)  loss_scale: 65536.0000 (54589.2082)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1788  max mem: 15572
[2025-01-12 22:37:57,266] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7377
[2025-01-12 22:37:57,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:37:57,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1820/2689]  eta: 0:08:37  lr: 0.000025  min_lr: 0.000000  loss: 4.9124 (4.8946)  loss_scale: 65536.0000 (54649.3224)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1775  max mem: 15572
Epoch: [2]  [1830/2689]  eta: 0:08:31  lr: 0.000025  min_lr: 0.000000  loss: 4.8743 (4.8943)  loss_scale: 65536.0000 (54708.7799)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1528  max mem: 15572
Epoch: [2]  [1840/2689]  eta: 0:08:25  lr: 0.000025  min_lr: 0.000000  loss: 4.8247 (4.8939)  loss_scale: 65536.0000 (54767.5915)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1375  max mem: 15572
Epoch: [2]  [1850/2689]  eta: 0:08:19  lr: 0.000025  min_lr: 0.000000  loss: 4.8471 (4.8939)  loss_scale: 65536.0000 (54825.7677)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0840  max mem: 15572
Epoch: [2]  [1860/2689]  eta: 0:08:13  lr: 0.000025  min_lr: 0.000000  loss: 4.8471 (4.8934)  loss_scale: 65536.0000 (54883.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0915  max mem: 15572
Epoch: [2]  [1870/2689]  eta: 0:08:07  lr: 0.000025  min_lr: 0.000000  loss: 4.8388 (4.8930)  loss_scale: 65536.0000 (54940.2544)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1964  max mem: 15572
Epoch: [2]  [1880/2689]  eta: 0:08:01  lr: 0.000025  min_lr: 0.000000  loss: 4.8319 (4.8924)  loss_scale: 65536.0000 (54996.5848)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1392  max mem: 15572
Epoch: [2]  [1890/2689]  eta: 0:07:55  lr: 0.000025  min_lr: 0.000000  loss: 4.8105 (4.8921)  loss_scale: 65536.0000 (55052.3194)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0850  max mem: 15572
Epoch: [2]  [1900/2689]  eta: 0:07:49  lr: 0.000025  min_lr: 0.000000  loss: 4.8105 (4.8922)  loss_scale: 65536.0000 (55107.4676)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1031  max mem: 15572
Epoch: [2]  [1910/2689]  eta: 0:07:43  lr: 0.000025  min_lr: 0.000000  loss: 4.8065 (4.8917)  loss_scale: 65536.0000 (55162.0387)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0942  max mem: 15572
Epoch: [2]  [1920/2689]  eta: 0:07:37  lr: 0.000025  min_lr: 0.000000  loss: 4.8333 (4.8917)  loss_scale: 65536.0000 (55216.0416)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1115  max mem: 15572
Epoch: [2]  [1930/2689]  eta: 0:07:31  lr: 0.000025  min_lr: 0.000000  loss: 4.8535 (4.8912)  loss_scale: 65536.0000 (55269.4852)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0933  max mem: 15572
[2025-01-12 22:39:09,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7502
[2025-01-12 22:39:09,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:39:09,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1940/2689]  eta: 0:07:24  lr: 0.000026  min_lr: 0.000000  loss: 4.8350 (4.8905)  loss_scale: 65536.0000 (55237.9681)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0671  max mem: 15572
Epoch: [2]  [1950/2689]  eta: 0:07:19  lr: 0.000026  min_lr: 0.000000  loss: 4.7879 (4.8902)  loss_scale: 32768.0000 (55122.7965)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1354  max mem: 15572
Epoch: [2]  [1960/2689]  eta: 0:07:13  lr: 0.000026  min_lr: 0.000000  loss: 4.8672 (4.8902)  loss_scale: 32768.0000 (55008.7996)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1705  max mem: 15572
Epoch: [2]  [1970/2689]  eta: 0:07:07  lr: 0.000026  min_lr: 0.000000  loss: 4.8726 (4.8899)  loss_scale: 32768.0000 (54895.9594)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0925  max mem: 15572
Epoch: [2]  [1980/2689]  eta: 0:07:00  lr: 0.000026  min_lr: 0.000000  loss: 4.8526 (4.8900)  loss_scale: 32768.0000 (54784.2585)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0459  max mem: 15572
Epoch: [2]  [1990/2689]  eta: 0:06:55  lr: 0.000026  min_lr: 0.000000  loss: 4.8066 (4.8896)  loss_scale: 32768.0000 (54673.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1275  max mem: 15572
Epoch: [2]  [2000/2689]  eta: 0:06:49  lr: 0.000026  min_lr: 0.000000  loss: 4.8322 (4.8896)  loss_scale: 32768.0000 (54564.2059)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.2402  max mem: 15572
Epoch: [2]  [2010/2689]  eta: 0:06:43  lr: 0.000026  min_lr: 0.000000  loss: 4.8811 (4.8894)  loss_scale: 32768.0000 (54455.8210)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1237  max mem: 15572
Epoch: [2]  [2020/2689]  eta: 0:06:37  lr: 0.000026  min_lr: 0.000000  loss: 4.7739 (4.8886)  loss_scale: 32768.0000 (54348.5087)  weight_decay: 0.0500 (0.0500)  time: 0.4707  data: 0.0086  max mem: 15572
Epoch: [2]  [2030/2689]  eta: 0:06:31  lr: 0.000026  min_lr: 0.000000  loss: 4.8099 (4.8888)  loss_scale: 32768.0000 (54242.2531)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1041  max mem: 15572
Epoch: [2]  [2040/2689]  eta: 0:06:25  lr: 0.000026  min_lr: 0.000000  loss: 4.8329 (4.8883)  loss_scale: 32768.0000 (54137.0387)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1540  max mem: 15572
Epoch: [2]  [2050/2689]  eta: 0:06:19  lr: 0.000026  min_lr: 0.000000  loss: 4.8140 (4.8875)  loss_scale: 32768.0000 (54032.8503)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1369  max mem: 15572
Epoch: [2]  [2060/2689]  eta: 0:06:13  lr: 0.000026  min_lr: 0.000000  loss: 4.7784 (4.8873)  loss_scale: 32768.0000 (53929.6730)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1420  max mem: 15572
[2025-01-12 22:40:25,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:40:25,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2070/2689]  eta: 0:06:07  lr: 0.000026  min_lr: 0.000000  loss: 4.8379 (4.8871)  loss_scale: 32768.0000 (53922.4259)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0694  max mem: 15572
Epoch: [2]  [2080/2689]  eta: 0:06:01  lr: 0.000026  min_lr: 0.000000  loss: 4.8824 (4.8871)  loss_scale: 65536.0000 (53978.2335)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1039  max mem: 15572
Epoch: [2]  [2090/2689]  eta: 0:05:55  lr: 0.000026  min_lr: 0.000000  loss: 4.8675 (4.8869)  loss_scale: 65536.0000 (54033.5074)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1252  max mem: 15572
Epoch: [2]  [2100/2689]  eta: 0:05:49  lr: 0.000026  min_lr: 0.000000  loss: 4.8499 (4.8865)  loss_scale: 65536.0000 (54088.2551)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0702  max mem: 15572
Epoch: [2]  [2110/2689]  eta: 0:05:43  lr: 0.000026  min_lr: 0.000000  loss: 4.7671 (4.8856)  loss_scale: 65536.0000 (54142.4841)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1365  max mem: 15572
Epoch: [2]  [2120/2689]  eta: 0:05:37  lr: 0.000026  min_lr: 0.000000  loss: 4.7555 (4.8852)  loss_scale: 65536.0000 (54196.2018)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1628  max mem: 15572
Epoch: [2]  [2130/2689]  eta: 0:05:31  lr: 0.000026  min_lr: 0.000000  loss: 4.7874 (4.8851)  loss_scale: 65536.0000 (54249.4153)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1505  max mem: 15572
Epoch: [2]  [2140/2689]  eta: 0:05:25  lr: 0.000026  min_lr: 0.000000  loss: 4.7414 (4.8845)  loss_scale: 65536.0000 (54302.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1321  max mem: 15572
Epoch: [2]  [2150/2689]  eta: 0:05:19  lr: 0.000026  min_lr: 0.000000  loss: 4.7414 (4.8843)  loss_scale: 65536.0000 (54354.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1282  max mem: 15572
Epoch: [2]  [2160/2689]  eta: 0:05:13  lr: 0.000026  min_lr: 0.000000  loss: 4.7733 (4.8836)  loss_scale: 65536.0000 (54406.1009)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1316  max mem: 15572
Epoch: [2]  [2170/2689]  eta: 0:05:07  lr: 0.000026  min_lr: 0.000000  loss: 4.7895 (4.8833)  loss_scale: 65536.0000 (54457.3671)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1366  max mem: 15572
Epoch: [2]  [2180/2689]  eta: 0:05:01  lr: 0.000026  min_lr: 0.000000  loss: 4.8233 (4.8830)  loss_scale: 65536.0000 (54508.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1238  max mem: 15572
Epoch: [2]  [2190/2689]  eta: 0:04:55  lr: 0.000026  min_lr: 0.000000  loss: 4.8238 (4.8827)  loss_scale: 65536.0000 (54558.4957)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0940  max mem: 15572
[2025-01-12 22:41:41,106] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:41:41,107] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:41:42,475] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7762
[2025-01-12 22:41:42,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:41:42,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2200/2689]  eta: 0:04:49  lr: 0.000026  min_lr: 0.000000  loss: 4.7920 (4.8823)  loss_scale: 65536.0000 (54697.6974)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1263  max mem: 15572
Epoch: [2]  [2210/2689]  eta: 0:04:44  lr: 0.000026  min_lr: 0.000000  loss: 4.7920 (4.8822)  loss_scale: 65536.0000 (54746.7173)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1402  max mem: 15572
Epoch: [2]  [2220/2689]  eta: 0:04:38  lr: 0.000026  min_lr: 0.000000  loss: 4.7965 (4.8818)  loss_scale: 65536.0000 (54795.2958)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1358  max mem: 15572
Epoch: [2]  [2230/2689]  eta: 0:04:32  lr: 0.000027  min_lr: 0.000000  loss: 4.8093 (4.8817)  loss_scale: 65536.0000 (54843.4388)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1118  max mem: 15572
Epoch: [2]  [2240/2689]  eta: 0:04:26  lr: 0.000027  min_lr: 0.000000  loss: 4.8379 (4.8812)  loss_scale: 65536.0000 (54891.1522)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1143  max mem: 15572
Epoch: [2]  [2250/2689]  eta: 0:04:20  lr: 0.000027  min_lr: 0.000000  loss: 4.7765 (4.8806)  loss_scale: 65536.0000 (54938.4416)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1037  max mem: 15572
Epoch: [2]  [2260/2689]  eta: 0:04:14  lr: 0.000027  min_lr: 0.000000  loss: 4.6875 (4.8801)  loss_scale: 65536.0000 (54985.3127)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1241  max mem: 15572
Epoch: [2]  [2270/2689]  eta: 0:04:08  lr: 0.000027  min_lr: 0.000000  loss: 4.7718 (4.8798)  loss_scale: 65536.0000 (55031.7710)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1091  max mem: 15572
Epoch: [2]  [2280/2689]  eta: 0:04:02  lr: 0.000027  min_lr: 0.000000  loss: 4.7598 (4.8795)  loss_scale: 65536.0000 (55077.8220)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0832  max mem: 15572
Epoch: [2]  [2290/2689]  eta: 0:03:56  lr: 0.000027  min_lr: 0.000000  loss: 4.7128 (4.8788)  loss_scale: 65536.0000 (55123.4710)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1684  max mem: 15572
Epoch: [2]  [2300/2689]  eta: 0:03:50  lr: 0.000027  min_lr: 0.000000  loss: 4.7227 (4.8782)  loss_scale: 65536.0000 (55168.7232)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1866  max mem: 15572
Epoch: [2]  [2310/2689]  eta: 0:03:44  lr: 0.000027  min_lr: 0.000000  loss: 4.8028 (4.8779)  loss_scale: 65536.0000 (55213.5837)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1808  max mem: 15572
Epoch: [2]  [2320/2689]  eta: 0:03:38  lr: 0.000027  min_lr: 0.000000  loss: 4.8119 (4.8774)  loss_scale: 65536.0000 (55258.0577)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1983  max mem: 15572
[2025-01-12 22:42:57,816] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:42:57,816] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:42:59,723] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7893
[2025-01-12 22:42:59,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:42:59,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2330/2689]  eta: 0:03:32  lr: 0.000027  min_lr: 0.000000  loss: 4.7889 (4.8769)  loss_scale: 65536.0000 (55358.3801)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1549  max mem: 15572
Epoch: [2]  [2340/2689]  eta: 0:03:26  lr: 0.000027  min_lr: 0.000000  loss: 4.7869 (4.8765)  loss_scale: 65536.0000 (55401.8556)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0885  max mem: 15572
Epoch: [2]  [2350/2689]  eta: 0:03:20  lr: 0.000027  min_lr: 0.000000  loss: 4.8143 (4.8766)  loss_scale: 65536.0000 (55444.9613)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0599  max mem: 15572
Epoch: [2]  [2360/2689]  eta: 0:03:14  lr: 0.000027  min_lr: 0.000000  loss: 4.8143 (4.8763)  loss_scale: 65536.0000 (55487.7018)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1100  max mem: 15572
[2025-01-12 22:43:20,773] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7932
[2025-01-12 22:43:20,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:43:20,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2370/2689]  eta: 0:03:09  lr: 0.000027  min_lr: 0.000000  loss: 4.8037 (4.8759)  loss_scale: 65536.0000 (55460.9802)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1680  max mem: 15572
Epoch: [2]  [2380/2689]  eta: 0:03:03  lr: 0.000027  min_lr: 0.000000  loss: 4.7681 (4.8756)  loss_scale: 32768.0000 (55365.6716)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1680  max mem: 15572
Epoch: [2]  [2390/2689]  eta: 0:02:57  lr: 0.000027  min_lr: 0.000000  loss: 4.7555 (4.8752)  loss_scale: 32768.0000 (55271.1602)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1377  max mem: 15572
Epoch: [2]  [2400/2689]  eta: 0:02:51  lr: 0.000027  min_lr: 0.000000  loss: 4.7716 (4.8749)  loss_scale: 32768.0000 (55177.4361)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1315  max mem: 15572
Epoch: [2]  [2410/2689]  eta: 0:02:45  lr: 0.000027  min_lr: 0.000000  loss: 4.8083 (4.8748)  loss_scale: 32768.0000 (55084.4894)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0861  max mem: 15572
Epoch: [2]  [2420/2689]  eta: 0:02:39  lr: 0.000027  min_lr: 0.000000  loss: 4.8184 (4.8746)  loss_scale: 32768.0000 (54992.3106)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0792  max mem: 15572
Epoch: [2]  [2430/2689]  eta: 0:02:33  lr: 0.000027  min_lr: 0.000000  loss: 4.8184 (4.8742)  loss_scale: 32768.0000 (54900.8902)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1548  max mem: 15572
[2025-01-12 22:44:00,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=41, lr=[2.6387238043774514e-07, 2.6387238043774514e-07, 3.769605434824931e-07, 3.769605434824931e-07, 5.385150621178473e-07, 5.385150621178473e-07, 7.693072315969248e-07, 7.693072315969248e-07, 1.0990103308527498e-06, 1.0990103308527498e-06, 1.5700147583610713e-06, 1.5700147583610713e-06, 2.2428782262301017e-06, 2.2428782262301017e-06, 3.2041117517572884e-06, 3.2041117517572884e-06, 4.577302502510412e-06, 4.577302502510412e-06, 6.539003575014876e-06, 6.539003575014876e-06, 9.341433678592679e-06, 9.341433678592679e-06, 1.33449052551324e-05, 1.33449052551324e-05, 1.9064150364474858e-05, 1.9064150364474858e-05, 2.723450052067837e-05, 2.723450052067837e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:44:00,672] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=27.74440371238683, CurrSamplesPerSec=30.681311285939483, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [2440/2689]  eta: 0:02:27  lr: 0.000027  min_lr: 0.000000  loss: 4.8556 (4.8746)  loss_scale: 32768.0000 (54810.2188)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1561  max mem: 15572
Epoch: [2]  [2450/2689]  eta: 0:02:21  lr: 0.000027  min_lr: 0.000000  loss: 4.8213 (4.8741)  loss_scale: 32768.0000 (54720.2872)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1122  max mem: 15572
Epoch: [2]  [2460/2689]  eta: 0:02:15  lr: 0.000027  min_lr: 0.000000  loss: 4.7198 (4.8735)  loss_scale: 32768.0000 (54631.0866)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0934  max mem: 15572
Epoch: [2]  [2470/2689]  eta: 0:02:09  lr: 0.000027  min_lr: 0.000000  loss: 4.8599 (4.8739)  loss_scale: 32768.0000 (54542.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1264  max mem: 15572
Epoch: [2]  [2480/2689]  eta: 0:02:03  lr: 0.000027  min_lr: 0.000000  loss: 4.8599 (4.8728)  loss_scale: 32768.0000 (54454.8424)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1608  max mem: 15572
Epoch: [2]  [2490/2689]  eta: 0:01:57  lr: 0.000027  min_lr: 0.000000  loss: 4.6422 (4.8725)  loss_scale: 32768.0000 (54367.7816)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1543  max mem: 15572
[2025-01-12 22:44:36,912] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:44:36,912] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2500/2689]  eta: 0:01:51  lr: 0.000027  min_lr: 0.000000  loss: 4.7627 (4.8721)  loss_scale: 32768.0000 (54360.0288)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1200  max mem: 15572
Epoch: [2]  [2510/2689]  eta: 0:01:45  lr: 0.000028  min_lr: 0.000000  loss: 4.7914 (4.8716)  loss_scale: 65536.0000 (54404.5368)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0713  max mem: 15572
Epoch: [2]  [2520/2689]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000000  loss: 4.8186 (4.8713)  loss_scale: 65536.0000 (54448.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0810  max mem: 15572
Epoch: [2]  [2530/2689]  eta: 0:01:34  lr: 0.000028  min_lr: 0.000000  loss: 4.8574 (4.8712)  loss_scale: 65536.0000 (54492.4978)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1157  max mem: 15572
Epoch: [2]  [2540/2689]  eta: 0:01:28  lr: 0.000028  min_lr: 0.000000  loss: 4.8144 (4.8707)  loss_scale: 65536.0000 (54535.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1054  max mem: 15572
Epoch: [2]  [2550/2689]  eta: 0:01:22  lr: 0.000028  min_lr: 0.000000  loss: 4.8144 (4.8705)  loss_scale: 65536.0000 (54579.0796)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1449  max mem: 15572
Epoch: [2]  [2560/2689]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000000  loss: 4.8383 (4.8705)  loss_scale: 65536.0000 (54621.8633)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1658  max mem: 15572
Epoch: [2]  [2570/2689]  eta: 0:01:10  lr: 0.000028  min_lr: 0.000000  loss: 4.8667 (4.8704)  loss_scale: 65536.0000 (54664.3143)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1665  max mem: 15572
Epoch: [2]  [2580/2689]  eta: 0:01:04  lr: 0.000028  min_lr: 0.000000  loss: 4.8656 (4.8702)  loss_scale: 65536.0000 (54706.4363)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1724  max mem: 15572
Epoch: [2]  [2590/2689]  eta: 0:00:58  lr: 0.000028  min_lr: 0.000000  loss: 4.7893 (4.8699)  loss_scale: 65536.0000 (54748.2331)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1705  max mem: 15572
Epoch: [2]  [2600/2689]  eta: 0:00:52  lr: 0.000028  min_lr: 0.000000  loss: 4.7304 (4.8694)  loss_scale: 65536.0000 (54789.7086)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1791  max mem: 15572
Epoch: [2]  [2610/2689]  eta: 0:00:46  lr: 0.000028  min_lr: 0.000000  loss: 4.7088 (4.8688)  loss_scale: 65536.0000 (54830.8663)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1810  max mem: 15572
Epoch: [2]  [2620/2689]  eta: 0:00:40  lr: 0.000028  min_lr: 0.000000  loss: 4.7820 (4.8687)  loss_scale: 65536.0000 (54871.7100)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1198  max mem: 15572
[2025-01-12 22:45:52,593] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:45:52,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:45:53,148] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8190
[2025-01-12 22:45:53,148] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:45:53,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2630/2689]  eta: 0:00:34  lr: 0.000028  min_lr: 0.000000  loss: 4.8466 (4.8687)  loss_scale: 65536.0000 (54937.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1072  max mem: 15572
Epoch: [2]  [2640/2689]  eta: 0:00:29  lr: 0.000028  min_lr: 0.000000  loss: 4.8169 (4.8686)  loss_scale: 65536.0000 (54977.2844)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1467  max mem: 15572
Epoch: [2]  [2650/2689]  eta: 0:00:23  lr: 0.000028  min_lr: 0.000000  loss: 4.7547 (4.8681)  loss_scale: 65536.0000 (55017.1135)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1449  max mem: 15572
Epoch: [2]  [2660/2689]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000000  loss: 4.7584 (4.8676)  loss_scale: 65536.0000 (55056.6434)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1667  max mem: 15572
Epoch: [2]  [2670/2689]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000000  loss: 4.7845 (4.8674)  loss_scale: 65536.0000 (55095.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1200  max mem: 15572
Epoch: [2]  [2680/2689]  eta: 0:00:05  lr: 0.000028  min_lr: 0.000000  loss: 4.8302 (4.8676)  loss_scale: 65536.0000 (55134.8184)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0302  max mem: 15572
Epoch: [2]  [2688/2689]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 4.9233 (4.8675)  loss_scale: 65536.0000 (55165.7627)  weight_decay: 0.0500 (0.0500)  time: 0.4218  data: 0.0005  max mem: 15572
Epoch: [2] Total time: 0:26:31 (0.5918 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 4.9233 (4.8675)  loss_scale: 65536.0000 (55165.7627)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1780
Indices to remove: tensor([   31,   326,   329,  ..., 33678, 33689, 33692], device='cuda:0')
length of data loader train is: 2540
num_training_steps_per_epoch is: 2540
Change step level LR scheduler!
Set warmup steps = 12700
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:24:02  loss: 4.0964 (4.0964)  acc1: 0.0000 (0.0000)  acc5: 38.8889 (38.8889)  time: 5.3047  data: 5.1310  max mem: 15572
Val:  [ 10/272]  eta: 0:03:18  loss: 5.0801 (4.7370)  acc1: 0.0000 (13.6364)  acc5: 0.0000 (21.7172)  time: 0.7563  data: 0.5840  max mem: 15572
Val:  [ 20/272]  eta: 0:02:31  loss: 4.8316 (4.6660)  acc1: 0.0000 (14.5503)  acc5: 0.0000 (19.3122)  time: 0.3648  data: 0.1883  max mem: 15572
Val:  [ 30/272]  eta: 0:01:59  loss: 4.7539 (4.6339)  acc1: 0.0000 (10.3943)  acc5: 0.0000 (20.2509)  time: 0.3465  data: 0.1669  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 4.1009 (4.4770)  acc1: 0.0000 (12.6016)  acc5: 22.2222 (26.5583)  time: 0.2635  data: 0.0760  max mem: 15572
Val:  [ 50/272]  eta: 0:01:34  loss: 4.0655 (4.4991)  acc1: 0.0000 (10.1307)  acc5: 27.7778 (27.5599)  time: 0.3179  data: 0.1257  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 4.0655 (4.4419)  acc1: 0.0000 (9.1075)  acc5: 22.2222 (29.4171)  time: 0.3424  data: 0.1459  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 4.1581 (4.3963)  acc1: 0.0000 (11.0329)  acc5: 22.2222 (29.8122)  time: 0.2851  data: 0.0902  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 4.3123 (4.4113)  acc1: 0.0000 (9.6708)  acc5: 0.0000 (29.2867)  time: 0.3044  data: 0.0977  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 4.7541 (4.4600)  acc1: 0.0000 (8.6081)  acc5: 0.0000 (26.1294)  time: 0.3103  data: 0.1002  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 4.7441 (4.4978)  acc1: 0.0000 (7.8108)  acc5: 0.0000 (24.8625)  time: 0.2974  data: 0.0981  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 4.7422 (4.5310)  acc1: 0.0000 (7.4074)  acc5: 0.0000 (24.0741)  time: 0.3102  data: 0.1076  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 4.7984 (4.5702)  acc1: 0.0000 (6.7952)  acc5: 0.0000 (22.1763)  time: 0.3173  data: 0.1212  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 4.7871 (4.5364)  acc1: 0.0000 (7.5488)  acc5: 0.0000 (24.5971)  time: 0.3505  data: 0.1620  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.3900 (4.5214)  acc1: 0.0000 (8.9046)  acc5: 5.5556 (24.8227)  time: 0.3370  data: 0.1328  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.4612 (4.5224)  acc1: 0.0000 (8.3149)  acc5: 0.0000 (23.6571)  time: 0.3178  data: 0.1008  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.3882 (4.5196)  acc1: 0.0000 (8.0400)  acc5: 5.5556 (23.1884)  time: 0.3110  data: 0.0948  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.4344 (4.5418)  acc1: 0.0000 (7.5699)  acc5: 11.1111 (22.5146)  time: 0.3205  data: 0.1124  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.7463 (4.5482)  acc1: 0.0000 (7.1516)  acc5: 0.0000 (21.4242)  time: 0.3487  data: 0.1492  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.8351 (4.5650)  acc1: 0.0000 (6.7772)  acc5: 0.0000 (20.3025)  time: 0.3307  data: 0.1277  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.8231 (4.5863)  acc1: 0.0000 (6.4400)  acc5: 0.0000 (19.2924)  time: 0.3468  data: 0.1487  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.8153 (4.6011)  acc1: 0.0000 (6.1348)  acc5: 0.0000 (18.6151)  time: 0.3430  data: 0.1480  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.6152 (4.5936)  acc1: 0.0000 (5.9075)  acc5: 0.0000 (18.7783)  time: 0.3455  data: 0.1477  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3503 (4.5846)  acc1: 0.0000 (5.7480)  acc5: 22.2222 (19.3843)  time: 0.3834  data: 0.1809  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.3503 (4.5850)  acc1: 0.0000 (5.7169)  acc5: 11.1111 (19.3177)  time: 0.3135  data: 0.1091  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.9154 (4.6100)  acc1: 0.0000 (5.4892)  acc5: 0.0000 (18.6587)  time: 0.2917  data: 0.0826  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.2670 (4.5727)  acc1: 0.0000 (6.1941)  acc5: 16.6667 (21.0089)  time: 0.3023  data: 0.1064  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.8546 (4.5717)  acc1: 0.0000 (5.9861)  acc5: 66.6667 (21.3407)  time: 0.2157  data: 0.0464  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.8546 (4.5732)  acc1: 0.0000 (5.9799)  acc5: 66.6667 (21.3393)  time: 0.2074  data: 0.0464  max mem: 15572
Val: Total time: 0:01:31 (0.3353 s / it)
* Acc@1 5.980 Acc@5 21.339 loss 4.573
Accuracy of the network on the 4883 val videos: 6.0%
[2025-01-12 22:48:01,024] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 22:48:01,028] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 22:48:01,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 22:48:03,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 22:48:03,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 5.98%
Epoch: [3]  [   0/2540]  eta: 6:41:20  lr: 0.000028  min_lr: 0.000000  loss: 4.6444 (4.6444)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4807  data: 8.9567  max mem: 15572
Epoch: [3]  [  10/2540]  eta: 0:59:24  lr: 0.000028  min_lr: 0.000000  loss: 4.8660 (4.8363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4090  data: 0.9370  max mem: 15572
Epoch: [3]  [  20/2540]  eta: 0:42:13  lr: 0.000028  min_lr: 0.000000  loss: 4.8183 (4.8309)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1298  max mem: 15572
Epoch: [3]  [  30/2540]  eta: 0:36:01  lr: 0.000028  min_lr: 0.000000  loss: 4.8394 (4.8494)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1216  max mem: 15572
Epoch: [3]  [  40/2540]  eta: 0:33:00  lr: 0.000028  min_lr: 0.000000  loss: 4.8264 (4.8333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1243  max mem: 15572
Epoch: [3]  [  50/2540]  eta: 0:31:28  lr: 0.000028  min_lr: 0.000000  loss: 4.8064 (4.8420)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1297  max mem: 15572
Epoch: [3]  [  60/2540]  eta: 0:29:44  lr: 0.000028  min_lr: 0.000000  loss: 4.8493 (4.8475)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0882  max mem: 15572
[2025-01-12 22:48:49,765] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:48:49,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:48:53,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8325
[2025-01-12 22:48:53,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:48:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [  70/2540]  eta: 0:28:45  lr: 0.000028  min_lr: 0.000000  loss: 4.8952 (4.8538)  loss_scale: 65536.0000 (71074.2535)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0787  max mem: 15572
Epoch: [3]  [  80/2540]  eta: 0:27:42  lr: 0.000028  min_lr: 0.000000  loss: 4.8572 (4.8498)  loss_scale: 65536.0000 (70390.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0743  max mem: 15572
Epoch: [3]  [  90/2540]  eta: 0:26:57  lr: 0.000028  min_lr: 0.000000  loss: 4.7811 (4.8430)  loss_scale: 65536.0000 (69857.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0647  max mem: 15572
Epoch: [3]  [ 100/2540]  eta: 0:26:29  lr: 0.000028  min_lr: 0.000000  loss: 4.8257 (4.8461)  loss_scale: 65536.0000 (69429.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1131  max mem: 15572
Epoch: [3]  [ 110/2540]  eta: 0:26:24  lr: 0.000029  min_lr: 0.000000  loss: 4.8313 (4.8399)  loss_scale: 65536.0000 (69078.4865)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1673  max mem: 15572
Epoch: [3]  [ 120/2540]  eta: 0:26:03  lr: 0.000029  min_lr: 0.000000  loss: 4.7755 (4.8363)  loss_scale: 65536.0000 (68785.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1715  max mem: 15572
Epoch: [3]  [ 130/2540]  eta: 0:25:59  lr: 0.000029  min_lr: 0.000000  loss: 4.7996 (4.8357)  loss_scale: 65536.0000 (68537.6489)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1663  max mem: 15572
Epoch: [3]  [ 140/2540]  eta: 0:25:35  lr: 0.000029  min_lr: 0.000000  loss: 4.7943 (4.8331)  loss_scale: 65536.0000 (68324.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1335  max mem: 15572
Epoch: [3]  [ 150/2540]  eta: 0:25:21  lr: 0.000029  min_lr: 0.000000  loss: 4.8440 (4.8362)  loss_scale: 65536.0000 (68140.0795)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1186  max mem: 15572
Epoch: [3]  [ 160/2540]  eta: 0:25:08  lr: 0.000029  min_lr: 0.000000  loss: 4.7784 (4.8290)  loss_scale: 65536.0000 (67978.3354)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1401  max mem: 15572
Epoch: [3]  [ 170/2540]  eta: 0:25:16  lr: 0.000029  min_lr: 0.000000  loss: 4.7725 (4.8290)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2144  max mem: 15572
Epoch: [3]  [ 180/2540]  eta: 0:24:54  lr: 0.000029  min_lr: 0.000000  loss: 4.8624 (4.8318)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1997  max mem: 15572
Epoch: [3]  [ 190/2540]  eta: 0:24:37  lr: 0.000029  min_lr: 0.000000  loss: 4.8161 (4.8288)  loss_scale: 65536.0000 (67594.7225)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0955  max mem: 15572
[2025-01-12 22:50:10,650] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:50:10,651] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 200/2540]  eta: 0:24:40  lr: 0.000029  min_lr: 0.000000  loss: 4.8041 (4.8267)  loss_scale: 65536.0000 (68144.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1822  max mem: 15572
[2025-01-12 22:50:13,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8460
[2025-01-12 22:50:13,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:50:13,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 210/2540]  eta: 0:24:29  lr: 0.000029  min_lr: 0.000000  loss: 4.7858 (4.8261)  loss_scale: 65536.0000 (69263.1659)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.2020  max mem: 15572
Epoch: [3]  [ 220/2540]  eta: 0:24:20  lr: 0.000029  min_lr: 0.000000  loss: 4.7780 (4.8222)  loss_scale: 65536.0000 (69094.5158)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1535  max mem: 15572
Epoch: [3]  [ 230/2540]  eta: 0:24:07  lr: 0.000029  min_lr: 0.000000  loss: 4.6468 (4.8157)  loss_scale: 65536.0000 (68940.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1449  max mem: 15572
Epoch: [3]  [ 240/2540]  eta: 0:24:02  lr: 0.000029  min_lr: 0.000000  loss: 4.6313 (4.8150)  loss_scale: 65536.0000 (68799.2033)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1595  max mem: 15572
Epoch: [3]  [ 250/2540]  eta: 0:23:54  lr: 0.000029  min_lr: 0.000000  loss: 4.7868 (4.8135)  loss_scale: 65536.0000 (68669.1952)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1646  max mem: 15572
Epoch: [3]  [ 260/2540]  eta: 0:23:51  lr: 0.000029  min_lr: 0.000000  loss: 4.8222 (4.8145)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1670  max mem: 15572
Epoch: [3]  [ 270/2540]  eta: 0:23:31  lr: 0.000029  min_lr: 0.000000  loss: 4.8395 (4.8167)  loss_scale: 65536.0000 (68437.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1155  max mem: 15572
Epoch: [3]  [ 280/2540]  eta: 0:23:27  lr: 0.000029  min_lr: 0.000000  loss: 4.9209 (4.8177)  loss_scale: 65536.0000 (68334.6904)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1087  max mem: 15572
Epoch: [3]  [ 290/2540]  eta: 0:23:09  lr: 0.000029  min_lr: 0.000000  loss: 4.7820 (4.8164)  loss_scale: 65536.0000 (68238.5155)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1161  max mem: 15572
Epoch: [3]  [ 300/2540]  eta: 0:22:47  lr: 0.000029  min_lr: 0.000000  loss: 4.7820 (4.8151)  loss_scale: 65536.0000 (68148.7309)  weight_decay: 0.0500 (0.0500)  time: 0.4410  data: 0.0223  max mem: 15572
Epoch: [3]  [ 310/2540]  eta: 0:22:32  lr: 0.000029  min_lr: 0.000000  loss: 4.8328 (4.8182)  loss_scale: 65536.0000 (68064.7203)  weight_decay: 0.0500 (0.0500)  time: 0.4466  data: 0.0007  max mem: 15572
Epoch: [3]  [ 320/2540]  eta: 0:22:22  lr: 0.000029  min_lr: 0.000000  loss: 4.8380 (4.8166)  loss_scale: 65536.0000 (67985.9439)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0390  max mem: 15572
Epoch: [3]  [ 330/2540]  eta: 0:22:19  lr: 0.000029  min_lr: 0.000000  loss: 4.7623 (4.8142)  loss_scale: 65536.0000 (67911.9275)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1332  max mem: 15572
[2025-01-12 22:51:26,479] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:51:26,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 340/2540]  eta: 0:22:18  lr: 0.000029  min_lr: 0.000000  loss: 4.7623 (4.8137)  loss_scale: 65536.0000 (69187.5660)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.1771  max mem: 15572
[2025-01-12 22:51:36,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8603
[2025-01-12 22:51:36,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:51:36,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 350/2540]  eta: 0:22:14  lr: 0.000029  min_lr: 0.000000  loss: 4.8192 (4.8134)  loss_scale: 131072.0000 (70390.5185)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.1659  max mem: 15572
Epoch: [3]  [ 360/2540]  eta: 0:22:14  lr: 0.000029  min_lr: 0.000000  loss: 4.8521 (4.8146)  loss_scale: 65536.0000 (70256.0443)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.2156  max mem: 15572
Epoch: [3]  [ 370/2540]  eta: 0:22:19  lr: 0.000029  min_lr: 0.000000  loss: 4.8521 (4.8147)  loss_scale: 65536.0000 (70128.8194)  weight_decay: 0.0500 (0.0500)  time: 0.7598  data: 0.3037  max mem: 15572
Epoch: [3]  [ 380/2540]  eta: 0:22:22  lr: 0.000030  min_lr: 0.000000  loss: 4.8300 (4.8126)  loss_scale: 65536.0000 (70008.2730)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.3115  max mem: 15572
Epoch: [3]  [ 390/2540]  eta: 0:22:15  lr: 0.000030  min_lr: 0.000000  loss: 4.7848 (4.8130)  loss_scale: 65536.0000 (69893.8926)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.2207  max mem: 15572
Epoch: [3]  [ 400/2540]  eta: 0:22:08  lr: 0.000030  min_lr: 0.000000  loss: 4.8135 (4.8112)  loss_scale: 65536.0000 (69785.2170)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1351  max mem: 15572
Epoch: [3]  [ 410/2540]  eta: 0:22:03  lr: 0.000030  min_lr: 0.000000  loss: 4.7966 (4.8099)  loss_scale: 65536.0000 (69681.8297)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1472  max mem: 15572
Epoch: [3]  [ 420/2540]  eta: 0:21:53  lr: 0.000030  min_lr: 0.000000  loss: 4.8204 (4.8114)  loss_scale: 65536.0000 (69583.3539)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1444  max mem: 15572
Epoch: [3]  [ 430/2540]  eta: 0:21:57  lr: 0.000030  min_lr: 0.000000  loss: 4.8204 (4.8105)  loss_scale: 65536.0000 (69489.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.2393  max mem: 15572
Epoch: [3]  [ 440/2540]  eta: 0:21:45  lr: 0.000030  min_lr: 0.000000  loss: 4.8384 (4.8119)  loss_scale: 65536.0000 (69399.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.2260  max mem: 15572
Epoch: [3]  [ 450/2540]  eta: 0:21:28  lr: 0.000030  min_lr: 0.000000  loss: 4.8462 (4.8133)  loss_scale: 65536.0000 (69314.1286)  weight_decay: 0.0500 (0.0500)  time: 0.4413  data: 0.0383  max mem: 15572
Epoch: [3]  [ 460/2540]  eta: 0:21:13  lr: 0.000030  min_lr: 0.000000  loss: 4.8538 (4.8139)  loss_scale: 65536.0000 (69232.1735)  weight_decay: 0.0500 (0.0500)  time: 0.4102  data: 0.0005  max mem: 15572
Epoch: [3]  [ 470/2540]  eta: 0:21:00  lr: 0.000030  min_lr: 0.000000  loss: 4.8203 (4.8132)  loss_scale: 65536.0000 (69153.6985)  weight_decay: 0.0500 (0.0500)  time: 0.4427  data: 0.0006  max mem: 15572
[2025-01-12 22:52:54,272] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:52:54,272] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:52:54,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8733
[2025-01-12 22:52:54,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:52:54,758] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 480/2540]  eta: 0:20:48  lr: 0.000030  min_lr: 0.000000  loss: 4.7733 (4.8124)  loss_scale: 65536.0000 (69214.7360)  weight_decay: 0.0500 (0.0500)  time: 0.4631  data: 0.0007  max mem: 15572
Epoch: [3]  [ 490/2540]  eta: 0:20:37  lr: 0.000030  min_lr: 0.000000  loss: 4.7889 (4.8122)  loss_scale: 65536.0000 (69139.8126)  weight_decay: 0.0500 (0.0500)  time: 0.4746  data: 0.0293  max mem: 15572
Epoch: [3]  [ 500/2540]  eta: 0:20:29  lr: 0.000030  min_lr: 0.000000  loss: 4.7889 (4.8119)  loss_scale: 65536.0000 (69067.8802)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0671  max mem: 15572
Epoch: [3]  [ 510/2540]  eta: 0:20:19  lr: 0.000030  min_lr: 0.000000  loss: 4.7508 (4.8102)  loss_scale: 65536.0000 (68998.7632)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0714  max mem: 15572
Epoch: [3]  [ 520/2540]  eta: 0:20:18  lr: 0.000030  min_lr: 0.000000  loss: 4.7772 (4.8102)  loss_scale: 65536.0000 (68932.2994)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1664  max mem: 15572
Epoch: [3]  [ 530/2540]  eta: 0:20:12  lr: 0.000030  min_lr: 0.000000  loss: 4.8145 (4.8095)  loss_scale: 65536.0000 (68868.3390)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.2043  max mem: 15572
Epoch: [3]  [ 540/2540]  eta: 0:20:04  lr: 0.000030  min_lr: 0.000000  loss: 4.7809 (4.8094)  loss_scale: 65536.0000 (68806.7431)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1278  max mem: 15572
Epoch: [3]  [ 550/2540]  eta: 0:19:57  lr: 0.000030  min_lr: 0.000000  loss: 4.8232 (4.8098)  loss_scale: 65536.0000 (68747.3829)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0826  max mem: 15572
Epoch: [3]  [ 560/2540]  eta: 0:19:52  lr: 0.000030  min_lr: 0.000000  loss: 4.8304 (4.8089)  loss_scale: 65536.0000 (68690.1390)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0906  max mem: 15572
Epoch: [3]  [ 570/2540]  eta: 0:19:42  lr: 0.000030  min_lr: 0.000000  loss: 4.7921 (4.8089)  loss_scale: 65536.0000 (68634.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0867  max mem: 15572
Epoch: [3]  [ 580/2540]  eta: 0:19:35  lr: 0.000030  min_lr: 0.000000  loss: 4.8107 (4.8095)  loss_scale: 65536.0000 (68581.5628)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0872  max mem: 15572
Epoch: [3]  [ 590/2540]  eta: 0:19:33  lr: 0.000030  min_lr: 0.000000  loss: 4.8268 (4.8095)  loss_scale: 65536.0000 (68530.0305)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1962  max mem: 15572
Epoch: [3]  [ 600/2540]  eta: 0:19:29  lr: 0.000030  min_lr: 0.000000  loss: 4.8050 (4.8093)  loss_scale: 65536.0000 (68480.2130)  weight_decay: 0.0500 (0.0500)  time: 0.6984  data: 0.2295  max mem: 15572
[2025-01-12 22:54:10,901] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:54:10,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 22:54:11,365] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8863
[2025-01-12 22:54:11,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:54:11,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 610/2540]  eta: 0:19:23  lr: 0.000030  min_lr: 0.000000  loss: 4.7856 (4.8094)  loss_scale: 65536.0000 (68539.2864)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1628  max mem: 15572
Epoch: [3]  [ 620/2540]  eta: 0:19:16  lr: 0.000030  min_lr: 0.000000  loss: 4.7850 (4.8097)  loss_scale: 65536.0000 (68490.9243)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1192  max mem: 15572
Epoch: [3]  [ 630/2540]  eta: 0:19:12  lr: 0.000030  min_lr: 0.000000  loss: 4.8586 (4.8105)  loss_scale: 65536.0000 (68444.0951)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1510  max mem: 15572
Epoch: [3]  [ 640/2540]  eta: 0:19:03  lr: 0.000030  min_lr: 0.000000  loss: 4.8779 (4.8105)  loss_scale: 65536.0000 (68398.7270)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1135  max mem: 15572
Epoch: [3]  [ 650/2540]  eta: 0:18:59  lr: 0.000031  min_lr: 0.000000  loss: 4.7629 (4.8108)  loss_scale: 65536.0000 (68354.7527)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1271  max mem: 15572
Epoch: [3]  [ 660/2540]  eta: 0:18:54  lr: 0.000031  min_lr: 0.000000  loss: 4.7918 (4.8113)  loss_scale: 65536.0000 (68312.1089)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.2108  max mem: 15572
Epoch: [3]  [ 670/2540]  eta: 0:18:49  lr: 0.000031  min_lr: 0.000000  loss: 4.7918 (4.8106)  loss_scale: 65536.0000 (68270.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1960  max mem: 15572
Epoch: [3]  [ 680/2540]  eta: 0:18:43  lr: 0.000031  min_lr: 0.000000  loss: 4.8302 (4.8106)  loss_scale: 65536.0000 (68230.5786)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1819  max mem: 15572
Epoch: [3]  [ 690/2540]  eta: 0:18:37  lr: 0.000031  min_lr: 0.000000  loss: 4.8091 (4.8110)  loss_scale: 65536.0000 (68191.5832)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1683  max mem: 15572
Epoch: [3]  [ 700/2540]  eta: 0:18:29  lr: 0.000031  min_lr: 0.000000  loss: 4.7681 (4.8113)  loss_scale: 65536.0000 (68153.7004)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1079  max mem: 15572
Epoch: [3]  [ 710/2540]  eta: 0:18:22  lr: 0.000031  min_lr: 0.000000  loss: 4.8324 (4.8124)  loss_scale: 65536.0000 (68116.8833)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0975  max mem: 15572
Epoch: [3]  [ 720/2540]  eta: 0:18:16  lr: 0.000031  min_lr: 0.000000  loss: 4.8324 (4.8126)  loss_scale: 65536.0000 (68081.0874)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1558  max mem: 15572
Epoch: [3]  [ 730/2540]  eta: 0:18:13  lr: 0.000031  min_lr: 0.000000  loss: 4.8051 (4.8125)  loss_scale: 65536.0000 (68046.2709)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.2101  max mem: 15572
[2025-01-12 22:55:30,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:55:30,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 740/2540]  eta: 0:18:07  lr: 0.000031  min_lr: 0.000000  loss: 4.8848 (4.8127)  loss_scale: 65536.0000 (68366.1646)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.1999  max mem: 15572
[2025-01-12 22:55:33,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8999
[2025-01-12 22:55:33,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:55:33,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-12 22:55:33,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=48, lr=[2.991302279131774e-07, 2.991302279131774e-07, 4.2732889701882496e-07, 4.2732889701882496e-07, 6.104698528840357e-07, 6.104698528840357e-07, 8.720997898343368e-07, 8.720997898343368e-07, 1.2458568426204812e-06, 1.2458568426204812e-06, 1.7797954894578303e-06, 1.7797954894578303e-06, 2.5425649849397577e-06, 2.5425649849397577e-06, 3.632235692771083e-06, 3.632235692771083e-06, 5.188908132530118e-06, 5.188908132530118e-06, 7.412725903614456e-06, 7.412725903614456e-06, 1.0589608433734936e-05, 1.0589608433734936e-05, 1.5128012048192767e-05, 1.5128012048192767e-05, 2.161144578313253e-05, 2.161144578313253e-05, 3.087349397590361e-05, 3.087349397590361e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 22:55:33,765] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=27.755028636956123, CurrSamplesPerSec=30.055527754920213, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 750/2540]  eta: 0:18:01  lr: 0.000031  min_lr: 0.000000  loss: 4.7754 (4.8128)  loss_scale: 65536.0000 (68590.2743)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1542  max mem: 15572
Epoch: [3]  [ 760/2540]  eta: 0:17:54  lr: 0.000031  min_lr: 0.000000  loss: 4.7440 (4.8124)  loss_scale: 65536.0000 (68550.1393)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1261  max mem: 15572
Epoch: [3]  [ 770/2540]  eta: 0:17:45  lr: 0.000031  min_lr: 0.000000  loss: 4.7439 (4.8117)  loss_scale: 65536.0000 (68511.0454)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0491  max mem: 15572
Epoch: [3]  [ 780/2540]  eta: 0:17:40  lr: 0.000031  min_lr: 0.000000  loss: 4.7817 (4.8124)  loss_scale: 65536.0000 (68472.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1060  max mem: 15572
[2025-01-12 22:55:56,262] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9038
[2025-01-12 22:55:56,262] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 22:55:56,262] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 790/2540]  eta: 0:17:32  lr: 0.000031  min_lr: 0.000000  loss: 4.7803 (4.8113)  loss_scale: 65536.0000 (68104.4147)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1180  max mem: 15572
Epoch: [3]  [ 800/2540]  eta: 0:17:24  lr: 0.000031  min_lr: 0.000000  loss: 4.7104 (4.8109)  loss_scale: 32768.0000 (67663.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0346  max mem: 15572
Epoch: [3]  [ 810/2540]  eta: 0:17:20  lr: 0.000031  min_lr: 0.000000  loss: 4.8022 (4.8111)  loss_scale: 32768.0000 (67232.9864)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1517  max mem: 15572
Epoch: [3]  [ 820/2540]  eta: 0:17:11  lr: 0.000031  min_lr: 0.000000  loss: 4.8005 (4.8107)  loss_scale: 32768.0000 (66813.1937)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1405  max mem: 15572
Epoch: [3]  [ 830/2540]  eta: 0:17:07  lr: 0.000031  min_lr: 0.000000  loss: 4.8107 (4.8106)  loss_scale: 32768.0000 (66403.5042)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1168  max mem: 15572
Epoch: [3]  [ 840/2540]  eta: 0:17:03  lr: 0.000031  min_lr: 0.000000  loss: 4.8432 (4.8106)  loss_scale: 32768.0000 (66003.5577)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.2121  max mem: 15572
Epoch: [3]  [ 850/2540]  eta: 0:16:55  lr: 0.000031  min_lr: 0.000000  loss: 4.7592 (4.8096)  loss_scale: 32768.0000 (65613.0106)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1325  max mem: 15572
Epoch: [3]  [ 860/2540]  eta: 0:16:49  lr: 0.000031  min_lr: 0.000000  loss: 4.7581 (4.8085)  loss_scale: 32768.0000 (65231.5354)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0946  max mem: 15572
Epoch: [3]  [ 870/2540]  eta: 0:16:42  lr: 0.000031  min_lr: 0.000000  loss: 4.7437 (4.8080)  loss_scale: 32768.0000 (64858.8197)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1200  max mem: 15572
Epoch: [3]  [ 880/2540]  eta: 0:16:37  lr: 0.000031  min_lr: 0.000000  loss: 4.8340 (4.8092)  loss_scale: 32768.0000 (64494.5653)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1423  max mem: 15572
Epoch: [3]  [ 890/2540]  eta: 0:16:29  lr: 0.000031  min_lr: 0.000000  loss: 4.8489 (4.8094)  loss_scale: 32768.0000 (64138.4871)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1131  max mem: 15572
Epoch: [3]  [ 900/2540]  eta: 0:16:24  lr: 0.000031  min_lr: 0.000000  loss: 4.7992 (4.8092)  loss_scale: 32768.0000 (63790.3130)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1034  max mem: 15572
Epoch: [3]  [ 910/2540]  eta: 0:16:17  lr: 0.000031  min_lr: 0.000000  loss: 4.7788 (4.8094)  loss_scale: 32768.0000 (63449.7827)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1325  max mem: 15572
[2025-01-12 22:57:11,762] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:57:11,763] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 920/2540]  eta: 0:16:11  lr: 0.000032  min_lr: 0.000000  loss: 4.7788 (4.8098)  loss_scale: 32768.0000 (63436.8556)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1214  max mem: 15572
Epoch: [3]  [ 930/2540]  eta: 0:16:05  lr: 0.000032  min_lr: 0.000000  loss: 4.7719 (4.8090)  loss_scale: 65536.0000 (63459.4028)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1379  max mem: 15572
Epoch: [3]  [ 940/2540]  eta: 0:15:59  lr: 0.000032  min_lr: 0.000000  loss: 4.7719 (4.8083)  loss_scale: 65536.0000 (63481.4708)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1370  max mem: 15572
Epoch: [3]  [ 950/2540]  eta: 0:15:54  lr: 0.000032  min_lr: 0.000000  loss: 4.7913 (4.8076)  loss_scale: 65536.0000 (63503.0747)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1428  max mem: 15572
Epoch: [3]  [ 960/2540]  eta: 0:15:47  lr: 0.000032  min_lr: 0.000000  loss: 4.7890 (4.8076)  loss_scale: 65536.0000 (63524.2289)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1269  max mem: 15572
Epoch: [3]  [ 970/2540]  eta: 0:15:40  lr: 0.000032  min_lr: 0.000000  loss: 4.7704 (4.8076)  loss_scale: 65536.0000 (63544.9475)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1213  max mem: 15572
Epoch: [3]  [ 980/2540]  eta: 0:15:34  lr: 0.000032  min_lr: 0.000000  loss: 4.7610 (4.8070)  loss_scale: 65536.0000 (63565.2436)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1357  max mem: 15572
Epoch: [3]  [ 990/2540]  eta: 0:15:27  lr: 0.000032  min_lr: 0.000000  loss: 4.7414 (4.8066)  loss_scale: 65536.0000 (63585.1302)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0913  max mem: 15572
Epoch: [3]  [1000/2540]  eta: 0:15:22  lr: 0.000032  min_lr: 0.000000  loss: 4.7748 (4.8059)  loss_scale: 65536.0000 (63604.6194)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1171  max mem: 15572
Epoch: [3]  [1010/2540]  eta: 0:15:14  lr: 0.000032  min_lr: 0.000000  loss: 4.7521 (4.8058)  loss_scale: 65536.0000 (63623.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0913  max mem: 15572
Epoch: [3]  [1020/2540]  eta: 0:15:10  lr: 0.000032  min_lr: 0.000000  loss: 4.8372 (4.8065)  loss_scale: 65536.0000 (63642.4525)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1409  max mem: 15572
Epoch: [3]  [1030/2540]  eta: 0:15:04  lr: 0.000032  min_lr: 0.000000  loss: 4.8372 (4.8061)  loss_scale: 65536.0000 (63660.8186)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.2086  max mem: 15572
[2025-01-12 22:58:28,295] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:58:28,295] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1040/2540]  eta: 0:14:58  lr: 0.000032  min_lr: 0.000000  loss: 4.6889 (4.8049)  loss_scale: 65536.0000 (63741.7867)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1552  max mem: 15572
[2025-01-12 22:58:29,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9297
[2025-01-12 22:58:29,191] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:58:29,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1050/2540]  eta: 0:14:54  lr: 0.000032  min_lr: 0.000000  loss: 4.7016 (4.8045)  loss_scale: 65536.0000 (63821.2141)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.2021  max mem: 15572
Epoch: [3]  [1060/2540]  eta: 0:14:46  lr: 0.000032  min_lr: 0.000000  loss: 4.8302 (4.8051)  loss_scale: 65536.0000 (63837.3761)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1547  max mem: 15572
Epoch: [3]  [1070/2540]  eta: 0:14:42  lr: 0.000032  min_lr: 0.000000  loss: 4.8419 (4.8047)  loss_scale: 65536.0000 (63853.2362)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1504  max mem: 15572
Epoch: [3]  [1080/2540]  eta: 0:14:36  lr: 0.000032  min_lr: 0.000000  loss: 4.7675 (4.8046)  loss_scale: 65536.0000 (63868.8030)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1771  max mem: 15572
Epoch: [3]  [1090/2540]  eta: 0:14:30  lr: 0.000032  min_lr: 0.000000  loss: 4.6661 (4.8033)  loss_scale: 65536.0000 (63884.0843)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1508  max mem: 15572
Epoch: [3]  [1100/2540]  eta: 0:14:23  lr: 0.000032  min_lr: 0.000000  loss: 4.6661 (4.8030)  loss_scale: 65536.0000 (63899.0881)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1441  max mem: 15572
Epoch: [3]  [1110/2540]  eta: 0:14:18  lr: 0.000032  min_lr: 0.000000  loss: 4.7542 (4.8023)  loss_scale: 65536.0000 (63913.8218)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1542  max mem: 15572
Epoch: [3]  [1120/2540]  eta: 0:14:12  lr: 0.000032  min_lr: 0.000000  loss: 4.7542 (4.8016)  loss_scale: 65536.0000 (63928.2926)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1806  max mem: 15572
Epoch: [3]  [1130/2540]  eta: 0:14:06  lr: 0.000032  min_lr: 0.000000  loss: 4.7836 (4.8019)  loss_scale: 65536.0000 (63942.5075)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1497  max mem: 15572
Epoch: [3]  [1140/2540]  eta: 0:14:00  lr: 0.000032  min_lr: 0.000000  loss: 4.8689 (4.8026)  loss_scale: 65536.0000 (63956.4733)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1067  max mem: 15572
Epoch: [3]  [1150/2540]  eta: 0:13:53  lr: 0.000032  min_lr: 0.000000  loss: 4.7684 (4.8020)  loss_scale: 65536.0000 (63970.1964)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0942  max mem: 15572
Epoch: [3]  [1160/2540]  eta: 0:13:48  lr: 0.000032  min_lr: 0.000000  loss: 4.7315 (4.8023)  loss_scale: 65536.0000 (63983.6830)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1565  max mem: 15572
Epoch: [3]  [1170/2540]  eta: 0:13:41  lr: 0.000032  min_lr: 0.000000  loss: 4.8028 (4.8023)  loss_scale: 65536.0000 (63996.9394)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1513  max mem: 15572
[2025-01-12 22:59:47,329] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 22:59:47,329] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1180/2540]  eta: 0:13:36  lr: 0.000032  min_lr: 0.000000  loss: 4.8300 (4.8019)  loss_scale: 65536.0000 (64564.8908)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1563  max mem: 15572
[2025-01-12 22:59:55,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9438
[2025-01-12 22:59:55,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 22:59:55,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1190/2540]  eta: 0:13:29  lr: 0.000033  min_lr: 0.000000  loss: 4.7389 (4.8012)  loss_scale: 131072.0000 (64683.0966)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1456  max mem: 15572
Epoch: [3]  [1200/2540]  eta: 0:13:23  lr: 0.000033  min_lr: 0.000000  loss: 4.7247 (4.8011)  loss_scale: 65536.0000 (64690.1982)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0800  max mem: 15572
Epoch: [3]  [1210/2540]  eta: 0:13:16  lr: 0.000033  min_lr: 0.000000  loss: 4.7863 (4.8016)  loss_scale: 65536.0000 (64697.1825)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1060  max mem: 15572
Epoch: [3]  [1220/2540]  eta: 0:13:11  lr: 0.000033  min_lr: 0.000000  loss: 4.7051 (4.8006)  loss_scale: 65536.0000 (64704.0524)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1583  max mem: 15572
Epoch: [3]  [1230/2540]  eta: 0:13:05  lr: 0.000033  min_lr: 0.000000  loss: 4.7227 (4.8005)  loss_scale: 65536.0000 (64710.8107)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1568  max mem: 15572
Epoch: [3]  [1240/2540]  eta: 0:12:58  lr: 0.000033  min_lr: 0.000000  loss: 4.8494 (4.8005)  loss_scale: 65536.0000 (64717.4601)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0833  max mem: 15572
Epoch: [3]  [1250/2540]  eta: 0:12:51  lr: 0.000033  min_lr: 0.000000  loss: 4.7386 (4.7995)  loss_scale: 65536.0000 (64724.0032)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0627  max mem: 15572
Epoch: [3]  [1260/2540]  eta: 0:12:46  lr: 0.000033  min_lr: 0.000000  loss: 4.7306 (4.7993)  loss_scale: 65536.0000 (64730.4425)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1351  max mem: 15572
Epoch: [3]  [1270/2540]  eta: 0:12:39  lr: 0.000033  min_lr: 0.000000  loss: 4.7701 (4.7995)  loss_scale: 65536.0000 (64736.7805)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1068  max mem: 15572
Epoch: [3]  [1280/2540]  eta: 0:12:32  lr: 0.000033  min_lr: 0.000000  loss: 4.7757 (4.7999)  loss_scale: 65536.0000 (64743.0195)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0225  max mem: 15572
Epoch: [3]  [1290/2540]  eta: 0:12:26  lr: 0.000033  min_lr: 0.000000  loss: 4.7692 (4.7993)  loss_scale: 65536.0000 (64749.1619)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0789  max mem: 15572
Epoch: [3]  [1300/2540]  eta: 0:12:20  lr: 0.000033  min_lr: 0.000000  loss: 4.7075 (4.7986)  loss_scale: 65536.0000 (64755.2098)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0883  max mem: 15572
Epoch: [3]  [1310/2540]  eta: 0:12:13  lr: 0.000033  min_lr: 0.000000  loss: 4.7324 (4.7986)  loss_scale: 65536.0000 (64761.1655)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0774  max mem: 15572
[2025-01-12 23:01:07,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:01:07,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:01:09,554] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9570
[2025-01-12 23:01:09,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:01:09,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1320/2540]  eta: 0:12:07  lr: 0.000033  min_lr: 0.000000  loss: 4.8098 (4.7988)  loss_scale: 65536.0000 (64915.8637)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0766  max mem: 15572
Epoch: [3]  [1330/2540]  eta: 0:12:01  lr: 0.000033  min_lr: 0.000000  loss: 4.7294 (4.7981)  loss_scale: 65536.0000 (64920.5229)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0940  max mem: 15572
Epoch: [3]  [1340/2540]  eta: 0:11:54  lr: 0.000033  min_lr: 0.000000  loss: 4.6503 (4.7969)  loss_scale: 65536.0000 (64925.1126)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1328  max mem: 15572
Epoch: [3]  [1350/2540]  eta: 0:11:48  lr: 0.000033  min_lr: 0.000000  loss: 4.7324 (4.7970)  loss_scale: 65536.0000 (64929.6343)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0976  max mem: 15572
Epoch: [3]  [1360/2540]  eta: 0:11:43  lr: 0.000033  min_lr: 0.000000  loss: 4.7499 (4.7963)  loss_scale: 65536.0000 (64934.0896)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1307  max mem: 15572
Epoch: [3]  [1370/2540]  eta: 0:11:36  lr: 0.000033  min_lr: 0.000000  loss: 4.7505 (4.7968)  loss_scale: 65536.0000 (64938.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1222  max mem: 15572
Epoch: [3]  [1380/2540]  eta: 0:11:30  lr: 0.000033  min_lr: 0.000000  loss: 4.8086 (4.7970)  loss_scale: 65536.0000 (64942.8067)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0903  max mem: 15572
Epoch: [3]  [1390/2540]  eta: 0:11:24  lr: 0.000033  min_lr: 0.000000  loss: 4.6672 (4.7955)  loss_scale: 65536.0000 (64947.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1254  max mem: 15572
Epoch: [3]  [1400/2540]  eta: 0:11:18  lr: 0.000033  min_lr: 0.000000  loss: 4.6365 (4.7954)  loss_scale: 65536.0000 (64951.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1318  max mem: 15572
Epoch: [3]  [1410/2540]  eta: 0:11:12  lr: 0.000033  min_lr: 0.000000  loss: 4.7165 (4.7953)  loss_scale: 65536.0000 (64955.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1466  max mem: 15572
Epoch: [3]  [1420/2540]  eta: 0:11:06  lr: 0.000033  min_lr: 0.000000  loss: 4.7853 (4.7956)  loss_scale: 65536.0000 (64959.5046)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1465  max mem: 15572
Epoch: [3]  [1430/2540]  eta: 0:11:01  lr: 0.000033  min_lr: 0.000000  loss: 4.7967 (4.7955)  loss_scale: 65536.0000 (64963.5332)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1686  max mem: 15572
Epoch: [3]  [1440/2540]  eta: 0:10:54  lr: 0.000033  min_lr: 0.000000  loss: 4.8002 (4.7953)  loss_scale: 65536.0000 (64967.5059)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1584  max mem: 15572
[2025-01-12 23:02:24,542] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:02:24,543] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:02:25,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9702
[2025-01-12 23:02:25,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:02:25,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1450/2540]  eta: 0:10:49  lr: 0.000033  min_lr: 0.000000  loss: 4.8017 (4.7954)  loss_scale: 65536.0000 (65106.9221)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1482  max mem: 15572
Epoch: [3]  [1460/2540]  eta: 0:10:42  lr: 0.000034  min_lr: 0.000000  loss: 4.7911 (4.7952)  loss_scale: 65536.0000 (65109.8590)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1022  max mem: 15572
Epoch: [3]  [1470/2540]  eta: 0:10:37  lr: 0.000034  min_lr: 0.000000  loss: 4.7387 (4.7946)  loss_scale: 65536.0000 (65112.7559)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1189  max mem: 15572
Epoch: [3]  [1480/2540]  eta: 0:10:31  lr: 0.000034  min_lr: 0.000000  loss: 4.7534 (4.7946)  loss_scale: 65536.0000 (65115.6138)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.2059  max mem: 15572
Epoch: [3]  [1490/2540]  eta: 0:10:25  lr: 0.000034  min_lr: 0.000000  loss: 4.7142 (4.7943)  loss_scale: 65536.0000 (65118.4333)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1568  max mem: 15572
Epoch: [3]  [1500/2540]  eta: 0:10:19  lr: 0.000034  min_lr: 0.000000  loss: 4.7052 (4.7942)  loss_scale: 65536.0000 (65121.2152)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1331  max mem: 15572
Epoch: [3]  [1510/2540]  eta: 0:10:13  lr: 0.000034  min_lr: 0.000000  loss: 4.7206 (4.7940)  loss_scale: 65536.0000 (65123.9603)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1478  max mem: 15572
Epoch: [3]  [1520/2540]  eta: 0:10:07  lr: 0.000034  min_lr: 0.000000  loss: 4.7119 (4.7935)  loss_scale: 65536.0000 (65126.6693)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1677  max mem: 15572
Epoch: [3]  [1530/2540]  eta: 0:10:02  lr: 0.000034  min_lr: 0.000000  loss: 4.7360 (4.7931)  loss_scale: 65536.0000 (65129.3429)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1918  max mem: 15572
Epoch: [3]  [1540/2540]  eta: 0:09:55  lr: 0.000034  min_lr: 0.000000  loss: 4.7407 (4.7925)  loss_scale: 65536.0000 (65131.9818)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1030  max mem: 15572
Epoch: [3]  [1550/2540]  eta: 0:09:49  lr: 0.000034  min_lr: 0.000000  loss: 4.7744 (4.7924)  loss_scale: 65536.0000 (65134.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0493  max mem: 15572
Epoch: [3]  [1560/2540]  eta: 0:09:43  lr: 0.000034  min_lr: 0.000000  loss: 4.7744 (4.7923)  loss_scale: 65536.0000 (65137.1582)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1107  max mem: 15572
Epoch: [3]  [1570/2540]  eta: 0:09:37  lr: 0.000034  min_lr: 0.000000  loss: 4.6918 (4.7920)  loss_scale: 65536.0000 (65139.6970)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1166  max mem: 15572
[2025-01-12 23:03:42,251] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:03:42,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1580/2540]  eta: 0:09:31  lr: 0.000034  min_lr: 0.000000  loss: 4.7323 (4.7916)  loss_scale: 65536.0000 (65349.4649)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1124  max mem: 15572
[2025-01-12 23:03:46,861] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9838
[2025-01-12 23:03:46,862] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:03:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1590/2540]  eta: 0:09:25  lr: 0.000034  min_lr: 0.000000  loss: 4.8044 (4.7917)  loss_scale: 65536.0000 (65433.0207)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1516  max mem: 15572
Epoch: [3]  [1600/2540]  eta: 0:09:19  lr: 0.000034  min_lr: 0.000000  loss: 4.8597 (4.7918)  loss_scale: 65536.0000 (65433.6640)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1630  max mem: 15572
Epoch: [3]  [1610/2540]  eta: 0:09:13  lr: 0.000034  min_lr: 0.000000  loss: 4.8362 (4.7917)  loss_scale: 65536.0000 (65434.2992)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1697  max mem: 15572
Epoch: [3]  [1620/2540]  eta: 0:09:07  lr: 0.000034  min_lr: 0.000000  loss: 4.7812 (4.7914)  loss_scale: 65536.0000 (65434.9266)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1465  max mem: 15572
Epoch: [3]  [1630/2540]  eta: 0:09:01  lr: 0.000034  min_lr: 0.000000  loss: 4.7812 (4.7914)  loss_scale: 65536.0000 (65435.5463)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1079  max mem: 15572
Epoch: [3]  [1640/2540]  eta: 0:08:56  lr: 0.000034  min_lr: 0.000000  loss: 4.6939 (4.7903)  loss_scale: 65536.0000 (65436.1584)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1729  max mem: 15572
Epoch: [3]  [1650/2540]  eta: 0:08:50  lr: 0.000034  min_lr: 0.000000  loss: 4.6844 (4.7899)  loss_scale: 65536.0000 (65436.7632)  weight_decay: 0.0500 (0.0500)  time: 0.6717  data: 0.2118  max mem: 15572
[2025-01-12 23:04:32,640] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9912
[2025-01-12 23:04:32,640] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:04:32,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1660/2540]  eta: 0:08:44  lr: 0.000034  min_lr: 0.000000  loss: 4.6896 (4.7901)  loss_scale: 65536.0000 (65358.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1332  max mem: 15572
Epoch: [3]  [1670/2540]  eta: 0:08:38  lr: 0.000034  min_lr: 0.000000  loss: 4.9205 (4.7903)  loss_scale: 32768.0000 (65163.4135)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1086  max mem: 15572
Epoch: [3]  [1680/2540]  eta: 0:08:32  lr: 0.000034  min_lr: 0.000000  loss: 4.8060 (4.7903)  loss_scale: 32768.0000 (64970.6984)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1845  max mem: 15572
Epoch: [3]  [1690/2540]  eta: 0:08:26  lr: 0.000034  min_lr: 0.000000  loss: 4.8060 (4.7904)  loss_scale: 32768.0000 (64780.2626)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1413  max mem: 15572
Epoch: [3]  [1700/2540]  eta: 0:08:20  lr: 0.000034  min_lr: 0.000000  loss: 4.8281 (4.7902)  loss_scale: 32768.0000 (64592.0658)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1333  max mem: 15572
Epoch: [3]  [1710/2540]  eta: 0:08:14  lr: 0.000034  min_lr: 0.000000  loss: 4.7541 (4.7901)  loss_scale: 32768.0000 (64406.0690)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1650  max mem: 15572
Epoch: [3]  [1720/2540]  eta: 0:08:08  lr: 0.000034  min_lr: 0.000000  loss: 4.7229 (4.7900)  loss_scale: 32768.0000 (64222.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1119  max mem: 15572
Epoch: [3]  [1730/2540]  eta: 0:08:02  lr: 0.000035  min_lr: 0.000000  loss: 4.7392 (4.7897)  loss_scale: 32768.0000 (64040.5222)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0673  max mem: 15572
Epoch: [3]  [1740/2540]  eta: 0:07:55  lr: 0.000035  min_lr: 0.000000  loss: 4.6538 (4.7888)  loss_scale: 32768.0000 (63860.8983)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0458  max mem: 15572
[2025-01-12 23:05:22,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[3.3489424368471945e-07, 3.3489424368471945e-07, 4.784203481210278e-07, 4.784203481210278e-07, 6.834576401728971e-07, 6.834576401728971e-07, 9.76368057389853e-07, 9.76368057389853e-07, 1.3948115105569329e-06, 1.3948115105569329e-06, 1.99258787222419e-06, 1.99258787222419e-06, 2.846554103177414e-06, 2.846554103177414e-06, 4.066505861682021e-06, 4.066505861682021e-06, 5.809294088117173e-06, 5.809294088117173e-06, 8.298991554453105e-06, 8.298991554453105e-06, 1.1855702220647291e-05, 1.1855702220647291e-05, 1.693671745806756e-05, 1.693671745806756e-05, 2.419531065438223e-05, 2.419531065438223e-05, 3.4564729506260334e-05, 3.4564729506260334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:05:22,679] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=27.736121234750456, CurrSamplesPerSec=20.04145454864005, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [1750/2540]  eta: 0:07:49  lr: 0.000035  min_lr: 0.000000  loss: 4.6428 (4.7886)  loss_scale: 32768.0000 (63683.3261)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0670  max mem: 15572
Epoch: [3]  [1760/2540]  eta: 0:07:43  lr: 0.000035  min_lr: 0.000000  loss: 4.7113 (4.7886)  loss_scale: 32768.0000 (63507.7706)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1305  max mem: 15572
Epoch: [3]  [1770/2540]  eta: 0:07:37  lr: 0.000035  min_lr: 0.000000  loss: 4.7926 (4.7890)  loss_scale: 32768.0000 (63334.1976)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1375  max mem: 15572
Epoch: [3]  [1780/2540]  eta: 0:07:31  lr: 0.000035  min_lr: 0.000000  loss: 4.7692 (4.7885)  loss_scale: 32768.0000 (63162.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0929  max mem: 15572
[2025-01-12 23:05:46,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:05:46,235] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1790/2540]  eta: 0:07:25  lr: 0.000035  min_lr: 0.000000  loss: 4.7646 (4.7885)  loss_scale: 32768.0000 (63084.3462)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0625  max mem: 15572
Epoch: [3]  [1800/2540]  eta: 0:07:19  lr: 0.000035  min_lr: 0.000000  loss: 4.7732 (4.7885)  loss_scale: 65536.0000 (63097.9589)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0952  max mem: 15572
Epoch: [3]  [1810/2540]  eta: 0:07:14  lr: 0.000035  min_lr: 0.000000  loss: 4.7265 (4.7883)  loss_scale: 65536.0000 (63111.4213)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.2111  max mem: 15572
Epoch: [3]  [1820/2540]  eta: 0:07:08  lr: 0.000035  min_lr: 0.000000  loss: 4.7358 (4.7882)  loss_scale: 65536.0000 (63124.7359)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1926  max mem: 15572
Epoch: [3]  [1830/2540]  eta: 0:07:02  lr: 0.000035  min_lr: 0.000000  loss: 4.6993 (4.7879)  loss_scale: 65536.0000 (63137.9050)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1516  max mem: 15572
Epoch: [3]  [1840/2540]  eta: 0:06:56  lr: 0.000035  min_lr: 0.000000  loss: 4.6837 (4.7875)  loss_scale: 65536.0000 (63150.9310)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1457  max mem: 15572
Epoch: [3]  [1850/2540]  eta: 0:06:50  lr: 0.000035  min_lr: 0.000000  loss: 4.6837 (4.7868)  loss_scale: 65536.0000 (63163.8163)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0974  max mem: 15572
Epoch: [3]  [1860/2540]  eta: 0:06:44  lr: 0.000035  min_lr: 0.000000  loss: 4.7212 (4.7868)  loss_scale: 65536.0000 (63176.5631)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1041  max mem: 15572
Epoch: [3]  [1870/2540]  eta: 0:06:38  lr: 0.000035  min_lr: 0.000000  loss: 4.7440 (4.7863)  loss_scale: 65536.0000 (63189.1737)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1630  max mem: 15572
Epoch: [3]  [1880/2540]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000000  loss: 4.7530 (4.7861)  loss_scale: 65536.0000 (63201.6502)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1453  max mem: 15572
Epoch: [3]  [1890/2540]  eta: 0:06:26  lr: 0.000035  min_lr: 0.000000  loss: 4.6584 (4.7857)  loss_scale: 65536.0000 (63213.9947)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1214  max mem: 15572
Epoch: [3]  [1900/2540]  eta: 0:06:20  lr: 0.000035  min_lr: 0.000000  loss: 4.6681 (4.7853)  loss_scale: 65536.0000 (63226.2094)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1560  max mem: 15572
Epoch: [3]  [1910/2540]  eta: 0:06:14  lr: 0.000035  min_lr: 0.000000  loss: 4.7347 (4.7855)  loss_scale: 65536.0000 (63238.2962)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1861  max mem: 15572
[2025-01-12 23:07:02,911] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:07:02,911] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:07:03,328] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10170
[2025-01-12 23:07:03,328] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:07:03,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1920/2540]  eta: 0:06:08  lr: 0.000035  min_lr: 0.000000  loss: 4.8150 (4.7855)  loss_scale: 65536.0000 (63284.3727)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.2144  max mem: 15572
Epoch: [3]  [1930/2540]  eta: 0:06:02  lr: 0.000035  min_lr: 0.000000  loss: 4.7472 (4.7851)  loss_scale: 65536.0000 (63296.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1736  max mem: 15572
Epoch: [3]  [1940/2540]  eta: 0:05:57  lr: 0.000035  min_lr: 0.000000  loss: 4.7599 (4.7852)  loss_scale: 65536.0000 (63307.5734)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1608  max mem: 15572
Epoch: [3]  [1950/2540]  eta: 0:05:50  lr: 0.000035  min_lr: 0.000000  loss: 4.7181 (4.7845)  loss_scale: 65536.0000 (63318.9954)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1120  max mem: 15572
Epoch: [3]  [1960/2540]  eta: 0:05:44  lr: 0.000035  min_lr: 0.000000  loss: 4.6946 (4.7843)  loss_scale: 65536.0000 (63330.3009)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0695  max mem: 15572
Epoch: [3]  [1970/2540]  eta: 0:05:38  lr: 0.000035  min_lr: 0.000000  loss: 4.6564 (4.7838)  loss_scale: 65536.0000 (63341.4916)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1288  max mem: 15572
Epoch: [3]  [1980/2540]  eta: 0:05:32  lr: 0.000035  min_lr: 0.000000  loss: 4.6411 (4.7834)  loss_scale: 65536.0000 (63352.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0861  max mem: 15572
Epoch: [3]  [1990/2540]  eta: 0:05:26  lr: 0.000035  min_lr: 0.000000  loss: 4.6356 (4.7828)  loss_scale: 65536.0000 (63363.5359)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1027  max mem: 15572
Epoch: [3]  [2000/2540]  eta: 0:05:20  lr: 0.000036  min_lr: 0.000000  loss: 4.6222 (4.7825)  loss_scale: 65536.0000 (63374.3928)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1077  max mem: 15572
Epoch: [3]  [2010/2540]  eta: 0:05:14  lr: 0.000036  min_lr: 0.000000  loss: 4.7588 (4.7822)  loss_scale: 65536.0000 (63385.1417)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0781  max mem: 15572
Epoch: [3]  [2020/2540]  eta: 0:05:08  lr: 0.000036  min_lr: 0.000000  loss: 4.8029 (4.7819)  loss_scale: 65536.0000 (63395.7843)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1423  max mem: 15572
Epoch: [3]  [2030/2540]  eta: 0:05:03  lr: 0.000036  min_lr: 0.000000  loss: 4.7934 (4.7819)  loss_scale: 65536.0000 (63406.3220)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1658  max mem: 15572
Epoch: [3]  [2040/2540]  eta: 0:04:56  lr: 0.000036  min_lr: 0.000000  loss: 4.7517 (4.7818)  loss_scale: 65536.0000 (63416.7565)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1287  max mem: 15572
[2025-01-12 23:08:19,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:08:19,564] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:08:23,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10305
[2025-01-12 23:08:23,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:08:23,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2050/2540]  eta: 0:04:51  lr: 0.000036  min_lr: 0.000000  loss: 4.6930 (4.7815)  loss_scale: 65536.0000 (63618.8084)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1298  max mem: 15572
Epoch: [3]  [2060/2540]  eta: 0:04:45  lr: 0.000036  min_lr: 0.000000  loss: 4.6754 (4.7806)  loss_scale: 65536.0000 (63628.1106)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1466  max mem: 15572
Epoch: [3]  [2070/2540]  eta: 0:04:39  lr: 0.000036  min_lr: 0.000000  loss: 4.6487 (4.7804)  loss_scale: 65536.0000 (63637.3230)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1129  max mem: 15572
Epoch: [3]  [2080/2540]  eta: 0:04:33  lr: 0.000036  min_lr: 0.000000  loss: 4.7076 (4.7803)  loss_scale: 65536.0000 (63646.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1084  max mem: 15572
Epoch: [3]  [2090/2540]  eta: 0:04:27  lr: 0.000036  min_lr: 0.000000  loss: 4.6970 (4.7796)  loss_scale: 65536.0000 (63655.4835)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1194  max mem: 15572
Epoch: [3]  [2100/2540]  eta: 0:04:21  lr: 0.000036  min_lr: 0.000000  loss: 4.5737 (4.7792)  loss_scale: 65536.0000 (63664.4341)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1714  max mem: 15572
Epoch: [3]  [2110/2540]  eta: 0:04:15  lr: 0.000036  min_lr: 0.000000  loss: 4.6779 (4.7790)  loss_scale: 65536.0000 (63673.2999)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1767  max mem: 15572
Epoch: [3]  [2120/2540]  eta: 0:04:09  lr: 0.000036  min_lr: 0.000000  loss: 4.6734 (4.7785)  loss_scale: 65536.0000 (63682.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1721  max mem: 15572
Epoch: [3]  [2130/2540]  eta: 0:04:03  lr: 0.000036  min_lr: 0.000000  loss: 4.6513 (4.7779)  loss_scale: 65536.0000 (63690.7818)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1576  max mem: 15572
Epoch: [3]  [2140/2540]  eta: 0:03:57  lr: 0.000036  min_lr: 0.000000  loss: 4.6226 (4.7773)  loss_scale: 65536.0000 (63699.4003)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1037  max mem: 15572
Epoch: [3]  [2150/2540]  eta: 0:03:51  lr: 0.000036  min_lr: 0.000000  loss: 4.7101 (4.7772)  loss_scale: 65536.0000 (63707.9386)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1514  max mem: 15572
Epoch: [3]  [2160/2540]  eta: 0:03:45  lr: 0.000036  min_lr: 0.000000  loss: 4.6317 (4.7764)  loss_scale: 65536.0000 (63716.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1359  max mem: 15572
Epoch: [3]  [2170/2540]  eta: 0:03:39  lr: 0.000036  min_lr: 0.000000  loss: 4.5750 (4.7758)  loss_scale: 65536.0000 (63724.7794)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1064  max mem: 15572
[2025-01-12 23:09:40,094] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:09:40,094] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2180/2540]  eta: 0:03:33  lr: 0.000036  min_lr: 0.000000  loss: 4.6697 (4.7757)  loss_scale: 65536.0000 (63793.1811)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1332  max mem: 15572
[2025-01-12 23:09:41,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10436
[2025-01-12 23:09:41,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:09:41,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2190/2540]  eta: 0:03:27  lr: 0.000036  min_lr: 0.000000  loss: 4.8106 (4.7756)  loss_scale: 65536.0000 (63801.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1020  max mem: 15572
Epoch: [3]  [2200/2540]  eta: 0:03:21  lr: 0.000036  min_lr: 0.000000  loss: 4.8347 (4.7758)  loss_scale: 65536.0000 (63809.0177)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0654  max mem: 15572
Epoch: [3]  [2210/2540]  eta: 0:03:15  lr: 0.000036  min_lr: 0.000000  loss: 4.7985 (4.7754)  loss_scale: 65536.0000 (63816.8286)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1541  max mem: 15572
Epoch: [3]  [2220/2540]  eta: 0:03:09  lr: 0.000036  min_lr: 0.000000  loss: 4.7616 (4.7756)  loss_scale: 65536.0000 (63824.5691)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1910  max mem: 15572
Epoch: [3]  [2230/2540]  eta: 0:03:03  lr: 0.000036  min_lr: 0.000000  loss: 4.7906 (4.7751)  loss_scale: 65536.0000 (63832.2403)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0775  max mem: 15572
Epoch: [3]  [2240/2540]  eta: 0:02:57  lr: 0.000036  min_lr: 0.000000  loss: 4.6613 (4.7746)  loss_scale: 65536.0000 (63839.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0609  max mem: 15572
Epoch: [3]  [2250/2540]  eta: 0:02:51  lr: 0.000036  min_lr: 0.000000  loss: 4.6808 (4.7743)  loss_scale: 65536.0000 (63847.3781)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0948  max mem: 15572
Epoch: [3]  [2260/2540]  eta: 0:02:46  lr: 0.000036  min_lr: 0.000000  loss: 4.7271 (4.7742)  loss_scale: 65536.0000 (63854.8465)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1297  max mem: 15572
Epoch: [3]  [2270/2540]  eta: 0:02:40  lr: 0.000037  min_lr: 0.000000  loss: 4.7271 (4.7737)  loss_scale: 65536.0000 (63862.2492)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1351  max mem: 15572
Epoch: [3]  [2280/2540]  eta: 0:02:34  lr: 0.000037  min_lr: 0.000000  loss: 4.7532 (4.7737)  loss_scale: 65536.0000 (63869.5870)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0914  max mem: 15572
Epoch: [3]  [2290/2540]  eta: 0:02:28  lr: 0.000037  min_lr: 0.000000  loss: 4.7350 (4.7734)  loss_scale: 65536.0000 (63876.8608)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0610  max mem: 15572
Epoch: [3]  [2300/2540]  eta: 0:02:22  lr: 0.000037  min_lr: 0.000000  loss: 4.7084 (4.7732)  loss_scale: 65536.0000 (63884.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0680  max mem: 15572
[2025-01-12 23:10:53,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:10:53,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2310/2540]  eta: 0:02:16  lr: 0.000037  min_lr: 0.000000  loss: 4.7246 (4.7725)  loss_scale: 65536.0000 (63919.5777)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0897  max mem: 15572
[2025-01-12 23:10:57,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10570
[2025-01-12 23:10:57,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:10:57,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2320/2540]  eta: 0:02:10  lr: 0.000037  min_lr: 0.000000  loss: 4.6350 (4.7718)  loss_scale: 65536.0000 (64039.4864)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1347  max mem: 15572
Epoch: [3]  [2330/2540]  eta: 0:02:04  lr: 0.000037  min_lr: 0.000000  loss: 4.6633 (4.7717)  loss_scale: 65536.0000 (64045.9065)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1279  max mem: 15572
Epoch: [3]  [2340/2540]  eta: 0:01:58  lr: 0.000037  min_lr: 0.000000  loss: 4.7108 (4.7718)  loss_scale: 65536.0000 (64052.2717)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1091  max mem: 15572
Epoch: [3]  [2350/2540]  eta: 0:01:52  lr: 0.000037  min_lr: 0.000000  loss: 4.7746 (4.7716)  loss_scale: 65536.0000 (64058.5827)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1369  max mem: 15572
Epoch: [3]  [2360/2540]  eta: 0:01:46  lr: 0.000037  min_lr: 0.000000  loss: 4.7104 (4.7714)  loss_scale: 65536.0000 (64064.8403)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0936  max mem: 15572
Epoch: [3]  [2370/2540]  eta: 0:01:40  lr: 0.000037  min_lr: 0.000000  loss: 4.6713 (4.7710)  loss_scale: 65536.0000 (64071.0451)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1870  max mem: 15572
Epoch: [3]  [2380/2540]  eta: 0:01:34  lr: 0.000037  min_lr: 0.000000  loss: 4.7420 (4.7710)  loss_scale: 65536.0000 (64077.1978)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.2384  max mem: 15572
Epoch: [3]  [2390/2540]  eta: 0:01:28  lr: 0.000037  min_lr: 0.000000  loss: 4.7790 (4.7708)  loss_scale: 65536.0000 (64083.2990)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1633  max mem: 15572
Epoch: [3]  [2400/2540]  eta: 0:01:22  lr: 0.000037  min_lr: 0.000000  loss: 4.7891 (4.7709)  loss_scale: 65536.0000 (64089.3494)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1629  max mem: 15572
Epoch: [3]  [2410/2540]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000000  loss: 4.8227 (4.7708)  loss_scale: 65536.0000 (64095.3496)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1217  max mem: 15572
Epoch: [3]  [2420/2540]  eta: 0:01:11  lr: 0.000037  min_lr: 0.000000  loss: 4.7505 (4.7706)  loss_scale: 65536.0000 (64101.3003)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1248  max mem: 15572
Epoch: [3]  [2430/2540]  eta: 0:01:05  lr: 0.000037  min_lr: 0.000000  loss: 4.7361 (4.7708)  loss_scale: 65536.0000 (64107.2020)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1685  max mem: 15572
Epoch: [3]  [2440/2540]  eta: 0:00:59  lr: 0.000037  min_lr: 0.000000  loss: 4.7411 (4.7709)  loss_scale: 65536.0000 (64113.0553)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2210  max mem: 15572
[2025-01-12 23:12:13,928] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:12:13,929] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2450/2540]  eta: 0:00:53  lr: 0.000037  min_lr: 0.000000  loss: 4.7158 (4.7706)  loss_scale: 65536.0000 (64306.0302)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1940  max mem: 15572
[2025-01-12 23:12:19,710] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10709
[2025-01-12 23:12:19,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:12:19,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2460/2540]  eta: 0:00:47  lr: 0.000037  min_lr: 0.000000  loss: 4.7158 (4.7702)  loss_scale: 65536.0000 (64390.9175)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1392  max mem: 15572
Epoch: [3]  [2470/2540]  eta: 0:00:41  lr: 0.000037  min_lr: 0.000000  loss: 4.7305 (4.7701)  loss_scale: 65536.0000 (64395.5516)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1315  max mem: 15572
Epoch: [3]  [2480/2540]  eta: 0:00:35  lr: 0.000037  min_lr: 0.000000  loss: 4.7884 (4.7702)  loss_scale: 65536.0000 (64400.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1172  max mem: 15572
Epoch: [3]  [2490/2540]  eta: 0:00:29  lr: 0.000037  min_lr: 0.000000  loss: 4.6733 (4.7696)  loss_scale: 65536.0000 (64404.7081)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1425  max mem: 15572
Epoch: [3]  [2500/2540]  eta: 0:00:23  lr: 0.000037  min_lr: 0.000000  loss: 4.5976 (4.7692)  loss_scale: 65536.0000 (64409.2315)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0936  max mem: 15572
Epoch: [3]  [2510/2540]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.6091 (4.7687)  loss_scale: 65536.0000 (64413.7188)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1473  max mem: 15572
Epoch: [3]  [2520/2540]  eta: 0:00:11  lr: 0.000037  min_lr: 0.000000  loss: 4.7578 (4.7685)  loss_scale: 65536.0000 (64418.1706)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.2030  max mem: 15572
Epoch: [3]  [2530/2540]  eta: 0:00:05  lr: 0.000037  min_lr: 0.000000  loss: 4.6575 (4.7680)  loss_scale: 65536.0000 (64422.5871)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0805  max mem: 15572
Epoch: [3]  [2539/2540]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.6839 (4.7683)  loss_scale: 65536.0000 (64426.5323)  weight_decay: 0.0500 (0.0500)  time: 0.4288  data: 0.0112  max mem: 15572
Epoch: [3] Total time: 0:25:03 (0.5920 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.6839 (4.7683)  loss_scale: 65536.0000 (64426.5323)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2547
Indices to remove: tensor([    1,     2,     4,  ..., 33590, 33642, 33645], device='cuda:0')
length of data loader train is: 2328
num_training_steps_per_epoch is: 2328
Change step level LR scheduler!
Set warmup steps = 11640
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:18:36  loss: 2.4819 (2.4819)  acc1: 61.1111 (61.1111)  acc5: 100.0000 (100.0000)  time: 4.1052  data: 3.9094  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 4.9683 (4.4765)  acc1: 0.0000 (9.0909)  acc5: 0.0000 (28.7879)  time: 0.7695  data: 0.5781  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 4.4167 (4.3635)  acc1: 0.0000 (6.6138)  acc5: 5.5556 (23.5450)  time: 0.3855  data: 0.1876  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 4.3778 (4.3336)  acc1: 0.0000 (9.4982)  acc5: 11.1111 (26.1649)  time: 0.2799  data: 0.0843  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.9907 (4.2285)  acc1: 0.0000 (7.1816)  acc5: 22.2222 (28.7263)  time: 0.2832  data: 0.0968  max mem: 15572
Val:  [ 50/272]  eta: 0:01:34  loss: 3.7957 (4.2242)  acc1: 0.0000 (6.8627)  acc5: 22.2222 (31.1547)  time: 0.3808  data: 0.1863  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 3.5212 (4.1398)  acc1: 5.5556 (10.9290)  acc5: 61.1111 (36.0656)  time: 0.3429  data: 0.1457  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 3.8942 (4.1316)  acc1: 0.0000 (9.8592)  acc5: 55.5556 (35.3678)  time: 0.2812  data: 0.0874  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 4.0495 (4.1551)  acc1: 0.0000 (9.1907)  acc5: 22.2222 (34.7737)  time: 0.3041  data: 0.1056  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 4.6760 (4.2180)  acc1: 0.0000 (8.1807)  acc5: 0.0000 (31.0745)  time: 0.3307  data: 0.1281  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 4.6760 (4.2703)  acc1: 0.0000 (7.8658)  acc5: 0.0000 (29.4829)  time: 0.3360  data: 0.1344  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 4.5970 (4.3170)  acc1: 0.0000 (7.1572)  acc5: 0.0000 (27.9780)  time: 0.3250  data: 0.1223  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 4.7980 (4.3617)  acc1: 0.0000 (6.6575)  acc5: 0.0000 (26.0331)  time: 0.3266  data: 0.1148  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 4.6957 (4.3063)  acc1: 0.0000 (8.8634)  acc5: 11.1111 (28.6684)  time: 0.3147  data: 0.1037  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 4.1938 (4.2956)  acc1: 0.0000 (8.2742)  acc5: 27.7778 (28.8416)  time: 0.3126  data: 0.1144  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.2338 (4.2992)  acc1: 0.0000 (7.8734)  acc5: 11.1111 (27.7410)  time: 0.3100  data: 0.1152  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.2339 (4.2973)  acc1: 0.0000 (7.9710)  acc5: 16.6667 (28.4334)  time: 0.3298  data: 0.1262  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.3458 (4.3218)  acc1: 0.0000 (7.6673)  acc5: 22.2222 (28.1352)  time: 0.3423  data: 0.1287  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.4827 (4.3201)  acc1: 0.0000 (7.5506)  acc5: 22.2222 (27.9619)  time: 0.3734  data: 0.1767  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.4827 (4.3314)  acc1: 0.0000 (7.2135)  acc5: 11.1111 (27.0506)  time: 0.3459  data: 0.1570  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.2969 (4.3357)  acc1: 0.0000 (6.8823)  acc5: 16.6667 (27.6396)  time: 0.2976  data: 0.0830  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.2819 (4.3506)  acc1: 0.0000 (6.6877)  acc5: 27.7778 (26.9879)  time: 0.3238  data: 0.1139  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.4316 (4.3521)  acc1: 0.0000 (6.4605)  acc5: 11.1111 (26.4957)  time: 0.3181  data: 0.1260  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3641 (4.3538)  acc1: 0.0000 (6.7821)  acc5: 22.2222 (26.7196)  time: 0.3308  data: 0.1421  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 4.2323 (4.3438)  acc1: 11.1111 (6.9848)  acc5: 44.4444 (27.5934)  time: 0.3232  data: 0.1375  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.2724 (4.3682)  acc1: 0.0000 (6.9500)  acc5: 16.6667 (27.0474)  time: 0.2654  data: 0.0796  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.2442 (4.3147)  acc1: 16.6667 (8.5568)  acc5: 55.5556 (29.0549)  time: 0.2698  data: 0.0786  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.7241 (4.3093)  acc1: 22.2222 (8.9586)  acc5: 61.1111 (29.3358)  time: 0.2312  data: 0.0563  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.7241 (4.3113)  acc1: 22.2222 (8.9494)  acc5: 61.1111 (29.3262)  time: 0.2204  data: 0.0562  max mem: 15572
Val: Total time: 0:01:30 (0.3317 s / it)
* Acc@1 8.949 Acc@5 29.326 loss 4.311
Accuracy of the network on the 4883 val videos: 8.9%
[2025-01-12 23:14:38,483] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 23:14:38,486] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 23:14:38,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 23:14:41,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 23:14:41,406] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 8.95%
Epoch: [4]  [   0/2328]  eta: 4:06:32  lr: 0.000038  min_lr: 0.000000  loss: 4.9658 (4.9658)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.3542  data: 5.8320  max mem: 15572
Epoch: [4]  [  10/2328]  eta: 0:44:07  lr: 0.000038  min_lr: 0.000000  loss: 4.8127 (4.7692)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1420  data: 0.7038  max mem: 15572
Epoch: [4]  [  20/2328]  eta: 0:33:39  lr: 0.000038  min_lr: 0.000000  loss: 4.7598 (4.7741)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1634  max mem: 15572
Epoch: [4]  [  30/2328]  eta: 0:31:18  lr: 0.000038  min_lr: 0.000000  loss: 4.7527 (4.7536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1955  max mem: 15572
Epoch: [4]  [  40/2328]  eta: 0:30:24  lr: 0.000038  min_lr: 0.000000  loss: 4.7754 (4.7528)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7162  data: 0.2719  max mem: 15572
[2025-01-12 23:15:15,403] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:15:15,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:15:16,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10841
[2025-01-12 23:15:16,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:15:16,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [  50/2328]  eta: 0:28:38  lr: 0.000038  min_lr: 0.000000  loss: 4.8106 (4.7503)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.2004  max mem: 15572
Epoch: [4]  [  60/2328]  eta: 0:27:17  lr: 0.000038  min_lr: 0.000000  loss: 4.7883 (4.7570)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1178  max mem: 15572
Epoch: [4]  [  70/2328]  eta: 0:26:22  lr: 0.000038  min_lr: 0.000000  loss: 4.7883 (4.7602)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1201  max mem: 15572
Epoch: [4]  [  80/2328]  eta: 0:26:03  lr: 0.000038  min_lr: 0.000000  loss: 4.8019 (4.7706)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1644  max mem: 15572
Epoch: [4]  [  90/2328]  eta: 0:25:58  lr: 0.000038  min_lr: 0.000000  loss: 4.8139 (4.7718)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.2304  max mem: 15572
Epoch: [4]  [ 100/2328]  eta: 0:25:35  lr: 0.000038  min_lr: 0.000000  loss: 4.7052 (4.7628)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.2121  max mem: 15572
Epoch: [4]  [ 110/2328]  eta: 0:25:13  lr: 0.000038  min_lr: 0.000000  loss: 4.7404 (4.7624)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1806  max mem: 15572
Epoch: [4]  [ 120/2328]  eta: 0:24:47  lr: 0.000038  min_lr: 0.000000  loss: 4.8023 (4.7677)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1713  max mem: 15572
Epoch: [4]  [ 130/2328]  eta: 0:24:33  lr: 0.000038  min_lr: 0.000000  loss: 4.7514 (4.7591)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1792  max mem: 15572
Epoch: [4]  [ 140/2328]  eta: 0:24:02  lr: 0.000038  min_lr: 0.000000  loss: 4.6919 (4.7544)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1461  max mem: 15572
Epoch: [4]  [ 150/2328]  eta: 0:23:41  lr: 0.000038  min_lr: 0.000000  loss: 4.7157 (4.7590)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1040  max mem: 15572
Epoch: [4]  [ 160/2328]  eta: 0:23:20  lr: 0.000038  min_lr: 0.000000  loss: 4.6892 (4.7492)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1079  max mem: 15572
Epoch: [4]  [ 170/2328]  eta: 0:23:17  lr: 0.000038  min_lr: 0.000000  loss: 4.7739 (4.7573)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1557  max mem: 15572
[2025-01-12 23:16:34,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:16:34,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 180/2328]  eta: 0:23:04  lr: 0.000038  min_lr: 0.000000  loss: 4.7739 (4.7524)  loss_scale: 65536.0000 (68794.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1756  max mem: 15572
[2025-01-12 23:16:39,905] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10979
[2025-01-12 23:16:39,906] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:16:39,906] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 190/2328]  eta: 0:22:50  lr: 0.000038  min_lr: 0.000000  loss: 4.6571 (4.7477)  loss_scale: 65536.0000 (69653.4450)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1414  max mem: 15572
Epoch: [4]  [ 200/2328]  eta: 0:22:29  lr: 0.000038  min_lr: 0.000000  loss: 4.7513 (4.7508)  loss_scale: 65536.0000 (69448.5970)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1003  max mem: 15572
[2025-01-12 23:16:50,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=62, lr=[3.7132532180318237e-07, 3.7132532180318237e-07, 5.304647454331178e-07, 5.304647454331178e-07, 7.578067791901683e-07, 7.578067791901683e-07, 1.082581113128812e-06, 1.082581113128812e-06, 1.5465444473268744e-06, 1.5465444473268744e-06, 2.2093492104669635e-06, 2.2093492104669635e-06, 3.156213157809948e-06, 3.156213157809948e-06, 4.508875939728498e-06, 4.508875939728498e-06, 6.441251342469282e-06, 6.441251342469282e-06, 9.201787632098976e-06, 9.201787632098976e-06, 1.3145410902998535e-05, 1.3145410902998535e-05, 1.8779158432855052e-05, 1.8779158432855052e-05, 2.6827369189792933e-05, 2.6827369189792933e-05, 3.832481312827562e-05, 3.832481312827562e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:16:50,783] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=27.77114515479385, CurrSamplesPerSec=27.807048715160835, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 210/2328]  eta: 0:22:21  lr: 0.000038  min_lr: 0.000000  loss: 4.7760 (4.7478)  loss_scale: 65536.0000 (69263.1659)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1208  max mem: 15572
Epoch: [4]  [ 220/2328]  eta: 0:22:12  lr: 0.000038  min_lr: 0.000000  loss: 4.6396 (4.7442)  loss_scale: 65536.0000 (69094.5158)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1610  max mem: 15572
Epoch: [4]  [ 230/2328]  eta: 0:21:58  lr: 0.000038  min_lr: 0.000000  loss: 4.7076 (4.7432)  loss_scale: 65536.0000 (68940.4675)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1276  max mem: 15572
Epoch: [4]  [ 240/2328]  eta: 0:21:51  lr: 0.000038  min_lr: 0.000000  loss: 4.8083 (4.7433)  loss_scale: 65536.0000 (68799.2033)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1342  max mem: 15572
Epoch: [4]  [ 250/2328]  eta: 0:21:46  lr: 0.000039  min_lr: 0.000000  loss: 4.8065 (4.7437)  loss_scale: 65536.0000 (68669.1952)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1730  max mem: 15572
Epoch: [4]  [ 260/2328]  eta: 0:21:27  lr: 0.000039  min_lr: 0.000000  loss: 4.6583 (4.7385)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1022  max mem: 15572
Epoch: [4]  [ 270/2328]  eta: 0:21:23  lr: 0.000039  min_lr: 0.000000  loss: 4.6964 (4.7408)  loss_scale: 65536.0000 (68437.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1061  max mem: 15572
Epoch: [4]  [ 280/2328]  eta: 0:21:16  lr: 0.000039  min_lr: 0.000000  loss: 4.7610 (4.7387)  loss_scale: 65536.0000 (68334.6904)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1753  max mem: 15572
Epoch: [4]  [ 290/2328]  eta: 0:21:07  lr: 0.000039  min_lr: 0.000000  loss: 4.6204 (4.7346)  loss_scale: 65536.0000 (68238.5155)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1284  max mem: 15572
Epoch: [4]  [ 300/2328]  eta: 0:20:53  lr: 0.000039  min_lr: 0.000000  loss: 4.6686 (4.7379)  loss_scale: 65536.0000 (68148.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0758  max mem: 15572
Epoch: [4]  [ 310/2328]  eta: 0:20:45  lr: 0.000039  min_lr: 0.000000  loss: 4.8251 (4.7396)  loss_scale: 65536.0000 (68064.7203)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0987  max mem: 15572
[2025-01-12 23:17:55,139] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:17:55,140] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:17:55,546] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11109
[2025-01-12 23:17:55,547] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:17:55,547] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 320/2328]  eta: 0:20:36  lr: 0.000039  min_lr: 0.000000  loss: 4.6566 (4.7366)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1348  max mem: 15572
Epoch: [4]  [ 330/2328]  eta: 0:20:29  lr: 0.000039  min_lr: 0.000000  loss: 4.6566 (4.7343)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1183  max mem: 15572
Epoch: [4]  [ 340/2328]  eta: 0:20:19  lr: 0.000039  min_lr: 0.000000  loss: 4.6831 (4.7334)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1163  max mem: 15572
Epoch: [4]  [ 350/2328]  eta: 0:20:11  lr: 0.000039  min_lr: 0.000000  loss: 4.7514 (4.7342)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1178  max mem: 15572
Epoch: [4]  [ 360/2328]  eta: 0:20:09  lr: 0.000039  min_lr: 0.000000  loss: 4.7240 (4.7319)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1766  max mem: 15572
Epoch: [4]  [ 370/2328]  eta: 0:19:56  lr: 0.000039  min_lr: 0.000000  loss: 4.7110 (4.7317)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1337  max mem: 15572
Epoch: [4]  [ 380/2328]  eta: 0:19:51  lr: 0.000039  min_lr: 0.000000  loss: 4.7286 (4.7298)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0915  max mem: 15572
Epoch: [4]  [ 390/2328]  eta: 0:19:47  lr: 0.000039  min_lr: 0.000000  loss: 4.7668 (4.7323)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1787  max mem: 15572
Epoch: [4]  [ 400/2328]  eta: 0:19:37  lr: 0.000039  min_lr: 0.000000  loss: 4.8692 (4.7341)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1422  max mem: 15572
Epoch: [4]  [ 410/2328]  eta: 0:19:32  lr: 0.000039  min_lr: 0.000000  loss: 4.7432 (4.7320)  loss_scale: 65536.0000 (67608.9148)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1297  max mem: 15572
Epoch: [4]  [ 420/2328]  eta: 0:19:28  lr: 0.000039  min_lr: 0.000000  loss: 4.7400 (4.7323)  loss_scale: 65536.0000 (67559.6770)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.1960  max mem: 15572
Epoch: [4]  [ 430/2328]  eta: 0:19:17  lr: 0.000039  min_lr: 0.000000  loss: 4.7032 (4.7324)  loss_scale: 65536.0000 (67512.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1179  max mem: 15572
Epoch: [4]  [ 440/2328]  eta: 0:19:07  lr: 0.000039  min_lr: 0.000000  loss: 4.7085 (4.7339)  loss_scale: 65536.0000 (67467.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5048  data: 0.0371  max mem: 15572
[2025-01-12 23:19:10,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:19:10,950] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:19:11,812] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11240
[2025-01-12 23:19:11,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:19:11,813] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 450/2328]  eta: 0:19:03  lr: 0.000039  min_lr: 0.000000  loss: 4.8133 (4.7335)  loss_scale: 65536.0000 (67715.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1366  max mem: 15572
Epoch: [4]  [ 460/2328]  eta: 0:18:57  lr: 0.000039  min_lr: 0.000000  loss: 4.7281 (4.7332)  loss_scale: 65536.0000 (67668.4078)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.1929  max mem: 15572
Epoch: [4]  [ 470/2328]  eta: 0:18:49  lr: 0.000039  min_lr: 0.000000  loss: 4.7491 (4.7340)  loss_scale: 65536.0000 (67623.1338)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1404  max mem: 15572
Epoch: [4]  [ 480/2328]  eta: 0:18:44  lr: 0.000039  min_lr: 0.000000  loss: 4.6893 (4.7327)  loss_scale: 65536.0000 (67579.7422)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1618  max mem: 15572
Epoch: [4]  [ 490/2328]  eta: 0:18:35  lr: 0.000039  min_lr: 0.000000  loss: 4.6437 (4.7298)  loss_scale: 65536.0000 (67538.1181)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1233  max mem: 15572
Epoch: [4]  [ 500/2328]  eta: 0:18:29  lr: 0.000040  min_lr: 0.000000  loss: 4.6542 (4.7308)  loss_scale: 65536.0000 (67498.1557)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1092  max mem: 15572
Epoch: [4]  [ 510/2328]  eta: 0:18:21  lr: 0.000040  min_lr: 0.000000  loss: 4.7570 (4.7322)  loss_scale: 65536.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1385  max mem: 15572
Epoch: [4]  [ 520/2328]  eta: 0:18:13  lr: 0.000040  min_lr: 0.000000  loss: 4.7651 (4.7326)  loss_scale: 65536.0000 (67422.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1038  max mem: 15572
Epoch: [4]  [ 530/2328]  eta: 0:18:06  lr: 0.000040  min_lr: 0.000000  loss: 4.7847 (4.7319)  loss_scale: 65536.0000 (67387.2994)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1012  max mem: 15572
Epoch: [4]  [ 540/2328]  eta: 0:18:00  lr: 0.000040  min_lr: 0.000000  loss: 4.7285 (4.7307)  loss_scale: 65536.0000 (67353.0795)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1092  max mem: 15572
Epoch: [4]  [ 550/2328]  eta: 0:17:49  lr: 0.000040  min_lr: 0.000000  loss: 4.7105 (4.7308)  loss_scale: 65536.0000 (67320.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0700  max mem: 15572
Epoch: [4]  [ 560/2328]  eta: 0:17:38  lr: 0.000040  min_lr: 0.000000  loss: 4.8025 (4.7324)  loss_scale: 65536.0000 (67288.2995)  weight_decay: 0.0500 (0.0500)  time: 0.4462  data: 0.0219  max mem: 15572
Epoch: [4]  [ 570/2328]  eta: 0:17:26  lr: 0.000040  min_lr: 0.000000  loss: 4.8484 (4.7357)  loss_scale: 65536.0000 (67257.6112)  weight_decay: 0.0500 (0.0500)  time: 0.4277  data: 0.0092  max mem: 15572
[2025-01-12 23:20:23,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:20:23,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:20:26,845] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11375
[2025-01-12 23:20:26,845] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:20:26,845] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 580/2328]  eta: 0:17:18  lr: 0.000040  min_lr: 0.000000  loss: 4.8625 (4.7356)  loss_scale: 65536.0000 (67904.7711)  weight_decay: 0.0500 (0.0500)  time: 0.4690  data: 0.0006  max mem: 15572
Epoch: [4]  [ 590/2328]  eta: 0:17:12  lr: 0.000040  min_lr: 0.000000  loss: 4.8371 (4.7366)  loss_scale: 65536.0000 (67864.6904)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0461  max mem: 15572
[2025-01-12 23:20:38,397] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11392
[2025-01-12 23:20:38,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:20:38,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 600/2328]  eta: 0:17:09  lr: 0.000040  min_lr: 0.000000  loss: 4.7737 (4.7352)  loss_scale: 65536.0000 (67607.8536)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.1694  max mem: 15572
Epoch: [4]  [ 610/2328]  eta: 0:17:08  lr: 0.000040  min_lr: 0.000000  loss: 4.6921 (4.7353)  loss_scale: 32768.0000 (67037.6432)  weight_decay: 0.0500 (0.0500)  time: 0.7443  data: 0.2591  max mem: 15572
Epoch: [4]  [ 620/2328]  eta: 0:17:05  lr: 0.000040  min_lr: 0.000000  loss: 4.7487 (4.7364)  loss_scale: 32768.0000 (66485.7971)  weight_decay: 0.0500 (0.0500)  time: 0.7300  data: 0.2331  max mem: 15572
Epoch: [4]  [ 630/2328]  eta: 0:16:59  lr: 0.000040  min_lr: 0.000000  loss: 4.8019 (4.7373)  loss_scale: 32768.0000 (65951.4422)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1680  max mem: 15572
Epoch: [4]  [ 640/2328]  eta: 0:16:58  lr: 0.000040  min_lr: 0.000000  loss: 4.7748 (4.7378)  loss_scale: 32768.0000 (65433.7598)  weight_decay: 0.0500 (0.0500)  time: 0.7063  data: 0.2248  max mem: 15572
Epoch: [4]  [ 650/2328]  eta: 0:16:53  lr: 0.000040  min_lr: 0.000000  loss: 4.7024 (4.7369)  loss_scale: 32768.0000 (64931.9816)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.2223  max mem: 15572
Epoch: [4]  [ 660/2328]  eta: 0:16:48  lr: 0.000040  min_lr: 0.000000  loss: 4.7751 (4.7380)  loss_scale: 32768.0000 (64445.3858)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1419  max mem: 15572
Epoch: [4]  [ 670/2328]  eta: 0:16:43  lr: 0.000040  min_lr: 0.000000  loss: 4.7990 (4.7390)  loss_scale: 32768.0000 (63973.2936)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1610  max mem: 15572
Epoch: [4]  [ 680/2328]  eta: 0:16:40  lr: 0.000040  min_lr: 0.000000  loss: 4.7715 (4.7392)  loss_scale: 32768.0000 (63515.0661)  weight_decay: 0.0500 (0.0500)  time: 0.6982  data: 0.2285  max mem: 15572
Epoch: [4]  [ 690/2328]  eta: 0:16:37  lr: 0.000040  min_lr: 0.000000  loss: 4.7696 (4.7401)  loss_scale: 32768.0000 (63070.1013)  weight_decay: 0.0500 (0.0500)  time: 0.7260  data: 0.2601  max mem: 15572
Epoch: [4]  [ 700/2328]  eta: 0:16:30  lr: 0.000040  min_lr: 0.000000  loss: 4.7195 (4.7393)  loss_scale: 32768.0000 (62637.8317)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1924  max mem: 15572
Epoch: [4]  [ 710/2328]  eta: 0:16:22  lr: 0.000040  min_lr: 0.000000  loss: 4.6924 (4.7399)  loss_scale: 32768.0000 (62217.7215)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1239  max mem: 15572
Epoch: [4]  [ 720/2328]  eta: 0:16:11  lr: 0.000040  min_lr: 0.000000  loss: 4.7891 (4.7400)  loss_scale: 32768.0000 (61809.2649)  weight_decay: 0.0500 (0.0500)  time: 0.4536  data: 0.0502  max mem: 15572
[2025-01-12 23:22:00,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:22:00,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 730/2328]  eta: 0:16:02  lr: 0.000040  min_lr: 0.000000  loss: 4.7035 (4.7386)  loss_scale: 32768.0000 (61636.1149)  weight_decay: 0.0500 (0.0500)  time: 0.4313  data: 0.0005  max mem: 15572
Epoch: [4]  [ 740/2328]  eta: 0:15:52  lr: 0.000040  min_lr: 0.000000  loss: 4.6082 (4.7363)  loss_scale: 65536.0000 (61688.7449)  weight_decay: 0.0500 (0.0500)  time: 0.4400  data: 0.0005  max mem: 15572
Epoch: [4]  [ 750/2328]  eta: 0:15:47  lr: 0.000041  min_lr: 0.000000  loss: 4.6082 (4.7345)  loss_scale: 65536.0000 (61739.9734)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0916  max mem: 15572
Epoch: [4]  [ 760/2328]  eta: 0:15:40  lr: 0.000041  min_lr: 0.000000  loss: 4.6386 (4.7339)  loss_scale: 65536.0000 (61789.8555)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1549  max mem: 15572
Epoch: [4]  [ 770/2328]  eta: 0:15:36  lr: 0.000041  min_lr: 0.000000  loss: 4.7380 (4.7330)  loss_scale: 65536.0000 (61838.4436)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1697  max mem: 15572
Epoch: [4]  [ 780/2328]  eta: 0:15:27  lr: 0.000041  min_lr: 0.000000  loss: 4.7410 (4.7328)  loss_scale: 65536.0000 (61885.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1065  max mem: 15572
Epoch: [4]  [ 790/2328]  eta: 0:15:21  lr: 0.000041  min_lr: 0.000000  loss: 4.6442 (4.7320)  loss_scale: 65536.0000 (61931.9343)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0534  max mem: 15572
Epoch: [4]  [ 800/2328]  eta: 0:15:14  lr: 0.000041  min_lr: 0.000000  loss: 4.6667 (4.7313)  loss_scale: 65536.0000 (61976.9288)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1130  max mem: 15572
Epoch: [4]  [ 810/2328]  eta: 0:15:09  lr: 0.000041  min_lr: 0.000000  loss: 4.7548 (4.7315)  loss_scale: 65536.0000 (62020.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1591  max mem: 15572
Epoch: [4]  [ 820/2328]  eta: 0:15:04  lr: 0.000041  min_lr: 0.000000  loss: 4.7043 (4.7306)  loss_scale: 65536.0000 (62063.6297)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1839  max mem: 15572
Epoch: [4]  [ 830/2328]  eta: 0:14:57  lr: 0.000041  min_lr: 0.000000  loss: 4.6875 (4.7303)  loss_scale: 65536.0000 (62105.4152)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1201  max mem: 15572
Epoch: [4]  [ 840/2328]  eta: 0:14:50  lr: 0.000041  min_lr: 0.000000  loss: 4.7881 (4.7310)  loss_scale: 65536.0000 (62146.2069)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0803  max mem: 15572
Epoch: [4]  [ 850/2328]  eta: 0:14:44  lr: 0.000041  min_lr: 0.000000  loss: 4.7006 (4.7298)  loss_scale: 65536.0000 (62186.0400)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1345  max mem: 15572
[2025-01-12 23:23:12,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:23:12,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:23:13,427] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11650
[2025-01-12 23:23:13,427] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:23:13,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 860/2328]  eta: 0:14:40  lr: 0.000041  min_lr: 0.000000  loss: 4.6835 (4.7292)  loss_scale: 65536.0000 (62301.0639)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.2066  max mem: 15572
Epoch: [4]  [ 870/2328]  eta: 0:14:33  lr: 0.000041  min_lr: 0.000000  loss: 4.7422 (4.7292)  loss_scale: 65536.0000 (62338.2044)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1660  max mem: 15572
Epoch: [4]  [ 880/2328]  eta: 0:14:26  lr: 0.000041  min_lr: 0.000000  loss: 4.8194 (4.7292)  loss_scale: 65536.0000 (62374.5017)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1002  max mem: 15572
Epoch: [4]  [ 890/2328]  eta: 0:14:20  lr: 0.000041  min_lr: 0.000000  loss: 4.7965 (4.7295)  loss_scale: 65536.0000 (62409.9843)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1149  max mem: 15572
Epoch: [4]  [ 900/2328]  eta: 0:14:14  lr: 0.000041  min_lr: 0.000000  loss: 4.7669 (4.7299)  loss_scale: 65536.0000 (62444.6792)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1350  max mem: 15572
Epoch: [4]  [ 910/2328]  eta: 0:14:06  lr: 0.000041  min_lr: 0.000000  loss: 4.7665 (4.7301)  loss_scale: 65536.0000 (62478.6125)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0855  max mem: 15572
Epoch: [4]  [ 920/2328]  eta: 0:14:00  lr: 0.000041  min_lr: 0.000000  loss: 4.7277 (4.7303)  loss_scale: 65536.0000 (62511.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0762  max mem: 15572
Epoch: [4]  [ 930/2328]  eta: 0:13:54  lr: 0.000041  min_lr: 0.000000  loss: 4.6511 (4.7291)  loss_scale: 65536.0000 (62544.2922)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1184  max mem: 15572
Epoch: [4]  [ 940/2328]  eta: 0:13:48  lr: 0.000041  min_lr: 0.000000  loss: 4.6058 (4.7288)  loss_scale: 65536.0000 (62576.0850)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1371  max mem: 15572
Epoch: [4]  [ 950/2328]  eta: 0:13:42  lr: 0.000041  min_lr: 0.000000  loss: 4.6572 (4.7280)  loss_scale: 65536.0000 (62607.2093)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1593  max mem: 15572
Epoch: [4]  [ 960/2328]  eta: 0:13:36  lr: 0.000041  min_lr: 0.000000  loss: 4.6396 (4.7274)  loss_scale: 65536.0000 (62637.6857)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1535  max mem: 15572
Epoch: [4]  [ 970/2328]  eta: 0:13:32  lr: 0.000041  min_lr: 0.000000  loss: 4.6669 (4.7281)  loss_scale: 65536.0000 (62667.5345)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.1941  max mem: 15572
Epoch: [4]  [ 980/2328]  eta: 0:13:25  lr: 0.000041  min_lr: 0.000000  loss: 4.7633 (4.7285)  loss_scale: 65536.0000 (62696.7747)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1628  max mem: 15572
[2025-01-12 23:24:30,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:24:30,429] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:24:32,935] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11785
[2025-01-12 23:24:32,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:24:32,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 990/2328]  eta: 0:13:17  lr: 0.000041  min_lr: 0.000000  loss: 4.7153 (4.7283)  loss_scale: 65536.0000 (63122.2119)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0723  max mem: 15572
Epoch: [4]  [1000/2328]  eta: 0:13:12  lr: 0.000042  min_lr: 0.000000  loss: 4.5859 (4.7267)  loss_scale: 65536.0000 (63146.3257)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0774  max mem: 15572
Epoch: [4]  [1010/2328]  eta: 0:13:05  lr: 0.000042  min_lr: 0.000000  loss: 4.7602 (4.7274)  loss_scale: 65536.0000 (63169.9624)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0733  max mem: 15572
Epoch: [4]  [1020/2328]  eta: 0:13:00  lr: 0.000042  min_lr: 0.000000  loss: 4.7300 (4.7269)  loss_scale: 65536.0000 (63193.1361)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1380  max mem: 15572
Epoch: [4]  [1030/2328]  eta: 0:12:53  lr: 0.000042  min_lr: 0.000000  loss: 4.6453 (4.7263)  loss_scale: 65536.0000 (63215.8603)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1832  max mem: 15572
Epoch: [4]  [1040/2328]  eta: 0:12:48  lr: 0.000042  min_lr: 0.000000  loss: 4.5874 (4.7262)  loss_scale: 65536.0000 (63238.1479)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1662  max mem: 15572
Epoch: [4]  [1050/2328]  eta: 0:12:43  lr: 0.000042  min_lr: 0.000000  loss: 4.7388 (4.7269)  loss_scale: 65536.0000 (63260.0114)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2079  max mem: 15572
Epoch: [4]  [1060/2328]  eta: 0:12:37  lr: 0.000042  min_lr: 0.000000  loss: 4.7388 (4.7269)  loss_scale: 65536.0000 (63281.4628)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1473  max mem: 15572
Epoch: [4]  [1070/2328]  eta: 0:12:31  lr: 0.000042  min_lr: 0.000000  loss: 4.7072 (4.7274)  loss_scale: 65536.0000 (63302.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1152  max mem: 15572
Epoch: [4]  [1080/2328]  eta: 0:12:25  lr: 0.000042  min_lr: 0.000000  loss: 4.7353 (4.7270)  loss_scale: 65536.0000 (63323.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1773  max mem: 15572
Epoch: [4]  [1090/2328]  eta: 0:12:20  lr: 0.000042  min_lr: 0.000000  loss: 4.7325 (4.7273)  loss_scale: 65536.0000 (63343.4574)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1893  max mem: 15572
Epoch: [4]  [1100/2328]  eta: 0:12:13  lr: 0.000042  min_lr: 0.000000  loss: 4.7657 (4.7277)  loss_scale: 65536.0000 (63363.3715)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1200  max mem: 15572
Epoch: [4]  [1110/2328]  eta: 0:12:07  lr: 0.000042  min_lr: 0.000000  loss: 4.7657 (4.7276)  loss_scale: 65536.0000 (63382.9271)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0970  max mem: 15572
[2025-01-12 23:25:51,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:25:51,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1120/2328]  eta: 0:12:02  lr: 0.000042  min_lr: 0.000000  loss: 4.7319 (4.7285)  loss_scale: 65536.0000 (63519.0580)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1697  max mem: 15572
[2025-01-12 23:25:56,000] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11920
[2025-01-12 23:25:56,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:25:56,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1130/2328]  eta: 0:11:56  lr: 0.000042  min_lr: 0.000000  loss: 4.7376 (4.7293)  loss_scale: 65536.0000 (63768.6720)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1717  max mem: 15572
Epoch: [4]  [1140/2328]  eta: 0:11:50  lr: 0.000042  min_lr: 0.000000  loss: 4.7376 (4.7295)  loss_scale: 65536.0000 (63784.1613)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1236  max mem: 15572
Epoch: [4]  [1150/2328]  eta: 0:11:43  lr: 0.000042  min_lr: 0.000000  loss: 4.7433 (4.7300)  loss_scale: 65536.0000 (63799.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1107  max mem: 15572
Epoch: [4]  [1160/2328]  eta: 0:11:37  lr: 0.000042  min_lr: 0.000000  loss: 4.7346 (4.7294)  loss_scale: 65536.0000 (63814.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1111  max mem: 15572
Epoch: [4]  [1170/2328]  eta: 0:11:31  lr: 0.000042  min_lr: 0.000000  loss: 4.6745 (4.7291)  loss_scale: 65536.0000 (63829.0418)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1359  max mem: 15572
Epoch: [4]  [1180/2328]  eta: 0:11:25  lr: 0.000042  min_lr: 0.000000  loss: 4.6745 (4.7286)  loss_scale: 65536.0000 (63843.4953)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1491  max mem: 15572
Epoch: [4]  [1190/2328]  eta: 0:11:19  lr: 0.000042  min_lr: 0.000000  loss: 4.5700 (4.7276)  loss_scale: 65536.0000 (63857.7061)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1104  max mem: 15572
Epoch: [4]  [1200/2328]  eta: 0:11:14  lr: 0.000042  min_lr: 0.000000  loss: 4.7150 (4.7274)  loss_scale: 65536.0000 (63871.6803)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1905  max mem: 15572
[2025-01-12 23:26:41,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=69, lr=[4.103464779405492e-07, 4.103464779405492e-07, 5.862092542007846e-07, 5.862092542007846e-07, 8.374417917154068e-07, 8.374417917154068e-07, 1.1963454167362953e-06, 1.1963454167362953e-06, 1.7090648810518505e-06, 1.7090648810518505e-06, 2.441521258645501e-06, 2.441521258645501e-06, 3.487887512350716e-06, 3.487887512350716e-06, 4.982696446215309e-06, 4.982696446215309e-06, 7.118137780307584e-06, 7.118137780307584e-06, 1.0168768257582264e-05, 1.0168768257582264e-05, 1.4526811796546092e-05, 1.4526811796546092e-05, 2.0752588280780132e-05, 2.0752588280780132e-05, 2.9646554686828763e-05, 2.9646554686828763e-05, 4.235222098118395e-05, 4.235222098118395e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:26:41,640] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=27.767873925928473, CurrSamplesPerSec=30.7304473017255, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1210/2328]  eta: 0:11:08  lr: 0.000042  min_lr: 0.000000  loss: 4.7536 (4.7278)  loss_scale: 65536.0000 (63885.4236)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.2587  max mem: 15572
Epoch: [4]  [1220/2328]  eta: 0:11:02  lr: 0.000042  min_lr: 0.000000  loss: 4.7934 (4.7280)  loss_scale: 65536.0000 (63898.9419)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1431  max mem: 15572
Epoch: [4]  [1230/2328]  eta: 0:10:56  lr: 0.000042  min_lr: 0.000000  loss: 4.6681 (4.7272)  loss_scale: 65536.0000 (63912.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1077  max mem: 15572
Epoch: [4]  [1240/2328]  eta: 0:10:49  lr: 0.000042  min_lr: 0.000000  loss: 4.6588 (4.7264)  loss_scale: 65536.0000 (63925.3247)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1081  max mem: 15572
Epoch: [4]  [1250/2328]  eta: 0:10:43  lr: 0.000043  min_lr: 0.000000  loss: 4.7375 (4.7270)  loss_scale: 65536.0000 (63938.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1086  max mem: 15572
[2025-01-12 23:27:10,923] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:27:10,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1260/2328]  eta: 0:10:37  lr: 0.000043  min_lr: 0.000000  loss: 4.6314 (4.7260)  loss_scale: 65536.0000 (64314.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1365  max mem: 15572
[2025-01-12 23:27:18,691] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12060
[2025-01-12 23:27:18,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:27:18,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1270/2328]  eta: 0:10:31  lr: 0.000043  min_lr: 0.000000  loss: 4.6502 (4.7262)  loss_scale: 131072.0000 (64530.5303)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1410  max mem: 15572
Epoch: [4]  [1280/2328]  eta: 0:10:25  lr: 0.000043  min_lr: 0.000000  loss: 4.7784 (4.7269)  loss_scale: 65536.0000 (64538.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1384  max mem: 15572
Epoch: [4]  [1290/2328]  eta: 0:10:19  lr: 0.000043  min_lr: 0.000000  loss: 4.7162 (4.7264)  loss_scale: 65536.0000 (64546.1069)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1309  max mem: 15572
Epoch: [4]  [1300/2328]  eta: 0:10:13  lr: 0.000043  min_lr: 0.000000  loss: 4.7162 (4.7264)  loss_scale: 65536.0000 (64553.7156)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1259  max mem: 15572
Epoch: [4]  [1310/2328]  eta: 0:10:07  lr: 0.000043  min_lr: 0.000000  loss: 4.7794 (4.7259)  loss_scale: 65536.0000 (64561.2082)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1097  max mem: 15572
Epoch: [4]  [1320/2328]  eta: 0:10:00  lr: 0.000043  min_lr: 0.000000  loss: 4.7872 (4.7256)  loss_scale: 65536.0000 (64568.5874)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0758  max mem: 15572
Epoch: [4]  [1330/2328]  eta: 0:09:54  lr: 0.000043  min_lr: 0.000000  loss: 4.7153 (4.7255)  loss_scale: 65536.0000 (64575.8557)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1309  max mem: 15572
Epoch: [4]  [1340/2328]  eta: 0:09:49  lr: 0.000043  min_lr: 0.000000  loss: 4.8046 (4.7258)  loss_scale: 65536.0000 (64583.0157)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1921  max mem: 15572
Epoch: [4]  [1350/2328]  eta: 0:09:42  lr: 0.000043  min_lr: 0.000000  loss: 4.6337 (4.7252)  loss_scale: 65536.0000 (64590.0696)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1118  max mem: 15572
Epoch: [4]  [1360/2328]  eta: 0:09:37  lr: 0.000043  min_lr: 0.000000  loss: 4.6267 (4.7258)  loss_scale: 65536.0000 (64597.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1459  max mem: 15572
Epoch: [4]  [1370/2328]  eta: 0:09:30  lr: 0.000043  min_lr: 0.000000  loss: 4.7069 (4.7255)  loss_scale: 65536.0000 (64603.8687)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1361  max mem: 15572
Epoch: [4]  [1380/2328]  eta: 0:09:24  lr: 0.000043  min_lr: 0.000000  loss: 4.7281 (4.7258)  loss_scale: 65536.0000 (64610.6184)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0923  max mem: 15572
Epoch: [4]  [1390/2328]  eta: 0:09:18  lr: 0.000043  min_lr: 0.000000  loss: 4.6806 (4.7251)  loss_scale: 65536.0000 (64617.2710)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1503  max mem: 15572
[2025-01-12 23:28:32,282] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:28:32,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1400/2328]  eta: 0:09:12  lr: 0.000043  min_lr: 0.000000  loss: 4.6358 (4.7240)  loss_scale: 65536.0000 (64951.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1580  max mem: 15572
[2025-01-12 23:28:36,938] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12196
[2025-01-12 23:28:36,938] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:28:36,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1410/2328]  eta: 0:09:06  lr: 0.000043  min_lr: 0.000000  loss: 4.4961 (4.7227)  loss_scale: 65536.0000 (64955.4189)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1484  max mem: 15572
Epoch: [4]  [1420/2328]  eta: 0:09:00  lr: 0.000043  min_lr: 0.000000  loss: 4.5877 (4.7223)  loss_scale: 65536.0000 (64959.5046)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1535  max mem: 15572
Epoch: [4]  [1430/2328]  eta: 0:08:54  lr: 0.000043  min_lr: 0.000000  loss: 4.7029 (4.7226)  loss_scale: 65536.0000 (64963.5332)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1323  max mem: 15572
Epoch: [4]  [1440/2328]  eta: 0:08:49  lr: 0.000043  min_lr: 0.000000  loss: 4.7352 (4.7226)  loss_scale: 65536.0000 (64967.5059)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1477  max mem: 15572
Epoch: [4]  [1450/2328]  eta: 0:08:43  lr: 0.000043  min_lr: 0.000000  loss: 4.6853 (4.7227)  loss_scale: 65536.0000 (64971.4238)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1551  max mem: 15572
Epoch: [4]  [1460/2328]  eta: 0:08:36  lr: 0.000043  min_lr: 0.000000  loss: 4.7035 (4.7226)  loss_scale: 65536.0000 (64975.2882)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0729  max mem: 15572
Epoch: [4]  [1470/2328]  eta: 0:08:30  lr: 0.000043  min_lr: 0.000000  loss: 4.7035 (4.7223)  loss_scale: 65536.0000 (64979.0999)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0607  max mem: 15572
Epoch: [4]  [1480/2328]  eta: 0:08:24  lr: 0.000043  min_lr: 0.000000  loss: 4.6310 (4.7212)  loss_scale: 65536.0000 (64982.8602)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1691  max mem: 15572
Epoch: [4]  [1490/2328]  eta: 0:08:18  lr: 0.000044  min_lr: 0.000000  loss: 4.6842 (4.7217)  loss_scale: 65536.0000 (64986.5701)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1767  max mem: 15572
Epoch: [4]  [1500/2328]  eta: 0:08:12  lr: 0.000044  min_lr: 0.000000  loss: 4.7820 (4.7220)  loss_scale: 65536.0000 (64990.2305)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1405  max mem: 15572
Epoch: [4]  [1510/2328]  eta: 0:08:06  lr: 0.000044  min_lr: 0.000000  loss: 4.7656 (4.7222)  loss_scale: 65536.0000 (64993.8425)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1188  max mem: 15572
Epoch: [4]  [1520/2328]  eta: 0:08:00  lr: 0.000044  min_lr: 0.000000  loss: 4.7210 (4.7220)  loss_scale: 65536.0000 (64997.4070)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0935  max mem: 15572
[2025-01-12 23:29:52,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:29:52,550] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1530/2328]  eta: 0:07:54  lr: 0.000044  min_lr: 0.000000  loss: 4.6669 (4.7217)  loss_scale: 65536.0000 (65043.7309)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1495  max mem: 15572
[2025-01-12 23:29:53,411] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12327
[2025-01-12 23:29:53,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:29:53,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1540/2328]  eta: 0:07:49  lr: 0.000044  min_lr: 0.000000  loss: 4.6526 (4.7206)  loss_scale: 65536.0000 (65089.4536)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.2163  max mem: 15572
Epoch: [4]  [1550/2328]  eta: 0:07:43  lr: 0.000044  min_lr: 0.000000  loss: 4.7101 (4.7208)  loss_scale: 65536.0000 (65092.3327)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1640  max mem: 15572
Epoch: [4]  [1560/2328]  eta: 0:07:36  lr: 0.000044  min_lr: 0.000000  loss: 4.7487 (4.7209)  loss_scale: 65536.0000 (65095.1749)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0675  max mem: 15572
Epoch: [4]  [1570/2328]  eta: 0:07:31  lr: 0.000044  min_lr: 0.000000  loss: 4.5743 (4.7199)  loss_scale: 65536.0000 (65097.9809)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1258  max mem: 15572
Epoch: [4]  [1580/2328]  eta: 0:07:25  lr: 0.000044  min_lr: 0.000000  loss: 4.5750 (4.7197)  loss_scale: 65536.0000 (65100.7514)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1578  max mem: 15572
Epoch: [4]  [1590/2328]  eta: 0:07:19  lr: 0.000044  min_lr: 0.000000  loss: 4.6306 (4.7195)  loss_scale: 65536.0000 (65103.4871)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1076  max mem: 15572
Epoch: [4]  [1600/2328]  eta: 0:07:13  lr: 0.000044  min_lr: 0.000000  loss: 4.6869 (4.7198)  loss_scale: 65536.0000 (65106.1886)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1150  max mem: 15572
Epoch: [4]  [1610/2328]  eta: 0:07:06  lr: 0.000044  min_lr: 0.000000  loss: 4.7405 (4.7196)  loss_scale: 65536.0000 (65108.8566)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1077  max mem: 15572
Epoch: [4]  [1620/2328]  eta: 0:07:01  lr: 0.000044  min_lr: 0.000000  loss: 4.7039 (4.7194)  loss_scale: 65536.0000 (65111.4917)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1344  max mem: 15572
Epoch: [4]  [1630/2328]  eta: 0:06:55  lr: 0.000044  min_lr: 0.000000  loss: 4.5772 (4.7185)  loss_scale: 65536.0000 (65114.0944)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1749  max mem: 15572
Epoch: [4]  [1640/2328]  eta: 0:06:48  lr: 0.000044  min_lr: 0.000000  loss: 4.5492 (4.7173)  loss_scale: 65536.0000 (65116.6654)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0765  max mem: 15572
Epoch: [4]  [1650/2328]  eta: 0:06:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5492 (4.7172)  loss_scale: 65536.0000 (65119.2053)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0602  max mem: 15572
Epoch: [4]  [1660/2328]  eta: 0:06:36  lr: 0.000044  min_lr: 0.000000  loss: 4.6429 (4.7164)  loss_scale: 65536.0000 (65121.7146)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0897  max mem: 15572
[2025-01-12 23:31:09,199] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:31:09,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1670/2328]  eta: 0:06:30  lr: 0.000044  min_lr: 0.000000  loss: 4.5961 (4.7160)  loss_scale: 65536.0000 (65516.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0413  max mem: 15572
[2025-01-12 23:31:17,146] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12472
[2025-01-12 23:31:17,146] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:31:17,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1680/2328]  eta: 0:06:24  lr: 0.000044  min_lr: 0.000000  loss: 4.5872 (4.7153)  loss_scale: 131072.0000 (65750.4247)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1028  max mem: 15572
Epoch: [4]  [1690/2328]  eta: 0:06:19  lr: 0.000044  min_lr: 0.000000  loss: 4.6237 (4.7154)  loss_scale: 65536.0000 (65749.1567)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1874  max mem: 15572
Epoch: [4]  [1700/2328]  eta: 0:06:13  lr: 0.000044  min_lr: 0.000000  loss: 4.6647 (4.7152)  loss_scale: 65536.0000 (65747.9036)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1671  max mem: 15572
Epoch: [4]  [1710/2328]  eta: 0:06:07  lr: 0.000044  min_lr: 0.000000  loss: 4.6246 (4.7144)  loss_scale: 65536.0000 (65746.6651)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.1861  max mem: 15572
Epoch: [4]  [1720/2328]  eta: 0:06:01  lr: 0.000044  min_lr: 0.000000  loss: 4.6246 (4.7137)  loss_scale: 65536.0000 (65745.4410)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1805  max mem: 15572
Epoch: [4]  [1730/2328]  eta: 0:05:55  lr: 0.000044  min_lr: 0.000000  loss: 4.6866 (4.7136)  loss_scale: 65536.0000 (65744.2311)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1285  max mem: 15572
Epoch: [4]  [1740/2328]  eta: 0:05:49  lr: 0.000045  min_lr: 0.000000  loss: 4.6749 (4.7135)  loss_scale: 65536.0000 (65743.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1265  max mem: 15572
Epoch: [4]  [1750/2328]  eta: 0:05:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6749 (4.7137)  loss_scale: 65536.0000 (65741.8527)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0925  max mem: 15572
Epoch: [4]  [1760/2328]  eta: 0:05:37  lr: 0.000045  min_lr: 0.000000  loss: 4.6340 (4.7131)  loss_scale: 65536.0000 (65740.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0992  max mem: 15572
Epoch: [4]  [1770/2328]  eta: 0:05:31  lr: 0.000045  min_lr: 0.000000  loss: 4.5780 (4.7129)  loss_scale: 65536.0000 (65739.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1244  max mem: 15572
Epoch: [4]  [1780/2328]  eta: 0:05:25  lr: 0.000045  min_lr: 0.000000  loss: 4.6921 (4.7133)  loss_scale: 65536.0000 (65738.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1582  max mem: 15572
Epoch: [4]  [1790/2328]  eta: 0:05:19  lr: 0.000045  min_lr: 0.000000  loss: 4.7562 (4.7134)  loss_scale: 65536.0000 (65737.2552)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1315  max mem: 15572
Epoch: [4]  [1800/2328]  eta: 0:05:13  lr: 0.000045  min_lr: 0.000000  loss: 4.6342 (4.7130)  loss_scale: 65536.0000 (65736.1377)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0743  max mem: 15572
[2025-01-12 23:32:33,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:32:33,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:32:34,468] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12603
[2025-01-12 23:32:34,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:32:34,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1810/2328]  eta: 0:05:07  lr: 0.000045  min_lr: 0.000000  loss: 4.6342 (4.7126)  loss_scale: 65536.0000 (65807.4081)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1198  max mem: 15572
Epoch: [4]  [1820/2328]  eta: 0:05:01  lr: 0.000045  min_lr: 0.000000  loss: 4.7184 (4.7123)  loss_scale: 65536.0000 (65805.9176)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1423  max mem: 15572
Epoch: [4]  [1830/2328]  eta: 0:04:55  lr: 0.000045  min_lr: 0.000000  loss: 4.7184 (4.7118)  loss_scale: 65536.0000 (65804.4435)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1373  max mem: 15572
Epoch: [4]  [1840/2328]  eta: 0:04:49  lr: 0.000045  min_lr: 0.000000  loss: 4.7732 (4.7119)  loss_scale: 65536.0000 (65802.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1040  max mem: 15572
Epoch: [4]  [1850/2328]  eta: 0:04:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6932 (4.7117)  loss_scale: 65536.0000 (65801.5429)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0837  max mem: 15572
Epoch: [4]  [1860/2328]  eta: 0:04:37  lr: 0.000045  min_lr: 0.000000  loss: 4.6689 (4.7117)  loss_scale: 65536.0000 (65800.1161)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1084  max mem: 15572
Epoch: [4]  [1870/2328]  eta: 0:04:31  lr: 0.000045  min_lr: 0.000000  loss: 4.6848 (4.7118)  loss_scale: 65536.0000 (65798.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.0929  max mem: 15572
Epoch: [4]  [1880/2328]  eta: 0:04:25  lr: 0.000045  min_lr: 0.000000  loss: 4.7521 (4.7121)  loss_scale: 65536.0000 (65797.3078)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0843  max mem: 15572
Epoch: [4]  [1890/2328]  eta: 0:04:19  lr: 0.000045  min_lr: 0.000000  loss: 4.7388 (4.7119)  loss_scale: 65536.0000 (65795.9260)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0931  max mem: 15572
Epoch: [4]  [1900/2328]  eta: 0:04:13  lr: 0.000045  min_lr: 0.000000  loss: 4.7081 (4.7119)  loss_scale: 65536.0000 (65794.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1270  max mem: 15572
Epoch: [4]  [1910/2328]  eta: 0:04:07  lr: 0.000045  min_lr: 0.000000  loss: 4.7081 (4.7119)  loss_scale: 65536.0000 (65793.2057)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1465  max mem: 15572
Epoch: [4]  [1920/2328]  eta: 0:04:01  lr: 0.000045  min_lr: 0.000000  loss: 4.7197 (4.7120)  loss_scale: 65536.0000 (65791.8667)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1577  max mem: 15572
Epoch: [4]  [1930/2328]  eta: 0:03:55  lr: 0.000045  min_lr: 0.000000  loss: 4.7197 (4.7119)  loss_scale: 65536.0000 (65790.5417)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1476  max mem: 15572
[2025-01-12 23:33:49,859] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:33:49,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:33:50,865] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12734
[2025-01-12 23:33:50,866] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:33:50,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1940/2328]  eta: 0:03:49  lr: 0.000045  min_lr: 0.000000  loss: 4.6906 (4.7117)  loss_scale: 65536.0000 (65856.7584)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1068  max mem: 15572
Epoch: [4]  [1950/2328]  eta: 0:03:44  lr: 0.000045  min_lr: 0.000000  loss: 4.6791 (4.7113)  loss_scale: 65536.0000 (65855.1143)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1492  max mem: 15572
Epoch: [4]  [1960/2328]  eta: 0:03:37  lr: 0.000045  min_lr: 0.000000  loss: 4.6791 (4.7112)  loss_scale: 65536.0000 (65853.4870)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0970  max mem: 15572
Epoch: [4]  [1970/2328]  eta: 0:03:32  lr: 0.000045  min_lr: 0.000000  loss: 4.6812 (4.7110)  loss_scale: 65536.0000 (65851.8762)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1259  max mem: 15572
Epoch: [4]  [1980/2328]  eta: 0:03:26  lr: 0.000045  min_lr: 0.000000  loss: 4.7121 (4.7114)  loss_scale: 65536.0000 (65850.2817)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1804  max mem: 15572
Epoch: [4]  [1990/2328]  eta: 0:03:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6766 (4.7110)  loss_scale: 65536.0000 (65848.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0843  max mem: 15572
Epoch: [4]  [2000/2328]  eta: 0:03:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5316 (4.7100)  loss_scale: 65536.0000 (65847.1404)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0852  max mem: 15572
Epoch: [4]  [2010/2328]  eta: 0:03:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5675 (4.7095)  loss_scale: 65536.0000 (65845.5932)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1154  max mem: 15572
Epoch: [4]  [2020/2328]  eta: 0:03:02  lr: 0.000046  min_lr: 0.000000  loss: 4.7377 (4.7099)  loss_scale: 65536.0000 (65844.0614)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1542  max mem: 15572
Epoch: [4]  [2030/2328]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000000  loss: 4.7377 (4.7093)  loss_scale: 65536.0000 (65842.5446)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1538  max mem: 15572
Epoch: [4]  [2040/2328]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000000  loss: 4.5334 (4.7083)  loss_scale: 65536.0000 (65841.0426)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1021  max mem: 15572
Epoch: [4]  [2050/2328]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.5569 (4.7082)  loss_scale: 65536.0000 (65839.5553)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1237  max mem: 15572
Epoch: [4]  [2060/2328]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000000  loss: 4.6983 (4.7079)  loss_scale: 65536.0000 (65838.0825)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1467  max mem: 15572
[2025-01-12 23:35:07,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:35:07,490] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:35:08,017] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12864
[2025-01-12 23:35:08,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:35:08,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2070/2328]  eta: 0:02:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6983 (4.7080)  loss_scale: 65536.0000 (65868.2685)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1253  max mem: 15572
Epoch: [4]  [2080/2328]  eta: 0:02:26  lr: 0.000046  min_lr: 0.000000  loss: 4.7575 (4.7083)  loss_scale: 65536.0000 (65866.6718)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1276  max mem: 15572
Epoch: [4]  [2090/2328]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.7441 (4.7083)  loss_scale: 65536.0000 (65865.0904)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1254  max mem: 15572
Epoch: [4]  [2100/2328]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6884 (4.7081)  loss_scale: 65536.0000 (65863.5240)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1765  max mem: 15572
Epoch: [4]  [2110/2328]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000000  loss: 4.7472 (4.7084)  loss_scale: 65536.0000 (65861.9725)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1811  max mem: 15572
Epoch: [4]  [2120/2328]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000000  loss: 4.6581 (4.7079)  loss_scale: 65536.0000 (65860.4356)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1356  max mem: 15572
Epoch: [4]  [2130/2328]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6722 (4.7082)  loss_scale: 65536.0000 (65858.9132)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1652  max mem: 15572
Epoch: [4]  [2140/2328]  eta: 0:01:51  lr: 0.000046  min_lr: 0.000000  loss: 4.6870 (4.7076)  loss_scale: 65536.0000 (65857.4050)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1215  max mem: 15572
Epoch: [4]  [2150/2328]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5660 (4.7070)  loss_scale: 65536.0000 (65855.9107)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1436  max mem: 15572
Epoch: [4]  [2160/2328]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000000  loss: 4.6132 (4.7066)  loss_scale: 65536.0000 (65854.4304)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1690  max mem: 15572
Epoch: [4]  [2170/2328]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.6430 (4.7066)  loss_scale: 65536.0000 (65852.9636)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1196  max mem: 15572
Epoch: [4]  [2180/2328]  eta: 0:01:27  lr: 0.000046  min_lr: 0.000000  loss: 4.7664 (4.7068)  loss_scale: 65536.0000 (65851.5103)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1367  max mem: 15572
Epoch: [4]  [2190/2328]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.6683 (4.7062)  loss_scale: 65536.0000 (65850.0703)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1533  max mem: 15572
[2025-01-12 23:36:25,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:36:25,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2200/2328]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6139 (4.7056)  loss_scale: 65536.0000 (65937.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1266  max mem: 15572
[2025-01-12 23:36:26,923] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12997
[2025-01-12 23:36:26,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:36:26,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-12 23:36:28,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=77, lr=[4.4936763407791597e-07, 4.4936763407791597e-07, 6.419537629684515e-07, 6.419537629684515e-07, 9.17076804240645e-07, 9.17076804240645e-07, 1.3101097203437786e-06, 1.3101097203437786e-06, 1.8715853147768267e-06, 1.8715853147768267e-06, 2.673693306824038e-06, 2.673693306824038e-06, 3.819561866891484e-06, 3.819561866891484e-06, 5.45651695270212e-06, 5.45651695270212e-06, 7.795024218145885e-06, 7.795024218145885e-06, 1.1135748883065551e-05, 1.1135748883065551e-05, 1.5908212690093647e-05, 1.5908212690093647e-05, 2.272601812870521e-05, 2.272601812870521e-05, 3.246574018386459e-05, 3.246574018386459e-05, 4.637962883409227e-05, 4.637962883409227e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:36:28,570] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=27.76534415035557, CurrSamplesPerSec=30.369849573701647, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [2210/2328]  eta: 0:01:09  lr: 0.000046  min_lr: 0.000000  loss: 4.6263 (4.7050)  loss_scale: 65536.0000 (65965.7929)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1300  max mem: 15572
Epoch: [4]  [2220/2328]  eta: 0:01:03  lr: 0.000046  min_lr: 0.000000  loss: 4.7165 (4.7053)  loss_scale: 65536.0000 (65963.8577)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1476  max mem: 15572
Epoch: [4]  [2230/2328]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000000  loss: 4.7549 (4.7055)  loss_scale: 65536.0000 (65961.9399)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1517  max mem: 15572
Epoch: [4]  [2240/2328]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6633 (4.7049)  loss_scale: 65536.0000 (65960.0393)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1161  max mem: 15572
Epoch: [4]  [2250/2328]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6474 (4.7047)  loss_scale: 65536.0000 (65958.1555)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0966  max mem: 15572
Epoch: [4]  [2260/2328]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6545 (4.7048)  loss_scale: 65536.0000 (65956.2884)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1662  max mem: 15572
Epoch: [4]  [2270/2328]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6003 (4.7040)  loss_scale: 65536.0000 (65954.4377)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1639  max mem: 15572
Epoch: [4]  [2280/2328]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5568 (4.7037)  loss_scale: 65536.0000 (65952.6032)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1340  max mem: 15572
Epoch: [4]  [2290/2328]  eta: 0:00:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5568 (4.7033)  loss_scale: 65536.0000 (65950.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1251  max mem: 15572
Epoch: [4]  [2300/2328]  eta: 0:00:16  lr: 0.000047  min_lr: 0.000000  loss: 4.6754 (4.7033)  loss_scale: 65536.0000 (65948.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1031  max mem: 15572
Epoch: [4]  [2310/2328]  eta: 0:00:10  lr: 0.000047  min_lr: 0.000000  loss: 4.7047 (4.7030)  loss_scale: 65536.0000 (65947.1952)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1201  max mem: 15572
Epoch: [4]  [2320/2328]  eta: 0:00:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6657 (4.7029)  loss_scale: 65536.0000 (65945.4235)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1627  max mem: 15572
Epoch: [4]  [2327/2328]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6322 (4.7028)  loss_scale: 65536.0000 (65944.1924)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0924  max mem: 15572
Epoch: [4] Total time: 0:22:56 (0.5915 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.6322 (4.7028)  loss_scale: 65536.0000 (65944.1924)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2993
Indices to remove: tensor([    3,     5,    11,  ..., 33603, 33644, 33654], device='cuda:0')
length of data loader train is: 2079
num_training_steps_per_epoch is: 2079
Change step level LR scheduler!
Set warmup steps = 10395
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:14  loss: 1.6834 (1.6834)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.2451  data: 4.0710  max mem: 15572
Val:  [ 10/272]  eta: 0:03:26  loss: 4.7274 (4.2171)  acc1: 0.0000 (20.7071)  acc5: 0.0000 (25.7576)  time: 0.7878  data: 0.5936  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 4.1839 (4.1742)  acc1: 0.0000 (14.5503)  acc5: 16.6667 (24.8677)  time: 0.3552  data: 0.1618  max mem: 15572
Val:  [ 30/272]  eta: 0:01:52  loss: 4.1455 (4.2180)  acc1: 0.0000 (12.0072)  acc5: 16.6667 (25.6272)  time: 0.2899  data: 0.1037  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 4.0432 (4.1374)  acc1: 0.0000 (10.8401)  acc5: 38.8889 (28.7263)  time: 0.3047  data: 0.1154  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 3.7931 (4.0667)  acc1: 0.0000 (10.7843)  acc5: 38.8889 (32.6797)  time: 0.3122  data: 0.1186  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 3.0140 (3.9166)  acc1: 16.6667 (16.4845)  acc5: 72.2222 (38.1603)  time: 0.3047  data: 0.1264  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.5553 (3.8841)  acc1: 5.5556 (15.1800)  acc5: 61.1111 (39.1236)  time: 0.2987  data: 0.1246  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.6925 (3.8948)  acc1: 0.0000 (15.0892)  acc5: 33.3333 (38.5460)  time: 0.3180  data: 0.1358  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 4.4618 (3.9606)  acc1: 0.0000 (13.5531)  acc5: 5.5556 (35.2869)  time: 0.3397  data: 0.1555  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 4.5281 (4.0211)  acc1: 0.0000 (13.0913)  acc5: 5.5556 (33.2233)  time: 0.3602  data: 0.1726  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 4.4227 (4.0619)  acc1: 0.0000 (12.1622)  acc5: 5.5556 (32.4324)  time: 0.3977  data: 0.2004  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 4.4227 (4.1014)  acc1: 0.0000 (11.2029)  acc5: 16.6667 (31.2213)  time: 0.3735  data: 0.1702  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 4.4204 (4.0600)  acc1: 0.0000 (11.9169)  acc5: 22.2222 (32.9517)  time: 0.3301  data: 0.1270  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 3.6801 (4.0490)  acc1: 0.0000 (11.4263)  acc5: 50.0000 (33.6091)  time: 0.3265  data: 0.1234  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.9898 (4.0510)  acc1: 0.0000 (10.9272)  acc5: 27.7778 (32.9286)  time: 0.3027  data: 0.0979  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.8831 (4.0349)  acc1: 5.5556 (11.1111)  acc5: 38.8889 (34.5411)  time: 0.3001  data: 0.0946  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.8099 (4.0530)  acc1: 11.1111 (11.5335)  acc5: 55.5556 (34.7303)  time: 0.3211  data: 0.1161  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.0224 (4.0545)  acc1: 5.5556 (11.2953)  acc5: 22.2222 (34.2848)  time: 0.3286  data: 0.1307  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.1976 (4.0671)  acc1: 0.0000 (10.9948)  acc5: 11.1111 (33.5660)  time: 0.3095  data: 0.1134  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.9916 (4.0668)  acc1: 0.0000 (11.2769)  acc5: 27.7778 (34.6048)  time: 0.3182  data: 0.1174  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.8891 (4.0791)  acc1: 5.5556 (11.4007)  acc5: 50.0000 (34.5445)  time: 0.3478  data: 0.1519  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.1928 (4.0822)  acc1: 5.5556 (11.1111)  acc5: 33.3333 (34.2634)  time: 0.3705  data: 0.1779  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.9308 (4.0755)  acc1: 5.5556 (11.8086)  acc5: 50.0000 (35.5219)  time: 0.2939  data: 0.1036  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.7316 (4.0604)  acc1: 33.3333 (12.5634)  acc5: 72.2222 (36.7220)  time: 0.2621  data: 0.0753  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.0189 (4.0873)  acc1: 0.0000 (12.1957)  acc5: 27.7778 (35.8123)  time: 0.3261  data: 0.1279  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.0799 (4.0309)  acc1: 16.6667 (14.0911)  acc5: 50.0000 (37.5905)  time: 0.3110  data: 0.1150  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.7423 (4.0372)  acc1: 22.2222 (13.8581)  acc5: 50.0000 (37.3514)  time: 0.2531  data: 0.0843  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.7423 (4.0388)  acc1: 20.0000 (13.8644)  acc5: 50.0000 (37.3336)  time: 0.2307  data: 0.0685  max mem: 15572
Val: Total time: 0:01:31 (0.3365 s / it)
* Acc@1 13.864 Acc@5 37.334 loss 4.039
Accuracy of the network on the 4883 val videos: 13.9%
[2025-01-12 23:39:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-12 23:39:10,493] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-12 23:39:10,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-12 23:39:13,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-12 23:39:13,310] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 13.86%
Epoch: [5]  [   0/2079]  eta: 5:35:40  lr: 0.000047  min_lr: 0.000000  loss: 4.8849 (4.8849)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.6875  data: 8.9531  max mem: 15572
[2025-01-12 23:39:24,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:39:24,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [  10/2079]  eta: 0:43:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6149 (4.6558)  loss_scale: 131072.0000 (113198.5455)  weight_decay: 0.0500 (0.0500)  time: 1.2759  data: 0.8145  max mem: 15572
Epoch: [5]  [  20/2079]  eta: 0:33:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6149 (4.6963)  loss_scale: 131072.0000 (121709.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0811  max mem: 15572
[2025-01-12 23:39:36,884] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13150
[2025-01-12 23:39:36,885] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:39:36,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [  30/2079]  eta: 0:28:39  lr: 0.000047  min_lr: 0.000000  loss: 4.7966 (4.7240)  loss_scale: 131072.0000 (116273.5484)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1386  max mem: 15572
Epoch: [5]  [  40/2079]  eta: 0:26:52  lr: 0.000047  min_lr: 0.000000  loss: 4.7670 (4.7243)  loss_scale: 65536.0000 (103898.5366)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1639  max mem: 15572
Epoch: [5]  [  50/2079]  eta: 0:25:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6767 (4.7258)  loss_scale: 65536.0000 (96376.4706)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1728  max mem: 15572
Epoch: [5]  [  60/2079]  eta: 0:24:26  lr: 0.000047  min_lr: 0.000000  loss: 4.7309 (4.7406)  loss_scale: 65536.0000 (91320.6557)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1419  max mem: 15572
Epoch: [5]  [  70/2079]  eta: 0:23:17  lr: 0.000047  min_lr: 0.000000  loss: 4.7512 (4.7385)  loss_scale: 65536.0000 (87689.0141)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1080  max mem: 15572
Epoch: [5]  [  80/2079]  eta: 0:22:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6795 (4.7323)  loss_scale: 65536.0000 (84954.0741)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1209  max mem: 15572
Epoch: [5]  [  90/2079]  eta: 0:22:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6976 (4.7345)  loss_scale: 65536.0000 (82820.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1674  max mem: 15572
Epoch: [5]  [ 100/2079]  eta: 0:21:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6948 (4.7187)  loss_scale: 65536.0000 (81108.9109)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1243  max mem: 15572
Epoch: [5]  [ 110/2079]  eta: 0:21:48  lr: 0.000047  min_lr: 0.000000  loss: 4.7127 (4.7217)  loss_scale: 65536.0000 (79705.9459)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1433  max mem: 15572
Epoch: [5]  [ 120/2079]  eta: 0:21:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6718 (4.7039)  loss_scale: 65536.0000 (78534.8760)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1811  max mem: 15572
Epoch: [5]  [ 130/2079]  eta: 0:21:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6686 (4.7134)  loss_scale: 65536.0000 (77542.5954)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1356  max mem: 15572
Epoch: [5]  [ 140/2079]  eta: 0:21:00  lr: 0.000047  min_lr: 0.000000  loss: 4.7068 (4.7117)  loss_scale: 65536.0000 (76691.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0985  max mem: 15572
Epoch: [5]  [ 150/2079]  eta: 0:20:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6287 (4.7052)  loss_scale: 65536.0000 (75952.3179)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1243  max mem: 15572
[2025-01-12 23:40:53,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:40:53,514] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:40:54,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13281
[2025-01-12 23:40:54,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:40:54,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 160/2079]  eta: 0:20:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5991 (4.7008)  loss_scale: 65536.0000 (76119.4534)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1871  max mem: 15572
Epoch: [5]  [ 170/2079]  eta: 0:20:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6390 (4.7042)  loss_scale: 65536.0000 (75500.5380)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1480  max mem: 15572
Epoch: [5]  [ 180/2079]  eta: 0:20:17  lr: 0.000047  min_lr: 0.000000  loss: 4.7157 (4.6996)  loss_scale: 65536.0000 (74950.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1463  max mem: 15572
Epoch: [5]  [ 190/2079]  eta: 0:20:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6218 (4.6949)  loss_scale: 65536.0000 (74457.1309)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1485  max mem: 15572
Epoch: [5]  [ 200/2079]  eta: 0:19:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6085 (4.6941)  loss_scale: 65536.0000 (74013.2935)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1263  max mem: 15572
Epoch: [5]  [ 210/2079]  eta: 0:19:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6404 (4.6936)  loss_scale: 65536.0000 (73611.5261)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1762  max mem: 15572
Epoch: [5]  [ 220/2079]  eta: 0:19:42  lr: 0.000047  min_lr: 0.000000  loss: 4.7377 (4.6957)  loss_scale: 65536.0000 (73246.1176)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1647  max mem: 15572
Epoch: [5]  [ 230/2079]  eta: 0:19:36  lr: 0.000047  min_lr: 0.000000  loss: 4.7468 (4.6983)  loss_scale: 65536.0000 (72912.3463)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1734  max mem: 15572
Epoch: [5]  [ 240/2079]  eta: 0:19:24  lr: 0.000047  min_lr: 0.000000  loss: 4.6956 (4.6974)  loss_scale: 65536.0000 (72606.2739)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1580  max mem: 15572
Epoch: [5]  [ 250/2079]  eta: 0:19:14  lr: 0.000047  min_lr: 0.000000  loss: 4.7204 (4.6981)  loss_scale: 65536.0000 (72324.5896)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1265  max mem: 15572
Epoch: [5]  [ 260/2079]  eta: 0:19:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6924 (4.6963)  loss_scale: 65536.0000 (72064.4904)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1464  max mem: 15572
Epoch: [5]  [ 270/2079]  eta: 0:18:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6410 (4.6940)  loss_scale: 65536.0000 (71823.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1165  max mem: 15572
Epoch: [5]  [ 280/2079]  eta: 0:18:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6140 (4.6923)  loss_scale: 65536.0000 (71599.8292)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1002  max mem: 15572
[2025-01-12 23:42:12,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:42:12,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 290/2079]  eta: 0:18:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6635 (4.6911)  loss_scale: 65536.0000 (72292.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1183  max mem: 15572
[2025-01-12 23:42:17,088] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13417
[2025-01-12 23:42:17,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:42:17,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 300/2079]  eta: 0:18:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6973 (4.6906)  loss_scale: 65536.0000 (72721.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0573  max mem: 15572
Epoch: [5]  [ 310/2079]  eta: 0:18:08  lr: 0.000047  min_lr: 0.000000  loss: 4.7022 (4.6942)  loss_scale: 65536.0000 (72489.9807)  weight_decay: 0.0500 (0.0500)  time: 0.4730  data: 0.0244  max mem: 15572
Epoch: [5]  [ 320/2079]  eta: 0:18:00  lr: 0.000047  min_lr: 0.000000  loss: 4.8036 (4.6962)  loss_scale: 65536.0000 (72273.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0915  max mem: 15572
Epoch: [5]  [ 330/2079]  eta: 0:17:51  lr: 0.000047  min_lr: 0.000000  loss: 4.7519 (4.6960)  loss_scale: 65536.0000 (72069.8006)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0962  max mem: 15572
Epoch: [5]  [ 340/2079]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000000  loss: 4.7255 (4.6963)  loss_scale: 65536.0000 (71878.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1688  max mem: 15572
Epoch: [5]  [ 350/2079]  eta: 0:17:40  lr: 0.000047  min_lr: 0.000000  loss: 4.7255 (4.6962)  loss_scale: 65536.0000 (71697.5043)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.2005  max mem: 15572
Epoch: [5]  [ 360/2079]  eta: 0:17:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6709 (4.6957)  loss_scale: 65536.0000 (71526.8255)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0935  max mem: 15572
Epoch: [5]  [ 370/2079]  eta: 0:17:22  lr: 0.000047  min_lr: 0.000000  loss: 4.7104 (4.6956)  loss_scale: 65536.0000 (71365.3477)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1210  max mem: 15572
Epoch: [5]  [ 380/2079]  eta: 0:17:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6896 (4.6932)  loss_scale: 65536.0000 (71212.3465)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1832  max mem: 15572
Epoch: [5]  [ 390/2079]  eta: 0:17:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6180 (4.6931)  loss_scale: 65536.0000 (71067.1714)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1544  max mem: 15572
Epoch: [5]  [ 400/2079]  eta: 0:17:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5981 (4.6900)  loss_scale: 65536.0000 (70929.2369)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1694  max mem: 15572
Epoch: [5]  [ 410/2079]  eta: 0:17:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6204 (4.6884)  loss_scale: 65536.0000 (70798.0146)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1667  max mem: 15572
Epoch: [5]  [ 420/2079]  eta: 0:16:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5812 (4.6868)  loss_scale: 65536.0000 (70673.0261)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0818  max mem: 15572
[2025-01-12 23:43:31,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:43:31,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:43:35,394] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13553
[2025-01-12 23:43:35,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:43:35,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 430/2079]  eta: 0:16:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6144 (4.6860)  loss_scale: 65536.0000 (71618.2274)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0556  max mem: 15572
Epoch: [5]  [ 440/2079]  eta: 0:16:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6300 (4.6848)  loss_scale: 65536.0000 (71480.3084)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0685  max mem: 15572
Epoch: [5]  [ 450/2079]  eta: 0:16:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6553 (4.6852)  loss_scale: 65536.0000 (71348.5055)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1092  max mem: 15572
Epoch: [5]  [ 460/2079]  eta: 0:16:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6495 (4.6856)  loss_scale: 65536.0000 (71222.4208)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1183  max mem: 15572
Epoch: [5]  [ 470/2079]  eta: 0:16:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5940 (4.6827)  loss_scale: 65536.0000 (71101.6900)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1285  max mem: 15572
Epoch: [5]  [ 480/2079]  eta: 0:16:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5257 (4.6814)  loss_scale: 65536.0000 (70985.9792)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1621  max mem: 15572
Epoch: [5]  [ 490/2079]  eta: 0:16:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5345 (4.6809)  loss_scale: 65536.0000 (70874.9817)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1226  max mem: 15572
Epoch: [5]  [ 500/2079]  eta: 0:15:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5185 (4.6793)  loss_scale: 65536.0000 (70768.4152)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0978  max mem: 15572
Epoch: [5]  [ 510/2079]  eta: 0:15:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5795 (4.6777)  loss_scale: 65536.0000 (70666.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1460  max mem: 15572
Epoch: [5]  [ 520/2079]  eta: 0:15:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6999 (4.6780)  loss_scale: 65536.0000 (70567.5547)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1518  max mem: 15572
Epoch: [5]  [ 530/2079]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6618 (4.6769)  loss_scale: 65536.0000 (70472.7985)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0962  max mem: 15572
Epoch: [5]  [ 540/2079]  eta: 0:15:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6061 (4.6752)  loss_scale: 65536.0000 (70381.5453)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0964  max mem: 15572
Epoch: [5]  [ 550/2079]  eta: 0:15:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5927 (4.6748)  loss_scale: 65536.0000 (70293.6044)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1251  max mem: 15572
[2025-01-12 23:44:50,416] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:44:50,417] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:44:51,455] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13683
[2025-01-12 23:44:51,456] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:44:51,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 560/2079]  eta: 0:15:15  lr: 0.000047  min_lr: 0.000000  loss: 4.7232 (4.6761)  loss_scale: 65536.0000 (70325.6185)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1394  max mem: 15572
Epoch: [5]  [ 570/2079]  eta: 0:15:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6453 (4.6745)  loss_scale: 65536.0000 (70241.7373)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.1758  max mem: 15572
Epoch: [5]  [ 580/2079]  eta: 0:15:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5677 (4.6751)  loss_scale: 65536.0000 (70160.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1470  max mem: 15572
Epoch: [5]  [ 590/2079]  eta: 0:14:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6307 (4.6759)  loss_scale: 65536.0000 (70082.4907)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0962  max mem: 15572
Epoch: [5]  [ 600/2079]  eta: 0:14:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6307 (4.6744)  loss_scale: 65536.0000 (70006.8419)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1019  max mem: 15572
Epoch: [5]  [ 610/2079]  eta: 0:14:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6002 (4.6740)  loss_scale: 65536.0000 (69933.6694)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1556  max mem: 15572
Epoch: [5]  [ 620/2079]  eta: 0:14:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6957 (4.6754)  loss_scale: 65536.0000 (69862.8535)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.2238  max mem: 15572
Epoch: [5]  [ 630/2079]  eta: 0:14:34  lr: 0.000047  min_lr: 0.000000  loss: 4.7461 (4.6755)  loss_scale: 65536.0000 (69794.2821)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.2101  max mem: 15572
Epoch: [5]  [ 640/2079]  eta: 0:14:29  lr: 0.000047  min_lr: 0.000000  loss: 4.7552 (4.6767)  loss_scale: 65536.0000 (69727.8502)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1877  max mem: 15572
Epoch: [5]  [ 650/2079]  eta: 0:14:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6442 (4.6764)  loss_scale: 65536.0000 (69663.4593)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1079  max mem: 15572
Epoch: [5]  [ 660/2079]  eta: 0:14:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6774 (4.6768)  loss_scale: 65536.0000 (69601.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0856  max mem: 15572
Epoch: [5]  [ 670/2079]  eta: 0:14:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6774 (4.6766)  loss_scale: 65536.0000 (69540.4352)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1354  max mem: 15572
Epoch: [5]  [ 680/2079]  eta: 0:14:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6102 (4.6754)  loss_scale: 65536.0000 (69481.6329)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0972  max mem: 15572
[2025-01-12 23:46:07,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:46:07,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 690/2079]  eta: 0:13:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6048 (4.6737)  loss_scale: 65536.0000 (69614.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1400  max mem: 15572
Epoch: [5]  [ 700/2079]  eta: 0:13:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5822 (4.6731)  loss_scale: 131072.0000 (70490.9330)  weight_decay: 0.0500 (0.0500)  time: 0.7151  data: 0.2824  max mem: 15572
[2025-01-12 23:46:21,357] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13832
[2025-01-12 23:46:21,357] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:46:21,357] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 710/2079]  eta: 0:13:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5475 (4.6710)  loss_scale: 131072.0000 (71158.6385)  weight_decay: 0.0500 (0.0500)  time: 0.7131  data: 0.2464  max mem: 15572
Epoch: [5]  [ 720/2079]  eta: 0:13:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6383 (4.6719)  loss_scale: 65536.0000 (71080.6546)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1240  max mem: 15572
Epoch: [5]  [ 730/2079]  eta: 0:13:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6476 (4.6714)  loss_scale: 65536.0000 (71004.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0818  max mem: 15572
Epoch: [5]  [ 740/2079]  eta: 0:13:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6796 (4.6715)  loss_scale: 65536.0000 (70931.0013)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1164  max mem: 15572
Epoch: [5]  [ 750/2079]  eta: 0:13:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6796 (4.6715)  loss_scale: 65536.0000 (70859.1638)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.1815  max mem: 15572
Epoch: [5]  [ 760/2079]  eta: 0:13:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5781 (4.6700)  loss_scale: 65536.0000 (70789.2142)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1238  max mem: 15572
Epoch: [5]  [ 770/2079]  eta: 0:13:07  lr: 0.000047  min_lr: 0.000000  loss: 4.6808 (4.6700)  loss_scale: 65536.0000 (70721.0791)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0840  max mem: 15572
Epoch: [5]  [ 780/2079]  eta: 0:12:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5753 (4.6679)  loss_scale: 65536.0000 (70654.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0597  max mem: 15572
Epoch: [5]  [ 790/2079]  eta: 0:12:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5753 (4.6689)  loss_scale: 65536.0000 (70589.9772)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1370  max mem: 15572
Epoch: [5]  [ 800/2079]  eta: 0:12:48  lr: 0.000047  min_lr: 0.000000  loss: 4.6634 (4.6676)  loss_scale: 65536.0000 (70526.8814)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1826  max mem: 15572
Epoch: [5]  [ 810/2079]  eta: 0:12:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6191 (4.6672)  loss_scale: 65536.0000 (70465.3416)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0777  max mem: 15572
Epoch: [5]  [ 820/2079]  eta: 0:12:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6441 (4.6674)  loss_scale: 65536.0000 (70405.3009)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0630  max mem: 15572
Epoch: [5]  [ 830/2079]  eta: 0:12:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6441 (4.6675)  loss_scale: 65536.0000 (70346.7052)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1321  max mem: 15572
[2025-01-12 23:47:35,923] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:47:35,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 840/2079]  eta: 0:12:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6155 (4.6665)  loss_scale: 65536.0000 (70523.2818)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1491  max mem: 15572
[2025-01-12 23:47:38,049] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13964
[2025-01-12 23:47:38,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:47:38,050] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 850/2079]  eta: 0:12:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6486 (4.6662)  loss_scale: 65536.0000 (70464.6769)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1420  max mem: 15572
Epoch: [5]  [ 860/2079]  eta: 0:12:08  lr: 0.000047  min_lr: 0.000000  loss: 4.7015 (4.6671)  loss_scale: 65536.0000 (70407.4332)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1124  max mem: 15572
Epoch: [5]  [ 870/2079]  eta: 0:12:03  lr: 0.000047  min_lr: 0.000000  loss: 4.8186 (4.6693)  loss_scale: 65536.0000 (70351.5040)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0980  max mem: 15572
[2025-01-12 23:47:57,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=84, lr=[4.5400500581556436e-07, 4.5400500581556436e-07, 6.485785797365206e-07, 6.485785797365206e-07, 9.265408281950296e-07, 9.265408281950296e-07, 1.323629754564328e-06, 1.323629754564328e-06, 1.8908996493776116e-06, 1.8908996493776116e-06, 2.701285213396588e-06, 2.701285213396588e-06, 3.85897887628084e-06, 3.85897887628084e-06, 5.512826966115487e-06, 5.512826966115487e-06, 7.875467094450695e-06, 7.875467094450695e-06, 1.1250667277786708e-05, 1.1250667277786708e-05, 1.6072381825409582e-05, 1.6072381825409582e-05, 2.2960545464870836e-05, 2.2960545464870836e-05, 3.2800779235529764e-05, 3.2800779235529764e-05, 4.6858256050756814e-05, 4.6858256050756814e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:47:57,721] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=27.7764499418241, CurrSamplesPerSec=30.36721100206705, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 880/2079]  eta: 0:11:55  lr: 0.000047  min_lr: 0.000000  loss: 4.8085 (4.6698)  loss_scale: 65536.0000 (70296.8445)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0843  max mem: 15572
Epoch: [5]  [ 890/2079]  eta: 0:11:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6265 (4.6695)  loss_scale: 65536.0000 (70243.4119)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1223  max mem: 15572
Epoch: [5]  [ 900/2079]  eta: 0:11:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5940 (4.6696)  loss_scale: 65536.0000 (70191.1654)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1707  max mem: 15572
Epoch: [5]  [ 910/2079]  eta: 0:11:39  lr: 0.000047  min_lr: 0.000000  loss: 4.7001 (4.6709)  loss_scale: 65536.0000 (70140.0659)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1445  max mem: 15572
[2025-01-12 23:48:21,621] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14036
[2025-01-12 23:48:21,621] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-12 23:48:21,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 920/2079]  eta: 0:11:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6662 (4.6703)  loss_scale: 65536.0000 (69805.4463)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1746  max mem: 15572
Epoch: [5]  [ 930/2079]  eta: 0:11:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6035 (4.6711)  loss_scale: 32768.0000 (69407.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1335  max mem: 15572
Epoch: [5]  [ 940/2079]  eta: 0:11:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6270 (4.6694)  loss_scale: 32768.0000 (69018.2529)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1165  max mem: 15572
Epoch: [5]  [ 950/2079]  eta: 0:11:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5512 (4.6695)  loss_scale: 32768.0000 (68637.0726)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1584  max mem: 15572
Epoch: [5]  [ 960/2079]  eta: 0:11:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6826 (4.6702)  loss_scale: 32768.0000 (68263.8252)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1453  max mem: 15572
Epoch: [5]  [ 970/2079]  eta: 0:11:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6462 (4.6695)  loss_scale: 32768.0000 (67898.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0866  max mem: 15572
Epoch: [5]  [ 980/2079]  eta: 0:10:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6554 (4.6702)  loss_scale: 32768.0000 (67540.1590)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.0852  max mem: 15572
Epoch: [5]  [ 990/2079]  eta: 0:10:49  lr: 0.000047  min_lr: 0.000000  loss: 4.7375 (4.6701)  loss_scale: 32768.0000 (67189.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1232  max mem: 15572
Epoch: [5]  [1000/2079]  eta: 0:10:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5277 (4.6690)  loss_scale: 32768.0000 (66845.4106)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.2038  max mem: 15572
Epoch: [5]  [1010/2079]  eta: 0:10:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6129 (4.6699)  loss_scale: 32768.0000 (66508.3442)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1441  max mem: 15572
Epoch: [5]  [1020/2079]  eta: 0:10:32  lr: 0.000047  min_lr: 0.000000  loss: 4.8852 (4.6715)  loss_scale: 32768.0000 (66177.8805)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0563  max mem: 15572
Epoch: [5]  [1030/2079]  eta: 0:10:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7738 (4.6724)  loss_scale: 32768.0000 (65853.8274)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1008  max mem: 15572
Epoch: [5]  [1040/2079]  eta: 0:10:17  lr: 0.000047  min_lr: 0.000000  loss: 4.7228 (4.6715)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4720  data: 0.0452  max mem: 15572
[2025-01-12 23:49:33,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:49:33,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1050/2079]  eta: 0:10:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5224 (4.6704)  loss_scale: 32768.0000 (65504.8221)  weight_decay: 0.0500 (0.0500)  time: 0.4261  data: 0.0005  max mem: 15572
Epoch: [5]  [1060/2079]  eta: 0:10:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5343 (4.6702)  loss_scale: 65536.0000 (65505.1159)  weight_decay: 0.0500 (0.0500)  time: 0.4427  data: 0.0007  max mem: 15572
Epoch: [5]  [1070/2079]  eta: 0:09:56  lr: 0.000047  min_lr: 0.000000  loss: 4.7480 (4.6714)  loss_scale: 65536.0000 (65505.4043)  weight_decay: 0.0500 (0.0500)  time: 0.4688  data: 0.0009  max mem: 15572
Epoch: [5]  [1080/2079]  eta: 0:09:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6473 (4.6701)  loss_scale: 65536.0000 (65505.6873)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.0841  max mem: 15572
Epoch: [5]  [1090/2079]  eta: 0:09:45  lr: 0.000047  min_lr: 0.000000  loss: 4.5349 (4.6684)  loss_scale: 65536.0000 (65505.9652)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.1843  max mem: 15572
Epoch: [5]  [1100/2079]  eta: 0:09:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6521 (4.6684)  loss_scale: 65536.0000 (65506.2380)  weight_decay: 0.0500 (0.0500)  time: 0.7413  data: 0.2744  max mem: 15572
Epoch: [5]  [1110/2079]  eta: 0:09:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6506 (4.6672)  loss_scale: 65536.0000 (65506.5059)  weight_decay: 0.0500 (0.0500)  time: 0.7960  data: 0.3361  max mem: 15572
Epoch: [5]  [1120/2079]  eta: 0:09:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5616 (4.6672)  loss_scale: 65536.0000 (65506.7690)  weight_decay: 0.0500 (0.0500)  time: 0.7504  data: 0.2834  max mem: 15572
Epoch: [5]  [1130/2079]  eta: 0:09:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6510 (4.6678)  loss_scale: 65536.0000 (65507.0274)  weight_decay: 0.0500 (0.0500)  time: 0.7437  data: 0.2759  max mem: 15572
Epoch: [5]  [1140/2079]  eta: 0:09:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6958 (4.6683)  loss_scale: 65536.0000 (65507.2813)  weight_decay: 0.0500 (0.0500)  time: 0.7311  data: 0.2648  max mem: 15572
Epoch: [5]  [1150/2079]  eta: 0:09:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6387 (4.6679)  loss_scale: 65536.0000 (65507.5308)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.2618  max mem: 15572
Epoch: [5]  [1160/2079]  eta: 0:09:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5962 (4.6676)  loss_scale: 65536.0000 (65507.7761)  weight_decay: 0.0500 (0.0500)  time: 0.7185  data: 0.2506  max mem: 15572
[2025-01-12 23:50:58,701] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:50:58,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1170/2079]  eta: 0:09:07  lr: 0.000047  min_lr: 0.000000  loss: 4.7523 (4.6685)  loss_scale: 65536.0000 (65563.9829)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.2141  max mem: 15572
[2025-01-12 23:51:00,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14295
[2025-01-12 23:51:00,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:51:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1180/2079]  eta: 0:09:00  lr: 0.000047  min_lr: 0.000000  loss: 4.7564 (4.6689)  loss_scale: 65536.0000 (65619.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1634  max mem: 15572
Epoch: [5]  [1190/2079]  eta: 0:08:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5817 (4.6678)  loss_scale: 65536.0000 (65618.5390)  weight_decay: 0.0500 (0.0500)  time: 0.4707  data: 0.0482  max mem: 15572
Epoch: [5]  [1200/2079]  eta: 0:08:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6373 (4.6687)  loss_scale: 65536.0000 (65617.8518)  weight_decay: 0.0500 (0.0500)  time: 0.4321  data: 0.0007  max mem: 15572
Epoch: [5]  [1210/2079]  eta: 0:08:39  lr: 0.000047  min_lr: 0.000000  loss: 4.7529 (4.6689)  loss_scale: 65536.0000 (65617.1759)  weight_decay: 0.0500 (0.0500)  time: 0.4624  data: 0.0009  max mem: 15572
Epoch: [5]  [1220/2079]  eta: 0:08:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6506 (4.6688)  loss_scale: 65536.0000 (65616.5111)  weight_decay: 0.0500 (0.0500)  time: 0.4639  data: 0.0009  max mem: 15572
Epoch: [5]  [1230/2079]  eta: 0:08:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6631 (4.6687)  loss_scale: 65536.0000 (65615.8570)  weight_decay: 0.0500 (0.0500)  time: 0.4656  data: 0.0008  max mem: 15572
Epoch: [5]  [1240/2079]  eta: 0:08:19  lr: 0.000047  min_lr: 0.000000  loss: 4.7027 (4.6684)  loss_scale: 65536.0000 (65615.2135)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0766  max mem: 15572
Epoch: [5]  [1250/2079]  eta: 0:08:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5486 (4.6681)  loss_scale: 65536.0000 (65614.5803)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.0918  max mem: 15572
Epoch: [5]  [1260/2079]  eta: 0:08:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5973 (4.6672)  loss_scale: 65536.0000 (65613.9572)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1104  max mem: 15572
Epoch: [5]  [1270/2079]  eta: 0:08:01  lr: 0.000047  min_lr: 0.000000  loss: 4.6231 (4.6667)  loss_scale: 65536.0000 (65613.3438)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1740  max mem: 15572
Epoch: [5]  [1280/2079]  eta: 0:07:55  lr: 0.000047  min_lr: 0.000000  loss: 4.7213 (4.6672)  loss_scale: 65536.0000 (65612.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1122  max mem: 15572
Epoch: [5]  [1290/2079]  eta: 0:07:49  lr: 0.000047  min_lr: 0.000000  loss: 4.7213 (4.6672)  loss_scale: 65536.0000 (65612.1456)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0874  max mem: 15572
Epoch: [5]  [1300/2079]  eta: 0:07:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6511 (4.6669)  loss_scale: 65536.0000 (65611.5603)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1528  max mem: 15572
[2025-01-12 23:52:08,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:52:08,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-12 23:52:15,182] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14433
[2025-01-12 23:52:15,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:52:15,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1310/2079]  eta: 0:07:38  lr: 0.000047  min_lr: 0.000000  loss: 4.7283 (4.6673)  loss_scale: 65536.0000 (66060.8879)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.1976  max mem: 15572
Epoch: [5]  [1320/2079]  eta: 0:07:32  lr: 0.000047  min_lr: 0.000000  loss: 4.7283 (4.6675)  loss_scale: 65536.0000 (66056.9145)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1913  max mem: 15572
Epoch: [5]  [1330/2079]  eta: 0:07:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6848 (4.6677)  loss_scale: 65536.0000 (66053.0008)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1552  max mem: 15572
Epoch: [5]  [1340/2079]  eta: 0:07:20  lr: 0.000047  min_lr: 0.000000  loss: 4.7090 (4.6680)  loss_scale: 65536.0000 (66049.1454)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1104  max mem: 15572
Epoch: [5]  [1350/2079]  eta: 0:07:14  lr: 0.000047  min_lr: 0.000000  loss: 4.7090 (4.6684)  loss_scale: 65536.0000 (66045.3472)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1059  max mem: 15572
Epoch: [5]  [1360/2079]  eta: 0:07:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6055 (4.6676)  loss_scale: 65536.0000 (66041.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1877  max mem: 15572
Epoch: [5]  [1370/2079]  eta: 0:07:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5928 (4.6681)  loss_scale: 65536.0000 (66037.9168)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.2203  max mem: 15572
Epoch: [5]  [1380/2079]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6627 (4.6679)  loss_scale: 65536.0000 (66034.2824)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1368  max mem: 15572
Epoch: [5]  [1390/2079]  eta: 0:06:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6370 (4.6678)  loss_scale: 65536.0000 (66030.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1237  max mem: 15572
Epoch: [5]  [1400/2079]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.7084 (4.6677)  loss_scale: 65536.0000 (66027.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1435  max mem: 15572
Epoch: [5]  [1410/2079]  eta: 0:06:38  lr: 0.000047  min_lr: 0.000000  loss: 4.7139 (4.6677)  loss_scale: 65536.0000 (66023.6882)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1264  max mem: 15572
Epoch: [5]  [1420/2079]  eta: 0:06:32  lr: 0.000047  min_lr: 0.000000  loss: 4.7139 (4.6679)  loss_scale: 65536.0000 (66020.2562)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1242  max mem: 15572
Epoch: [5]  [1430/2079]  eta: 0:06:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6606 (4.6680)  loss_scale: 65536.0000 (66016.8721)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0639  max mem: 15572
[2025-01-12 23:53:28,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:53:28,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1440/2079]  eta: 0:06:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5897 (4.6675)  loss_scale: 65536.0000 (66104.4941)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0772  max mem: 15572
[2025-01-12 23:53:34,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14572
[2025-01-12 23:53:34,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:53:34,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1450/2079]  eta: 0:06:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5329 (4.6662)  loss_scale: 65536.0000 (66461.9049)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1744  max mem: 15572
Epoch: [5]  [1460/2079]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6221 (4.6659)  loss_scale: 65536.0000 (66455.5674)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1721  max mem: 15572
Epoch: [5]  [1470/2079]  eta: 0:06:01  lr: 0.000047  min_lr: 0.000000  loss: 4.6431 (4.6659)  loss_scale: 65536.0000 (66449.3161)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.1136  max mem: 15572
Epoch: [5]  [1480/2079]  eta: 0:05:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6569 (4.6658)  loss_scale: 65536.0000 (66443.1492)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0853  max mem: 15572
Epoch: [5]  [1490/2079]  eta: 0:05:49  lr: 0.000047  min_lr: 0.000000  loss: 4.7059 (4.6661)  loss_scale: 65536.0000 (66437.0651)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1066  max mem: 15572
Epoch: [5]  [1500/2079]  eta: 0:05:43  lr: 0.000047  min_lr: 0.000000  loss: 4.7041 (4.6665)  loss_scale: 65536.0000 (66431.0620)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1587  max mem: 15572
Epoch: [5]  [1510/2079]  eta: 0:05:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6789 (4.6664)  loss_scale: 65536.0000 (66425.1383)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1365  max mem: 15572
Epoch: [5]  [1520/2079]  eta: 0:05:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6789 (4.6666)  loss_scale: 65536.0000 (66419.2926)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1305  max mem: 15572
Epoch: [5]  [1530/2079]  eta: 0:05:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7178 (4.6667)  loss_scale: 65536.0000 (66413.5232)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1663  max mem: 15572
Epoch: [5]  [1540/2079]  eta: 0:05:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5752 (4.6663)  loss_scale: 65536.0000 (66407.8287)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1195  max mem: 15572
Epoch: [5]  [1550/2079]  eta: 0:05:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5497 (4.6654)  loss_scale: 65536.0000 (66402.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0856  max mem: 15572
Epoch: [5]  [1560/2079]  eta: 0:05:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5665 (4.6648)  loss_scale: 65536.0000 (66396.6586)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1157  max mem: 15572
Epoch: [5]  [1570/2079]  eta: 0:05:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6159 (4.6645)  loss_scale: 65536.0000 (66391.1801)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1818  max mem: 15572
[2025-01-12 23:54:51,804] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:54:51,804] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1580/2079]  eta: 0:04:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6159 (4.6642)  loss_scale: 65536.0000 (66510.1278)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.1997  max mem: 15572
Epoch: [5]  [1590/2079]  eta: 0:04:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5885 (4.6638)  loss_scale: 131072.0000 (66915.9221)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1625  max mem: 15572
[2025-01-12 23:55:03,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14721
[2025-01-12 23:55:03,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:55:03,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1600/2079]  eta: 0:04:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4663 (4.6629)  loss_scale: 131072.0000 (67193.8438)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1217  max mem: 15572
Epoch: [5]  [1610/2079]  eta: 0:04:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5669 (4.6626)  loss_scale: 65536.0000 (67183.5531)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1182  max mem: 15572
Epoch: [5]  [1620/2079]  eta: 0:04:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6003 (4.6623)  loss_scale: 65536.0000 (67173.3893)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1359  max mem: 15572
Epoch: [5]  [1630/2079]  eta: 0:04:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6462 (4.6628)  loss_scale: 65536.0000 (67163.3501)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1892  max mem: 15572
Epoch: [5]  [1640/2079]  eta: 0:04:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6873 (4.6621)  loss_scale: 65536.0000 (67153.4333)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1504  max mem: 15572
Epoch: [5]  [1650/2079]  eta: 0:04:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5107 (4.6616)  loss_scale: 65536.0000 (67143.6366)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0716  max mem: 15572
Epoch: [5]  [1660/2079]  eta: 0:04:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5094 (4.6608)  loss_scale: 65536.0000 (67133.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1012  max mem: 15572
Epoch: [5]  [1670/2079]  eta: 0:04:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5607 (4.6611)  loss_scale: 65536.0000 (67124.3950)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0951  max mem: 15572
Epoch: [5]  [1680/2079]  eta: 0:03:56  lr: 0.000047  min_lr: 0.000000  loss: 4.7176 (4.6611)  loss_scale: 65536.0000 (67114.9459)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0743  max mem: 15572
Epoch: [5]  [1690/2079]  eta: 0:03:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5521 (4.6607)  loss_scale: 65536.0000 (67105.6085)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1592  max mem: 15572
Epoch: [5]  [1700/2079]  eta: 0:03:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5005 (4.6599)  loss_scale: 65536.0000 (67096.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1194  max mem: 15572
Epoch: [5]  [1710/2079]  eta: 0:03:38  lr: 0.000047  min_lr: 0.000000  loss: 4.7179 (4.6610)  loss_scale: 65536.0000 (67087.2613)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0455  max mem: 15572
Epoch: [5]  [1720/2079]  eta: 0:03:32  lr: 0.000047  min_lr: 0.000000  loss: 4.7634 (4.6610)  loss_scale: 65536.0000 (67078.2475)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1564  max mem: 15572
[2025-01-12 23:56:19,100] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:56:19,101] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1730/2079]  eta: 0:03:26  lr: 0.000047  min_lr: 0.000000  loss: 4.7164 (4.6617)  loss_scale: 65536.0000 (67220.7787)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.2232  max mem: 15572
[2025-01-12 23:56:24,571] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14858
[2025-01-12 23:56:24,572] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:56:24,572] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1740/2079]  eta: 0:03:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6299 (4.6604)  loss_scale: 65536.0000 (67361.6726)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.2019  max mem: 15572
Epoch: [5]  [1750/2079]  eta: 0:03:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5377 (4.6599)  loss_scale: 65536.0000 (67351.2461)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1098  max mem: 15572
Epoch: [5]  [1760/2079]  eta: 0:03:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6148 (4.6597)  loss_scale: 65536.0000 (67340.9381)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0654  max mem: 15572
Epoch: [5]  [1770/2079]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6374 (4.6597)  loss_scale: 65536.0000 (67330.7465)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1261  max mem: 15572
Epoch: [5]  [1780/2079]  eta: 0:02:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6663 (4.6593)  loss_scale: 65536.0000 (67320.6693)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1167  max mem: 15572
Epoch: [5]  [1790/2079]  eta: 0:02:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6663 (4.6593)  loss_scale: 65536.0000 (67310.7046)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1275  max mem: 15572
Epoch: [5]  [1800/2079]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6398 (4.6590)  loss_scale: 65536.0000 (67300.8506)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1858  max mem: 15572
Epoch: [5]  [1810/2079]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6816 (4.6587)  loss_scale: 65536.0000 (67291.1055)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1207  max mem: 15572
Epoch: [5]  [1820/2079]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000000  loss: 4.7125 (4.6593)  loss_scale: 65536.0000 (67281.4673)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0840  max mem: 15572
Epoch: [5]  [1830/2079]  eta: 0:02:27  lr: 0.000047  min_lr: 0.000000  loss: 4.7152 (4.6595)  loss_scale: 65536.0000 (67271.9345)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1078  max mem: 15572
Epoch: [5]  [1840/2079]  eta: 0:02:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6548 (4.6593)  loss_scale: 65536.0000 (67262.5052)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1736  max mem: 15572
Epoch: [5]  [1850/2079]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5829 (4.6589)  loss_scale: 65536.0000 (67253.1777)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2052  max mem: 15572
Epoch: [5]  [1860/2079]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6012 (4.6587)  loss_scale: 65536.0000 (67243.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1382  max mem: 15572
[2025-01-12 23:57:37,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:57:37,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1870/2079]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6127 (4.6580)  loss_scale: 65536.0000 (67480.0128)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1255  max mem: 15572
[2025-01-12 23:57:43,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14997
[2025-01-12 23:57:43,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:57:43,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-12 23:57:45,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=91, lr=[4.5342352527843214e-07, 4.5342352527843214e-07, 6.477478932549032e-07, 6.477478932549032e-07, 9.253541332212903e-07, 9.253541332212903e-07, 1.3219344760304147e-06, 1.3219344760304147e-06, 1.8884778229005928e-06, 1.8884778229005928e-06, 2.697825461286561e-06, 2.697825461286561e-06, 3.854036373266516e-06, 3.854036373266516e-06, 5.505766247523595e-06, 5.505766247523595e-06, 7.865380353605135e-06, 7.865380353605135e-06, 1.1236257648007339e-05, 1.1236257648007339e-05, 1.6051796640010482e-05, 1.6051796640010482e-05, 2.2931138057157833e-05, 2.2931138057157833e-05, 3.2758768653082624e-05, 3.2758768653082624e-05, 4.679824093297518e-05, 4.679824093297518e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-12 23:57:45,456] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=27.787207174241914, CurrSamplesPerSec=30.612974172327217, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [1880/2079]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6055 (4.6580)  loss_scale: 65536.0000 (67574.2010)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1150  max mem: 15572
Epoch: [5]  [1890/2079]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6055 (4.6580)  loss_scale: 65536.0000 (67563.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0836  max mem: 15572
Epoch: [5]  [1900/2079]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.7081 (4.6580)  loss_scale: 65536.0000 (67552.7575)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1007  max mem: 15572
Epoch: [5]  [1910/2079]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6008 (4.6576)  loss_scale: 65536.0000 (67542.2041)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1409  max mem: 15572
Epoch: [5]  [1920/2079]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6008 (4.6575)  loss_scale: 65536.0000 (67531.7605)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1406  max mem: 15572
Epoch: [5]  [1930/2079]  eta: 0:01:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5478 (4.6568)  loss_scale: 65536.0000 (67521.4252)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1301  max mem: 15572
Epoch: [5]  [1940/2079]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5178 (4.6558)  loss_scale: 65536.0000 (67511.1963)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1002  max mem: 15572
Epoch: [5]  [1950/2079]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5237 (4.6555)  loss_scale: 65536.0000 (67501.0723)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1184  max mem: 15572
Epoch: [5]  [1960/2079]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6480 (4.6555)  loss_scale: 65536.0000 (67491.0515)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1149  max mem: 15572
Epoch: [5]  [1970/2079]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6222 (4.6554)  loss_scale: 65536.0000 (67481.1324)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1127  max mem: 15572
Epoch: [5]  [1980/2079]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5258 (4.6549)  loss_scale: 65536.0000 (67471.3135)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1870  max mem: 15572
Epoch: [5]  [1990/2079]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5375 (4.6545)  loss_scale: 65536.0000 (67461.5932)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1596  max mem: 15572
Epoch: [5]  [2000/2079]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5958 (4.6545)  loss_scale: 65536.0000 (67451.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1386  max mem: 15572
[2025-01-12 23:58:59,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-12 23:58:59,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2010/2079]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5958 (4.6541)  loss_scale: 65536.0000 (67703.1527)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1271  max mem: 15572
[2025-01-12 23:59:05,235] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15136
[2025-01-12 23:59:05,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-12 23:59:05,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2020/2079]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6517 (4.6538)  loss_scale: 65536.0000 (67757.2845)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0967  max mem: 15572
Epoch: [5]  [2030/2079]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.7236 (4.6542)  loss_scale: 65536.0000 (67746.3476)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1027  max mem: 15572
Epoch: [5]  [2040/2079]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6638 (4.6546)  loss_scale: 65536.0000 (67735.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1493  max mem: 15572
Epoch: [5]  [2050/2079]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6638 (4.6544)  loss_scale: 65536.0000 (67724.7938)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1721  max mem: 15572
Epoch: [5]  [2060/2079]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6409 (4.6543)  loss_scale: 65536.0000 (67714.1737)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1657  max mem: 15572
Epoch: [5]  [2070/2079]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.6005 (4.6534)  loss_scale: 65536.0000 (67703.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1161  max mem: 15572
Epoch: [5]  [2078/2079]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5373 (4.6533)  loss_scale: 65536.0000 (67695.3151)  weight_decay: 0.0500 (0.0500)  time: 0.4502  data: 0.0237  max mem: 15572
Epoch: [5] Total time: 0:20:28 (0.5908 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.5373 (4.6533)  loss_scale: 65536.0000 (67695.3151)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 3108
Indices to remove: tensor([   20,    40,    45,  ..., 33675, 33685, 33686], device='cuda:0')
length of data loader train is: 1820
num_training_steps_per_epoch is: 1820
Change step level LR scheduler!
Set warmup steps = 9100
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:20:08  loss: 2.2472 (2.2472)  acc1: 50.0000 (50.0000)  acc5: 100.0000 (100.0000)  time: 4.4420  data: 4.2611  max mem: 15572
Val:  [ 10/272]  eta: 0:03:31  loss: 4.3974 (4.1685)  acc1: 0.0000 (14.1414)  acc5: 5.5556 (25.2525)  time: 0.8072  data: 0.6019  max mem: 15572
Val:  [ 20/272]  eta: 0:02:20  loss: 4.0270 (4.0631)  acc1: 0.0000 (11.6402)  acc5: 27.7778 (29.3651)  time: 0.3639  data: 0.1675  max mem: 15572
Val:  [ 30/272]  eta: 0:01:55  loss: 4.0780 (4.0933)  acc1: 0.0000 (9.3190)  acc5: 33.3333 (30.1075)  time: 0.2980  data: 0.1028  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 3.8202 (3.9804)  acc1: 5.5556 (10.2981)  acc5: 38.8889 (35.9079)  time: 0.2937  data: 0.1016  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 3.5476 (3.9064)  acc1: 11.1111 (12.9630)  acc5: 50.0000 (38.8889)  time: 0.3256  data: 0.1333  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 2.9207 (3.7693)  acc1: 27.7778 (18.9435)  acc5: 72.2222 (43.2605)  time: 0.3475  data: 0.1416  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 3.1477 (3.6902)  acc1: 27.7778 (20.5008)  acc5: 72.2222 (47.5743)  time: 0.3052  data: 0.1020  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 3.2903 (3.7146)  acc1: 11.1111 (19.4787)  acc5: 66.6667 (46.5021)  time: 0.3221  data: 0.1131  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 4.3682 (3.7983)  acc1: 0.0000 (17.4603)  acc5: 11.1111 (42.0635)  time: 0.3364  data: 0.1232  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 4.3682 (3.8526)  acc1: 0.0000 (15.9516)  acc5: 5.5556 (40.4840)  time: 0.3520  data: 0.1507  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.1956 (3.8926)  acc1: 0.0000 (15.2653)  acc5: 22.2222 (39.7898)  time: 0.3416  data: 0.1415  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 4.1952 (3.9193)  acc1: 5.5556 (15.0138)  acc5: 22.2222 (38.9807)  time: 0.3256  data: 0.1097  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.1242 (3.8861)  acc1: 5.5556 (15.7761)  acc5: 27.7778 (40.2036)  time: 0.3166  data: 0.1009  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 3.4491 (3.8696)  acc1: 22.2222 (16.8637)  acc5: 50.0000 (40.7407)  time: 0.3025  data: 0.0941  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.7654 (3.8698)  acc1: 11.1111 (16.4091)  acc5: 33.3333 (40.1766)  time: 0.3148  data: 0.0980  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.8536 (3.8656)  acc1: 5.5556 (15.8040)  acc5: 33.3333 (40.6142)  time: 0.3188  data: 0.1069  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.8592 (3.8826)  acc1: 5.5556 (15.7570)  acc5: 50.0000 (40.3509)  time: 0.2861  data: 0.0943  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.7671 (3.8713)  acc1: 5.5556 (15.1934)  acc5: 38.8889 (40.9454)  time: 0.2686  data: 0.0837  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.8682 (3.8813)  acc1: 0.0000 (14.9796)  acc5: 33.3333 (40.0524)  time: 0.3193  data: 0.1208  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.8005 (3.8730)  acc1: 0.0000 (15.6440)  acc5: 38.8889 (41.2383)  time: 0.3055  data: 0.0986  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.8636 (3.8907)  acc1: 11.1111 (15.4028)  acc5: 50.0000 (41.0742)  time: 0.2641  data: 0.0629  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.1444 (3.8913)  acc1: 5.5556 (15.8874)  acc5: 38.8889 (41.2016)  time: 0.2995  data: 0.1073  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.6697 (3.8844)  acc1: 27.7778 (16.5224)  acc5: 66.6667 (42.2799)  time: 0.3542  data: 0.1605  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.5787 (3.8672)  acc1: 27.7778 (16.9894)  acc5: 66.6667 (43.4071)  time: 0.3344  data: 0.1378  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.6564 (3.8912)  acc1: 5.5556 (16.4896)  acc5: 55.5556 (42.4745)  time: 0.3208  data: 0.1248  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.5573 (3.8423)  acc1: 11.1111 (18.3695)  acc5: 61.1111 (44.0826)  time: 0.2779  data: 0.0764  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.6638 (3.8510)  acc1: 27.7778 (18.0607)  acc5: 61.1111 (43.7884)  time: 0.2055  data: 0.0283  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.6638 (3.8540)  acc1: 22.2222 (18.0627)  acc5: 55.5556 (43.7641)  time: 0.1975  data: 0.0282  max mem: 15572
Val: Total time: 0:01:29 (0.3277 s / it)
* Acc@1 18.063 Acc@5 43.764 loss 3.854
Accuracy of the network on the 4883 val videos: 18.1%
[2025-01-13 00:01:11,340] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:01:11,342] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:01:11,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:01:14,079] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:01:14,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 18.06%
Epoch: [6]  [   0/1820]  eta: 3:39:29  lr: 0.000047  min_lr: 0.000000  loss: 4.6715 (4.6715)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.2361  data: 6.7999  max mem: 15572
Epoch: [6]  [  10/1820]  eta: 0:41:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6846 (4.7139)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3785  data: 0.9430  max mem: 15572
Epoch: [6]  [  20/1820]  eta: 0:29:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6494 (4.6640)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.2111  max mem: 15572
Epoch: [6]  [  30/1820]  eta: 0:23:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5820 (4.6616)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4744  data: 0.0328  max mem: 15572
Epoch: [6]  [  40/1820]  eta: 0:21:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6451 (4.6693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0417  max mem: 15572
Epoch: [6]  [  50/1820]  eta: 0:20:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6486 (4.6683)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0755  max mem: 15572
Epoch: [6]  [  60/1820]  eta: 0:19:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6168 (4.6631)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1196  max mem: 15572
[2025-01-13 00:01:58,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:01:58,272] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [  70/1820]  eta: 0:19:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6521 (4.6622)  loss_scale: 65536.0000 (72920.3380)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1631  max mem: 15572
[2025-01-13 00:02:02,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15273
[2025-01-13 00:02:02,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:02:02,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [  80/1820]  eta: 0:19:16  lr: 0.000047  min_lr: 0.000000  loss: 4.6755 (4.6571)  loss_scale: 65536.0000 (72008.6914)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1880  max mem: 15572
Epoch: [6]  [  90/1820]  eta: 0:18:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6442 (4.6584)  loss_scale: 65536.0000 (71297.4066)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1651  max mem: 15572
Epoch: [6]  [ 100/1820]  eta: 0:18:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7104 (4.6629)  loss_scale: 65536.0000 (70726.9703)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1274  max mem: 15572
Epoch: [6]  [ 110/1820]  eta: 0:18:07  lr: 0.000047  min_lr: 0.000000  loss: 4.7104 (4.6622)  loss_scale: 65536.0000 (70259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1221  max mem: 15572
Epoch: [6]  [ 120/1820]  eta: 0:17:59  lr: 0.000047  min_lr: 0.000000  loss: 4.6348 (4.6592)  loss_scale: 65536.0000 (69868.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1403  max mem: 15572
Epoch: [6]  [ 130/1820]  eta: 0:17:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5907 (4.6623)  loss_scale: 65536.0000 (69538.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1524  max mem: 15572
Epoch: [6]  [ 140/1820]  eta: 0:17:41  lr: 0.000047  min_lr: 0.000000  loss: 4.7429 (4.6670)  loss_scale: 65536.0000 (69254.3546)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1602  max mem: 15572
Epoch: [6]  [ 150/1820]  eta: 0:17:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6052 (4.6582)  loss_scale: 65536.0000 (69008.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1803  max mem: 15572
Epoch: [6]  [ 160/1820]  eta: 0:17:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5728 (4.6585)  loss_scale: 65536.0000 (68792.4472)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1924  max mem: 15572
Epoch: [6]  [ 170/1820]  eta: 0:17:20  lr: 0.000047  min_lr: 0.000000  loss: 4.6081 (4.6575)  loss_scale: 65536.0000 (68602.0117)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1762  max mem: 15572
Epoch: [6]  [ 180/1820]  eta: 0:17:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5335 (4.6520)  loss_scale: 65536.0000 (68432.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1581  max mem: 15572
Epoch: [6]  [ 190/1820]  eta: 0:17:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5159 (4.6451)  loss_scale: 65536.0000 (68280.9634)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1762  max mem: 15572
[2025-01-13 00:03:20,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:03:20,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 200/1820]  eta: 0:16:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6035 (4.6436)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1739  max mem: 15572
[2025-01-13 00:03:23,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15406
[2025-01-13 00:03:23,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:03:23,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 210/1820]  eta: 0:16:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6762 (4.6485)  loss_scale: 65536.0000 (69263.1659)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1354  max mem: 15572
Epoch: [6]  [ 220/1820]  eta: 0:16:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6899 (4.6477)  loss_scale: 65536.0000 (69094.5158)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1439  max mem: 15572
Epoch: [6]  [ 230/1820]  eta: 0:16:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5771 (4.6419)  loss_scale: 65536.0000 (68940.4675)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1845  max mem: 15572
Epoch: [6]  [ 240/1820]  eta: 0:16:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6797 (4.6423)  loss_scale: 65536.0000 (68799.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1740  max mem: 15572
Epoch: [6]  [ 250/1820]  eta: 0:16:15  lr: 0.000047  min_lr: 0.000000  loss: 4.6039 (4.6409)  loss_scale: 65536.0000 (68669.1952)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1403  max mem: 15572
Epoch: [6]  [ 260/1820]  eta: 0:16:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6340 (4.6416)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1700  max mem: 15572
Epoch: [6]  [ 270/1820]  eta: 0:16:02  lr: 0.000047  min_lr: 0.000000  loss: 4.6340 (4.6385)  loss_scale: 65536.0000 (68437.9631)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1514  max mem: 15572
Epoch: [6]  [ 280/1820]  eta: 0:15:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5107 (4.6386)  loss_scale: 65536.0000 (68334.6904)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0741  max mem: 15572
Epoch: [6]  [ 290/1820]  eta: 0:15:40  lr: 0.000047  min_lr: 0.000000  loss: 4.7142 (4.6423)  loss_scale: 65536.0000 (68238.5155)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0901  max mem: 15572
Epoch: [6]  [ 300/1820]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.7342 (4.6463)  loss_scale: 65536.0000 (68148.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1160  max mem: 15572
Epoch: [6]  [ 310/1820]  eta: 0:15:25  lr: 0.000047  min_lr: 0.000000  loss: 4.7459 (4.6476)  loss_scale: 65536.0000 (68064.7203)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1181  max mem: 15572
Epoch: [6]  [ 320/1820]  eta: 0:15:14  lr: 0.000047  min_lr: 0.000000  loss: 4.7231 (4.6495)  loss_scale: 65536.0000 (67985.9439)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0773  max mem: 15572
Epoch: [6]  [ 330/1820]  eta: 0:15:05  lr: 0.000047  min_lr: 0.000000  loss: 4.7215 (4.6494)  loss_scale: 65536.0000 (67911.9275)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0638  max mem: 15572
[2025-01-13 00:04:37,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:04:37,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:04:41,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15542
[2025-01-13 00:04:41,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:04:41,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 340/1820]  eta: 0:14:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6452 (4.6480)  loss_scale: 65536.0000 (69187.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0965  max mem: 15572
Epoch: [6]  [ 350/1820]  eta: 0:14:51  lr: 0.000047  min_lr: 0.000000  loss: 4.6268 (4.6489)  loss_scale: 65536.0000 (69083.5328)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1221  max mem: 15572
Epoch: [6]  [ 360/1820]  eta: 0:14:42  lr: 0.000047  min_lr: 0.000000  loss: 4.6745 (4.6506)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1068  max mem: 15572
Epoch: [6]  [ 370/1820]  eta: 0:14:38  lr: 0.000047  min_lr: 0.000000  loss: 4.7523 (4.6528)  loss_scale: 65536.0000 (68892.2911)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1383  max mem: 15572
Epoch: [6]  [ 380/1820]  eta: 0:14:28  lr: 0.000047  min_lr: 0.000000  loss: 4.7106 (4.6533)  loss_scale: 65536.0000 (68804.1995)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1294  max mem: 15572
Epoch: [6]  [ 390/1820]  eta: 0:14:22  lr: 0.000047  min_lr: 0.000000  loss: 4.6518 (4.6528)  loss_scale: 65536.0000 (68720.6138)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1040  max mem: 15572
Epoch: [6]  [ 400/1820]  eta: 0:14:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5839 (4.6505)  loss_scale: 65536.0000 (68641.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1043  max mem: 15572
Epoch: [6]  [ 410/1820]  eta: 0:14:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5871 (4.6489)  loss_scale: 65536.0000 (68565.6448)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1347  max mem: 15572
Epoch: [6]  [ 420/1820]  eta: 0:14:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5498 (4.6459)  loss_scale: 65536.0000 (68493.6817)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1513  max mem: 15572
Epoch: [6]  [ 430/1820]  eta: 0:13:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5416 (4.6455)  loss_scale: 65536.0000 (68425.0580)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0975  max mem: 15572
Epoch: [6]  [ 440/1820]  eta: 0:13:47  lr: 0.000047  min_lr: 0.000000  loss: 4.5867 (4.6449)  loss_scale: 65536.0000 (68359.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1168  max mem: 15572
Epoch: [6]  [ 450/1820]  eta: 0:13:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5867 (4.6433)  loss_scale: 65536.0000 (68296.9401)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1648  max mem: 15572
Epoch: [6]  [ 460/1820]  eta: 0:13:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6686 (4.6434)  loss_scale: 65536.0000 (68237.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1204  max mem: 15572
[2025-01-13 00:05:54,827] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:05:54,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 470/1820]  eta: 0:13:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6347 (4.6414)  loss_scale: 65536.0000 (68457.9873)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0488  max mem: 15572
[2025-01-13 00:05:58,359] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15677
[2025-01-13 00:05:58,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:05:58,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 480/1820]  eta: 0:13:17  lr: 0.000047  min_lr: 0.000000  loss: 4.6121 (4.6409)  loss_scale: 65536.0000 (68942.2370)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0641  max mem: 15572
Epoch: [6]  [ 490/1820]  eta: 0:13:14  lr: 0.000047  min_lr: 0.000000  loss: 4.5444 (4.6389)  loss_scale: 65536.0000 (68872.8635)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1676  max mem: 15572
Epoch: [6]  [ 500/1820]  eta: 0:13:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5868 (4.6396)  loss_scale: 65536.0000 (68806.2595)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1740  max mem: 15572
Epoch: [6]  [ 510/1820]  eta: 0:13:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6735 (4.6407)  loss_scale: 65536.0000 (68742.2622)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1548  max mem: 15572
Epoch: [6]  [ 520/1820]  eta: 0:12:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6526 (4.6388)  loss_scale: 65536.0000 (68680.7217)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.1906  max mem: 15572
Epoch: [6]  [ 530/1820]  eta: 0:12:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5142 (4.6372)  loss_scale: 65536.0000 (68621.4991)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1350  max mem: 15572
Epoch: [6]  [ 540/1820]  eta: 0:12:44  lr: 0.000047  min_lr: 0.000000  loss: 4.5365 (4.6361)  loss_scale: 65536.0000 (68564.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1149  max mem: 15572
Epoch: [6]  [ 550/1820]  eta: 0:12:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6122 (4.6363)  loss_scale: 65536.0000 (68509.5027)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1596  max mem: 15572
Epoch: [6]  [ 560/1820]  eta: 0:12:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6203 (4.6364)  loss_scale: 65536.0000 (68456.4991)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1636  max mem: 15572
Epoch: [6]  [ 570/1820]  eta: 0:12:26  lr: 0.000047  min_lr: 0.000000  loss: 4.6977 (4.6373)  loss_scale: 65536.0000 (68405.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1051  max mem: 15572
Epoch: [6]  [ 580/1820]  eta: 0:12:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5660 (4.6346)  loss_scale: 65536.0000 (68355.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0998  max mem: 15572
Epoch: [6]  [ 590/1820]  eta: 0:12:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5548 (4.6362)  loss_scale: 65536.0000 (68308.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1399  max mem: 15572
Epoch: [6]  [ 600/1820]  eta: 0:12:09  lr: 0.000047  min_lr: 0.000000  loss: 4.7075 (4.6352)  loss_scale: 65536.0000 (68262.1231)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1922  max mem: 15572
[2025-01-13 00:07:15,140] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:07:15,141] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 610/1820]  eta: 0:12:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5253 (4.6352)  loss_scale: 65536.0000 (68968.3273)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1765  max mem: 15572
[2025-01-13 00:07:22,508] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15819
[2025-01-13 00:07:22,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:07:22,508] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 620/1820]  eta: 0:11:53  lr: 0.000047  min_lr: 0.000000  loss: 4.6621 (4.6370)  loss_scale: 131072.0000 (69546.2544)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0873  max mem: 15572
Epoch: [6]  [ 630/1820]  eta: 0:11:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6857 (4.6371)  loss_scale: 65536.0000 (69482.7005)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1428  max mem: 15572
Epoch: [6]  [ 640/1820]  eta: 0:11:42  lr: 0.000047  min_lr: 0.000000  loss: 4.5843 (4.6361)  loss_scale: 65536.0000 (69421.1295)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1688  max mem: 15572
Epoch: [6]  [ 650/1820]  eta: 0:11:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5843 (4.6352)  loss_scale: 65536.0000 (69361.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1359  max mem: 15572
Epoch: [6]  [ 660/1820]  eta: 0:11:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6178 (4.6356)  loss_scale: 65536.0000 (69303.5764)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.2103  max mem: 15572
Epoch: [6]  [ 670/1820]  eta: 0:11:24  lr: 0.000047  min_lr: 0.000000  loss: 4.7545 (4.6382)  loss_scale: 65536.0000 (69247.4277)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1756  max mem: 15572
Epoch: [6]  [ 680/1820]  eta: 0:11:19  lr: 0.000047  min_lr: 0.000000  loss: 4.7790 (4.6402)  loss_scale: 65536.0000 (69192.9280)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1389  max mem: 15572
Epoch: [6]  [ 690/1820]  eta: 0:11:11  lr: 0.000047  min_lr: 0.000000  loss: 4.7277 (4.6404)  loss_scale: 65536.0000 (69140.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1150  max mem: 15572
Epoch: [6]  [ 700/1820]  eta: 0:11:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5714 (4.6380)  loss_scale: 65536.0000 (69088.5934)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0930  max mem: 15572
Epoch: [6]  [ 710/1820]  eta: 0:10:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5714 (4.6376)  loss_scale: 65536.0000 (69038.6273)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1162  max mem: 15572
Epoch: [6]  [ 720/1820]  eta: 0:10:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5965 (4.6377)  loss_scale: 65536.0000 (68990.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1234  max mem: 15572
Epoch: [6]  [ 730/1820]  eta: 0:10:49  lr: 0.000047  min_lr: 0.000000  loss: 4.5504 (4.6362)  loss_scale: 65536.0000 (68942.7962)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.2088  max mem: 15572
Epoch: [6]  [ 740/1820]  eta: 0:10:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5504 (4.6350)  loss_scale: 65536.0000 (68896.8205)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1989  max mem: 15572
[2025-01-13 00:08:39,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:08:39,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:08:40,648] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15951
[2025-01-13 00:08:40,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:08:40,649] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 750/1820]  eta: 0:10:36  lr: 0.000047  min_lr: 0.000000  loss: 4.6318 (4.6354)  loss_scale: 65536.0000 (69113.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1114  max mem: 15572
Epoch: [6]  [ 760/1820]  eta: 0:10:31  lr: 0.000047  min_lr: 0.000000  loss: 4.6407 (4.6359)  loss_scale: 65536.0000 (69066.8489)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1369  max mem: 15572
Epoch: [6]  [ 770/1820]  eta: 0:10:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6349 (4.6351)  loss_scale: 65536.0000 (69021.0532)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1865  max mem: 15572
Epoch: [6]  [ 780/1820]  eta: 0:10:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5654 (4.6340)  loss_scale: 65536.0000 (68976.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1432  max mem: 15572
Epoch: [6]  [ 790/1820]  eta: 0:10:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4396 (4.6318)  loss_scale: 65536.0000 (68932.9355)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1134  max mem: 15572
[2025-01-13 00:09:09,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=98, lr=[4.5228034595024233e-07, 4.5228034595024233e-07, 6.461147799289176e-07, 6.461147799289176e-07, 9.230211141841682e-07, 9.230211141841682e-07, 1.3186015916916688e-06, 1.3186015916916688e-06, 1.883716559559527e-06, 1.883716559559527e-06, 2.6910236565136103e-06, 2.6910236565136103e-06, 3.844319509305157e-06, 3.844319509305157e-06, 5.491885013293083e-06, 5.491885013293083e-06, 7.845550018990118e-06, 7.845550018990118e-06, 1.1207928598557314e-05, 1.1207928598557314e-05, 1.601132656936759e-05, 1.601132656936759e-05, 2.287332367052513e-05, 2.287332367052513e-05, 3.267617667217876e-05, 3.267617667217876e-05, 4.6680252388826805e-05, 4.6680252388826805e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:09:09,576] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=27.81612104553161, CurrSamplesPerSec=32.26242924996955, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 800/1820]  eta: 0:10:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5512 (4.6323)  loss_scale: 65536.0000 (68890.5268)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1518  max mem: 15572
Epoch: [6]  [ 810/1820]  eta: 0:10:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5797 (4.6313)  loss_scale: 65536.0000 (68849.1640)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1648  max mem: 15572
Epoch: [6]  [ 820/1820]  eta: 0:09:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6121 (4.6314)  loss_scale: 65536.0000 (68808.8088)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1261  max mem: 15572
Epoch: [6]  [ 830/1820]  eta: 0:09:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6830 (4.6322)  loss_scale: 65536.0000 (68769.4248)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1425  max mem: 15572
Epoch: [6]  [ 840/1820]  eta: 0:09:43  lr: 0.000047  min_lr: 0.000000  loss: 4.6821 (4.6325)  loss_scale: 65536.0000 (68730.9774)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1748  max mem: 15572
Epoch: [6]  [ 850/1820]  eta: 0:09:37  lr: 0.000047  min_lr: 0.000000  loss: 4.6115 (4.6318)  loss_scale: 65536.0000 (68693.4336)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1454  max mem: 15572
Epoch: [6]  [ 860/1820]  eta: 0:09:31  lr: 0.000047  min_lr: 0.000000  loss: 4.5610 (4.6312)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1237  max mem: 15572
Epoch: [6]  [ 870/1820]  eta: 0:09:25  lr: 0.000047  min_lr: 0.000000  loss: 4.6704 (4.6317)  loss_scale: 65536.0000 (68620.9323)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1083  max mem: 15572
[2025-01-13 00:09:57,242] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:09:57,243] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 880/1820]  eta: 0:09:18  lr: 0.000047  min_lr: 0.000000  loss: 4.6736 (4.6327)  loss_scale: 65536.0000 (68809.0806)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1151  max mem: 15572
[2025-01-13 00:09:59,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16086
[2025-01-13 00:09:59,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:09:59,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 890/1820]  eta: 0:09:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5941 (4.6307)  loss_scale: 65536.0000 (68993.0056)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1435  max mem: 15572
Epoch: [6]  [ 900/1820]  eta: 0:09:06  lr: 0.000047  min_lr: 0.000000  loss: 4.5290 (4.6295)  loss_scale: 65536.0000 (68954.6371)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1253  max mem: 15572
Epoch: [6]  [ 910/1820]  eta: 0:09:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5290 (4.6296)  loss_scale: 65536.0000 (68917.1109)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1494  max mem: 15572
Epoch: [6]  [ 920/1820]  eta: 0:08:55  lr: 0.000047  min_lr: 0.000000  loss: 4.6425 (4.6287)  loss_scale: 65536.0000 (68880.3996)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1863  max mem: 15572
Epoch: [6]  [ 930/1820]  eta: 0:08:49  lr: 0.000047  min_lr: 0.000000  loss: 4.6115 (4.6287)  loss_scale: 65536.0000 (68844.4769)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1172  max mem: 15572
Epoch: [6]  [ 940/1820]  eta: 0:08:42  lr: 0.000047  min_lr: 0.000000  loss: 4.7865 (4.6310)  loss_scale: 65536.0000 (68809.3177)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0777  max mem: 15572
Epoch: [6]  [ 950/1820]  eta: 0:08:36  lr: 0.000047  min_lr: 0.000000  loss: 4.7819 (4.6315)  loss_scale: 65536.0000 (68774.8980)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1079  max mem: 15572
Epoch: [6]  [ 960/1820]  eta: 0:08:30  lr: 0.000047  min_lr: 0.000000  loss: 4.7055 (4.6322)  loss_scale: 65536.0000 (68741.1946)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1190  max mem: 15572
Epoch: [6]  [ 970/1820]  eta: 0:08:24  lr: 0.000047  min_lr: 0.000000  loss: 4.7109 (4.6316)  loss_scale: 65536.0000 (68708.1854)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1734  max mem: 15572
Epoch: [6]  [ 980/1820]  eta: 0:08:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5629 (4.6303)  loss_scale: 65536.0000 (68675.8491)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1464  max mem: 15572
Epoch: [6]  [ 990/1820]  eta: 0:08:12  lr: 0.000047  min_lr: 0.000000  loss: 4.5629 (4.6303)  loss_scale: 65536.0000 (68644.1655)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1084  max mem: 15572
Epoch: [6]  [1000/1820]  eta: 0:08:06  lr: 0.000047  min_lr: 0.000000  loss: 4.6917 (4.6306)  loss_scale: 65536.0000 (68613.1149)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1304  max mem: 15572
Epoch: [6]  [1010/1820]  eta: 0:08:00  lr: 0.000047  min_lr: 0.000000  loss: 4.6135 (4.6303)  loss_scale: 65536.0000 (68582.6785)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0849  max mem: 15572
[2025-01-13 00:11:15,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:11:15,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:11:16,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16217
[2025-01-13 00:11:16,428] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:11:16,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1020/1820]  eta: 0:07:54  lr: 0.000047  min_lr: 0.000000  loss: 4.6135 (4.6302)  loss_scale: 65536.0000 (68681.2145)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1140  max mem: 15572
Epoch: [6]  [1030/1820]  eta: 0:07:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6075 (4.6302)  loss_scale: 65536.0000 (68650.7081)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1394  max mem: 15572
Epoch: [6]  [1040/1820]  eta: 0:07:42  lr: 0.000047  min_lr: 0.000000  loss: 4.5187 (4.6284)  loss_scale: 65536.0000 (68620.7877)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1366  max mem: 15572
Epoch: [6]  [1050/1820]  eta: 0:07:36  lr: 0.000047  min_lr: 0.000000  loss: 4.5696 (4.6292)  loss_scale: 65536.0000 (68591.4367)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1861  max mem: 15572
Epoch: [6]  [1060/1820]  eta: 0:07:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6046 (4.6287)  loss_scale: 65536.0000 (68562.6390)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1681  max mem: 15572
Epoch: [6]  [1070/1820]  eta: 0:07:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5156 (4.6276)  loss_scale: 65536.0000 (68534.3791)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1354  max mem: 15572
Epoch: [6]  [1080/1820]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5156 (4.6269)  loss_scale: 65536.0000 (68506.6420)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1605  max mem: 15572
Epoch: [6]  [1090/1820]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 4.6005 (4.6268)  loss_scale: 65536.0000 (68479.4134)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1382  max mem: 15572
Epoch: [6]  [1100/1820]  eta: 0:07:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5738 (4.6263)  loss_scale: 65536.0000 (68452.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1491  max mem: 15572
Epoch: [6]  [1110/1820]  eta: 0:07:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5953 (4.6271)  loss_scale: 65536.0000 (68426.4266)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1934  max mem: 15572
Epoch: [6]  [1120/1820]  eta: 0:06:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6766 (4.6269)  loss_scale: 65536.0000 (68400.6423)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1679  max mem: 15572
Epoch: [6]  [1130/1820]  eta: 0:06:50  lr: 0.000047  min_lr: 0.000000  loss: 4.6017 (4.6272)  loss_scale: 65536.0000 (68375.3139)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1580  max mem: 15572
Epoch: [6]  [1140/1820]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.7233 (4.6280)  loss_scale: 65536.0000 (68350.4294)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1667  max mem: 15572
[2025-01-13 00:12:37,053] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:12:37,053] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:12:37,914] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16348
[2025-01-13 00:12:37,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:12:37,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1150/1820]  eta: 0:06:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6662 (4.6281)  loss_scale: 65536.0000 (68439.8540)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1691  max mem: 15572
Epoch: [6]  [1160/1820]  eta: 0:06:33  lr: 0.000047  min_lr: 0.000000  loss: 4.6554 (4.6278)  loss_scale: 65536.0000 (68414.8424)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1475  max mem: 15572
Epoch: [6]  [1170/1820]  eta: 0:06:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5398 (4.6272)  loss_scale: 65536.0000 (68390.2579)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0852  max mem: 15572
Epoch: [6]  [1180/1820]  eta: 0:06:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5398 (4.6275)  loss_scale: 65536.0000 (68366.0898)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0564  max mem: 15572
Epoch: [6]  [1190/1820]  eta: 0:06:14  lr: 0.000047  min_lr: 0.000000  loss: 4.6246 (4.6274)  loss_scale: 65536.0000 (68342.3275)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0968  max mem: 15572
Epoch: [6]  [1200/1820]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.6246 (4.6277)  loss_scale: 65536.0000 (68318.9609)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1821  max mem: 15572
Epoch: [6]  [1210/1820]  eta: 0:06:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5870 (4.6284)  loss_scale: 65536.0000 (68295.9802)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1499  max mem: 15572
Epoch: [6]  [1220/1820]  eta: 0:05:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5554 (4.6267)  loss_scale: 65536.0000 (68273.3759)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1152  max mem: 15572
Epoch: [6]  [1230/1820]  eta: 0:05:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5588 (4.6268)  loss_scale: 65536.0000 (68251.1389)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1782  max mem: 15572
Epoch: [6]  [1240/1820]  eta: 0:05:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6226 (4.6271)  loss_scale: 65536.0000 (68229.2603)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.1814  max mem: 15572
Epoch: [6]  [1250/1820]  eta: 0:05:38  lr: 0.000047  min_lr: 0.000000  loss: 4.6226 (4.6265)  loss_scale: 65536.0000 (68207.7314)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0800  max mem: 15572
Epoch: [6]  [1260/1820]  eta: 0:05:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5925 (4.6256)  loss_scale: 65536.0000 (68186.5440)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0624  max mem: 15572
Epoch: [6]  [1270/1820]  eta: 0:05:27  lr: 0.000047  min_lr: 0.000000  loss: 4.6278 (4.6268)  loss_scale: 65536.0000 (68165.6900)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1309  max mem: 15572
[2025-01-13 00:13:54,474] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:13:54,474] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:13:55,859] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16480
[2025-01-13 00:13:55,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:13:55,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1280/1820]  eta: 0:05:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5495 (4.6253)  loss_scale: 65536.0000 (68298.6417)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1694  max mem: 15572
Epoch: [6]  [1290/1820]  eta: 0:05:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4512 (4.6247)  loss_scale: 65536.0000 (68277.2424)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1525  max mem: 15572
Epoch: [6]  [1300/1820]  eta: 0:05:09  lr: 0.000047  min_lr: 0.000000  loss: 4.6185 (4.6252)  loss_scale: 65536.0000 (68256.1722)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1277  max mem: 15572
Epoch: [6]  [1310/1820]  eta: 0:05:03  lr: 0.000047  min_lr: 0.000000  loss: 4.6895 (4.6259)  loss_scale: 65536.0000 (68235.4233)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1782  max mem: 15572
Epoch: [6]  [1320/1820]  eta: 0:04:57  lr: 0.000047  min_lr: 0.000000  loss: 4.6375 (4.6253)  loss_scale: 65536.0000 (68214.9886)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.1835  max mem: 15572
Epoch: [6]  [1330/1820]  eta: 0:04:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5273 (4.6251)  loss_scale: 65536.0000 (68194.8610)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1269  max mem: 15572
Epoch: [6]  [1340/1820]  eta: 0:04:45  lr: 0.000047  min_lr: 0.000000  loss: 4.6306 (4.6255)  loss_scale: 65536.0000 (68175.0336)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1173  max mem: 15572
Epoch: [6]  [1350/1820]  eta: 0:04:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5438 (4.6246)  loss_scale: 65536.0000 (68155.4996)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1580  max mem: 15572
Epoch: [6]  [1360/1820]  eta: 0:04:33  lr: 0.000047  min_lr: 0.000000  loss: 4.5191 (4.6248)  loss_scale: 65536.0000 (68136.2528)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1616  max mem: 15572
Epoch: [6]  [1370/1820]  eta: 0:04:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6628 (4.6253)  loss_scale: 65536.0000 (68117.2867)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1648  max mem: 15572
Epoch: [6]  [1380/1820]  eta: 0:04:21  lr: 0.000047  min_lr: 0.000000  loss: 4.6501 (4.6250)  loss_scale: 65536.0000 (68098.5952)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1256  max mem: 15572
Epoch: [6]  [1390/1820]  eta: 0:04:16  lr: 0.000047  min_lr: 0.000000  loss: 4.6144 (4.6246)  loss_scale: 65536.0000 (68080.1725)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1370  max mem: 15572
Epoch: [6]  [1400/1820]  eta: 0:04:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5405 (4.6237)  loss_scale: 65536.0000 (68062.0128)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1713  max mem: 15572
[2025-01-13 00:15:13,190] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:15:13,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:15:13,670] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16610
[2025-01-13 00:15:13,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:15:13,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1410/1820]  eta: 0:04:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5405 (4.6236)  loss_scale: 65536.0000 (68090.5571)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1211  max mem: 15572
Epoch: [6]  [1420/1820]  eta: 0:03:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5528 (4.6232)  loss_scale: 65536.0000 (68072.5799)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1599  max mem: 15572
Epoch: [6]  [1430/1820]  eta: 0:03:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6640 (4.6242)  loss_scale: 65536.0000 (68054.8539)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1439  max mem: 15572
Epoch: [6]  [1440/1820]  eta: 0:03:46  lr: 0.000047  min_lr: 0.000000  loss: 4.6640 (4.6236)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0920  max mem: 15572
Epoch: [6]  [1450/1820]  eta: 0:03:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6273 (4.6233)  loss_scale: 65536.0000 (68020.1351)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.2103  max mem: 15572
Epoch: [6]  [1460/1820]  eta: 0:03:34  lr: 0.000047  min_lr: 0.000000  loss: 4.6526 (4.6235)  loss_scale: 65536.0000 (68003.1321)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.2249  max mem: 15572
Epoch: [6]  [1470/1820]  eta: 0:03:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6209 (4.6235)  loss_scale: 65536.0000 (67986.3603)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1766  max mem: 15572
Epoch: [6]  [1480/1820]  eta: 0:03:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5544 (4.6231)  loss_scale: 65536.0000 (67969.8150)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1196  max mem: 15572
Epoch: [6]  [1490/1820]  eta: 0:03:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5648 (4.6230)  loss_scale: 65536.0000 (67953.4916)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0426  max mem: 15572
Epoch: [6]  [1500/1820]  eta: 0:03:10  lr: 0.000047  min_lr: 0.000000  loss: 4.6240 (4.6233)  loss_scale: 65536.0000 (67937.3857)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0828  max mem: 15572
Epoch: [6]  [1510/1820]  eta: 0:03:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6240 (4.6236)  loss_scale: 65536.0000 (67921.4931)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1708  max mem: 15572
Epoch: [6]  [1520/1820]  eta: 0:02:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5598 (4.6233)  loss_scale: 65536.0000 (67905.8093)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1883  max mem: 15572
Epoch: [6]  [1530/1820]  eta: 0:02:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5598 (4.6232)  loss_scale: 65536.0000 (67890.3305)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1546  max mem: 15572
[2025-01-13 00:16:32,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:16:32,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1540/1820]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5757 (4.6236)  loss_scale: 65536.0000 (68045.1655)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.2097  max mem: 15572
[2025-01-13 00:16:36,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16746
[2025-01-13 00:16:36,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:16:36,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1550/1820]  eta: 0:02:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6400 (4.6240)  loss_scale: 65536.0000 (68155.7498)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1914  max mem: 15572
Epoch: [6]  [1560/1820]  eta: 0:02:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6298 (4.6238)  loss_scale: 65536.0000 (68138.9673)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1155  max mem: 15572
Epoch: [6]  [1570/1820]  eta: 0:02:28  lr: 0.000047  min_lr: 0.000000  loss: 4.6496 (4.6242)  loss_scale: 65536.0000 (68122.3985)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0521  max mem: 15572
Epoch: [6]  [1580/1820]  eta: 0:02:22  lr: 0.000047  min_lr: 0.000000  loss: 4.7044 (4.6248)  loss_scale: 65536.0000 (68106.0392)  weight_decay: 0.0500 (0.0500)  time: 0.4617  data: 0.0083  max mem: 15572
Epoch: [6]  [1590/1820]  eta: 0:02:16  lr: 0.000047  min_lr: 0.000000  loss: 4.6134 (4.6243)  loss_scale: 65536.0000 (68089.8856)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0935  max mem: 15572
Epoch: [6]  [1600/1820]  eta: 0:02:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5678 (4.6239)  loss_scale: 65536.0000 (68073.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1614  max mem: 15572
Epoch: [6]  [1610/1820]  eta: 0:02:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5912 (4.6236)  loss_scale: 65536.0000 (68058.1800)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1029  max mem: 15572
Epoch: [6]  [1620/1820]  eta: 0:01:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5912 (4.6237)  loss_scale: 65536.0000 (68042.6206)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0970  max mem: 15572
Epoch: [6]  [1630/1820]  eta: 0:01:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5963 (4.6240)  loss_scale: 65536.0000 (68027.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1520  max mem: 15572
Epoch: [6]  [1640/1820]  eta: 0:01:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6178 (4.6239)  loss_scale: 65536.0000 (68012.0707)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1675  max mem: 15572
Epoch: [6]  [1650/1820]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000000  loss: 4.6178 (4.6241)  loss_scale: 65536.0000 (67997.0733)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1087  max mem: 15572
Epoch: [6]  [1660/1820]  eta: 0:01:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5662 (4.6236)  loss_scale: 65536.0000 (67982.2565)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1426  max mem: 15572
Epoch: [6]  [1670/1820]  eta: 0:01:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5537 (4.6232)  loss_scale: 65536.0000 (67967.6170)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1496  max mem: 15572
[2025-01-13 00:17:49,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:17:49,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1680/1820]  eta: 0:01:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5632 (4.6231)  loss_scale: 65536.0000 (68265.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0825  max mem: 15572
[2025-01-13 00:17:59,557] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16892
[2025-01-13 00:17:59,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:17:59,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1690/1820]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5459 (4.6224)  loss_scale: 131072.0000 (68597.7055)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1313  max mem: 15572
Epoch: [6]  [1700/1820]  eta: 0:01:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4657 (4.6213)  loss_scale: 65536.0000 (68579.7061)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1237  max mem: 15572
Epoch: [6]  [1710/1820]  eta: 0:01:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4258 (4.6202)  loss_scale: 65536.0000 (68561.9170)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1360  max mem: 15572
Epoch: [6]  [1720/1820]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000000  loss: 4.5792 (4.6203)  loss_scale: 65536.0000 (68544.3347)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1486  max mem: 15572
Epoch: [6]  [1730/1820]  eta: 0:00:53  lr: 0.000047  min_lr: 0.000000  loss: 4.6825 (4.6204)  loss_scale: 65536.0000 (68526.9555)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1103  max mem: 15572
Epoch: [6]  [1740/1820]  eta: 0:00:47  lr: 0.000047  min_lr: 0.000000  loss: 4.6883 (4.6208)  loss_scale: 65536.0000 (68509.7760)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0861  max mem: 15572
Epoch: [6]  [1750/1820]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000000  loss: 4.7312 (4.6214)  loss_scale: 65536.0000 (68492.7927)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1015  max mem: 15572
Epoch: [6]  [1760/1820]  eta: 0:00:35  lr: 0.000047  min_lr: 0.000000  loss: 4.6902 (4.6215)  loss_scale: 65536.0000 (68476.0023)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1792  max mem: 15572
Epoch: [6]  [1770/1820]  eta: 0:00:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5382 (4.6208)  loss_scale: 65536.0000 (68459.4015)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1734  max mem: 15572
Epoch: [6]  [1780/1820]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.6809 (4.6215)  loss_scale: 65536.0000 (68442.9871)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0630  max mem: 15572
Epoch: [6]  [1790/1820]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.7038 (4.6210)  loss_scale: 65536.0000 (68426.7560)  weight_decay: 0.0500 (0.0500)  time: 0.4660  data: 0.0007  max mem: 15572
[2025-01-13 00:18:59,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=105, lr=[4.5056736278743275e-07, 4.5056736278743275e-07, 6.43667661124904e-07, 6.43667661124904e-07, 9.195252301784344e-07, 9.195252301784344e-07, 1.3136074716834778e-06, 1.3136074716834778e-06, 1.8765821024049682e-06, 1.8765821024049682e-06, 2.68083157486424e-06, 2.68083157486424e-06, 3.829759392663201e-06, 3.829759392663201e-06, 5.471084846661716e-06, 5.471084846661716e-06, 7.815835495231023e-06, 7.815835495231023e-06, 1.1165479278901462e-05, 1.1165479278901462e-05, 1.5950684684144946e-05, 1.5950684684144946e-05, 2.2786692405921354e-05, 2.2786692405921354e-05, 3.255241772274479e-05, 3.255241772274479e-05, 4.650345388963542e-05, 4.650345388963542e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:18:59,588] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=27.813129271282378, CurrSamplesPerSec=28.796105873286407, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [1800/1820]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6432 (4.6214)  loss_scale: 65536.0000 (68410.7052)  weight_decay: 0.0500 (0.0500)  time: 0.4789  data: 0.0008  max mem: 15572
Epoch: [6]  [1810/1820]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.6646 (4.6216)  loss_scale: 65536.0000 (68394.8316)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0916  max mem: 15572
[2025-01-13 00:19:13,190] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:19:13,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1819/1820]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.6338 (4.6215)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1896  max mem: 15572
Epoch: [6] Total time: 0:17:59 (0.5932 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.6338 (4.6215)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2920
Indices to remove: tensor([    6,    35,    62,  ..., 33668, 33677, 33696], device='cuda:0')
length of data loader train is: 1576
num_training_steps_per_epoch is: 1576
Change step level LR scheduler!
Set warmup steps = 7880
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:36:11  loss: 1.6882 (1.6882)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 7.9837  data: 7.7924  max mem: 15572
Val:  [ 10/272]  eta: 0:04:22  loss: 3.8647 (3.8228)  acc1: 5.5556 (18.6869)  acc5: 38.8889 (40.4040)  time: 1.0006  data: 0.8012  max mem: 15572
Val:  [ 20/272]  eta: 0:02:41  loss: 3.8551 (3.8170)  acc1: 5.5556 (16.6667)  acc5: 50.0000 (43.6508)  time: 0.2758  data: 0.0818  max mem: 15572
Val:  [ 30/272]  eta: 0:02:15  loss: 3.7456 (3.8636)  acc1: 11.1111 (14.8746)  acc5: 50.0000 (42.4731)  time: 0.3172  data: 0.1270  max mem: 15572
Val:  [ 40/272]  eta: 0:01:59  loss: 3.6870 (3.8140)  acc1: 11.1111 (14.9051)  acc5: 44.4444 (44.1734)  time: 0.3828  data: 0.1905  max mem: 15572
Val:  [ 50/272]  eta: 0:01:47  loss: 3.6535 (3.7474)  acc1: 16.6667 (17.2113)  acc5: 44.4444 (46.2963)  time: 0.3672  data: 0.1643  max mem: 15572
Val:  [ 60/272]  eta: 0:01:42  loss: 2.9425 (3.6383)  acc1: 38.8889 (21.5847)  acc5: 72.2222 (49.7268)  time: 0.4142  data: 0.2137  max mem: 15572
Val:  [ 70/272]  eta: 0:01:31  loss: 3.0391 (3.5575)  acc1: 44.4444 (24.6479)  acc5: 72.2222 (53.4429)  time: 0.3736  data: 0.1800  max mem: 15572
Val:  [ 80/272]  eta: 0:01:25  loss: 3.2356 (3.5906)  acc1: 22.2222 (22.9767)  acc5: 66.6667 (52.2634)  time: 0.3301  data: 0.1313  max mem: 15572
Val:  [ 90/272]  eta: 0:01:19  loss: 4.0526 (3.6557)  acc1: 5.5556 (21.1844)  acc5: 27.7778 (49.0232)  time: 0.3834  data: 0.1873  max mem: 15572
Val:  [100/272]  eta: 0:01:14  loss: 4.1045 (3.7107)  acc1: 5.5556 (20.0770)  acc5: 27.7778 (47.4147)  time: 0.3732  data: 0.1745  max mem: 15572
Val:  [110/272]  eta: 0:01:08  loss: 4.0920 (3.7608)  acc1: 5.5556 (18.5185)  acc5: 33.3333 (45.3954)  time: 0.3537  data: 0.1527  max mem: 15572
Val:  [120/272]  eta: 0:01:03  loss: 4.0920 (3.7884)  acc1: 5.5556 (17.8145)  acc5: 38.8889 (44.9036)  time: 0.3537  data: 0.1574  max mem: 15572
Val:  [130/272]  eta: 0:00:59  loss: 3.8570 (3.7554)  acc1: 11.1111 (19.2536)  acc5: 44.4444 (45.8439)  time: 0.3872  data: 0.1892  max mem: 15572
Val:  [140/272]  eta: 0:00:54  loss: 3.4181 (3.7501)  acc1: 16.6667 (18.9125)  acc5: 55.5556 (46.0205)  time: 0.3756  data: 0.1729  max mem: 15572
Val:  [150/272]  eta: 0:00:50  loss: 3.6471 (3.7385)  acc1: 11.1111 (19.0949)  acc5: 44.4444 (45.9161)  time: 0.3690  data: 0.1583  max mem: 15572
Val:  [160/272]  eta: 0:00:45  loss: 3.7183 (3.7330)  acc1: 16.6667 (19.2892)  acc5: 44.4444 (46.3423)  time: 0.3335  data: 0.1369  max mem: 15572
Val:  [170/272]  eta: 0:00:39  loss: 3.7562 (3.7508)  acc1: 11.1111 (18.5510)  acc5: 44.4444 (45.8090)  time: 0.2489  data: 0.0728  max mem: 15572
Val:  [180/272]  eta: 0:00:34  loss: 3.6734 (3.7328)  acc1: 11.1111 (18.2934)  acc5: 44.4444 (46.6544)  time: 0.1966  data: 0.0241  max mem: 15572
Val:  [190/272]  eta: 0:00:30  loss: 3.6791 (3.7450)  acc1: 11.1111 (17.9174)  acc5: 44.4444 (45.8988)  time: 0.1667  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.6791 (3.7396)  acc1: 11.1111 (18.6844)  acc5: 44.4444 (46.7109)  time: 0.1777  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.5119 (3.7428)  acc1: 33.3333 (19.4839)  acc5: 61.1111 (47.1827)  time: 0.2016  data: 0.0009  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.7137 (3.7394)  acc1: 22.2222 (19.8341)  acc5: 55.5556 (47.1845)  time: 0.2390  data: 0.0360  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.3925 (3.7312)  acc1: 38.8889 (20.7311)  acc5: 61.1111 (48.1722)  time: 0.3063  data: 0.1059  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.3925 (3.7203)  acc1: 33.3333 (21.0235)  acc5: 72.2222 (48.8243)  time: 0.3679  data: 0.1633  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.6414 (3.7376)  acc1: 16.6667 (20.6729)  acc5: 55.5556 (48.2514)  time: 0.3406  data: 0.1488  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.2482 (3.6935)  acc1: 16.6667 (22.5841)  acc5: 72.2222 (49.8297)  time: 0.2905  data: 0.1069  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2482 (3.6957)  acc1: 16.6667 (22.1402)  acc5: 72.2222 (49.7745)  time: 0.2422  data: 0.0685  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2482 (3.6978)  acc1: 16.6667 (22.1380)  acc5: 61.1111 (49.7440)  time: 0.2249  data: 0.0575  max mem: 15572
Val: Total time: 0:01:32 (0.3403 s / it)
* Acc@1 22.138 Acc@5 49.744 loss 3.698
Accuracy of the network on the 4883 val videos: 22.1%
[2025-01-13 00:20:46,744] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:20:46,747] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:20:46,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:20:49,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:20:49,632] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 22.14%
Epoch: [7]  [   0/1576]  eta: 3:24:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3361 (4.3361)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7866  data: 7.1901  max mem: 15572
[2025-01-13 00:21:00,985] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17028
[2025-01-13 00:21:00,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:21:00,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [  10/1576]  eta: 0:31:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4980 (4.5396)  loss_scale: 131072.0000 (101282.9091)  weight_decay: 0.0500 (0.0500)  time: 1.2045  data: 0.7559  max mem: 15572
Epoch: [7]  [  20/1576]  eta: 0:22:49  lr: 0.000046  min_lr: 0.000000  loss: 4.5494 (4.5715)  loss_scale: 65536.0000 (84260.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0793  max mem: 15572
Epoch: [7]  [  30/1576]  eta: 0:19:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5540 (4.5631)  loss_scale: 65536.0000 (78220.3871)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0623  max mem: 15572
Epoch: [7]  [  40/1576]  eta: 0:18:22  lr: 0.000046  min_lr: 0.000000  loss: 4.5691 (4.5803)  loss_scale: 65536.0000 (75126.6341)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1016  max mem: 15572
Epoch: [7]  [  50/1576]  eta: 0:17:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5839 (4.5720)  loss_scale: 65536.0000 (73246.1176)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1500  max mem: 15572
Epoch: [7]  [  60/1576]  eta: 0:17:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5839 (4.5845)  loss_scale: 65536.0000 (71982.1639)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1798  max mem: 15572
Epoch: [7]  [  70/1576]  eta: 0:16:44  lr: 0.000046  min_lr: 0.000000  loss: 4.5863 (4.5848)  loss_scale: 65536.0000 (71074.2535)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1058  max mem: 15572
Epoch: [7]  [  80/1576]  eta: 0:16:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5906 (4.5898)  loss_scale: 65536.0000 (70390.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0776  max mem: 15572
Epoch: [7]  [  90/1576]  eta: 0:15:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5906 (4.5873)  loss_scale: 65536.0000 (69857.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0964  max mem: 15572
Epoch: [7]  [ 100/1576]  eta: 0:15:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6860 (4.6084)  loss_scale: 65536.0000 (69429.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1110  max mem: 15572
Epoch: [7]  [ 110/1576]  eta: 0:15:28  lr: 0.000046  min_lr: 0.000000  loss: 4.7060 (4.6085)  loss_scale: 65536.0000 (69078.4865)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1346  max mem: 15572
Epoch: [7]  [ 120/1576]  eta: 0:15:17  lr: 0.000046  min_lr: 0.000000  loss: 4.6635 (4.6032)  loss_scale: 65536.0000 (68785.7190)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1436  max mem: 15572
Epoch: [7]  [ 130/1576]  eta: 0:15:13  lr: 0.000046  min_lr: 0.000000  loss: 4.6137 (4.6042)  loss_scale: 65536.0000 (68537.6489)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1803  max mem: 15572
[2025-01-13 00:22:16,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:22:16,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:22:16,796] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17158
[2025-01-13 00:22:16,797] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:22:16,797] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 140/1576]  eta: 0:15:06  lr: 0.000046  min_lr: 0.000000  loss: 4.5985 (4.6054)  loss_scale: 65536.0000 (68789.5603)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1671  max mem: 15572
Epoch: [7]  [ 150/1576]  eta: 0:15:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5819 (4.6116)  loss_scale: 65536.0000 (68574.0927)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1813  max mem: 15572
Epoch: [7]  [ 160/1576]  eta: 0:14:54  lr: 0.000046  min_lr: 0.000000  loss: 4.5819 (4.6065)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1709  max mem: 15572
Epoch: [7]  [ 170/1576]  eta: 0:14:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4754 (4.6041)  loss_scale: 65536.0000 (68218.7602)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0943  max mem: 15572
Epoch: [7]  [ 180/1576]  eta: 0:14:36  lr: 0.000046  min_lr: 0.000000  loss: 4.6857 (4.6133)  loss_scale: 65536.0000 (68070.5414)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1346  max mem: 15572
Epoch: [7]  [ 190/1576]  eta: 0:14:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6749 (4.6162)  loss_scale: 65536.0000 (67937.8429)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1635  max mem: 15572
Epoch: [7]  [ 200/1576]  eta: 0:14:24  lr: 0.000046  min_lr: 0.000000  loss: 4.6204 (4.6124)  loss_scale: 65536.0000 (67818.3483)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1901  max mem: 15572
Epoch: [7]  [ 210/1576]  eta: 0:14:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5507 (4.6139)  loss_scale: 65536.0000 (67710.1801)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1805  max mem: 15572
Epoch: [7]  [ 220/1576]  eta: 0:14:04  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.6138)  loss_scale: 65536.0000 (67611.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1189  max mem: 15572
Epoch: [7]  [ 230/1576]  eta: 0:13:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.6135)  loss_scale: 65536.0000 (67521.9394)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1621  max mem: 15572
Epoch: [7]  [ 240/1576]  eta: 0:13:48  lr: 0.000046  min_lr: 0.000000  loss: 4.6030 (4.6187)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1404  max mem: 15572
Epoch: [7]  [ 250/1576]  eta: 0:13:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5298 (4.6128)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0985  max mem: 15572
Epoch: [7]  [ 260/1576]  eta: 0:13:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5298 (4.6177)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1354  max mem: 15572
[2025-01-13 00:23:35,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:23:35,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:23:36,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17290
[2025-01-13 00:23:36,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:23:36,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 270/1576]  eta: 0:13:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5989 (4.6179)  loss_scale: 65536.0000 (67954.3026)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.2021  max mem: 15572
Epoch: [7]  [ 280/1576]  eta: 0:13:16  lr: 0.000046  min_lr: 0.000000  loss: 4.5989 (4.6164)  loss_scale: 65536.0000 (67868.2420)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1444  max mem: 15572
Epoch: [7]  [ 290/1576]  eta: 0:13:10  lr: 0.000046  min_lr: 0.000000  loss: 4.6785 (4.6193)  loss_scale: 65536.0000 (67788.0962)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0914  max mem: 15572
Epoch: [7]  [ 300/1576]  eta: 0:13:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5674 (4.6164)  loss_scale: 65536.0000 (67713.2757)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.0992  max mem: 15572
Epoch: [7]  [ 310/1576]  eta: 0:12:54  lr: 0.000046  min_lr: 0.000000  loss: 4.7617 (4.6237)  loss_scale: 65536.0000 (67643.2669)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1096  max mem: 15572
Epoch: [7]  [ 320/1576]  eta: 0:12:52  lr: 0.000046  min_lr: 0.000000  loss: 4.7617 (4.6207)  loss_scale: 65536.0000 (67577.6199)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1864  max mem: 15572
Epoch: [7]  [ 330/1576]  eta: 0:12:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5587 (4.6213)  loss_scale: 65536.0000 (67515.9396)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1497  max mem: 15572
Epoch: [7]  [ 340/1576]  eta: 0:12:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6235 (4.6184)  loss_scale: 65536.0000 (67457.8768)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1205  max mem: 15572
Epoch: [7]  [ 350/1576]  eta: 0:12:32  lr: 0.000046  min_lr: 0.000000  loss: 4.5082 (4.6175)  loss_scale: 65536.0000 (67403.1225)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1628  max mem: 15572
Epoch: [7]  [ 360/1576]  eta: 0:12:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5540 (4.6168)  loss_scale: 65536.0000 (67351.4017)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1641  max mem: 15572
Epoch: [7]  [ 370/1576]  eta: 0:12:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5859 (4.6176)  loss_scale: 65536.0000 (67302.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1207  max mem: 15572
Epoch: [7]  [ 380/1576]  eta: 0:12:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6135 (4.6208)  loss_scale: 65536.0000 (67256.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1212  max mem: 15572
Epoch: [7]  [ 390/1576]  eta: 0:12:04  lr: 0.000046  min_lr: 0.000000  loss: 4.6550 (4.6213)  loss_scale: 65536.0000 (67212.1125)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1404  max mem: 15572
[2025-01-13 00:24:52,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:24:52,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 400/1576]  eta: 0:11:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6197 (4.6224)  loss_scale: 65536.0000 (67824.0399)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1081  max mem: 15572
[2025-01-13 00:24:58,915] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17430
[2025-01-13 00:24:58,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:24:58,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 410/1576]  eta: 0:11:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6197 (4.6225)  loss_scale: 131072.0000 (68884.5547)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1038  max mem: 15572
Epoch: [7]  [ 420/1576]  eta: 0:11:44  lr: 0.000046  min_lr: 0.000000  loss: 4.5921 (4.6231)  loss_scale: 65536.0000 (68805.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1258  max mem: 15572
Epoch: [7]  [ 430/1576]  eta: 0:11:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5488 (4.6217)  loss_scale: 65536.0000 (68729.1694)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.1387  max mem: 15572
Epoch: [7]  [ 440/1576]  eta: 0:11:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5430 (4.6218)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0700  max mem: 15572
Epoch: [7]  [ 450/1576]  eta: 0:11:23  lr: 0.000046  min_lr: 0.000000  loss: 4.6692 (4.6237)  loss_scale: 65536.0000 (68587.5654)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0826  max mem: 15572
Epoch: [7]  [ 460/1576]  eta: 0:11:16  lr: 0.000046  min_lr: 0.000000  loss: 4.7694 (4.6256)  loss_scale: 65536.0000 (68521.3709)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1286  max mem: 15572
Epoch: [7]  [ 470/1576]  eta: 0:11:10  lr: 0.000046  min_lr: 0.000000  loss: 4.7434 (4.6272)  loss_scale: 65536.0000 (68457.9873)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1159  max mem: 15572
Epoch: [7]  [ 480/1576]  eta: 0:11:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5343 (4.6232)  loss_scale: 65536.0000 (68397.2391)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0883  max mem: 15572
Epoch: [7]  [ 490/1576]  eta: 0:10:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4635 (4.6223)  loss_scale: 65536.0000 (68338.9654)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0624  max mem: 15572
Epoch: [7]  [ 500/1576]  eta: 0:10:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5528 (4.6229)  loss_scale: 65536.0000 (68283.0180)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1342  max mem: 15572
Epoch: [7]  [ 510/1576]  eta: 0:10:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5088 (4.6226)  loss_scale: 65536.0000 (68229.2603)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1804  max mem: 15572
Epoch: [7]  [ 520/1576]  eta: 0:10:34  lr: 0.000046  min_lr: 0.000000  loss: 4.5300 (4.6232)  loss_scale: 65536.0000 (68177.5662)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1068  max mem: 15572
Epoch: [7]  [ 530/1576]  eta: 0:10:29  lr: 0.000046  min_lr: 0.000000  loss: 4.5300 (4.6209)  loss_scale: 65536.0000 (68127.8192)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1171  max mem: 15572
[2025-01-13 00:26:12,587] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:26:12,588] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 540/1576]  eta: 0:10:23  lr: 0.000046  min_lr: 0.000000  loss: 4.5298 (4.6222)  loss_scale: 65536.0000 (68564.4658)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1705  max mem: 15572
[2025-01-13 00:26:17,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17565
[2025-01-13 00:26:17,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:26:17,028] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 550/1576]  eta: 0:10:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6408 (4.6220)  loss_scale: 65536.0000 (68747.3829)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1009  max mem: 15572
Epoch: [7]  [ 560/1576]  eta: 0:10:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6125 (4.6219)  loss_scale: 65536.0000 (68690.1390)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1315  max mem: 15572
Epoch: [7]  [ 570/1576]  eta: 0:10:04  lr: 0.000046  min_lr: 0.000000  loss: 4.6286 (4.6233)  loss_scale: 65536.0000 (68634.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1062  max mem: 15572
Epoch: [7]  [ 580/1576]  eta: 0:09:58  lr: 0.000046  min_lr: 0.000000  loss: 4.7191 (4.6238)  loss_scale: 65536.0000 (68581.5628)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0668  max mem: 15572
Epoch: [7]  [ 590/1576]  eta: 0:09:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5895 (4.6227)  loss_scale: 65536.0000 (68530.0305)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1242  max mem: 15572
Epoch: [7]  [ 600/1576]  eta: 0:09:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5122 (4.6195)  loss_scale: 65536.0000 (68480.2130)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1270  max mem: 15572
Epoch: [7]  [ 610/1576]  eta: 0:09:38  lr: 0.000046  min_lr: 0.000000  loss: 4.4983 (4.6175)  loss_scale: 65536.0000 (68432.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1305  max mem: 15572
Epoch: [7]  [ 620/1576]  eta: 0:09:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6662 (4.6189)  loss_scale: 65536.0000 (68385.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1451  max mem: 15572
Epoch: [7]  [ 630/1576]  eta: 0:09:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6662 (4.6176)  loss_scale: 65536.0000 (68340.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1393  max mem: 15572
Epoch: [7]  [ 640/1576]  eta: 0:09:21  lr: 0.000046  min_lr: 0.000000  loss: 4.6285 (4.6180)  loss_scale: 65536.0000 (68296.4867)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1524  max mem: 15572
Epoch: [7]  [ 650/1576]  eta: 0:09:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6788 (4.6181)  loss_scale: 65536.0000 (68254.0829)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1783  max mem: 15572
Epoch: [7]  [ 660/1576]  eta: 0:09:09  lr: 0.000046  min_lr: 0.000000  loss: 4.6282 (4.6178)  loss_scale: 65536.0000 (68212.9622)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1425  max mem: 15572
Epoch: [7]  [ 670/1576]  eta: 0:09:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5354 (4.6162)  loss_scale: 65536.0000 (68173.0671)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1277  max mem: 15572
[2025-01-13 00:27:33,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:27:33,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:27:36,634] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17699
[2025-01-13 00:27:36,634] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:27:36,634] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 680/1576]  eta: 0:08:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5720 (4.6166)  loss_scale: 65536.0000 (68615.5184)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1220  max mem: 15572
Epoch: [7]  [ 690/1576]  eta: 0:08:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6343 (4.6156)  loss_scale: 65536.0000 (68570.9522)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1099  max mem: 15572
Epoch: [7]  [ 700/1576]  eta: 0:08:43  lr: 0.000046  min_lr: 0.000000  loss: 4.6083 (4.6150)  loss_scale: 65536.0000 (68527.6576)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0892  max mem: 15572
Epoch: [7]  [ 710/1576]  eta: 0:08:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6083 (4.6145)  loss_scale: 65536.0000 (68485.5809)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0994  max mem: 15572
Epoch: [7]  [ 720/1576]  eta: 0:08:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6047 (4.6150)  loss_scale: 65536.0000 (68444.6713)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1628  max mem: 15572
Epoch: [7]  [ 730/1576]  eta: 0:08:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6342 (4.6161)  loss_scale: 65536.0000 (68404.8810)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.1949  max mem: 15572
Epoch: [7]  [ 740/1576]  eta: 0:08:21  lr: 0.000046  min_lr: 0.000000  loss: 4.6342 (4.6160)  loss_scale: 65536.0000 (68366.1646)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1854  max mem: 15572
Epoch: [7]  [ 750/1576]  eta: 0:08:13  lr: 0.000046  min_lr: 0.000000  loss: 4.6137 (4.6148)  loss_scale: 65536.0000 (68328.4794)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0891  max mem: 15572
Epoch: [7]  [ 760/1576]  eta: 0:08:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5997 (4.6136)  loss_scale: 65536.0000 (68291.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0773  max mem: 15572
Epoch: [7]  [ 770/1576]  eta: 0:08:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5997 (4.6132)  loss_scale: 65536.0000 (68256.0415)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1266  max mem: 15572
Epoch: [7]  [ 780/1576]  eta: 0:07:54  lr: 0.000046  min_lr: 0.000000  loss: 4.5433 (4.6122)  loss_scale: 65536.0000 (68221.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1170  max mem: 15572
Epoch: [7]  [ 790/1576]  eta: 0:07:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6776 (4.6129)  loss_scale: 65536.0000 (68187.2668)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.2091  max mem: 15572
Epoch: [7]  [ 800/1576]  eta: 0:07:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5118 (4.6113)  loss_scale: 65536.0000 (68154.1673)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.2324  max mem: 15572
[2025-01-13 00:28:52,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:28:52,548] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 810/1576]  eta: 0:07:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5086 (4.6112)  loss_scale: 65536.0000 (68525.9285)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1458  max mem: 15572
[2025-01-13 00:28:56,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17835
[2025-01-13 00:28:56,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:28:56,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 820/1576]  eta: 0:07:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6093 (4.6111)  loss_scale: 65536.0000 (68649.1596)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0914  max mem: 15572
Epoch: [7]  [ 830/1576]  eta: 0:07:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5346 (4.6105)  loss_scale: 65536.0000 (68611.6968)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1162  max mem: 15572
Epoch: [7]  [ 840/1576]  eta: 0:07:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5104 (4.6102)  loss_scale: 65536.0000 (68575.1249)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1195  max mem: 15572
Epoch: [7]  [ 850/1576]  eta: 0:07:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4771 (4.6091)  loss_scale: 65536.0000 (68539.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0982  max mem: 15572
Epoch: [7]  [ 860/1576]  eta: 0:07:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5411 (4.6085)  loss_scale: 65536.0000 (68504.5296)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1506  max mem: 15572
Epoch: [7]  [ 870/1576]  eta: 0:07:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5734 (4.6073)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1578  max mem: 15572
Epoch: [7]  [ 880/1576]  eta: 0:06:55  lr: 0.000046  min_lr: 0.000000  loss: 4.6285 (4.6081)  loss_scale: 65536.0000 (68437.1396)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1111  max mem: 15572
Epoch: [7]  [ 890/1576]  eta: 0:06:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6702 (4.6090)  loss_scale: 65536.0000 (68404.5791)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1391  max mem: 15572
Epoch: [7]  [ 900/1576]  eta: 0:06:43  lr: 0.000046  min_lr: 0.000000  loss: 4.7031 (4.6103)  loss_scale: 65536.0000 (68372.7414)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1761  max mem: 15572
Epoch: [7]  [ 910/1576]  eta: 0:06:37  lr: 0.000046  min_lr: 0.000000  loss: 4.7033 (4.6113)  loss_scale: 65536.0000 (68341.6026)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1474  max mem: 15572
Epoch: [7]  [ 920/1576]  eta: 0:06:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5812 (4.6107)  loss_scale: 65536.0000 (68311.1401)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1337  max mem: 15572
Epoch: [7]  [ 930/1576]  eta: 0:06:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5722 (4.6098)  loss_scale: 65536.0000 (68281.3319)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1774  max mem: 15572
Epoch: [7]  [ 940/1576]  eta: 0:06:20  lr: 0.000046  min_lr: 0.000000  loss: 4.5784 (4.6102)  loss_scale: 65536.0000 (68252.1573)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1965  max mem: 15572
[2025-01-13 00:30:13,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:30:13,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:30:14,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17966
[2025-01-13 00:30:14,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:30:14,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 950/1576]  eta: 0:06:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5722 (4.6105)  loss_scale: 65536.0000 (68361.4217)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1918  max mem: 15572
Epoch: [7]  [ 960/1576]  eta: 0:06:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5722 (4.6095)  loss_scale: 65536.0000 (68332.0208)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.2027  max mem: 15572
Epoch: [7]  [ 970/1576]  eta: 0:06:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6223 (4.6098)  loss_scale: 65536.0000 (68303.2255)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1465  max mem: 15572
[2025-01-13 00:30:35,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=113, lr=[4.479232789900983e-07, 4.479232789900983e-07, 6.398903985572833e-07, 6.398903985572833e-07, 9.141291407961192e-07, 9.141291407961192e-07, 1.3058987725658846e-06, 1.3058987725658846e-06, 1.865569675094121e-06, 1.865569675094121e-06, 2.665099535848744e-06, 2.665099535848744e-06, 3.807285051212492e-06, 3.807285051212492e-06, 5.438978644589275e-06, 5.438978644589275e-06, 7.769969492270392e-06, 7.769969492270392e-06, 1.1099956417529133e-05, 1.1099956417529133e-05, 1.585708059647019e-05, 1.585708059647019e-05, 2.2652972280671704e-05, 2.2652972280671704e-05, 3.236138897238815e-05, 3.236138897238815e-05, 4.623055567484022e-05, 4.623055567484022e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:30:35,177] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=27.807134256122914, CurrSamplesPerSec=30.657259648680405, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 980/1576]  eta: 0:05:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5244 (4.6087)  loss_scale: 65536.0000 (68275.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0905  max mem: 15572
Epoch: [7]  [ 990/1576]  eta: 0:05:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4880 (4.6086)  loss_scale: 65536.0000 (68247.3784)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1169  max mem: 15572
Epoch: [7]  [1000/1576]  eta: 0:05:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6843 (4.6092)  loss_scale: 65536.0000 (68220.2917)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1027  max mem: 15572
Epoch: [7]  [1010/1576]  eta: 0:05:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5824 (4.6086)  loss_scale: 65536.0000 (68193.7409)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0694  max mem: 15572
Epoch: [7]  [1020/1576]  eta: 0:05:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6055 (4.6102)  loss_scale: 65536.0000 (68167.7101)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0887  max mem: 15572
Epoch: [7]  [1030/1576]  eta: 0:05:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6150 (4.6084)  loss_scale: 65536.0000 (68142.1843)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1488  max mem: 15572
Epoch: [7]  [1040/1576]  eta: 0:05:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4722 (4.6071)  loss_scale: 65536.0000 (68117.1489)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1905  max mem: 15572
Epoch: [7]  [1050/1576]  eta: 0:05:13  lr: 0.000046  min_lr: 0.000000  loss: 4.5732 (4.6076)  loss_scale: 65536.0000 (68092.5899)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0899  max mem: 15572
Epoch: [7]  [1060/1576]  eta: 0:05:07  lr: 0.000046  min_lr: 0.000000  loss: 4.6947 (4.6085)  loss_scale: 65536.0000 (68068.4939)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0512  max mem: 15572
Epoch: [7]  [1070/1576]  eta: 0:05:01  lr: 0.000046  min_lr: 0.000000  loss: 4.7289 (4.6091)  loss_scale: 65536.0000 (68044.8478)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1156  max mem: 15572
[2025-01-13 00:31:29,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:31:29,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:31:32,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18098
[2025-01-13 00:31:32,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:31:32,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1080/1576]  eta: 0:04:55  lr: 0.000046  min_lr: 0.000000  loss: 4.8191 (4.6112)  loss_scale: 65536.0000 (68203.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1358  max mem: 15572
Epoch: [7]  [1090/1576]  eta: 0:04:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6726 (4.6109)  loss_scale: 65536.0000 (68179.0651)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1227  max mem: 15572
Epoch: [7]  [1100/1576]  eta: 0:04:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5546 (4.6101)  loss_scale: 65536.0000 (68155.0590)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1805  max mem: 15572
Epoch: [7]  [1110/1576]  eta: 0:04:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5374 (4.6096)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.2148  max mem: 15572
Epoch: [7]  [1120/1576]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6070 (4.6101)  loss_scale: 65536.0000 (68108.3318)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1748  max mem: 15572
Epoch: [7]  [1130/1576]  eta: 0:04:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6901 (4.6109)  loss_scale: 65536.0000 (68085.5880)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1267  max mem: 15572
Epoch: [7]  [1140/1576]  eta: 0:04:19  lr: 0.000046  min_lr: 0.000000  loss: 4.6504 (4.6104)  loss_scale: 65536.0000 (68063.2428)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1143  max mem: 15572
Epoch: [7]  [1150/1576]  eta: 0:04:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5899 (4.6103)  loss_scale: 65536.0000 (68041.2858)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.2031  max mem: 15572
Epoch: [7]  [1160/1576]  eta: 0:04:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5816 (4.6089)  loss_scale: 65536.0000 (68019.7071)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1895  max mem: 15572
Epoch: [7]  [1170/1576]  eta: 0:04:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6326 (4.6095)  loss_scale: 65536.0000 (67998.4970)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1639  max mem: 15572
Epoch: [7]  [1180/1576]  eta: 0:03:55  lr: 0.000046  min_lr: 0.000000  loss: 4.6545 (4.6092)  loss_scale: 65536.0000 (67977.6461)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1052  max mem: 15572
Epoch: [7]  [1190/1576]  eta: 0:03:49  lr: 0.000046  min_lr: 0.000000  loss: 4.7075 (4.6106)  loss_scale: 65536.0000 (67957.1453)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0921  max mem: 15572
Epoch: [7]  [1200/1576]  eta: 0:03:43  lr: 0.000046  min_lr: 0.000000  loss: 4.7075 (4.6115)  loss_scale: 65536.0000 (67936.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1373  max mem: 15572
[2025-01-13 00:32:48,397] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:32:48,398] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1210/1576]  eta: 0:03:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6814 (4.6118)  loss_scale: 65536.0000 (68241.8629)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1060  max mem: 15572
[2025-01-13 00:32:52,446] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18236
[2025-01-13 00:32:52,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:32:52,447] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1220/1576]  eta: 0:03:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6107 (4.6125)  loss_scale: 65536.0000 (68380.7240)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1693  max mem: 15572
Epoch: [7]  [1230/1576]  eta: 0:03:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6451 (4.6132)  loss_scale: 65536.0000 (68357.6149)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1678  max mem: 15572
Epoch: [7]  [1240/1576]  eta: 0:03:19  lr: 0.000046  min_lr: 0.000000  loss: 4.7380 (4.6141)  loss_scale: 65536.0000 (68334.8783)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1149  max mem: 15572
Epoch: [7]  [1250/1576]  eta: 0:03:13  lr: 0.000046  min_lr: 0.000000  loss: 4.7354 (4.6146)  loss_scale: 65536.0000 (68312.5052)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0985  max mem: 15572
Epoch: [7]  [1260/1576]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 4.6284 (4.6143)  loss_scale: 65536.0000 (68290.4869)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0829  max mem: 15572
Epoch: [7]  [1270/1576]  eta: 0:03:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5774 (4.6145)  loss_scale: 65536.0000 (68268.8151)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1301  max mem: 15572
Epoch: [7]  [1280/1576]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000000  loss: 4.6278 (4.6143)  loss_scale: 65536.0000 (68247.4817)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1870  max mem: 15572
Epoch: [7]  [1290/1576]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6633 (4.6149)  loss_scale: 65536.0000 (68226.4787)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1793  max mem: 15572
Epoch: [7]  [1300/1576]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6722 (4.6145)  loss_scale: 65536.0000 (68205.7986)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1305  max mem: 15572
Epoch: [7]  [1310/1576]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6124 (4.6143)  loss_scale: 65536.0000 (68185.4340)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0676  max mem: 15572
Epoch: [7]  [1320/1576]  eta: 0:02:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6124 (4.6141)  loss_scale: 65536.0000 (68165.3777)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0466  max mem: 15572
Epoch: [7]  [1330/1576]  eta: 0:02:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6109 (4.6139)  loss_scale: 65536.0000 (68145.6228)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1270  max mem: 15572
Epoch: [7]  [1340/1576]  eta: 0:02:19  lr: 0.000046  min_lr: 0.000000  loss: 4.5235 (4.6131)  loss_scale: 65536.0000 (68126.1626)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1145  max mem: 15572
[2025-01-13 00:34:06,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:34:06,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:34:06,855] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18366
[2025-01-13 00:34:06,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:34:06,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1350/1576]  eta: 0:02:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4507 (4.6122)  loss_scale: 65536.0000 (68155.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0924  max mem: 15572
Epoch: [7]  [1360/1576]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5248 (4.6130)  loss_scale: 65536.0000 (68136.2528)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1530  max mem: 15572
Epoch: [7]  [1370/1576]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6874 (4.6131)  loss_scale: 65536.0000 (68117.2867)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1532  max mem: 15572
Epoch: [7]  [1380/1576]  eta: 0:01:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5813 (4.6129)  loss_scale: 65536.0000 (68098.5952)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1602  max mem: 15572
Epoch: [7]  [1390/1576]  eta: 0:01:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6641 (4.6137)  loss_scale: 65536.0000 (68080.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2106  max mem: 15572
Epoch: [7]  [1400/1576]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6862 (4.6137)  loss_scale: 65536.0000 (68062.0128)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1776  max mem: 15572
Epoch: [7]  [1410/1576]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000000  loss: 4.6862 (4.6136)  loss_scale: 65536.0000 (68044.1106)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1628  max mem: 15572
Epoch: [7]  [1420/1576]  eta: 0:01:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4839 (4.6128)  loss_scale: 65536.0000 (68026.4602)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1727  max mem: 15572
Epoch: [7]  [1430/1576]  eta: 0:01:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3975 (4.6116)  loss_scale: 65536.0000 (68009.0566)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0741  max mem: 15572
Epoch: [7]  [1440/1576]  eta: 0:01:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3902 (4.6114)  loss_scale: 65536.0000 (67991.8945)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0773  max mem: 15572
Epoch: [7]  [1450/1576]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000000  loss: 4.5855 (4.6113)  loss_scale: 65536.0000 (67974.9690)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1515  max mem: 15572
Epoch: [7]  [1460/1576]  eta: 0:01:08  lr: 0.000046  min_lr: 0.000000  loss: 4.6259 (4.6116)  loss_scale: 65536.0000 (67958.2752)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1570  max mem: 15572
Epoch: [7]  [1470/1576]  eta: 0:01:02  lr: 0.000046  min_lr: 0.000000  loss: 4.6112 (4.6119)  loss_scale: 65536.0000 (67941.8083)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1630  max mem: 15572
[2025-01-13 00:35:25,408] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:35:25,408] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:35:29,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18500
[2025-01-13 00:35:29,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:35:29,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1480/1576]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6012 (4.6124)  loss_scale: 65536.0000 (68146.8197)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1725  max mem: 15572
Epoch: [7]  [1490/1576]  eta: 0:00:51  lr: 0.000046  min_lr: 0.000000  loss: 4.6012 (4.6122)  loss_scale: 65536.0000 (68129.3092)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1533  max mem: 15572
Epoch: [7]  [1500/1576]  eta: 0:00:45  lr: 0.000046  min_lr: 0.000000  loss: 4.6047 (4.6127)  loss_scale: 65536.0000 (68112.0320)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1211  max mem: 15572
Epoch: [7]  [1510/1576]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000000  loss: 4.6672 (4.6126)  loss_scale: 65536.0000 (68094.9835)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1229  max mem: 15572
Epoch: [7]  [1520/1576]  eta: 0:00:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5609 (4.6114)  loss_scale: 65536.0000 (68078.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1334  max mem: 15572
Epoch: [7]  [1530/1576]  eta: 0:00:27  lr: 0.000046  min_lr: 0.000000  loss: 4.5563 (4.6113)  loss_scale: 65536.0000 (68061.5545)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1242  max mem: 15572
Epoch: [7]  [1540/1576]  eta: 0:00:21  lr: 0.000046  min_lr: 0.000000  loss: 4.5698 (4.6112)  loss_scale: 65536.0000 (68045.1655)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0894  max mem: 15572
Epoch: [7]  [1550/1576]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000000  loss: 4.5569 (4.6113)  loss_scale: 65536.0000 (68028.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1201  max mem: 15572
Epoch: [7]  [1560/1576]  eta: 0:00:09  lr: 0.000046  min_lr: 0.000000  loss: 4.6306 (4.6114)  loss_scale: 65536.0000 (68013.0173)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1578  max mem: 15572
Epoch: [7]  [1570/1576]  eta: 0:00:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5765 (4.6105)  loss_scale: 65536.0000 (67997.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.1003  max mem: 15572
Epoch: [7]  [1575/1576]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.5765 (4.6098)  loss_scale: 65536.0000 (67989.4416)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.1001  max mem: 15572
Epoch: [7] Total time: 0:15:33 (0.5920 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.5765 (4.6098)  loss_scale: 65536.0000 (67989.4416)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2528
Indices to remove: tensor([   33,    85,   107,  ..., 33698, 33703, 33708], device='cuda:0')
length of data loader train is: 1366
num_training_steps_per_epoch is: 1366
Change step level LR scheduler!
Set warmup steps = 6830
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:27:41  loss: 1.5822 (1.5822)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.1101  data: 5.9268  max mem: 15572
Val:  [ 10/272]  eta: 0:03:49  loss: 3.8902 (3.7923)  acc1: 16.6667 (19.1919)  acc5: 33.3333 (36.3636)  time: 0.8775  data: 0.6891  max mem: 15572
Val:  [ 20/272]  eta: 0:02:20  loss: 3.8583 (3.7999)  acc1: 16.6667 (18.7831)  acc5: 38.8889 (41.5344)  time: 0.2807  data: 0.0830  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 3.6972 (3.8063)  acc1: 16.6667 (20.0717)  acc5: 44.4444 (41.9355)  time: 0.2325  data: 0.0317  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.5061 (3.7099)  acc1: 16.6667 (18.4282)  acc5: 50.0000 (46.3415)  time: 0.2874  data: 0.0875  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 3.3708 (3.6041)  acc1: 16.6667 (21.6776)  acc5: 61.1111 (49.7821)  time: 0.3519  data: 0.1506  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 2.8575 (3.5343)  acc1: 22.2222 (22.4954)  acc5: 72.2222 (52.7322)  time: 0.3491  data: 0.1493  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 3.0152 (3.4602)  acc1: 38.8889 (25.9781)  acc5: 77.7778 (56.4945)  time: 0.3007  data: 0.0942  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.0944 (3.4631)  acc1: 33.3333 (24.6914)  acc5: 72.2222 (56.3100)  time: 0.3147  data: 0.1061  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 3.9576 (3.5367)  acc1: 5.5556 (22.5885)  acc5: 33.3333 (52.8083)  time: 0.3393  data: 0.1425  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.9899 (3.5866)  acc1: 5.5556 (21.5622)  acc5: 33.3333 (51.7602)  time: 0.3219  data: 0.1244  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.9261 (3.6281)  acc1: 5.5556 (20.2202)  acc5: 33.3333 (50.1502)  time: 0.3279  data: 0.1255  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.9996 (3.6524)  acc1: 5.5556 (19.8806)  acc5: 38.8889 (49.6786)  time: 0.3486  data: 0.1524  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.8211 (3.6276)  acc1: 16.6667 (20.7379)  acc5: 44.4444 (50.4241)  time: 0.3515  data: 0.1432  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 3.5164 (3.6324)  acc1: 11.1111 (19.9764)  acc5: 50.0000 (50.3940)  time: 0.3309  data: 0.1057  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.5164 (3.6146)  acc1: 16.6667 (20.2355)  acc5: 50.0000 (50.9566)  time: 0.3197  data: 0.0645  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.4639 (3.6055)  acc1: 22.2222 (21.0490)  acc5: 55.5556 (51.4493)  time: 0.3232  data: 0.0715  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.6301 (3.6260)  acc1: 16.6667 (20.3379)  acc5: 50.0000 (50.8447)  time: 0.3244  data: 0.0864  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.4993 (3.5980)  acc1: 16.6667 (21.1172)  acc5: 55.5556 (51.9644)  time: 0.3234  data: 0.0909  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.3171 (3.6059)  acc1: 16.6667 (20.3898)  acc5: 55.5556 (51.1926)  time: 0.3064  data: 0.0807  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.4560 (3.6148)  acc1: 16.6667 (20.6744)  acc5: 55.5556 (51.2714)  time: 0.2946  data: 0.0764  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.5807 (3.6257)  acc1: 27.7778 (21.1690)  acc5: 61.1111 (51.3955)  time: 0.3165  data: 0.1237  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.6528 (3.6157)  acc1: 27.7778 (21.6440)  acc5: 55.5556 (51.6088)  time: 0.3744  data: 0.1784  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.4268 (3.6153)  acc1: 22.2222 (22.0298)  acc5: 66.6667 (52.0443)  time: 0.3502  data: 0.1474  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.4371 (3.6049)  acc1: 27.7778 (22.2453)  acc5: 66.6667 (52.7201)  time: 0.2794  data: 0.0610  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.5336 (3.6253)  acc1: 16.6667 (22.0673)  acc5: 61.1111 (51.9699)  time: 0.3163  data: 0.0902  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.2938 (3.5920)  acc1: 33.3333 (23.5632)  acc5: 66.6667 (53.1290)  time: 0.3415  data: 0.1366  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.4426 (3.6005)  acc1: 22.2222 (23.1242)  acc5: 61.1111 (52.7060)  time: 0.2349  data: 0.0601  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.4426 (3.6035)  acc1: 16.6667 (23.1006)  acc5: 55.5556 (52.6725)  time: 0.2256  data: 0.0600  max mem: 15572
Val: Total time: 0:01:31 (0.3365 s / it)
* Acc@1 23.101 Acc@5 52.673 loss 3.604
Accuracy of the network on the 4883 val videos: 23.1%
[2025-01-13 00:37:54,551] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:37:54,556] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:37:54,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:37:57,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:37:57,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 23.10%
Epoch: [8]  [   0/1366]  eta: 3:26:10  lr: 0.000046  min_lr: 0.000000  loss: 4.6622 (4.6622)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.0563  data: 8.3494  max mem: 15572
Epoch: [8]  [  10/1366]  eta: 0:27:31  lr: 0.000046  min_lr: 0.000000  loss: 4.5951 (4.5676)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2179  data: 0.7661  max mem: 15572
Epoch: [8]  [  20/1366]  eta: 0:21:19  lr: 0.000046  min_lr: 0.000000  loss: 4.6203 (4.6317)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1234  max mem: 15572
Epoch: [8]  [  30/1366]  eta: 0:18:05  lr: 0.000046  min_lr: 0.000000  loss: 4.5352 (4.5828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1519  max mem: 15572
[2025-01-13 00:38:22,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:38:22,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:38:23,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18630
[2025-01-13 00:38:23,549] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:38:23,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [  40/1366]  eta: 0:16:52  lr: 0.000046  min_lr: 0.000000  loss: 4.4528 (4.5759)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1145  max mem: 15572
Epoch: [8]  [  50/1366]  eta: 0:16:19  lr: 0.000046  min_lr: 0.000000  loss: 4.5562 (4.5837)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1936  max mem: 15572
Epoch: [8]  [  60/1366]  eta: 0:15:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5562 (4.5871)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.1972  max mem: 15572
Epoch: [8]  [  70/1366]  eta: 0:15:11  lr: 0.000046  min_lr: 0.000000  loss: 4.5943 (4.5889)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1401  max mem: 15572
Epoch: [8]  [  80/1366]  eta: 0:14:47  lr: 0.000046  min_lr: 0.000000  loss: 4.6124 (4.5914)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1333  max mem: 15572
Epoch: [8]  [  90/1366]  eta: 0:14:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5555 (4.5874)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0793  max mem: 15572
Epoch: [8]  [ 100/1366]  eta: 0:13:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6211 (4.5992)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0641  max mem: 15572
Epoch: [8]  [ 110/1366]  eta: 0:13:41  lr: 0.000046  min_lr: 0.000000  loss: 4.6534 (4.5944)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1569  max mem: 15572
Epoch: [8]  [ 120/1366]  eta: 0:13:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6534 (4.6001)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1432  max mem: 15572
Epoch: [8]  [ 130/1366]  eta: 0:13:08  lr: 0.000046  min_lr: 0.000000  loss: 4.6283 (4.6005)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0826  max mem: 15572
Epoch: [8]  [ 140/1366]  eta: 0:12:56  lr: 0.000046  min_lr: 0.000000  loss: 4.6201 (4.6017)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0955  max mem: 15572
Epoch: [8]  [ 150/1366]  eta: 0:12:47  lr: 0.000046  min_lr: 0.000000  loss: 4.7138 (4.6072)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1473  max mem: 15572
Epoch: [8]  [ 160/1366]  eta: 0:12:34  lr: 0.000046  min_lr: 0.000000  loss: 4.6658 (4.6061)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1415  max mem: 15572
[2025-01-13 00:39:38,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:39:38,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 170/1366]  eta: 0:12:25  lr: 0.000046  min_lr: 0.000000  loss: 4.5472 (4.6070)  loss_scale: 65536.0000 (69751.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1267  max mem: 15572
[2025-01-13 00:39:47,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18775
[2025-01-13 00:39:47,522] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:39:47,522] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 180/1366]  eta: 0:12:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6199 (4.6108)  loss_scale: 131072.0000 (71691.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1371  max mem: 15572
Epoch: [8]  [ 190/1366]  eta: 0:12:03  lr: 0.000046  min_lr: 0.000000  loss: 4.6199 (4.6101)  loss_scale: 65536.0000 (71369.0471)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0811  max mem: 15572
Epoch: [8]  [ 200/1366]  eta: 0:11:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6590 (4.6128)  loss_scale: 65536.0000 (71078.8458)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0927  max mem: 15572
Epoch: [8]  [ 210/1366]  eta: 0:11:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5911 (4.6120)  loss_scale: 65536.0000 (70816.1517)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1429  max mem: 15572
Epoch: [8]  [ 220/1366]  eta: 0:11:43  lr: 0.000046  min_lr: 0.000000  loss: 4.7133 (4.6177)  loss_scale: 65536.0000 (70577.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1203  max mem: 15572
Epoch: [8]  [ 230/1366]  eta: 0:11:35  lr: 0.000046  min_lr: 0.000000  loss: 4.6136 (4.6172)  loss_scale: 65536.0000 (70358.9957)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1180  max mem: 15572
Epoch: [8]  [ 240/1366]  eta: 0:11:33  lr: 0.000046  min_lr: 0.000000  loss: 4.6102 (4.6179)  loss_scale: 65536.0000 (70158.8714)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1956  max mem: 15572
Epoch: [8]  [ 250/1366]  eta: 0:11:30  lr: 0.000046  min_lr: 0.000000  loss: 4.6357 (4.6145)  loss_scale: 65536.0000 (69974.6932)  weight_decay: 0.0500 (0.0500)  time: 0.6928  data: 0.2548  max mem: 15572
Epoch: [8]  [ 260/1366]  eta: 0:11:19  lr: 0.000046  min_lr: 0.000000  loss: 4.5432 (4.6140)  loss_scale: 65536.0000 (69804.6284)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1446  max mem: 15572
Epoch: [8]  [ 270/1366]  eta: 0:11:14  lr: 0.000046  min_lr: 0.000000  loss: 4.6069 (4.6156)  loss_scale: 65536.0000 (69647.1144)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1171  max mem: 15572
Epoch: [8]  [ 280/1366]  eta: 0:11:05  lr: 0.000046  min_lr: 0.000000  loss: 4.6267 (4.6136)  loss_scale: 65536.0000 (69500.8114)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1450  max mem: 15572
Epoch: [8]  [ 290/1366]  eta: 0:10:56  lr: 0.000046  min_lr: 0.000000  loss: 4.6318 (4.6158)  loss_scale: 65536.0000 (69364.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0948  max mem: 15572
Epoch: [8]  [ 300/1366]  eta: 0:10:53  lr: 0.000046  min_lr: 0.000000  loss: 4.6871 (4.6193)  loss_scale: 65536.0000 (69237.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1661  max mem: 15572
[2025-01-13 00:41:05,550] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:41:05,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 310/1366]  eta: 0:10:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5604 (4.6176)  loss_scale: 65536.0000 (70171.9871)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1839  max mem: 15572
[2025-01-13 00:41:09,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18912
[2025-01-13 00:41:09,438] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:41:09,438] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 320/1366]  eta: 0:10:40  lr: 0.000046  min_lr: 0.000000  loss: 4.5220 (4.6167)  loss_scale: 65536.0000 (70640.0498)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1468  max mem: 15572
Epoch: [8]  [ 330/1366]  eta: 0:10:33  lr: 0.000046  min_lr: 0.000000  loss: 4.6206 (4.6192)  loss_scale: 65536.0000 (70485.8489)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1221  max mem: 15572
Epoch: [8]  [ 340/1366]  eta: 0:10:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6828 (4.6211)  loss_scale: 65536.0000 (70340.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0924  max mem: 15572
Epoch: [8]  [ 350/1366]  eta: 0:10:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6487 (4.6230)  loss_scale: 65536.0000 (70203.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1333  max mem: 15572
Epoch: [8]  [ 360/1366]  eta: 0:10:12  lr: 0.000046  min_lr: 0.000000  loss: 4.5237 (4.6181)  loss_scale: 65536.0000 (70074.5042)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1242  max mem: 15572
Epoch: [8]  [ 370/1366]  eta: 0:10:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5101 (4.6185)  loss_scale: 65536.0000 (69952.1725)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1360  max mem: 15572
Epoch: [8]  [ 380/1366]  eta: 0:10:00  lr: 0.000046  min_lr: 0.000000  loss: 4.6844 (4.6196)  loss_scale: 65536.0000 (69836.2625)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1436  max mem: 15572
Epoch: [8]  [ 390/1366]  eta: 0:09:53  lr: 0.000046  min_lr: 0.000000  loss: 4.5950 (4.6165)  loss_scale: 65536.0000 (69726.2813)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1037  max mem: 15572
Epoch: [8]  [ 400/1366]  eta: 0:09:46  lr: 0.000046  min_lr: 0.000000  loss: 4.5476 (4.6159)  loss_scale: 65536.0000 (69621.7855)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1168  max mem: 15572
[2025-01-13 00:42:01,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=120, lr=[4.443259866320544e-07, 4.443259866320544e-07, 6.347514094743635e-07, 6.347514094743635e-07, 9.067877278205194e-07, 9.067877278205194e-07, 1.2954110397435993e-06, 1.2954110397435993e-06, 1.8505871996337132e-06, 1.8505871996337132e-06, 2.643695999476733e-06, 2.643695999476733e-06, 3.7767085706810477e-06, 3.7767085706810477e-06, 5.395297958115783e-06, 5.395297958115783e-06, 7.707568511593976e-06, 7.707568511593976e-06, 1.1010812159419967e-05, 1.1010812159419967e-05, 1.5729731656314237e-05, 1.5729731656314237e-05, 2.2471045223306056e-05, 2.2471045223306056e-05, 3.210149317615151e-05, 3.210149317615151e-05, 4.585927596593073e-05, 4.585927596593073e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:42:01,553] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=27.824374014736406, CurrSamplesPerSec=26.11651746890301, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [ 410/1366]  eta: 0:09:40  lr: 0.000046  min_lr: 0.000000  loss: 4.5666 (4.6158)  loss_scale: 65536.0000 (69522.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1491  max mem: 15572
Epoch: [8]  [ 420/1366]  eta: 0:09:35  lr: 0.000046  min_lr: 0.000000  loss: 4.5873 (4.6150)  loss_scale: 65536.0000 (69427.6865)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1896  max mem: 15572
Epoch: [8]  [ 430/1366]  eta: 0:09:28  lr: 0.000046  min_lr: 0.000000  loss: 4.6987 (4.6162)  loss_scale: 65536.0000 (69337.3921)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1613  max mem: 15572
Epoch: [8]  [ 440/1366]  eta: 0:09:21  lr: 0.000046  min_lr: 0.000000  loss: 4.7099 (4.6156)  loss_scale: 65536.0000 (69251.1927)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1073  max mem: 15572
[2025-01-13 00:42:26,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:42:26,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:42:31,023] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19047
[2025-01-13 00:42:31,024] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:42:31,024] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 450/1366]  eta: 0:09:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6052 (4.6128)  loss_scale: 65536.0000 (70040.6918)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1604  max mem: 15572
Epoch: [8]  [ 460/1366]  eta: 0:09:10  lr: 0.000046  min_lr: 0.000000  loss: 4.4674 (4.6117)  loss_scale: 65536.0000 (69942.9761)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1813  max mem: 15572
Epoch: [8]  [ 470/1366]  eta: 0:09:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5921 (4.6111)  loss_scale: 65536.0000 (69849.4098)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1326  max mem: 15572
Epoch: [8]  [ 480/1366]  eta: 0:08:56  lr: 0.000046  min_lr: 0.000000  loss: 4.5640 (4.6108)  loss_scale: 65536.0000 (69759.7339)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0958  max mem: 15572
Epoch: [8]  [ 490/1366]  eta: 0:08:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6963 (4.6133)  loss_scale: 65536.0000 (69673.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1070  max mem: 15572
Epoch: [8]  [ 500/1366]  eta: 0:08:44  lr: 0.000046  min_lr: 0.000000  loss: 4.7404 (4.6135)  loss_scale: 65536.0000 (69591.1218)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1796  max mem: 15572
Epoch: [8]  [ 510/1366]  eta: 0:08:38  lr: 0.000046  min_lr: 0.000000  loss: 4.7202 (4.6158)  loss_scale: 65536.0000 (69511.7652)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.1841  max mem: 15572
Epoch: [8]  [ 520/1366]  eta: 0:08:32  lr: 0.000046  min_lr: 0.000000  loss: 4.7088 (4.6163)  loss_scale: 65536.0000 (69435.4549)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1295  max mem: 15572
Epoch: [8]  [ 530/1366]  eta: 0:08:25  lr: 0.000046  min_lr: 0.000000  loss: 4.6032 (4.6177)  loss_scale: 65536.0000 (69362.0188)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0902  max mem: 15572
Epoch: [8]  [ 540/1366]  eta: 0:08:19  lr: 0.000046  min_lr: 0.000000  loss: 4.5349 (4.6125)  loss_scale: 65536.0000 (69291.2976)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1168  max mem: 15572
Epoch: [8]  [ 550/1366]  eta: 0:08:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4584 (4.6133)  loss_scale: 65536.0000 (69223.1434)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1442  max mem: 15572
Epoch: [8]  [ 560/1366]  eta: 0:08:08  lr: 0.000046  min_lr: 0.000000  loss: 4.7095 (4.6135)  loss_scale: 65536.0000 (69157.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1801  max mem: 15572
Epoch: [8]  [ 570/1366]  eta: 0:08:00  lr: 0.000046  min_lr: 0.000000  loss: 4.6378 (4.6142)  loss_scale: 65536.0000 (69093.9965)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1585  max mem: 15572
[2025-01-13 00:43:47,693] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:43:47,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 580/1366]  eta: 0:07:55  lr: 0.000046  min_lr: 0.000000  loss: 4.5710 (4.6160)  loss_scale: 65536.0000 (69371.1532)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1202  max mem: 15572
[2025-01-13 00:43:53,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19185
[2025-01-13 00:43:53,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:43:53,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 590/1366]  eta: 0:07:49  lr: 0.000046  min_lr: 0.000000  loss: 4.6586 (4.6157)  loss_scale: 65536.0000 (69971.6007)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1715  max mem: 15572
Epoch: [8]  [ 600/1366]  eta: 0:07:43  lr: 0.000046  min_lr: 0.000000  loss: 4.5903 (4.6148)  loss_scale: 65536.0000 (69897.7970)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1764  max mem: 15572
Epoch: [8]  [ 610/1366]  eta: 0:07:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5903 (4.6152)  loss_scale: 65536.0000 (69826.4092)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1071  max mem: 15572
Epoch: [8]  [ 620/1366]  eta: 0:07:30  lr: 0.000046  min_lr: 0.000000  loss: 4.7034 (4.6174)  loss_scale: 65536.0000 (69757.3205)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0976  max mem: 15572
Epoch: [8]  [ 630/1366]  eta: 0:07:24  lr: 0.000046  min_lr: 0.000000  loss: 4.7983 (4.6200)  loss_scale: 65536.0000 (69690.4216)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1776  max mem: 15572
Epoch: [8]  [ 640/1366]  eta: 0:07:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6611 (4.6195)  loss_scale: 65536.0000 (69625.6100)  weight_decay: 0.0500 (0.0500)  time: 0.7121  data: 0.2553  max mem: 15572
Epoch: [8]  [ 650/1366]  eta: 0:07:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6523 (4.6207)  loss_scale: 65536.0000 (69562.7896)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1543  max mem: 15572
Epoch: [8]  [ 660/1366]  eta: 0:07:06  lr: 0.000046  min_lr: 0.000000  loss: 4.6269 (4.6182)  loss_scale: 65536.0000 (69501.8699)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0542  max mem: 15572
Epoch: [8]  [ 670/1366]  eta: 0:07:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4612 (4.6170)  loss_scale: 65536.0000 (69442.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.1856  max mem: 15572
Epoch: [8]  [ 680/1366]  eta: 0:06:54  lr: 0.000046  min_lr: 0.000000  loss: 4.5544 (4.6173)  loss_scale: 65536.0000 (69385.3979)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1516  max mem: 15572
Epoch: [8]  [ 690/1366]  eta: 0:06:48  lr: 0.000046  min_lr: 0.000000  loss: 4.5544 (4.6163)  loss_scale: 65536.0000 (69329.6903)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0800  max mem: 15572
Epoch: [8]  [ 700/1366]  eta: 0:06:42  lr: 0.000046  min_lr: 0.000000  loss: 4.5705 (4.6174)  loss_scale: 65536.0000 (69275.5720)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1518  max mem: 15572
Epoch: [8]  [ 710/1366]  eta: 0:06:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5951 (4.6171)  loss_scale: 65536.0000 (69222.9761)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1762  max mem: 15572
[2025-01-13 00:45:11,475] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:45:11,475] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 720/1366]  eta: 0:06:30  lr: 0.000046  min_lr: 0.000000  loss: 4.5813 (4.6167)  loss_scale: 65536.0000 (69626.3190)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1679  max mem: 15572
[2025-01-13 00:45:15,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19322
[2025-01-13 00:45:15,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:45:15,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 730/1366]  eta: 0:06:24  lr: 0.000046  min_lr: 0.000000  loss: 4.5813 (4.6159)  loss_scale: 65536.0000 (69839.3215)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1719  max mem: 15572
Epoch: [8]  [ 740/1366]  eta: 0:06:17  lr: 0.000046  min_lr: 0.000000  loss: 4.5760 (4.6151)  loss_scale: 65536.0000 (69781.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1319  max mem: 15572
Epoch: [8]  [ 750/1366]  eta: 0:06:11  lr: 0.000046  min_lr: 0.000000  loss: 4.6351 (4.6159)  loss_scale: 65536.0000 (69724.7190)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1001  max mem: 15572
Epoch: [8]  [ 760/1366]  eta: 0:06:05  lr: 0.000046  min_lr: 0.000000  loss: 4.6069 (4.6146)  loss_scale: 65536.0000 (69669.6767)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1261  max mem: 15572
Epoch: [8]  [ 770/1366]  eta: 0:05:58  lr: 0.000046  min_lr: 0.000000  loss: 4.5879 (4.6145)  loss_scale: 65536.0000 (69616.0623)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0847  max mem: 15572
Epoch: [8]  [ 780/1366]  eta: 0:05:52  lr: 0.000046  min_lr: 0.000000  loss: 4.5202 (4.6131)  loss_scale: 65536.0000 (69563.8207)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0776  max mem: 15572
Epoch: [8]  [ 790/1366]  eta: 0:05:46  lr: 0.000046  min_lr: 0.000000  loss: 4.4595 (4.6122)  loss_scale: 65536.0000 (69512.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1733  max mem: 15572
Epoch: [8]  [ 800/1366]  eta: 0:05:40  lr: 0.000046  min_lr: 0.000000  loss: 4.4595 (4.6094)  loss_scale: 65536.0000 (69463.2509)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1825  max mem: 15572
Epoch: [8]  [ 810/1366]  eta: 0:05:34  lr: 0.000046  min_lr: 0.000000  loss: 4.4835 (4.6099)  loss_scale: 65536.0000 (69414.8261)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1317  max mem: 15572
Epoch: [8]  [ 820/1366]  eta: 0:05:28  lr: 0.000046  min_lr: 0.000000  loss: 4.7629 (4.6122)  loss_scale: 65536.0000 (69367.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1191  max mem: 15572
Epoch: [8]  [ 830/1366]  eta: 0:05:22  lr: 0.000046  min_lr: 0.000000  loss: 4.7504 (4.6131)  loss_scale: 65536.0000 (69321.4729)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1048  max mem: 15572
Epoch: [8]  [ 840/1366]  eta: 0:05:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6123 (4.6126)  loss_scale: 65536.0000 (69276.4614)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1272  max mem: 15572
Epoch: [8]  [ 850/1366]  eta: 0:05:09  lr: 0.000046  min_lr: 0.000000  loss: 4.7716 (4.6140)  loss_scale: 65536.0000 (69232.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1423  max mem: 15572
[2025-01-13 00:46:30,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:46:30,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:46:34,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19458
[2025-01-13 00:46:34,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:46:34,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 860/1366]  eta: 0:05:03  lr: 0.000046  min_lr: 0.000000  loss: 4.7716 (4.6148)  loss_scale: 65536.0000 (69722.3879)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1118  max mem: 15572
Epoch: [8]  [ 870/1366]  eta: 0:04:57  lr: 0.000046  min_lr: 0.000000  loss: 4.6898 (4.6133)  loss_scale: 65536.0000 (69674.3238)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1161  max mem: 15572
Epoch: [8]  [ 880/1366]  eta: 0:04:51  lr: 0.000046  min_lr: 0.000000  loss: 4.7176 (4.6145)  loss_scale: 65536.0000 (69627.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1503  max mem: 15572
Epoch: [8]  [ 890/1366]  eta: 0:04:45  lr: 0.000046  min_lr: 0.000000  loss: 4.6635 (4.6144)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1410  max mem: 15572
Epoch: [8]  [ 900/1366]  eta: 0:04:39  lr: 0.000046  min_lr: 0.000000  loss: 4.6126 (4.6151)  loss_scale: 65536.0000 (69536.5327)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1497  max mem: 15572
Epoch: [8]  [ 910/1366]  eta: 0:04:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6317 (4.6142)  loss_scale: 65536.0000 (69492.6191)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0944  max mem: 15572
Epoch: [8]  [ 920/1366]  eta: 0:04:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6565 (4.6143)  loss_scale: 65536.0000 (69449.6591)  weight_decay: 0.0500 (0.0500)  time: 0.4889  data: 0.0269  max mem: 15572
Epoch: [8]  [ 930/1366]  eta: 0:04:20  lr: 0.000046  min_lr: 0.000000  loss: 4.6170 (4.6134)  loss_scale: 65536.0000 (69407.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0884  max mem: 15572
Epoch: [8]  [ 940/1366]  eta: 0:04:14  lr: 0.000046  min_lr: 0.000000  loss: 4.6063 (4.6138)  loss_scale: 65536.0000 (69366.4782)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0625  max mem: 15572
Epoch: [8]  [ 950/1366]  eta: 0:04:07  lr: 0.000046  min_lr: 0.000000  loss: 4.6714 (4.6138)  loss_scale: 65536.0000 (69326.1998)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0188  max mem: 15572
Epoch: [8]  [ 960/1366]  eta: 0:04:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5544 (4.6130)  loss_scale: 65536.0000 (69286.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1092  max mem: 15572
Epoch: [8]  [ 970/1366]  eta: 0:03:55  lr: 0.000046  min_lr: 0.000000  loss: 4.6104 (4.6135)  loss_scale: 65536.0000 (69248.1318)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1801  max mem: 15572
Epoch: [8]  [ 980/1366]  eta: 0:03:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6345 (4.6137)  loss_scale: 65536.0000 (69210.2915)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1701  max mem: 15572
[2025-01-13 00:47:47,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:47:47,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:47:47,783] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19588
[2025-01-13 00:47:47,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:47:47,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 990/1366]  eta: 0:03:43  lr: 0.000046  min_lr: 0.000000  loss: 4.6330 (4.6134)  loss_scale: 65536.0000 (69239.3461)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1179  max mem: 15572
Epoch: [8]  [1000/1366]  eta: 0:03:37  lr: 0.000046  min_lr: 0.000000  loss: 4.6258 (4.6138)  loss_scale: 65536.0000 (69202.3497)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1159  max mem: 15572
Epoch: [8]  [1010/1366]  eta: 0:03:31  lr: 0.000046  min_lr: 0.000000  loss: 4.6445 (4.6142)  loss_scale: 65536.0000 (69166.0851)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0791  max mem: 15572
Epoch: [8]  [1020/1366]  eta: 0:03:24  lr: 0.000046  min_lr: 0.000000  loss: 4.6751 (4.6148)  loss_scale: 65536.0000 (69130.5309)  weight_decay: 0.0500 (0.0500)  time: 0.4125  data: 0.0006  max mem: 15572
Epoch: [8]  [1030/1366]  eta: 0:03:18  lr: 0.000046  min_lr: 0.000000  loss: 4.5868 (4.6142)  loss_scale: 65536.0000 (69095.6663)  weight_decay: 0.0500 (0.0500)  time: 0.4375  data: 0.0007  max mem: 15572
Epoch: [8]  [1040/1366]  eta: 0:03:12  lr: 0.000046  min_lr: 0.000000  loss: 4.6052 (4.6139)  loss_scale: 65536.0000 (69061.4717)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0935  max mem: 15572
Epoch: [8]  [1050/1366]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 4.5769 (4.6133)  loss_scale: 65536.0000 (69027.9277)  weight_decay: 0.0500 (0.0500)  time: 0.7084  data: 0.2268  max mem: 15572
Epoch: [8]  [1060/1366]  eta: 0:03:01  lr: 0.000046  min_lr: 0.000000  loss: 4.5200 (4.6126)  loss_scale: 65536.0000 (68995.0160)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.2517  max mem: 15572
Epoch: [8]  [1070/1366]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000000  loss: 4.5555 (4.6124)  loss_scale: 65536.0000 (68962.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.2001  max mem: 15572
Epoch: [8]  [1080/1366]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000000  loss: 4.5649 (4.6120)  loss_scale: 65536.0000 (68931.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1579  max mem: 15572
Epoch: [8]  [1090/1366]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6807 (4.6128)  loss_scale: 65536.0000 (68899.9010)  weight_decay: 0.0500 (0.0500)  time: 0.7222  data: 0.2577  max mem: 15572
Epoch: [8]  [1100/1366]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000000  loss: 4.6807 (4.6132)  loss_scale: 65536.0000 (68869.3479)  weight_decay: 0.0500 (0.0500)  time: 0.7507  data: 0.2961  max mem: 15572
Epoch: [8]  [1110/1366]  eta: 0:02:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6297 (4.6133)  loss_scale: 65536.0000 (68839.3447)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.2011  max mem: 15572
[2025-01-13 00:49:08,771] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:49:08,772] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:49:09,212] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19718
[2025-01-13 00:49:09,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:49:09,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1120/1366]  eta: 0:02:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6120 (4.6133)  loss_scale: 65536.0000 (68868.3390)  weight_decay: 0.0500 (0.0500)  time: 0.6973  data: 0.2213  max mem: 15572
Epoch: [8]  [1130/1366]  eta: 0:02:21  lr: 0.000046  min_lr: 0.000000  loss: 4.6406 (4.6143)  loss_scale: 65536.0000 (68838.8753)  weight_decay: 0.0500 (0.0500)  time: 0.7715  data: 0.3045  max mem: 15572
Epoch: [8]  [1140/1366]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.6547 (4.6143)  loss_scale: 65536.0000 (68809.9281)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.2999  max mem: 15572
Epoch: [8]  [1150/1366]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5528 (4.6140)  loss_scale: 65536.0000 (68781.4839)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.2110  max mem: 15572
Epoch: [8]  [1160/1366]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000000  loss: 4.5454 (4.6131)  loss_scale: 65536.0000 (68753.5297)  weight_decay: 0.0500 (0.0500)  time: 0.4879  data: 0.0814  max mem: 15572
Epoch: [8]  [1170/1366]  eta: 0:01:57  lr: 0.000045  min_lr: 0.000000  loss: 4.5636 (4.6139)  loss_scale: 65536.0000 (68726.0529)  weight_decay: 0.0500 (0.0500)  time: 0.4217  data: 0.0005  max mem: 15572
Epoch: [8]  [1180/1366]  eta: 0:01:51  lr: 0.000045  min_lr: 0.000000  loss: 4.6864 (4.6141)  loss_scale: 65536.0000 (68699.0415)  weight_decay: 0.0500 (0.0500)  time: 0.4477  data: 0.0007  max mem: 15572
Epoch: [8]  [1190/1366]  eta: 0:01:44  lr: 0.000045  min_lr: 0.000000  loss: 4.5299 (4.6130)  loss_scale: 65536.0000 (68672.4836)  weight_decay: 0.0500 (0.0500)  time: 0.4520  data: 0.0008  max mem: 15572
Epoch: [8]  [1200/1366]  eta: 0:01:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4738 (4.6129)  loss_scale: 65536.0000 (68646.3680)  weight_decay: 0.0500 (0.0500)  time: 0.4909  data: 0.0479  max mem: 15572
Epoch: [8]  [1210/1366]  eta: 0:01:32  lr: 0.000045  min_lr: 0.000000  loss: 4.5976 (4.6122)  loss_scale: 65536.0000 (68620.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1448  max mem: 15572
Epoch: [8]  [1220/1366]  eta: 0:01:26  lr: 0.000045  min_lr: 0.000000  loss: 4.5971 (4.6119)  loss_scale: 65536.0000 (68595.4201)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1774  max mem: 15572
Epoch: [8]  [1230/1366]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000000  loss: 4.5971 (4.6119)  loss_scale: 65536.0000 (68570.5670)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1554  max mem: 15572
Epoch: [8]  [1240/1366]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000000  loss: 4.6202 (4.6123)  loss_scale: 65536.0000 (68546.1144)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1807  max mem: 15572
[2025-01-13 00:50:23,009] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:50:23,009] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1250/1366]  eta: 0:01:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5819 (4.6120)  loss_scale: 65536.0000 (68626.8265)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1441  max mem: 15572
[2025-01-13 00:50:24,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19850
[2025-01-13 00:50:24,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:50:24,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1260/1366]  eta: 0:01:03  lr: 0.000045  min_lr: 0.000000  loss: 4.5827 (4.6114)  loss_scale: 65536.0000 (68654.2871)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1243  max mem: 15572
Epoch: [8]  [1270/1366]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000000  loss: 4.5904 (4.6112)  loss_scale: 65536.0000 (68629.7530)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1719  max mem: 15572
Epoch: [8]  [1280/1366]  eta: 0:00:51  lr: 0.000045  min_lr: 0.000000  loss: 4.5418 (4.6108)  loss_scale: 65536.0000 (68605.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1390  max mem: 15572
Epoch: [8]  [1290/1366]  eta: 0:00:45  lr: 0.000045  min_lr: 0.000000  loss: 4.7108 (4.6118)  loss_scale: 65536.0000 (68581.8249)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1443  max mem: 15572
Epoch: [8]  [1300/1366]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000000  loss: 4.7108 (4.6124)  loss_scale: 65536.0000 (68558.4135)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1252  max mem: 15572
Epoch: [8]  [1310/1366]  eta: 0:00:33  lr: 0.000045  min_lr: 0.000000  loss: 4.5836 (4.6107)  loss_scale: 65536.0000 (68535.3593)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1259  max mem: 15572
Epoch: [8]  [1320/1366]  eta: 0:00:27  lr: 0.000045  min_lr: 0.000000  loss: 4.4932 (4.6111)  loss_scale: 65536.0000 (68512.6540)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1754  max mem: 15572
[2025-01-13 00:51:09,953] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19926
[2025-01-13 00:51:09,953] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:51:09,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1330/1366]  eta: 0:00:21  lr: 0.000045  min_lr: 0.000000  loss: 4.6650 (4.6113)  loss_scale: 65536.0000 (68416.4328)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1615  max mem: 15572
Epoch: [8]  [1340/1366]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000000  loss: 4.6667 (4.6113)  loss_scale: 32768.0000 (68150.5981)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1058  max mem: 15572
Epoch: [8]  [1350/1366]  eta: 0:00:09  lr: 0.000045  min_lr: 0.000000  loss: 4.6045 (4.6106)  loss_scale: 32768.0000 (67888.6987)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0801  max mem: 15572
Epoch: [8]  [1360/1366]  eta: 0:00:03  lr: 0.000045  min_lr: 0.000000  loss: 4.5669 (4.6106)  loss_scale: 32768.0000 (67630.6481)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.1000  max mem: 15572
Epoch: [8]  [1365/1366]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5723 (4.6102)  loss_scale: 32768.0000 (67503.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0999  max mem: 15572
Epoch: [8] Total time: 0:13:31 (0.5943 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.5723 (4.6102)  loss_scale: 32768.0000 (67503.0395)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 2170
Indices to remove: tensor([    0,    28,    39,  ..., 33684, 33700, 33705], device='cuda:0')
length of data loader train is: 1185
num_training_steps_per_epoch is: 1185
Change step level LR scheduler!
Set warmup steps = 5925
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:21:51  loss: 1.4945 (1.4945)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 4.8207  data: 4.5532  max mem: 15572
Val:  [ 10/272]  eta: 0:03:54  loss: 3.8687 (3.7086)  acc1: 11.1111 (19.1919)  acc5: 33.3333 (35.8586)  time: 0.8935  data: 0.6986  max mem: 15572
Val:  [ 20/272]  eta: 0:02:39  loss: 3.8635 (3.7712)  acc1: 16.6667 (19.0476)  acc5: 38.8889 (39.9471)  time: 0.4228  data: 0.2272  max mem: 15572
Val:  [ 30/272]  eta: 0:01:58  loss: 3.8308 (3.8008)  acc1: 11.1111 (15.7706)  acc5: 44.4444 (39.9642)  time: 0.2662  data: 0.0709  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.4814 (3.6800)  acc1: 11.1111 (17.2087)  acc5: 55.5556 (45.6640)  time: 0.1978  data: 0.0099  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 3.3187 (3.6202)  acc1: 22.2222 (19.2810)  acc5: 55.5556 (48.1481)  time: 0.2872  data: 0.0912  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 3.0851 (3.5380)  acc1: 33.3333 (22.6776)  acc5: 72.2222 (51.1840)  time: 0.3566  data: 0.1564  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 2.9764 (3.4601)  acc1: 38.8889 (26.3693)  acc5: 77.7778 (55.0861)  time: 0.3190  data: 0.1094  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.0720 (3.4605)  acc1: 33.3333 (26.6804)  acc5: 72.2222 (55.4870)  time: 0.3052  data: 0.0716  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.6156 (3.4822)  acc1: 22.2222 (26.0684)  acc5: 50.0000 (54.7009)  time: 0.3269  data: 0.0961  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.6722 (3.5289)  acc1: 22.2222 (25.5776)  acc5: 44.4444 (53.9054)  time: 0.3511  data: 0.1436  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 3.8872 (3.5704)  acc1: 5.5556 (23.8739)  acc5: 44.4444 (52.4525)  time: 0.3827  data: 0.1864  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.8697 (3.5930)  acc1: 5.5556 (23.3242)  acc5: 44.4444 (52.2039)  time: 0.3482  data: 0.1520  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.6708 (3.5803)  acc1: 16.6667 (23.1976)  acc5: 44.4444 (52.2901)  time: 0.2670  data: 0.0753  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 3.3844 (3.5724)  acc1: 22.2222 (23.4043)  acc5: 55.5556 (52.7187)  time: 0.2852  data: 0.0913  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.3687 (3.5557)  acc1: 22.2222 (23.2892)  acc5: 55.5556 (53.2745)  time: 0.2842  data: 0.0718  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.3895 (3.5548)  acc1: 16.6667 (22.9814)  acc5: 61.1111 (53.5197)  time: 0.2814  data: 0.0761  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.5507 (3.5710)  acc1: 11.1111 (22.3522)  acc5: 50.0000 (53.0214)  time: 0.3305  data: 0.1313  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.5082 (3.5404)  acc1: 22.2222 (23.6341)  acc5: 55.5556 (54.3278)  time: 0.3491  data: 0.1458  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.3657 (3.5612)  acc1: 5.5556 (22.6294)  acc5: 55.5556 (53.0541)  time: 0.3284  data: 0.1195  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.6191 (3.5702)  acc1: 5.5556 (22.5263)  acc5: 44.4444 (52.9298)  time: 0.3057  data: 0.0964  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.5419 (3.5801)  acc1: 22.2222 (22.9068)  acc5: 55.5556 (52.8963)  time: 0.3348  data: 0.1345  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5991 (3.5784)  acc1: 22.2222 (23.0769)  acc5: 50.0000 (53.0417)  time: 0.3246  data: 0.1273  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.4286 (3.5778)  acc1: 16.6667 (23.2083)  acc5: 61.1111 (53.3911)  time: 0.3444  data: 0.1389  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.2992 (3.5610)  acc1: 33.3333 (23.8128)  acc5: 66.6667 (54.3568)  time: 0.4027  data: 0.1899  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2992 (3.5707)  acc1: 22.2222 (23.6830)  acc5: 66.6667 (53.8070)  time: 0.3772  data: 0.1770  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.2500 (3.5425)  acc1: 33.3333 (24.9255)  acc5: 72.2222 (55.0660)  time: 0.3019  data: 0.1081  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2500 (3.5459)  acc1: 27.7778 (24.5592)  acc5: 72.2222 (55.0431)  time: 0.2347  data: 0.0573  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2500 (3.5488)  acc1: 22.2222 (24.5341)  acc5: 66.6667 (55.0276)  time: 0.1928  data: 0.0207  max mem: 15572
Val: Total time: 0:01:31 (0.3380 s / it)
* Acc@1 24.534 Acc@5 55.028 loss 3.549
Accuracy of the network on the 4883 val videos: 24.5%
[2025-01-13 00:53:01,417] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 00:53:01,421] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 00:53:01,421] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 00:53:04,235] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 00:53:04,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.53%
Epoch: [9]  [   0/1185]  eta: 2:45:40  lr: 0.000045  min_lr: 0.000000  loss: 4.6703 (4.6703)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3883  data: 7.9281  max mem: 15572
Epoch: [9]  [  10/1185]  eta: 0:23:29  lr: 0.000045  min_lr: 0.000000  loss: 4.6908 (4.6441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1997  data: 0.7419  max mem: 15572
Epoch: [9]  [  20/1185]  eta: 0:18:04  lr: 0.000045  min_lr: 0.000000  loss: 4.6484 (4.6214)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0910  max mem: 15572
Epoch: [9]  [  30/1185]  eta: 0:16:05  lr: 0.000045  min_lr: 0.000000  loss: 4.5166 (4.5957)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1700  max mem: 15572
[2025-01-13 00:53:32,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=128, lr=[4.394896276098511e-07, 4.394896276098511e-07, 6.278423251569302e-07, 6.278423251569302e-07, 8.969176073670433e-07, 8.969176073670433e-07, 1.2813108676672047e-06, 1.2813108676672047e-06, 1.8304440966674354e-06, 1.8304440966674354e-06, 2.614920138096336e-06, 2.614920138096336e-06, 3.7356001972804806e-06, 3.7356001972804806e-06, 5.336571710400687e-06, 5.336571710400687e-06, 7.623673872000981e-06, 7.623673872000981e-06, 1.0890962674287119e-05, 1.0890962674287119e-05, 1.5558518106124456e-05, 1.5558518106124456e-05, 2.222645443732065e-05, 2.222645443732065e-05, 3.1752077767600936e-05, 3.1752077767600936e-05, 4.5360111096572764e-05, 4.5360111096572764e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 00:53:32,962] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=27.836812731479053, CurrSamplesPerSec=24.570541212767672, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [  40/1185]  eta: 0:14:22  lr: 0.000045  min_lr: 0.000000  loss: 4.5325 (4.6167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1128  max mem: 15572
Epoch: [9]  [  50/1185]  eta: 0:13:42  lr: 0.000045  min_lr: 0.000000  loss: 4.5415 (4.5998)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0974  max mem: 15572
Epoch: [9]  [  60/1185]  eta: 0:12:49  lr: 0.000045  min_lr: 0.000000  loss: 4.5144 (4.5983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0840  max mem: 15572
Epoch: [9]  [  70/1185]  eta: 0:12:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6584 (4.6063)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1191  max mem: 15572
Epoch: [9]  [  80/1185]  eta: 0:12:13  lr: 0.000045  min_lr: 0.000000  loss: 4.6568 (4.6125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1413  max mem: 15572
Epoch: [9]  [  90/1185]  eta: 0:12:10  lr: 0.000045  min_lr: 0.000000  loss: 4.6105 (4.6175)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1491  max mem: 15572
[2025-01-13 00:54:05,432] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:54:05,433] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 100/1185]  eta: 0:11:55  lr: 0.000045  min_lr: 0.000000  loss: 4.6218 (4.6183)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1781  max mem: 15572
Epoch: [9]  [ 110/1185]  eta: 0:11:36  lr: 0.000045  min_lr: 0.000000  loss: 4.6218 (4.6225)  loss_scale: 65536.0000 (38672.1441)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1006  max mem: 15572
Epoch: [9]  [ 120/1185]  eta: 0:11:16  lr: 0.000045  min_lr: 0.000000  loss: 4.5406 (4.6186)  loss_scale: 65536.0000 (40892.2975)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0699  max mem: 15572
Epoch: [9]  [ 130/1185]  eta: 0:11:07  lr: 0.000045  min_lr: 0.000000  loss: 4.6192 (4.6240)  loss_scale: 65536.0000 (42773.4962)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1031  max mem: 15572
Epoch: [9]  [ 140/1185]  eta: 0:10:52  lr: 0.000045  min_lr: 0.000000  loss: 4.7046 (4.6194)  loss_scale: 65536.0000 (44387.8582)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1083  max mem: 15572
Epoch: [9]  [ 150/1185]  eta: 0:10:50  lr: 0.000045  min_lr: 0.000000  loss: 4.4547 (4.6092)  loss_scale: 65536.0000 (45788.3974)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1422  max mem: 15572
Epoch: [9]  [ 160/1185]  eta: 0:10:42  lr: 0.000045  min_lr: 0.000000  loss: 4.5566 (4.6093)  loss_scale: 65536.0000 (47014.9565)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1737  max mem: 15572
Epoch: [9]  [ 170/1185]  eta: 0:10:43  lr: 0.000045  min_lr: 0.000000  loss: 4.5985 (4.6026)  loss_scale: 65536.0000 (48098.0585)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.2031  max mem: 15572
Epoch: [9]  [ 180/1185]  eta: 0:10:30  lr: 0.000045  min_lr: 0.000000  loss: 4.4955 (4.5970)  loss_scale: 65536.0000 (49061.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1694  max mem: 15572
Epoch: [9]  [ 190/1185]  eta: 0:10:21  lr: 0.000045  min_lr: 0.000000  loss: 4.5567 (4.5995)  loss_scale: 65536.0000 (49924.0209)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0909  max mem: 15572
Epoch: [9]  [ 200/1185]  eta: 0:10:14  lr: 0.000045  min_lr: 0.000000  loss: 4.6227 (4.6035)  loss_scale: 65536.0000 (50700.7363)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1252  max mem: 15572
Epoch: [9]  [ 210/1185]  eta: 0:10:05  lr: 0.000045  min_lr: 0.000000  loss: 4.6085 (4.6022)  loss_scale: 65536.0000 (51403.8294)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1026  max mem: 15572
[2025-01-13 00:55:19,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:55:19,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 220/1185]  eta: 0:10:06  lr: 0.000045  min_lr: 0.000000  loss: 4.5323 (4.5975)  loss_scale: 65536.0000 (52636.3801)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.2033  max mem: 15572
[2025-01-13 00:55:24,032] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20186
[2025-01-13 00:55:24,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:55:24,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 230/1185]  eta: 0:09:57  lr: 0.000045  min_lr: 0.000000  loss: 4.4682 (4.5989)  loss_scale: 65536.0000 (53478.5108)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.2062  max mem: 15572
Epoch: [9]  [ 240/1185]  eta: 0:09:49  lr: 0.000045  min_lr: 0.000000  loss: 4.6769 (4.6005)  loss_scale: 65536.0000 (53978.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.0940  max mem: 15572
Epoch: [9]  [ 250/1185]  eta: 0:09:44  lr: 0.000045  min_lr: 0.000000  loss: 4.6769 (4.5993)  loss_scale: 65536.0000 (54439.2669)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1458  max mem: 15572
Epoch: [9]  [ 260/1185]  eta: 0:09:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5832 (4.5981)  loss_scale: 65536.0000 (54864.4291)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1413  max mem: 15572
Epoch: [9]  [ 270/1185]  eta: 0:09:29  lr: 0.000045  min_lr: 0.000000  loss: 4.5621 (4.5957)  loss_scale: 65536.0000 (55258.2140)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1409  max mem: 15572
Epoch: [9]  [ 280/1185]  eta: 0:09:22  lr: 0.000045  min_lr: 0.000000  loss: 4.6871 (4.6018)  loss_scale: 65536.0000 (55623.9715)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1687  max mem: 15572
Epoch: [9]  [ 290/1185]  eta: 0:09:13  lr: 0.000045  min_lr: 0.000000  loss: 4.5940 (4.5964)  loss_scale: 65536.0000 (55964.5911)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1123  max mem: 15572
Epoch: [9]  [ 300/1185]  eta: 0:09:05  lr: 0.000045  min_lr: 0.000000  loss: 4.4926 (4.5964)  loss_scale: 65536.0000 (56282.5781)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0866  max mem: 15572
Epoch: [9]  [ 310/1185]  eta: 0:08:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5740 (4.5965)  loss_scale: 65536.0000 (56580.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0740  max mem: 15572
Epoch: [9]  [ 320/1185]  eta: 0:08:47  lr: 0.000045  min_lr: 0.000000  loss: 4.6618 (4.6003)  loss_scale: 65536.0000 (56859.1153)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0842  max mem: 15572
Epoch: [9]  [ 330/1185]  eta: 0:08:39  lr: 0.000045  min_lr: 0.000000  loss: 4.7304 (4.6025)  loss_scale: 65536.0000 (57121.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0966  max mem: 15572
Epoch: [9]  [ 340/1185]  eta: 0:08:35  lr: 0.000045  min_lr: 0.000000  loss: 4.6950 (4.6042)  loss_scale: 65536.0000 (57368.0235)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1431  max mem: 15572
Epoch: [9]  [ 350/1185]  eta: 0:08:30  lr: 0.000045  min_lr: 0.000000  loss: 4.6950 (4.6056)  loss_scale: 65536.0000 (57600.7293)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.2048  max mem: 15572
[2025-01-13 00:56:39,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:56:39,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 00:56:40,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20318
[2025-01-13 00:56:40,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:56:40,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 360/1185]  eta: 0:08:25  lr: 0.000045  min_lr: 0.000000  loss: 4.5967 (4.6049)  loss_scale: 65536.0000 (58365.1634)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.2057  max mem: 15572
Epoch: [9]  [ 370/1185]  eta: 0:08:15  lr: 0.000045  min_lr: 0.000000  loss: 4.5687 (4.6039)  loss_scale: 65536.0000 (58558.4474)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1108  max mem: 15572
Epoch: [9]  [ 380/1185]  eta: 0:08:06  lr: 0.000045  min_lr: 0.000000  loss: 4.5687 (4.6042)  loss_scale: 65536.0000 (58741.5853)  weight_decay: 0.0500 (0.0500)  time: 0.4615  data: 0.0147  max mem: 15572
Epoch: [9]  [ 390/1185]  eta: 0:08:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5040 (4.6013)  loss_scale: 65536.0000 (58915.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0879  max mem: 15572
Epoch: [9]  [ 400/1185]  eta: 0:07:51  lr: 0.000045  min_lr: 0.000000  loss: 4.5172 (4.6003)  loss_scale: 65536.0000 (59080.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0740  max mem: 15572
Epoch: [9]  [ 410/1185]  eta: 0:07:43  lr: 0.000045  min_lr: 0.000000  loss: 4.6473 (4.6044)  loss_scale: 65536.0000 (59237.5280)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0050  max mem: 15572
Epoch: [9]  [ 420/1185]  eta: 0:07:35  lr: 0.000045  min_lr: 0.000000  loss: 4.7259 (4.6068)  loss_scale: 65536.0000 (59387.1354)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0413  max mem: 15572
Epoch: [9]  [ 430/1185]  eta: 0:07:28  lr: 0.000045  min_lr: 0.000000  loss: 4.6619 (4.6067)  loss_scale: 65536.0000 (59529.8005)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0705  max mem: 15572
Epoch: [9]  [ 440/1185]  eta: 0:07:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5122 (4.6059)  loss_scale: 65536.0000 (59665.9955)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1235  max mem: 15572
Epoch: [9]  [ 450/1185]  eta: 0:07:17  lr: 0.000045  min_lr: 0.000000  loss: 4.5122 (4.6038)  loss_scale: 65536.0000 (59796.1508)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1646  max mem: 15572
Epoch: [9]  [ 460/1185]  eta: 0:07:10  lr: 0.000045  min_lr: 0.000000  loss: 4.5673 (4.6053)  loss_scale: 65536.0000 (59920.6594)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1092  max mem: 15572
Epoch: [9]  [ 470/1185]  eta: 0:07:04  lr: 0.000045  min_lr: 0.000000  loss: 4.6153 (4.6053)  loss_scale: 65536.0000 (60039.8811)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0957  max mem: 15572
Epoch: [9]  [ 480/1185]  eta: 0:06:59  lr: 0.000045  min_lr: 0.000000  loss: 4.5496 (4.6039)  loss_scale: 65536.0000 (60154.1455)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1567  max mem: 15572
[2025-01-13 00:57:51,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:57:51,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 490/1185]  eta: 0:06:53  lr: 0.000045  min_lr: 0.000000  loss: 4.5206 (4.6033)  loss_scale: 65536.0000 (61331.5519)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1768  max mem: 15572
[2025-01-13 00:57:57,909] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20457
[2025-01-13 00:57:57,910] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 00:57:57,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 500/1185]  eta: 0:06:49  lr: 0.000045  min_lr: 0.000000  loss: 4.5930 (4.6031)  loss_scale: 65536.0000 (61677.0938)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.2219  max mem: 15572
Epoch: [9]  [ 510/1185]  eta: 0:06:42  lr: 0.000045  min_lr: 0.000000  loss: 4.5397 (4.6018)  loss_scale: 65536.0000 (61752.6106)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.2139  max mem: 15572
Epoch: [9]  [ 520/1185]  eta: 0:06:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5397 (4.6023)  loss_scale: 65536.0000 (61825.2284)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0959  max mem: 15572
Epoch: [9]  [ 530/1185]  eta: 0:06:28  lr: 0.000045  min_lr: 0.000000  loss: 4.5695 (4.5991)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0439  max mem: 15572
Epoch: [9]  [ 540/1185]  eta: 0:06:23  lr: 0.000045  min_lr: 0.000000  loss: 4.4659 (4.5985)  loss_scale: 65536.0000 (61962.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1038  max mem: 15572
Epoch: [9]  [ 550/1185]  eta: 0:06:16  lr: 0.000045  min_lr: 0.000000  loss: 4.6194 (4.5985)  loss_scale: 65536.0000 (62027.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1309  max mem: 15572
[2025-01-13 00:58:34,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20518
[2025-01-13 00:58:34,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 00:58:34,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 560/1185]  eta: 0:06:10  lr: 0.000045  min_lr: 0.000000  loss: 4.5854 (4.5976)  loss_scale: 65536.0000 (61680.9412)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1233  max mem: 15572
Epoch: [9]  [ 570/1185]  eta: 0:06:04  lr: 0.000045  min_lr: 0.000000  loss: 4.5854 (4.5980)  loss_scale: 32768.0000 (61174.5849)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1439  max mem: 15572
Epoch: [9]  [ 580/1185]  eta: 0:05:58  lr: 0.000045  min_lr: 0.000000  loss: 4.6423 (4.6001)  loss_scale: 32768.0000 (60685.6592)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1391  max mem: 15572
Epoch: [9]  [ 590/1185]  eta: 0:05:51  lr: 0.000045  min_lr: 0.000000  loss: 4.6296 (4.5981)  loss_scale: 32768.0000 (60213.2792)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0915  max mem: 15572
Epoch: [9]  [ 600/1185]  eta: 0:05:45  lr: 0.000045  min_lr: 0.000000  loss: 4.5111 (4.5985)  loss_scale: 32768.0000 (59756.6190)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0823  max mem: 15572
Epoch: [9]  [ 610/1185]  eta: 0:05:39  lr: 0.000045  min_lr: 0.000000  loss: 4.6042 (4.5991)  loss_scale: 32768.0000 (59314.9067)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1272  max mem: 15572
Epoch: [9]  [ 620/1185]  eta: 0:05:33  lr: 0.000045  min_lr: 0.000000  loss: 4.6267 (4.5999)  loss_scale: 32768.0000 (58887.4203)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1000  max mem: 15572
Epoch: [9]  [ 630/1185]  eta: 0:05:27  lr: 0.000045  min_lr: 0.000000  loss: 4.6049 (4.5996)  loss_scale: 32768.0000 (58473.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1082  max mem: 15572
Epoch: [9]  [ 640/1185]  eta: 0:05:20  lr: 0.000045  min_lr: 0.000000  loss: 4.6049 (4.6008)  loss_scale: 32768.0000 (58072.4618)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1223  max mem: 15572
Epoch: [9]  [ 650/1185]  eta: 0:05:14  lr: 0.000045  min_lr: 0.000000  loss: 4.7034 (4.6017)  loss_scale: 32768.0000 (57683.7604)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1113  max mem: 15572
Epoch: [9]  [ 660/1185]  eta: 0:05:08  lr: 0.000045  min_lr: 0.000000  loss: 4.7322 (4.6038)  loss_scale: 32768.0000 (57306.8200)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0953  max mem: 15572
Epoch: [9]  [ 670/1185]  eta: 0:05:02  lr: 0.000045  min_lr: 0.000000  loss: 4.6347 (4.6043)  loss_scale: 32768.0000 (56941.1148)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0736  max mem: 15572
Epoch: [9]  [ 680/1185]  eta: 0:04:56  lr: 0.000045  min_lr: 0.000000  loss: 4.5810 (4.6028)  loss_scale: 32768.0000 (56586.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1159  max mem: 15572
[2025-01-13 00:59:46,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 00:59:46,580] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 690/1185]  eta: 0:04:50  lr: 0.000045  min_lr: 0.000000  loss: 4.6520 (4.6049)  loss_scale: 32768.0000 (56620.8278)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1256  max mem: 15572
Epoch: [9]  [ 700/1185]  eta: 0:04:44  lr: 0.000045  min_lr: 0.000000  loss: 4.6587 (4.6041)  loss_scale: 65536.0000 (56748.0057)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1206  max mem: 15572
Epoch: [9]  [ 710/1185]  eta: 0:04:39  lr: 0.000045  min_lr: 0.000000  loss: 4.5808 (4.6028)  loss_scale: 65536.0000 (56871.6062)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2347  max mem: 15572
Epoch: [9]  [ 720/1185]  eta: 0:04:33  lr: 0.000045  min_lr: 0.000000  loss: 4.5671 (4.6023)  loss_scale: 65536.0000 (56991.7781)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.2117  max mem: 15572
Epoch: [9]  [ 730/1185]  eta: 0:04:27  lr: 0.000045  min_lr: 0.000000  loss: 4.6096 (4.6031)  loss_scale: 65536.0000 (57108.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1291  max mem: 15572
Epoch: [9]  [ 740/1185]  eta: 0:04:21  lr: 0.000045  min_lr: 0.000000  loss: 4.6176 (4.6040)  loss_scale: 65536.0000 (57222.3914)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1379  max mem: 15572
Epoch: [9]  [ 750/1185]  eta: 0:04:15  lr: 0.000045  min_lr: 0.000000  loss: 4.5875 (4.6029)  loss_scale: 65536.0000 (57333.0919)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1096  max mem: 15572
Epoch: [9]  [ 760/1185]  eta: 0:04:09  lr: 0.000045  min_lr: 0.000000  loss: 4.6312 (4.6049)  loss_scale: 65536.0000 (57440.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.0776  max mem: 15572
Epoch: [9]  [ 770/1185]  eta: 0:04:03  lr: 0.000045  min_lr: 0.000000  loss: 4.7221 (4.6047)  loss_scale: 65536.0000 (57545.8781)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0406  max mem: 15572
Epoch: [9]  [ 780/1185]  eta: 0:03:57  lr: 0.000045  min_lr: 0.000000  loss: 4.6753 (4.6046)  loss_scale: 65536.0000 (57648.1844)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1156  max mem: 15572
Epoch: [9]  [ 790/1185]  eta: 0:03:51  lr: 0.000045  min_lr: 0.000000  loss: 4.5789 (4.6033)  loss_scale: 65536.0000 (57747.9039)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1041  max mem: 15572
Epoch: [9]  [ 800/1185]  eta: 0:03:45  lr: 0.000045  min_lr: 0.000000  loss: 4.6461 (4.6039)  loss_scale: 65536.0000 (57845.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0721  max mem: 15572
Epoch: [9]  [ 810/1185]  eta: 0:03:39  lr: 0.000045  min_lr: 0.000000  loss: 4.6540 (4.6037)  loss_scale: 65536.0000 (57939.9655)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0875  max mem: 15572
[2025-01-13 01:01:00,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:01:00,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:01:00,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20777
[2025-01-13 01:01:00,958] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:01:00,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 820/1185]  eta: 0:03:33  lr: 0.000045  min_lr: 0.000000  loss: 4.6069 (4.6054)  loss_scale: 65536.0000 (58192.1364)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0734  max mem: 15572
Epoch: [9]  [ 830/1185]  eta: 0:03:27  lr: 0.000045  min_lr: 0.000000  loss: 4.5656 (4.6037)  loss_scale: 65536.0000 (58280.5102)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0793  max mem: 15572
Epoch: [9]  [ 840/1185]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000000  loss: 4.3640 (4.6027)  loss_scale: 65536.0000 (58366.7824)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0284  max mem: 15572
Epoch: [9]  [ 850/1185]  eta: 0:03:15  lr: 0.000045  min_lr: 0.000000  loss: 4.4936 (4.6028)  loss_scale: 65536.0000 (58451.0270)  weight_decay: 0.0500 (0.0500)  time: 0.4738  data: 0.0008  max mem: 15572
Epoch: [9]  [ 860/1185]  eta: 0:03:09  lr: 0.000045  min_lr: 0.000000  loss: 4.5852 (4.6030)  loss_scale: 65536.0000 (58533.3148)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0608  max mem: 15572
Epoch: [9]  [ 870/1185]  eta: 0:03:03  lr: 0.000045  min_lr: 0.000000  loss: 4.6267 (4.6026)  loss_scale: 65536.0000 (58613.7130)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1248  max mem: 15572
Epoch: [9]  [ 880/1185]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000000  loss: 4.6673 (4.6047)  loss_scale: 65536.0000 (58692.2860)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0858  max mem: 15572
Epoch: [9]  [ 890/1185]  eta: 0:02:51  lr: 0.000045  min_lr: 0.000000  loss: 4.6896 (4.6032)  loss_scale: 65536.0000 (58769.0954)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1128  max mem: 15572
Epoch: [9]  [ 900/1185]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000000  loss: 4.5632 (4.6023)  loss_scale: 65536.0000 (58844.1998)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1274  max mem: 15572
Epoch: [9]  [ 910/1185]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000000  loss: 4.6550 (4.6039)  loss_scale: 65536.0000 (58917.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0899  max mem: 15572
Epoch: [9]  [ 920/1185]  eta: 0:02:34  lr: 0.000045  min_lr: 0.000000  loss: 4.5903 (4.6032)  loss_scale: 65536.0000 (58989.5157)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1020  max mem: 15572
Epoch: [9]  [ 930/1185]  eta: 0:02:28  lr: 0.000045  min_lr: 0.000000  loss: 4.6154 (4.6039)  loss_scale: 65536.0000 (59059.8324)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0986  max mem: 15572
Epoch: [9]  [ 940/1185]  eta: 0:02:22  lr: 0.000045  min_lr: 0.000000  loss: 4.6931 (4.6052)  loss_scale: 65536.0000 (59128.6546)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0732  max mem: 15572
[2025-01-13 01:02:14,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:02:14,049] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:02:14,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20907
[2025-01-13 01:02:14,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:02:14,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 950/1185]  eta: 0:02:16  lr: 0.000045  min_lr: 0.000000  loss: 4.6277 (4.6038)  loss_scale: 65536.0000 (59264.9422)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0456  max mem: 15572
Epoch: [9]  [ 960/1185]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000000  loss: 4.6635 (4.6057)  loss_scale: 65536.0000 (59330.1977)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0384  max mem: 15572
Epoch: [9]  [ 970/1185]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000000  loss: 4.6685 (4.6048)  loss_scale: 65536.0000 (59394.1092)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0497  max mem: 15572
Epoch: [9]  [ 980/1185]  eta: 0:01:59  lr: 0.000045  min_lr: 0.000000  loss: 4.5025 (4.6034)  loss_scale: 65536.0000 (59456.7176)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1318  max mem: 15572
Epoch: [9]  [ 990/1185]  eta: 0:01:53  lr: 0.000045  min_lr: 0.000000  loss: 4.5586 (4.6048)  loss_scale: 65536.0000 (59518.0626)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1365  max mem: 15572
Epoch: [9]  [1000/1185]  eta: 0:01:47  lr: 0.000045  min_lr: 0.000000  loss: 4.6794 (4.6043)  loss_scale: 65536.0000 (59578.1818)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.2007  max mem: 15572
Epoch: [9]  [1010/1185]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000000  loss: 4.5282 (4.6036)  loss_scale: 65536.0000 (59637.1118)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1667  max mem: 15572
Epoch: [9]  [1020/1185]  eta: 0:01:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5600 (4.6028)  loss_scale: 65536.0000 (59694.8874)  weight_decay: 0.0500 (0.0500)  time: 0.5095  data: 0.0456  max mem: 15572
Epoch: [9]  [1030/1185]  eta: 0:01:30  lr: 0.000045  min_lr: 0.000000  loss: 4.5821 (4.6033)  loss_scale: 65536.0000 (59751.5422)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1360  max mem: 15572
[2025-01-13 01:03:09,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=134, lr=[4.328065142976979e-07, 4.328065142976979e-07, 6.182950204252828e-07, 6.182950204252828e-07, 8.832786006075469e-07, 8.832786006075469e-07, 1.2618265722964956e-06, 1.2618265722964956e-06, 1.8026093889949938e-06, 1.8026093889949938e-06, 2.5751562699928484e-06, 2.5751562699928484e-06, 3.678794671418355e-06, 3.678794671418355e-06, 5.25542095916908e-06, 5.25542095916908e-06, 7.507744227384399e-06, 7.507744227384399e-06, 1.0725348896263429e-05, 1.0725348896263429e-05, 1.532192699466204e-05, 1.532192699466204e-05, 2.188846713523149e-05, 2.188846713523149e-05, 3.126923876461642e-05, 3.126923876461642e-05, 4.4670341092309166e-05, 4.4670341092309166e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:03:09,019] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=27.822074754704637, CurrSamplesPerSec=31.224636939403357, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1040/1185]  eta: 0:01:24  lr: 0.000045  min_lr: 0.000000  loss: 4.6952 (4.6044)  loss_scale: 65536.0000 (59807.1085)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1674  max mem: 15572
Epoch: [9]  [1050/1185]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000000  loss: 4.6061 (4.6042)  loss_scale: 65536.0000 (59861.6175)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1438  max mem: 15572
Epoch: [9]  [1060/1185]  eta: 0:01:12  lr: 0.000045  min_lr: 0.000000  loss: 4.6061 (4.6043)  loss_scale: 65536.0000 (59915.0990)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1693  max mem: 15572
Epoch: [9]  [1070/1185]  eta: 0:01:07  lr: 0.000045  min_lr: 0.000000  loss: 4.5319 (4.6034)  loss_scale: 65536.0000 (59967.5817)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1607  max mem: 15572
[2025-01-13 01:03:30,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:03:30,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:03:31,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21038
[2025-01-13 01:03:31,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:03:31,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1080/1185]  eta: 0:01:01  lr: 0.000045  min_lr: 0.000000  loss: 4.4832 (4.6040)  loss_scale: 65536.0000 (60140.3441)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0635  max mem: 15572
Epoch: [9]  [1090/1185]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000000  loss: 4.5318 (4.6030)  loss_scale: 65536.0000 (60189.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0900  max mem: 15572
Epoch: [9]  [1100/1185]  eta: 0:00:49  lr: 0.000045  min_lr: 0.000000  loss: 4.5821 (4.6039)  loss_scale: 65536.0000 (60238.3579)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1344  max mem: 15572
Epoch: [9]  [1110/1185]  eta: 0:00:43  lr: 0.000045  min_lr: 0.000000  loss: 4.5497 (4.6026)  loss_scale: 65536.0000 (60286.0414)  weight_decay: 0.0500 (0.0500)  time: 0.4973  data: 0.0620  max mem: 15572
Epoch: [9]  [1120/1185]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000000  loss: 4.4235 (4.6023)  loss_scale: 65536.0000 (60332.8742)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0939  max mem: 15572
[2025-01-13 01:04:00,889] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21089
[2025-01-13 01:04:00,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:04:00,890] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1130/1185]  eta: 0:00:32  lr: 0.000045  min_lr: 0.000000  loss: 4.5218 (4.6021)  loss_scale: 65536.0000 (60205.0433)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.2155  max mem: 15572
Epoch: [9]  [1140/1185]  eta: 0:00:26  lr: 0.000045  min_lr: 0.000000  loss: 4.6220 (4.6019)  loss_scale: 32768.0000 (59964.5784)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.2026  max mem: 15572
Epoch: [9]  [1150/1185]  eta: 0:00:20  lr: 0.000045  min_lr: 0.000000  loss: 4.6537 (4.6020)  loss_scale: 32768.0000 (59728.2919)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1213  max mem: 15572
Epoch: [9]  [1160/1185]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000000  loss: 4.6537 (4.6013)  loss_scale: 32768.0000 (59496.0758)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0909  max mem: 15572
Epoch: [9]  [1170/1185]  eta: 0:00:08  lr: 0.000045  min_lr: 0.000000  loss: 4.5847 (4.6015)  loss_scale: 32768.0000 (59267.8258)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0890  max mem: 15572
Epoch: [9]  [1180/1185]  eta: 0:00:02  lr: 0.000045  min_lr: 0.000000  loss: 4.6039 (4.6020)  loss_scale: 32768.0000 (59043.4412)  weight_decay: 0.0500 (0.0500)  time: 0.4750  data: 0.0558  max mem: 15572
Epoch: [9]  [1184/1185]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.6431 (4.6026)  loss_scale: 32768.0000 (58954.7477)  weight_decay: 0.0500 (0.0500)  time: 0.4100  data: 0.0005  max mem: 15572
Epoch: [9] Total time: 0:11:26 (0.5796 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.6431 (4.6026)  loss_scale: 32768.0000 (58954.7477)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1838
Indices to remove: tensor([   12,    60,   143,  ..., 33657, 33665, 33679], device='cuda:0')
length of data loader train is: 1032
num_training_steps_per_epoch is: 1032
Change step level LR scheduler!
Set warmup steps = 5160
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 01:04:31,275] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-13 01:04:31,277] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-13 01:04:31,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-13 01:04:31,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-13 01:04:31,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/272]  eta: 0:25:04  loss: 1.6187 (1.6187)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.5300  data: 5.3319  max mem: 15572
Val:  [ 10/272]  eta: 0:03:28  loss: 3.5149 (3.4108)  acc1: 27.7778 (27.7778)  acc5: 50.0000 (53.5354)  time: 0.7958  data: 0.6164  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 3.4962 (3.4339)  acc1: 27.7778 (29.3651)  acc5: 55.5556 (56.8783)  time: 0.2790  data: 0.0879  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.4962 (3.5350)  acc1: 22.2222 (26.1649)  acc5: 55.5556 (53.2258)  time: 0.2383  data: 0.0420  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.4380 (3.4919)  acc1: 22.2222 (25.8808)  acc5: 55.5556 (55.4201)  time: 0.3036  data: 0.1039  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 3.4161 (3.4297)  acc1: 22.2222 (25.7081)  acc5: 61.1111 (57.0806)  time: 0.3477  data: 0.1505  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 2.8215 (3.3538)  acc1: 27.7778 (28.6885)  acc5: 72.2222 (59.4718)  time: 0.3238  data: 0.1381  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 2.8215 (3.3010)  acc1: 44.4444 (31.4554)  acc5: 72.2222 (62.0501)  time: 0.3087  data: 0.1205  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 2.9279 (3.2730)  acc1: 44.4444 (32.7846)  acc5: 72.2222 (63.2373)  time: 0.3064  data: 0.1067  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.5889 (3.3317)  acc1: 22.2222 (30.7692)  acc5: 55.5556 (60.8669)  time: 0.2928  data: 0.0908  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.7242 (3.3747)  acc1: 22.2222 (30.4730)  acc5: 50.0000 (60.2310)  time: 0.3013  data: 0.1186  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6277 (3.4203)  acc1: 22.2222 (29.0791)  acc5: 55.5556 (58.6086)  time: 0.3129  data: 0.1259  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.7493 (3.4545)  acc1: 16.6667 (28.2828)  acc5: 55.5556 (57.7135)  time: 0.3124  data: 0.1049  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.6044 (3.4529)  acc1: 11.1111 (26.9720)  acc5: 55.5556 (57.2095)  time: 0.3447  data: 0.1294  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.4645 (3.4522)  acc1: 11.1111 (26.4775)  acc5: 55.5556 (57.4074)  time: 0.3154  data: 0.1055  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.4645 (3.4426)  acc1: 16.6667 (26.1957)  acc5: 55.5556 (57.4687)  time: 0.2894  data: 0.0804  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.3617 (3.4393)  acc1: 16.6667 (25.6729)  acc5: 55.5556 (57.9365)  time: 0.3381  data: 0.1277  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.5782 (3.4563)  acc1: 11.1111 (25.0162)  acc5: 55.5556 (57.5049)  time: 0.3119  data: 0.1197  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.4295 (3.4318)  acc1: 16.6667 (26.2124)  acc5: 61.1111 (58.3180)  time: 0.2849  data: 0.0892  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.4295 (3.4504)  acc1: 16.6667 (25.1018)  acc5: 50.0000 (57.3008)  time: 0.3482  data: 0.1281  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.5588 (3.4627)  acc1: 5.5556 (24.3505)  acc5: 44.4444 (56.8546)  time: 0.3360  data: 0.1018  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.4382 (3.4682)  acc1: 11.1111 (24.7499)  acc5: 61.1111 (56.9510)  time: 0.3138  data: 0.0943  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4382 (3.4677)  acc1: 33.3333 (24.8366)  acc5: 61.1111 (57.0890)  time: 0.3444  data: 0.1282  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.2318 (3.4618)  acc1: 22.2222 (25.1323)  acc5: 72.2222 (57.8403)  time: 0.3059  data: 0.0831  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2945 (3.4543)  acc1: 16.6667 (24.8732)  acc5: 72.2222 (58.3679)  time: 0.2697  data: 0.0661  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.3970 (3.4663)  acc1: 11.1111 (24.6348)  acc5: 55.5556 (57.8353)  time: 0.2806  data: 0.0929  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0916 (3.4425)  acc1: 22.2222 (25.4151)  acc5: 72.2222 (58.8548)  time: 0.2910  data: 0.0999  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.1139 (3.4406)  acc1: 33.3333 (25.4818)  acc5: 72.2222 (58.8971)  time: 0.2277  data: 0.0484  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.1139 (3.4437)  acc1: 27.7778 (25.4761)  acc5: 72.2222 (58.8777)  time: 0.2201  data: 0.0483  max mem: 15572
Val: Total time: 0:01:27 (0.3221 s / it)
* Acc@1 25.476 Acc@5 58.878 loss 3.444
Accuracy of the network on the 4883 val videos: 25.5%
[2025-01-13 01:05:59,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:05:59,467] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:05:59,467] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:06:02,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:06:02,089] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 25.48%
Epoch: [10]  [   0/1032]  eta: 1:51:22  lr: 0.000045  min_lr: 0.000000  loss: 4.7793 (4.7793)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4756  data: 5.8274  max mem: 15572
Epoch: [10]  [  10/1032]  eta: 0:18:39  lr: 0.000045  min_lr: 0.000000  loss: 4.7067 (4.7067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0953  data: 0.6437  max mem: 15572
Epoch: [10]  [  20/1032]  eta: 0:14:39  lr: 0.000045  min_lr: 0.000000  loss: 4.6698 (4.6316)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1538  max mem: 15572
Epoch: [10]  [  30/1032]  eta: 0:12:49  lr: 0.000045  min_lr: 0.000000  loss: 4.5700 (4.6080)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1475  max mem: 15572
Epoch: [10]  [  40/1032]  eta: 0:12:00  lr: 0.000045  min_lr: 0.000000  loss: 4.6224 (4.6373)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1272  max mem: 15572
Epoch: [10]  [  50/1032]  eta: 0:11:23  lr: 0.000045  min_lr: 0.000000  loss: 4.7384 (4.6466)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1238  max mem: 15572
Epoch: [10]  [  60/1032]  eta: 0:10:57  lr: 0.000045  min_lr: 0.000000  loss: 4.6864 (4.6462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1093  max mem: 15572
[2025-01-13 01:06:47,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:06:47,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [  70/1032]  eta: 0:10:38  lr: 0.000044  min_lr: 0.000000  loss: 4.6255 (4.6466)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1322  max mem: 15572
Epoch: [10]  [  80/1032]  eta: 0:10:26  lr: 0.000044  min_lr: 0.000000  loss: 4.6383 (4.6470)  loss_scale: 65536.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1646  max mem: 15572
Epoch: [10]  [  90/1032]  eta: 0:10:13  lr: 0.000044  min_lr: 0.000000  loss: 4.6466 (4.6434)  loss_scale: 65536.0000 (40689.9341)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1744  max mem: 15572
Epoch: [10]  [ 100/1032]  eta: 0:09:51  lr: 0.000044  min_lr: 0.000000  loss: 4.5894 (4.6434)  loss_scale: 65536.0000 (43149.9406)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0990  max mem: 15572
Epoch: [10]  [ 110/1032]  eta: 0:09:35  lr: 0.000044  min_lr: 0.000000  loss: 4.6416 (4.6542)  loss_scale: 65536.0000 (45166.7027)  weight_decay: 0.0500 (0.0500)  time: 0.5009  data: 0.0558  max mem: 15572
Epoch: [10]  [ 120/1032]  eta: 0:09:26  lr: 0.000044  min_lr: 0.000000  loss: 4.6254 (4.6496)  loss_scale: 65536.0000 (46850.1157)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1194  max mem: 15572
Epoch: [10]  [ 130/1032]  eta: 0:09:19  lr: 0.000044  min_lr: 0.000000  loss: 4.6027 (4.6513)  loss_scale: 65536.0000 (48276.5191)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1518  max mem: 15572
Epoch: [10]  [ 140/1032]  eta: 0:09:11  lr: 0.000044  min_lr: 0.000000  loss: 4.6334 (4.6480)  loss_scale: 65536.0000 (49500.5957)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1459  max mem: 15572
Epoch: [10]  [ 150/1032]  eta: 0:09:03  lr: 0.000044  min_lr: 0.000000  loss: 4.5099 (4.6353)  loss_scale: 65536.0000 (50562.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1416  max mem: 15572
Epoch: [10]  [ 160/1032]  eta: 0:08:57  lr: 0.000044  min_lr: 0.000000  loss: 4.4364 (4.6219)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1478  max mem: 15572
Epoch: [10]  [ 170/1032]  eta: 0:08:52  lr: 0.000044  min_lr: 0.000000  loss: 4.4569 (4.6168)  loss_scale: 65536.0000 (52313.8246)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1804  max mem: 15572
Epoch: [10]  [ 180/1032]  eta: 0:08:44  lr: 0.000044  min_lr: 0.000000  loss: 4.4569 (4.6101)  loss_scale: 65536.0000 (53044.3315)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1622  max mem: 15572
Epoch: [10]  [ 190/1032]  eta: 0:08:37  lr: 0.000044  min_lr: 0.000000  loss: 4.5456 (4.6080)  loss_scale: 65536.0000 (53698.3455)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1208  max mem: 15572
[2025-01-13 01:08:04,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:08:04,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:08:05,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21348
[2025-01-13 01:08:05,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:08:05,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 200/1032]  eta: 0:08:33  lr: 0.000044  min_lr: 0.000000  loss: 4.5654 (4.6067)  loss_scale: 65536.0000 (54939.3831)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1722  max mem: 15572
Epoch: [10]  [ 210/1032]  eta: 0:08:23  lr: 0.000044  min_lr: 0.000000  loss: 4.6735 (4.6094)  loss_scale: 65536.0000 (55441.5924)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1375  max mem: 15572
Epoch: [10]  [ 220/1032]  eta: 0:08:16  lr: 0.000044  min_lr: 0.000000  loss: 4.7288 (4.6109)  loss_scale: 65536.0000 (55898.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1022  max mem: 15572
Epoch: [10]  [ 230/1032]  eta: 0:08:08  lr: 0.000044  min_lr: 0.000000  loss: 4.5443 (4.6056)  loss_scale: 65536.0000 (56315.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1329  max mem: 15572
Epoch: [10]  [ 240/1032]  eta: 0:08:02  lr: 0.000044  min_lr: 0.000000  loss: 4.5443 (4.6044)  loss_scale: 65536.0000 (56698.1577)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1309  max mem: 15572
Epoch: [10]  [ 250/1032]  eta: 0:07:56  lr: 0.000044  min_lr: 0.000000  loss: 4.7294 (4.6127)  loss_scale: 65536.0000 (57050.2629)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1483  max mem: 15572
Epoch: [10]  [ 260/1032]  eta: 0:07:49  lr: 0.000044  min_lr: 0.000000  loss: 4.6173 (4.6081)  loss_scale: 65536.0000 (57375.3870)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1389  max mem: 15572
Epoch: [10]  [ 270/1032]  eta: 0:07:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5089 (4.6049)  loss_scale: 65536.0000 (57676.5166)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1279  max mem: 15572
Epoch: [10]  [ 280/1032]  eta: 0:07:35  lr: 0.000044  min_lr: 0.000000  loss: 4.6076 (4.6052)  loss_scale: 65536.0000 (57956.2135)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1214  max mem: 15572
Epoch: [10]  [ 290/1032]  eta: 0:07:28  lr: 0.000044  min_lr: 0.000000  loss: 4.5727 (4.6020)  loss_scale: 65536.0000 (58216.6873)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1238  max mem: 15572
Epoch: [10]  [ 300/1032]  eta: 0:07:19  lr: 0.000044  min_lr: 0.000000  loss: 4.5701 (4.6037)  loss_scale: 65536.0000 (58459.8538)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0838  max mem: 15572
Epoch: [10]  [ 310/1032]  eta: 0:07:13  lr: 0.000044  min_lr: 0.000000  loss: 4.5845 (4.6004)  loss_scale: 65536.0000 (58687.3826)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0940  max mem: 15572
Epoch: [10]  [ 320/1032]  eta: 0:07:07  lr: 0.000044  min_lr: 0.000000  loss: 4.5845 (4.6014)  loss_scale: 65536.0000 (58900.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1508  max mem: 15572
[2025-01-13 01:09:20,426] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:09:20,426] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:09:21,009] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21478
[2025-01-13 01:09:21,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:09:21,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 330/1032]  eta: 0:07:02  lr: 0.000044  min_lr: 0.000000  loss: 4.6730 (4.6030)  loss_scale: 65536.0000 (59299.1903)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1746  max mem: 15572
Epoch: [10]  [ 340/1032]  eta: 0:06:54  lr: 0.000044  min_lr: 0.000000  loss: 4.6410 (4.6035)  loss_scale: 65536.0000 (59482.0880)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1133  max mem: 15572
Epoch: [10]  [ 350/1032]  eta: 0:06:49  lr: 0.000044  min_lr: 0.000000  loss: 4.6169 (4.6029)  loss_scale: 65536.0000 (59654.5641)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1186  max mem: 15572
Epoch: [10]  [ 360/1032]  eta: 0:06:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5940 (4.6022)  loss_scale: 65536.0000 (59817.4848)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1424  max mem: 15572
Epoch: [10]  [ 370/1032]  eta: 0:06:35  lr: 0.000044  min_lr: 0.000000  loss: 4.5401 (4.5996)  loss_scale: 65536.0000 (59971.6226)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0823  max mem: 15572
Epoch: [10]  [ 380/1032]  eta: 0:06:28  lr: 0.000044  min_lr: 0.000000  loss: 4.5495 (4.6017)  loss_scale: 65536.0000 (60117.6693)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1080  max mem: 15572
Epoch: [10]  [ 390/1032]  eta: 0:06:21  lr: 0.000044  min_lr: 0.000000  loss: 4.5495 (4.5978)  loss_scale: 65536.0000 (60256.2455)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0883  max mem: 15572
Epoch: [10]  [ 400/1032]  eta: 0:06:15  lr: 0.000044  min_lr: 0.000000  loss: 4.5712 (4.5978)  loss_scale: 65536.0000 (60387.9102)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0612  max mem: 15572
Epoch: [10]  [ 410/1032]  eta: 0:06:09  lr: 0.000044  min_lr: 0.000000  loss: 4.5744 (4.5978)  loss_scale: 65536.0000 (60513.1679)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0841  max mem: 15572
Epoch: [10]  [ 420/1032]  eta: 0:06:02  lr: 0.000044  min_lr: 0.000000  loss: 4.5631 (4.6002)  loss_scale: 65536.0000 (60632.4751)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0828  max mem: 15572
Epoch: [10]  [ 430/1032]  eta: 0:05:55  lr: 0.000044  min_lr: 0.000000  loss: 4.6539 (4.6038)  loss_scale: 65536.0000 (60746.2459)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0340  max mem: 15572
Epoch: [10]  [ 440/1032]  eta: 0:05:48  lr: 0.000044  min_lr: 0.000000  loss: 4.7573 (4.6061)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0655  max mem: 15572
Epoch: [10]  [ 450/1032]  eta: 0:05:42  lr: 0.000044  min_lr: 0.000000  loss: 4.5687 (4.6028)  loss_scale: 65536.0000 (60958.6519)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1066  max mem: 15572
[2025-01-13 01:10:31,626] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:10:31,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 460/1032]  eta: 0:05:35  lr: 0.000044  min_lr: 0.000000  loss: 4.5263 (4.6016)  loss_scale: 65536.0000 (61484.4252)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0421  max mem: 15572
[2025-01-13 01:10:33,114] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21610
[2025-01-13 01:10:33,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:10:33,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 470/1032]  eta: 0:05:29  lr: 0.000044  min_lr: 0.000000  loss: 4.5136 (4.6003)  loss_scale: 65536.0000 (61570.4459)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0246  max mem: 15572
Epoch: [10]  [ 480/1032]  eta: 0:05:23  lr: 0.000044  min_lr: 0.000000  loss: 4.5422 (4.5999)  loss_scale: 65536.0000 (61652.8898)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1010  max mem: 15572
Epoch: [10]  [ 490/1032]  eta: 0:05:17  lr: 0.000044  min_lr: 0.000000  loss: 4.6516 (4.6011)  loss_scale: 65536.0000 (61731.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1006  max mem: 15572
Epoch: [10]  [ 500/1032]  eta: 0:05:10  lr: 0.000044  min_lr: 0.000000  loss: 4.6480 (4.6021)  loss_scale: 65536.0000 (61807.9042)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0329  max mem: 15572
Epoch: [10]  [ 510/1032]  eta: 0:05:05  lr: 0.000044  min_lr: 0.000000  loss: 4.5702 (4.6017)  loss_scale: 65536.0000 (61880.8611)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0732  max mem: 15572
Epoch: [10]  [ 520/1032]  eta: 0:04:59  lr: 0.000044  min_lr: 0.000000  loss: 4.5627 (4.6000)  loss_scale: 65536.0000 (61951.0173)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1218  max mem: 15572
Epoch: [10]  [ 530/1032]  eta: 0:04:53  lr: 0.000044  min_lr: 0.000000  loss: 4.6125 (4.6015)  loss_scale: 65536.0000 (62018.5311)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1005  max mem: 15572
Epoch: [10]  [ 540/1032]  eta: 0:04:47  lr: 0.000044  min_lr: 0.000000  loss: 4.6767 (4.6032)  loss_scale: 65536.0000 (62083.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0429  max mem: 15572
Epoch: [10]  [ 550/1032]  eta: 0:04:40  lr: 0.000044  min_lr: 0.000000  loss: 4.7123 (4.6053)  loss_scale: 65536.0000 (62146.2069)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0182  max mem: 15572
Epoch: [10]  [ 560/1032]  eta: 0:04:34  lr: 0.000044  min_lr: 0.000000  loss: 4.6727 (4.6063)  loss_scale: 65536.0000 (62206.6310)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0716  max mem: 15572
Epoch: [10]  [ 570/1032]  eta: 0:04:29  lr: 0.000044  min_lr: 0.000000  loss: 4.7166 (4.6092)  loss_scale: 65536.0000 (62264.9387)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1750  max mem: 15572
Epoch: [10]  [ 580/1032]  eta: 0:04:22  lr: 0.000044  min_lr: 0.000000  loss: 4.6971 (4.6089)  loss_scale: 65536.0000 (62321.2392)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1341  max mem: 15572
[2025-01-13 01:11:45,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:11:45,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 590/1032]  eta: 0:04:16  lr: 0.000044  min_lr: 0.000000  loss: 4.6383 (4.6096)  loss_scale: 65536.0000 (62486.5245)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0483  max mem: 15572
[2025-01-13 01:11:45,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21740
[2025-01-13 01:11:45,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:11:45,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 600/1032]  eta: 0:04:10  lr: 0.000044  min_lr: 0.000000  loss: 4.5397 (4.6076)  loss_scale: 65536.0000 (62537.2646)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1059  max mem: 15572
Epoch: [10]  [ 610/1032]  eta: 0:04:04  lr: 0.000044  min_lr: 0.000000  loss: 4.5394 (4.6080)  loss_scale: 65536.0000 (62586.3437)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1062  max mem: 15572
Epoch: [10]  [ 620/1032]  eta: 0:03:58  lr: 0.000044  min_lr: 0.000000  loss: 4.7032 (4.6088)  loss_scale: 65536.0000 (62633.8422)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0946  max mem: 15572
Epoch: [10]  [ 630/1032]  eta: 0:03:52  lr: 0.000044  min_lr: 0.000000  loss: 4.6137 (4.6076)  loss_scale: 65536.0000 (62679.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0729  max mem: 15572
Epoch: [10]  [ 640/1032]  eta: 0:03:46  lr: 0.000044  min_lr: 0.000000  loss: 4.5404 (4.6074)  loss_scale: 65536.0000 (62724.3931)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0852  max mem: 15572
Epoch: [10]  [ 650/1032]  eta: 0:03:40  lr: 0.000044  min_lr: 0.000000  loss: 4.5425 (4.6062)  loss_scale: 65536.0000 (62767.5822)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1111  max mem: 15572
Epoch: [10]  [ 660/1032]  eta: 0:03:34  lr: 0.000044  min_lr: 0.000000  loss: 4.6081 (4.6069)  loss_scale: 65536.0000 (62809.4644)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0646  max mem: 15572
Epoch: [10]  [ 670/1032]  eta: 0:03:29  lr: 0.000044  min_lr: 0.000000  loss: 4.6081 (4.6065)  loss_scale: 65536.0000 (62850.0984)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0762  max mem: 15572
Epoch: [10]  [ 680/1032]  eta: 0:03:23  lr: 0.000044  min_lr: 0.000000  loss: 4.5681 (4.6082)  loss_scale: 65536.0000 (62889.5389)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.1314  max mem: 15572
Epoch: [10]  [ 690/1032]  eta: 0:03:17  lr: 0.000044  min_lr: 0.000000  loss: 4.5489 (4.6061)  loss_scale: 65536.0000 (62927.8379)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1109  max mem: 15572
Epoch: [10]  [ 700/1032]  eta: 0:03:12  lr: 0.000044  min_lr: 0.000000  loss: 4.5011 (4.6061)  loss_scale: 65536.0000 (62965.0442)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0900  max mem: 15572
Epoch: [10]  [ 710/1032]  eta: 0:03:06  lr: 0.000044  min_lr: 0.000000  loss: 4.6731 (4.6072)  loss_scale: 65536.0000 (63001.2039)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1662  max mem: 15572
[2025-01-13 01:12:59,469] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:12:59,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 720/1032]  eta: 0:03:00  lr: 0.000044  min_lr: 0.000000  loss: 4.6557 (4.6079)  loss_scale: 65536.0000 (63127.2566)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1324  max mem: 15572
[2025-01-13 01:13:02,315] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21875
[2025-01-13 01:13:02,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:13:02,316] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 730/1032]  eta: 0:02:54  lr: 0.000044  min_lr: 0.000000  loss: 4.6476 (4.6087)  loss_scale: 65536.0000 (63608.4706)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0930  max mem: 15572
Epoch: [10]  [ 740/1032]  eta: 0:02:49  lr: 0.000044  min_lr: 0.000000  loss: 4.5994 (4.6069)  loss_scale: 65536.0000 (63634.4831)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1603  max mem: 15572
Epoch: [10]  [ 750/1032]  eta: 0:02:43  lr: 0.000044  min_lr: 0.000000  loss: 4.6916 (4.6103)  loss_scale: 65536.0000 (63659.8029)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1341  max mem: 15572
Epoch: [10]  [ 760/1032]  eta: 0:02:37  lr: 0.000044  min_lr: 0.000000  loss: 4.7722 (4.6102)  loss_scale: 65536.0000 (63684.4573)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0884  max mem: 15572
[2025-01-13 01:13:28,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21917
[2025-01-13 01:13:28,078] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:13:28,078] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 770/1032]  eta: 0:02:31  lr: 0.000044  min_lr: 0.000000  loss: 4.5079 (4.6097)  loss_scale: 65536.0000 (63580.9702)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0818  max mem: 15572
Epoch: [10]  [ 780/1032]  eta: 0:02:25  lr: 0.000044  min_lr: 0.000000  loss: 4.5950 (4.6109)  loss_scale: 32768.0000 (63186.4379)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0949  max mem: 15572
Epoch: [10]  [ 790/1032]  eta: 0:02:20  lr: 0.000044  min_lr: 0.000000  loss: 4.5950 (4.6097)  loss_scale: 32768.0000 (62801.8812)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0882  max mem: 15572
Epoch: [10]  [ 800/1032]  eta: 0:02:14  lr: 0.000044  min_lr: 0.000000  loss: 4.4520 (4.6084)  loss_scale: 32768.0000 (62426.9263)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0973  max mem: 15572
Epoch: [10]  [ 810/1032]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000000  loss: 4.4914 (4.6088)  loss_scale: 32768.0000 (62061.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0694  max mem: 15572
Epoch: [10]  [ 820/1032]  eta: 0:02:02  lr: 0.000044  min_lr: 0.000000  loss: 4.7055 (4.6086)  loss_scale: 32768.0000 (61704.4190)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0319  max mem: 15572
Epoch: [10]  [ 830/1032]  eta: 0:01:56  lr: 0.000044  min_lr: 0.000000  loss: 4.7522 (4.6108)  loss_scale: 32768.0000 (61356.2070)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1206  max mem: 15572
Epoch: [10]  [ 840/1032]  eta: 0:01:50  lr: 0.000044  min_lr: 0.000000  loss: 4.7823 (4.6115)  loss_scale: 32768.0000 (61016.2759)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1518  max mem: 15572
[2025-01-13 01:14:14,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=142, lr=[4.2387280533038436e-07, 4.2387280533038436e-07, 6.055325790434063e-07, 6.055325790434063e-07, 8.650465414905804e-07, 8.650465414905804e-07, 1.2357807735579723e-06, 1.2357807735579723e-06, 1.7654011050828175e-06, 1.7654011050828175e-06, 2.5220015786897395e-06, 2.5220015786897395e-06, 3.6028593981281994e-06, 3.6028593981281994e-06, 5.146941997326e-06, 5.146941997326e-06, 7.352774281894285e-06, 7.352774281894285e-06, 1.050396325984898e-05, 1.050396325984898e-05, 1.5005661799784256e-05, 1.5005661799784256e-05, 2.143665971397751e-05, 2.143665971397751e-05, 3.0623799591396446e-05, 3.0623799591396446e-05, 4.3748285130566356e-05, 4.3748285130566356e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:14:14,103] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=27.811650809686803, CurrSamplesPerSec=23.11932451339848, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 850/1032]  eta: 0:01:45  lr: 0.000044  min_lr: 0.000000  loss: 4.5300 (4.6105)  loss_scale: 32768.0000 (60684.3337)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1094  max mem: 15572
Epoch: [10]  [ 860/1032]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000000  loss: 4.5709 (4.6124)  loss_scale: 32768.0000 (60360.1022)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0508  max mem: 15572
Epoch: [10]  [ 870/1032]  eta: 0:01:33  lr: 0.000044  min_lr: 0.000000  loss: 4.7153 (4.6134)  loss_scale: 32768.0000 (60043.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0685  max mem: 15572
Epoch: [10]  [ 880/1032]  eta: 0:01:27  lr: 0.000044  min_lr: 0.000000  loss: 4.7014 (4.6128)  loss_scale: 32768.0000 (59733.7208)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1531  max mem: 15572
Epoch: [10]  [ 890/1032]  eta: 0:01:21  lr: 0.000044  min_lr: 0.000000  loss: 4.4970 (4.6114)  loss_scale: 32768.0000 (59431.0752)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1068  max mem: 15572
[2025-01-13 01:14:39,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:14:39,810] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 900/1032]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000000  loss: 4.6243 (4.6130)  loss_scale: 32768.0000 (59280.6215)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0614  max mem: 15572
Epoch: [10]  [ 910/1032]  eta: 0:01:10  lr: 0.000044  min_lr: 0.000000  loss: 4.7044 (4.6139)  loss_scale: 65536.0000 (59349.2865)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1007  max mem: 15572
Epoch: [10]  [ 920/1032]  eta: 0:01:04  lr: 0.000044  min_lr: 0.000000  loss: 4.7577 (4.6155)  loss_scale: 65536.0000 (59416.4604)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1343  max mem: 15572
Epoch: [10]  [ 930/1032]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000000  loss: 4.5696 (4.6145)  loss_scale: 65536.0000 (59482.1912)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1268  max mem: 15572
Epoch: [10]  [ 940/1032]  eta: 0:00:53  lr: 0.000044  min_lr: 0.000000  loss: 4.5100 (4.6134)  loss_scale: 65536.0000 (59546.5250)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1131  max mem: 15572
Epoch: [10]  [ 950/1032]  eta: 0:00:47  lr: 0.000044  min_lr: 0.000000  loss: 4.5254 (4.6132)  loss_scale: 65536.0000 (59609.5058)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0686  max mem: 15572
Epoch: [10]  [ 960/1032]  eta: 0:00:41  lr: 0.000044  min_lr: 0.000000  loss: 4.7124 (4.6148)  loss_scale: 65536.0000 (59671.1759)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0683  max mem: 15572
Epoch: [10]  [ 970/1032]  eta: 0:00:35  lr: 0.000044  min_lr: 0.000000  loss: 4.6527 (4.6132)  loss_scale: 65536.0000 (59731.5757)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1325  max mem: 15572
Epoch: [10]  [ 980/1032]  eta: 0:00:29  lr: 0.000044  min_lr: 0.000000  loss: 4.4198 (4.6115)  loss_scale: 65536.0000 (59790.7441)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1393  max mem: 15572
Epoch: [10]  [ 990/1032]  eta: 0:00:24  lr: 0.000044  min_lr: 0.000000  loss: 4.4485 (4.6112)  loss_scale: 65536.0000 (59848.7185)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1182  max mem: 15572
Epoch: [10]  [1000/1032]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000000  loss: 4.6183 (4.6117)  loss_scale: 65536.0000 (59905.5345)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1065  max mem: 15572
Epoch: [10]  [1010/1032]  eta: 0:00:12  lr: 0.000044  min_lr: 0.000000  loss: 4.6261 (4.6118)  loss_scale: 65536.0000 (59961.2265)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0582  max mem: 15572
Epoch: [10]  [1020/1032]  eta: 0:00:06  lr: 0.000044  min_lr: 0.000000  loss: 4.6527 (4.6118)  loss_scale: 65536.0000 (60015.8276)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0160  max mem: 15572
[2025-01-13 01:15:52,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:15:52,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:15:54,211] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22179
[2025-01-13 01:15:54,211] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:15:54,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1030/1032]  eta: 0:00:01  lr: 0.000044  min_lr: 0.000000  loss: 4.6527 (4.6122)  loss_scale: 65536.0000 (60387.1969)  weight_decay: 0.0500 (0.0500)  time: 0.4612  data: 0.0159  max mem: 15572
Epoch: [10]  [1031/1032]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000000  loss: 4.6805 (4.6128)  loss_scale: 65536.0000 (60392.1860)  weight_decay: 0.0500 (0.0500)  time: 0.4542  data: 0.0159  max mem: 15572
Epoch: [10] Total time: 0:09:52 (0.5744 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000000  loss: 4.6805 (4.6128)  loss_scale: 65536.0000 (60392.1860)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1676
Indices to remove: tensor([    7,    24,    34,  ..., 33615, 33618, 33622], device='cuda:0')
length of data loader train is: 892
num_training_steps_per_epoch is: 892
Change step level LR scheduler!
Set warmup steps = 4460
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:26:55  loss: 1.7015 (1.7015)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.9397  data: 5.7640  max mem: 15572
Val:  [ 10/272]  eta: 0:03:35  loss: 3.5777 (3.5049)  acc1: 16.6667 (22.2222)  acc5: 44.4444 (47.9798)  time: 0.8234  data: 0.6140  max mem: 15572
Val:  [ 20/272]  eta: 0:02:25  loss: 3.4953 (3.4650)  acc1: 22.2222 (23.5450)  acc5: 44.4444 (52.6455)  time: 0.3079  data: 0.0889  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 3.4771 (3.4859)  acc1: 22.2222 (23.6559)  acc5: 55.5556 (54.8387)  time: 0.3009  data: 0.0786  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 3.4506 (3.4642)  acc1: 22.2222 (23.3062)  acc5: 55.5556 (56.3686)  time: 0.2755  data: 0.0725  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 3.3576 (3.4122)  acc1: 27.7778 (25.3813)  acc5: 61.1111 (58.4967)  time: 0.2170  data: 0.0335  max mem: 15572
Val:  [ 60/272]  eta: 0:01:14  loss: 3.1034 (3.3651)  acc1: 33.3333 (26.4117)  acc5: 72.2222 (60.2004)  time: 0.1845  data: 0.0049  max mem: 15572
Val:  [ 70/272]  eta: 0:01:06  loss: 3.0537 (3.3304)  acc1: 33.3333 (28.8732)  acc5: 72.2222 (62.1283)  time: 0.2009  data: 0.0080  max mem: 15572
Val:  [ 80/272]  eta: 0:01:04  loss: 3.1860 (3.3150)  acc1: 38.8889 (29.4239)  acc5: 72.2222 (62.2771)  time: 0.2854  data: 0.0794  max mem: 15572
Val:  [ 90/272]  eta: 0:01:01  loss: 3.5792 (3.3660)  acc1: 22.2222 (27.8999)  acc5: 50.0000 (59.8291)  time: 0.3636  data: 0.1619  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5967 (3.3845)  acc1: 22.2222 (28.8229)  acc5: 50.0000 (60.0660)  time: 0.3823  data: 0.1854  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 3.4888 (3.4178)  acc1: 27.7778 (27.4775)  acc5: 61.1111 (59.1592)  time: 0.3765  data: 0.1831  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.6591 (3.4404)  acc1: 16.6667 (27.0891)  acc5: 44.4444 (58.3563)  time: 0.3299  data: 0.1365  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.5029 (3.4391)  acc1: 16.6667 (26.2935)  acc5: 55.5556 (58.2697)  time: 0.3139  data: 0.1027  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.4477 (3.4373)  acc1: 16.6667 (25.9259)  acc5: 55.5556 (58.5500)  time: 0.3364  data: 0.1222  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.3695 (3.4300)  acc1: 16.6667 (25.9014)  acc5: 61.1111 (58.8300)  time: 0.3704  data: 0.1808  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.4006 (3.4309)  acc1: 16.6667 (25.5694)  acc5: 55.5556 (58.7647)  time: 0.3669  data: 0.1664  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.4792 (3.4496)  acc1: 11.1111 (25.1787)  acc5: 50.0000 (58.2196)  time: 0.3325  data: 0.1232  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3298 (3.4325)  acc1: 16.6667 (25.2609)  acc5: 61.1111 (58.9012)  time: 0.3523  data: 0.1535  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.1325 (3.4390)  acc1: 16.6667 (24.8109)  acc5: 61.1111 (58.3188)  time: 0.3639  data: 0.1575  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.2219 (3.4389)  acc1: 5.5556 (24.6269)  acc5: 50.0000 (58.4854)  time: 0.3424  data: 0.1383  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.2931 (3.4445)  acc1: 16.6667 (24.5392)  acc5: 72.2222 (58.6625)  time: 0.3355  data: 0.1399  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.3754 (3.4429)  acc1: 22.2222 (24.8617)  acc5: 66.6667 (58.8487)  time: 0.3140  data: 0.1235  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.2810 (3.4388)  acc1: 33.3333 (25.1563)  acc5: 66.6667 (59.1871)  time: 0.3089  data: 0.1138  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.2641 (3.4268)  acc1: 22.2222 (25.0346)  acc5: 72.2222 (59.8894)  time: 0.3851  data: 0.1758  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2832 (3.4377)  acc1: 11.1111 (24.5020)  acc5: 66.6667 (59.3847)  time: 0.3860  data: 0.1817  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.0062 (3.4125)  acc1: 22.2222 (25.6066)  acc5: 72.2222 (60.4725)  time: 0.3810  data: 0.1927  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.1493 (3.4132)  acc1: 27.7778 (25.3998)  acc5: 77.7778 (60.5986)  time: 0.3244  data: 0.1546  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.1493 (3.4165)  acc1: 27.7778 (25.3737)  acc5: 72.2222 (60.5775)  time: 0.3183  data: 0.1546  max mem: 15572
Val: Total time: 0:01:33 (0.3437 s / it)
* Acc@1 25.374 Acc@5 60.578 loss 3.417
Accuracy of the network on the 4883 val videos: 25.4%
Max accuracy: 25.48%
Epoch: [11]  [  0/892]  eta: 2:54:42  lr: 0.000044  min_lr: 0.000000  loss: 4.8289 (4.8289)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 11.7512  data: 11.1284  max mem: 15572
Epoch: [11]  [ 10/892]  eta: 0:22:15  lr: 0.000044  min_lr: 0.000000  loss: 4.7094 (4.5929)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5147  data: 1.0126  max mem: 15572
[2025-01-13 01:17:49,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22201
[2025-01-13 01:17:49,578] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:17:49,578] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 20/892]  eta: 0:14:21  lr: 0.000044  min_lr: 0.000000  loss: 4.6483 (4.6021)  loss_scale: 65536.0000 (63975.6190)  weight_decay: 0.0500 (0.0500)  time: 0.4492  data: 0.0051  max mem: 15572
Epoch: [11]  [ 30/892]  eta: 0:11:31  lr: 0.000044  min_lr: 0.000000  loss: 4.6224 (4.6098)  loss_scale: 32768.0000 (53908.6452)  weight_decay: 0.0500 (0.0500)  time: 0.4101  data: 0.0049  max mem: 15572
Epoch: [11]  [ 40/892]  eta: 0:10:05  lr: 0.000044  min_lr: 0.000000  loss: 4.5307 (4.5903)  loss_scale: 32768.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  time: 0.4200  data: 0.0005  max mem: 15572
Epoch: [11]  [ 50/892]  eta: 0:09:14  lr: 0.000044  min_lr: 0.000000  loss: 4.5307 (4.5907)  loss_scale: 32768.0000 (45618.1961)  weight_decay: 0.0500 (0.0500)  time: 0.4370  data: 0.0004  max mem: 15572
Epoch: [11]  [ 60/892]  eta: 0:08:37  lr: 0.000043  min_lr: 0.000000  loss: 4.5207 (4.5790)  loss_scale: 32768.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  time: 0.4399  data: 0.0005  max mem: 15572
Epoch: [11]  [ 70/892]  eta: 0:08:16  lr: 0.000043  min_lr: 0.000000  loss: 4.5958 (4.5915)  loss_scale: 32768.0000 (41998.4225)  weight_decay: 0.0500 (0.0500)  time: 0.4654  data: 0.0264  max mem: 15572
Epoch: [11]  [ 80/892]  eta: 0:08:06  lr: 0.000043  min_lr: 0.000000  loss: 4.6660 (4.6123)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0798  max mem: 15572
Epoch: [11]  [ 90/892]  eta: 0:08:07  lr: 0.000043  min_lr: 0.000000  loss: 4.7470 (4.6209)  loss_scale: 32768.0000 (39969.7582)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1627  max mem: 15572
Epoch: [11]  [100/892]  eta: 0:07:55  lr: 0.000043  min_lr: 0.000000  loss: 4.7503 (4.6355)  loss_scale: 32768.0000 (39256.7129)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1430  max mem: 15572
Epoch: [11]  [110/892]  eta: 0:07:51  lr: 0.000043  min_lr: 0.000000  loss: 4.6206 (4.6284)  loss_scale: 32768.0000 (38672.1441)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1182  max mem: 15572
Epoch: [11]  [120/892]  eta: 0:07:49  lr: 0.000043  min_lr: 0.000000  loss: 4.5316 (4.6212)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1851  max mem: 15572
Epoch: [11]  [130/892]  eta: 0:07:39  lr: 0.000043  min_lr: 0.000000  loss: 4.6304 (4.6245)  loss_scale: 32768.0000 (37770.7481)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1581  max mem: 15572
Epoch: [11]  [140/892]  eta: 0:07:29  lr: 0.000043  min_lr: 0.000000  loss: 4.6541 (4.6310)  loss_scale: 32768.0000 (37415.9433)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.1009  max mem: 15572
[2025-01-13 01:18:58,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:18:58,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [150/892]  eta: 0:07:23  lr: 0.000043  min_lr: 0.000000  loss: 4.6894 (4.6387)  loss_scale: 32768.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1193  max mem: 15572
Epoch: [11]  [160/892]  eta: 0:07:12  lr: 0.000043  min_lr: 0.000000  loss: 4.6894 (4.6383)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0908  max mem: 15572
Epoch: [11]  [170/892]  eta: 0:07:04  lr: 0.000043  min_lr: 0.000000  loss: 4.5994 (4.6339)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0548  max mem: 15572
Epoch: [11]  [180/892]  eta: 0:06:59  lr: 0.000043  min_lr: 0.000000  loss: 4.5565 (4.6301)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1195  max mem: 15572
Epoch: [11]  [190/892]  eta: 0:06:49  lr: 0.000043  min_lr: 0.000000  loss: 4.5386 (4.6283)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1170  max mem: 15572
Epoch: [11]  [200/892]  eta: 0:06:42  lr: 0.000043  min_lr: 0.000000  loss: 4.6372 (4.6338)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0812  max mem: 15572
Epoch: [11]  [210/892]  eta: 0:06:35  lr: 0.000043  min_lr: 0.000000  loss: 4.6372 (4.6305)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0935  max mem: 15572
Epoch: [11]  [220/892]  eta: 0:06:28  lr: 0.000043  min_lr: 0.000000  loss: 4.6227 (4.6277)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0956  max mem: 15572
Epoch: [11]  [230/892]  eta: 0:06:24  lr: 0.000043  min_lr: 0.000000  loss: 4.5169 (4.6252)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1310  max mem: 15572
Epoch: [11]  [240/892]  eta: 0:06:20  lr: 0.000043  min_lr: 0.000000  loss: 4.5196 (4.6272)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1888  max mem: 15572
Epoch: [11]  [250/892]  eta: 0:06:16  lr: 0.000043  min_lr: 0.000000  loss: 4.6123 (4.6281)  loss_scale: 65536.0000 (48695.0757)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.2202  max mem: 15572
Epoch: [11]  [260/892]  eta: 0:06:10  lr: 0.000043  min_lr: 0.000000  loss: 4.6474 (4.6290)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1930  max mem: 15572
Epoch: [11]  [270/892]  eta: 0:06:04  lr: 0.000043  min_lr: 0.000000  loss: 4.7217 (4.6301)  loss_scale: 65536.0000 (49937.9483)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1503  max mem: 15572
[2025-01-13 01:20:11,521] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:20:11,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:20:12,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22461
[2025-01-13 01:20:12,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:20:12,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [280/892]  eta: 0:05:57  lr: 0.000043  min_lr: 0.000000  loss: 4.7576 (4.6337)  loss_scale: 65536.0000 (51192.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1027  max mem: 15572
Epoch: [11]  [290/892]  eta: 0:05:50  lr: 0.000043  min_lr: 0.000000  loss: 4.7041 (4.6365)  loss_scale: 65536.0000 (51685.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0918  max mem: 15572
Epoch: [11]  [300/892]  eta: 0:05:45  lr: 0.000043  min_lr: 0.000000  loss: 4.6014 (4.6328)  loss_scale: 65536.0000 (52145.7542)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1293  max mem: 15572
Epoch: [11]  [310/892]  eta: 0:05:40  lr: 0.000043  min_lr: 0.000000  loss: 4.5899 (4.6325)  loss_scale: 65536.0000 (52576.3087)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1644  max mem: 15572
Epoch: [11]  [320/892]  eta: 0:05:32  lr: 0.000043  min_lr: 0.000000  loss: 4.6343 (4.6312)  loss_scale: 65536.0000 (52980.0374)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0980  max mem: 15572
Epoch: [11]  [330/892]  eta: 0:05:26  lr: 0.000043  min_lr: 0.000000  loss: 4.7392 (4.6361)  loss_scale: 65536.0000 (53359.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0532  max mem: 15572
Epoch: [11]  [340/892]  eta: 0:05:19  lr: 0.000043  min_lr: 0.000000  loss: 4.7530 (4.6356)  loss_scale: 65536.0000 (53716.4575)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0773  max mem: 15572
Epoch: [11]  [350/892]  eta: 0:05:13  lr: 0.000043  min_lr: 0.000000  loss: 4.6948 (4.6374)  loss_scale: 65536.0000 (54053.1966)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0786  max mem: 15572
Epoch: [11]  [360/892]  eta: 0:05:08  lr: 0.000043  min_lr: 0.000000  loss: 4.5675 (4.6326)  loss_scale: 65536.0000 (54371.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1189  max mem: 15572
Epoch: [11]  [370/892]  eta: 0:05:01  lr: 0.000043  min_lr: 0.000000  loss: 4.5675 (4.6334)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1257  max mem: 15572
Epoch: [11]  [380/892]  eta: 0:04:55  lr: 0.000043  min_lr: 0.000000  loss: 4.5104 (4.6301)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.1127  max mem: 15572
Epoch: [11]  [390/892]  eta: 0:04:49  lr: 0.000043  min_lr: 0.000000  loss: 4.6154 (4.6325)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1286  max mem: 15572
Epoch: [11]  [400/892]  eta: 0:04:44  lr: 0.000043  min_lr: 0.000000  loss: 4.7491 (4.6351)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1595  max mem: 15572
[2025-01-13 01:21:25,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:21:25,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:21:26,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22591
[2025-01-13 01:21:26,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:21:26,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [410/892]  eta: 0:04:38  lr: 0.000043  min_lr: 0.000000  loss: 4.6338 (4.6319)  loss_scale: 65536.0000 (55888.9732)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1319  max mem: 15572
Epoch: [11]  [420/892]  eta: 0:04:32  lr: 0.000043  min_lr: 0.000000  loss: 4.4739 (4.6329)  loss_scale: 65536.0000 (56118.1188)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1201  max mem: 15572
Epoch: [11]  [430/892]  eta: 0:04:27  lr: 0.000043  min_lr: 0.000000  loss: 4.7055 (4.6336)  loss_scale: 65536.0000 (56336.6311)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1849  max mem: 15572
Epoch: [11]  [440/892]  eta: 0:04:20  lr: 0.000043  min_lr: 0.000000  loss: 4.6675 (4.6329)  loss_scale: 65536.0000 (56545.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1276  max mem: 15572
Epoch: [11]  [450/892]  eta: 0:04:14  lr: 0.000043  min_lr: 0.000000  loss: 4.6636 (4.6335)  loss_scale: 65536.0000 (56744.5854)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0683  max mem: 15572
Epoch: [11]  [460/892]  eta: 0:04:08  lr: 0.000043  min_lr: 0.000000  loss: 4.7116 (4.6362)  loss_scale: 65536.0000 (56935.2885)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0815  max mem: 15572
Epoch: [11]  [470/892]  eta: 0:04:02  lr: 0.000043  min_lr: 0.000000  loss: 4.6908 (4.6354)  loss_scale: 65536.0000 (57117.8938)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1146  max mem: 15572
Epoch: [11]  [480/892]  eta: 0:03:56  lr: 0.000043  min_lr: 0.000000  loss: 4.6029 (4.6355)  loss_scale: 65536.0000 (57292.9064)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1318  max mem: 15572
Epoch: [11]  [490/892]  eta: 0:03:50  lr: 0.000043  min_lr: 0.000000  loss: 4.6127 (4.6357)  loss_scale: 65536.0000 (57460.7902)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0838  max mem: 15572
Epoch: [11]  [500/892]  eta: 0:03:45  lr: 0.000043  min_lr: 0.000000  loss: 4.5933 (4.6342)  loss_scale: 65536.0000 (57621.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1258  max mem: 15572
Epoch: [11]  [510/892]  eta: 0:03:39  lr: 0.000043  min_lr: 0.000000  loss: 4.5037 (4.6301)  loss_scale: 65536.0000 (57776.8454)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1687  max mem: 15572
Epoch: [11]  [520/892]  eta: 0:03:33  lr: 0.000043  min_lr: 0.000000  loss: 4.5037 (4.6294)  loss_scale: 65536.0000 (57925.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1590  max mem: 15572
Epoch: [11]  [530/892]  eta: 0:03:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5821 (4.6284)  loss_scale: 65536.0000 (58069.0923)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1229  max mem: 15572
[2025-01-13 01:22:39,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:22:39,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [540/892]  eta: 0:03:22  lr: 0.000043  min_lr: 0.000000  loss: 4.6935 (4.6292)  loss_scale: 65536.0000 (58449.3900)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1301  max mem: 15572
[2025-01-13 01:22:40,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22722
[2025-01-13 01:22:40,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:22:40,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [550/892]  eta: 0:03:16  lr: 0.000043  min_lr: 0.000000  loss: 4.6905 (4.6297)  loss_scale: 65536.0000 (58578.0036)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1395  max mem: 15572
Epoch: [11]  [560/892]  eta: 0:03:10  lr: 0.000043  min_lr: 0.000000  loss: 4.6767 (4.6308)  loss_scale: 65536.0000 (58702.0321)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1067  max mem: 15572
[2025-01-13 01:22:56,976] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22750
[2025-01-13 01:22:56,977] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:22:56,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [570/892]  eta: 0:03:05  lr: 0.000043  min_lr: 0.000000  loss: 4.7196 (4.6335)  loss_scale: 65536.0000 (58706.9422)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1560  max mem: 15572
Epoch: [11]  [580/892]  eta: 0:02:59  lr: 0.000043  min_lr: 0.000000  loss: 4.7245 (4.6339)  loss_scale: 32768.0000 (58260.4888)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1773  max mem: 15572
Epoch: [11]  [590/892]  eta: 0:02:53  lr: 0.000043  min_lr: 0.000000  loss: 4.6829 (4.6332)  loss_scale: 32768.0000 (57829.1438)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1188  max mem: 15572
Epoch: [11]  [600/892]  eta: 0:02:47  lr: 0.000043  min_lr: 0.000000  loss: 4.6838 (4.6346)  loss_scale: 32768.0000 (57412.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1369  max mem: 15572
Epoch: [11]  [610/892]  eta: 0:02:42  lr: 0.000043  min_lr: 0.000000  loss: 4.6598 (4.6327)  loss_scale: 32768.0000 (57008.8118)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1596  max mem: 15572
Epoch: [11]  [620/892]  eta: 0:02:36  lr: 0.000043  min_lr: 0.000000  loss: 4.6269 (4.6324)  loss_scale: 32768.0000 (56618.4605)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1479  max mem: 15572
Epoch: [11]  [630/892]  eta: 0:02:30  lr: 0.000043  min_lr: 0.000000  loss: 4.7807 (4.6327)  loss_scale: 32768.0000 (56240.4818)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1494  max mem: 15572
Epoch: [11]  [640/892]  eta: 0:02:24  lr: 0.000043  min_lr: 0.000000  loss: 4.5510 (4.6321)  loss_scale: 32768.0000 (55874.2964)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0931  max mem: 15572
Epoch: [11]  [650/892]  eta: 0:02:18  lr: 0.000043  min_lr: 0.000000  loss: 4.5022 (4.6313)  loss_scale: 32768.0000 (55519.3610)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0975  max mem: 15572
Epoch: [11]  [660/892]  eta: 0:02:13  lr: 0.000043  min_lr: 0.000000  loss: 4.4792 (4.6298)  loss_scale: 32768.0000 (55175.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1156  max mem: 15572
Epoch: [11]  [670/892]  eta: 0:02:07  lr: 0.000043  min_lr: 0.000000  loss: 4.5593 (4.6304)  loss_scale: 32768.0000 (54841.2280)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0922  max mem: 15572
Epoch: [11]  [680/892]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000000  loss: 4.6181 (4.6301)  loss_scale: 32768.0000 (54517.0984)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1785  max mem: 15572
Epoch: [11]  [690/892]  eta: 0:01:55  lr: 0.000043  min_lr: 0.000000  loss: 4.6299 (4.6299)  loss_scale: 32768.0000 (54202.3502)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1565  max mem: 15572
[2025-01-13 01:24:10,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:24:10,090] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [700/892]  eta: 0:01:50  lr: 0.000043  min_lr: 0.000000  loss: 4.5411 (4.6268)  loss_scale: 32768.0000 (54036.8160)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0647  max mem: 15572
Epoch: [11]  [710/892]  eta: 0:01:44  lr: 0.000043  min_lr: 0.000000  loss: 4.5433 (4.6282)  loss_scale: 65536.0000 (54198.5485)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0768  max mem: 15572
Epoch: [11]  [720/892]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000000  loss: 4.6869 (4.6280)  loss_scale: 65536.0000 (54355.7947)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1115  max mem: 15572
Epoch: [11]  [730/892]  eta: 0:01:32  lr: 0.000043  min_lr: 0.000000  loss: 4.5836 (4.6274)  loss_scale: 65536.0000 (54508.7387)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1302  max mem: 15572
Epoch: [11]  [740/892]  eta: 0:01:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5677 (4.6272)  loss_scale: 65536.0000 (54657.5547)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.2075  max mem: 15572
Epoch: [11]  [750/892]  eta: 0:01:21  lr: 0.000043  min_lr: 0.000000  loss: 4.6196 (4.6283)  loss_scale: 65536.0000 (54802.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1870  max mem: 15572
Epoch: [11]  [760/892]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000000  loss: 4.5722 (4.6272)  loss_scale: 65536.0000 (54943.4534)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0439  max mem: 15572
Epoch: [11]  [770/892]  eta: 0:01:09  lr: 0.000043  min_lr: 0.000000  loss: 4.5722 (4.6277)  loss_scale: 65536.0000 (55080.8405)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0399  max mem: 15572
Epoch: [11]  [780/892]  eta: 0:01:04  lr: 0.000043  min_lr: 0.000000  loss: 4.7471 (4.6295)  loss_scale: 65536.0000 (55214.7093)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0366  max mem: 15572
Epoch: [11]  [790/892]  eta: 0:00:58  lr: 0.000043  min_lr: 0.000000  loss: 4.6210 (4.6281)  loss_scale: 65536.0000 (55345.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0993  max mem: 15572
Epoch: [11]  [800/892]  eta: 0:00:52  lr: 0.000043  min_lr: 0.000000  loss: 4.5401 (4.6285)  loss_scale: 65536.0000 (55472.4195)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1479  max mem: 15572
Epoch: [11]  [810/892]  eta: 0:00:46  lr: 0.000043  min_lr: 0.000000  loss: 4.6362 (4.6279)  loss_scale: 65536.0000 (55596.5080)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0651  max mem: 15572
[2025-01-13 01:25:16,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=148, lr=[4.118293260701519e-07, 4.118293260701519e-07, 5.883276086716457e-07, 5.883276086716457e-07, 8.404680123880653e-07, 8.404680123880653e-07, 1.2006685891258078e-06, 1.2006685891258078e-06, 1.7152408416082968e-06, 1.7152408416082968e-06, 2.450344059440424e-06, 2.450344059440424e-06, 3.5004915134863204e-06, 3.5004915134863204e-06, 5.0007021621233155e-06, 5.0007021621233155e-06, 7.1438602316047355e-06, 7.1438602316047355e-06, 1.0205514616578195e-05, 1.0205514616578195e-05, 1.4579306595111707e-05, 1.4579306595111707e-05, 2.0827580850159585e-05, 2.0827580850159585e-05, 2.9753686928799407e-05, 2.9753686928799407e-05, 4.2505267041142014e-05, 4.2505267041142014e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:25:16,378] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=27.831012488670446, CurrSamplesPerSec=32.616680685828634, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [820/892]  eta: 0:00:41  lr: 0.000043  min_lr: 0.000000  loss: 4.6362 (4.6279)  loss_scale: 65536.0000 (55717.5737)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0503  max mem: 15572
[2025-01-13 01:25:20,272] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:25:20,273] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:25:21,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23008
[2025-01-13 01:25:21,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:25:21,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [830/892]  eta: 0:00:35  lr: 0.000042  min_lr: 0.000000  loss: 4.7023 (4.6290)  loss_scale: 65536.0000 (55914.5897)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.1026  max mem: 15572
[2025-01-13 01:25:24,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23014
[2025-01-13 01:25:24,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:25:24,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [840/892]  eta: 0:00:29  lr: 0.000042  min_lr: 0.000000  loss: 4.7323 (4.6296)  loss_scale: 65536.0000 (55717.2889)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1318  max mem: 15572
Epoch: [11]  [850/892]  eta: 0:00:23  lr: 0.000042  min_lr: 0.000000  loss: 4.6486 (4.6293)  loss_scale: 32768.0000 (55447.6146)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0809  max mem: 15572
Epoch: [11]  [860/892]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000000  loss: 4.5804 (4.6291)  loss_scale: 32768.0000 (55184.2044)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0872  max mem: 15572
Epoch: [11]  [870/892]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000000  loss: 4.5332 (4.6279)  loss_scale: 32768.0000 (54926.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1371  max mem: 15572
Epoch: [11]  [880/892]  eta: 0:00:06  lr: 0.000042  min_lr: 0.000000  loss: 4.5557 (4.6280)  loss_scale: 32768.0000 (54675.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1027  max mem: 15572
Epoch: [11]  [890/892]  eta: 0:00:01  lr: 0.000042  min_lr: 0.000000  loss: 4.6517 (4.6275)  loss_scale: 32768.0000 (54429.4501)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0797  max mem: 15572
Epoch: [11]  [891/892]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000000  loss: 4.6324 (4.6271)  loss_scale: 32768.0000 (54405.1659)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0797  max mem: 15572
Epoch: [11] Total time: 0:08:26 (0.5683 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000000  loss: 4.6324 (4.6271)  loss_scale: 32768.0000 (54405.1659)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1333
Indices to remove: tensor([    8,   103,   120,  ..., 33688, 33695, 33706], device='cuda:0')
length of data loader train is: 781
num_training_steps_per_epoch is: 781
Change step level LR scheduler!
Set warmup steps = 3905
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:21:43  loss: 1.9889 (1.9889)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 4.7926  data: 4.5929  max mem: 15572
Val:  [ 10/272]  eta: 0:03:12  loss: 3.4894 (3.4523)  acc1: 22.2222 (24.2424)  acc5: 50.0000 (53.0303)  time: 0.7330  data: 0.5339  max mem: 15572
Val:  [ 20/272]  eta: 0:02:12  loss: 3.5350 (3.4750)  acc1: 22.2222 (24.6032)  acc5: 50.0000 (55.0265)  time: 0.3104  data: 0.1171  max mem: 15572
Val:  [ 30/272]  eta: 0:01:41  loss: 3.5710 (3.5004)  acc1: 22.2222 (24.0143)  acc5: 50.0000 (54.8387)  time: 0.2456  data: 0.0534  max mem: 15572
Val:  [ 40/272]  eta: 0:01:31  loss: 3.4516 (3.4701)  acc1: 22.2222 (24.6612)  acc5: 50.0000 (56.2331)  time: 0.2591  data: 0.0667  max mem: 15572
Val:  [ 50/272]  eta: 0:01:23  loss: 3.3921 (3.4133)  acc1: 27.7778 (26.9063)  acc5: 61.1111 (59.0414)  time: 0.3101  data: 0.1187  max mem: 15572
Val:  [ 60/272]  eta: 0:01:17  loss: 2.9546 (3.3607)  acc1: 38.8889 (28.2332)  acc5: 72.2222 (60.7468)  time: 0.3002  data: 0.0979  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 2.9680 (3.3164)  acc1: 44.4444 (31.3772)  acc5: 72.2222 (62.9108)  time: 0.3144  data: 0.1106  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 3.1162 (3.3127)  acc1: 38.8889 (31.4815)  acc5: 72.2222 (63.0316)  time: 0.3248  data: 0.1192  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.4653 (3.3358)  acc1: 27.7778 (30.8913)  acc5: 61.1111 (62.1490)  time: 0.3121  data: 0.1027  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5008 (3.3745)  acc1: 22.2222 (30.0880)  acc5: 55.5556 (61.3311)  time: 0.3042  data: 0.0952  max mem: 15572
Val:  [110/272]  eta: 0:00:54  loss: 3.5651 (3.4006)  acc1: 16.6667 (28.6286)  acc5: 55.5556 (60.4104)  time: 0.2874  data: 0.0798  max mem: 15572
Val:  [120/272]  eta: 0:00:50  loss: 3.5573 (3.4144)  acc1: 16.6667 (28.3747)  acc5: 55.5556 (60.2388)  time: 0.2750  data: 0.0800  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 3.3849 (3.4111)  acc1: 22.2222 (28.0322)  acc5: 61.1111 (59.9237)  time: 0.3184  data: 0.1155  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 3.3778 (3.4156)  acc1: 22.2222 (27.3050)  acc5: 55.5556 (59.6533)  time: 0.3099  data: 0.0972  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.3207 (3.4105)  acc1: 16.6667 (26.8580)  acc5: 55.5556 (59.4923)  time: 0.2970  data: 0.1073  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 3.3965 (3.4088)  acc1: 22.2222 (26.7426)  acc5: 55.5556 (59.6618)  time: 0.3060  data: 0.1236  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.4019 (3.4181)  acc1: 22.2222 (26.5757)  acc5: 61.1111 (59.3567)  time: 0.2989  data: 0.1016  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 3.1838 (3.4032)  acc1: 22.2222 (26.3659)  acc5: 66.6667 (60.0675)  time: 0.3133  data: 0.1091  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.2368 (3.4156)  acc1: 16.6667 (25.7999)  acc5: 66.6667 (59.3659)  time: 0.3049  data: 0.1127  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.3397 (3.4150)  acc1: 16.6667 (25.5390)  acc5: 55.5556 (59.5357)  time: 0.3172  data: 0.1399  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2137 (3.4190)  acc1: 22.2222 (25.5924)  acc5: 66.6667 (59.5313)  time: 0.3208  data: 0.1414  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.3845 (3.4152)  acc1: 27.7778 (25.8673)  acc5: 61.1111 (59.6280)  time: 0.3050  data: 0.1174  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.1580 (3.4027)  acc1: 38.8889 (27.0563)  acc5: 72.2222 (60.5099)  time: 0.3125  data: 0.1090  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0638 (3.3882)  acc1: 38.8889 (27.3167)  acc5: 77.7778 (61.2955)  time: 0.3074  data: 0.1103  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1561 (3.4025)  acc1: 22.2222 (26.9146)  acc5: 66.6667 (60.5356)  time: 0.3083  data: 0.1248  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1029 (3.3791)  acc1: 27.7778 (27.8842)  acc5: 72.2222 (61.5581)  time: 0.3073  data: 0.1240  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.1731 (3.3780)  acc1: 27.7778 (27.5933)  acc5: 72.2222 (61.5826)  time: 0.2341  data: 0.0699  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.1731 (3.3808)  acc1: 22.2222 (27.5650)  acc5: 72.2222 (61.5605)  time: 0.2276  data: 0.0698  max mem: 15572
Val: Total time: 0:01:25 (0.3155 s / it)
* Acc@1 27.565 Acc@5 61.561 loss 3.381
Accuracy of the network on the 4883 val videos: 27.6%
[2025-01-13 01:27:21,734] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:27:21,737] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:27:21,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:27:24,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:27:24,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 27.57%
Epoch: [12]  [  0/781]  eta: 1:39:22  lr: 0.000042  min_lr: 0.000000  loss: 5.1783 (5.1783)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6346  data: 7.2139  max mem: 15572
Epoch: [12]  [ 10/781]  eta: 0:15:04  lr: 0.000042  min_lr: 0.000000  loss: 4.6902 (4.7272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1726  data: 0.7257  max mem: 15572
Epoch: [12]  [ 20/781]  eta: 0:10:42  lr: 0.000042  min_lr: 0.000000  loss: 4.6551 (4.6416)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0568  max mem: 15572
Epoch: [12]  [ 30/781]  eta: 0:09:54  lr: 0.000042  min_lr: 0.000000  loss: 4.5780 (4.6452)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1381  max mem: 15572
Epoch: [12]  [ 40/781]  eta: 0:08:57  lr: 0.000042  min_lr: 0.000000  loss: 4.5780 (4.6112)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1718  max mem: 15572
Epoch: [12]  [ 50/781]  eta: 0:08:27  lr: 0.000042  min_lr: 0.000000  loss: 4.6073 (4.5994)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1172  max mem: 15572
Epoch: [12]  [ 60/781]  eta: 0:08:01  lr: 0.000042  min_lr: 0.000000  loss: 4.6073 (4.5950)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1124  max mem: 15572
[2025-01-13 01:28:11,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:28:11,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 70/781]  eta: 0:07:50  lr: 0.000042  min_lr: 0.000000  loss: 4.6133 (4.5919)  loss_scale: 32768.0000 (33229.5211)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1287  max mem: 15572
Epoch: [12]  [ 80/781]  eta: 0:07:36  lr: 0.000042  min_lr: 0.000000  loss: 4.6645 (4.5927)  loss_scale: 65536.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1570  max mem: 15572
Epoch: [12]  [ 90/781]  eta: 0:07:16  lr: 0.000042  min_lr: 0.000000  loss: 4.6751 (4.5983)  loss_scale: 65536.0000 (40329.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0987  max mem: 15572
Epoch: [12]  [100/781]  eta: 0:07:09  lr: 0.000042  min_lr: 0.000000  loss: 4.5415 (4.5932)  loss_scale: 65536.0000 (42825.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0998  max mem: 15572
Epoch: [12]  [110/781]  eta: 0:06:59  lr: 0.000042  min_lr: 0.000000  loss: 4.5876 (4.5922)  loss_scale: 65536.0000 (44871.4955)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1382  max mem: 15572
Epoch: [12]  [120/781]  eta: 0:06:50  lr: 0.000042  min_lr: 0.000000  loss: 4.6645 (4.5993)  loss_scale: 65536.0000 (46579.3058)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1194  max mem: 15572
Epoch: [12]  [130/781]  eta: 0:06:35  lr: 0.000042  min_lr: 0.000000  loss: 4.6676 (4.5970)  loss_scale: 65536.0000 (48026.3817)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0580  max mem: 15572
Epoch: [12]  [140/781]  eta: 0:06:29  lr: 0.000042  min_lr: 0.000000  loss: 4.6676 (4.6084)  loss_scale: 65536.0000 (49268.1986)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0966  max mem: 15572
Epoch: [12]  [150/781]  eta: 0:06:21  lr: 0.000042  min_lr: 0.000000  loss: 4.7440 (4.6169)  loss_scale: 65536.0000 (50345.5364)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1604  max mem: 15572
Epoch: [12]  [160/781]  eta: 0:06:13  lr: 0.000042  min_lr: 0.000000  loss: 4.6261 (4.6165)  loss_scale: 65536.0000 (51289.0435)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1341  max mem: 15572
Epoch: [12]  [170/781]  eta: 0:06:02  lr: 0.000042  min_lr: 0.000000  loss: 4.5913 (4.6199)  loss_scale: 65536.0000 (52122.1988)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0906  max mem: 15572
Epoch: [12]  [180/781]  eta: 0:05:57  lr: 0.000042  min_lr: 0.000000  loss: 4.6125 (4.6213)  loss_scale: 65536.0000 (52863.2928)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1130  max mem: 15572
Epoch: [12]  [190/781]  eta: 0:05:51  lr: 0.000042  min_lr: 0.000000  loss: 4.6369 (4.6236)  loss_scale: 65536.0000 (53526.7853)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1556  max mem: 15572
[2025-01-13 01:29:22,856] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:29:22,856] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [200/781]  eta: 0:05:44  lr: 0.000042  min_lr: 0.000000  loss: 4.6197 (4.6242)  loss_scale: 65536.0000 (55102.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1241  max mem: 15572
[2025-01-13 01:29:24,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23274
[2025-01-13 01:29:24,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:29:24,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [210/781]  eta: 0:05:38  lr: 0.000042  min_lr: 0.000000  loss: 4.5767 (4.6231)  loss_scale: 65536.0000 (55596.8910)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1305  max mem: 15572
Epoch: [12]  [220/781]  eta: 0:05:30  lr: 0.000042  min_lr: 0.000000  loss: 4.5250 (4.6204)  loss_scale: 65536.0000 (56046.6244)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1069  max mem: 15572
Epoch: [12]  [230/781]  eta: 0:05:27  lr: 0.000042  min_lr: 0.000000  loss: 4.5250 (4.6198)  loss_scale: 65536.0000 (56457.4199)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1684  max mem: 15572
Epoch: [12]  [240/781]  eta: 0:05:21  lr: 0.000042  min_lr: 0.000000  loss: 4.6866 (4.6262)  loss_scale: 65536.0000 (56834.1245)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1964  max mem: 15572
Epoch: [12]  [250/781]  eta: 0:05:12  lr: 0.000042  min_lr: 0.000000  loss: 4.6022 (4.6247)  loss_scale: 65536.0000 (57180.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0698  max mem: 15572
Epoch: [12]  [260/781]  eta: 0:05:05  lr: 0.000042  min_lr: 0.000000  loss: 4.5391 (4.6238)  loss_scale: 65536.0000 (57500.9349)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0543  max mem: 15572
Epoch: [12]  [270/781]  eta: 0:04:57  lr: 0.000042  min_lr: 0.000000  loss: 4.5901 (4.6267)  loss_scale: 65536.0000 (57797.4317)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0579  max mem: 15572
Epoch: [12]  [280/781]  eta: 0:04:51  lr: 0.000042  min_lr: 0.000000  loss: 4.7203 (4.6302)  loss_scale: 65536.0000 (58072.8256)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0616  max mem: 15572
Epoch: [12]  [290/781]  eta: 0:04:46  lr: 0.000042  min_lr: 0.000000  loss: 4.6775 (4.6325)  loss_scale: 65536.0000 (58329.2921)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1365  max mem: 15572
Epoch: [12]  [300/781]  eta: 0:04:40  lr: 0.000042  min_lr: 0.000000  loss: 4.6014 (4.6296)  loss_scale: 65536.0000 (58568.7176)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1562  max mem: 15572
Epoch: [12]  [310/781]  eta: 0:04:35  lr: 0.000042  min_lr: 0.000000  loss: 4.5548 (4.6300)  loss_scale: 65536.0000 (58792.7460)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1854  max mem: 15572
Epoch: [12]  [320/781]  eta: 0:04:28  lr: 0.000042  min_lr: 0.000000  loss: 4.5928 (4.6293)  loss_scale: 65536.0000 (59002.8162)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1585  max mem: 15572
[2025-01-13 01:30:37,698] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:30:37,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [330/781]  eta: 0:04:23  lr: 0.000042  min_lr: 0.000000  loss: 4.5592 (4.6247)  loss_scale: 65536.0000 (59398.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1275  max mem: 15572
[2025-01-13 01:30:38,235] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23404
[2025-01-13 01:30:38,235] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:30:38,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [340/781]  eta: 0:04:18  lr: 0.000042  min_lr: 0.000000  loss: 4.5939 (4.6259)  loss_scale: 65536.0000 (59578.1818)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1890  max mem: 15572
Epoch: [12]  [350/781]  eta: 0:04:11  lr: 0.000042  min_lr: 0.000000  loss: 4.6764 (4.6252)  loss_scale: 65536.0000 (59747.9202)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1341  max mem: 15572
Epoch: [12]  [360/781]  eta: 0:04:05  lr: 0.000042  min_lr: 0.000000  loss: 4.7358 (4.6264)  loss_scale: 65536.0000 (59908.2548)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0817  max mem: 15572
Epoch: [12]  [370/781]  eta: 0:03:58  lr: 0.000042  min_lr: 0.000000  loss: 4.6712 (4.6232)  loss_scale: 65536.0000 (60059.9461)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0875  max mem: 15572
Epoch: [12]  [380/781]  eta: 0:03:52  lr: 0.000042  min_lr: 0.000000  loss: 4.5744 (4.6245)  loss_scale: 65536.0000 (60203.6745)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0825  max mem: 15572
Epoch: [12]  [390/781]  eta: 0:03:45  lr: 0.000042  min_lr: 0.000000  loss: 4.7157 (4.6264)  loss_scale: 65536.0000 (60340.0512)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0758  max mem: 15572
Epoch: [12]  [400/781]  eta: 0:03:40  lr: 0.000042  min_lr: 0.000000  loss: 4.6109 (4.6270)  loss_scale: 65536.0000 (60469.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1154  max mem: 15572
Epoch: [12]  [410/781]  eta: 0:03:35  lr: 0.000042  min_lr: 0.000000  loss: 4.6829 (4.6283)  loss_scale: 65536.0000 (60592.8954)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1573  max mem: 15572
Epoch: [12]  [420/781]  eta: 0:03:28  lr: 0.000042  min_lr: 0.000000  loss: 4.6829 (4.6258)  loss_scale: 65536.0000 (60710.3088)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0805  max mem: 15572
Epoch: [12]  [430/781]  eta: 0:03:22  lr: 0.000042  min_lr: 0.000000  loss: 4.5880 (4.6254)  loss_scale: 65536.0000 (60822.2738)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1031  max mem: 15572
Epoch: [12]  [440/781]  eta: 0:03:16  lr: 0.000042  min_lr: 0.000000  loss: 4.5891 (4.6248)  loss_scale: 65536.0000 (60929.1610)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1493  max mem: 15572
Epoch: [12]  [450/781]  eta: 0:03:11  lr: 0.000042  min_lr: 0.000000  loss: 4.5891 (4.6244)  loss_scale: 65536.0000 (61031.3082)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1179  max mem: 15572
[2025-01-13 01:31:50,385] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:31:50,386] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [460/781]  eta: 0:03:05  lr: 0.000042  min_lr: 0.000000  loss: 4.5798 (4.6247)  loss_scale: 65536.0000 (61271.1844)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1233  max mem: 15572
[2025-01-13 01:31:54,875] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23540
[2025-01-13 01:31:54,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:31:54,876] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [470/781]  eta: 0:02:59  lr: 0.000042  min_lr: 0.000000  loss: 4.5798 (4.6233)  loss_scale: 65536.0000 (62196.5860)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1253  max mem: 15572
Epoch: [12]  [480/781]  eta: 0:02:53  lr: 0.000042  min_lr: 0.000000  loss: 4.4969 (4.6210)  loss_scale: 65536.0000 (62266.0125)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1096  max mem: 15572
Epoch: [12]  [490/781]  eta: 0:02:47  lr: 0.000042  min_lr: 0.000000  loss: 4.5815 (4.6237)  loss_scale: 65536.0000 (62332.6110)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1207  max mem: 15572
Epoch: [12]  [500/781]  eta: 0:02:42  lr: 0.000042  min_lr: 0.000000  loss: 4.7439 (4.6252)  loss_scale: 65536.0000 (62396.5509)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1665  max mem: 15572
Epoch: [12]  [510/781]  eta: 0:02:36  lr: 0.000042  min_lr: 0.000000  loss: 4.5887 (4.6250)  loss_scale: 65536.0000 (62457.9883)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1487  max mem: 15572
Epoch: [12]  [520/781]  eta: 0:02:30  lr: 0.000042  min_lr: 0.000000  loss: 4.5951 (4.6249)  loss_scale: 65536.0000 (62517.0672)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1587  max mem: 15572
Epoch: [12]  [530/781]  eta: 0:02:24  lr: 0.000042  min_lr: 0.000000  loss: 4.6668 (4.6264)  loss_scale: 65536.0000 (62573.9209)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1317  max mem: 15572
Epoch: [12]  [540/781]  eta: 0:02:19  lr: 0.000042  min_lr: 0.000000  loss: 4.6409 (4.6266)  loss_scale: 65536.0000 (62628.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1204  max mem: 15572
Epoch: [12]  [550/781]  eta: 0:02:13  lr: 0.000041  min_lr: 0.000000  loss: 4.6727 (4.6283)  loss_scale: 65536.0000 (62681.4374)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1846  max mem: 15572
Epoch: [12]  [560/781]  eta: 0:02:07  lr: 0.000041  min_lr: 0.000000  loss: 4.6586 (4.6264)  loss_scale: 65536.0000 (62732.3209)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1354  max mem: 15572
Epoch: [12]  [570/781]  eta: 0:02:02  lr: 0.000041  min_lr: 0.000000  loss: 4.5468 (4.6247)  loss_scale: 65536.0000 (62781.4221)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1342  max mem: 15572
Epoch: [12]  [580/781]  eta: 0:01:56  lr: 0.000041  min_lr: 0.000000  loss: 4.5499 (4.6242)  loss_scale: 65536.0000 (62828.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1536  max mem: 15572
Epoch: [12]  [590/781]  eta: 0:01:50  lr: 0.000041  min_lr: 0.000000  loss: 4.6089 (4.6240)  loss_scale: 65536.0000 (62874.6396)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1455  max mem: 15572
[2025-01-13 01:33:10,227] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:33:10,228] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:33:11,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23671
[2025-01-13 01:33:11,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:33:11,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [600/781]  eta: 0:01:44  lr: 0.000041  min_lr: 0.000000  loss: 4.7540 (4.6245)  loss_scale: 65536.0000 (63137.0116)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1356  max mem: 15572
Epoch: [12]  [610/781]  eta: 0:01:38  lr: 0.000041  min_lr: 0.000000  loss: 4.7285 (4.6255)  loss_scale: 65536.0000 (63176.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1028  max mem: 15572
Epoch: [12]  [620/781]  eta: 0:01:32  lr: 0.000041  min_lr: 0.000000  loss: 4.6021 (4.6244)  loss_scale: 65536.0000 (63214.2738)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0628  max mem: 15572
Epoch: [12]  [630/781]  eta: 0:01:27  lr: 0.000041  min_lr: 0.000000  loss: 4.5609 (4.6239)  loss_scale: 65536.0000 (63251.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1077  max mem: 15572
Epoch: [12]  [640/781]  eta: 0:01:21  lr: 0.000041  min_lr: 0.000000  loss: 4.6871 (4.6251)  loss_scale: 65536.0000 (63286.7145)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1784  max mem: 15572
Epoch: [12]  [650/781]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000000  loss: 4.7595 (4.6267)  loss_scale: 65536.0000 (63321.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1527  max mem: 15572
Epoch: [12]  [660/781]  eta: 0:01:09  lr: 0.000041  min_lr: 0.000000  loss: 4.7319 (4.6275)  loss_scale: 65536.0000 (63354.7716)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1321  max mem: 15572
Epoch: [12]  [670/781]  eta: 0:01:03  lr: 0.000041  min_lr: 0.000000  loss: 4.6278 (4.6281)  loss_scale: 65536.0000 (63387.2787)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0823  max mem: 15572
Epoch: [12]  [680/781]  eta: 0:00:58  lr: 0.000041  min_lr: 0.000000  loss: 4.5943 (4.6272)  loss_scale: 65536.0000 (63418.8311)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0881  max mem: 15572
[2025-01-13 01:34:02,197] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23761
[2025-01-13 01:34:02,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:34:02,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [690/781]  eta: 0:00:52  lr: 0.000041  min_lr: 0.000000  loss: 4.5176 (4.6286)  loss_scale: 65536.0000 (63307.2069)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1667  max mem: 15572
Epoch: [12]  [700/781]  eta: 0:00:46  lr: 0.000041  min_lr: 0.000000  loss: 4.5176 (4.6264)  loss_scale: 32768.0000 (62871.5549)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1214  max mem: 15572
Epoch: [12]  [710/781]  eta: 0:00:40  lr: 0.000041  min_lr: 0.000000  loss: 4.5096 (4.6255)  loss_scale: 32768.0000 (62448.1575)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1261  max mem: 15572
Epoch: [12]  [720/781]  eta: 0:00:35  lr: 0.000041  min_lr: 0.000000  loss: 4.5069 (4.6244)  loss_scale: 32768.0000 (62036.5049)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.2118  max mem: 15572
Epoch: [12]  [730/781]  eta: 0:00:29  lr: 0.000041  min_lr: 0.000000  loss: 4.6468 (4.6245)  loss_scale: 32768.0000 (61636.1149)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1551  max mem: 15572
Epoch: [12]  [740/781]  eta: 0:00:23  lr: 0.000041  min_lr: 0.000000  loss: 4.6991 (4.6243)  loss_scale: 32768.0000 (61246.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0727  max mem: 15572
Epoch: [12]  [750/781]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000000  loss: 4.5905 (4.6237)  loss_scale: 32768.0000 (60867.3236)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0935  max mem: 15572
Epoch: [12]  [760/781]  eta: 0:00:12  lr: 0.000041  min_lr: 0.000000  loss: 4.5817 (4.6239)  loss_scale: 32768.0000 (60498.0815)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0934  max mem: 15572
Epoch: [12]  [770/781]  eta: 0:00:06  lr: 0.000041  min_lr: 0.000000  loss: 4.6472 (4.6231)  loss_scale: 32768.0000 (60138.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0582  max mem: 15572
Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.6472 (4.6246)  loss_scale: 32768.0000 (59787.9641)  weight_decay: 0.0500 (0.0500)  time: 0.4516  data: 0.0395  max mem: 15572
Epoch: [12] Total time: 0:07:27 (0.5728 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.6472 (4.6246)  loss_scale: 32768.0000 (59787.9641)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 1150
Indices to remove: tensor([   70,   101,   164,  ..., 33612, 33693, 33701], device='cuda:0')
length of data loader train is: 685
num_training_steps_per_epoch is: 685
Change step level LR scheduler!
Set warmup steps = 3425
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:55  loss: 1.8509 (1.8509)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 4.3935  data: 4.1536  max mem: 15572
Val:  [ 10/272]  eta: 0:03:14  loss: 3.6570 (3.5131)  acc1: 16.6667 (23.7374)  acc5: 44.4444 (51.0101)  time: 0.7420  data: 0.5450  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 3.5524 (3.4032)  acc1: 22.2222 (27.7778)  acc5: 50.0000 (57.6720)  time: 0.3218  data: 0.1337  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.4219 (3.4802)  acc1: 27.7778 (24.9104)  acc5: 55.5556 (54.3011)  time: 0.2661  data: 0.0708  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.3482 (3.4325)  acc1: 22.2222 (24.6612)  acc5: 55.5556 (56.9106)  time: 0.2860  data: 0.0865  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.2626 (3.3829)  acc1: 27.7778 (25.8170)  acc5: 55.5556 (58.9325)  time: 0.3125  data: 0.1163  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 3.0164 (3.3140)  acc1: 33.3333 (28.6885)  acc5: 72.2222 (61.4754)  time: 0.3089  data: 0.1121  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 3.0164 (3.2897)  acc1: 38.8889 (29.8122)  acc5: 72.2222 (63.3020)  time: 0.2815  data: 0.0872  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.2027 (3.3131)  acc1: 27.7778 (29.1495)  acc5: 66.6667 (62.6886)  time: 0.3102  data: 0.1155  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.5372 (3.3448)  acc1: 22.2222 (28.8156)  acc5: 55.5556 (61.5385)  time: 0.3444  data: 0.1622  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.5472 (3.3786)  acc1: 22.2222 (28.1078)  acc5: 55.5556 (60.8911)  time: 0.3197  data: 0.1344  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6899 (3.4075)  acc1: 11.1111 (26.5265)  acc5: 50.0000 (60.2102)  time: 0.3187  data: 0.1270  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5849 (3.4186)  acc1: 16.6667 (26.7218)  acc5: 50.0000 (60.0551)  time: 0.3508  data: 0.1527  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.3423 (3.3955)  acc1: 33.3333 (27.7354)  acc5: 61.1111 (60.5174)  time: 0.3403  data: 0.1228  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1984 (3.3949)  acc1: 33.3333 (27.4232)  acc5: 61.1111 (60.5989)  time: 0.3194  data: 0.1131  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.1010 (3.3821)  acc1: 16.6667 (27.4834)  acc5: 61.1111 (60.8536)  time: 0.3119  data: 0.1252  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.2295 (3.3775)  acc1: 22.2222 (27.4672)  acc5: 66.6667 (61.1111)  time: 0.2997  data: 0.1073  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3007 (3.3865)  acc1: 22.2222 (27.0630)  acc5: 61.1111 (60.6888)  time: 0.2900  data: 0.0983  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.2121 (3.3748)  acc1: 16.6667 (26.5807)  acc5: 61.1111 (61.1111)  time: 0.2949  data: 0.0931  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.4221 (3.3918)  acc1: 11.1111 (26.1489)  acc5: 55.5556 (60.1803)  time: 0.3263  data: 0.1183  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4295 (3.3874)  acc1: 11.1111 (26.1194)  acc5: 44.4444 (60.3096)  time: 0.3146  data: 0.1253  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.4435 (3.4023)  acc1: 16.6667 (25.7767)  acc5: 55.5556 (59.9789)  time: 0.2870  data: 0.1071  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5421 (3.4028)  acc1: 22.2222 (25.9175)  acc5: 55.5556 (60.0553)  time: 0.2902  data: 0.1132  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2963 (3.3993)  acc1: 27.7778 (26.3588)  acc5: 66.6667 (60.5339)  time: 0.3454  data: 0.1558  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0684 (3.3835)  acc1: 38.8889 (26.9710)  acc5: 77.7778 (61.4338)  time: 0.3272  data: 0.1310  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0945 (3.3938)  acc1: 33.3333 (26.5383)  acc5: 72.2222 (60.8455)  time: 0.2877  data: 0.0921  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1424 (3.3702)  acc1: 38.8889 (27.7565)  acc5: 72.2222 (61.8135)  time: 0.3085  data: 0.1109  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3426 (3.3776)  acc1: 33.3333 (27.2653)  acc5: 61.1111 (61.3366)  time: 0.2270  data: 0.0540  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3426 (3.3794)  acc1: 22.2222 (27.2578)  acc5: 61.1111 (61.3352)  time: 0.2212  data: 0.0540  max mem: 15572
Val: Total time: 0:01:27 (0.3212 s / it)
* Acc@1 27.258 Acc@5 61.335 loss 3.379
Accuracy of the network on the 4883 val videos: 27.3%
Max accuracy: 27.57%
Epoch: [13]  [  0/685]  eta: 1:26:17  lr: 0.000041  min_lr: 0.000000  loss: 4.6177 (4.6177)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.5586  data: 7.0765  max mem: 15572
Epoch: [13]  [ 10/685]  eta: 0:14:37  lr: 0.000041  min_lr: 0.000000  loss: 4.6775 (4.7283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2993  data: 0.8505  max mem: 15572
Epoch: [13]  [ 20/685]  eta: 0:09:53  lr: 0.000041  min_lr: 0.000000  loss: 4.7004 (4.7057)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1142  max mem: 15572
Epoch: [13]  [ 30/685]  eta: 0:08:28  lr: 0.000041  min_lr: 0.000000  loss: 4.5492 (4.6568)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0429  max mem: 15572
[2025-01-13 01:36:47,047] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:36:47,048] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 40/685]  eta: 0:07:46  lr: 0.000041  min_lr: 0.000000  loss: 4.5800 (4.6644)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.0946  max mem: 15572
Epoch: [13]  [ 50/685]  eta: 0:07:22  lr: 0.000041  min_lr: 0.000000  loss: 4.6679 (4.6548)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1303  max mem: 15572
Epoch: [13]  [ 60/685]  eta: 0:07:04  lr: 0.000041  min_lr: 0.000000  loss: 4.6894 (4.6600)  loss_scale: 65536.0000 (46197.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1546  max mem: 15572
Epoch: [13]  [ 70/685]  eta: 0:06:45  lr: 0.000041  min_lr: 0.000000  loss: 4.7047 (4.6632)  loss_scale: 65536.0000 (48921.2394)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1287  max mem: 15572
Epoch: [13]  [ 80/685]  eta: 0:06:33  lr: 0.000041  min_lr: 0.000000  loss: 4.6664 (4.6652)  loss_scale: 65536.0000 (50972.4444)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1158  max mem: 15572
Epoch: [13]  [ 90/685]  eta: 0:06:25  lr: 0.000041  min_lr: 0.000000  loss: 4.5995 (4.6498)  loss_scale: 65536.0000 (52572.8352)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1485  max mem: 15572
Epoch: [13]  [100/685]  eta: 0:06:15  lr: 0.000041  min_lr: 0.000000  loss: 4.6701 (4.6667)  loss_scale: 65536.0000 (53856.3168)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1650  max mem: 15572
Epoch: [13]  [110/685]  eta: 0:06:04  lr: 0.000041  min_lr: 0.000000  loss: 4.7105 (4.6731)  loss_scale: 65536.0000 (54908.5405)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1433  max mem: 15572
Epoch: [13]  [120/685]  eta: 0:05:54  lr: 0.000041  min_lr: 0.000000  loss: 4.6631 (4.6742)  loss_scale: 65536.0000 (55786.8430)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1189  max mem: 15572
Epoch: [13]  [130/685]  eta: 0:05:44  lr: 0.000041  min_lr: 0.000000  loss: 4.6631 (4.6702)  loss_scale: 65536.0000 (56531.0534)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.1052  max mem: 15572
Epoch: [13]  [140/685]  eta: 0:05:39  lr: 0.000041  min_lr: 0.000000  loss: 4.5948 (4.6636)  loss_scale: 65536.0000 (57169.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1557  max mem: 15572
[2025-01-13 01:37:49,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=155, lr=[3.9528328583329935e-07, 3.9528328583329935e-07, 5.646904083332848e-07, 5.646904083332848e-07, 8.067005833332642e-07, 8.067005833332642e-07, 1.152429404761806e-06, 1.152429404761806e-06, 1.6463277210882944e-06, 1.6463277210882944e-06, 2.3518967444118494e-06, 2.3518967444118494e-06, 3.3598524920169274e-06, 3.3598524920169274e-06, 4.799789274309897e-06, 4.799789274309897e-06, 6.85684182044271e-06, 6.85684182044271e-06, 9.795488314918158e-06, 9.795488314918158e-06, 1.3993554735597369e-05, 1.3993554735597369e-05, 1.9990792479424815e-05, 1.9990792479424815e-05, 2.8558274970606878e-05, 2.8558274970606878e-05, 4.0797535672295545e-05, 4.0797535672295545e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:37:49,783] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=27.867210721426588, CurrSamplesPerSec=31.57770948956583, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [150/685]  eta: 0:05:32  lr: 0.000041  min_lr: 0.000000  loss: 4.6283 (4.6652)  loss_scale: 65536.0000 (57723.7616)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1874  max mem: 15572
Epoch: [13]  [160/685]  eta: 0:05:21  lr: 0.000041  min_lr: 0.000000  loss: 4.6811 (4.6672)  loss_scale: 65536.0000 (58208.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0819  max mem: 15572
[2025-01-13 01:37:59,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:37:59,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:38:00,089] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24019
[2025-01-13 01:38:00,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:38:00,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [170/685]  eta: 0:05:10  lr: 0.000041  min_lr: 0.000000  loss: 4.6102 (4.6608)  loss_scale: 65536.0000 (59020.7251)  weight_decay: 0.0500 (0.0500)  time: 0.4571  data: 0.0006  max mem: 15572
Epoch: [13]  [180/685]  eta: 0:05:03  lr: 0.000041  min_lr: 0.000000  loss: 4.5466 (4.6529)  loss_scale: 65536.0000 (59380.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0717  max mem: 15572
Epoch: [13]  [190/685]  eta: 0:04:57  lr: 0.000041  min_lr: 0.000000  loss: 4.6347 (4.6536)  loss_scale: 65536.0000 (59702.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1587  max mem: 15572
Epoch: [13]  [200/685]  eta: 0:04:52  lr: 0.000041  min_lr: 0.000000  loss: 4.6716 (4.6564)  loss_scale: 65536.0000 (59993.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1697  max mem: 15572
Epoch: [13]  [210/685]  eta: 0:04:45  lr: 0.000041  min_lr: 0.000000  loss: 4.7152 (4.6564)  loss_scale: 65536.0000 (60255.8483)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1162  max mem: 15572
Epoch: [13]  [220/685]  eta: 0:04:36  lr: 0.000041  min_lr: 0.000000  loss: 4.6085 (4.6556)  loss_scale: 65536.0000 (60494.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0469  max mem: 15572
Epoch: [13]  [230/685]  eta: 0:04:31  lr: 0.000041  min_lr: 0.000000  loss: 4.6707 (4.6575)  loss_scale: 65536.0000 (60713.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1175  max mem: 15572
Epoch: [13]  [240/685]  eta: 0:04:24  lr: 0.000041  min_lr: 0.000000  loss: 4.7184 (4.6599)  loss_scale: 65536.0000 (60913.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1400  max mem: 15572
Epoch: [13]  [250/685]  eta: 0:04:17  lr: 0.000041  min_lr: 0.000000  loss: 4.6755 (4.6588)  loss_scale: 65536.0000 (61097.3068)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0736  max mem: 15572
Epoch: [13]  [260/685]  eta: 0:04:09  lr: 0.000041  min_lr: 0.000000  loss: 4.6755 (4.6617)  loss_scale: 65536.0000 (61267.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0498  max mem: 15572
Epoch: [13]  [270/685]  eta: 0:04:04  lr: 0.000041  min_lr: 0.000000  loss: 4.6426 (4.6569)  loss_scale: 65536.0000 (61424.8856)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.0998  max mem: 15572
Epoch: [13]  [280/685]  eta: 0:03:58  lr: 0.000041  min_lr: 0.000000  loss: 4.6393 (4.6578)  loss_scale: 65536.0000 (61571.1886)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1569  max mem: 15572
Epoch: [13]  [290/685]  eta: 0:03:51  lr: 0.000040  min_lr: 0.000000  loss: 4.6479 (4.6594)  loss_scale: 65536.0000 (61707.4364)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1301  max mem: 15572
[2025-01-13 01:39:12,625] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:39:12,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:39:14,826] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24152
[2025-01-13 01:39:14,827] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:39:14,827] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [300/685]  eta: 0:03:45  lr: 0.000040  min_lr: 0.000000  loss: 4.6955 (4.6592)  loss_scale: 65536.0000 (62705.5415)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1154  max mem: 15572
Epoch: [13]  [310/685]  eta: 0:03:39  lr: 0.000040  min_lr: 0.000000  loss: 4.7509 (4.6620)  loss_scale: 65536.0000 (62796.5531)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1337  max mem: 15572
[2025-01-13 01:39:22,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24166
[2025-01-13 01:39:22,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:39:22,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [320/685]  eta: 0:03:34  lr: 0.000040  min_lr: 0.000000  loss: 4.7043 (4.6598)  loss_scale: 65536.0000 (61963.1651)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1692  max mem: 15572
Epoch: [13]  [330/685]  eta: 0:03:27  lr: 0.000040  min_lr: 0.000000  loss: 4.7378 (4.6646)  loss_scale: 32768.0000 (61081.1360)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0962  max mem: 15572
Epoch: [13]  [340/685]  eta: 0:03:21  lr: 0.000040  min_lr: 0.000000  loss: 4.6779 (4.6631)  loss_scale: 32768.0000 (60250.8387)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0909  max mem: 15572
Epoch: [13]  [350/685]  eta: 0:03:15  lr: 0.000040  min_lr: 0.000000  loss: 4.6443 (4.6619)  loss_scale: 32768.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1468  max mem: 15572
Epoch: [13]  [360/685]  eta: 0:03:09  lr: 0.000040  min_lr: 0.000000  loss: 4.7497 (4.6642)  loss_scale: 32768.0000 (58728.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1153  max mem: 15572
Epoch: [13]  [370/685]  eta: 0:03:03  lr: 0.000040  min_lr: 0.000000  loss: 4.8090 (4.6649)  loss_scale: 32768.0000 (58028.5067)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1123  max mem: 15572
Epoch: [13]  [380/685]  eta: 0:02:57  lr: 0.000040  min_lr: 0.000000  loss: 4.6439 (4.6637)  loss_scale: 32768.0000 (57365.5013)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1046  max mem: 15572
Epoch: [13]  [390/685]  eta: 0:02:50  lr: 0.000040  min_lr: 0.000000  loss: 4.5813 (4.6657)  loss_scale: 32768.0000 (56736.4092)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0884  max mem: 15572
Epoch: [13]  [400/685]  eta: 0:02:45  lr: 0.000040  min_lr: 0.000000  loss: 4.6481 (4.6648)  loss_scale: 32768.0000 (56138.6933)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1294  max mem: 15572
Epoch: [13]  [410/685]  eta: 0:02:39  lr: 0.000040  min_lr: 0.000000  loss: 4.7865 (4.6672)  loss_scale: 32768.0000 (55570.0633)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1732  max mem: 15572
Epoch: [13]  [420/685]  eta: 0:02:34  lr: 0.000040  min_lr: 0.000000  loss: 4.7936 (4.6671)  loss_scale: 32768.0000 (55028.4466)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1967  max mem: 15572
Epoch: [13]  [430/685]  eta: 0:02:28  lr: 0.000040  min_lr: 0.000000  loss: 4.6564 (4.6648)  loss_scale: 32768.0000 (54511.9629)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1503  max mem: 15572
Epoch: [13]  [440/685]  eta: 0:02:22  lr: 0.000040  min_lr: 0.000000  loss: 4.6011 (4.6623)  loss_scale: 32768.0000 (54018.9025)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1357  max mem: 15572
[2025-01-13 01:40:36,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:40:36,653] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [450/685]  eta: 0:02:16  lr: 0.000040  min_lr: 0.000000  loss: 4.6055 (4.6627)  loss_scale: 32768.0000 (54274.2705)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1513  max mem: 15572
Epoch: [13]  [460/685]  eta: 0:02:10  lr: 0.000040  min_lr: 0.000000  loss: 4.7523 (4.6653)  loss_scale: 65536.0000 (54518.5597)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0964  max mem: 15572
Epoch: [13]  [470/685]  eta: 0:02:04  lr: 0.000040  min_lr: 0.000000  loss: 4.6593 (4.6640)  loss_scale: 65536.0000 (54752.4756)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1074  max mem: 15572
Epoch: [13]  [480/685]  eta: 0:01:58  lr: 0.000040  min_lr: 0.000000  loss: 4.6084 (4.6638)  loss_scale: 65536.0000 (54976.6653)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0880  max mem: 15572
Epoch: [13]  [490/685]  eta: 0:01:52  lr: 0.000040  min_lr: 0.000000  loss: 4.6973 (4.6651)  loss_scale: 65536.0000 (55191.7230)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0413  max mem: 15572
Epoch: [13]  [500/685]  eta: 0:01:46  lr: 0.000040  min_lr: 0.000000  loss: 4.7140 (4.6637)  loss_scale: 65536.0000 (55398.1956)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0744  max mem: 15572
Epoch: [13]  [510/685]  eta: 0:01:41  lr: 0.000040  min_lr: 0.000000  loss: 4.6731 (4.6635)  loss_scale: 65536.0000 (55596.5871)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1615  max mem: 15572
Epoch: [13]  [520/685]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000000  loss: 4.5492 (4.6612)  loss_scale: 65536.0000 (55787.3628)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1333  max mem: 15572
Epoch: [13]  [530/685]  eta: 0:01:29  lr: 0.000040  min_lr: 0.000000  loss: 4.4838 (4.6580)  loss_scale: 65536.0000 (55970.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1094  max mem: 15572
Epoch: [13]  [540/685]  eta: 0:01:23  lr: 0.000040  min_lr: 0.000000  loss: 4.5444 (4.6575)  loss_scale: 65536.0000 (56147.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1325  max mem: 15572
Epoch: [13]  [550/685]  eta: 0:01:17  lr: 0.000040  min_lr: 0.000000  loss: 4.6618 (4.6568)  loss_scale: 65536.0000 (56318.1416)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1065  max mem: 15572
Epoch: [13]  [560/685]  eta: 0:01:11  lr: 0.000040  min_lr: 0.000000  loss: 4.7174 (4.6584)  loss_scale: 65536.0000 (56482.4528)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0641  max mem: 15572
[2025-01-13 01:41:47,619] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:41:47,620] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:41:48,049] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24424
[2025-01-13 01:41:48,049] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:41:48,049] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [570/685]  eta: 0:01:06  lr: 0.000040  min_lr: 0.000000  loss: 4.6367 (4.6546)  loss_scale: 65536.0000 (56755.7828)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0910  max mem: 15572
Epoch: [13]  [580/685]  eta: 0:01:00  lr: 0.000040  min_lr: 0.000000  loss: 4.5334 (4.6539)  loss_scale: 65536.0000 (56906.9053)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1366  max mem: 15572
Epoch: [13]  [590/685]  eta: 0:00:54  lr: 0.000040  min_lr: 0.000000  loss: 4.5359 (4.6512)  loss_scale: 65536.0000 (57052.9137)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1358  max mem: 15572
Epoch: [13]  [600/685]  eta: 0:00:49  lr: 0.000040  min_lr: 0.000000  loss: 4.4099 (4.6475)  loss_scale: 65536.0000 (57194.0632)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1523  max mem: 15572
Epoch: [13]  [610/685]  eta: 0:00:43  lr: 0.000040  min_lr: 0.000000  loss: 4.5655 (4.6479)  loss_scale: 65536.0000 (57330.5925)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1524  max mem: 15572
Epoch: [13]  [620/685]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000000  loss: 4.6593 (4.6471)  loss_scale: 65536.0000 (57462.7246)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1226  max mem: 15572
Epoch: [13]  [630/685]  eta: 0:00:31  lr: 0.000040  min_lr: 0.000000  loss: 4.6538 (4.6482)  loss_scale: 65536.0000 (57590.6688)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0913  max mem: 15572
Epoch: [13]  [640/685]  eta: 0:00:25  lr: 0.000040  min_lr: 0.000000  loss: 4.6879 (4.6489)  loss_scale: 65536.0000 (57714.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0975  max mem: 15572
Epoch: [13]  [650/685]  eta: 0:00:20  lr: 0.000040  min_lr: 0.000000  loss: 4.6299 (4.6474)  loss_scale: 65536.0000 (57834.7650)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1077  max mem: 15572
Epoch: [13]  [660/685]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000000  loss: 4.5307 (4.6464)  loss_scale: 65536.0000 (57951.2738)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1633  max mem: 15572
Epoch: [13]  [670/685]  eta: 0:00:08  lr: 0.000040  min_lr: 0.000000  loss: 4.6034 (4.6471)  loss_scale: 65536.0000 (58064.3100)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1402  max mem: 15572
Epoch: [13]  [680/685]  eta: 0:00:02  lr: 0.000040  min_lr: 0.000000  loss: 4.6254 (4.6468)  loss_scale: 65536.0000 (58174.0264)  weight_decay: 0.0500 (0.0500)  time: 0.4601  data: 0.0399  max mem: 15572
Epoch: [13]  [684/685]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000000  loss: 4.6254 (4.6474)  loss_scale: 65536.0000 (58217.0161)  weight_decay: 0.0500 (0.0500)  time: 0.4496  data: 0.0398  max mem: 15572
Epoch: [13] Total time: 0:06:32 (0.5725 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000000  loss: 4.6254 (4.6474)  loss_scale: 65536.0000 (58217.0161)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 916
Indices to remove: tensor([  233,   239,   260,   264,   313,   351,   405,   406,   446,   505,
          524,   539,   544,   549,   559,   565,   570,   571,   576,   600,
          618,   621,   622,   639,   642,   687,   711,   713,   719,   720,
          731,   758,   772,   788,   823,   828,   838,   846,   865,   888,
          901,   921,   924,   960,   995,  1034,  1050,  1238,  1523,  1537,
         1538,  1551,  1576,  1590,  1654,  1655,  1749,  1762,  1776,  1782,
         1787,  1790,  1835,  1917,  1927,  1932,  1989,  2013,  2056,  2076,
         2116,  2134,  2148,  2169,  2219,  2233,  2238,  2266,  2301,  2306,
         2338,  2348,  2354,  2357,  2369,  2379,  2391,  2426,  2486,  2538,
         2540,  2543,  2612,  2695,  2738,  2742,  2752,  2755,  2758,  2810,
         2824,  2840,  2871,  2873,  2876,  2879,  2960,  3035,  3059,  3060,
         3063,  3155,  3170,  3182,  3200,  3206,  3212,  3278,  3287,  3490,
         3531,  3663,  3677,  3683,  3690,  3810,  3823,  3827,  3966,  4045,
         4063,  4105,  4124,  4135,  4205,  4258,  4268,  4273,  4276,  4284,
         4288,  4314,  4324,  4428,  4440,  4454,  4469,  4472,  4499,  4520,
         4564,  4573,  4588,  4642,  4688,  4714,  4747,  4752,  4878,  4905,
         4940,  4978,  4992,  4998,  5011,  5018,  5022,  5049,  5067,  5068,
         5073,  5136,  5167,  5254,  5326,  5375,  5387,  5405,  5419,  5448,
         5485,  5530,  5642,  5684,  5755,  5781,  5803,  5844,  5880,  5886,
         5891,  5934,  5947,  6000,  6007,  6100,  6102,  6149,  6170,  6221,
         6323,  6352,  6409,  6436,  6495,  6539,  6554,  6590,  6603,  6794,
         6842,  6853,  7039,  7048,  7086,  7089,  7099,  7108,  7125,  7133,
         7146,  7153,  7162,  7169,  7217,  7223,  7236,  7248,  7251,  7348,
         7381,  7393,  7414,  7482,  7639,  7641,  7654,  7659,  7761,  7881,
         7942,  7987,  8012,  8110,  8200,  8306,  8457,  8463,  8490,  8510,
         8530,  8565,  8777,  8819,  8837,  8936,  8966,  9076,  9382,  9403,
         9479,  9592,  9664,  9880,  9933,  9975, 10028, 10050, 10170, 10177,
        10200, 10223, 10228, 10299, 10430, 10444, 10446, 10502, 10515, 10564,
        10567, 10592, 10605, 10658, 10689, 10704, 10748, 10754, 10787, 10812,
        10826, 10857, 10873, 10884, 10962, 10963, 10975, 11012, 11013, 11035,
        11125, 11143, 11166, 11206, 11211, 11213, 11258, 11355, 11375, 11409,
        11419, 11469, 11610, 11667, 11692, 11701, 11770, 11816, 11829, 11841,
        11854, 11879, 11999, 12000, 12006, 12037, 12057, 12064, 12099, 12100,
        12128, 12132, 12168, 12197, 12208, 12211, 12212, 12227, 12258, 12291,
        12318, 12344, 12380, 12421, 12443, 12463, 12499, 12512, 12519, 12546,
        12556, 12567, 12587, 12617, 12638, 12643, 12650, 12673, 12682, 12692,
        12699, 12714, 12734, 12767, 12773, 12780, 12789, 12803, 12819, 12863,
        12879, 12895, 12913, 12935, 12953, 13018, 13023, 13116, 13117, 13121,
        13135, 13145, 13161, 13162, 13188, 13206, 13260, 13272, 13274, 13283,
        13294, 13351, 13383, 13404, 13424, 13446, 13478, 13510, 13512, 13515,
        13517, 13519, 13538, 13542, 13548, 13569, 13575, 13591, 13612, 13618,
        13710, 13823, 13948, 13971, 14052, 14171, 14216, 14266, 14271, 14341,
        14406, 14409, 14416, 14433, 14439, 14469, 14470, 14499, 14500, 14526,
        14554, 14562, 14594, 14608, 14645, 14650, 14658, 14675, 14676, 14767,
        14783, 14869, 14871, 14993, 15044, 15130, 15167, 15216, 15274, 15342,
        15359, 15360, 15432, 15505, 15553, 15556, 15602, 15620, 15624, 15625,
        15626, 15648, 15652, 15667, 15675, 15691, 15709, 15711, 15857, 15952,
        16353, 16371, 16465, 16530, 16543, 16545, 16554, 16594, 16621, 16664,
        16684, 16696, 16724, 16728, 16732, 16829, 16934, 17053, 17067, 17267,
        17782, 17845, 17861, 17872, 17880, 17884, 17891, 17896, 17903, 17938,
        17951, 17997, 18005, 18022, 18037, 18099, 18140, 18155, 18194, 18205,
        18229, 18244, 18255, 18350, 18372, 18373, 18376, 18417, 18503, 18507,
        18601, 18625, 18629, 18708, 18737, 18772, 18808, 18809, 18849, 18858,
        18860, 18887, 18902, 18904, 18924, 18931, 18982, 18983, 19004, 19012,
        19077, 19099, 19127, 19308, 19332, 19398, 19447, 19453, 19477, 19481,
        19543, 19578, 19582, 19586, 19588, 19651, 19664, 19683, 19722, 19731,
        19737, 19770, 19781, 19790, 19802, 19805, 19811, 19846, 19857, 19861,
        19871, 19885, 19886, 19888, 19889, 19890, 19899, 19908, 19943, 19959,
        20075, 20101, 20140, 20203, 20247, 20337, 20348, 20353, 20370, 20425,
        20466, 20508, 20603, 20625, 20639, 20676, 20679, 20722, 20738, 20749,
        20770, 20826, 20853, 20896, 20898, 20934, 20965, 20999, 21026, 21035,
        21042, 21049, 21068, 21171, 21361, 21388, 21501, 21578, 21619, 21628,
        21737, 21787, 21805, 21855, 21866, 21883, 21909, 21917, 21935, 22003,
        22051, 22091, 22101, 22102, 22119, 22190, 22197, 22202, 22204, 22213,
        22215, 22227, 22267, 22410, 22411, 22550, 22593, 22597, 22613, 22626,
        22636, 22655, 22674, 22677, 22689, 22691, 22692, 22705, 22712, 22723,
        22741, 22753, 22776, 22817, 22837, 22852, 22936, 22954, 22959, 22969,
        22970, 22998, 23000, 23011, 23052, 23066, 23107, 23165, 23220, 23314,
        23372, 23436, 23470, 23495, 23521, 23532, 23554, 23572, 23604, 23606,
        23650, 23736, 23809, 23895, 23914, 23926, 23936, 23939, 23977, 23979,
        24032, 24076, 24127, 24190, 24205, 24214, 24331, 24384, 24385, 24449,
        24466, 24481, 24597, 24659, 24665, 24669, 24675, 24706, 24711, 24735,
        24740, 24780, 24867, 24898, 24925, 24975, 25010, 25073, 25090, 25152,
        25185, 25221, 25274, 25295, 25306, 25336, 25347, 25352, 25358, 25370,
        25389, 25475, 25491, 25503, 25518, 25523, 25546, 25568, 25570, 25581,
        25603, 25604, 25605, 25611, 25617, 25620, 25626, 25629, 25633, 25643,
        25648, 25693, 25722, 25814, 25886, 25940, 25962, 25990, 25999, 26002,
        26003, 26022, 26121, 26141, 26192, 26259, 26317, 26331, 26403, 26475,
        26493, 26605, 26626, 26672, 26675, 26711, 26723, 26842, 27324, 27327,
        27336, 27355, 27380, 27458, 27476, 27508, 27540, 27569, 27632, 27655,
        27707, 27733, 27741, 27742, 27829, 27831, 27852, 28226, 28228, 28231,
        28271, 28282, 28311, 28343, 28427, 28469, 28472, 28577, 28598, 28616,
        28690, 28721, 28906, 28993, 29008, 29099, 29150, 29173, 29254, 29308,
        29331, 29351, 29386, 29399, 29491, 29500, 29573, 29605, 29626, 29690,
        29705, 29850, 29945, 29951, 30018, 30086, 30092, 30283, 30326, 30380,
        30524, 30546, 30550, 30572, 30592, 30595, 30617, 30682, 30734, 30741,
        30780, 30784, 30787, 30818, 30851, 30856, 30955, 30956, 30968, 30992,
        31020, 31104, 31122, 31173, 31184, 31225, 31270, 31320, 31344, 31375,
        31395, 31445, 31528, 31554, 31582, 31654, 31742, 31798, 31873, 31972,
        32048, 32362, 32662, 32699, 32766, 32777, 32783, 32787, 32788, 32865,
        32904, 32948, 33034, 33417, 33474, 33501, 33520, 33541, 33570, 33595,
        33602, 33613, 33636, 33643, 33656, 33664], device='cuda:0')
length of data loader train is: 609
num_training_steps_per_epoch is: 609
Change step level LR scheduler!
Set warmup steps = 3045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:22:12  loss: 2.0078 (2.0078)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 4.9007  data: 4.7301  max mem: 15572
Val:  [ 10/272]  eta: 0:03:19  loss: 3.6117 (3.5010)  acc1: 22.2222 (27.7778)  acc5: 50.0000 (54.0404)  time: 0.7626  data: 0.5505  max mem: 15572
Val:  [ 20/272]  eta: 0:02:12  loss: 3.5324 (3.4201)  acc1: 27.7778 (29.6296)  acc5: 55.5556 (58.2011)  time: 0.3075  data: 0.1057  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 3.3703 (3.4585)  acc1: 27.7778 (26.8817)  acc5: 61.1111 (58.4229)  time: 0.2865  data: 0.0951  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.3289 (3.4367)  acc1: 27.7778 (27.5068)  acc5: 61.1111 (59.4851)  time: 0.3205  data: 0.1227  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 3.2600 (3.3866)  acc1: 27.7778 (28.4314)  acc5: 66.6667 (60.6754)  time: 0.3341  data: 0.1391  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 2.9110 (3.3030)  acc1: 38.8889 (32.2404)  acc5: 72.2222 (62.4772)  time: 0.3143  data: 0.1010  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 3.0300 (3.2652)  acc1: 33.3333 (32.4726)  acc5: 72.2222 (64.7105)  time: 0.2977  data: 0.0845  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 3.1340 (3.2711)  acc1: 33.3333 (32.7160)  acc5: 72.2222 (64.8834)  time: 0.3288  data: 0.1281  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.6088 (3.3136)  acc1: 22.2222 (31.1355)  acc5: 55.5556 (63.1258)  time: 0.3458  data: 0.1501  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.6605 (3.3592)  acc1: 16.6667 (29.5380)  acc5: 55.5556 (62.2662)  time: 0.3282  data: 0.1356  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 3.8237 (3.4079)  acc1: 5.5556 (27.3273)  acc5: 50.0000 (60.6106)  time: 0.3171  data: 0.1112  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.8237 (3.4340)  acc1: 5.5556 (26.7218)  acc5: 50.0000 (59.9633)  time: 0.2989  data: 0.0904  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.3991 (3.4074)  acc1: 22.2222 (27.3961)  acc5: 61.1111 (60.6022)  time: 0.2977  data: 0.0957  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1085 (3.3931)  acc1: 27.7778 (27.2656)  acc5: 66.6667 (61.0717)  time: 0.3129  data: 0.1172  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.0654 (3.3719)  acc1: 22.2222 (27.5938)  acc5: 66.6667 (61.8469)  time: 0.2966  data: 0.0973  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.1981 (3.3660)  acc1: 22.2222 (27.2947)  acc5: 72.2222 (62.3533)  time: 0.3079  data: 0.1021  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.2843 (3.3757)  acc1: 22.2222 (26.9981)  acc5: 66.6667 (61.8908)  time: 0.3434  data: 0.1245  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.1172 (3.3606)  acc1: 22.2222 (27.2253)  acc5: 66.6667 (62.4002)  time: 0.3274  data: 0.1072  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3200 (3.3793)  acc1: 22.2222 (26.5271)  acc5: 55.5556 (61.1693)  time: 0.3006  data: 0.0964  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.3526 (3.3755)  acc1: 22.2222 (26.7275)  acc5: 50.0000 (61.2493)  time: 0.3198  data: 0.1220  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1797 (3.3895)  acc1: 22.2222 (26.3033)  acc5: 66.6667 (61.0585)  time: 0.3294  data: 0.1113  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4789 (3.3884)  acc1: 22.2222 (26.4203)  acc5: 61.1111 (61.1362)  time: 0.3016  data: 0.0798  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.2301 (3.3827)  acc1: 33.3333 (27.1525)  acc5: 66.6667 (61.7124)  time: 0.2984  data: 0.0937  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0531 (3.3684)  acc1: 44.4444 (27.6856)  acc5: 83.3333 (62.5173)  time: 0.3523  data: 0.1590  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1513 (3.3774)  acc1: 33.3333 (27.2687)  acc5: 66.6667 (61.9965)  time: 0.3335  data: 0.1537  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1513 (3.3546)  acc1: 33.3333 (28.4163)  acc5: 66.6667 (62.8778)  time: 0.2455  data: 0.0668  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0921 (3.3589)  acc1: 38.8889 (28.2083)  acc5: 66.6667 (62.6691)  time: 0.1832  data: 0.0189  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0921 (3.3617)  acc1: 33.3333 (28.1794)  acc5: 66.6667 (62.6459)  time: 0.1775  data: 0.0189  max mem: 15572
Val: Total time: 0:01:28 (0.3239 s / it)
* Acc@1 28.179 Acc@5 62.646 loss 3.362
Accuracy of the network on the 4883 val videos: 28.2%
[2025-01-13 01:44:19,640] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:44:19,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:44:19,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:44:22,327] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:44:22,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 28.18%
Epoch: [14]  [  0/609]  eta: 1:08:33  lr: 0.000040  min_lr: 0.000000  loss: 4.9568 (4.9568)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7544  data: 6.2839  max mem: 15572
Epoch: [14]  [ 10/609]  eta: 0:10:15  lr: 0.000040  min_lr: 0.000000  loss: 4.7936 (4.7243)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0282  data: 0.5811  max mem: 15572
[2025-01-13 01:44:35,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:44:35,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:44:36,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24555
[2025-01-13 01:44:36,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:44:36,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 20/609]  eta: 0:07:10  lr: 0.000040  min_lr: 0.000000  loss: 4.6868 (4.6895)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  time: 0.4294  data: 0.0057  max mem: 15572
Epoch: [14]  [ 30/609]  eta: 0:06:22  lr: 0.000040  min_lr: 0.000000  loss: 4.6985 (4.6725)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.4587  data: 0.0008  max mem: 15572
Epoch: [14]  [ 40/609]  eta: 0:05:54  lr: 0.000040  min_lr: 0.000000  loss: 4.7305 (4.6788)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0008  max mem: 15572
Epoch: [14]  [ 50/609]  eta: 0:05:35  lr: 0.000040  min_lr: 0.000000  loss: 4.7268 (4.6593)  loss_scale: 65536.0000 (68106.0392)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0152  max mem: 15572
Epoch: [14]  [ 60/609]  eta: 0:05:39  lr: 0.000039  min_lr: 0.000000  loss: 4.7063 (4.6692)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1292  max mem: 15572
Epoch: [14]  [ 70/609]  eta: 0:05:36  lr: 0.000039  min_lr: 0.000000  loss: 4.5907 (4.6567)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.6870  data: 0.1814  max mem: 15572
Epoch: [14]  [ 80/609]  eta: 0:05:35  lr: 0.000039  min_lr: 0.000000  loss: 4.5872 (4.6559)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6819  data: 0.1663  max mem: 15572
Epoch: [14]  [ 90/609]  eta: 0:05:30  lr: 0.000039  min_lr: 0.000000  loss: 4.7033 (4.6688)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.1915  max mem: 15572
Epoch: [14]  [100/609]  eta: 0:05:24  lr: 0.000039  min_lr: 0.000000  loss: 4.5952 (4.6609)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1895  max mem: 15572
Epoch: [14]  [110/609]  eta: 0:05:21  lr: 0.000039  min_lr: 0.000000  loss: 4.5841 (4.6686)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.2189  max mem: 15572
Epoch: [14]  [120/609]  eta: 0:05:13  lr: 0.000039  min_lr: 0.000000  loss: 4.7804 (4.6730)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.1945  max mem: 15572
Epoch: [14]  [130/609]  eta: 0:05:12  lr: 0.000039  min_lr: 0.000000  loss: 4.6839 (4.6731)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.7036  data: 0.2455  max mem: 15572
[2025-01-13 01:45:48,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24670
[2025-01-13 01:45:48,385] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:45:48,385] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [140/609]  eta: 0:05:11  lr: 0.000039  min_lr: 0.000000  loss: 4.6655 (4.6693)  loss_scale: 32768.0000 (64141.6170)  weight_decay: 0.0500 (0.0500)  time: 0.8053  data: 0.3431  max mem: 15572
Epoch: [14]  [150/609]  eta: 0:05:02  lr: 0.000039  min_lr: 0.000000  loss: 4.6495 (4.6654)  loss_scale: 32768.0000 (62063.8940)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.2192  max mem: 15572
Epoch: [14]  [160/609]  eta: 0:04:48  lr: 0.000039  min_lr: 0.000000  loss: 4.6997 (4.6710)  loss_scale: 32768.0000 (60244.2733)  weight_decay: 0.0500 (0.0500)  time: 0.4790  data: 0.0492  max mem: 15572
Epoch: [14]  [170/609]  eta: 0:04:36  lr: 0.000039  min_lr: 0.000000  loss: 4.7277 (4.6723)  loss_scale: 32768.0000 (58637.4737)  weight_decay: 0.0500 (0.0500)  time: 0.4143  data: 0.0004  max mem: 15572
Epoch: [14]  [180/609]  eta: 0:04:25  lr: 0.000039  min_lr: 0.000000  loss: 4.6446 (4.6691)  loss_scale: 32768.0000 (57208.2210)  weight_decay: 0.0500 (0.0500)  time: 0.4407  data: 0.0005  max mem: 15572
Epoch: [14]  [190/609]  eta: 0:04:16  lr: 0.000039  min_lr: 0.000000  loss: 4.6239 (4.6700)  loss_scale: 32768.0000 (55928.6283)  weight_decay: 0.0500 (0.0500)  time: 0.4603  data: 0.0099  max mem: 15572
Epoch: [14]  [200/609]  eta: 0:04:09  lr: 0.000039  min_lr: 0.000000  loss: 4.6890 (4.6707)  loss_scale: 32768.0000 (54776.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0671  max mem: 15572
Epoch: [14]  [210/609]  eta: 0:04:03  lr: 0.000039  min_lr: 0.000000  loss: 4.7372 (4.6764)  loss_scale: 32768.0000 (53733.3081)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1316  max mem: 15572
Epoch: [14]  [220/609]  eta: 0:03:57  lr: 0.000039  min_lr: 0.000000  loss: 4.7372 (4.6733)  loss_scale: 32768.0000 (52784.6516)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1605  max mem: 15572
Epoch: [14]  [230/609]  eta: 0:03:50  lr: 0.000039  min_lr: 0.000000  loss: 4.5801 (4.6754)  loss_scale: 32768.0000 (51918.1299)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1690  max mem: 15572
Epoch: [14]  [240/609]  eta: 0:03:45  lr: 0.000039  min_lr: 0.000000  loss: 4.5801 (4.6726)  loss_scale: 32768.0000 (51123.5187)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1926  max mem: 15572
Epoch: [14]  [250/609]  eta: 0:03:38  lr: 0.000039  min_lr: 0.000000  loss: 4.4064 (4.6595)  loss_scale: 32768.0000 (50392.2231)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1367  max mem: 15572
[2025-01-13 01:47:00,954] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:47:00,954] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [260/609]  eta: 0:03:31  lr: 0.000039  min_lr: 0.000000  loss: 4.5084 (4.6583)  loss_scale: 32768.0000 (49842.5134)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1099  max mem: 15572
Epoch: [14]  [270/609]  eta: 0:03:25  lr: 0.000039  min_lr: 0.000000  loss: 4.5771 (4.6555)  loss_scale: 65536.0000 (50421.6089)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1533  max mem: 15572
Epoch: [14]  [280/609]  eta: 0:03:19  lr: 0.000039  min_lr: 0.000000  loss: 4.7105 (4.6583)  loss_scale: 65536.0000 (50959.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1391  max mem: 15572
Epoch: [14]  [290/609]  eta: 0:03:12  lr: 0.000039  min_lr: 0.000000  loss: 4.6228 (4.6523)  loss_scale: 65536.0000 (51460.3986)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1237  max mem: 15572
Epoch: [14]  [300/609]  eta: 0:03:05  lr: 0.000039  min_lr: 0.000000  loss: 4.5529 (4.6492)  loss_scale: 65536.0000 (51928.0266)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0744  max mem: 15572
Epoch: [14]  [310/609]  eta: 0:02:58  lr: 0.000039  min_lr: 0.000000  loss: 4.6137 (4.6512)  loss_scale: 65536.0000 (52365.5820)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0586  max mem: 15572
Epoch: [14]  [320/609]  eta: 0:02:51  lr: 0.000039  min_lr: 0.000000  loss: 4.7058 (4.6502)  loss_scale: 65536.0000 (52775.8754)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0898  max mem: 15572
Epoch: [14]  [330/609]  eta: 0:02:45  lr: 0.000039  min_lr: 0.000000  loss: 4.6737 (4.6508)  loss_scale: 65536.0000 (53161.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1061  max mem: 15572
Epoch: [14]  [340/609]  eta: 0:02:39  lr: 0.000039  min_lr: 0.000000  loss: 4.5952 (4.6477)  loss_scale: 65536.0000 (53524.2698)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1304  max mem: 15572
Epoch: [14]  [350/609]  eta: 0:02:33  lr: 0.000039  min_lr: 0.000000  loss: 4.6019 (4.6487)  loss_scale: 65536.0000 (53866.4843)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1110  max mem: 15572
Epoch: [14]  [360/609]  eta: 0:02:26  lr: 0.000039  min_lr: 0.000000  loss: 4.6019 (4.6459)  loss_scale: 65536.0000 (54189.7396)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0685  max mem: 15572
Epoch: [14]  [370/609]  eta: 0:02:20  lr: 0.000039  min_lr: 0.000000  loss: 4.5185 (4.6451)  loss_scale: 65536.0000 (54495.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0517  max mem: 15572
Epoch: [14]  [380/609]  eta: 0:02:14  lr: 0.000039  min_lr: 0.000000  loss: 4.6310 (4.6458)  loss_scale: 65536.0000 (54785.3438)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1109  max mem: 15572
[2025-01-13 01:48:10,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:48:10,884] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:48:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24928
[2025-01-13 01:48:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:48:11,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [390/609]  eta: 0:02:08  lr: 0.000039  min_lr: 0.000000  loss: 4.7189 (4.6460)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1206  max mem: 15572
Epoch: [14]  [400/609]  eta: 0:02:02  lr: 0.000039  min_lr: 0.000000  loss: 4.7189 (4.6485)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0408  max mem: 15572
Epoch: [14]  [410/609]  eta: 0:01:56  lr: 0.000039  min_lr: 0.000000  loss: 4.7113 (4.6478)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0617  max mem: 15572
Epoch: [14]  [420/609]  eta: 0:01:49  lr: 0.000039  min_lr: 0.000000  loss: 4.7982 (4.6513)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0704  max mem: 15572
Epoch: [14]  [430/609]  eta: 0:01:43  lr: 0.000039  min_lr: 0.000000  loss: 4.7982 (4.6520)  loss_scale: 65536.0000 (56184.5754)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0340  max mem: 15572
Epoch: [14]  [440/609]  eta: 0:01:38  lr: 0.000039  min_lr: 0.000000  loss: 4.7819 (4.6553)  loss_scale: 65536.0000 (56396.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1390  max mem: 15572
Epoch: [14]  [450/609]  eta: 0:01:33  lr: 0.000038  min_lr: 0.000000  loss: 4.6953 (4.6537)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.2426  max mem: 15572
[2025-01-13 01:48:52,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=162, lr=[3.7261408799050773e-07, 3.7261408799050773e-07, 5.323058399864397e-07, 5.323058399864397e-07, 7.604369142663424e-07, 7.604369142663424e-07, 1.0863384489519179e-06, 1.0863384489519179e-06, 1.5519120699313112e-06, 1.5519120699313112e-06, 2.2170172427590163e-06, 2.2170172427590163e-06, 3.167167489655738e-06, 3.167167489655738e-06, 4.5245249852224825e-06, 4.5245249852224825e-06, 6.463607121746404e-06, 6.463607121746404e-06, 9.233724459637721e-06, 9.233724459637721e-06, 1.3191034942339601e-05, 1.3191034942339601e-05, 1.884433563191372e-05, 1.884433563191372e-05, 2.6920479474162457e-05, 2.6920479474162457e-05, 3.845782782023208e-05, 3.845782782023208e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 01:48:52,539] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=27.87702286885842, CurrSamplesPerSec=24.632264518269757, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [460/609]  eta: 0:01:27  lr: 0.000038  min_lr: 0.000000  loss: 4.6705 (4.6541)  loss_scale: 65536.0000 (56793.1280)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1936  max mem: 15572
Epoch: [14]  [470/609]  eta: 0:01:21  lr: 0.000038  min_lr: 0.000000  loss: 4.6230 (4.6541)  loss_scale: 65536.0000 (56978.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1272  max mem: 15572
Epoch: [14]  [480/609]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000000  loss: 4.6064 (4.6543)  loss_scale: 65536.0000 (57156.6570)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0834  max mem: 15572
Epoch: [14]  [490/609]  eta: 0:01:09  lr: 0.000038  min_lr: 0.000000  loss: 4.7149 (4.6551)  loss_scale: 65536.0000 (57327.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0845  max mem: 15572
Epoch: [14]  [500/609]  eta: 0:01:03  lr: 0.000038  min_lr: 0.000000  loss: 4.7287 (4.6541)  loss_scale: 65536.0000 (57491.1617)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1076  max mem: 15572
Epoch: [14]  [510/609]  eta: 0:00:57  lr: 0.000038  min_lr: 0.000000  loss: 4.4810 (4.6515)  loss_scale: 65536.0000 (57648.5949)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1583  max mem: 15572
[2025-01-13 01:49:25,801] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:49:25,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:49:26,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25058
[2025-01-13 01:49:26,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:49:26,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [520/609]  eta: 0:00:51  lr: 0.000038  min_lr: 0.000000  loss: 4.5961 (4.6520)  loss_scale: 65536.0000 (57925.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1348  max mem: 15572
Epoch: [14]  [530/609]  eta: 0:00:45  lr: 0.000038  min_lr: 0.000000  loss: 4.6738 (4.6509)  loss_scale: 65536.0000 (58069.0923)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0682  max mem: 15572
Epoch: [14]  [540/609]  eta: 0:00:40  lr: 0.000038  min_lr: 0.000000  loss: 4.5972 (4.6489)  loss_scale: 65536.0000 (58207.1128)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1273  max mem: 15572
Epoch: [14]  [550/609]  eta: 0:00:34  lr: 0.000038  min_lr: 0.000000  loss: 4.6199 (4.6491)  loss_scale: 65536.0000 (58340.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1617  max mem: 15572
Epoch: [14]  [560/609]  eta: 0:00:28  lr: 0.000038  min_lr: 0.000000  loss: 4.6199 (4.6473)  loss_scale: 65536.0000 (58468.3922)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1180  max mem: 15572
Epoch: [14]  [570/609]  eta: 0:00:22  lr: 0.000038  min_lr: 0.000000  loss: 4.5137 (4.6463)  loss_scale: 65536.0000 (58592.1681)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1011  max mem: 15572
Epoch: [14]  [580/609]  eta: 0:00:16  lr: 0.000038  min_lr: 0.000000  loss: 4.5015 (4.6438)  loss_scale: 65536.0000 (58711.6833)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0824  max mem: 15572
Epoch: [14]  [590/609]  eta: 0:00:11  lr: 0.000038  min_lr: 0.000000  loss: 4.5925 (4.6461)  loss_scale: 65536.0000 (58827.1540)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0795  max mem: 15572
Epoch: [14]  [600/609]  eta: 0:00:05  lr: 0.000038  min_lr: 0.000000  loss: 4.6952 (4.6466)  loss_scale: 65536.0000 (58938.7820)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.1021  max mem: 15572
Epoch: [14]  [608/609]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 4.6581 (4.6468)  loss_scale: 65536.0000 (59025.4450)  weight_decay: 0.0500 (0.0500)  time: 0.4571  data: 0.0522  max mem: 15572
Epoch: [14] Total time: 0:05:51 (0.5772 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 4.6581 (4.6468)  loss_scale: 65536.0000 (59025.4450)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 859
Indices to remove: tensor([  242,   249,   282,   286,   333,   379,   383,   387,   426,   440,
          450,   473,   476,   510,   520,   527,   531,   532,   558,   562,
          569,   589,   593,   595,   624,   683,   701,   702,   738,   777,
          786,   829,   845,   873,   917,  1189,  1259,  1311,  1349,  1399,
         1512,  1594,  1610,  1644,  1661,  1673,  1679,  1700,  1715,  1794,
         1816,  1820,  1890,  1901,  1912,  1915,  1925,  1942,  2053,  2074,
         2114,  2194,  2209,  2229,  2241,  2329,  2367,  2416,  2438,  2473,
         2526,  2614,  2636,  2648,  2667,  2690,  2700,  2709,  2711,  2740,
         2756,  2761,  2772,  2787,  2815,  2874,  2906,  2917,  2921,  2946,
         2956,  2974,  2983,  3034,  3040,  3043,  3049,  3053,  3146,  3216,
         3225,  3264,  3277,  3305,  3438,  3444,  3477,  3499,  3510,  3523,
         3527,  3538,  3552,  3572,  3815,  3864,  3867,  3874,  3943,  4034,
         4035,  4110,  4134,  4147,  4175,  4185,  4220,  4229,  4293,  4299,
         4333,  4349,  4391,  4429,  4449,  4556,  4586,  4680,  4692,  4703,
         4704,  4729,  4766,  4810,  4824,  4833,  4910,  4950,  5009,  5034,
         5107,  5138,  5174,  5205,  5214,  5230,  5331,  5357,  5361,  5428,
         5605,  5623,  5634,  5734,  5818,  5915,  5924,  6021,  6023,  6035,
         6098,  6119,  6174,  6182,  6236,  6250,  6293,  6316,  6383,  6401,
         6417,  6502,  6565,  6580,  6581,  6834,  6871,  6874,  6876,  6901,
         6918,  6929,  6932,  6949,  6955,  6965,  7010,  7015,  7021,  7041,
         7091,  7106,  7117,  7120,  7202,  7216,  7227,  7235,  7402,  7432,
         7439,  7621,  7665,  7673,  7705,  7722,  7838,  7851,  7859,  7913,
         7991,  7995,  8017,  8021,  8022,  8034,  8043,  8044,  8056,  8061,
         8089,  8095,  8161,  8165,  8221,  8277,  8357,  8362,  8446,  8447,
         8503,  8545,  8579,  8657,  8903,  9163,  9164,  9335,  9466,  9478,
         9512,  9563,  9565,  9663,  9708,  9778,  9794,  9906, 10112, 10133,
        10148, 10227, 10320, 10340, 10358, 10363, 10467, 10498, 10503, 10505,
        10526, 10570, 10577, 10589, 10591, 10595, 10654, 10664, 10693, 10694,
        10700, 10703, 10732, 10743, 10773, 10780, 10798, 10979, 11064, 11106,
        11183, 11186, 11210, 11222, 11267, 11306, 11569, 11576, 11613, 11788,
        11843, 11875, 11932, 12023, 12031, 12081, 12090, 12120, 12123, 12148,
        12155, 12158, 12202, 12229, 12231, 12237, 12254, 12268, 12306, 12316,
        12337, 12339, 12346, 12370, 12372, 12379, 12417, 12431, 12534, 12553,
        12619, 12741, 12751, 12782, 12792, 12887, 12951, 12985, 13013, 13059,
        13072, 13106, 13197, 13250, 13289, 13297, 13339, 13375, 13377, 13379,
        13398, 13448, 13488, 13498, 13524, 13551, 13554, 13568, 13579, 13586,
        13711, 13726, 13734, 13847, 13959, 13973, 14014, 14034, 14079, 14102,
        14110, 14113, 14128, 14130, 14217, 14367, 14408, 14436, 14477, 14478,
        14494, 14504, 14508, 14516, 14525, 14533, 14596, 14630, 14644, 14661,
        14683, 14708, 14723, 14755, 14760, 14822, 14884, 14912, 14938, 14948,
        14987, 15000, 15027, 15041, 15088, 15143, 15144, 15153, 15164, 15205,
        15252, 15270, 15313, 15337, 15448, 15509, 15532, 15536, 15540, 15546,
        15634, 15738, 16105, 16305, 16345, 16412, 16439, 16466, 16471, 16515,
        16532, 16533, 16549, 16584, 16591, 16602, 16645, 16650, 16687, 16692,
        16700, 16718, 16774, 16897, 17331, 17851, 17897, 17898, 17899, 17922,
        17947, 17972, 17985, 17986, 18027, 18051, 18070, 18093, 18098, 18167,
        18242, 18260, 18285, 18302, 18306, 18307, 18345, 18359, 18392, 18410,
        18455, 18479, 18514, 18523, 18524, 18530, 18544, 18674, 18676, 18698,
        18703, 18720, 18883, 18903, 18939, 19056, 19087, 19139, 19194, 19223,
        19361, 19415, 19448, 19451, 19468, 19499, 19528, 19537, 19542, 19555,
        19595, 19608, 19633, 19734, 19785, 19863, 19896, 19933, 19937, 19960,
        19987, 20026, 20128, 20146, 20194, 20205, 20216, 20306, 20315, 20402,
        20413, 20444, 20548, 20554, 20648, 20661, 20757, 20799, 20947, 20993,
        21016, 21033, 21071, 21095, 21121, 21135, 21145, 21182, 21313, 21458,
        21539, 21556, 21580, 21649, 21707, 21723, 21728, 21733, 21755, 21762,
        21764, 21773, 21795, 21796, 21798, 21800, 21880, 21946, 22007, 22056,
        22081, 22152, 22201, 22236, 22289, 22359, 22462, 22475, 22476, 22478,
        22479, 22547, 22600, 22617, 22635, 22661, 22680, 22684, 22772, 22787,
        22815, 22858, 22861, 22867, 22885, 22889, 22896, 22910, 22924, 22941,
        22943, 22958, 22982, 23027, 23040, 23068, 23081, 23085, 23091, 23093,
        23116, 23129, 23241, 23273, 23291, 23368, 23414, 23465, 23519, 23540,
        23610, 23649, 23673, 23760, 23846, 23901, 23962, 24002, 24074, 24098,
        24114, 24211, 24240, 24406, 24434, 24438, 24524, 24572, 24614, 24629,
        24638, 24646, 24690, 24742, 24782, 24799, 24802, 24815, 24876, 24955,
        24968, 24969, 24996, 25030, 25036, 25057, 25070, 25124, 25144, 25194,
        25219, 25256, 25277, 25330, 25369, 25396, 25406, 25425, 25428, 25447,
        25469, 25541, 25543, 25558, 25562, 25577, 25585, 25612, 25664, 25681,
        25697, 25769, 25810, 25865, 25884, 25891, 25908, 25998, 26076, 26092,
        26175, 26224, 26244, 26263, 26358, 26366, 26424, 26432, 26433, 26450,
        26488, 26512, 26646, 26671, 26754, 26771, 26781, 26793, 26809, 26820,
        26833, 26868, 27062, 27223, 27257, 27371, 27430, 27489, 27530, 27533,
        27570, 27586, 27604, 27609, 27620, 27628, 27662, 27665, 27679, 27683,
        27717, 27726, 27763, 27777, 27842, 27884, 28112, 28161, 28169, 28217,
        28220, 28261, 28296, 28309, 28310, 28325, 28361, 28387, 28405, 28435,
        28478, 28535, 28573, 28588, 28621, 28646, 28670, 28676, 28687, 28708,
        28737, 28738, 28749, 28750, 28783, 28790, 28841, 28919, 28970, 28974,
        29083, 29108, 29143, 29151, 29154, 29177, 29178, 29185, 29197, 29246,
        29290, 29306, 29321, 29428, 29523, 29628, 29636, 29639, 29648, 29666,
        29829, 29838, 29898, 29936, 29957, 30009, 30016, 30127, 30142, 30143,
        30233, 30235, 30268, 30288, 30291, 30318, 30336, 30343, 30349, 30367,
        30376, 30441, 30467, 30479, 30490, 30491, 30492, 30522, 30542, 30553,
        30591, 30616, 30622, 30814, 30838, 30869, 30907, 30918, 30943, 30945,
        30964, 30977, 30999, 31114, 31132, 31180, 31206, 31245, 31296, 31364,
        31373, 31443, 31448, 31571, 31887, 31923, 31979, 31988, 32233, 32439,
        32470, 32761, 32771, 32784, 32796, 32798, 32957, 33016, 33018, 33093,
        33138, 33212, 33458, 33537, 33576, 33647, 33662, 33667, 33681],
       device='cuda:0')
length of data loader train is: 537
num_training_steps_per_epoch is: 537
Change step level LR scheduler!
Set warmup steps = 2685
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:22:08  loss: 2.0535 (2.0535)  acc1: 72.2222 (72.2222)  acc5: 100.0000 (100.0000)  time: 4.8834  data: 4.6679  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 3.4329 (3.3937)  acc1: 22.2222 (27.7778)  acc5: 61.1111 (61.6162)  time: 0.7450  data: 0.5292  max mem: 15572
Val:  [ 20/272]  eta: 0:02:07  loss: 3.4045 (3.3675)  acc1: 27.7778 (31.7460)  acc5: 61.1111 (62.1693)  time: 0.2869  data: 0.0839  max mem: 15572
Val:  [ 30/272]  eta: 0:01:42  loss: 3.3500 (3.4072)  acc1: 33.3333 (29.0323)  acc5: 61.1111 (60.5735)  time: 0.2490  data: 0.0487  max mem: 15572
Val:  [ 40/272]  eta: 0:01:31  loss: 3.4478 (3.4135)  acc1: 22.2222 (27.9133)  acc5: 61.1111 (60.1626)  time: 0.2817  data: 0.0781  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 3.3288 (3.3538)  acc1: 27.7778 (28.6492)  acc5: 66.6667 (61.9826)  time: 0.3307  data: 0.1266  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 2.8974 (3.2818)  acc1: 38.8889 (30.7832)  acc5: 72.2222 (63.8434)  time: 0.3247  data: 0.1184  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 2.8974 (3.2604)  acc1: 33.3333 (30.8294)  acc5: 72.2222 (65.2582)  time: 0.3043  data: 0.1025  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.0916 (3.2623)  acc1: 27.7778 (30.5213)  acc5: 66.6667 (65.2263)  time: 0.3070  data: 0.1055  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.5187 (3.3017)  acc1: 22.2222 (28.8156)  acc5: 61.1111 (63.6752)  time: 0.2933  data: 0.0743  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5583 (3.3385)  acc1: 11.1111 (27.2277)  acc5: 55.5556 (62.7613)  time: 0.2883  data: 0.0715  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 3.7470 (3.3771)  acc1: 11.1111 (25.6256)  acc5: 44.4444 (61.1111)  time: 0.3075  data: 0.0933  max mem: 15572
Val:  [120/272]  eta: 0:00:50  loss: 3.7283 (3.3947)  acc1: 11.1111 (25.8953)  acc5: 44.4444 (60.8356)  time: 0.2861  data: 0.0669  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 3.3091 (3.3764)  acc1: 27.7778 (26.8872)  acc5: 61.1111 (61.2383)  time: 0.2915  data: 0.0846  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1221 (3.3692)  acc1: 27.7778 (26.9110)  acc5: 66.6667 (61.2293)  time: 0.3410  data: 0.1120  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.0176 (3.3459)  acc1: 27.7778 (27.7042)  acc5: 72.2222 (62.1781)  time: 0.3007  data: 0.0635  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 3.1364 (3.3417)  acc1: 33.3333 (27.7433)  acc5: 72.2222 (62.7674)  time: 0.2878  data: 0.0761  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3241 (3.3533)  acc1: 22.2222 (27.7453)  acc5: 66.6667 (62.1183)  time: 0.3174  data: 0.1187  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.1724 (3.3383)  acc1: 27.7778 (27.9006)  acc5: 61.1111 (62.5537)  time: 0.3297  data: 0.1347  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.1683 (3.3543)  acc1: 22.2222 (27.3415)  acc5: 55.5556 (61.4311)  time: 0.3373  data: 0.1464  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2344 (3.3518)  acc1: 27.7778 (27.4185)  acc5: 50.0000 (61.4704)  time: 0.2935  data: 0.1022  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1859 (3.3594)  acc1: 27.7778 (27.6461)  acc5: 72.2222 (61.6114)  time: 0.3046  data: 0.1089  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.4617 (3.3603)  acc1: 27.7778 (27.5515)  acc5: 72.2222 (61.6642)  time: 0.3288  data: 0.1187  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.1491 (3.3490)  acc1: 27.7778 (28.4031)  acc5: 72.2222 (62.3377)  time: 0.2964  data: 0.0872  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.9451 (3.3327)  acc1: 44.4444 (28.8612)  acc5: 83.3333 (63.1397)  time: 0.2950  data: 0.0832  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0276 (3.3387)  acc1: 22.2222 (28.3975)  acc5: 66.6667 (62.7269)  time: 0.2890  data: 0.0751  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0276 (3.3157)  acc1: 38.8889 (29.6083)  acc5: 72.2222 (63.7080)  time: 0.2854  data: 0.0913  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0770 (3.3186)  acc1: 38.8889 (29.3358)  acc5: 77.7778 (63.5711)  time: 0.2333  data: 0.0578  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0770 (3.3214)  acc1: 38.8889 (29.3058)  acc5: 72.2222 (63.5470)  time: 0.2239  data: 0.0577  max mem: 15572
Val: Total time: 0:01:25 (0.3156 s / it)
* Acc@1 29.306 Acc@5 63.547 loss 3.321
Accuracy of the network on the 4883 val videos: 29.3%
[2025-01-13 01:51:39,764] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-13 01:51:39,778] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-13 01:51:39,778] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-13 01:51:42,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-13 01:51:42,387] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 29.31%
Epoch: [15]  [  0/537]  eta: 1:14:01  lr: 0.000038  min_lr: 0.000000  loss: 5.0207 (5.0207)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2713  data: 7.7960  max mem: 15572
Epoch: [15]  [ 10/537]  eta: 0:10:39  lr: 0.000038  min_lr: 0.000000  loss: 4.6465 (4.6012)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2133  data: 0.7943  max mem: 15572
Epoch: [15]  [ 20/537]  eta: 0:07:30  lr: 0.000038  min_lr: 0.000000  loss: 4.6465 (4.6268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0731  max mem: 15572
Epoch: [15]  [ 30/537]  eta: 0:06:55  lr: 0.000038  min_lr: 0.000000  loss: 4.6541 (4.6462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1716  max mem: 15572
[2025-01-13 01:52:12,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:52:12,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:52:13,453] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25188
[2025-01-13 01:52:13,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:52:13,454] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 40/537]  eta: 0:06:16  lr: 0.000038  min_lr: 0.000000  loss: 4.6532 (4.6300)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.2034  max mem: 15572
Epoch: [15]  [ 50/537]  eta: 0:05:50  lr: 0.000038  min_lr: 0.000000  loss: 4.5674 (4.6373)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1176  max mem: 15572
Epoch: [15]  [ 60/537]  eta: 0:05:32  lr: 0.000038  min_lr: 0.000000  loss: 4.7161 (4.6572)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1402  max mem: 15572
Epoch: [15]  [ 70/537]  eta: 0:05:13  lr: 0.000038  min_lr: 0.000000  loss: 4.7639 (4.6793)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1112  max mem: 15572
Epoch: [15]  [ 80/537]  eta: 0:05:02  lr: 0.000038  min_lr: 0.000000  loss: 4.7049 (4.6840)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1034  max mem: 15572
Epoch: [15]  [ 90/537]  eta: 0:04:54  lr: 0.000038  min_lr: 0.000000  loss: 4.6572 (4.6865)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1616  max mem: 15572
Epoch: [15]  [100/537]  eta: 0:04:46  lr: 0.000038  min_lr: 0.000000  loss: 4.6371 (4.6781)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1770  max mem: 15572
Epoch: [15]  [110/537]  eta: 0:04:36  lr: 0.000038  min_lr: 0.000000  loss: 4.6502 (4.6733)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1468  max mem: 15572
Epoch: [15]  [120/537]  eta: 0:04:26  lr: 0.000038  min_lr: 0.000000  loss: 4.6542 (4.6688)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1089  max mem: 15572
[2025-01-13 01:53:02,145] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25273
[2025-01-13 01:53:02,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:53:02,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [130/537]  eta: 0:04:16  lr: 0.000038  min_lr: 0.000000  loss: 4.7484 (4.6735)  loss_scale: 65536.0000 (64535.4504)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0867  max mem: 15572
Epoch: [15]  [140/537]  eta: 0:04:07  lr: 0.000038  min_lr: 0.000000  loss: 4.6488 (4.6714)  loss_scale: 32768.0000 (62282.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0792  max mem: 15572
Epoch: [15]  [150/537]  eta: 0:03:57  lr: 0.000038  min_lr: 0.000000  loss: 4.6066 (4.6722)  loss_scale: 32768.0000 (60327.8411)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0724  max mem: 15572
Epoch: [15]  [160/537]  eta: 0:03:50  lr: 0.000038  min_lr: 0.000000  loss: 4.5625 (4.6670)  loss_scale: 32768.0000 (58616.0497)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0931  max mem: 15572
Epoch: [15]  [170/537]  eta: 0:03:42  lr: 0.000038  min_lr: 0.000000  loss: 4.5625 (4.6610)  loss_scale: 32768.0000 (57104.4678)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0908  max mem: 15572
Epoch: [15]  [180/537]  eta: 0:03:38  lr: 0.000038  min_lr: 0.000000  loss: 4.6439 (4.6557)  loss_scale: 32768.0000 (55759.9116)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1475  max mem: 15572
Epoch: [15]  [190/537]  eta: 0:03:30  lr: 0.000037  min_lr: 0.000000  loss: 4.6605 (4.6563)  loss_scale: 32768.0000 (54556.1466)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1554  max mem: 15572
Epoch: [15]  [200/537]  eta: 0:03:23  lr: 0.000037  min_lr: 0.000000  loss: 4.7585 (4.6575)  loss_scale: 32768.0000 (53472.1592)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0990  max mem: 15572
Epoch: [15]  [210/537]  eta: 0:03:15  lr: 0.000037  min_lr: 0.000000  loss: 4.7413 (4.6578)  loss_scale: 32768.0000 (52490.9194)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0773  max mem: 15572
Epoch: [15]  [220/537]  eta: 0:03:09  lr: 0.000037  min_lr: 0.000000  loss: 4.7020 (4.6609)  loss_scale: 32768.0000 (51598.4796)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1022  max mem: 15572
Epoch: [15]  [230/537]  eta: 0:03:03  lr: 0.000037  min_lr: 0.000000  loss: 4.5835 (4.6551)  loss_scale: 32768.0000 (50783.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1398  max mem: 15572
Epoch: [15]  [240/537]  eta: 0:02:57  lr: 0.000037  min_lr: 0.000000  loss: 4.5573 (4.6539)  loss_scale: 32768.0000 (50035.7842)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1455  max mem: 15572
Epoch: [15]  [250/537]  eta: 0:02:51  lr: 0.000037  min_lr: 0.000000  loss: 4.6462 (4.6564)  loss_scale: 32768.0000 (49347.8247)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1483  max mem: 15572
[2025-01-13 01:54:14,093] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:54:14,094] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [260/537]  eta: 0:02:44  lr: 0.000037  min_lr: 0.000000  loss: 4.7265 (4.6547)  loss_scale: 32768.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0832  max mem: 15572
Epoch: [15]  [270/537]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000000  loss: 4.6124 (4.6543)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0588  max mem: 15572
Epoch: [15]  [280/537]  eta: 0:02:32  lr: 0.000037  min_lr: 0.000000  loss: 4.5829 (4.6515)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1229  max mem: 15572
Epoch: [15]  [290/537]  eta: 0:02:26  lr: 0.000037  min_lr: 0.000000  loss: 4.6884 (4.6555)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1561  max mem: 15572
Epoch: [15]  [300/537]  eta: 0:02:20  lr: 0.000037  min_lr: 0.000000  loss: 4.7447 (4.6582)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1153  max mem: 15572
Epoch: [15]  [310/537]  eta: 0:02:14  lr: 0.000037  min_lr: 0.000000  loss: 4.6891 (4.6538)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0897  max mem: 15572
Epoch: [15]  [320/537]  eta: 0:02:07  lr: 0.000037  min_lr: 0.000000  loss: 4.5403 (4.6492)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0659  max mem: 15572
Epoch: [15]  [330/537]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000000  loss: 4.6128 (4.6493)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1012  max mem: 15572
Epoch: [15]  [340/537]  eta: 0:01:55  lr: 0.000037  min_lr: 0.000000  loss: 4.7187 (4.6505)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1203  max mem: 15572
Epoch: [15]  [350/537]  eta: 0:01:49  lr: 0.000037  min_lr: 0.000000  loss: 4.7007 (4.6512)  loss_scale: 65536.0000 (53679.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1414  max mem: 15572
Epoch: [15]  [360/537]  eta: 0:01:43  lr: 0.000037  min_lr: 0.000000  loss: 4.6956 (4.6507)  loss_scale: 65536.0000 (54008.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1676  max mem: 15572
Epoch: [15]  [370/537]  eta: 0:01:38  lr: 0.000037  min_lr: 0.000000  loss: 4.6895 (4.6493)  loss_scale: 65536.0000 (54318.9218)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1458  max mem: 15572
Epoch: [15]  [380/537]  eta: 0:01:32  lr: 0.000037  min_lr: 0.000000  loss: 4.4926 (4.6467)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1357  max mem: 15572
[2025-01-13 01:55:26,788] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25529
[2025-01-13 01:55:26,788] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:55:26,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [390/537]  eta: 0:01:26  lr: 0.000037  min_lr: 0.000000  loss: 4.4829 (4.6447)  loss_scale: 32768.0000 (54054.6292)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1168  max mem: 15572
Epoch: [15]  [400/537]  eta: 0:01:19  lr: 0.000037  min_lr: 0.000000  loss: 4.6427 (4.6474)  loss_scale: 32768.0000 (53523.7905)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0572  max mem: 15572
Epoch: [15]  [410/537]  eta: 0:01:13  lr: 0.000037  min_lr: 0.000000  loss: 4.5947 (4.6445)  loss_scale: 32768.0000 (53018.7835)  weight_decay: 0.0500 (0.0500)  time: 0.4777  data: 0.0482  max mem: 15572
Epoch: [15]  [420/537]  eta: 0:01:08  lr: 0.000037  min_lr: 0.000000  loss: 4.5259 (4.6451)  loss_scale: 32768.0000 (52537.7672)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1560  max mem: 15572
Epoch: [15]  [430/537]  eta: 0:01:02  lr: 0.000037  min_lr: 0.000000  loss: 4.5810 (4.6436)  loss_scale: 32768.0000 (52079.0719)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1548  max mem: 15572
Epoch: [15]  [440/537]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000000  loss: 4.6143 (4.6439)  loss_scale: 32768.0000 (51641.1791)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0881  max mem: 15572
Epoch: [15]  [450/537]  eta: 0:00:50  lr: 0.000037  min_lr: 0.000000  loss: 4.7023 (4.6440)  loss_scale: 32768.0000 (51222.7051)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0834  max mem: 15572
Epoch: [15]  [460/537]  eta: 0:00:44  lr: 0.000037  min_lr: 0.000000  loss: 4.6476 (4.6431)  loss_scale: 32768.0000 (50822.3861)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1356  max mem: 15572
Epoch: [15]  [470/537]  eta: 0:00:39  lr: 0.000037  min_lr: 0.000000  loss: 4.6564 (4.6446)  loss_scale: 32768.0000 (50439.0658)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1567  max mem: 15572
Epoch: [15]  [480/537]  eta: 0:00:33  lr: 0.000037  min_lr: 0.000000  loss: 4.6564 (4.6449)  loss_scale: 32768.0000 (50071.6840)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1153  max mem: 15572
Epoch: [15]  [490/537]  eta: 0:00:27  lr: 0.000037  min_lr: 0.000000  loss: 4.5923 (4.6448)  loss_scale: 32768.0000 (49719.2668)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1662  max mem: 15572
Epoch: [15]  [500/537]  eta: 0:00:21  lr: 0.000036  min_lr: 0.000000  loss: 4.7417 (4.6472)  loss_scale: 32768.0000 (49380.9182)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1336  max mem: 15572
[2025-01-13 01:56:39,551] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:56:39,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [510/537]  eta: 0:00:15  lr: 0.000036  min_lr: 0.000000  loss: 4.7659 (4.6475)  loss_scale: 32768.0000 (49119.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0741  max mem: 15572
Epoch: [15]  [520/537]  eta: 0:00:09  lr: 0.000036  min_lr: 0.000000  loss: 4.7531 (4.6465)  loss_scale: 65536.0000 (49435.0250)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1024  max mem: 15572
Epoch: [15]  [530/537]  eta: 0:00:04  lr: 0.000036  min_lr: 0.000000  loss: 4.6592 (4.6469)  loss_scale: 65536.0000 (49738.2448)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0967  max mem: 15572
Epoch: [15]  [536/537]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000000  loss: 4.5950 (4.6463)  loss_scale: 65536.0000 (49914.7561)  weight_decay: 0.0500 (0.0500)  time: 0.4545  data: 0.0450  max mem: 15572
Epoch: [15] Total time: 0:05:10 (0.5780 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000000  loss: 4.5950 (4.6463)  loss_scale: 65536.0000 (49914.7561)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 710
Indices to remove: tensor([   43,   213,   262,   267,   283,   284,   287,   305,   334,   347,
          373,   380,   396,   411,   451,   470,   475,   500,   514,   528,
          588,   606,   637,   638,   673,   706,   709,   776,   794,   810,
          813,   847,   848,   860,   894,   918,  1033,  1068,  1098,  1246,
         1276,  1350,  1468,  1603,  1659,  1702,  1727,  1730,  1731,  1769,
         1805,  1826,  1829,  1862,  1866,  1874,  1880,  1885,  1904,  1939,
         2003,  2110,  2164,  2188,  2200,  2224,  2432,  2478,  2516,  2549,
         2655,  2682,  2769,  2795,  2806,  2827,  2833,  2922,  2999,  3014,
         3026,  3074,  3087,  3089,  3118,  3154,  3201,  3334,  3348,  3373,
         3476,  3486,  3563,  3571,  3574,  3590,  3626,  3682,  3717,  3941,
         3948,  3949,  3976,  3993,  4018,  4066,  4080,  4089,  4143,  4156,
         4177,  4186,  4188,  4215,  4281,  4337,  4388,  4441,  4558,  4624,
         4715,  4755,  4839,  4873,  4898,  4908,  4930,  5033,  5065,  5074,
         5076,  5098,  5126,  5168,  5215,  5339,  5342,  5567,  5666,  5741,
         5758,  5809,  5864,  6019,  6076,  6164,  6275,  6519,  6540,  6576,
         6578,  6695,  6850,  6875,  6890,  6944,  6963,  6995,  6997,  7074,
         7460,  7501,  7558,  7599,  7651,  7757,  7772,  7785,  7802,  7828,
         7844,  7846,  7866,  7885,  8127,  8144,  8151,  8217,  8222,  8312,
         8322,  8367,  8377,  8527,  8556,  8766,  8786,  8853,  8909,  8969,
         9157,  9190,  9369,  9733,  9797,  9946,  9948, 10008, 10066, 10069,
        10095, 10109, 10127, 10204, 10259, 10272, 10283, 10290, 10356, 10463,
        10492, 10555, 10573, 10608, 10661, 10678, 10699, 10774, 10843, 10855,
        10863, 10882, 10893, 10976, 10985, 10998, 11002, 11003, 11014, 11022,
        11121, 11142, 11197, 11229, 11620, 11768, 11776, 11778, 11969, 12087,
        12095, 12105, 12107, 12109, 12153, 12203, 12246, 12270, 12292, 12293,
        12332, 12349, 12363, 12473, 12531, 12536, 12599, 12618, 12620, 12625,
        12627, 12635, 12653, 12676, 12678, 12684, 12707, 12712, 12718, 12743,
        12756, 12786, 12854, 12896, 12904, 12914, 12921, 12961, 12966, 12997,
        13017, 13054, 13101, 13126, 13151, 13253, 13255, 13327, 13331, 13332,
        13335, 13347, 13350, 13360, 13419, 13443, 13468, 13511, 13535, 13549,
        13574, 13620, 13643, 13767, 13776, 14160, 14223, 14227, 14233, 14253,
        14282, 14304, 14322, 14371, 14378, 14389, 14506, 14511, 14663, 14671,
        14914, 14941, 14958, 14960, 14968, 14992, 15016, 15066, 15071, 15073,
        15096, 15110, 15268, 15307, 15419, 15475, 15479, 15528, 15643, 15655,
        15669, 15699, 15831, 15855, 16403, 16410, 16431, 16451, 16456, 16464,
        16480, 16503, 16510, 16512, 16519, 16573, 16577, 16583, 16624, 16627,
        16644, 16652, 16669, 16821, 17056, 17869, 17871, 17881, 17919, 17921,
        17933, 18041, 18110, 18144, 18162, 18178, 18193, 18198, 18209, 18218,
        18292, 18333, 18390, 18480, 18489, 18528, 18592, 18656, 18689, 18712,
        18717, 18731, 18829, 18989, 18994, 19013, 19093, 19184, 19189, 19224,
        19225, 19277, 19289, 19300, 19305, 19309, 19318, 19326, 19335, 19338,
        19431, 19446, 19456, 19463, 19585, 19611, 19663, 19810, 19900, 19941,
        19944, 20014, 20047, 20069, 20094, 20099, 20114, 20207, 20221, 20298,
        20377, 20386, 20400, 20439, 20443, 20467, 20558, 20613, 20694, 20731,
        20949, 20983, 21006, 21044, 21237, 21331, 21360, 21504, 21714, 21717,
        21726, 21763, 21783, 21817, 21838, 21839, 21842, 21895, 21898, 21902,
        21916, 21939, 21953, 21972, 21974, 22022, 22034, 22076, 22154, 22173,
        22203, 22207, 22222, 22237, 22281, 22282, 22362, 22371, 22384, 22414,
        22473, 22474, 22553, 22622, 22627, 22629, 22637, 22668, 22690, 22717,
        22764, 22780, 22834, 22844, 22869, 22891, 22904, 22907, 23019, 23030,
        23064, 23099, 23118, 23252, 23264, 23315, 23657, 23753, 23954, 23983,
        24059, 24091, 24157, 24164, 24229, 24275, 24345, 24356, 24389, 24413,
        24432, 24536, 24558, 24565, 24603, 24745, 24793, 24834, 24875, 24902,
        24909, 24931, 24983, 25024, 25064, 25105, 25123, 25127, 25191, 25196,
        25233, 25240, 25262, 25281, 25371, 25463, 25528, 25637, 25666, 25683,
        25781, 25785, 25799, 25845, 25923, 26041, 26058, 26078, 26113, 26155,
        26174, 26267, 26278, 26368, 26371, 26523, 26564, 26582, 26614, 26632,
        26689, 26697, 26709, 26719, 26740, 26741, 26768, 26777, 26817, 26841,
        26850, 26866, 26871, 26881, 26961, 27000, 27144, 27250, 27297, 27344,
        27361, 27366, 27392, 27577, 27580, 27599, 27699, 27703, 27706, 27759,
        27895, 27914, 27992, 28019, 28088, 28155, 28229, 28240, 28276, 28438,
        28466, 28490, 28494, 28521, 28543, 28569, 28572, 28643, 28711, 28772,
        29020, 29123, 29239, 29242, 29300, 29304, 29377, 29407, 29424, 29452,
        29458, 29468, 29487, 29488, 29542, 29577, 29646, 29663, 29681, 29720,
        29736, 29744, 29764, 29974, 29985, 30044, 30240, 30265, 30364, 30385,
        30407, 30412, 30415, 30423, 30438, 30466, 30525, 30548, 30625, 30655,
        30662, 30694, 30711, 30827, 30876, 30884, 30903, 30910, 30916, 30928,
        30971, 30988, 31039, 31042, 31093, 31141, 31154, 31174, 31203, 31343,
        31356, 31361, 31469, 31544, 31684, 31767, 31802, 31865, 32080, 32443,
        32506, 32631, 32760, 32810, 32835, 32869, 32885, 33049, 33199, 33202,
        33327, 33414, 33519, 33527, 33530, 33543, 33553, 33592, 33635, 33694],
       device='cuda:0')
length of data loader train is: 478
num_training_steps_per_epoch is: 478
Change step level LR scheduler!
Set warmup steps = 2390
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:42  loss: 2.1330 (2.1330)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 4.3460  data: 4.1334  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 3.5903 (3.4650)  acc1: 22.2222 (26.7677)  acc5: 50.0000 (53.0303)  time: 0.7469  data: 0.5548  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 3.4872 (3.4324)  acc1: 22.2222 (27.5132)  acc5: 50.0000 (56.0847)  time: 0.3409  data: 0.1539  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.3842 (3.4488)  acc1: 27.7778 (27.4194)  acc5: 55.5556 (56.4516)  time: 0.2623  data: 0.0811  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 3.3889 (3.4205)  acc1: 16.6667 (25.6098)  acc5: 55.5556 (58.6721)  time: 0.2827  data: 0.0953  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.2585 (3.3612)  acc1: 22.2222 (27.1242)  acc5: 61.1111 (60.8932)  time: 0.3123  data: 0.1239  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 2.8563 (3.2968)  acc1: 33.3333 (28.9617)  acc5: 77.7778 (63.0237)  time: 0.3024  data: 0.1217  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 2.9192 (3.2816)  acc1: 33.3333 (29.4210)  acc5: 77.7778 (64.0063)  time: 0.3321  data: 0.1430  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.0340 (3.2674)  acc1: 27.7778 (29.9040)  acc5: 61.1111 (64.2661)  time: 0.2917  data: 0.0992  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.5042 (3.2964)  acc1: 27.7778 (28.9988)  acc5: 61.1111 (63.4921)  time: 0.2789  data: 0.0968  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5727 (3.3335)  acc1: 16.6667 (27.5028)  acc5: 61.1111 (62.5413)  time: 0.3182  data: 0.1327  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 3.6893 (3.3724)  acc1: 5.5556 (25.5756)  acc5: 44.4444 (61.1111)  time: 0.3179  data: 0.1203  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.6837 (3.3869)  acc1: 11.1111 (26.0790)  acc5: 55.5556 (61.2489)  time: 0.3360  data: 0.1336  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.2453 (3.3650)  acc1: 38.8889 (27.2265)  acc5: 66.6667 (61.9169)  time: 0.3216  data: 0.1288  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.0510 (3.3581)  acc1: 27.7778 (27.0292)  acc5: 66.6667 (62.1355)  time: 0.2979  data: 0.1145  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.0518 (3.3398)  acc1: 27.7778 (27.8146)  acc5: 72.2222 (62.8771)  time: 0.2957  data: 0.1112  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2096 (3.3401)  acc1: 27.7778 (27.6052)  acc5: 72.2222 (63.1125)  time: 0.3267  data: 0.1398  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3921 (3.3557)  acc1: 22.2222 (27.1605)  acc5: 61.1111 (62.5081)  time: 0.3428  data: 0.1529  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.2906 (3.3380)  acc1: 22.2222 (27.3481)  acc5: 61.1111 (62.9220)  time: 0.2939  data: 0.1031  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2860 (3.3529)  acc1: 27.7778 (26.7888)  acc5: 50.0000 (61.9255)  time: 0.2986  data: 0.1093  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2860 (3.3528)  acc1: 22.2222 (26.6446)  acc5: 55.5556 (61.9127)  time: 0.2975  data: 0.1150  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2626 (3.3595)  acc1: 27.7778 (26.9352)  acc5: 66.6667 (61.9800)  time: 0.3025  data: 0.1228  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.3293 (3.3615)  acc1: 33.3333 (26.7974)  acc5: 66.6667 (61.9658)  time: 0.3092  data: 0.1279  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2426 (3.3541)  acc1: 33.3333 (27.3689)  acc5: 72.2222 (62.5301)  time: 0.3086  data: 0.1189  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.9786 (3.3387)  acc1: 44.4444 (27.8008)  acc5: 83.3333 (63.3472)  time: 0.3323  data: 0.1374  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0196 (3.3406)  acc1: 22.2222 (27.4236)  acc5: 77.7778 (63.4573)  time: 0.2972  data: 0.0969  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0495 (3.3158)  acc1: 44.4444 (28.8208)  acc5: 77.7778 (64.4317)  time: 0.2924  data: 0.0962  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0495 (3.3174)  acc1: 44.4444 (28.7823)  acc5: 77.7778 (64.4731)  time: 0.2384  data: 0.0740  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0495 (3.3206)  acc1: 38.8889 (28.7733)  acc5: 77.7778 (64.4686)  time: 0.2320  data: 0.0740  max mem: 15572
Val: Total time: 0:01:26 (0.3193 s / it)
* Acc@1 28.773 Acc@5 64.469 loss 3.321
Accuracy of the network on the 4883 val videos: 28.8%
Max accuracy: 29.31%
Epoch: [16]  [  0/478]  eta: 1:08:14  lr: 0.000036  min_lr: 0.000000  loss: 4.6299 (4.6299)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5655  data: 8.1418  max mem: 15572
Epoch: [16]  [ 10/478]  eta: 0:09:25  lr: 0.000036  min_lr: 0.000000  loss: 4.5754 (4.5664)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2078  data: 0.7407  max mem: 15572
Epoch: [16]  [ 20/478]  eta: 0:06:52  lr: 0.000036  min_lr: 0.000000  loss: 4.5754 (4.6274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0666  max mem: 15572
Epoch: [16]  [ 30/478]  eta: 0:06:09  lr: 0.000036  min_lr: 0.000000  loss: 4.7284 (4.6708)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1774  max mem: 15572
Epoch: [16]  [ 40/478]  eta: 0:05:20  lr: 0.000036  min_lr: 0.000000  loss: 4.6819 (4.6503)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1115  max mem: 15572
Epoch: [16]  [ 50/478]  eta: 0:04:52  lr: 0.000036  min_lr: 0.000000  loss: 4.6739 (4.6492)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4657  data: 0.0245  max mem: 15572
Epoch: [16]  [ 60/478]  eta: 0:04:33  lr: 0.000036  min_lr: 0.000000  loss: 4.6315 (4.6445)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0535  max mem: 15572
Epoch: [16]  [ 70/478]  eta: 0:04:21  lr: 0.000036  min_lr: 0.000000  loss: 4.5992 (4.6346)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0828  max mem: 15572
Epoch: [16]  [ 80/478]  eta: 0:04:14  lr: 0.000036  min_lr: 0.000000  loss: 4.6488 (4.6315)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1431  max mem: 15572
Epoch: [16]  [ 90/478]  eta: 0:04:03  lr: 0.000036  min_lr: 0.000000  loss: 4.6488 (4.6311)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1276  max mem: 15572
Epoch: [16]  [100/478]  eta: 0:03:53  lr: 0.000036  min_lr: 0.000000  loss: 4.5816 (4.6251)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0903  max mem: 15572
[2025-01-13 01:59:22,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 01:59:22,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 01:59:23,984] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25789
[2025-01-13 01:59:23,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 01:59:23,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [110/478]  eta: 0:03:46  lr: 0.000036  min_lr: 0.000000  loss: 4.4795 (4.6219)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1236  max mem: 15572
Epoch: [16]  [120/478]  eta: 0:03:38  lr: 0.000036  min_lr: 0.000000  loss: 4.6208 (4.6341)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1270  max mem: 15572
Epoch: [16]  [130/478]  eta: 0:03:30  lr: 0.000036  min_lr: 0.000000  loss: 4.6964 (4.6322)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0991  max mem: 15572
Epoch: [16]  [140/478]  eta: 0:03:23  lr: 0.000036  min_lr: 0.000000  loss: 4.6480 (4.6364)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0984  max mem: 15572
Epoch: [16]  [150/478]  eta: 0:03:14  lr: 0.000036  min_lr: 0.000000  loss: 4.7521 (4.6422)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0666  max mem: 15572
Epoch: [16]  [160/478]  eta: 0:03:08  lr: 0.000036  min_lr: 0.000000  loss: 4.6586 (4.6477)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0670  max mem: 15572
[2025-01-13 01:59:58,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25850
[2025-01-13 01:59:58,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 01:59:58,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [170/478]  eta: 0:03:02  lr: 0.000036  min_lr: 0.000000  loss: 4.6859 (4.6551)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1401  max mem: 15572
Epoch: [16]  [180/478]  eta: 0:02:58  lr: 0.000036  min_lr: 0.000000  loss: 4.7228 (4.6534)  loss_scale: 32768.0000 (63725.6133)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.2112  max mem: 15572
Epoch: [16]  [190/478]  eta: 0:02:50  lr: 0.000036  min_lr: 0.000000  loss: 4.7242 (4.6589)  loss_scale: 32768.0000 (62104.7958)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1565  max mem: 15572
Epoch: [16]  [200/478]  eta: 0:02:44  lr: 0.000036  min_lr: 0.000000  loss: 4.4968 (4.6474)  loss_scale: 32768.0000 (60645.2537)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0886  max mem: 15572
Epoch: [16]  [210/478]  eta: 0:02:38  lr: 0.000036  min_lr: 0.000000  loss: 4.4743 (4.6455)  loss_scale: 32768.0000 (59324.0569)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1227  max mem: 15572
Epoch: [16]  [220/478]  eta: 0:02:32  lr: 0.000036  min_lr: 0.000000  loss: 4.6456 (4.6447)  loss_scale: 32768.0000 (58122.4253)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1102  max mem: 15572
Epoch: [16]  [230/478]  eta: 0:02:26  lr: 0.000036  min_lr: 0.000000  loss: 4.6955 (4.6525)  loss_scale: 32768.0000 (57024.8312)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1197  max mem: 15572
Epoch: [16]  [240/478]  eta: 0:02:20  lr: 0.000035  min_lr: 0.000000  loss: 4.7624 (4.6518)  loss_scale: 32768.0000 (56018.3237)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1243  max mem: 15572
Epoch: [16]  [250/478]  eta: 0:02:14  lr: 0.000035  min_lr: 0.000000  loss: 4.6687 (4.6491)  loss_scale: 32768.0000 (55092.0159)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1247  max mem: 15572
Epoch: [16]  [260/478]  eta: 0:02:08  lr: 0.000035  min_lr: 0.000000  loss: 4.6240 (4.6486)  loss_scale: 32768.0000 (54236.6897)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1395  max mem: 15572
Epoch: [16]  [270/478]  eta: 0:02:02  lr: 0.000035  min_lr: 0.000000  loss: 4.5076 (4.6444)  loss_scale: 32768.0000 (53444.4871)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1204  max mem: 15572
Epoch: [16]  [280/478]  eta: 0:01:55  lr: 0.000035  min_lr: 0.000000  loss: 4.5644 (4.6468)  loss_scale: 32768.0000 (52708.6690)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0828  max mem: 15572
Epoch: [16]  [290/478]  eta: 0:01:49  lr: 0.000035  min_lr: 0.000000  loss: 4.7049 (4.6488)  loss_scale: 32768.0000 (52023.4227)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0503  max mem: 15572
[2025-01-13 02:01:11,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:01:11,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [300/478]  eta: 0:01:43  lr: 0.000035  min_lr: 0.000000  loss: 4.6020 (4.6466)  loss_scale: 32768.0000 (52145.7542)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0856  max mem: 15572
Epoch: [16]  [310/478]  eta: 0:01:37  lr: 0.000035  min_lr: 0.000000  loss: 4.5661 (4.6442)  loss_scale: 65536.0000 (52576.3087)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1095  max mem: 15572
[2025-01-13 02:01:23,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=168, lr=[3.4091108956303983e-07, 3.4091108956303983e-07, 4.870158422329141e-07, 4.870158422329141e-07, 6.957369174755916e-07, 6.957369174755916e-07, 9.93909882107988e-07, 9.93909882107988e-07, 1.4198712601542687e-06, 1.4198712601542687e-06, 2.028387514506098e-06, 2.028387514506098e-06, 2.897696449294426e-06, 2.897696449294426e-06, 4.139566356134895e-06, 4.139566356134895e-06, 5.91366622304985e-06, 5.91366622304985e-06, 8.44809460435693e-06, 8.44809460435693e-06, 1.2068706577652756e-05, 1.2068706577652756e-05, 1.7241009396646797e-05, 1.7241009396646797e-05, 2.463001342378114e-05, 2.463001342378114e-05, 3.518573346254449e-05, 3.518573346254449e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:01:23,687] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=27.89683044680348, CurrSamplesPerSec=27.74492264695425, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [320/478]  eta: 0:01:31  lr: 0.000035  min_lr: 0.000000  loss: 4.6193 (4.6443)  loss_scale: 65536.0000 (52980.0374)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1546  max mem: 15572
Epoch: [16]  [330/478]  eta: 0:01:25  lr: 0.000035  min_lr: 0.000000  loss: 4.6216 (4.6431)  loss_scale: 65536.0000 (53359.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1490  max mem: 15572
Epoch: [16]  [340/478]  eta: 0:01:19  lr: 0.000035  min_lr: 0.000000  loss: 4.6303 (4.6431)  loss_scale: 65536.0000 (53716.4575)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0912  max mem: 15572
Epoch: [16]  [350/478]  eta: 0:01:14  lr: 0.000035  min_lr: 0.000000  loss: 4.7310 (4.6459)  loss_scale: 65536.0000 (54053.1966)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1285  max mem: 15572
Epoch: [16]  [360/478]  eta: 0:01:08  lr: 0.000035  min_lr: 0.000000  loss: 4.6242 (4.6445)  loss_scale: 65536.0000 (54371.2798)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.2090  max mem: 15572
Epoch: [16]  [370/478]  eta: 0:01:03  lr: 0.000035  min_lr: 0.000000  loss: 4.5754 (4.6416)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.7246  data: 0.2823  max mem: 15572
Epoch: [16]  [380/478]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000000  loss: 4.4207 (4.6374)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1887  max mem: 15572
Epoch: [16]  [390/478]  eta: 0:00:51  lr: 0.000035  min_lr: 0.000000  loss: 4.5501 (4.6374)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0604  max mem: 15572
Epoch: [16]  [400/478]  eta: 0:00:45  lr: 0.000035  min_lr: 0.000000  loss: 4.7021 (4.6405)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0454  max mem: 15572
Epoch: [16]  [410/478]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000000  loss: 4.7036 (4.6397)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0230  max mem: 15572
Epoch: [16]  [420/478]  eta: 0:00:33  lr: 0.000035  min_lr: 0.000000  loss: 4.6465 (4.6395)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0907  max mem: 15572
[2025-01-13 02:02:24,919] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:02:24,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:02:26,682] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26111
[2025-01-13 02:02:26,683] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:02:26,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [430/478]  eta: 0:00:27  lr: 0.000035  min_lr: 0.000000  loss: 4.5858 (4.6395)  loss_scale: 65536.0000 (56792.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1112  max mem: 15572
Epoch: [16]  [440/478]  eta: 0:00:21  lr: 0.000035  min_lr: 0.000000  loss: 4.6087 (4.6415)  loss_scale: 65536.0000 (56991.0567)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1229  max mem: 15572
Epoch: [16]  [450/478]  eta: 0:00:16  lr: 0.000035  min_lr: 0.000000  loss: 4.6229 (4.6394)  loss_scale: 65536.0000 (57180.5233)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1372  max mem: 15572
Epoch: [16]  [460/478]  eta: 0:00:10  lr: 0.000035  min_lr: 0.000000  loss: 4.6229 (4.6423)  loss_scale: 65536.0000 (57361.7701)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1010  max mem: 15572
Epoch: [16]  [470/478]  eta: 0:00:04  lr: 0.000035  min_lr: 0.000000  loss: 4.7162 (4.6432)  loss_scale: 65536.0000 (57535.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1172  max mem: 15572
Epoch: [16]  [477/478]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 4.6305 (4.6429)  loss_scale: 65536.0000 (57652.4854)  weight_decay: 0.0500 (0.0500)  time: 0.4583  data: 0.0560  max mem: 15572
Epoch: [16] Total time: 0:04:34 (0.5740 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000000  loss: 4.6305 (4.6429)  loss_scale: 65536.0000 (57652.4854)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 611
Indices to remove: tensor([  250,   251,   255,   270,   280,   288,   289,   292,   294,   338,
          464,   487,   499,   575,   597,   601,   625,   718,   750,   756,
          778,   803,   859,   937,  1165,  1193,  1278,  1289,  1364,  1445,
         1485,  1516,  1532,  1601,  1692,  1718,  1735,  1755,  1765,  1774,
         1811,  1812,  1817,  1857,  1870,  1878,  1893,  1914,  1919,  1948,
         1949,  1963,  1965,  1972,  2043,  2069,  2077,  2094,  2119,  2251,
         2262,  2269,  2298,  2350,  2498,  2503,  2512,  2541,  2556,  2573,
         2605,  2623,  2660,  2664,  2725,  2735,  2849,  2881,  2911,  2940,
         2947,  2950,  3031,  3141,  3153,  3158,  3173,  3198,  3233,  3258,
         3259,  3345,  3393,  3440,  3470,  3565,  3679,  4019,  4075,  4182,
         4316,  4326,  4408,  4411,  4442,  4617,  4654,  4668,  4673,  4706,
         4722,  4748,  4787,  4934,  5023,  5051,  5066,  5095,  5146,  5157,
         5201,  5204,  5472,  5570,  5878,  5904,  5921,  6024,  6116,  6136,
         6485,  6560,  6927,  6933,  6948,  6975,  6984,  7000,  7050,  7075,
         7139,  7686,  7687,  7710,  7824,  7878,  7890,  7929,  7933,  8003,
         8013,  8103,  8124,  8179,  8215,  8224,  8532,  8774,  8985,  8992,
         9160,  9176,  9446,  9662,  9677,  9694,  9730,  9841,  9953,  9972,
        10048, 10058, 10070, 10072, 10088, 10106, 10110, 10119, 10151, 10158,
        10391, 10440, 10445, 10476, 10575, 10618, 10651, 10668, 10677, 10690,
        10813, 10833, 10847, 10871, 10879, 10898, 10924, 11044, 11065, 11088,
        11098, 11113, 11132, 11149, 11185, 11208, 11212, 11219, 11319, 11335,
        11348, 11387, 11627, 11746, 11753, 11789, 11876, 11900, 12098, 12104,
        12115, 12125, 12126, 12127, 12129, 12149, 12164, 12241, 12272, 12282,
        12362, 12368, 12404, 12439, 12455, 12550, 12562, 12629, 12634, 12697,
        12765, 12866, 12900, 12924, 12981, 12989, 12995, 13033, 13056, 13082,
        13178, 13263, 13313, 13340, 13345, 13361, 13376, 13384, 13386, 13395,
        13408, 13415, 13460, 13464, 13606, 13705, 13732, 13746, 13780, 13798,
        13799, 13816, 13817, 13862, 13918, 13920, 13955, 13965, 14011, 14170,
        14261, 14352, 14366, 14407, 14434, 14458, 14460, 14515, 14521, 14604,
        14653, 14732, 14864, 14932, 14952, 14998, 15064, 15072, 15124, 15326,
        15353, 15380, 15390, 15442, 15464, 15531, 15551, 15600, 15603, 16023,
        16241, 16335, 16349, 16379, 16408, 16426, 16457, 16487, 16560, 16608,
        16609, 16618, 16626, 16633, 16638, 16642, 16827, 16973, 17839, 17864,
        17917, 18009, 18016, 18043, 18069, 18087, 18111, 18122, 18127, 18129,
        18130, 18133, 18262, 18272, 18485, 18660, 18664, 18678, 18686, 18697,
        18740, 18752, 18788, 18795, 18912, 18944, 18975, 19041, 19063, 19083,
        19094, 19251, 19357, 19535, 19538, 19572, 19574, 19577, 19702, 19798,
        19842, 19877, 19921, 19923, 19924, 19945, 19955, 19965, 19979, 19980,
        19985, 19998, 20021, 20064, 20067, 20070, 20125, 20166, 20267, 20278,
        20311, 20520, 20690, 20784, 20792, 20794, 20891, 21001, 21002, 21018,
        21052, 21339, 21450, 21672, 21766, 21861, 21870, 21886, 21889, 22075,
        22180, 22181, 22191, 22217, 22234, 22269, 22408, 22454, 22592, 22633,
        22667, 22726, 22734, 22752, 22758, 22761, 22866, 22899, 22922, 22926,
        22955, 23009, 23033, 23072, 23080, 23119, 23132, 23195, 23208, 23310,
        23339, 23508, 23615, 23633, 23697, 23704, 23873, 23887, 24148, 24192,
        24195, 24296, 24297, 24354, 24494, 24502, 24544, 24582, 24602, 24717,
        24729, 24787, 24846, 24855, 24856, 24884, 24926, 25033, 25187, 25236,
        25377, 25422, 25468, 25490, 25495, 25498, 25509, 25510, 25512, 25525,
        25531, 25534, 25539, 25542, 25547, 25569, 25591, 25654, 25709, 25757,
        25856, 25910, 25984, 25993, 26122, 26245, 26384, 26389, 26395, 26410,
        26452, 26485, 26520, 26674, 26714, 26732, 26733, 26734, 26789, 26851,
        26869, 27079, 27103, 27182, 27228, 27340, 27347, 27402, 27407, 27483,
        27492, 27494, 27583, 27670, 27718, 27823, 27824, 27856, 27872, 27890,
        28225, 28396, 28434, 28497, 28546, 28567, 28586, 28638, 28693, 28732,
        28736, 28778, 28835, 28852, 28967, 29079, 29236, 29625, 29861, 29919,
        30354, 30404, 30452, 30469, 30511, 30519, 30574, 30678, 30689, 30691,
        30703, 30706, 30839, 30848, 30901, 30922, 30947, 30949, 30957, 30975,
        30981, 31009, 31010, 31040, 31148, 31200, 31219, 31222, 31260, 31319,
        31337, 31351, 31384, 31390, 31401, 31472, 31533, 31559, 31750, 31751,
        32017, 32129, 32405, 32583, 32768, 32776, 32779, 32794, 32795, 32815,
        32838, 32879, 32914, 32923, 32932, 33039, 33103, 33236, 33451, 33521,
        33637], device='cuda:0')
length of data loader train is: 427
num_training_steps_per_epoch is: 427
Change step level LR scheduler!
Set warmup steps = 2135
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:29:27  loss: 2.1647 (2.1647)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 6.4972  data: 6.3229  max mem: 15572
Val:  [ 10/272]  eta: 0:03:56  loss: 3.4952 (3.4093)  acc1: 22.2222 (25.7576)  acc5: 55.5556 (58.0808)  time: 0.9011  data: 0.7036  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 3.3951 (3.3847)  acc1: 27.7778 (29.8942)  acc5: 55.5556 (59.7884)  time: 0.2657  data: 0.0711  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 3.3580 (3.4032)  acc1: 33.3333 (28.8530)  acc5: 61.1111 (60.0358)  time: 0.2168  data: 0.0135  max mem: 15572
Val:  [ 40/272]  eta: 0:01:43  loss: 3.3346 (3.3906)  acc1: 22.2222 (27.6423)  acc5: 66.6667 (61.1111)  time: 0.3275  data: 0.1144  max mem: 15572
Val:  [ 50/272]  eta: 0:01:34  loss: 3.3077 (3.3433)  acc1: 27.7778 (27.9956)  acc5: 66.6667 (62.8540)  time: 0.3784  data: 0.1754  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 2.9755 (3.2973)  acc1: 27.7778 (28.3242)  acc5: 72.2222 (63.9344)  time: 0.3085  data: 0.1124  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 2.9501 (3.2755)  acc1: 27.7778 (30.0469)  acc5: 72.2222 (64.9452)  time: 0.2945  data: 0.1056  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.0905 (3.2585)  acc1: 33.3333 (30.4527)  acc5: 66.6667 (65.5693)  time: 0.3056  data: 0.1046  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 3.5146 (3.3020)  acc1: 22.2222 (29.4261)  acc5: 55.5556 (64.0415)  time: 0.3161  data: 0.1053  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.6017 (3.3365)  acc1: 16.6667 (27.9978)  acc5: 55.5556 (63.2563)  time: 0.3175  data: 0.1040  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 3.6838 (3.3720)  acc1: 11.1111 (26.2262)  acc5: 44.4444 (61.7117)  time: 0.3108  data: 0.0950  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.6838 (3.3779)  acc1: 16.6667 (26.9513)  acc5: 55.5556 (61.8916)  time: 0.3113  data: 0.0960  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.1552 (3.3465)  acc1: 44.4444 (28.5411)  acc5: 66.6667 (63.0619)  time: 0.3141  data: 0.0965  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.9982 (3.3384)  acc1: 44.4444 (28.8022)  acc5: 72.2222 (63.2782)  time: 0.3303  data: 0.1223  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.1683 (3.3265)  acc1: 33.3333 (29.1023)  acc5: 66.6667 (63.6130)  time: 0.3386  data: 0.1370  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.2881 (3.3316)  acc1: 22.2222 (28.5714)  acc5: 66.6667 (63.6646)  time: 0.3317  data: 0.1245  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.4622 (3.3457)  acc1: 16.6667 (28.1676)  acc5: 55.5556 (62.9630)  time: 0.3023  data: 0.0990  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3038 (3.3308)  acc1: 22.2222 (28.1461)  acc5: 66.6667 (63.3824)  time: 0.3005  data: 0.1057  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.1989 (3.3455)  acc1: 22.2222 (27.6905)  acc5: 61.1111 (62.5945)  time: 0.3644  data: 0.1685  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.2797 (3.3429)  acc1: 22.2222 (27.7778)  acc5: 55.5556 (62.6866)  time: 0.3346  data: 0.1478  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.1651 (3.3464)  acc1: 27.7778 (27.8041)  acc5: 77.7778 (62.9542)  time: 0.2899  data: 0.0965  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.3544 (3.3512)  acc1: 27.7778 (27.7526)  acc5: 66.6667 (62.7200)  time: 0.3270  data: 0.1296  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.1279 (3.3397)  acc1: 33.3333 (28.6436)  acc5: 66.6667 (63.3237)  time: 0.3309  data: 0.1403  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.9883 (3.3272)  acc1: 33.3333 (28.6307)  acc5: 83.3333 (63.9926)  time: 0.3117  data: 0.1247  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1134 (3.3320)  acc1: 16.6667 (28.2869)  acc5: 72.2222 (63.8557)  time: 0.2719  data: 0.0824  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.1471 (3.3148)  acc1: 33.3333 (29.0762)  acc5: 77.7778 (64.7297)  time: 0.2637  data: 0.0710  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2747 (3.3152)  acc1: 33.3333 (29.0078)  acc5: 77.7778 (64.8626)  time: 0.2148  data: 0.0426  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2747 (3.3173)  acc1: 33.3333 (28.9986)  acc5: 77.7778 (64.8372)  time: 0.2089  data: 0.0426  max mem: 15572
Val: Total time: 0:01:29 (0.3283 s / it)
* Acc@1 28.999 Acc@5 64.837 loss 3.317
Accuracy of the network on the 4883 val videos: 29.0%
Max accuracy: 29.31%
Epoch: [17]  [  0/427]  eta: 0:53:43  lr: 0.000035  min_lr: 0.000000  loss: 4.7377 (4.7377)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.5502  data: 7.0585  max mem: 15572
Epoch: [17]  [ 10/427]  eta: 0:08:43  lr: 0.000035  min_lr: 0.000000  loss: 4.7053 (4.7319)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2557  data: 0.8086  max mem: 15572
Epoch: [17]  [ 20/427]  eta: 0:06:37  lr: 0.000034  min_lr: 0.000000  loss: 4.6692 (4.7238)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.1945  max mem: 15572
Epoch: [17]  [ 30/427]  eta: 0:05:19  lr: 0.000034  min_lr: 0.000000  loss: 4.6692 (4.7066)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1030  max mem: 15572
Epoch: [17]  [ 40/427]  eta: 0:04:40  lr: 0.000034  min_lr: 0.000000  loss: 4.6233 (4.6823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4594  data: 0.0161  max mem: 15572
Epoch: [17]  [ 50/427]  eta: 0:04:26  lr: 0.000034  min_lr: 0.000000  loss: 4.5387 (4.6665)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1078  max mem: 15572
Epoch: [17]  [ 60/427]  eta: 0:04:10  lr: 0.000034  min_lr: 0.000000  loss: 4.5358 (4.6502)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1495  max mem: 15572
Epoch: [17]  [ 70/427]  eta: 0:04:04  lr: 0.000034  min_lr: 0.000000  loss: 4.5253 (4.6384)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1794  max mem: 15572
[2025-01-13 02:05:15,985] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:05:15,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:05:17,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26243
[2025-01-13 02:05:17,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:05:17,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 80/427]  eta: 0:03:51  lr: 0.000034  min_lr: 0.000000  loss: 4.6820 (4.6457)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1599  max mem: 15572
Epoch: [17]  [ 90/427]  eta: 0:03:39  lr: 0.000034  min_lr: 0.000000  loss: 4.6471 (4.6407)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0720  max mem: 15572
Epoch: [17]  [100/427]  eta: 0:03:28  lr: 0.000034  min_lr: 0.000000  loss: 4.6524 (4.6566)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0780  max mem: 15572
Epoch: [17]  [110/427]  eta: 0:03:19  lr: 0.000034  min_lr: 0.000000  loss: 4.5940 (4.6487)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0803  max mem: 15572
Epoch: [17]  [120/427]  eta: 0:03:11  lr: 0.000034  min_lr: 0.000000  loss: 4.6050 (4.6584)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0887  max mem: 15572
Epoch: [17]  [130/427]  eta: 0:03:04  lr: 0.000034  min_lr: 0.000000  loss: 4.7226 (4.6574)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1198  max mem: 15572
Epoch: [17]  [140/427]  eta: 0:02:57  lr: 0.000034  min_lr: 0.000000  loss: 4.6272 (4.6622)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1404  max mem: 15572
Epoch: [17]  [150/427]  eta: 0:02:52  lr: 0.000034  min_lr: 0.000000  loss: 4.6272 (4.6579)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1752  max mem: 15572
Epoch: [17]  [160/427]  eta: 0:02:45  lr: 0.000034  min_lr: 0.000000  loss: 4.6564 (4.6616)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1926  max mem: 15572
Epoch: [17]  [170/427]  eta: 0:02:37  lr: 0.000034  min_lr: 0.000000  loss: 4.6690 (4.6558)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1090  max mem: 15572
Epoch: [17]  [180/427]  eta: 0:02:30  lr: 0.000034  min_lr: 0.000000  loss: 4.5916 (4.6562)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0514  max mem: 15572
Epoch: [17]  [190/427]  eta: 0:02:23  lr: 0.000034  min_lr: 0.000000  loss: 4.6688 (4.6589)  loss_scale: 65536.0000 (66565.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0747  max mem: 15572
Epoch: [17]  [200/427]  eta: 0:02:16  lr: 0.000034  min_lr: 0.000000  loss: 4.7163 (4.6598)  loss_scale: 65536.0000 (66514.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1010  max mem: 15572
[2025-01-13 02:06:29,634] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:06:29,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [210/427]  eta: 0:02:10  lr: 0.000034  min_lr: 0.000000  loss: 4.7667 (4.6659)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1110  max mem: 15572
[2025-01-13 02:06:30,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26374
[2025-01-13 02:06:30,535] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:06:30,535] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [220/427]  eta: 0:02:04  lr: 0.000034  min_lr: 0.000000  loss: 4.7650 (4.6659)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1277  max mem: 15572
Epoch: [17]  [230/427]  eta: 0:01:57  lr: 0.000034  min_lr: 0.000000  loss: 4.6596 (4.6663)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1320  max mem: 15572
Epoch: [17]  [240/427]  eta: 0:01:50  lr: 0.000034  min_lr: 0.000000  loss: 4.6650 (4.6656)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0816  max mem: 15572
Epoch: [17]  [250/427]  eta: 0:01:44  lr: 0.000033  min_lr: 0.000000  loss: 4.6270 (4.6624)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0780  max mem: 15572
Epoch: [17]  [260/427]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000000  loss: 4.6272 (4.6644)  loss_scale: 65536.0000 (66791.4789)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0999  max mem: 15572
Epoch: [17]  [270/427]  eta: 0:01:32  lr: 0.000033  min_lr: 0.000000  loss: 4.6272 (4.6611)  loss_scale: 65536.0000 (66745.1513)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1299  max mem: 15572
Epoch: [17]  [280/427]  eta: 0:01:26  lr: 0.000033  min_lr: 0.000000  loss: 4.6137 (4.6631)  loss_scale: 65536.0000 (66702.1210)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1605  max mem: 15572
Epoch: [17]  [290/427]  eta: 0:01:20  lr: 0.000033  min_lr: 0.000000  loss: 4.6996 (4.6625)  loss_scale: 65536.0000 (66662.0481)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1144  max mem: 15572
Epoch: [17]  [300/427]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000000  loss: 4.7000 (4.6641)  loss_scale: 65536.0000 (66624.6379)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0558  max mem: 15572
Epoch: [17]  [310/427]  eta: 0:01:08  lr: 0.000033  min_lr: 0.000000  loss: 4.7060 (4.6641)  loss_scale: 65536.0000 (66589.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0806  max mem: 15572
Epoch: [17]  [320/427]  eta: 0:01:02  lr: 0.000033  min_lr: 0.000000  loss: 4.6642 (4.6635)  loss_scale: 65536.0000 (66556.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1219  max mem: 15572
Epoch: [17]  [330/427]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000000  loss: 4.6379 (4.6638)  loss_scale: 65536.0000 (66525.9698)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1152  max mem: 15572
[2025-01-13 02:07:43,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:07:43,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [340/427]  eta: 0:00:50  lr: 0.000033  min_lr: 0.000000  loss: 4.7512 (4.6650)  loss_scale: 65536.0000 (66689.1261)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1424  max mem: 15572
[2025-01-13 02:07:44,776] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26506
[2025-01-13 02:07:44,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:07:44,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [350/427]  eta: 0:00:45  lr: 0.000033  min_lr: 0.000000  loss: 4.7015 (4.6653)  loss_scale: 65536.0000 (67029.6980)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.1762  max mem: 15572
Epoch: [17]  [360/427]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000000  loss: 4.6634 (4.6665)  loss_scale: 65536.0000 (66988.3213)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1790  max mem: 15572
Epoch: [17]  [370/427]  eta: 0:00:33  lr: 0.000033  min_lr: 0.000000  loss: 4.6891 (4.6672)  loss_scale: 65536.0000 (66949.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1611  max mem: 15572
Epoch: [17]  [380/427]  eta: 0:00:27  lr: 0.000033  min_lr: 0.000000  loss: 4.7614 (4.6715)  loss_scale: 65536.0000 (66912.0840)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1308  max mem: 15572
Epoch: [17]  [390/427]  eta: 0:00:21  lr: 0.000033  min_lr: 0.000000  loss: 4.7953 (4.6733)  loss_scale: 65536.0000 (66876.8900)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1576  max mem: 15572
Epoch: [17]  [400/427]  eta: 0:00:15  lr: 0.000033  min_lr: 0.000000  loss: 4.7255 (4.6740)  loss_scale: 65536.0000 (66843.4514)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1392  max mem: 15572
Epoch: [17]  [410/427]  eta: 0:00:09  lr: 0.000033  min_lr: 0.000000  loss: 4.7255 (4.6746)  loss_scale: 65536.0000 (66811.6399)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0925  max mem: 15572
Epoch: [17]  [420/427]  eta: 0:00:04  lr: 0.000033  min_lr: 0.000000  loss: 4.7390 (4.6757)  loss_scale: 65536.0000 (66781.3397)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0939  max mem: 15572
Epoch: [17]  [426/427]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000000  loss: 4.7390 (4.6770)  loss_scale: 65536.0000 (66763.8407)  weight_decay: 0.0500 (0.0500)  time: 0.4551  data: 0.0436  max mem: 15572
Epoch: [17] Total time: 0:04:09 (0.5833 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000000  loss: 4.7390 (4.6770)  loss_scale: 65536.0000 (66763.8407)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 537
Indices to remove: tensor([   16,   117,   253,   261,   323,   345,   371,   381,   395,   415,
          431,   555,   605,   656,   675,   694,   704,   749,   753,   760,
          891,   916,  1111,  1204,  1267,  1475,  1501,  1527,  1619,  1669,
         1685,  1706,  1724,  1840,  1856,  1871,  1895,  2058,  2079,  2091,
         2149,  2210,  2215,  2303,  2313,  2326,  2328,  2345,  2351,  2388,
         2415,  2444,  2468,  2506,  2520,  2613,  2721,  2791,  2841,  2891,
         2935,  3073,  3176,  3189,  3199,  3203,  3215,  3238,  3256,  3268,
         3294,  3333,  3370,  3391,  3399,  3478,  3525,  3536,  3851,  4026,
         4033,  4067,  4375,  4503,  4519,  4530,  4541,  4575,  4647,  4695,
         4700,  4822,  4874,  4904,  4939,  4947,  4960,  4962,  5028,  5039,
         5040,  5097,  5285,  5353,  5527,  5739,  6095,  6219,  6270,  6462,
         6492,  6528,  6896,  6910,  6954,  6985,  6994,  7005,  7055,  7065,
         7080,  7109,  7118,  7175,  7239,  7343,  7417,  7925,  7927,  8086,
         8134,  8349,  8440,  8981,  9167,  9402,  9416,  9554,  9621,  9766,
         9973, 10022, 10063, 10065, 10079, 10103, 10140, 10147, 10180, 10364,
        10478, 10533, 10617, 10669, 10685, 10845, 10862, 10869, 10909, 10919,
        10982, 11009, 11018, 11049, 11059, 11061, 11081, 11151, 11165, 11284,
        11425, 11810, 11826, 11846, 11857, 12114, 12119, 12135, 12154, 12162,
        12165, 12170, 12179, 12181, 12191, 12221, 12222, 12322, 12350, 12494,
        12507, 12569, 12575, 12669, 12670, 12723, 12813, 12919, 13005, 13164,
        13282, 13302, 13311, 13387, 13414, 13651, 13690, 13696, 13793, 13820,
        13923, 13991, 14017, 14031, 14305, 14308, 14342, 14386, 14411, 14415,
        14423, 14428, 14449, 14452, 14534, 14553, 14602, 14613, 14641, 14747,
        14751, 14763, 14789, 14792, 14835, 14849, 14881, 14890, 14962, 15094,
        15095, 15101, 15109, 15195, 15222, 15228, 15292, 15369, 15403, 15415,
        15456, 15611, 15656, 15705, 15706, 16069, 16417, 16455, 16494, 16551,
        16578, 16666, 16935, 17228, 17272, 17432, 17854, 17910, 17940, 17948,
        17949, 17982, 17991, 18012, 18014, 18092, 18107, 18123, 18153, 18225,
        18251, 18328, 18512, 18624, 18684, 18911, 19174, 19432, 19433, 19457,
        19668, 19764, 19792, 19814, 19910, 19929, 19962, 19971, 20035, 20100,
        20139, 20314, 20397, 20417, 20568, 20609, 20862, 20989, 21048, 21051,
        21056, 21244, 21282, 21299, 21465, 21478, 21708, 21754, 21769, 21772,
        21799, 21894, 21927, 21938, 21940, 22037, 22059, 22167, 22195, 22218,
        22230, 22255, 22284, 22318, 22372, 22375, 22535, 22606, 22638, 22648,
        22649, 22660, 22695, 22701, 22743, 22745, 22777, 22826, 22939, 22984,
        23028, 23098, 23281, 23416, 23471, 23543, 23592, 23616, 23617, 23718,
        23723, 23733, 23734, 24065, 24066, 24070, 24095, 24256, 24278, 24372,
        24373, 24378, 24473, 24526, 24534, 24647, 24748, 24776, 24821, 24824,
        24830, 24847, 24857, 24932, 24993, 25128, 25193, 25201, 25341, 25346,
        25362, 25444, 25457, 25492, 25520, 25521, 25532, 25533, 25572, 25610,
        25673, 25678, 25684, 25700, 25853, 25913, 25932, 25980, 26005, 26020,
        26275, 26322, 26340, 26367, 26460, 26610, 26618, 26633, 26698, 26745,
        26752, 26761, 26800, 26804, 26805, 26815, 26843, 26848, 26892, 27016,
        27117, 27179, 27192, 27219, 27222, 27241, 27284, 27386, 27389, 27395,
        27467, 27475, 27641, 27731, 27837, 27874, 27887, 28017, 28018, 28041,
        28048, 28111, 28141, 28197, 28308, 28344, 28352, 28379, 28468, 28516,
        28763, 28840, 29049, 29276, 29335, 29360, 29622, 29644, 29976, 30051,
        30198, 30242, 30251, 30292, 30422, 30442, 30449, 30470, 30472, 30486,
        30498, 30539, 30544, 30567, 30607, 30745, 30769, 30791, 30865, 30900,
        30920, 30931, 30934, 30961, 31036, 31166, 31317, 31332, 31335, 31363,
        31396, 31403, 31409, 31411, 31441, 31549, 31574, 31591, 31593, 31867,
        31886, 31964, 32105, 32504, 32813, 32874, 32890, 32894, 32895, 32899,
        32903, 32931, 32994, 33028, 33072, 33165, 33292, 33336, 33441, 33449,
        33457, 33490, 33512, 33584, 33629, 33660, 33702], device='cuda:0')
length of data loader train is: 382
num_training_steps_per_epoch is: 382
Change step level LR scheduler!
Set warmup steps = 1910
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:17:39  loss: 2.0642 (2.0642)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 3.8951  data: 3.7153  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 3.6975 (3.5854)  acc1: 11.1111 (20.7071)  acc5: 50.0000 (51.0101)  time: 0.7715  data: 0.5691  max mem: 15572
Val:  [ 20/272]  eta: 0:02:19  loss: 3.6115 (3.5164)  acc1: 16.6667 (23.8095)  acc5: 50.0000 (55.0265)  time: 0.3848  data: 0.1838  max mem: 15572
Val:  [ 30/272]  eta: 0:01:55  loss: 3.4146 (3.5113)  acc1: 22.2222 (22.4014)  acc5: 55.5556 (55.3763)  time: 0.3123  data: 0.1176  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 3.2540 (3.4323)  acc1: 22.2222 (22.7642)  acc5: 66.6667 (59.2141)  time: 0.3100  data: 0.1157  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 3.1725 (3.3731)  acc1: 27.7778 (24.9455)  acc5: 72.2222 (61.6558)  time: 0.3290  data: 0.1157  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 3.0460 (3.3309)  acc1: 27.7778 (24.4080)  acc5: 72.2222 (62.8415)  time: 0.3287  data: 0.1195  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 3.0460 (3.3052)  acc1: 33.3333 (27.6995)  acc5: 72.2222 (64.2410)  time: 0.3300  data: 0.1352  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 3.2397 (3.3019)  acc1: 33.3333 (28.3265)  acc5: 66.6667 (64.6091)  time: 0.3425  data: 0.1411  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 3.5539 (3.3371)  acc1: 27.7778 (28.1441)  acc5: 55.5556 (63.6142)  time: 0.3328  data: 0.1308  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 3.5732 (3.3676)  acc1: 22.2222 (27.0627)  acc5: 50.0000 (62.5963)  time: 0.3467  data: 0.1442  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.7383 (3.3991)  acc1: 11.1111 (25.5756)  acc5: 50.0000 (61.5616)  time: 0.3111  data: 0.1078  max mem: 15572
Val:  [120/272]  eta: 0:00:56  loss: 3.5059 (3.4108)  acc1: 11.1111 (25.9412)  acc5: 55.5556 (61.1570)  time: 0.3301  data: 0.1210  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 3.2238 (3.3858)  acc1: 27.7778 (27.4385)  acc5: 66.6667 (62.2137)  time: 0.3936  data: 0.1725  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 3.1513 (3.3815)  acc1: 44.4444 (28.0536)  acc5: 72.2222 (62.3325)  time: 0.3565  data: 0.1285  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 3.2872 (3.3750)  acc1: 27.7778 (28.1089)  acc5: 55.5556 (62.2884)  time: 0.3092  data: 0.0851  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 3.4302 (3.3806)  acc1: 22.2222 (27.7778)  acc5: 55.5556 (61.9393)  time: 0.3092  data: 0.1003  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.5157 (3.3948)  acc1: 11.1111 (27.0630)  acc5: 55.5556 (61.4685)  time: 0.3141  data: 0.1226  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.4083 (3.3751)  acc1: 16.6667 (27.2253)  acc5: 66.6667 (62.1854)  time: 0.3220  data: 0.1337  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 3.2463 (3.3832)  acc1: 22.2222 (26.7307)  acc5: 66.6667 (61.5183)  time: 0.3241  data: 0.1359  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.3085 (3.3725)  acc1: 22.2222 (27.0315)  acc5: 61.1111 (61.9956)  time: 0.3265  data: 0.1451  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.2198 (3.3800)  acc1: 27.7778 (26.8299)  acc5: 72.2222 (62.0326)  time: 0.3229  data: 0.1316  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.4849 (3.3842)  acc1: 27.7778 (26.9734)  acc5: 66.6667 (61.7647)  time: 0.2987  data: 0.1037  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.3636 (3.3830)  acc1: 27.7778 (27.2487)  acc5: 72.2222 (62.0972)  time: 0.3152  data: 0.1265  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.2215 (3.3714)  acc1: 33.3333 (27.2706)  acc5: 72.2222 (62.6325)  time: 0.3274  data: 0.1249  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2448 (3.3738)  acc1: 22.2222 (26.8703)  acc5: 72.2222 (62.5055)  time: 0.3198  data: 0.1240  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.1622 (3.3549)  acc1: 27.7778 (27.7991)  acc5: 72.2222 (63.3674)  time: 0.2753  data: 0.0966  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2814 (3.3576)  acc1: 33.3333 (27.7983)  acc5: 72.2222 (63.4481)  time: 0.1961  data: 0.0316  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2814 (3.3604)  acc1: 27.7778 (27.7903)  acc5: 72.2222 (63.4446)  time: 0.1745  data: 0.0194  max mem: 15572
Val: Total time: 0:01:31 (0.3352 s / it)
* Acc@1 27.790 Acc@5 63.445 loss 3.360
Accuracy of the network on the 4883 val videos: 27.8%
Max accuracy: 29.31%
Epoch: [18]  [  0/382]  eta: 0:47:52  lr: 0.000033  min_lr: 0.000000  loss: 4.7918 (4.7918)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.5188  data: 7.0373  max mem: 15572
Epoch: [18]  [ 10/382]  eta: 0:07:35  lr: 0.000033  min_lr: 0.000000  loss: 4.7918 (4.6856)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2237  data: 0.7379  max mem: 15572
Epoch: [18]  [ 20/382]  eta: 0:05:26  lr: 0.000033  min_lr: 0.000000  loss: 4.6829 (4.6734)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0973  max mem: 15572
Epoch: [18]  [ 30/382]  eta: 0:04:44  lr: 0.000033  min_lr: 0.000000  loss: 4.5799 (4.6466)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1163  max mem: 15572
Epoch: [18]  [ 40/382]  eta: 0:04:16  lr: 0.000032  min_lr: 0.000000  loss: 4.5799 (4.6365)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1442  max mem: 15572
[2025-01-13 02:10:37,017] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:10:37,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:10:37,490] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26636
[2025-01-13 02:10:37,491] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:10:37,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 50/382]  eta: 0:03:52  lr: 0.000032  min_lr: 0.000000  loss: 4.5460 (4.6111)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0793  max mem: 15572
Epoch: [18]  [ 60/382]  eta: 0:03:42  lr: 0.000032  min_lr: 0.000000  loss: 4.5971 (4.6149)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0864  max mem: 15572
Epoch: [18]  [ 70/382]  eta: 0:03:35  lr: 0.000032  min_lr: 0.000000  loss: 4.6835 (4.6198)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.1836  max mem: 15572
Epoch: [18]  [ 80/382]  eta: 0:03:19  lr: 0.000032  min_lr: 0.000000  loss: 4.7688 (4.6385)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1095  max mem: 15572
Epoch: [18]  [ 90/382]  eta: 0:03:13  lr: 0.000032  min_lr: 0.000000  loss: 4.7426 (4.6470)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1225  max mem: 15572
Epoch: [18]  [100/382]  eta: 0:03:03  lr: 0.000032  min_lr: 0.000000  loss: 4.7065 (4.6523)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1506  max mem: 15572
Epoch: [18]  [110/382]  eta: 0:02:56  lr: 0.000032  min_lr: 0.000000  loss: 4.6206 (4.6495)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1180  max mem: 15572
Epoch: [18]  [120/382]  eta: 0:02:47  lr: 0.000032  min_lr: 0.000000  loss: 4.7016 (4.6580)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1093  max mem: 15572
Epoch: [18]  [130/382]  eta: 0:02:40  lr: 0.000032  min_lr: 0.000000  loss: 4.6987 (4.6575)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0734  max mem: 15572
[2025-01-13 02:11:27,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26721
[2025-01-13 02:11:27,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:11:27,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [140/382]  eta: 0:02:32  lr: 0.000032  min_lr: 0.000000  loss: 4.6873 (4.6623)  loss_scale: 32768.0000 (63676.8227)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1117  max mem: 15572
Epoch: [18]  [150/382]  eta: 0:02:25  lr: 0.000032  min_lr: 0.000000  loss: 4.6208 (4.6581)  loss_scale: 32768.0000 (61629.8808)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.0920  max mem: 15572
Epoch: [18]  [160/382]  eta: 0:02:18  lr: 0.000032  min_lr: 0.000000  loss: 4.6090 (4.6571)  loss_scale: 32768.0000 (59837.2174)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.0541  max mem: 15572
Epoch: [18]  [170/382]  eta: 0:02:11  lr: 0.000032  min_lr: 0.000000  loss: 4.6090 (4.6499)  loss_scale: 32768.0000 (58254.2222)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0459  max mem: 15572
Epoch: [18]  [180/382]  eta: 0:02:04  lr: 0.000032  min_lr: 0.000000  loss: 4.5610 (4.6501)  loss_scale: 32768.0000 (56846.1436)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1004  max mem: 15572
Epoch: [18]  [190/382]  eta: 0:01:58  lr: 0.000032  min_lr: 0.000000  loss: 4.6986 (4.6542)  loss_scale: 32768.0000 (55585.5079)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1590  max mem: 15572
Epoch: [18]  [200/382]  eta: 0:01:51  lr: 0.000032  min_lr: 0.000000  loss: 4.7448 (4.6567)  loss_scale: 32768.0000 (54450.3085)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1285  max mem: 15572
Epoch: [18]  [210/382]  eta: 0:01:44  lr: 0.000032  min_lr: 0.000000  loss: 4.7031 (4.6610)  loss_scale: 32768.0000 (53422.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0486  max mem: 15572
Epoch: [18]  [220/382]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000000  loss: 4.7031 (4.6632)  loss_scale: 32768.0000 (52488.1086)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0431  max mem: 15572
Epoch: [18]  [230/382]  eta: 0:01:31  lr: 0.000031  min_lr: 0.000000  loss: 4.7227 (4.6640)  loss_scale: 32768.0000 (51634.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0826  max mem: 15572
Epoch: [18]  [240/382]  eta: 0:01:25  lr: 0.000031  min_lr: 0.000000  loss: 4.7684 (4.6700)  loss_scale: 32768.0000 (50851.5851)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1175  max mem: 15572
Epoch: [18]  [250/382]  eta: 0:01:18  lr: 0.000031  min_lr: 0.000000  loss: 4.7128 (4.6658)  loss_scale: 32768.0000 (50131.1235)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0904  max mem: 15572
[2025-01-13 02:12:39,037] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:12:39,037] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [260/382]  eta: 0:01:12  lr: 0.000031  min_lr: 0.000000  loss: 4.6344 (4.6679)  loss_scale: 32768.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0750  max mem: 15572
Epoch: [18]  [270/382]  eta: 0:01:06  lr: 0.000031  min_lr: 0.000000  loss: 4.6562 (4.6692)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1251  max mem: 15572
[2025-01-13 02:12:49,905] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26870
[2025-01-13 02:12:49,906] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:12:49,906] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [280/382]  eta: 0:01:00  lr: 0.000031  min_lr: 0.000000  loss: 4.7473 (4.6679)  loss_scale: 65536.0000 (50609.6512)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0633  max mem: 15572
Epoch: [18]  [290/382]  eta: 0:00:54  lr: 0.000031  min_lr: 0.000000  loss: 4.7547 (4.6718)  loss_scale: 32768.0000 (49996.5361)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0355  max mem: 15572
Epoch: [18]  [300/382]  eta: 0:00:47  lr: 0.000031  min_lr: 0.000000  loss: 4.7284 (4.6728)  loss_scale: 32768.0000 (49424.1595)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0353  max mem: 15572
Epoch: [18]  [310/382]  eta: 0:00:41  lr: 0.000031  min_lr: 0.000000  loss: 4.7149 (4.6745)  loss_scale: 32768.0000 (48888.5916)  weight_decay: 0.0500 (0.0500)  time: 0.4668  data: 0.0009  max mem: 15572
Epoch: [18]  [320/382]  eta: 0:00:35  lr: 0.000031  min_lr: 0.000000  loss: 4.6486 (4.6715)  loss_scale: 32768.0000 (48386.3925)  weight_decay: 0.0500 (0.0500)  time: 0.4806  data: 0.0010  max mem: 15572
Epoch: [18]  [330/382]  eta: 0:00:29  lr: 0.000031  min_lr: 0.000000  loss: 4.6099 (4.6730)  loss_scale: 32768.0000 (47914.5378)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0013  max mem: 15572
Epoch: [18]  [340/382]  eta: 0:00:24  lr: 0.000031  min_lr: 0.000000  loss: 4.6391 (4.6686)  loss_scale: 32768.0000 (47470.3578)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0264  max mem: 15572
Epoch: [18]  [350/382]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000000  loss: 4.6391 (4.6701)  loss_scale: 32768.0000 (47051.4872)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1521  max mem: 15572
Epoch: [18]  [360/382]  eta: 0:00:12  lr: 0.000031  min_lr: 0.000000  loss: 4.7095 (4.6726)  loss_scale: 32768.0000 (46655.8227)  weight_decay: 0.0500 (0.0500)  time: 0.7099  data: 0.2396  max mem: 15572
Epoch: [18]  [370/382]  eta: 0:00:06  lr: 0.000031  min_lr: 0.000000  loss: 4.7095 (4.6741)  loss_scale: 32768.0000 (46281.4879)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.1887  max mem: 15572
Epoch: [18]  [380/382]  eta: 0:00:01  lr: 0.000031  min_lr: 0.000000  loss: 4.7115 (4.6733)  loss_scale: 32768.0000 (45926.8031)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1283  max mem: 15572
Epoch: [18]  [381/382]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000000  loss: 4.7007 (4.6733)  loss_scale: 32768.0000 (45892.3560)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1282  max mem: 15572
Epoch: [18] Total time: 0:03:42 (0.5812 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000000  loss: 4.7007 (4.6733)  loss_scale: 32768.0000 (45892.3560)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 455
Indices to remove: tensor([  222,   247,   277,   297,   378,   491,   496,   665,   832,   870,
         1005,  1150,  1474,  1546,  1573,  1596,  1602,  1626,  1641,  1651,
         1664,  1689,  1713,  1743,  1750,  1922,  2025,  2140,  2189,  2250,
         2294,  2399,  2402,  2548,  2577,  2918,  2990,  2995,  3097,  3114,
         3245,  3246,  3271,  3284,  3418,  3577,  3702,  4024,  4030,  4061,
         4138,  4410,  4455,  4487,  4540,  4565,  4607,  4629,  4834,  4835,
         4836,  4931,  4946,  4956,  4986,  5020,  5090,  5175,  6045,  6187,
         6244,  6292,  6414,  6479,  6770,  6894,  6936,  6960,  6996,  7004,
         7102,  7129,  7138,  7366,  7419,  7607,  7862,  7888,  7893,  7895,
         7905,  7931,  8189,  8258,  8428,  8430,  8476,  8514,  8574,  8585,
         8709,  8896,  9118,  9685,  9734,  9840,  9983, 10113, 10132, 10257,
        10400, 10424, 10427, 10542, 10691, 10712, 10859, 10876, 10897, 10901,
        10938, 11041, 11066, 11129, 11131, 11209, 11659, 11669, 11740, 11755,
        11762, 11796, 11862, 11864, 11871, 11897, 11904, 11906, 11909, 11910,
        12103, 12134, 12142, 12180, 12182, 12184, 12198, 12243, 12302, 12314,
        12331, 12351, 12355, 12415, 12478, 12481, 12529, 12592, 12604, 12652,
        12691, 12693, 12806, 12818, 12848, 12861, 12874, 12943, 12946, 12991,
        13021, 13105, 13112, 13142, 13227, 13280, 13306, 13388, 13401, 13402,
        13442, 13479, 13481, 13608, 13654, 13709, 13731, 13819, 13912, 13999,
        14124, 14198, 14361, 14376, 14555, 14556, 14570, 14615, 14632, 14714,
        14839, 14861, 15133, 15149, 15191, 15198, 15544, 15572, 15698, 15787,
        15997, 16012, 16351, 16360, 16367, 16390, 16446, 16511, 16702, 17455,
        17548, 17551, 17710, 17712, 17743, 17824, 17882, 17901, 17924, 18138,
        18187, 18300, 18335, 18447, 18521, 18564, 18623, 18654, 18665, 18766,
        18885, 18892, 19107, 19375, 19404, 19414, 19460, 19496, 19539, 19599,
        19800, 20076, 20223, 20283, 20318, 20457, 20471, 20528, 20544, 20562,
        20566, 20580, 20673, 20858, 20906, 21022, 21222, 21267, 21279, 21808,
        21930, 22000, 22086, 22088, 22098, 22106, 22131, 22156, 22172, 22175,
        22319, 22458, 22659, 22669, 22685, 22688, 22697, 22709, 22713, 22716,
        22803, 22874, 22900, 22914, 22930, 23043, 23148, 23216, 23274, 23328,
        23390, 23408, 23445, 23488, 23498, 23596, 23642, 23719, 23808, 23953,
        24063, 24307, 24387, 24452, 24529, 24578, 24589, 24608, 24683, 24792,
        24844, 24860, 24893, 25032, 25054, 25067, 25106, 25155, 25424, 25439,
        25487, 25615, 25631, 25635, 25639, 25671, 25718, 25735, 25772, 25778,
        25844, 25868, 25903, 25970, 25976, 26128, 26142, 26226, 26232, 26239,
        26248, 26300, 26327, 26399, 26401, 26412, 26415, 26420, 26597, 26603,
        26673, 26743, 26792, 26801, 26813, 26835, 26935, 27011, 27277, 27286,
        27318, 27329, 27453, 27463, 27471, 27506, 27536, 27548, 27615, 27698,
        27760, 27791, 28065, 28137, 28252, 28288, 28363, 28372, 28382, 28392,
        28485, 28505, 28552, 28771, 29124, 29204, 29309, 29315, 29653, 29798,
        29897, 29900, 29947, 29952, 29998, 30108, 30129, 30249, 30372, 30523,
        30596, 30620, 30632, 30657, 30764, 30929, 30935, 30953, 30965, 31031,
        31037, 31126, 31151, 31190, 31256, 31327, 31362, 31369, 31374, 31377,
        31485, 31565, 31850, 32011, 32495, 32845, 32872, 32917, 32926, 33009,
        33035, 33243, 33373, 33382, 33446, 33448, 33463, 33514, 33526, 33528,
        33531, 33548, 33653, 33680, 33697], device='cuda:0')
length of data loader train is: 344
num_training_steps_per_epoch is: 344
Change step level LR scheduler!
Set warmup steps = 1720
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:26:41  loss: 2.0911 (2.0911)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 5.8892  data: 5.6385  max mem: 15572
Val:  [ 10/272]  eta: 0:04:14  loss: 3.6620 (3.5289)  acc1: 16.6667 (22.2222)  acc5: 50.0000 (53.0303)  time: 0.9701  data: 0.7478  max mem: 15572
Val:  [ 20/272]  eta: 0:02:37  loss: 3.5591 (3.4747)  acc1: 22.2222 (25.6614)  acc5: 50.0000 (56.6138)  time: 0.3634  data: 0.1598  max mem: 15572
Val:  [ 30/272]  eta: 0:02:04  loss: 3.4419 (3.5001)  acc1: 22.2222 (23.6559)  acc5: 55.5556 (55.0179)  time: 0.2613  data: 0.0682  max mem: 15572
Val:  [ 40/272]  eta: 0:01:55  loss: 3.4417 (3.4776)  acc1: 22.2222 (22.6287)  acc5: 55.5556 (56.0976)  time: 0.3671  data: 0.1607  max mem: 15572
Val:  [ 50/272]  eta: 0:01:46  loss: 3.3974 (3.4217)  acc1: 27.7778 (24.4009)  acc5: 66.6667 (58.3878)  time: 0.4299  data: 0.2198  max mem: 15572
Val:  [ 60/272]  eta: 0:01:36  loss: 3.0405 (3.3641)  acc1: 27.7778 (25.3188)  acc5: 72.2222 (60.6557)  time: 0.3649  data: 0.1531  max mem: 15572
Val:  [ 70/272]  eta: 0:01:29  loss: 3.0405 (3.3290)  acc1: 33.3333 (27.8560)  acc5: 72.2222 (62.1283)  time: 0.3428  data: 0.1394  max mem: 15572
Val:  [ 80/272]  eta: 0:01:23  loss: 3.1784 (3.3355)  acc1: 33.3333 (27.9150)  acc5: 61.1111 (62.4829)  time: 0.3685  data: 0.1519  max mem: 15572
Val:  [ 90/272]  eta: 0:01:15  loss: 3.5090 (3.3620)  acc1: 22.2222 (27.1673)  acc5: 61.1111 (61.5995)  time: 0.3180  data: 0.1146  max mem: 15572
Val:  [100/272]  eta: 0:01:07  loss: 3.5818 (3.3892)  acc1: 16.6667 (25.9076)  acc5: 55.5556 (61.1661)  time: 0.2171  data: 0.0460  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 3.6115 (3.4157)  acc1: 11.1111 (25.1752)  acc5: 50.0000 (60.4104)  time: 0.1730  data: 0.0004  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5627 (3.4175)  acc1: 22.2222 (26.0331)  acc5: 55.5556 (60.6061)  time: 0.1637  data: 0.0004  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.1984 (3.3930)  acc1: 38.8889 (27.1416)  acc5: 66.6667 (61.2383)  time: 0.1868  data: 0.0005  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1508 (3.3801)  acc1: 38.8889 (28.3294)  acc5: 66.6667 (61.5445)  time: 0.2233  data: 0.0106  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.1991 (3.3658)  acc1: 27.7778 (28.4400)  acc5: 61.1111 (61.9573)  time: 0.3060  data: 0.1036  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.2572 (3.3638)  acc1: 22.2222 (28.5369)  acc5: 66.6667 (62.3533)  time: 0.3917  data: 0.1836  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.2911 (3.3743)  acc1: 22.2222 (27.9402)  acc5: 61.1111 (61.6634)  time: 0.3666  data: 0.1390  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.2026 (3.3567)  acc1: 22.2222 (28.1768)  acc5: 61.1111 (62.3082)  time: 0.3096  data: 0.0944  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2026 (3.3722)  acc1: 22.2222 (27.5160)  acc5: 61.1111 (61.6347)  time: 0.2957  data: 0.0989  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.3197 (3.3664)  acc1: 22.2222 (27.5567)  acc5: 61.1111 (61.8850)  time: 0.3262  data: 0.1211  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2601 (3.3779)  acc1: 22.2222 (27.3828)  acc5: 61.1111 (61.4797)  time: 0.3286  data: 0.1235  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5716 (3.3826)  acc1: 22.2222 (27.4007)  acc5: 50.0000 (61.2368)  time: 0.3132  data: 0.1274  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.1646 (3.3796)  acc1: 22.2222 (27.7778)  acc5: 72.2222 (61.5681)  time: 0.3113  data: 0.1290  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0472 (3.3677)  acc1: 27.7778 (27.7086)  acc5: 77.7778 (62.1946)  time: 0.3104  data: 0.1243  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1963 (3.3714)  acc1: 22.2222 (27.4015)  acc5: 72.2222 (62.0629)  time: 0.3107  data: 0.1303  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0353 (3.3518)  acc1: 33.3333 (28.3312)  acc5: 72.2222 (62.8991)  time: 0.2871  data: 0.1075  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0450 (3.3525)  acc1: 44.4444 (28.3723)  acc5: 72.2222 (62.8946)  time: 0.2168  data: 0.0443  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0450 (3.3550)  acc1: 40.0000 (28.3842)  acc5: 72.2222 (62.8917)  time: 0.2017  data: 0.0379  max mem: 15572
Val: Total time: 0:01:28 (0.3239 s / it)
* Acc@1 28.384 Acc@5 62.892 loss 3.355
Accuracy of the network on the 4883 val videos: 28.4%
Max accuracy: 29.31%
Epoch: [19]  [  0/344]  eta: 0:52:47  lr: 0.000031  min_lr: 0.000000  loss: 4.0742 (4.0742)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2072  data: 8.7815  max mem: 15572
Epoch: [19]  [ 10/344]  eta: 0:07:27  lr: 0.000031  min_lr: 0.000000  loss: 4.5086 (4.5430)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3402  data: 0.8915  max mem: 15572
Epoch: [19]  [ 20/344]  eta: 0:05:02  lr: 0.000031  min_lr: 0.000000  loss: 4.6596 (4.6336)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0711  max mem: 15572
[2025-01-13 02:15:37,040] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:15:37,041] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-13 02:15:37,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=175, lr=[2.9589149494535685e-07, 2.9589149494535685e-07, 4.2270213563622414e-07, 4.2270213563622414e-07, 6.038601937660345e-07, 6.038601937660345e-07, 8.626574196657636e-07, 8.626574196657636e-07, 1.2323677423796623e-06, 1.2323677423796623e-06, 1.7605253462566607e-06, 1.7605253462566607e-06, 2.5150362089380866e-06, 2.5150362089380866e-06, 3.592908869911553e-06, 3.592908869911553e-06, 5.132726957016504e-06, 5.132726957016504e-06, 7.33246708145215e-06, 7.33246708145215e-06, 1.047495297350307e-05, 1.047495297350307e-05, 1.4964218533575817e-05, 1.4964218533575817e-05, 2.1377455047965453e-05, 2.1377455047965453e-05, 3.053922149709351e-05, 3.053922149709351e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:15:37,052] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=27.897564808739677, CurrSamplesPerSec=31.195530006043047, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 30/344]  eta: 0:04:08  lr: 0.000031  min_lr: 0.000000  loss: 4.5864 (4.6339)  loss_scale: 32768.0000 (36996.1290)  weight_decay: 0.0500 (0.0500)  time: 0.4891  data: 0.0448  max mem: 15572
Epoch: [19]  [ 40/344]  eta: 0:03:43  lr: 0.000030  min_lr: 0.000000  loss: 4.5479 (4.6095)  loss_scale: 65536.0000 (43957.0732)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0900  max mem: 15572
Epoch: [19]  [ 50/344]  eta: 0:03:28  lr: 0.000030  min_lr: 0.000000  loss: 4.5319 (4.6091)  loss_scale: 65536.0000 (48188.2353)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1339  max mem: 15572
Epoch: [19]  [ 60/344]  eta: 0:03:12  lr: 0.000030  min_lr: 0.000000  loss: 4.6644 (4.6290)  loss_scale: 65536.0000 (51032.1311)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1157  max mem: 15572
[2025-01-13 02:15:57,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27034
[2025-01-13 02:15:57,953] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:15:57,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 70/344]  eta: 0:03:03  lr: 0.000030  min_lr: 0.000000  loss: 4.7075 (4.6278)  loss_scale: 65536.0000 (48921.2394)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1345  max mem: 15572
Epoch: [19]  [ 80/344]  eta: 0:02:50  lr: 0.000030  min_lr: 0.000000  loss: 4.7075 (4.6484)  loss_scale: 32768.0000 (46927.0123)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1088  max mem: 15572
Epoch: [19]  [ 90/344]  eta: 0:02:42  lr: 0.000030  min_lr: 0.000000  loss: 4.6987 (4.6406)  loss_scale: 32768.0000 (45371.0769)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0869  max mem: 15572
Epoch: [19]  [100/344]  eta: 0:02:35  lr: 0.000030  min_lr: 0.000000  loss: 4.5327 (4.6304)  loss_scale: 32768.0000 (44123.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1543  max mem: 15572
Epoch: [19]  [110/344]  eta: 0:02:26  lr: 0.000030  min_lr: 0.000000  loss: 4.6548 (4.6414)  loss_scale: 32768.0000 (43100.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1267  max mem: 15572
Epoch: [19]  [120/344]  eta: 0:02:20  lr: 0.000030  min_lr: 0.000000  loss: 4.7565 (4.6541)  loss_scale: 32768.0000 (42246.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1351  max mem: 15572
Epoch: [19]  [130/344]  eta: 0:02:12  lr: 0.000030  min_lr: 0.000000  loss: 4.7259 (4.6574)  loss_scale: 32768.0000 (41522.8092)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1455  max mem: 15572
Epoch: [19]  [140/344]  eta: 0:02:07  lr: 0.000030  min_lr: 0.000000  loss: 4.6728 (4.6587)  loss_scale: 32768.0000 (40901.9007)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1703  max mem: 15572
Epoch: [19]  [150/344]  eta: 0:01:59  lr: 0.000030  min_lr: 0.000000  loss: 4.6728 (4.6609)  loss_scale: 32768.0000 (40363.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1376  max mem: 15572
Epoch: [19]  [160/344]  eta: 0:01:52  lr: 0.000030  min_lr: 0.000000  loss: 4.6183 (4.6602)  loss_scale: 32768.0000 (39891.4783)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0843  max mem: 15572
Epoch: [19]  [170/344]  eta: 0:01:46  lr: 0.000030  min_lr: 0.000000  loss: 4.6183 (4.6606)  loss_scale: 32768.0000 (39474.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1209  max mem: 15572
Epoch: [19]  [180/344]  eta: 0:01:40  lr: 0.000030  min_lr: 0.000000  loss: 4.7053 (4.6654)  loss_scale: 32768.0000 (39104.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1542  max mem: 15572
Epoch: [19]  [190/344]  eta: 0:01:33  lr: 0.000030  min_lr: 0.000000  loss: 4.7677 (4.6725)  loss_scale: 32768.0000 (38772.6073)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1489  max mem: 15572
[2025-01-13 02:17:11,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:17:11,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [200/344]  eta: 0:01:26  lr: 0.000030  min_lr: 0.000000  loss: 4.7535 (4.6721)  loss_scale: 32768.0000 (40104.1194)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0821  max mem: 15572
Epoch: [19]  [210/344]  eta: 0:01:20  lr: 0.000029  min_lr: 0.000000  loss: 4.6717 (4.6720)  loss_scale: 65536.0000 (41309.4218)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1215  max mem: 15572
Epoch: [19]  [220/344]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000000  loss: 4.6363 (4.6669)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1128  max mem: 15572
Epoch: [19]  [230/344]  eta: 0:01:08  lr: 0.000029  min_lr: 0.000000  loss: 4.5344 (4.6710)  loss_scale: 65536.0000 (43406.9610)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0882  max mem: 15572
Epoch: [19]  [240/344]  eta: 0:01:01  lr: 0.000029  min_lr: 0.000000  loss: 4.5800 (4.6669)  loss_scale: 65536.0000 (44325.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1067  max mem: 15572
Epoch: [19]  [250/344]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000000  loss: 4.5800 (4.6627)  loss_scale: 65536.0000 (45170.2311)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1277  max mem: 15572
Epoch: [19]  [260/344]  eta: 0:00:50  lr: 0.000029  min_lr: 0.000000  loss: 4.5899 (4.6579)  loss_scale: 65536.0000 (45950.5287)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1710  max mem: 15572
Epoch: [19]  [270/344]  eta: 0:00:43  lr: 0.000029  min_lr: 0.000000  loss: 4.5899 (4.6584)  loss_scale: 65536.0000 (46673.2399)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.1041  max mem: 15572
Epoch: [19]  [280/344]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000000  loss: 4.7788 (4.6593)  loss_scale: 65536.0000 (47344.5125)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1222  max mem: 15572
Epoch: [19]  [290/344]  eta: 0:00:31  lr: 0.000029  min_lr: 0.000000  loss: 4.7824 (4.6645)  loss_scale: 65536.0000 (47969.6495)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1180  max mem: 15572
Epoch: [19]  [300/344]  eta: 0:00:26  lr: 0.000029  min_lr: 0.000000  loss: 4.7417 (4.6657)  loss_scale: 65536.0000 (48553.2492)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1298  max mem: 15572
Epoch: [19]  [310/344]  eta: 0:00:20  lr: 0.000029  min_lr: 0.000000  loss: 4.6942 (4.6645)  loss_scale: 65536.0000 (49099.3183)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1554  max mem: 15572
[2025-01-13 02:18:22,529] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:18:22,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [320/344]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000000  loss: 4.7541 (4.6673)  loss_scale: 65536.0000 (50019.6885)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0381  max mem: 15572
[2025-01-13 02:18:23,762] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27294
[2025-01-13 02:18:23,762] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:18:23,762] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [330/344]  eta: 0:00:08  lr: 0.000029  min_lr: 0.000000  loss: 4.6790 (4.6665)  loss_scale: 65536.0000 (50686.4532)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0591  max mem: 15572
Epoch: [19]  [340/344]  eta: 0:00:02  lr: 0.000029  min_lr: 0.000000  loss: 4.6696 (4.6654)  loss_scale: 65536.0000 (51121.9238)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0591  max mem: 15572
Epoch: [19]  [343/344]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 4.6696 (4.6655)  loss_scale: 65536.0000 (51247.6279)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0590  max mem: 15572
Epoch: [19] Total time: 0:03:20 (0.5817 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 4.6696 (4.6655)  loss_scale: 65536.0000 (51247.6279)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 435
Indices to remove: tensor([  241,   319,   369,   375,   382,   480,   486,   521,   568,   577,
          607,   620,   705,   742,   746,   755,   761,   855,  1513,  1552,
         1631,  1647,  1649,  1681,  1698,  1740,  1815,  1851,  1977,  2026,
         2062,  2100,  2105,  2181,  2273,  2365,  2472,  2518,  2544,  2723,
         2835,  2842,  2863,  2984,  3125,  3237,  3409,  3637,  3668,  3903,
         3912,  3919,  4221,  4251,  4334,  4361,  4397,  4402,  4604,  4663,
         4687,  4843,  4848,  4897,  4907,  5061,  5093,  5303,  5319,  5475,
         5771,  5794,  5812,  5841,  5889,  6049,  6126,  6157,  6843,  6897,
         6903,  6922,  6987,  7028,  7040,  7124,  7154,  7177,  7178,  7182,
         7280,  7303,  7423,  7497,  8001,  8228,  8431,  8443,  8444,  8449,
         8491,  8529,  8561,  8983,  9134, 10001, 10016, 10078, 10096, 10129,
        10173, 10244, 10282, 10284, 10296, 10306, 10421, 10443, 10538, 10543,
        10609, 10649, 10708, 10874, 10914, 10933, 10983, 10994, 11007, 11008,
        11023, 11058, 11091, 11405, 11462, 11506, 11644, 11654, 11670, 11690,
        11748, 11771, 11774, 11812, 12055, 12110, 12140, 12173, 12250, 12354,
        12371, 12394, 12408, 12433, 12434, 12435, 12438, 12452, 12453, 12477,
        12486, 12491, 12543, 12558, 12589, 12612, 12639, 12657, 12690, 12709,
        12711, 12783, 12814, 12977, 13063, 13172, 13269, 13309, 13364, 13378,
        13403, 13425, 13529, 13539, 13625, 13626, 13669, 13788, 13811, 13876,
        14037, 14183, 14462, 14484, 14523, 14540, 14584, 14666, 14794, 14805,
        14828, 14877, 14893, 15212, 15267, 15330, 15347, 15372, 15487, 15555,
        15710, 15785, 15915, 16083, 16110, 16247, 16261, 16358, 16369, 16437,
        16501, 16647, 16701, 17501, 17518, 17775, 17913, 17989, 17994, 17998,
        18010, 18063, 18097, 18115, 18197, 18321, 18419, 18445, 18658, 18725,
        18798, 18800, 18806, 18812, 18886, 18935, 19003, 19100, 19122, 19395,
        19406, 19421, 19445, 19682, 19788, 19874, 19876, 19914, 20023, 20054,
        20154, 20156, 20178, 20186, 20214, 20358, 20395, 20416, 20449, 20527,
        20641, 20695, 20745, 20815, 20830, 20833, 20990, 21031, 21038, 21053,
        21350, 21488, 21646, 21832, 21892, 21901, 22020, 22063, 22232, 22331,
        22361, 22398, 22536, 22610, 22693, 22718, 22727, 22807, 22831, 22890,
        22935, 23017, 23018, 23101, 23149, 23206, 23244, 23307, 23420, 23429,
        23457, 23479, 23566, 23717, 23746, 23773, 23787, 23797, 23910, 24340,
        24412, 24446, 24634, 24813, 24822, 24886, 24892, 24912, 25047, 25094,
        25110, 25276, 25327, 25367, 25527, 25535, 25553, 25672, 25738, 25742,
        25895, 25986, 26147, 26207, 26210, 26320, 26350, 26499, 26537, 26830,
        27014, 27147, 27163, 27256, 27314, 27349, 27354, 27367, 27496, 27502,
        27549, 27568, 27594, 27602, 27689, 27695, 27714, 27727, 27769, 27822,
        27920, 28033, 28245, 28362, 28439, 28446, 28486, 28685, 28742, 28905,
        29227, 29378, 29495, 29946, 29960, 29965, 29979, 30004, 30017, 30028,
        30320, 30403, 30475, 30554, 30564, 30610, 30727, 30940, 30979, 30994,
        31055, 31091, 31136, 31205, 31292, 31334, 31349, 31372, 31399, 31431,
        31432, 31478, 31479, 31486, 31496, 31579, 31734, 31785, 31902, 31975,
        32053, 32441, 32898, 32927, 32968, 33324, 33325, 33398, 33483, 33502,
        33566, 33666, 33682, 33683, 33699], device='cuda:0')
length of data loader train is: 308
num_training_steps_per_epoch is: 308
Change step level LR scheduler!
Set warmup steps = 1540
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 02:18:34,161] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-13 02:18:34,166] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-13 02:18:34,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-13 02:18:34,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-13 02:18:34,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/272]  eta: 0:28:35  loss: 2.3347 (2.3347)  acc1: 72.2222 (72.2222)  acc5: 100.0000 (100.0000)  time: 6.3084  data: 6.1195  max mem: 15572
Val:  [ 10/272]  eta: 0:03:37  loss: 3.4653 (3.4665)  acc1: 27.7778 (26.2626)  acc5: 55.5556 (60.1010)  time: 0.8302  data: 0.6267  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 3.4661 (3.4484)  acc1: 22.2222 (28.0423)  acc5: 50.0000 (59.7884)  time: 0.2396  data: 0.0391  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 3.4783 (3.4862)  acc1: 16.6667 (23.1183)  acc5: 55.5556 (58.4229)  time: 0.2426  data: 0.0533  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.5725 (3.4877)  acc1: 16.6667 (22.7642)  acc5: 55.5556 (58.2656)  time: 0.2707  data: 0.0866  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 3.4617 (3.4364)  acc1: 22.2222 (24.5098)  acc5: 61.1111 (60.1307)  time: 0.2955  data: 0.1072  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 3.1463 (3.3880)  acc1: 27.7778 (23.9526)  acc5: 72.2222 (61.2933)  time: 0.3128  data: 0.1233  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 3.1463 (3.3565)  acc1: 27.7778 (26.7606)  acc5: 72.2222 (62.7543)  time: 0.2962  data: 0.1057  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.2378 (3.3526)  acc1: 33.3333 (27.1605)  acc5: 66.6667 (63.4431)  time: 0.3391  data: 0.1513  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.5086 (3.3889)  acc1: 22.2222 (26.0684)  acc5: 61.1111 (61.9658)  time: 0.3515  data: 0.1423  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.6007 (3.4065)  acc1: 16.6667 (25.1375)  acc5: 55.5556 (61.6612)  time: 0.3332  data: 0.1171  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.5240 (3.4303)  acc1: 16.6667 (24.1742)  acc5: 55.5556 (60.6607)  time: 0.3087  data: 0.1156  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.4972 (3.4338)  acc1: 22.2222 (25.0689)  acc5: 55.5556 (60.8356)  time: 0.3030  data: 0.0998  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2228 (3.4091)  acc1: 33.3333 (26.3359)  acc5: 66.6667 (61.7048)  time: 0.3233  data: 0.1202  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.0964 (3.3942)  acc1: 44.4444 (27.5020)  acc5: 72.2222 (62.1749)  time: 0.3317  data: 0.1449  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2184 (3.3842)  acc1: 22.2222 (27.2627)  acc5: 66.6667 (62.3252)  time: 0.3074  data: 0.1197  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.3445 (3.3824)  acc1: 22.2222 (27.2257)  acc5: 72.2222 (62.8709)  time: 0.2829  data: 0.0899  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.4255 (3.3928)  acc1: 22.2222 (26.4782)  acc5: 66.6667 (62.3132)  time: 0.3592  data: 0.1661  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.2667 (3.3769)  acc1: 16.6667 (26.8570)  acc5: 61.1111 (62.9527)  time: 0.3616  data: 0.1652  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.1459 (3.3851)  acc1: 27.7778 (26.3234)  acc5: 61.1111 (62.3618)  time: 0.3084  data: 0.1059  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.1895 (3.3754)  acc1: 27.7778 (26.6722)  acc5: 61.1111 (62.7695)  time: 0.3040  data: 0.1021  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1895 (3.3813)  acc1: 27.7778 (26.5666)  acc5: 72.2222 (62.8489)  time: 0.2922  data: 0.0964  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5230 (3.3856)  acc1: 22.2222 (26.3952)  acc5: 66.6667 (62.7200)  time: 0.3180  data: 0.1297  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.3338 (3.3810)  acc1: 22.2222 (26.6715)  acc5: 66.6667 (63.2275)  time: 0.3463  data: 0.1540  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1426 (3.3700)  acc1: 22.2222 (26.4177)  acc5: 77.7778 (63.6929)  time: 0.3134  data: 0.1120  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1836 (3.3689)  acc1: 16.6667 (26.2948)  acc5: 72.2222 (63.7672)  time: 0.2625  data: 0.0603  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.9626 (3.3500)  acc1: 38.8889 (27.4372)  acc5: 72.2222 (64.5594)  time: 0.2703  data: 0.0816  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0278 (3.3461)  acc1: 50.0000 (27.7368)  acc5: 72.2222 (64.6576)  time: 0.2300  data: 0.0573  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0278 (3.3496)  acc1: 44.4444 (27.7084)  acc5: 72.2222 (64.6119)  time: 0.2239  data: 0.0572  max mem: 15572
Val: Total time: 0:01:28 (0.3236 s / it)
* Acc@1 27.708 Acc@5 64.612 loss 3.350
Accuracy of the network on the 4883 val videos: 27.7%
Max accuracy: 29.31%
Epoch: [20]  [  0/308]  eta: 0:41:26  lr: 0.000029  min_lr: 0.000000  loss: 4.9720 (4.9720)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0723  data: 7.6128  max mem: 15572
Epoch: [20]  [ 10/308]  eta: 0:06:10  lr: 0.000029  min_lr: 0.000000  loss: 4.7515 (4.7028)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2437  data: 0.7829  max mem: 15572
Epoch: [20]  [ 20/308]  eta: 0:04:21  lr: 0.000029  min_lr: 0.000000  loss: 4.6711 (4.6733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0990  max mem: 15572
Epoch: [20]  [ 30/308]  eta: 0:03:46  lr: 0.000028  min_lr: 0.000000  loss: 4.5510 (4.6504)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1411  max mem: 15572
Epoch: [20]  [ 40/308]  eta: 0:03:25  lr: 0.000028  min_lr: 0.000000  loss: 4.5982 (4.6190)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1942  max mem: 15572
Epoch: [20]  [ 50/308]  eta: 0:03:10  lr: 0.000028  min_lr: 0.000000  loss: 4.6404 (4.6300)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1958  max mem: 15572
Epoch: [20]  [ 60/308]  eta: 0:02:57  lr: 0.000028  min_lr: 0.000000  loss: 4.6776 (4.6365)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1578  max mem: 15572
Epoch: [20]  [ 70/308]  eta: 0:02:43  lr: 0.000028  min_lr: 0.000000  loss: 4.6951 (4.6408)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0999  max mem: 15572
Epoch: [20]  [ 80/308]  eta: 0:02:32  lr: 0.000028  min_lr: 0.000000  loss: 4.7395 (4.6607)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0895  max mem: 15572
Epoch: [20]  [ 90/308]  eta: 0:02:24  lr: 0.000028  min_lr: 0.000000  loss: 4.7248 (4.6601)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1438  max mem: 15572
Epoch: [20]  [100/308]  eta: 0:02:16  lr: 0.000028  min_lr: 0.000000  loss: 4.6814 (4.6611)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1599  max mem: 15572
[2025-01-13 02:21:12,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:21:12,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:21:13,132] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27424
[2025-01-13 02:21:13,133] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:21:13,133] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [110/308]  eta: 0:02:09  lr: 0.000028  min_lr: 0.000000  loss: 4.7687 (4.6639)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1718  max mem: 15572
Epoch: [20]  [120/308]  eta: 0:02:01  lr: 0.000028  min_lr: 0.000000  loss: 4.7917 (4.6668)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1567  max mem: 15572
Epoch: [20]  [130/308]  eta: 0:01:54  lr: 0.000028  min_lr: 0.000000  loss: 4.7506 (4.6661)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1210  max mem: 15572
Epoch: [20]  [140/308]  eta: 0:01:47  lr: 0.000028  min_lr: 0.000000  loss: 4.6934 (4.6740)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1377  max mem: 15572
[2025-01-13 02:21:35,321] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27459
[2025-01-13 02:21:35,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:21:35,322] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [150/308]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000000  loss: 4.5980 (4.6644)  loss_scale: 65536.0000 (64233.9603)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1172  max mem: 15572
Epoch: [20]  [160/308]  eta: 0:01:33  lr: 0.000028  min_lr: 0.000000  loss: 4.5980 (4.6659)  loss_scale: 32768.0000 (62279.5528)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1362  max mem: 15572
Epoch: [20]  [170/308]  eta: 0:01:26  lr: 0.000028  min_lr: 0.000000  loss: 4.7737 (4.6756)  loss_scale: 32768.0000 (60553.7310)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1385  max mem: 15572
Epoch: [20]  [180/308]  eta: 0:01:20  lr: 0.000027  min_lr: 0.000000  loss: 4.7322 (4.6752)  loss_scale: 32768.0000 (59018.6077)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1202  max mem: 15572
Epoch: [20]  [190/308]  eta: 0:01:13  lr: 0.000027  min_lr: 0.000000  loss: 4.6955 (4.6725)  loss_scale: 32768.0000 (57644.2304)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1340  max mem: 15572
Epoch: [20]  [200/308]  eta: 0:01:07  lr: 0.000027  min_lr: 0.000000  loss: 4.6827 (4.6715)  loss_scale: 32768.0000 (56406.6070)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1443  max mem: 15572
Epoch: [20]  [210/308]  eta: 0:01:00  lr: 0.000027  min_lr: 0.000000  loss: 4.6802 (4.6731)  loss_scale: 32768.0000 (55286.2938)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1379  max mem: 15572
Epoch: [20]  [220/308]  eta: 0:00:54  lr: 0.000027  min_lr: 0.000000  loss: 4.6802 (4.6750)  loss_scale: 32768.0000 (54267.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0747  max mem: 15572
Epoch: [20]  [230/308]  eta: 0:00:47  lr: 0.000027  min_lr: 0.000000  loss: 4.7798 (4.6761)  loss_scale: 32768.0000 (53336.6580)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1202  max mem: 15572
Epoch: [20]  [240/308]  eta: 0:00:41  lr: 0.000027  min_lr: 0.000000  loss: 4.8042 (4.6832)  loss_scale: 32768.0000 (52483.1867)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.1013  max mem: 15572
Epoch: [20]  [250/308]  eta: 0:00:35  lr: 0.000027  min_lr: 0.000000  loss: 4.7941 (4.6803)  loss_scale: 32768.0000 (51697.7211)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0785  max mem: 15572
Epoch: [20]  [260/308]  eta: 0:00:28  lr: 0.000027  min_lr: 0.000000  loss: 4.6719 (4.6802)  loss_scale: 32768.0000 (50972.4444)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1060  max mem: 15572
Epoch: [20]  [270/308]  eta: 0:00:22  lr: 0.000027  min_lr: 0.000000  loss: 4.6600 (4.6817)  loss_scale: 32768.0000 (50300.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0874  max mem: 15572
[2025-01-13 02:22:48,378] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:22:48,378] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [280/308]  eta: 0:00:16  lr: 0.000027  min_lr: 0.000000  loss: 4.6243 (4.6810)  loss_scale: 32768.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1303  max mem: 15572
Epoch: [20]  [290/308]  eta: 0:00:10  lr: 0.000027  min_lr: 0.000000  loss: 4.6448 (4.6839)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1092  max mem: 15572
Epoch: [20]  [300/308]  eta: 0:00:04  lr: 0.000027  min_lr: 0.000000  loss: 4.6172 (4.6791)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1255  max mem: 15572
Epoch: [20]  [307/308]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000000  loss: 4.6148 (4.6786)  loss_scale: 65536.0000 (52024.5195)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.1034  max mem: 15572
Epoch: [20] Total time: 0:03:03 (0.5963 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000000  loss: 4.6148 (4.6786)  loss_scale: 65536.0000 (52024.5195)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 342
Indices to remove: tensor([  258,   317,   459,   494,   526,   592,   669,   737,   743,   822,
          834,  1286,  1491,  1587,  1598,  1653,  1682,  1767,  2243,  2309,
         2334,  2491,  2493,  2500,  2501,  2504,  2507,  2510,  2515,  2611,
         2663,  2680,  2776,  2880,  2928,  2954,  3079,  3427,  3456,  3629,
         3657,  3890,  3932,  3964,  4377,  4419,  4478,  4634,  4683,  4702,
         4773,  4775,  4808,  4816,  4827,  4840,  4858,  4889,  4892,  4900,
         4997,  5052,  6147,  6255,  6349,  6516,  6545,  6880,  6891,  6912,
         6974,  7038,  7141,  7185,  7188,  7212,  7232,  7532,  7548,  7858,
         7869,  7904,  7959,  7960,  8408,  8450,  8452,  8488,  8492,  9271,
         9796,  9979, 10162, 10167, 10202, 10210, 10231, 10362, 10384, 10455,
        10491, 10495, 10549, 10641, 10734, 10841, 10931, 10957, 10968, 11020,
        11039, 11178, 11497, 11882, 11927, 11959, 12091, 12137, 12146, 12248,
        12287, 12459, 12577, 12596, 12655, 12665, 12867, 12885, 12916, 13060,
        13061, 13124, 13169, 13194, 13244, 13357, 13371, 13374, 13409, 13416,
        13496, 13501, 13521, 13523, 13587, 13864, 13878, 13910, 13998, 14043,
        14103, 14193, 14324, 14490, 14495, 14512, 14568, 14667, 14687, 14717,
        14725, 14798, 14811, 14929, 14981, 15062, 15152, 15227, 15396, 15472,
        15550, 15608, 15696, 15730, 15773, 15923, 16199, 16362, 16387, 16401,
        16415, 16419, 16453, 16459, 16490, 16588, 16616, 16674, 16686, 17445,
        17841, 17900, 17944, 17980, 18002, 18008, 18191, 18210, 18317, 18351,
        18442, 18557, 19359, 19926, 19972, 20018, 20108, 20236, 20281, 20282,
        20332, 20374, 20516, 20691, 20765, 20860, 20972, 21027, 21036, 21064,
        21791, 21827, 21872, 21874, 21926, 22456, 22477, 22628, 22645, 22819,
        22843, 22884, 22906, 22950, 23004, 23078, 23269, 23272, 23306, 23421,
        23438, 23489, 23524, 23553, 23640, 23810, 23891, 24004, 24107, 24136,
        24539, 24641, 24668, 24806, 24808, 24896, 24962, 25085, 25135, 25173,
        25239, 25264, 25278, 25290, 25433, 25513, 25515, 25584, 25657, 25665,
        25675, 25688, 25694, 25979, 26013, 26054, 26064, 26098, 26390, 26542,
        26797, 26878, 27076, 27113, 27346, 27426, 27625, 27643, 27653, 28126,
        28385, 28483, 28677, 28776, 28916, 29086, 29348, 29357, 29370, 29497,
        29569, 29578, 29651, 29739, 29879, 29943, 29988, 30477, 30501, 30618,
        30857, 30902, 30904, 30908, 30926, 30969, 31007, 31316, 31345, 31371,
        31405, 31412, 31414, 31444, 31594, 31770, 31778, 31796, 32098, 32769,
        32789, 32792, 32811, 33030, 33046, 33197, 33269, 33271, 33524, 33529,
        33533, 33598], device='cuda:0')
length of data loader train is: 280
num_training_steps_per_epoch is: 280
Change step level LR scheduler!
Set warmup steps = 1400
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:02  loss: 2.3877 (2.3877)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 4.1992  data: 4.0013  max mem: 15572
Val:  [ 10/272]  eta: 0:03:01  loss: 3.4692 (3.4314)  acc1: 22.2222 (24.7475)  acc5: 55.5556 (59.5960)  time: 0.6946  data: 0.5025  max mem: 15572
Val:  [ 20/272]  eta: 0:02:08  loss: 3.4467 (3.3796)  acc1: 22.2222 (27.5132)  acc5: 55.5556 (61.3757)  time: 0.3254  data: 0.1372  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 3.4467 (3.4195)  acc1: 16.6667 (22.5806)  acc5: 61.1111 (60.0358)  time: 0.2904  data: 0.1043  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 3.4061 (3.4079)  acc1: 16.6667 (22.6287)  acc5: 61.1111 (61.3821)  time: 0.2786  data: 0.0912  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.3606 (3.3694)  acc1: 22.2222 (23.6383)  acc5: 61.1111 (63.0719)  time: 0.3041  data: 0.1178  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 3.0706 (3.3361)  acc1: 16.6667 (23.4973)  acc5: 72.2222 (63.4791)  time: 0.3199  data: 0.1357  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 3.0706 (3.3245)  acc1: 27.7778 (25.7433)  acc5: 72.2222 (64.5540)  time: 0.3351  data: 0.1429  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.2847 (3.3236)  acc1: 27.7778 (25.7888)  acc5: 66.6667 (64.6091)  time: 0.3123  data: 0.1250  max mem: 15572
Val:  [ 90/272]  eta: 0:01:02  loss: 3.5423 (3.3588)  acc1: 22.2222 (25.0305)  acc5: 55.5556 (62.8816)  time: 0.2596  data: 0.0813  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 3.6115 (3.3879)  acc1: 11.1111 (24.0374)  acc5: 55.5556 (62.2662)  time: 0.2835  data: 0.1003  max mem: 15572
Val:  [110/272]  eta: 0:00:54  loss: 3.6820 (3.4150)  acc1: 11.1111 (23.0731)  acc5: 55.5556 (61.2613)  time: 0.3035  data: 0.1186  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.6419 (3.4217)  acc1: 16.6667 (24.0129)  acc5: 55.5556 (61.2489)  time: 0.3401  data: 0.1603  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 3.2222 (3.3906)  acc1: 33.3333 (25.7422)  acc5: 72.2222 (62.3410)  time: 0.3390  data: 0.1605  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 2.9840 (3.3784)  acc1: 38.8889 (26.6745)  acc5: 77.7778 (62.8054)  time: 0.3124  data: 0.1187  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.0978 (3.3656)  acc1: 22.2222 (26.7844)  acc5: 72.2222 (63.0611)  time: 0.2807  data: 0.0891  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2426 (3.3598)  acc1: 22.2222 (26.7771)  acc5: 72.2222 (63.5611)  time: 0.2783  data: 0.0871  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3549 (3.3685)  acc1: 22.2222 (26.3158)  acc5: 66.6667 (63.1579)  time: 0.3400  data: 0.1320  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3066 (3.3558)  acc1: 27.7778 (26.6728)  acc5: 66.6667 (63.7508)  time: 0.3072  data: 0.1058  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.1603 (3.3724)  acc1: 22.2222 (26.2362)  acc5: 61.1111 (62.7109)  time: 0.3098  data: 0.1257  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.1811 (3.3619)  acc1: 22.2222 (26.4787)  acc5: 61.1111 (62.9077)  time: 0.3245  data: 0.1374  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2472 (3.3684)  acc1: 22.2222 (26.3296)  acc5: 72.2222 (62.8225)  time: 0.3051  data: 0.1029  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5152 (3.3771)  acc1: 22.2222 (26.0432)  acc5: 61.1111 (62.3429)  time: 0.3443  data: 0.1512  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.3432 (3.3671)  acc1: 33.3333 (26.8879)  acc5: 66.6667 (62.9630)  time: 0.3412  data: 0.1521  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.9367 (3.3522)  acc1: 38.8889 (27.2476)  acc5: 83.3333 (63.6929)  time: 0.3075  data: 0.1090  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0369 (3.3533)  acc1: 22.2222 (27.0031)  acc5: 77.7778 (63.7229)  time: 0.3068  data: 0.1030  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1393 (3.3385)  acc1: 33.3333 (27.9693)  acc5: 77.7778 (64.4530)  time: 0.3064  data: 0.1103  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.0655 (3.3324)  acc1: 38.8889 (28.2698)  acc5: 77.7778 (64.5961)  time: 0.2258  data: 0.0627  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.0655 (3.3348)  acc1: 38.8889 (28.2408)  acc5: 77.7778 (64.5914)  time: 0.2196  data: 0.0627  max mem: 15572
Val: Total time: 0:01:26 (0.3195 s / it)
* Acc@1 28.241 Acc@5 64.591 loss 3.335
Accuracy of the network on the 4883 val videos: 28.2%
Max accuracy: 29.31%
Epoch: [21]  [  0/280]  eta: 0:42:28  lr: 0.000027  min_lr: 0.000000  loss: 5.0561 (5.0561)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1028  data: 8.6581  max mem: 15572
Epoch: [21]  [ 10/280]  eta: 0:05:40  lr: 0.000027  min_lr: 0.000000  loss: 4.7030 (4.7390)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2615  data: 0.8008  max mem: 15572
Epoch: [21]  [ 20/280]  eta: 0:04:14  lr: 0.000026  min_lr: 0.000000  loss: 4.6416 (4.6923)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1278  max mem: 15572
Epoch: [21]  [ 30/280]  eta: 0:03:31  lr: 0.000026  min_lr: 0.000000  loss: 4.6264 (4.6677)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1766  max mem: 15572
Epoch: [21]  [ 40/280]  eta: 0:03:04  lr: 0.000026  min_lr: 0.000000  loss: 4.6725 (4.6846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0970  max mem: 15572
Epoch: [21]  [ 50/280]  eta: 0:02:47  lr: 0.000026  min_lr: 0.000000  loss: 4.7438 (4.6701)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1017  max mem: 15572
Epoch: [21]  [ 60/280]  eta: 0:02:32  lr: 0.000026  min_lr: 0.000000  loss: 4.5993 (4.6674)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.1047  max mem: 15572
Epoch: [21]  [ 70/280]  eta: 0:02:23  lr: 0.000026  min_lr: 0.000000  loss: 4.6702 (4.6728)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1357  max mem: 15572
Epoch: [21]  [ 80/280]  eta: 0:02:13  lr: 0.000026  min_lr: 0.000000  loss: 4.7044 (4.6820)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1647  max mem: 15572
Epoch: [21]  [ 90/280]  eta: 0:02:04  lr: 0.000026  min_lr: 0.000000  loss: 4.6542 (4.6682)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1203  max mem: 15572
[2025-01-13 02:25:33,815] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:25:33,815] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:25:34,606] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27718
[2025-01-13 02:25:34,606] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:25:34,607] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [100/280]  eta: 0:01:56  lr: 0.000026  min_lr: 0.000000  loss: 4.6039 (4.6676)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1254  max mem: 15572
Epoch: [21]  [110/280]  eta: 0:01:48  lr: 0.000026  min_lr: 0.000000  loss: 4.7453 (4.6762)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1130  max mem: 15572
Epoch: [21]  [120/280]  eta: 0:01:41  lr: 0.000026  min_lr: 0.000000  loss: 4.6879 (4.6766)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1050  max mem: 15572
Epoch: [21]  [130/280]  eta: 0:01:34  lr: 0.000026  min_lr: 0.000000  loss: 4.6686 (4.6779)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1320  max mem: 15572
Epoch: [21]  [140/280]  eta: 0:01:28  lr: 0.000026  min_lr: 0.000000  loss: 4.6423 (4.6756)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1326  max mem: 15572
Epoch: [21]  [150/280]  eta: 0:01:20  lr: 0.000025  min_lr: 0.000000  loss: 4.6116 (4.6750)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0708  max mem: 15572
Epoch: [21]  [160/280]  eta: 0:01:13  lr: 0.000025  min_lr: 0.000000  loss: 4.6207 (4.6721)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0469  max mem: 15572
Epoch: [21]  [170/280]  eta: 0:01:07  lr: 0.000025  min_lr: 0.000000  loss: 4.6378 (4.6679)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1377  max mem: 15572
Epoch: [21]  [180/280]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000000  loss: 4.6627 (4.6750)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1765  max mem: 15572
Epoch: [21]  [190/280]  eta: 0:00:54  lr: 0.000025  min_lr: 0.000000  loss: 4.7405 (4.6774)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1530  max mem: 15572
Epoch: [21]  [200/280]  eta: 0:00:49  lr: 0.000025  min_lr: 0.000000  loss: 4.6782 (4.6775)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1909  max mem: 15572
Epoch: [21]  [210/280]  eta: 0:00:42  lr: 0.000025  min_lr: 0.000000  loss: 4.7189 (4.6795)  loss_scale: 65536.0000 (66157.1943)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1830  max mem: 15572
Epoch: [21]  [220/280]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000000  loss: 4.7181 (4.6794)  loss_scale: 65536.0000 (66129.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1260  max mem: 15572
[2025-01-13 02:26:49,295] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:26:49,296] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:26:50,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27849
[2025-01-13 02:26:50,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:26:50,134] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [230/280]  eta: 0:00:30  lr: 0.000025  min_lr: 0.000000  loss: 4.6472 (4.6829)  loss_scale: 65536.0000 (66670.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1221  max mem: 15572
Epoch: [21]  [240/280]  eta: 0:00:24  lr: 0.000025  min_lr: 0.000000  loss: 4.6472 (4.6825)  loss_scale: 65536.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1452  max mem: 15572
Epoch: [21]  [250/280]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000000  loss: 4.7242 (4.6843)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1960  max mem: 15572
Epoch: [21]  [260/280]  eta: 0:00:12  lr: 0.000025  min_lr: 0.000000  loss: 4.7153 (4.6834)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1329  max mem: 15572
Epoch: [21]  [270/280]  eta: 0:00:05  lr: 0.000025  min_lr: 0.000000  loss: 4.6182 (4.6802)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0588  max mem: 15572
Epoch: [21]  [279/280]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 4.6013 (4.6768)  loss_scale: 65536.0000 (66472.2286)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0588  max mem: 15572
Epoch: [21] Total time: 0:02:46 (0.5939 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 4.6013 (4.6768)  loss_scale: 65536.0000 (66472.2286)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 307
Indices to remove: tensor([   81,   244,   302,   354,   421,   424,   461,   483,   572,   608,
          905,  1317,  1550,  1568,  1604,  1921,  1933,  2075,  2137,  2151,
         2161,  2172,  2176,  2271,  2302,  2368,  2482,  2494,  2587,  2640,
         2734,  2955,  3296,  3322,  3400,  3445,  3491,  3498,  3541,  3665,
         3805,  3852,  3855,  4336,  4652,  4772,  4785,  4926,  4943,  4964,
         4991,  5035,  5196,  5409,  5434,  5639,  5738,  5795,  5885,  6224,
         6684,  7011,  7054,  7088,  7136,  7142,  7190,  7240,  7397,  7974,
         8046,  8132,  8204,  8418,  8421,  8607,  9728,  9967,  9990,  9994,
        10021, 10183, 10214, 10253, 10318, 10338, 10393, 10428, 10436, 10504,
        10513, 10596, 10672, 10714, 10849, 10964, 10974, 10988, 10997, 11046,
        11048, 11083, 11180, 11205, 11586, 11731, 11798, 11869, 11965, 11989,
        12086, 12213, 12264, 12290, 12335, 12426, 12584, 12680, 12700, 12958,
        12973, 13030, 13076, 13109, 13261, 13429, 13440, 13557, 13589, 13653,
        13815, 13893, 13967, 14209, 14231, 14270, 14418, 14461, 14482, 14501,
        14622, 14689, 14695, 14772, 14874, 14878, 14937, 15082, 15117, 15163,
        15255, 15332, 15410, 15458, 15575, 16356, 16357, 16373, 16382, 16392,
        16411, 16418, 16421, 16493, 17426, 17961, 17981, 18081, 18091, 18131,
        18195, 18204, 18222, 18565, 18569, 18574, 18580, 18644, 19085, 19090,
        19301, 19352, 19379, 19386, 19422, 19529, 19728, 19742, 19821, 19950,
        20036, 20037, 20158, 20293, 20364, 20961, 20994, 21110, 21293, 21641,
        21784, 21813, 21833, 22041, 22189, 22199, 22360, 22453, 22464, 22480,
        22599, 22618, 22676, 22886, 22893, 23155, 23158, 23261, 23285, 23446,
        23513, 23646, 23804, 24235, 24320, 24504, 24505, 24548, 24552, 24554,
        24781, 24790, 24823, 24887, 25013, 25100, 25222, 25298, 25323, 25442,
        25467, 25597, 25687, 25843, 25893, 26055, 26177, 26225, 26243, 26272,
        26277, 26304, 26310, 26325, 26344, 26357, 26456, 26536, 26601, 26713,
        26717, 26747, 27092, 27445, 27501, 27542, 27669, 27677, 28029, 28036,
        28062, 28200, 28232, 28762, 28826, 29025, 30008, 30057, 30058, 30099,
        30128, 30234, 30351, 30460, 30500, 30534, 30864, 30877, 30886, 30891,
        30924, 31229, 31389, 31537, 32485, 32759, 32764, 32772, 32801, 32863,
        33044, 33235, 33244, 33274, 33344, 33468, 33621], device='cuda:0')
length of data loader train is: 254
num_training_steps_per_epoch is: 254
Change step level LR scheduler!
Set warmup steps = 1270
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:16:50  loss: 2.4211 (2.4211)  acc1: 61.1111 (61.1111)  acc5: 100.0000 (100.0000)  time: 3.7133  data: 3.4543  max mem: 15572
Val:  [ 10/272]  eta: 0:03:01  loss: 3.4678 (3.4215)  acc1: 27.7778 (27.2727)  acc5: 61.1111 (61.6162)  time: 0.6909  data: 0.4994  max mem: 15572
Val:  [ 20/272]  eta: 0:02:07  loss: 3.4678 (3.4078)  acc1: 27.7778 (28.0423)  acc5: 55.5556 (61.6402)  time: 0.3460  data: 0.1574  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.4646 (3.4019)  acc1: 22.2222 (26.3441)  acc5: 55.5556 (61.4695)  time: 0.2936  data: 0.1035  max mem: 15572
Val:  [ 40/272]  eta: 0:01:30  loss: 3.2770 (3.3726)  acc1: 16.6667 (25.4743)  acc5: 66.6667 (63.0081)  time: 0.2713  data: 0.0789  max mem: 15572
Val:  [ 50/272]  eta: 0:01:23  loss: 3.2753 (3.3411)  acc1: 27.7778 (26.6885)  acc5: 66.6667 (64.8148)  time: 0.2854  data: 0.0978  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 3.1021 (3.3148)  acc1: 27.7778 (26.0474)  acc5: 77.7778 (65.1184)  time: 0.3327  data: 0.1278  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 3.1021 (3.3177)  acc1: 22.2222 (27.6213)  acc5: 72.2222 (65.1800)  time: 0.3395  data: 0.1328  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.3373 (3.3217)  acc1: 27.7778 (27.5034)  acc5: 66.6667 (65.2949)  time: 0.3347  data: 0.1493  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.5443 (3.3573)  acc1: 22.2222 (26.1294)  acc5: 55.5556 (63.6142)  time: 0.3201  data: 0.1383  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.5444 (3.3779)  acc1: 11.1111 (25.0825)  acc5: 61.1111 (62.9813)  time: 0.3010  data: 0.1013  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.5336 (3.3973)  acc1: 16.6667 (24.0741)  acc5: 61.1111 (62.2122)  time: 0.3251  data: 0.1092  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5010 (3.3977)  acc1: 16.6667 (24.9770)  acc5: 61.1111 (62.3508)  time: 0.3272  data: 0.1196  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2287 (3.3735)  acc1: 38.8889 (26.7176)  acc5: 66.6667 (63.1891)  time: 0.3185  data: 0.1134  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1824 (3.3693)  acc1: 44.4444 (27.5020)  acc5: 66.6667 (63.3176)  time: 0.2921  data: 0.0913  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2398 (3.3651)  acc1: 33.3333 (27.5202)  acc5: 61.1111 (63.3554)  time: 0.3083  data: 0.1117  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.3328 (3.3653)  acc1: 22.2222 (27.3982)  acc5: 61.1111 (63.5266)  time: 0.3521  data: 0.1518  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.4325 (3.3802)  acc1: 22.2222 (26.8356)  acc5: 61.1111 (62.6706)  time: 0.3349  data: 0.1375  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3401 (3.3670)  acc1: 16.6667 (26.9490)  acc5: 61.1111 (63.1676)  time: 0.3028  data: 0.1087  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.1164 (3.3737)  acc1: 27.7778 (26.6434)  acc5: 61.1111 (62.3037)  time: 0.3087  data: 0.1150  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.1941 (3.3635)  acc1: 27.7778 (26.8933)  acc5: 61.1111 (62.5760)  time: 0.3398  data: 0.1587  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1447 (3.3669)  acc1: 27.7778 (26.8036)  acc5: 66.6667 (62.6646)  time: 0.3200  data: 0.1424  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4512 (3.3718)  acc1: 22.2222 (26.6466)  acc5: 66.6667 (62.4434)  time: 0.3020  data: 0.1104  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.3257 (3.3669)  acc1: 22.2222 (26.7677)  acc5: 72.2222 (62.9389)  time: 0.3127  data: 0.1175  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1149 (3.3513)  acc1: 33.3333 (27.1554)  acc5: 83.3333 (63.6238)  time: 0.3085  data: 0.1026  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1420 (3.3536)  acc1: 22.2222 (27.0252)  acc5: 77.7778 (63.5015)  time: 0.3075  data: 0.1026  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0716 (3.3359)  acc1: 38.8889 (28.0545)  acc5: 72.2222 (64.3040)  time: 0.2947  data: 0.1203  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.1188 (3.3322)  acc1: 44.4444 (28.2083)  acc5: 72.2222 (64.5551)  time: 0.2131  data: 0.0546  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.1188 (3.3337)  acc1: 38.8889 (28.1999)  acc5: 72.2222 (64.5505)  time: 0.2072  data: 0.0546  max mem: 15572
Val: Total time: 0:01:27 (0.3226 s / it)
* Acc@1 28.200 Acc@5 64.550 loss 3.334
Accuracy of the network on the 4883 val videos: 28.2%
Max accuracy: 29.31%
Epoch: [22]  [  0/254]  eta: 0:36:52  lr: 0.000025  min_lr: 0.000000  loss: 4.7043 (4.7043)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7113  data: 8.1817  max mem: 15572
Epoch: [22]  [ 10/254]  eta: 0:05:31  lr: 0.000024  min_lr: 0.000000  loss: 4.5366 (4.5553)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3593  data: 0.8992  max mem: 15572
Epoch: [22]  [ 20/254]  eta: 0:03:37  lr: 0.000024  min_lr: 0.000000  loss: 4.5493 (4.5781)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0858  max mem: 15572
Epoch: [22]  [ 30/254]  eta: 0:03:05  lr: 0.000024  min_lr: 0.000000  loss: 4.6383 (4.6246)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0834  max mem: 15572
Epoch: [22]  [ 40/254]  eta: 0:02:42  lr: 0.000024  min_lr: 0.000000  loss: 4.6626 (4.6344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1273  max mem: 15572
Epoch: [22]  [ 50/254]  eta: 0:02:23  lr: 0.000024  min_lr: 0.000000  loss: 4.7646 (4.6388)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0609  max mem: 15572
Epoch: [22]  [ 60/254]  eta: 0:02:17  lr: 0.000024  min_lr: 0.000000  loss: 4.7115 (4.6349)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1588  max mem: 15572
Epoch: [22]  [ 70/254]  eta: 0:02:04  lr: 0.000024  min_lr: 0.000000  loss: 4.7355 (4.6488)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1843  max mem: 15572
[2025-01-13 02:29:37,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:29:37,354] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:29:39,197] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27982
[2025-01-13 02:29:39,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:29:39,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 80/254]  eta: 0:01:54  lr: 0.000024  min_lr: 0.000000  loss: 4.7355 (4.6527)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0719  max mem: 15572
Epoch: [22]  [ 90/254]  eta: 0:01:44  lr: 0.000024  min_lr: 0.000000  loss: 4.6547 (4.6533)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0635  max mem: 15572
[2025-01-13 02:29:48,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=182, lr=[2.2987601337361343e-07, 2.2987601337361343e-07, 3.283943048194478e-07, 3.283943048194478e-07, 4.6913472117063976e-07, 4.6913472117063976e-07, 6.701924588151997e-07, 6.701924588151997e-07, 9.574177983074282e-07, 9.574177983074282e-07, 1.3677397118677546e-06, 1.3677397118677546e-06, 1.9539138740967923e-06, 1.9539138740967923e-06, 2.7913055344239894e-06, 2.7913055344239894e-06, 3.987579334891414e-06, 3.987579334891414e-06, 5.696541906987734e-06, 5.696541906987734e-06, 8.137917009982477e-06, 8.137917009982477e-06, 1.1625595728546398e-05, 1.1625595728546398e-05, 1.6607993897923427e-05, 1.6607993897923427e-05, 2.3725705568462038e-05, 2.3725705568462038e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:29:48,725] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=27.9236483335634, CurrSamplesPerSec=29.237207470492184, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [100/254]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000000  loss: 4.6620 (4.6570)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0705  max mem: 15572
Epoch: [22]  [110/254]  eta: 0:01:30  lr: 0.000024  min_lr: 0.000000  loss: 4.5702 (4.6510)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1297  max mem: 15572
Epoch: [22]  [120/254]  eta: 0:01:23  lr: 0.000024  min_lr: 0.000000  loss: 4.5799 (4.6483)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1535  max mem: 15572
Epoch: [22]  [130/254]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000000  loss: 4.6847 (4.6519)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1506  max mem: 15572
Epoch: [22]  [140/254]  eta: 0:01:09  lr: 0.000023  min_lr: 0.000000  loss: 4.6597 (4.6477)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1307  max mem: 15572
Epoch: [22]  [150/254]  eta: 0:01:03  lr: 0.000023  min_lr: 0.000000  loss: 4.6025 (4.6496)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1119  max mem: 15572
Epoch: [22]  [160/254]  eta: 0:00:58  lr: 0.000023  min_lr: 0.000000  loss: 4.6025 (4.6456)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.1812  max mem: 15572
Epoch: [22]  [170/254]  eta: 0:00:51  lr: 0.000023  min_lr: 0.000000  loss: 4.5428 (4.6417)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1650  max mem: 15572
Epoch: [22]  [180/254]  eta: 0:00:45  lr: 0.000023  min_lr: 0.000000  loss: 4.6800 (4.6516)  loss_scale: 65536.0000 (66984.3094)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1103  max mem: 15572
Epoch: [22]  [190/254]  eta: 0:00:39  lr: 0.000023  min_lr: 0.000000  loss: 4.7175 (4.6550)  loss_scale: 65536.0000 (66908.4817)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1413  max mem: 15572
Epoch: [22]  [200/254]  eta: 0:00:32  lr: 0.000023  min_lr: 0.000000  loss: 4.6724 (4.6516)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1012  max mem: 15572
[2025-01-13 02:30:54,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:30:54,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:30:54,708] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28112
[2025-01-13 02:30:54,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:30:54,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [210/254]  eta: 0:00:26  lr: 0.000023  min_lr: 0.000000  loss: 4.6484 (4.6535)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0899  max mem: 15572
Epoch: [22]  [220/254]  eta: 0:00:20  lr: 0.000023  min_lr: 0.000000  loss: 4.6842 (4.6589)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1090  max mem: 15572
Epoch: [22]  [230/254]  eta: 0:00:14  lr: 0.000023  min_lr: 0.000000  loss: 4.6530 (4.6509)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0820  max mem: 15572
Epoch: [22]  [240/254]  eta: 0:00:08  lr: 0.000023  min_lr: 0.000000  loss: 4.6163 (4.6509)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0910  max mem: 15572
Epoch: [22]  [250/254]  eta: 0:00:02  lr: 0.000022  min_lr: 0.000000  loss: 4.6755 (4.6528)  loss_scale: 65536.0000 (66841.4980)  weight_decay: 0.0500 (0.0500)  time: 0.4910  data: 0.0719  max mem: 15572
Epoch: [22]  [253/254]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000000  loss: 4.6760 (4.6531)  loss_scale: 65536.0000 (66826.0787)  weight_decay: 0.0500 (0.0500)  time: 0.4637  data: 0.0460  max mem: 15572
Epoch: [22] Total time: 0:02:30 (0.5917 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000000  loss: 4.6760 (4.6531)  loss_scale: 65536.0000 (66826.0787)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 283
Indices to remove: tensor([  271,   279,   298,   342,   422,   437,   484,   654,   781,   796,
          800,   816,   897,  1462,  1476,  1511,  1518,  1585,  1719,  1737,
         1745,  1896,  1924,  1952,  2054,  2057,  2078,  2107,  2136,  2144,
         2173,  2346,  2375,  2449,  2552,  2572,  2633,  2767,  2853,  2969,
         3072,  3132,  3288,  3329,  3631,  3712,  3722,  3935,  3956,  4001,
         4068,  4187,  4277,  4285,  4439,  4594,  4761,  4877,  5580,  5583,
         5708,  5775,  5870,  5942,  6329,  6718,  7025,  7509,  7572,  7648,
         7723,  7876,  8106,  8164,  9909, 10042, 10082, 10220, 10221, 10354,
        10442, 10489, 10593, 10757, 10788, 10835, 10864, 11016, 11055, 11123,
        11602, 11660, 11833, 11849, 11902, 11984, 12050, 12112, 12124, 12145,
        12147, 12298, 12310, 12325, 12660, 12733, 12764, 12820, 12894, 12932,
        13011, 13195, 13205, 13436, 13438, 13461, 13462, 13467, 13585, 13805,
        13813, 13865, 14051, 14137, 14640, 14648, 14677, 14702, 14797, 14966,
        14971, 14999, 15021, 15127, 15287, 15300, 15316, 15703, 16041, 16354,
        16395, 16470, 16699, 17887, 17905, 17955, 18032, 18058, 18060, 18109,
        18289, 18867, 18951, 19465, 19478, 19804, 19833, 19894, 19949, 19996,
        20034, 20511, 20629, 20729, 20790, 20817, 20941, 21023, 21327, 21599,
        21692, 21758, 21844, 21961, 22064, 22107, 22132, 22200, 22245, 22268,
        22401, 22725, 22795, 22985, 23035, 23063, 23311, 23327, 23334, 23377,
        23400, 23565, 23605, 23667, 23684, 23692, 23896, 23949, 24062, 24069,
        24071, 24161, 24204, 24241, 24458, 24477, 24483, 24817, 24852, 25026,
        25102, 25117, 25133, 25254, 25263, 25429, 25504, 25628, 25721, 25737,
        25744, 25764, 25877, 25900, 26040, 26266, 26269, 26297, 26309, 26323,
        26509, 27288, 27298, 27578, 27633, 27656, 27840, 27875, 28255, 28317,
        28391, 28526, 28545, 28696, 29069, 29139, 29279, 29312, 29339, 29507,
        29732, 30144, 30257, 30277, 30353, 30430, 30448, 30547, 30619, 30885,
        30952, 30985, 31003, 31084, 31280, 31311, 31338, 31358, 31382, 31518,
        31520, 31600, 31610, 32140, 32762, 32780, 32802, 32855, 32871, 32933,
        33442, 33455, 33469], device='cuda:0')
length of data loader train is: 230
num_training_steps_per_epoch is: 230
Change step level LR scheduler!
Set warmup steps = 1150
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:23:03  loss: 2.4315 (2.4315)  acc1: 61.1111 (61.1111)  acc5: 100.0000 (100.0000)  time: 5.0869  data: 4.8639  max mem: 15572
Val:  [ 10/272]  eta: 0:03:26  loss: 3.3852 (3.3735)  acc1: 27.7778 (29.7980)  acc5: 61.1111 (63.6364)  time: 0.7877  data: 0.5817  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 3.3994 (3.3820)  acc1: 27.7778 (29.8942)  acc5: 61.1111 (62.4339)  time: 0.3231  data: 0.1176  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 3.4512 (3.4128)  acc1: 27.7778 (27.4194)  acc5: 61.1111 (61.4695)  time: 0.2486  data: 0.0412  max mem: 15572
Val:  [ 40/272]  eta: 0:01:29  loss: 3.3419 (3.3975)  acc1: 16.6667 (26.0163)  acc5: 66.6667 (62.8726)  time: 0.2105  data: 0.0072  max mem: 15572
Val:  [ 50/272]  eta: 0:01:22  loss: 3.3376 (3.3681)  acc1: 27.7778 (27.5599)  acc5: 66.6667 (63.9434)  time: 0.2715  data: 0.0753  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 3.0617 (3.3347)  acc1: 27.7778 (27.0492)  acc5: 72.2222 (64.4809)  time: 0.3124  data: 0.1132  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 3.0643 (3.3355)  acc1: 22.2222 (27.9343)  acc5: 72.2222 (64.9452)  time: 0.3229  data: 0.1272  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.4808 (3.3498)  acc1: 16.6667 (27.0919)  acc5: 55.5556 (64.0604)  time: 0.3406  data: 0.1470  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.3836 (3.3527)  acc1: 22.2222 (27.2894)  acc5: 61.1111 (64.1636)  time: 0.3597  data: 0.1584  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.3836 (3.3708)  acc1: 22.2222 (26.2926)  acc5: 66.6667 (64.0814)  time: 0.3590  data: 0.1588  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.5521 (3.3932)  acc1: 11.1111 (25.1251)  acc5: 61.1111 (62.7127)  time: 0.3121  data: 0.1128  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5333 (3.3992)  acc1: 22.2222 (25.7576)  acc5: 61.1111 (62.5344)  time: 0.2816  data: 0.0917  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.1814 (3.3741)  acc1: 38.8889 (27.2265)  acc5: 61.1111 (63.4012)  time: 0.2791  data: 0.0925  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.0816 (3.3642)  acc1: 38.8889 (28.0930)  acc5: 72.2222 (63.7510)  time: 0.3081  data: 0.1053  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.1426 (3.3510)  acc1: 38.8889 (28.5136)  acc5: 66.6667 (64.3120)  time: 0.3063  data: 0.0945  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.1864 (3.3458)  acc1: 33.3333 (28.7440)  acc5: 77.7778 (64.8723)  time: 0.2878  data: 0.0873  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3014 (3.3530)  acc1: 16.6667 (28.1352)  acc5: 72.2222 (64.5549)  time: 0.3047  data: 0.1079  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.2926 (3.3415)  acc1: 16.6667 (28.0540)  acc5: 72.2222 (65.0399)  time: 0.3217  data: 0.1114  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.1792 (3.3581)  acc1: 22.2222 (27.6614)  acc5: 61.1111 (63.9907)  time: 0.3069  data: 0.1027  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2987 (3.3531)  acc1: 22.2222 (27.7225)  acc5: 61.1111 (64.0962)  time: 0.2862  data: 0.0846  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1980 (3.3589)  acc1: 22.2222 (27.4882)  acc5: 61.1111 (63.9284)  time: 0.2872  data: 0.0895  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.5234 (3.3639)  acc1: 16.6667 (27.3253)  acc5: 61.1111 (63.6501)  time: 0.3140  data: 0.1284  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2191 (3.3581)  acc1: 22.2222 (27.4170)  acc5: 72.2222 (64.1655)  time: 0.3661  data: 0.1750  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1072 (3.3450)  acc1: 33.3333 (27.7778)  acc5: 77.7778 (64.7994)  time: 0.3240  data: 0.1381  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1113 (3.3471)  acc1: 27.7778 (27.6007)  acc5: 77.7778 (64.8296)  time: 0.2309  data: 0.0461  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1702 (3.3351)  acc1: 33.3333 (28.3312)  acc5: 77.7778 (65.5385)  time: 0.2254  data: 0.0319  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2298 (3.3331)  acc1: 38.8889 (28.6183)  acc5: 72.2222 (65.4777)  time: 0.2133  data: 0.0399  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2298 (3.3349)  acc1: 38.8889 (28.6299)  acc5: 72.2222 (65.4720)  time: 0.2070  data: 0.0399  max mem: 15572
Val: Total time: 0:01:25 (0.3135 s / it)
* Acc@1 28.630 Acc@5 65.472 loss 3.335
Accuracy of the network on the 4883 val videos: 28.6%
Max accuracy: 29.31%
Epoch: [23]  [  0/230]  eta: 0:34:36  lr: 0.000022  min_lr: 0.000000  loss: 4.7950 (4.7950)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.0264  data: 8.5161  max mem: 15572
Epoch: [23]  [ 10/230]  eta: 0:04:49  lr: 0.000022  min_lr: 0.000000  loss: 4.6251 (4.6482)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3162  data: 0.8597  max mem: 15572
Epoch: [23]  [ 20/230]  eta: 0:03:25  lr: 0.000022  min_lr: 0.000000  loss: 4.6772 (4.7121)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1183  max mem: 15572
Epoch: [23]  [ 30/230]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000000  loss: 4.7458 (4.6754)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1314  max mem: 15572
Epoch: [23]  [ 40/230]  eta: 0:02:22  lr: 0.000022  min_lr: 0.000000  loss: 4.6057 (4.6617)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0721  max mem: 15572
Epoch: [23]  [ 50/230]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000000  loss: 4.6057 (4.6427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0566  max mem: 15572
Epoch: [23]  [ 60/230]  eta: 0:01:56  lr: 0.000022  min_lr: 0.000000  loss: 4.6774 (4.6478)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1046  max mem: 15572
Epoch: [23]  [ 70/230]  eta: 0:01:48  lr: 0.000022  min_lr: 0.000000  loss: 4.8156 (4.6610)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1512  max mem: 15572
Epoch: [23]  [ 80/230]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000000  loss: 4.7161 (4.6607)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1299  max mem: 15572
[2025-01-13 02:33:37,995] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:33:37,995] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:33:38,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28242
[2025-01-13 02:33:38,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:33:38,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 90/230]  eta: 0:01:29  lr: 0.000022  min_lr: 0.000000  loss: 4.6371 (4.6463)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0517  max mem: 15572
Epoch: [23]  [100/230]  eta: 0:01:22  lr: 0.000021  min_lr: 0.000000  loss: 4.6371 (4.6553)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.1000  max mem: 15572
Epoch: [23]  [110/230]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000000  loss: 4.7266 (4.6629)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1525  max mem: 15572
Epoch: [23]  [120/230]  eta: 0:01:08  lr: 0.000021  min_lr: 0.000000  loss: 4.7183 (4.6663)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1332  max mem: 15572
Epoch: [23]  [130/230]  eta: 0:01:02  lr: 0.000021  min_lr: 0.000000  loss: 4.6058 (4.6598)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1583  max mem: 15572
Epoch: [23]  [140/230]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 4.6058 (4.6608)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.2019  max mem: 15572
Epoch: [23]  [150/230]  eta: 0:00:49  lr: 0.000021  min_lr: 0.000000  loss: 4.6691 (4.6625)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1535  max mem: 15572
Epoch: [23]  [160/230]  eta: 0:00:43  lr: 0.000021  min_lr: 0.000000  loss: 4.7097 (4.6658)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1396  max mem: 15572
Epoch: [23]  [170/230]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 4.7097 (4.6669)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1772  max mem: 15572
Epoch: [23]  [180/230]  eta: 0:00:30  lr: 0.000021  min_lr: 0.000000  loss: 4.6379 (4.6644)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1470  max mem: 15572
Epoch: [23]  [190/230]  eta: 0:00:24  lr: 0.000021  min_lr: 0.000000  loss: 4.6653 (4.6654)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1883  max mem: 15572
Epoch: [23]  [200/230]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 4.7125 (4.6704)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1413  max mem: 15572
Epoch: [23]  [210/230]  eta: 0:00:12  lr: 0.000020  min_lr: 0.000000  loss: 4.7507 (4.6721)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0654  max mem: 15572
[2025-01-13 02:34:53,506] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:34:53,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:34:54,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28373
[2025-01-13 02:34:54,350] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:34:54,350] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [220/230]  eta: 0:00:06  lr: 0.000020  min_lr: 0.000000  loss: 4.7314 (4.6715)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0692  max mem: 15572
Epoch: [23]  [229/230]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 4.6175 (4.6709)  loss_scale: 65536.0000 (66390.8174)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0800  max mem: 15572
Epoch: [23] Total time: 0:02:17 (0.5986 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 4.6175 (4.6709)  loss_scale: 65536.0000 (66390.8174)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 249
Indices to remove: tensor([  308,   330,   385,   471,   603,   640,   662,   826,   871,  1125,
         1199,  1202,  1385,  1426,  1786,  2093,  2152,  2316,  2364,  2423,
         2485,  2502,  2505,  2517,  2534,  2545,  2550,  2558,  2714,  2905,
         3298,  3372,  3426,  3437,  3507,  3671,  3726,  4248,  4407,  4580,
         4602,  4726,  4728,  4764,  4813,  4821,  4884,  4919,  4949,  5103,
         5764,  5865,  6314,  6779,  6864,  7107,  7156,  7996,  8037,  8125,
         8195,  8336,  8345,  8433,  8468,  8811,  9903,  9968, 10005, 10030,
        10055, 10157, 10285, 10335, 10423, 10490, 10616, 10721, 10727, 10797,
        10802, 10851, 10861, 10993, 11196, 11223, 11291, 11347, 11456, 11477,
        11611, 11661, 11668, 11784, 11815, 11926, 12014, 12022, 12062, 12223,
        12313, 12327, 12333, 12367, 12496, 12515, 12557, 12594, 12666, 12681,
        12730, 12899, 12993, 13125, 13243, 13584, 13737, 13916, 13985, 14046,
        14090, 14476, 14635, 14922, 14974, 15042, 15335, 15386, 15393, 15708,
        15719, 16436, 16476, 16483, 16517, 16518, 16589, 16632, 17853, 17918,
        17992, 18019, 18134, 18152, 18165, 18232, 18309, 18439, 18901, 18958,
        18963, 19014, 19026, 19229, 19470, 20524, 20541, 20685, 20929, 21003,
        21154, 21489, 21511, 21779, 21906, 22249, 22277, 22301, 22307, 22463,
        22485, 22679, 22728, 22759, 22962, 23053, 23276, 23575, 23581, 23739,
        23941, 24064, 24068, 24352, 24677, 24687, 24696, 24714, 24865, 25409,
        25431, 25493, 25501, 25545, 25550, 25588, 25647, 25651, 25702, 25708,
        25710, 25714, 25848, 25878, 25901, 25950, 26056, 26254, 26276, 26280,
        26299, 26336, 26451, 26543, 26643, 26982, 26995, 27317, 27523, 28101,
        28312, 28370, 28459, 28524, 29175, 29223, 29494, 29606, 30090, 30273,
        30342, 30730, 30892, 30951, 31302, 31330, 31331, 31404, 31570, 31952,
        32818, 32850, 32878, 32900, 32915, 33196, 33290, 33509, 33586],
       device='cuda:0')
length of data loader train is: 210
num_training_steps_per_epoch is: 210
Change step level LR scheduler!
Set warmup steps = 1050
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:25:27  loss: 2.5854 (2.5854)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 5.6170  data: 5.4384  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 3.5164 (3.4514)  acc1: 16.6667 (19.6970)  acc5: 61.1111 (60.6061)  time: 0.7742  data: 0.5826  max mem: 15572
Val:  [ 20/272]  eta: 0:02:26  loss: 3.4265 (3.4096)  acc1: 16.6667 (23.5450)  acc5: 61.1111 (61.6402)  time: 0.3303  data: 0.1377  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 3.4265 (3.4281)  acc1: 27.7778 (23.8351)  acc5: 61.1111 (60.5735)  time: 0.3277  data: 0.1317  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.3456 (3.4062)  acc1: 22.2222 (23.0352)  acc5: 61.1111 (62.3306)  time: 0.2491  data: 0.0512  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 3.3368 (3.3624)  acc1: 27.7778 (25.0545)  acc5: 72.2222 (63.7255)  time: 0.2631  data: 0.0583  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 3.0377 (3.3350)  acc1: 27.7778 (24.9545)  acc5: 72.2222 (64.0255)  time: 0.3061  data: 0.1023  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 3.2021 (3.3288)  acc1: 27.7778 (26.5258)  acc5: 66.6667 (64.4757)  time: 0.3251  data: 0.1378  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 3.4055 (3.3436)  acc1: 22.2222 (25.9259)  acc5: 61.1111 (64.0604)  time: 0.3465  data: 0.1639  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.4250 (3.3563)  acc1: 22.2222 (25.4579)  acc5: 55.5556 (63.4921)  time: 0.3427  data: 0.1479  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.4250 (3.3742)  acc1: 11.1111 (24.8625)  acc5: 61.1111 (63.4213)  time: 0.3344  data: 0.1260  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 3.5757 (3.3976)  acc1: 11.1111 (24.0240)  acc5: 55.5556 (62.1121)  time: 0.3045  data: 0.0977  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.5645 (3.4034)  acc1: 22.2222 (25.2066)  acc5: 55.5556 (62.0294)  time: 0.3190  data: 0.1134  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2431 (3.3769)  acc1: 44.4444 (26.9296)  acc5: 66.6667 (63.1043)  time: 0.3119  data: 0.1101  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.0957 (3.3725)  acc1: 38.8889 (27.3838)  acc5: 72.2222 (63.2782)  time: 0.2749  data: 0.0831  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.2094 (3.3593)  acc1: 38.8889 (27.8882)  acc5: 66.6667 (63.5026)  time: 0.3414  data: 0.1429  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.1702 (3.3537)  acc1: 27.7778 (28.0538)  acc5: 72.2222 (64.1132)  time: 0.3274  data: 0.1207  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3751 (3.3610)  acc1: 22.2222 (27.4854)  acc5: 72.2222 (63.6127)  time: 0.2392  data: 0.0312  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3147 (3.3498)  acc1: 16.6667 (27.4095)  acc5: 66.6667 (64.1805)  time: 0.2496  data: 0.0501  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.1717 (3.3644)  acc1: 22.2222 (27.0797)  acc5: 61.1111 (63.2054)  time: 0.3339  data: 0.1291  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.2842 (3.3559)  acc1: 22.2222 (27.1144)  acc5: 61.1111 (63.5987)  time: 0.3424  data: 0.1349  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2778 (3.3648)  acc1: 22.2222 (26.9352)  acc5: 66.6667 (63.2965)  time: 0.2951  data: 0.1024  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5879 (3.3732)  acc1: 16.6667 (26.6968)  acc5: 50.0000 (62.8959)  time: 0.3015  data: 0.1006  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.3367 (3.3699)  acc1: 16.6667 (27.2246)  acc5: 66.6667 (63.2756)  time: 0.3383  data: 0.1356  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0755 (3.3577)  acc1: 33.3333 (27.5473)  acc5: 77.7778 (63.8543)  time: 0.3427  data: 0.1241  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1383 (3.3588)  acc1: 27.7778 (27.4900)  acc5: 77.7778 (63.9221)  time: 0.3251  data: 0.1033  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1457 (3.3463)  acc1: 38.8889 (28.2035)  acc5: 77.7778 (64.5381)  time: 0.2637  data: 0.0805  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2867 (3.3488)  acc1: 33.3333 (28.1673)  acc5: 66.6667 (64.4116)  time: 0.1743  data: 0.0121  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2867 (3.3509)  acc1: 33.3333 (28.1794)  acc5: 66.6667 (64.4071)  time: 0.1692  data: 0.0121  max mem: 15572
Val: Total time: 0:01:27 (0.3214 s / it)
* Acc@1 28.179 Acc@5 64.407 loss 3.351
Accuracy of the network on the 4883 val videos: 28.2%
Max accuracy: 29.31%
Epoch: [24]  [  0/210]  eta: 0:26:54  lr: 0.000020  min_lr: 0.000000  loss: 4.4563 (4.4563)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6884  data: 7.2614  max mem: 15572
Epoch: [24]  [ 10/210]  eta: 0:03:57  lr: 0.000020  min_lr: 0.000000  loss: 4.5309 (4.5472)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1871  data: 0.7303  max mem: 15572
Epoch: [24]  [ 20/210]  eta: 0:02:51  lr: 0.000020  min_lr: 0.000000  loss: 4.6471 (4.6069)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1072  max mem: 15572
Epoch: [24]  [ 30/210]  eta: 0:02:22  lr: 0.000020  min_lr: 0.000000  loss: 4.6798 (4.6340)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1208  max mem: 15572
Epoch: [24]  [ 40/210]  eta: 0:02:05  lr: 0.000020  min_lr: 0.000000  loss: 4.7095 (4.6299)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1239  max mem: 15572
Epoch: [24]  [ 50/210]  eta: 0:01:54  lr: 0.000020  min_lr: 0.000000  loss: 4.6592 (4.6277)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1650  max mem: 15572
Epoch: [24]  [ 60/210]  eta: 0:01:42  lr: 0.000020  min_lr: 0.000000  loss: 4.6662 (4.6410)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1204  max mem: 15572
Epoch: [24]  [ 70/210]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 4.6203 (4.6379)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1312  max mem: 15572
Epoch: [24]  [ 80/210]  eta: 0:01:26  lr: 0.000020  min_lr: 0.000000  loss: 4.6125 (4.6390)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1643  max mem: 15572
Epoch: [24]  [ 90/210]  eta: 0:01:19  lr: 0.000019  min_lr: 0.000000  loss: 4.6251 (4.6382)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1491  max mem: 15572
Epoch: [24]  [100/210]  eta: 0:01:11  lr: 0.000019  min_lr: 0.000000  loss: 4.6474 (4.6379)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1277  max mem: 15572
Epoch: [24]  [110/210]  eta: 0:01:03  lr: 0.000019  min_lr: 0.000000  loss: 4.7231 (4.6465)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0862  max mem: 15572
[2025-01-13 02:37:40,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:37:40,300] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:37:40,802] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28503
[2025-01-13 02:37:40,803] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:37:40,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [120/210]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 4.6751 (4.6462)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0960  max mem: 15572
Epoch: [24]  [130/210]  eta: 0:00:50  lr: 0.000019  min_lr: 0.000000  loss: 4.7037 (4.6516)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1213  max mem: 15572
Epoch: [24]  [140/210]  eta: 0:00:43  lr: 0.000019  min_lr: 0.000000  loss: 4.7037 (4.6520)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1428  max mem: 15572
Epoch: [24]  [150/210]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 4.6589 (4.6541)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1089  max mem: 15572
Epoch: [24]  [160/210]  eta: 0:00:30  lr: 0.000019  min_lr: 0.000000  loss: 4.6858 (4.6559)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1573  max mem: 15572
Epoch: [24]  [170/210]  eta: 0:00:24  lr: 0.000019  min_lr: 0.000000  loss: 4.7118 (4.6616)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1706  max mem: 15572
Epoch: [24]  [180/210]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 4.6778 (4.6553)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1320  max mem: 15572
Epoch: [24]  [190/210]  eta: 0:00:12  lr: 0.000018  min_lr: 0.000000  loss: 4.5916 (4.6579)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1742  max mem: 15572
Epoch: [24]  [200/210]  eta: 0:00:06  lr: 0.000018  min_lr: 0.000000  loss: 4.6423 (4.6587)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1061  max mem: 15572
[2025-01-13 02:38:32,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28594
[2025-01-13 02:38:32,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-13 02:38:32,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [209/210]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 4.7313 (4.6610)  loss_scale: 65536.0000 (65223.9238)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.1058  max mem: 15572
Epoch: [24] Total time: 0:02:05 (0.5994 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 4.7313 (4.6610)  loss_scale: 65536.0000 (65223.9238)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 262
Indices to remove: tensor([  257,   357,   566,   671,   830,   851,   867,  1580,  1711,  1791,
         1935,  2102,  2314,  2323,  2495,  2499,  2686,  2717,  2726,  2770,
         2811,  2968,  2979,  3219,  3300,  3342,  3413,  3511,  4012,  4064,
         4473,  4488,  4495,  4498,  4626,  4666,  4760,  4806,  4927,  4970,
         5587,  6104,  6302,  6542,  6861,  6942,  6945,  7037,  7811,  7867,
         7898,  7953,  8000,  8145,  9166,  9453, 10019, 10203, 10261, 10397,
        10449, 10457, 10474, 10530, 10537, 10550, 10576, 10764, 10765, 10806,
        10846, 10900, 10987, 11000, 11027, 11034, 11056, 11092, 11255, 11310,
        11320, 11482, 11485, 11696, 11758, 11831, 11912, 12020, 12150, 12199,
        12224, 12239, 12873, 12923, 13028, 13034, 13049, 13171, 13235, 13257,
        13298, 13518, 13581, 13744, 13937, 14000, 14297, 14391, 14405, 14412,
        14712, 14729, 14753, 14936, 14991, 15053, 15169, 15202, 15237, 15273,
        15355, 15601, 15622, 15701, 15781, 16094, 16273, 16280, 16444, 16477,
        16479, 16492, 16559, 16611, 16698, 17450, 18035, 18066, 18139, 18163,
        18254, 18295, 18352, 18356, 18631, 18758, 19002, 19022, 19054, 19091,
        19256, 19290, 19344, 19362, 19383, 19713, 19776, 19803, 19968, 20129,
        20161, 20300, 20321, 20432, 20582, 20788, 20912, 21000, 21019, 21136,
        21477, 21837, 21841, 21950, 21986, 22028, 22082, 22100, 22148, 22266,
        22285, 22642, 22706, 22811, 23070, 23956, 24014, 24031, 24067, 24103,
        24129, 24158, 24261, 24367, 24546, 24741, 24769, 24903, 25186, 25310,
        25524, 25712, 25760, 25774, 25960, 26166, 26293, 26306, 26484, 26585,
        26613, 26779, 26783, 26802, 26845, 26852, 26970, 27923, 27977, 28164,
        28266, 28320, 28355, 28384, 28823, 28904, 29144, 29221, 29266, 29278,
        29498, 29822, 30030, 30049, 30243, 30361, 30405, 30573, 30594, 30643,
        30712, 30785, 30905, 30914, 30939, 31125, 31339, 31360, 31408, 31416,
        31519, 32238, 32775, 32808, 32817, 32826, 32888, 32892, 32907, 33456,
        33577, 33704], device='cuda:0')
length of data loader train is: 188
num_training_steps_per_epoch is: 188
Change step level LR scheduler!
Set warmup steps = 940
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:18:23  loss: 2.6350 (2.6350)  acc1: 50.0000 (50.0000)  acc5: 100.0000 (100.0000)  time: 4.0552  data: 3.8817  max mem: 15572
Val:  [ 10/272]  eta: 0:03:03  loss: 3.4147 (3.4111)  acc1: 22.2222 (23.2323)  acc5: 66.6667 (63.6364)  time: 0.7006  data: 0.4921  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 3.4147 (3.3950)  acc1: 27.7778 (26.7196)  acc5: 61.1111 (63.2275)  time: 0.3378  data: 0.1307  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 3.4733 (3.4119)  acc1: 27.7778 (26.8817)  acc5: 61.1111 (62.5448)  time: 0.3015  data: 0.1048  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 3.4097 (3.4054)  acc1: 22.2222 (25.7453)  acc5: 61.1111 (63.0081)  time: 0.3021  data: 0.0978  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 3.3415 (3.3700)  acc1: 22.2222 (26.9063)  acc5: 61.1111 (63.8344)  time: 0.3163  data: 0.1072  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 3.0586 (3.3336)  acc1: 27.7778 (26.9581)  acc5: 72.2222 (64.7541)  time: 0.3218  data: 0.1132  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.1250 (3.3310)  acc1: 27.7778 (28.4820)  acc5: 72.2222 (65.1800)  time: 0.3172  data: 0.1071  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.3868 (3.3306)  acc1: 27.7778 (28.3265)  acc5: 61.1111 (65.1578)  time: 0.2958  data: 0.0983  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.3868 (3.3380)  acc1: 22.2222 (27.8999)  acc5: 66.6667 (64.9573)  time: 0.3237  data: 0.1288  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.4887 (3.3612)  acc1: 16.6667 (27.1177)  acc5: 55.5556 (64.0264)  time: 0.3014  data: 0.1141  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6096 (3.3821)  acc1: 11.1111 (25.8258)  acc5: 50.0000 (62.8128)  time: 0.2902  data: 0.1001  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.4986 (3.3882)  acc1: 16.6667 (26.4463)  acc5: 55.5556 (62.8099)  time: 0.3419  data: 0.1367  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.1928 (3.3659)  acc1: 33.3333 (27.8626)  acc5: 66.6667 (63.6980)  time: 0.3482  data: 0.1345  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1864 (3.3645)  acc1: 38.8889 (28.1718)  acc5: 72.2222 (63.7510)  time: 0.3197  data: 0.1133  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.2622 (3.3496)  acc1: 38.8889 (28.9183)  acc5: 66.6667 (64.3488)  time: 0.2440  data: 0.0524  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.1264 (3.3447)  acc1: 33.3333 (29.0200)  acc5: 72.2222 (64.7343)  time: 0.3188  data: 0.1324  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3367 (3.3534)  acc1: 16.6667 (28.2651)  acc5: 66.6667 (64.3275)  time: 0.3460  data: 0.1635  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.2882 (3.3476)  acc1: 16.6667 (27.8392)  acc5: 66.6667 (64.6409)  time: 0.3011  data: 0.1106  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2460 (3.3655)  acc1: 22.2222 (27.4869)  acc5: 55.5556 (63.5544)  time: 0.3069  data: 0.1114  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.3666 (3.3600)  acc1: 22.2222 (27.5843)  acc5: 55.5556 (63.7369)  time: 0.2984  data: 0.0986  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2173 (3.3650)  acc1: 22.2222 (27.4618)  acc5: 61.1111 (63.4281)  time: 0.3028  data: 0.1135  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5253 (3.3694)  acc1: 22.2222 (27.3002)  acc5: 61.1111 (63.1976)  time: 0.3265  data: 0.1520  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.2337 (3.3627)  acc1: 27.7778 (27.8018)  acc5: 77.7778 (63.8047)  time: 0.3765  data: 0.1733  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0403 (3.3474)  acc1: 44.4444 (28.3310)  acc5: 83.3333 (64.4767)  time: 0.2915  data: 0.0840  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0688 (3.3488)  acc1: 33.3333 (28.1983)  acc5: 77.7778 (64.4533)  time: 0.2204  data: 0.0287  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1013 (3.3339)  acc1: 38.8889 (28.9485)  acc5: 77.7778 (65.1980)  time: 0.2778  data: 0.0682  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2904 (3.3356)  acc1: 33.3333 (29.0488)  acc5: 72.2222 (65.0062)  time: 0.2721  data: 0.0842  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2904 (3.3373)  acc1: 33.3333 (29.0190)  acc5: 61.1111 (65.0010)  time: 0.2617  data: 0.0842  max mem: 15572
Val: Total time: 0:01:27 (0.3217 s / it)
* Acc@1 29.019 Acc@5 65.001 loss 3.337
Accuracy of the network on the 4883 val videos: 29.0%
Max accuracy: 29.31%
Epoch: [25]  [  0/188]  eta: 0:29:48  lr: 0.000018  min_lr: 0.000000  loss: 4.5716 (4.5716)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.5144  data: 9.0993  max mem: 15572
Epoch: [25]  [ 10/188]  eta: 0:03:59  lr: 0.000018  min_lr: 0.000000  loss: 4.6538 (4.6668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3446  data: 0.9135  max mem: 15572
Epoch: [25]  [ 20/188]  eta: 0:02:47  lr: 0.000018  min_lr: 0.000000  loss: 4.7777 (4.7328)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1464  max mem: 15572
Epoch: [25]  [ 30/188]  eta: 0:02:17  lr: 0.000018  min_lr: 0.000000  loss: 4.8178 (4.7282)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1725  max mem: 15572
Epoch: [25]  [ 40/188]  eta: 0:02:00  lr: 0.000018  min_lr: 0.000000  loss: 4.7414 (4.7164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1620  max mem: 15572
Epoch: [25]  [ 50/188]  eta: 0:01:42  lr: 0.000018  min_lr: 0.000000  loss: 4.6791 (4.6960)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0887  max mem: 15572
Epoch: [25]  [ 60/188]  eta: 0:01:29  lr: 0.000018  min_lr: 0.000000  loss: 4.7276 (4.7089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4651  data: 0.0252  max mem: 15572
Epoch: [25]  [ 70/188]  eta: 0:01:20  lr: 0.000017  min_lr: 0.000000  loss: 4.7793 (4.7068)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0886  max mem: 15572
Epoch: [25]  [ 80/188]  eta: 0:01:10  lr: 0.000017  min_lr: 0.000000  loss: 4.7181 (4.7131)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0762  max mem: 15572
Epoch: [25]  [ 90/188]  eta: 0:01:03  lr: 0.000017  min_lr: 0.000000  loss: 4.6903 (4.7010)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0747  max mem: 15572
Epoch: [25]  [100/188]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 4.6868 (4.7128)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0806  max mem: 15572
Epoch: [25]  [110/188]  eta: 0:00:48  lr: 0.000017  min_lr: 0.000000  loss: 4.6911 (4.7081)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0478  max mem: 15572
Epoch: [25]  [120/188]  eta: 0:00:41  lr: 0.000017  min_lr: 0.000000  loss: 4.6262 (4.7014)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0655  max mem: 15572
[2025-01-13 02:41:17,814] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:41:17,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [130/188]  eta: 0:00:34  lr: 0.000017  min_lr: 0.000000  loss: 4.6851 (4.7051)  loss_scale: 32768.0000 (34268.8244)  weight_decay: 0.0500 (0.0500)  time: 0.4663  data: 0.0396  max mem: 15572
Epoch: [25]  [140/188]  eta: 0:00:28  lr: 0.000017  min_lr: 0.000000  loss: 4.6851 (4.7099)  loss_scale: 65536.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  time: 0.4219  data: 0.0038  max mem: 15572
Epoch: [25]  [150/188]  eta: 0:00:22  lr: 0.000017  min_lr: 0.000000  loss: 4.7218 (4.7097)  loss_scale: 65536.0000 (38410.1722)  weight_decay: 0.0500 (0.0500)  time: 0.4767  data: 0.0288  max mem: 15572
Epoch: [25]  [160/188]  eta: 0:00:16  lr: 0.000017  min_lr: 0.000000  loss: 4.6745 (4.7022)  loss_scale: 65536.0000 (40095.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1248  max mem: 15572
Epoch: [25]  [170/188]  eta: 0:00:10  lr: 0.000016  min_lr: 0.000000  loss: 4.6176 (4.6902)  loss_scale: 65536.0000 (41582.7836)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1443  max mem: 15572
Epoch: [25]  [180/188]  eta: 0:00:04  lr: 0.000016  min_lr: 0.000000  loss: 4.5934 (4.6860)  loss_scale: 65536.0000 (42906.1657)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1995  max mem: 15572
Epoch: [25]  [187/188]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 4.5840 (4.6838)  loss_scale: 65536.0000 (43748.7660)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1518  max mem: 15572
Epoch: [25] Total time: 0:01:50 (0.5887 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 4.5840 (4.6838)  loss_scale: 65536.0000 (43748.7660)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 227
Indices to remove: tensor([  235,   265,   401,   485,   511,   516,   815,  1345,  1458,  1488,
         1624,  1934,  1979,  2032,  2038,  2047,  2129,  2310,  2325,  2489,
         2496,  2497,  2519,  2542,  2547,  2588,  2952,  3029,  3039,  3095,
         3218,  3584,  3908,  3955,  4223,  4274,  4329,  5024,  5390,  5487,
         5778,  5950,  5954,  6424,  6556,  6656,  6862,  6888,  6899,  6983,
         7584,  8035,  8102,  9472, 10015, 10091, 10248, 10350, 10697, 10724,
        10749, 10852, 10953, 11026, 11053, 11063, 11489, 11526, 11764, 11832,
        11934, 11970, 12059, 12159, 12171, 12175, 12252, 12353, 12483, 12581,
        12776, 12800, 12957, 13175, 13271, 13385, 13412, 14168, 14286, 14498,
        14507, 14543, 14544, 14618, 14696, 14738, 14851, 14860, 14866, 15084,
        15387, 15447, 15510, 15527, 15645, 15707, 16156, 16404, 16481, 16522,
        16570, 16582, 16613, 16865, 17031, 17221, 17885, 17959, 18090, 18106,
        18311, 18643, 18790, 19171, 19193, 19323, 19410, 19458, 19514, 19637,
        19927, 20096, 20098, 20180, 20494, 20514, 20540, 20838, 20953, 21496,
        21702, 21771, 22150, 22184, 22305, 22437, 22562, 22616, 22788, 22901,
        23425, 23549, 23651, 23836, 23961, 23964, 24007, 24391, 24626, 24684,
        24827, 24935, 24997, 25006, 25162, 25199, 25311, 25450, 25607, 25656,
        25851, 25872, 26229, 26234, 26281, 26282, 26286, 26292, 26334, 26422,
        26464, 26473, 26650, 26864, 27294, 27543, 27576, 27685, 28233, 28307,
        28314, 28540, 28548, 28648, 28824, 28963, 29122, 29337, 29634, 29816,
        29909, 29941, 30012, 30015, 30048, 30298, 30484, 30933, 31059, 31103,
        31215, 31262, 31333, 31386, 31398, 31464, 31573, 31620, 32782, 32800,
        32841, 33440, 33478, 33591, 33594, 33619, 33649], device='cuda:0')
length of data loader train is: 169
num_training_steps_per_epoch is: 169
Change step level LR scheduler!
Set warmup steps = 845
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:29:43  loss: 2.6168 (2.6168)  acc1: 50.0000 (50.0000)  acc5: 100.0000 (100.0000)  time: 6.5576  data: 6.3713  max mem: 15572
Val:  [ 10/272]  eta: 0:04:28  loss: 3.5369 (3.5427)  acc1: 22.2222 (21.7172)  acc5: 55.5556 (55.0505)  time: 1.0248  data: 0.8154  max mem: 15572
Val:  [ 20/272]  eta: 0:02:50  loss: 3.4549 (3.4691)  acc1: 22.2222 (25.9259)  acc5: 61.1111 (58.7302)  time: 0.3842  data: 0.1789  max mem: 15572
Val:  [ 30/272]  eta: 0:02:07  loss: 3.4412 (3.4710)  acc1: 22.2222 (24.9104)  acc5: 61.1111 (59.4982)  time: 0.2503  data: 0.0557  max mem: 15572
Val:  [ 40/272]  eta: 0:01:55  loss: 3.3406 (3.4468)  acc1: 22.2222 (24.1192)  acc5: 66.6667 (61.3821)  time: 0.3054  data: 0.1046  max mem: 15572
Val:  [ 50/272]  eta: 0:01:47  loss: 3.3397 (3.3976)  acc1: 22.2222 (25.9259)  acc5: 66.6667 (63.2898)  time: 0.4156  data: 0.2136  max mem: 15572
Val:  [ 60/272]  eta: 0:01:37  loss: 3.0392 (3.3627)  acc1: 27.7778 (25.4098)  acc5: 72.2222 (63.7523)  time: 0.3822  data: 0.1799  max mem: 15572
Val:  [ 70/272]  eta: 0:01:30  loss: 3.1785 (3.3557)  acc1: 27.7778 (26.6041)  acc5: 72.2222 (64.3975)  time: 0.3585  data: 0.1565  max mem: 15572
Val:  [ 80/272]  eta: 0:01:23  loss: 3.3990 (3.3695)  acc1: 27.7778 (26.3374)  acc5: 61.1111 (63.5802)  time: 0.3516  data: 0.1485  max mem: 15572
Val:  [ 90/272]  eta: 0:01:16  loss: 3.5043 (3.3878)  acc1: 22.2222 (25.6410)  acc5: 55.5556 (62.8816)  time: 0.3162  data: 0.1034  max mem: 15572
Val:  [100/272]  eta: 0:01:11  loss: 3.5029 (3.4000)  acc1: 16.6667 (25.2475)  acc5: 61.1111 (62.8713)  time: 0.3397  data: 0.1245  max mem: 15572
Val:  [110/272]  eta: 0:01:06  loss: 3.5628 (3.4210)  acc1: 11.1111 (24.1241)  acc5: 55.5556 (61.6617)  time: 0.3801  data: 0.1670  max mem: 15572
Val:  [120/272]  eta: 0:01:02  loss: 3.5481 (3.4225)  acc1: 22.2222 (25.0230)  acc5: 55.5556 (61.9376)  time: 0.4029  data: 0.1946  max mem: 15572
Val:  [130/272]  eta: 0:00:57  loss: 3.1481 (3.3917)  acc1: 38.8889 (27.0568)  acc5: 72.2222 (63.0619)  time: 0.3586  data: 0.1603  max mem: 15572
Val:  [140/272]  eta: 0:00:52  loss: 3.0955 (3.3816)  acc1: 44.4444 (27.8172)  acc5: 72.2222 (63.4358)  time: 0.3145  data: 0.1169  max mem: 15572
Val:  [150/272]  eta: 0:00:46  loss: 3.2140 (3.3689)  acc1: 38.8889 (28.4768)  acc5: 72.2222 (63.9809)  time: 0.2518  data: 0.0618  max mem: 15572
Val:  [160/272]  eta: 0:00:41  loss: 3.2887 (3.3698)  acc1: 27.7778 (28.5024)  acc5: 72.2222 (64.3547)  time: 0.1700  data: 0.0005  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.4347 (3.3793)  acc1: 16.6667 (27.8103)  acc5: 66.6667 (63.7427)  time: 0.1682  data: 0.0004  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3759 (3.3705)  acc1: 16.6667 (27.5629)  acc5: 61.1111 (64.2726)  time: 0.1653  data: 0.0004  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2329 (3.3855)  acc1: 16.6667 (27.1670)  acc5: 55.5556 (63.2344)  time: 0.1740  data: 0.0005  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.2663 (3.3777)  acc1: 22.2222 (27.2526)  acc5: 61.1111 (63.5158)  time: 0.1809  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.1594 (3.3835)  acc1: 22.2222 (27.2249)  acc5: 66.6667 (63.3491)  time: 0.2204  data: 0.0348  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.6374 (3.3916)  acc1: 22.2222 (26.9231)  acc5: 55.5556 (62.8457)  time: 0.3005  data: 0.1071  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2679 (3.3863)  acc1: 33.3333 (27.5373)  acc5: 66.6667 (63.2756)  time: 0.3307  data: 0.1234  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0575 (3.3704)  acc1: 38.8889 (27.9161)  acc5: 77.7778 (63.9465)  time: 0.3176  data: 0.1132  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1378 (3.3711)  acc1: 33.3333 (27.7778)  acc5: 77.7778 (63.9442)  time: 0.3241  data: 0.1464  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0900 (3.3553)  acc1: 38.8889 (28.6718)  acc5: 77.7778 (64.6020)  time: 0.3155  data: 0.1357  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.1810 (3.3572)  acc1: 33.3333 (28.5978)  acc5: 72.2222 (64.5961)  time: 0.2642  data: 0.0955  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.1810 (3.3590)  acc1: 33.3333 (28.6095)  acc5: 72.2222 (64.6119)  time: 0.2408  data: 0.0782  max mem: 15572
Val: Total time: 0:01:28 (0.3236 s / it)
* Acc@1 28.609 Acc@5 64.612 loss 3.359
Accuracy of the network on the 4883 val videos: 28.6%
Max accuracy: 29.31%
Epoch: [26]  [  0/169]  eta: 0:21:01  lr: 0.000016  min_lr: 0.000000  loss: 4.7005 (4.7005)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.4622  data: 6.9628  max mem: 15572
Epoch: [26]  [ 10/169]  eta: 0:03:11  lr: 0.000016  min_lr: 0.000000  loss: 4.5834 (4.6687)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2038  data: 0.7756  max mem: 15572
Epoch: [26]  [ 20/169]  eta: 0:02:14  lr: 0.000016  min_lr: 0.000000  loss: 4.7031 (4.7149)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1386  max mem: 15572
Epoch: [26]  [ 30/169]  eta: 0:01:50  lr: 0.000016  min_lr: 0.000000  loss: 4.7982 (4.7116)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1157  max mem: 15572
Epoch: [26]  [ 40/169]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 4.7084 (4.7029)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1128  max mem: 15572
Epoch: [26]  [ 50/169]  eta: 0:01:23  lr: 0.000016  min_lr: 0.000000  loss: 4.6939 (4.6890)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1006  max mem: 15572
Epoch: [26]  [ 60/169]  eta: 0:01:12  lr: 0.000016  min_lr: 0.000000  loss: 4.6901 (4.6902)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0698  max mem: 15572
[2025-01-13 02:44:04,214] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:44:04,214] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:44:05,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28853
[2025-01-13 02:44:05,098] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:44:05,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 70/169]  eta: 0:01:04  lr: 0.000015  min_lr: 0.000000  loss: 4.6872 (4.6852)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0881  max mem: 15572
Epoch: [26]  [ 80/169]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 4.7715 (4.7044)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1508  max mem: 15572
Epoch: [26]  [ 90/169]  eta: 0:00:50  lr: 0.000015  min_lr: 0.000000  loss: 4.7915 (4.7029)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1593  max mem: 15572
Epoch: [26]  [100/169]  eta: 0:00:43  lr: 0.000015  min_lr: 0.000000  loss: 4.6047 (4.6874)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1260  max mem: 15572
Epoch: [26]  [110/169]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 4.6047 (4.6805)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1407  max mem: 15572
Epoch: [26]  [120/169]  eta: 0:00:30  lr: 0.000015  min_lr: 0.000000  loss: 4.6867 (4.6826)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1201  max mem: 15572
Epoch: [26]  [130/169]  eta: 0:00:24  lr: 0.000015  min_lr: 0.000000  loss: 4.6858 (4.6833)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1164  max mem: 15572
Epoch: [26]  [140/169]  eta: 0:00:17  lr: 0.000015  min_lr: 0.000000  loss: 4.6801 (4.6771)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1749  max mem: 15572
Epoch: [26]  [150/169]  eta: 0:00:11  lr: 0.000014  min_lr: 0.000000  loss: 4.6452 (4.6776)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1590  max mem: 15572
Epoch: [26]  [160/169]  eta: 0:00:05  lr: 0.000014  min_lr: 0.000000  loss: 4.6452 (4.6724)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0938  max mem: 15572
Epoch: [26]  [168/169]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 4.6119 (4.6721)  loss_scale: 65536.0000 (66311.5740)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0735  max mem: 15572
Epoch: [26] Total time: 0:01:41 (0.5982 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 4.6119 (4.6721)  loss_scale: 65536.0000 (66311.5740)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 174
Indices to remove: tensor([  245,   246,   285,   540,   745,   767,   899,  1443,  1536,  1584,
         2051,  2061,  2154,  2286,  2341,  2530,  2672,  2852,  2854,  3165,
         3285,  3603,  3898,  4051,  4302,  4476,  4506,  4610,  4961,  5096,
         6557,  6846,  6856,  6859,  6895,  6941,  7424,  7744,  7922,  8396,
         8550,  9573,  9758,  9863, 10195, 10662, 10779, 11158, 11169, 11249,
        11268, 11293, 11323, 11448, 11494, 11860, 12027, 12352, 12450, 12579,
        12758, 12772, 12774, 12965, 13147, 13222, 13406, 13756, 13787, 13838,
        14414, 14454, 14479, 14538, 14569, 14879, 15173, 15586, 16341, 16434,
        17904, 17925, 17950, 18073, 18117, 18394, 18556, 18704, 18759, 18950,
        18960, 19162, 19474, 19479, 19557, 19645, 19733, 19813, 19818, 19895,
        20286, 20571, 20847, 21437, 21711, 21732, 21750, 21757, 22146, 22286,
        22778, 22859, 22919, 23002, 23106, 23284, 23389, 23587, 23785, 23838,
        24829, 24973, 24981, 25231, 25319, 25397, 25618, 25859, 25882, 25892,
        26188, 26250, 26295, 26298, 26314, 26566, 26762, 26857, 26867, 27052,
        27387, 27485, 27509, 27564, 27637, 27657, 28237, 28253, 28277, 28368,
        28568, 29362, 29410, 29429, 29443, 29464, 29540, 29619, 29654, 30919,
        30948, 31391, 31393, 31577, 31694, 32935, 32943, 33085, 33334, 33447,
        33534, 33605, 33611, 33690], device='cuda:0')
length of data loader train is: 154
num_training_steps_per_epoch is: 154
Change step level LR scheduler!
Set warmup steps = 770
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:56  loss: 2.6952 (2.6952)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 4.3982  data: 4.2236  max mem: 15572
Val:  [ 10/272]  eta: 0:03:25  loss: 3.5211 (3.4985)  acc1: 16.6667 (18.1818)  acc5: 61.1111 (60.1010)  time: 0.7832  data: 0.6034  max mem: 15572
Val:  [ 20/272]  eta: 0:02:30  loss: 3.4949 (3.4368)  acc1: 16.6667 (22.4868)  acc5: 61.1111 (61.1111)  time: 0.4058  data: 0.2226  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 3.4503 (3.4586)  acc1: 22.2222 (22.0430)  acc5: 61.1111 (60.7527)  time: 0.3235  data: 0.1373  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 3.4595 (3.4564)  acc1: 22.2222 (22.0867)  acc5: 61.1111 (61.2466)  time: 0.2490  data: 0.0626  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 3.4461 (3.4188)  acc1: 22.2222 (23.7473)  acc5: 61.1111 (62.3094)  time: 0.2866  data: 0.0854  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 3.1560 (3.3776)  acc1: 27.7778 (24.5902)  acc5: 72.2222 (63.2058)  time: 0.3107  data: 0.1134  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.1833 (3.3608)  acc1: 33.3333 (26.7606)  acc5: 72.2222 (64.3975)  time: 0.2859  data: 0.1026  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.3427 (3.3680)  acc1: 33.3333 (26.6118)  acc5: 61.1111 (63.9232)  time: 0.2718  data: 0.0746  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.5009 (3.3901)  acc1: 22.2222 (25.7631)  acc5: 55.5556 (62.6984)  time: 0.2751  data: 0.0754  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.5553 (3.4067)  acc1: 22.2222 (25.0825)  acc5: 55.5556 (62.4312)  time: 0.3278  data: 0.1409  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6048 (3.4315)  acc1: 11.1111 (23.9740)  acc5: 55.5556 (61.2112)  time: 0.3451  data: 0.1552  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5953 (3.4328)  acc1: 16.6667 (24.9770)  acc5: 55.5556 (61.3866)  time: 0.3287  data: 0.1304  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2236 (3.4050)  acc1: 38.8889 (26.5055)  acc5: 72.2222 (62.3834)  time: 0.3132  data: 0.1126  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1867 (3.3965)  acc1: 38.8889 (27.1474)  acc5: 72.2222 (62.6478)  time: 0.2728  data: 0.0662  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.1867 (3.3791)  acc1: 38.8889 (27.9617)  acc5: 66.6667 (63.3554)  time: 0.2957  data: 0.0924  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.1917 (3.3750)  acc1: 27.7778 (27.8123)  acc5: 72.2222 (63.8371)  time: 0.3317  data: 0.1328  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3518 (3.3801)  acc1: 22.2222 (27.2904)  acc5: 66.6667 (63.4503)  time: 0.3190  data: 0.1257  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3518 (3.3732)  acc1: 22.2222 (27.1332)  acc5: 66.6667 (63.8122)  time: 0.3080  data: 0.1090  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2513 (3.3879)  acc1: 16.6667 (26.8179)  acc5: 55.5556 (62.8563)  time: 0.3102  data: 0.1004  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.3300 (3.3815)  acc1: 22.2222 (26.6722)  acc5: 55.5556 (62.9630)  time: 0.2961  data: 0.0906  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2218 (3.3845)  acc1: 22.2222 (26.7509)  acc5: 66.6667 (63.0858)  time: 0.2353  data: 0.0436  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.4418 (3.3905)  acc1: 22.2222 (26.6466)  acc5: 66.6667 (62.7702)  time: 0.2752  data: 0.0899  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2641 (3.3857)  acc1: 33.3333 (27.1044)  acc5: 66.6667 (63.2516)  time: 0.3422  data: 0.1429  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1076 (3.3707)  acc1: 33.3333 (27.3398)  acc5: 77.7778 (63.8543)  time: 0.3046  data: 0.1088  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1257 (3.3709)  acc1: 27.7778 (27.3351)  acc5: 77.7778 (63.9221)  time: 0.3110  data: 0.0991  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.0922 (3.3560)  acc1: 33.3333 (28.0758)  acc5: 77.7778 (64.7297)  time: 0.3032  data: 0.0945  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2526 (3.3584)  acc1: 33.3333 (28.1878)  acc5: 66.6667 (64.5961)  time: 0.2317  data: 0.0640  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2526 (3.3599)  acc1: 33.3333 (28.1999)  acc5: 77.7778 (64.6119)  time: 0.2237  data: 0.0639  max mem: 15572
Val: Total time: 0:01:26 (0.3181 s / it)
* Acc@1 28.200 Acc@5 64.612 loss 3.360
Accuracy of the network on the 4883 val videos: 28.2%
Max accuracy: 29.31%
Epoch: [27]  [  0/154]  eta: 0:21:42  lr: 0.000014  min_lr: 0.000000  loss: 4.5596 (4.5596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.4565  data: 8.0342  max mem: 15572
Epoch: [27]  [ 10/154]  eta: 0:03:08  lr: 0.000014  min_lr: 0.000000  loss: 4.7140 (4.7462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3106  data: 0.8757  max mem: 15572
Epoch: [27]  [ 20/154]  eta: 0:02:03  lr: 0.000014  min_lr: 0.000000  loss: 4.7140 (4.6944)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1006  max mem: 15572
[2025-01-13 02:46:50,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:46:50,486] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:46:51,338] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28984
[2025-01-13 02:46:51,339] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:46:51,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 30/154]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000000  loss: 4.7651 (4.7212)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0740  max mem: 15572
Epoch: [27]  [ 40/154]  eta: 0:01:21  lr: 0.000014  min_lr: 0.000000  loss: 4.7675 (4.7264)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0538  max mem: 15572
[2025-01-13 02:46:59,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=189, lr=[1.3283060407528312e-07, 1.3283060407528312e-07, 1.8975800582183307e-07, 1.8975800582183307e-07, 2.710828654597616e-07, 2.710828654597616e-07, 3.8726123637108796e-07, 3.8726123637108796e-07, 5.532303376729828e-07, 5.532303376729828e-07, 7.903290538185469e-07, 7.903290538185469e-07, 1.129041505455067e-06, 1.129041505455067e-06, 1.6129164363643817e-06, 1.6129164363643817e-06, 2.304166337663402e-06, 2.304166337663402e-06, 3.291666196662004e-06, 3.291666196662004e-06, 4.70238028094572e-06, 4.70238028094572e-06, 6.717686115636743e-06, 6.717686115636743e-06, 9.596694450909634e-06, 9.596694450909634e-06, 1.3709563501299477e-05, 1.3709563501299477e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 02:46:59,301] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=27.94899240889187, CurrSamplesPerSec=29.24249033513016, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 50/154]  eta: 0:01:11  lr: 0.000014  min_lr: 0.000000  loss: 4.7414 (4.7221)  loss_scale: 65536.0000 (68106.0392)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0493  max mem: 15572
Epoch: [27]  [ 60/154]  eta: 0:01:01  lr: 0.000014  min_lr: 0.000000  loss: 4.6373 (4.7094)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0613  max mem: 15572
Epoch: [27]  [ 70/154]  eta: 0:00:54  lr: 0.000013  min_lr: 0.000000  loss: 4.7621 (4.7235)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1206  max mem: 15572
Epoch: [27]  [ 80/154]  eta: 0:00:46  lr: 0.000013  min_lr: 0.000000  loss: 4.7844 (4.7128)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1484  max mem: 15572
Epoch: [27]  [ 90/154]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000000  loss: 4.6896 (4.7143)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1303  max mem: 15572
Epoch: [27]  [100/154]  eta: 0:00:34  lr: 0.000013  min_lr: 0.000000  loss: 4.6984 (4.7128)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1970  max mem: 15572
Epoch: [27]  [110/154]  eta: 0:00:27  lr: 0.000013  min_lr: 0.000000  loss: 4.7859 (4.7215)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1453  max mem: 15572
Epoch: [27]  [120/154]  eta: 0:00:21  lr: 0.000013  min_lr: 0.000000  loss: 4.6944 (4.7090)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0912  max mem: 15572
Epoch: [27]  [130/154]  eta: 0:00:14  lr: 0.000013  min_lr: 0.000000  loss: 4.7259 (4.7195)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1421  max mem: 15572
Epoch: [27]  [140/154]  eta: 0:00:08  lr: 0.000013  min_lr: 0.000000  loss: 4.7811 (4.7177)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1404  max mem: 15572
Epoch: [27]  [150/154]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000000  loss: 4.7226 (4.7083)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0562  max mem: 15572
Epoch: [27]  [153/154]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 4.6772 (4.7092)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)  time: 0.4718  data: 0.0561  max mem: 15572
Epoch: [27] Total time: 0:01:32 (0.5985 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 4.6772 (4.7092)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 158
Indices to remove: tensor([  263,   291,   320,   515,   560,   684,   747,  1586,  1887,  1902,
         2143,  2157,  2524,  2718,  2775,  2855,  2867,  3022,  3054,  3193,
         3265,  3336,  3402,  3457,  3463,  3505,  3960,  4106,  4154,  4172,
         4539,  4563,  4876,  6247,  6454,  6624,  6857,  6968,  7078,  7562,
         7847,  7966,  8067,  9392,  9969, 10014, 10160, 10174, 10193, 10219,
        10612, 10657, 10775, 11025, 11406, 11856, 12101, 12106, 12177, 12289,
        12614, 12616, 13108, 13166, 13370, 13802, 14496, 14862, 14975, 15026,
        15055, 15136, 15179, 15239, 15289, 15481, 15664, 15702, 16066, 16270,
        16675, 16690, 16941, 16956, 17584, 17929, 17934, 17954, 17964, 18119,
        18312, 18585, 18777, 19546, 19632, 19767, 19812, 19834, 20127, 20325,
        20407, 20632, 20864, 22239, 22487, 22598, 22879, 22883, 22911, 23242,
        23245, 23663, 23817, 23869, 24292, 24661, 24819, 25063, 25266, 25349,
        25454, 25552, 25561, 25888, 26016, 26273, 26458, 27180, 27359, 27408,
        27446, 27591, 28109, 28331, 28707, 29213, 29593, 29738, 29760, 29833,
        30083, 30307, 30327, 30613, 30775, 30873, 30888, 31120, 31234, 31295,
        31402, 31422, 31601, 32057, 32185, 33495, 33545, 33669],
       device='cuda:0')
length of data loader train is: 141
num_training_steps_per_epoch is: 141
Change step level LR scheduler!
Set warmup steps = 705
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:22:01  loss: 2.6761 (2.6761)  acc1: 27.7778 (27.7778)  acc5: 100.0000 (100.0000)  time: 4.8573  data: 4.6636  max mem: 15572
Val:  [ 10/272]  eta: 0:03:25  loss: 3.5054 (3.5306)  acc1: 16.6667 (19.1919)  acc5: 55.5556 (58.0808)  time: 0.7848  data: 0.5782  max mem: 15572
Val:  [ 20/272]  eta: 0:02:11  loss: 3.4881 (3.4625)  acc1: 16.6667 (24.0741)  acc5: 55.5556 (59.7884)  time: 0.3033  data: 0.1093  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 3.4881 (3.4812)  acc1: 22.2222 (22.9391)  acc5: 55.5556 (60.2151)  time: 0.2389  data: 0.0504  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 3.5186 (3.4693)  acc1: 22.2222 (22.6287)  acc5: 66.6667 (61.7886)  time: 0.2747  data: 0.0843  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 3.3748 (3.4266)  acc1: 22.2222 (23.8562)  acc5: 66.6667 (62.6362)  time: 0.3599  data: 0.1611  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 3.1253 (3.3781)  acc1: 27.7778 (24.2259)  acc5: 72.2222 (63.8434)  time: 0.3959  data: 0.1956  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 3.1253 (3.3578)  acc1: 33.3333 (26.6041)  acc5: 72.2222 (65.0235)  time: 0.3237  data: 0.1269  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.3237 (3.3659)  acc1: 27.7778 (26.3374)  acc5: 66.6667 (64.7462)  time: 0.2548  data: 0.0481  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.5450 (3.3910)  acc1: 16.6667 (25.0916)  acc5: 55.5556 (63.0037)  time: 0.2668  data: 0.0675  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.5812 (3.4108)  acc1: 11.1111 (24.0374)  acc5: 55.5556 (62.5413)  time: 0.3621  data: 0.1683  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6348 (3.4376)  acc1: 11.1111 (22.9730)  acc5: 50.0000 (61.1612)  time: 0.3354  data: 0.1385  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5902 (3.4399)  acc1: 16.6667 (24.0129)  acc5: 61.1111 (61.2948)  time: 0.2649  data: 0.0734  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2137 (3.4080)  acc1: 38.8889 (25.7846)  acc5: 72.2222 (62.4682)  time: 0.3417  data: 0.1464  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1413 (3.3960)  acc1: 38.8889 (26.5563)  acc5: 77.7778 (63.0812)  time: 0.3438  data: 0.1414  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.1599 (3.3788)  acc1: 33.3333 (27.2259)  acc5: 66.6667 (63.7233)  time: 0.2589  data: 0.0606  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2132 (3.3750)  acc1: 27.7778 (27.1912)  acc5: 72.2222 (64.1132)  time: 0.2502  data: 0.0612  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3764 (3.3802)  acc1: 22.2222 (26.7381)  acc5: 61.1111 (63.6777)  time: 0.3380  data: 0.1449  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3322 (3.3725)  acc1: 22.2222 (26.4886)  acc5: 61.1111 (64.2419)  time: 0.3432  data: 0.1438  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3225 (3.3898)  acc1: 16.6667 (26.1198)  acc5: 61.1111 (63.2344)  time: 0.2899  data: 0.0974  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4023 (3.3861)  acc1: 16.6667 (25.8706)  acc5: 55.5556 (63.2670)  time: 0.3021  data: 0.1040  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2734 (3.3927)  acc1: 16.6667 (25.8294)  acc5: 61.1111 (63.2438)  time: 0.3166  data: 0.1182  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5051 (3.3967)  acc1: 22.2222 (25.8170)  acc5: 61.1111 (62.8708)  time: 0.2936  data: 0.1050  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.3039 (3.3907)  acc1: 33.3333 (26.3829)  acc5: 72.2222 (63.3478)  time: 0.2916  data: 0.1035  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1277 (3.3775)  acc1: 33.3333 (26.6943)  acc5: 77.7778 (63.9235)  time: 0.3617  data: 0.1690  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1545 (3.3782)  acc1: 33.3333 (26.7375)  acc5: 72.2222 (63.8114)  time: 0.3770  data: 0.1827  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1863 (3.3650)  acc1: 33.3333 (27.4372)  acc5: 77.7778 (64.5381)  time: 0.2974  data: 0.1087  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2536 (3.3686)  acc1: 33.3333 (27.4498)  acc5: 72.2222 (64.2886)  time: 0.1946  data: 0.0294  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2536 (3.3700)  acc1: 33.3333 (27.4831)  acc5: 72.2222 (64.2843)  time: 0.1880  data: 0.0294  max mem: 15572
Val: Total time: 0:01:27 (0.3228 s / it)
* Acc@1 27.483 Acc@5 64.284 loss 3.370
Accuracy of the network on the 4883 val videos: 27.5%
Max accuracy: 29.31%
Epoch: [28]  [  0/141]  eta: 0:22:09  lr: 0.000012  min_lr: 0.000000  loss: 4.9730 (4.9730)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4321  data: 8.8357  max mem: 15572
[2025-01-13 02:49:39,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:49:39,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:49:39,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29114
[2025-01-13 02:49:39,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:49:39,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 10/141]  eta: 0:02:47  lr: 0.000012  min_lr: 0.000000  loss: 4.5488 (4.6213)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 1.2787  data: 0.8129  max mem: 15572
Epoch: [28]  [ 20/141]  eta: 0:01:59  lr: 0.000012  min_lr: 0.000000  loss: 4.4774 (4.5947)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1124  max mem: 15572
Epoch: [28]  [ 30/141]  eta: 0:01:34  lr: 0.000012  min_lr: 0.000000  loss: 4.6958 (4.6394)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1598  max mem: 15572
Epoch: [28]  [ 40/141]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 4.7814 (4.6503)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0620  max mem: 15572
Epoch: [28]  [ 50/141]  eta: 0:01:04  lr: 0.000012  min_lr: 0.000000  loss: 4.7839 (4.6736)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0616  max mem: 15572
Epoch: [28]  [ 60/141]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 4.7757 (4.6854)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1319  max mem: 15572
Epoch: [28]  [ 70/141]  eta: 0:00:48  lr: 0.000011  min_lr: 0.000000  loss: 4.6999 (4.6688)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1476  max mem: 15572
Epoch: [28]  [ 80/141]  eta: 0:00:40  lr: 0.000011  min_lr: 0.000000  loss: 4.5992 (4.6708)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1571  max mem: 15572
Epoch: [28]  [ 90/141]  eta: 0:00:33  lr: 0.000011  min_lr: 0.000000  loss: 4.7594 (4.6660)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1331  max mem: 15572
Epoch: [28]  [100/141]  eta: 0:00:26  lr: 0.000011  min_lr: 0.000000  loss: 4.7594 (4.6710)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.0943  max mem: 15572
Epoch: [28]  [110/141]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000000  loss: 4.7333 (4.6739)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0991  max mem: 15572
Epoch: [28]  [120/141]  eta: 0:00:13  lr: 0.000011  min_lr: 0.000000  loss: 4.7749 (4.6801)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0989  max mem: 15572
Epoch: [28]  [130/141]  eta: 0:00:06  lr: 0.000011  min_lr: 0.000000  loss: 4.7018 (4.6801)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1111  max mem: 15572
[2025-01-13 02:50:51,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:50:51,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:50:52,165] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29244
[2025-01-13 02:50:52,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:50:52,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [140/141]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 4.6475 (4.6801)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0723  max mem: 15572
Epoch: [28] Total time: 0:01:26 (0.6129 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 4.6475 (4.6801)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 145
Indices to remove: tensor([  232,   331,   346,   376,   430,   497,   517,   710,  1151,  1341,
         2120,  2175,  2207,  2304,  2403,  2850,  2895,  3009,  3428,  3500,
         4120,  4148,  4358,  4414,  4598,  4661,  4667,  4720,  4969,  4982,
         5007,  5210,  5635,  6870,  6893,  6913,  7003,  7907,  8004,  8524,
         8581,  9115,  9939, 10041, 10920, 10992, 11542, 11724, 12133, 12143,
        12152, 12347, 12407, 12428, 12474, 12568, 12603, 12868, 13182, 13432,
        13594, 13661, 13753, 13761, 14571, 14589, 14690, 15348, 15499, 15623,
        16391, 16402, 16488, 16691, 17212, 17515, 17849, 17850, 17916, 18054,
        18164, 18820, 18923, 19239, 19507, 19878, 20109, 20420, 20442, 20700,
        21040, 21749, 21810, 21820, 21836, 22125, 22640, 22665, 22672, 22768,
        22804, 22842, 22849, 23032, 23077, 23422, 23738, 24567, 24609, 24791,
        24907, 24922, 24928, 25147, 25228, 25326, 25438, 25720, 26172, 26223,
        26242, 26249, 26257, 26572, 26806, 26870, 27478, 27525, 27582, 27616,
        28167, 28433, 28536, 28585, 28850, 29196, 29525, 30147, 30287, 30345,
        30502, 31175, 31788, 32346, 33616], device='cuda:0')
length of data loader train is: 129
num_training_steps_per_epoch is: 129
Change step level LR scheduler!
Set warmup steps = 645
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:20:51  loss: 2.6788 (2.6788)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 4.6029  data: 4.4257  max mem: 15572
Val:  [ 10/272]  eta: 0:03:17  loss: 3.4953 (3.4843)  acc1: 16.6667 (19.6970)  acc5: 55.5556 (59.5960)  time: 0.7557  data: 0.5646  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 3.4746 (3.4323)  acc1: 16.6667 (24.6032)  acc5: 55.5556 (61.9048)  time: 0.3074  data: 0.1091  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.4746 (3.4701)  acc1: 16.6667 (24.0143)  acc5: 61.1111 (60.9319)  time: 0.2593  data: 0.0553  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.5116 (3.4685)  acc1: 22.2222 (23.7127)  acc5: 61.1111 (61.3821)  time: 0.2866  data: 0.0880  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 3.4553 (3.4200)  acc1: 27.7778 (24.8366)  acc5: 61.1111 (62.3094)  time: 0.3515  data: 0.1616  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 3.0777 (3.3747)  acc1: 27.7778 (25.5009)  acc5: 72.2222 (63.2969)  time: 0.3775  data: 0.1805  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 3.1590 (3.3510)  acc1: 33.3333 (27.9343)  acc5: 72.2222 (64.7105)  time: 0.3120  data: 0.1104  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.2854 (3.3524)  acc1: 27.7778 (27.8464)  acc5: 66.6667 (64.7462)  time: 0.2458  data: 0.0430  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.5498 (3.3780)  acc1: 22.2222 (26.6178)  acc5: 61.1111 (63.4921)  time: 0.2757  data: 0.0733  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5615 (3.4016)  acc1: 16.6667 (25.4675)  acc5: 55.5556 (62.7613)  time: 0.2949  data: 0.0900  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6531 (3.4252)  acc1: 11.1111 (24.4244)  acc5: 50.0000 (61.7618)  time: 0.3234  data: 0.1053  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5669 (3.4322)  acc1: 16.6667 (25.2525)  acc5: 61.1111 (61.8916)  time: 0.3323  data: 0.1005  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.2589 (3.4022)  acc1: 38.8889 (26.8872)  acc5: 72.2222 (62.9347)  time: 0.2886  data: 0.0693  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1651 (3.3908)  acc1: 38.8889 (27.6990)  acc5: 77.7778 (63.1600)  time: 0.2963  data: 0.1091  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.1651 (3.3744)  acc1: 33.3333 (28.4400)  acc5: 66.6667 (63.7969)  time: 0.3057  data: 0.1132  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2219 (3.3709)  acc1: 27.7778 (28.1919)  acc5: 72.2222 (64.1822)  time: 0.3091  data: 0.0956  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3677 (3.3764)  acc1: 22.2222 (28.0052)  acc5: 66.6667 (63.9051)  time: 0.2924  data: 0.0824  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3648 (3.3696)  acc1: 22.2222 (27.6857)  acc5: 66.6667 (64.3033)  time: 0.3116  data: 0.1124  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.3544 (3.3883)  acc1: 16.6667 (27.2833)  acc5: 55.5556 (63.2344)  time: 0.3055  data: 0.1143  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.3851 (3.3863)  acc1: 16.6667 (26.9486)  acc5: 55.5556 (63.2946)  time: 0.3181  data: 0.1342  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2518 (3.3918)  acc1: 22.2222 (26.9879)  acc5: 61.1111 (63.1122)  time: 0.3265  data: 0.1259  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5068 (3.3951)  acc1: 27.7778 (27.0236)  acc5: 61.1111 (62.8708)  time: 0.3109  data: 0.1004  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2115 (3.3865)  acc1: 38.8889 (27.6575)  acc5: 66.6667 (63.4440)  time: 0.3838  data: 0.1820  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0934 (3.3746)  acc1: 33.3333 (27.8239)  acc5: 77.7778 (63.9004)  time: 0.3467  data: 0.1508  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.1408 (3.3753)  acc1: 27.7778 (27.7556)  acc5: 66.6667 (63.7672)  time: 0.2710  data: 0.0749  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1684 (3.3627)  acc1: 38.8889 (28.5653)  acc5: 77.7778 (64.5807)  time: 0.2831  data: 0.0831  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3367 (3.3649)  acc1: 33.3333 (28.5773)  acc5: 77.7778 (64.4321)  time: 0.2134  data: 0.0397  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3367 (3.3661)  acc1: 33.3333 (28.6299)  acc5: 77.7778 (64.4481)  time: 0.2043  data: 0.0396  max mem: 15572
Val: Total time: 0:01:26 (0.3195 s / it)
* Acc@1 28.630 Acc@5 64.448 loss 3.366
Accuracy of the network on the 4883 val videos: 28.6%
Max accuracy: 29.31%
Epoch: [29]  [  0/129]  eta: 0:14:28  lr: 0.000011  min_lr: 0.000000  loss: 4.8712 (4.8712)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7336  data: 6.2828  max mem: 15572
Epoch: [29]  [ 10/129]  eta: 0:02:22  lr: 0.000010  min_lr: 0.000000  loss: 4.7390 (4.7009)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1938  data: 0.7378  max mem: 15572
Epoch: [29]  [ 20/129]  eta: 0:01:36  lr: 0.000010  min_lr: 0.000000  loss: 4.7015 (4.7194)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1361  max mem: 15572
Epoch: [29]  [ 30/129]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 4.6882 (4.7381)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1069  max mem: 15572
Epoch: [29]  [ 40/129]  eta: 0:01:06  lr: 0.000010  min_lr: 0.000000  loss: 4.7323 (4.7513)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1683  max mem: 15572
Epoch: [29]  [ 50/129]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 4.7323 (4.7405)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1554  max mem: 15572
Epoch: [29]  [ 60/129]  eta: 0:00:47  lr: 0.000010  min_lr: 0.000000  loss: 4.6443 (4.7116)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1061  max mem: 15572
Epoch: [29]  [ 70/129]  eta: 0:00:39  lr: 0.000010  min_lr: 0.000000  loss: 4.6443 (4.7206)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1067  max mem: 15572
Epoch: [29]  [ 80/129]  eta: 0:00:32  lr: 0.000009  min_lr: 0.000000  loss: 4.6705 (4.7192)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1290  max mem: 15572
Epoch: [29]  [ 90/129]  eta: 0:00:25  lr: 0.000009  min_lr: 0.000000  loss: 4.6705 (4.7243)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1313  max mem: 15572
Epoch: [29]  [100/129]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 4.7376 (4.7280)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1166  max mem: 15572
Epoch: [29]  [110/129]  eta: 0:00:12  lr: 0.000009  min_lr: 0.000000  loss: 4.6777 (4.7164)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1530  max mem: 15572
Epoch: [29]  [120/129]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.6256 (4.7180)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1576  max mem: 15572
[2025-01-13 02:53:38,635] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:53:38,635] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:53:39,019] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29374
[2025-01-13 02:53:39,019] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:53:39,019] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [128/129]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 4.7346 (4.7168)  loss_scale: 65536.0000 (66044.0310)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0690  max mem: 15572
Epoch: [29] Total time: 0:01:19 (0.6162 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 4.7346 (4.7168)  loss_scale: 65536.0000 (66044.0310)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 127
Indices to remove: tensor([  419,   534,   557,   839,  1500,  1528,  1560,  1823,  1960,  2024,
         2117,  2150,  2480,  2703,  2959,  3030,  4350,  4380,  4694,  4757,
         4990,  5248,  5586,  5926,  6060,  6967,  6988,  7882,  8032,  8147,
         8413,  8553,  8584,  9546, 10077, 10186, 10265, 10574, 10807, 10948,
        10950, 10970, 11086, 11107, 11424, 11516, 12097, 12183, 12247, 12521,
        12573, 12826, 12864, 14547, 14631, 14637, 14739, 15661, 15848, 16427,
        16462, 17002, 17038, 17260, 17578, 17888, 18029, 18231, 18274, 18879,
        19052, 19068, 19596, 19640, 19952, 20565, 20822, 21472, 21563, 21793,
        21831, 21949, 22043, 22306, 22370, 22596, 22773, 22779, 22938, 24961,
        25038, 25167, 25294, 25413, 25435, 25508, 25554, 25598, 25705, 26165,
        26279, 26321, 26328, 26335, 26606, 26716, 26996, 27027, 27438, 28031,
        28052, 28263, 28264, 28322, 28611, 28883, 28936, 30022, 30168, 30253,
        30401, 30406, 30465, 30531, 30673, 31324, 32224], device='cuda:0')
length of data loader train is: 119
num_training_steps_per_epoch is: 119
Change step level LR scheduler!
Set warmup steps = 595
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 02:53:40,852] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-13 02:53:40,856] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-13 02:53:40,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-13 02:53:41,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-13 02:53:41,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/272]  eta: 0:17:16  loss: 2.7891 (2.7891)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 3.8104  data: 3.6035  max mem: 15572
Val:  [ 10/272]  eta: 0:03:01  loss: 3.5011 (3.5334)  acc1: 16.6667 (18.6869)  acc5: 61.1111 (61.1111)  time: 0.6928  data: 0.4895  max mem: 15572
Val:  [ 20/272]  eta: 0:02:11  loss: 3.4789 (3.4652)  acc1: 22.2222 (24.8677)  acc5: 61.1111 (62.9630)  time: 0.3592  data: 0.1616  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 3.4404 (3.4946)  acc1: 22.2222 (23.6559)  acc5: 61.1111 (60.9319)  time: 0.3123  data: 0.1189  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 3.5673 (3.4916)  acc1: 16.6667 (22.7642)  acc5: 55.5556 (60.7046)  time: 0.3027  data: 0.1032  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 3.4391 (3.4343)  acc1: 22.2222 (24.0741)  acc5: 61.1111 (61.5468)  time: 0.2959  data: 0.1004  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 3.0691 (3.3838)  acc1: 27.7778 (24.8634)  acc5: 72.2222 (62.8415)  time: 0.2548  data: 0.0700  max mem: 15572
Val:  [ 70/272]  eta: 0:01:11  loss: 3.1209 (3.3536)  acc1: 33.3333 (27.8560)  acc5: 77.7778 (64.7105)  time: 0.2748  data: 0.0906  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.2267 (3.3622)  acc1: 27.7778 (27.3663)  acc5: 66.6667 (64.4719)  time: 0.3422  data: 0.1563  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.5546 (3.3898)  acc1: 16.6667 (26.1294)  acc5: 55.5556 (63.0647)  time: 0.3352  data: 0.1488  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 3.5951 (3.4117)  acc1: 11.1111 (25.0825)  acc5: 55.5556 (62.5413)  time: 0.2858  data: 0.1011  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6192 (3.4402)  acc1: 11.1111 (24.0240)  acc5: 50.0000 (61.3113)  time: 0.3302  data: 0.1420  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5977 (3.4456)  acc1: 22.2222 (25.0230)  acc5: 55.5556 (61.3407)  time: 0.3364  data: 0.1372  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.3292 (3.4184)  acc1: 38.8889 (26.6327)  acc5: 66.6667 (62.2561)  time: 0.2971  data: 0.1033  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.1458 (3.4035)  acc1: 38.8889 (27.5020)  acc5: 72.2222 (62.7266)  time: 0.3181  data: 0.1366  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.1685 (3.3855)  acc1: 38.8889 (27.8882)  acc5: 72.2222 (63.3186)  time: 0.3012  data: 0.1173  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.1685 (3.3801)  acc1: 27.7778 (27.9503)  acc5: 72.2222 (63.6991)  time: 0.2955  data: 0.1084  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3193 (3.3827)  acc1: 27.7778 (27.6153)  acc5: 66.6667 (63.4503)  time: 0.3021  data: 0.1218  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3760 (3.3768)  acc1: 22.2222 (27.3481)  acc5: 72.2222 (63.8428)  time: 0.3064  data: 0.1214  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3780 (3.3973)  acc1: 16.6667 (26.9343)  acc5: 55.5556 (62.7109)  time: 0.3186  data: 0.1237  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4208 (3.3970)  acc1: 16.6667 (26.5064)  acc5: 55.5556 (62.5207)  time: 0.3210  data: 0.1259  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2974 (3.4035)  acc1: 16.6667 (26.4613)  acc5: 61.1111 (62.4013)  time: 0.3089  data: 0.1090  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5534 (3.4057)  acc1: 22.2222 (26.4957)  acc5: 61.1111 (62.1669)  time: 0.3064  data: 0.1107  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2684 (3.3999)  acc1: 38.8889 (26.9360)  acc5: 72.2222 (62.7225)  time: 0.3387  data: 0.1422  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2165 (3.3891)  acc1: 27.7778 (26.9710)  acc5: 77.7778 (63.1858)  time: 0.3220  data: 0.1186  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2326 (3.3906)  acc1: 22.2222 (26.9588)  acc5: 72.2222 (63.1253)  time: 0.3211  data: 0.1206  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1935 (3.3791)  acc1: 38.8889 (27.5011)  acc5: 72.2222 (63.8357)  time: 0.3187  data: 0.1236  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3482 (3.3827)  acc1: 33.3333 (27.5933)  acc5: 66.6667 (63.5916)  time: 0.2268  data: 0.0580  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3482 (3.3840)  acc1: 33.3333 (27.6469)  acc5: 72.2222 (63.6084)  time: 0.2179  data: 0.0580  max mem: 15572
Val: Total time: 0:01:27 (0.3214 s / it)
* Acc@1 27.647 Acc@5 63.608 loss 3.384
Accuracy of the network on the 4883 val videos: 27.6%
Max accuracy: 29.31%
Epoch: [30]  [  0/119]  eta: 0:17:19  lr: 0.000009  min_lr: 0.000000  loss: 4.3383 (4.3383)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7370  data: 8.3052  max mem: 15572
Epoch: [30]  [ 10/119]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000000  loss: 4.5163 (4.5737)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1896  data: 0.7555  max mem: 15572
Epoch: [30]  [ 20/119]  eta: 0:01:26  lr: 0.000009  min_lr: 0.000000  loss: 4.6156 (4.6433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4857  data: 0.0480  max mem: 15572
Epoch: [30]  [ 30/119]  eta: 0:01:08  lr: 0.000008  min_lr: 0.000000  loss: 4.6156 (4.6332)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1086  max mem: 15572
Epoch: [30]  [ 40/119]  eta: 0:00:57  lr: 0.000008  min_lr: 0.000000  loss: 4.6609 (4.6610)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1199  max mem: 15572
Epoch: [30]  [ 50/119]  eta: 0:00:49  lr: 0.000008  min_lr: 0.000000  loss: 4.7205 (4.6768)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1909  max mem: 15572
Epoch: [30]  [ 60/119]  eta: 0:00:40  lr: 0.000008  min_lr: 0.000000  loss: 4.7235 (4.6919)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1844  max mem: 15572
Epoch: [30]  [ 70/119]  eta: 0:00:33  lr: 0.000008  min_lr: 0.000000  loss: 4.6815 (4.6721)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1357  max mem: 15572
Epoch: [30]  [ 80/119]  eta: 0:00:25  lr: 0.000008  min_lr: 0.000000  loss: 4.6667 (4.6789)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1207  max mem: 15572
Epoch: [30]  [ 90/119]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 4.7415 (4.6758)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0703  max mem: 15572
Epoch: [30]  [100/119]  eta: 0:00:12  lr: 0.000008  min_lr: 0.000000  loss: 4.7415 (4.6889)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1050  max mem: 15572
Epoch: [30]  [110/119]  eta: 0:00:05  lr: 0.000007  min_lr: 0.000000  loss: 4.7518 (4.6908)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1212  max mem: 15572
Epoch: [30]  [118/119]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 4.7342 (4.6938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4848  data: 0.0662  max mem: 15572
Epoch: [30] Total time: 0:01:13 (0.6160 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 4.7342 (4.6938)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 129
Indices to remove: tensor([  435,  1431,  1849,  1992,  2132,  2284,  2562,  2750,  4416,  4474,
         4561,  4582,  4587,  5004,  5054,  5147,  5332,  5555,  6058,  6398,
         6434,  6608,  6865,  6872,  7002,  7090,  7196,  7601,  8090,  8239,
         8289,  8304,  8475,  8516,  9113,  9976, 10024, 10416, 11712, 11727,
        11892, 11896, 12108, 12116, 12204, 12360, 12606, 12810, 12858, 13648,
        13729, 13860, 14035, 14970, 15093, 15465, 15565, 15639, 15885, 15967,
        16497, 16585, 16648, 16651, 16793, 17203, 17843, 19354, 19425, 19502,
        19520, 19736, 19963, 20002, 20192, 20231, 21066, 21373, 21493, 21558,
        22229, 22347, 22730, 22956, 22961, 22996, 23356, 23585, 23703, 23728,
        24615, 24881, 24920, 24936, 25259, 25466, 25517, 25706, 26471, 26500,
        27078, 27333, 27403, 27457, 28035, 28265, 28425, 28897, 29883, 29901,
        29955, 30069, 30312, 30328, 30391, 30913, 30996, 31005, 31192, 31216,
        31252, 31350, 31394, 31514, 31711, 32791, 33205, 33377, 33399],
       device='cuda:0')
length of data loader train is: 108
num_training_steps_per_epoch is: 108
Change step level LR scheduler!
Set warmup steps = 540
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:21  loss: 2.8151 (2.8151)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 4.2687  data: 4.0854  max mem: 15572
Val:  [ 10/272]  eta: 0:03:29  loss: 3.4923 (3.5144)  acc1: 16.6667 (19.1919)  acc5: 61.1111 (59.5960)  time: 0.8006  data: 0.5854  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 3.4134 (3.4396)  acc1: 22.2222 (26.4550)  acc5: 61.1111 (62.4339)  time: 0.3552  data: 0.1458  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 3.4174 (3.4804)  acc1: 22.2222 (24.1935)  acc5: 61.1111 (60.7527)  time: 0.2385  data: 0.0286  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.5289 (3.4710)  acc1: 22.2222 (23.0352)  acc5: 61.1111 (60.8401)  time: 0.2616  data: 0.0385  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.4097 (3.4219)  acc1: 22.2222 (24.2919)  acc5: 61.1111 (62.2004)  time: 0.3068  data: 0.0985  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 3.0300 (3.3773)  acc1: 27.7778 (25.0455)  acc5: 72.2222 (63.3880)  time: 0.3137  data: 0.1259  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 3.0902 (3.3539)  acc1: 33.3333 (27.6995)  acc5: 77.7778 (65.1800)  time: 0.3139  data: 0.1251  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.2758 (3.3566)  acc1: 27.7778 (27.7092)  acc5: 72.2222 (64.8148)  time: 0.3333  data: 0.1338  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.5227 (3.3827)  acc1: 22.2222 (26.9841)  acc5: 50.0000 (63.4310)  time: 0.3284  data: 0.1303  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.6060 (3.4060)  acc1: 16.6667 (25.6326)  acc5: 55.5556 (63.1463)  time: 0.3297  data: 0.1303  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6454 (3.4292)  acc1: 11.1111 (24.8749)  acc5: 55.5556 (62.1622)  time: 0.3339  data: 0.1298  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.5461 (3.4326)  acc1: 22.2222 (25.6657)  acc5: 55.5556 (62.3508)  time: 0.2888  data: 0.0965  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.2413 (3.4059)  acc1: 38.8889 (27.3537)  acc5: 72.2222 (63.2740)  time: 0.2888  data: 0.0819  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.2066 (3.3961)  acc1: 38.8889 (27.9354)  acc5: 72.2222 (63.5540)  time: 0.3058  data: 0.0900  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2066 (3.3795)  acc1: 38.8889 (28.4768)  acc5: 66.6667 (64.2016)  time: 0.2878  data: 0.0858  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.1678 (3.3741)  acc1: 22.2222 (28.3989)  acc5: 77.7778 (64.6653)  time: 0.2857  data: 0.0887  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 3.3558 (3.3779)  acc1: 22.2222 (28.2326)  acc5: 72.2222 (64.3600)  time: 0.2909  data: 0.1107  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3581 (3.3731)  acc1: 22.2222 (27.8699)  acc5: 66.6667 (64.5181)  time: 0.3290  data: 0.1512  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3581 (3.3931)  acc1: 16.6667 (27.4287)  acc5: 55.5556 (63.4380)  time: 0.3381  data: 0.1475  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4505 (3.3917)  acc1: 16.6667 (26.9762)  acc5: 55.5556 (63.2670)  time: 0.2878  data: 0.1036  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2830 (3.3980)  acc1: 22.2222 (26.9616)  acc5: 61.1111 (63.1385)  time: 0.3121  data: 0.1221  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5169 (3.3997)  acc1: 22.2222 (27.0488)  acc5: 61.1111 (62.9462)  time: 0.3355  data: 0.1414  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2038 (3.3924)  acc1: 44.4444 (27.7537)  acc5: 72.2222 (63.3959)  time: 0.3134  data: 0.1245  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1726 (3.3809)  acc1: 27.7778 (27.6395)  acc5: 77.7778 (63.8774)  time: 0.3080  data: 0.1136  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2043 (3.3828)  acc1: 22.2222 (27.5564)  acc5: 77.7778 (63.8114)  time: 0.3039  data: 0.1146  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1870 (3.3697)  acc1: 38.8889 (28.1822)  acc5: 77.7778 (64.5807)  time: 0.2981  data: 0.1116  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3061 (3.3730)  acc1: 33.3333 (28.2903)  acc5: 66.6667 (64.4116)  time: 0.2397  data: 0.0678  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3061 (3.3748)  acc1: 33.3333 (28.3023)  acc5: 77.7778 (64.4276)  time: 0.2331  data: 0.0677  max mem: 15572
Val: Total time: 0:01:27 (0.3205 s / it)
* Acc@1 28.302 Acc@5 64.428 loss 3.375
Accuracy of the network on the 4883 val videos: 28.3%
Max accuracy: 29.31%
Epoch: [31]  [  0/108]  eta: 0:13:45  lr: 0.000007  min_lr: 0.000000  loss: 4.8386 (4.8386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.6434  data: 7.1528  max mem: 15572
[2025-01-13 02:57:58,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 02:57:58,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 02:57:59,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29505
[2025-01-13 02:57:59,769] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 02:57:59,769] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 10/108]  eta: 0:01:50  lr: 0.000007  min_lr: 0.000000  loss: 4.7928 (4.7412)  loss_scale: 65536.0000 (77451.6364)  weight_decay: 0.0500 (0.0500)  time: 1.1243  data: 0.7002  max mem: 15572
Epoch: [31]  [ 20/108]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000000  loss: 4.6771 (4.7044)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0826  max mem: 15572
Epoch: [31]  [ 30/108]  eta: 0:00:58  lr: 0.000007  min_lr: 0.000000  loss: 4.6680 (4.6916)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1199  max mem: 15572
Epoch: [31]  [ 40/108]  eta: 0:00:48  lr: 0.000007  min_lr: 0.000000  loss: 4.6936 (4.7020)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1511  max mem: 15572
Epoch: [31]  [ 50/108]  eta: 0:00:40  lr: 0.000007  min_lr: 0.000000  loss: 4.6695 (4.6963)  loss_scale: 65536.0000 (68106.0392)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1594  max mem: 15572
Epoch: [31]  [ 60/108]  eta: 0:00:31  lr: 0.000006  min_lr: 0.000000  loss: 4.7269 (4.6939)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0961  max mem: 15572
Epoch: [31]  [ 70/108]  eta: 0:00:24  lr: 0.000006  min_lr: 0.000000  loss: 4.7475 (4.6893)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1080  max mem: 15572
Epoch: [31]  [ 80/108]  eta: 0:00:17  lr: 0.000006  min_lr: 0.000000  loss: 4.6891 (4.6899)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1381  max mem: 15572
Epoch: [31]  [ 90/108]  eta: 0:00:11  lr: 0.000006  min_lr: 0.000000  loss: 4.6215 (4.6856)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1565  max mem: 15572
Epoch: [31]  [100/108]  eta: 0:00:05  lr: 0.000006  min_lr: 0.000000  loss: 4.5566 (4.6765)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1656  max mem: 15572
Epoch: [31]  [107/108]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 4.5949 (4.6741)  loss_scale: 65536.0000 (66749.6296)  weight_decay: 0.0500 (0.0500)  time: 0.4634  data: 0.0625  max mem: 15572
Epoch: [31] Total time: 0:01:06 (0.6155 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 4.5949 (4.6741)  loss_scale: 65536.0000 (66749.6296)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 105
Indices to remove: tensor([  339,   541,   591,   727,   833,  1198,  1422,  1614,  2205,  2263,
         2593,  3446,  4451,  4475,  4493,  4676,  4721,  5079,  5724,  6331,
         7045,  7186,  7229,  7632,  7739,  7767,  7965,  8066,  8220,  9068,
         9170,  9617, 10464, 10601, 10936, 10942, 11374, 11779, 12075, 12174,
        12296, 12804, 13258, 13329, 13490, 14074, 14744, 14834, 14919, 15249,
        15670, 16185, 17313, 17943, 18065, 19069, 19201, 19848, 20235, 20289,
        21266, 21770, 21819, 21835, 22653, 22724, 22755, 22823, 22981, 23015,
        23082, 23907, 23916, 24566, 24598, 25251, 25699, 25724, 26021, 26193,
        26454, 26654, 27304, 27311, 27348, 27390, 27515, 27702, 27775, 27942,
        28635, 28830, 28833, 29267, 29694, 30540, 30585, 30647, 30790, 31058,
        31082, 31406, 32875, 33204, 33535], device='cuda:0')
length of data loader train is: 99
num_training_steps_per_epoch is: 99
Change step level LR scheduler!
Set warmup steps = 495
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:17:11  loss: 2.8156 (2.8156)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 3.7904  data: 3.5790  max mem: 15572
Val:  [ 10/272]  eta: 0:03:09  loss: 3.4579 (3.4828)  acc1: 16.6667 (18.6869)  acc5: 66.6667 (64.6465)  time: 0.7219  data: 0.5267  max mem: 15572
Val:  [ 20/272]  eta: 0:02:09  loss: 3.4097 (3.4168)  acc1: 22.2222 (26.1905)  acc5: 66.6667 (65.0794)  time: 0.3506  data: 0.1508  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 3.3735 (3.4517)  acc1: 27.7778 (25.6272)  acc5: 61.1111 (62.9032)  time: 0.2951  data: 0.0897  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 3.5576 (3.4512)  acc1: 22.2222 (24.9322)  acc5: 61.1111 (63.0081)  time: 0.3034  data: 0.1051  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 3.4490 (3.4100)  acc1: 22.2222 (25.5991)  acc5: 61.1111 (63.9434)  time: 0.2819  data: 0.0911  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 3.1364 (3.3699)  acc1: 22.2222 (26.2295)  acc5: 72.2222 (64.6630)  time: 0.2871  data: 0.0943  max mem: 15572
Val:  [ 70/272]  eta: 0:01:11  loss: 3.1364 (3.3460)  acc1: 33.3333 (28.8732)  acc5: 77.7778 (66.1972)  time: 0.2894  data: 0.1038  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 3.2504 (3.3484)  acc1: 27.7778 (28.3951)  acc5: 72.2222 (65.8436)  time: 0.2852  data: 0.1057  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 3.4922 (3.3722)  acc1: 16.6667 (27.2894)  acc5: 55.5556 (64.4689)  time: 0.3165  data: 0.1301  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 3.5917 (3.3954)  acc1: 11.1111 (26.1276)  acc5: 55.5556 (63.8064)  time: 0.3088  data: 0.1187  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 3.6277 (3.4199)  acc1: 11.1111 (25.1251)  acc5: 55.5556 (62.5125)  time: 0.3249  data: 0.1314  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.5587 (3.4256)  acc1: 16.6667 (26.0331)  acc5: 55.5556 (62.5344)  time: 0.3168  data: 0.1320  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.2515 (3.4010)  acc1: 38.8889 (27.5233)  acc5: 72.2222 (63.4012)  time: 0.3171  data: 0.1308  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.2250 (3.3924)  acc1: 38.8889 (27.9748)  acc5: 72.2222 (63.7116)  time: 0.3464  data: 0.1542  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2453 (3.3785)  acc1: 33.3333 (28.4400)  acc5: 66.6667 (64.0177)  time: 0.3192  data: 0.1290  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.1655 (3.3739)  acc1: 27.7778 (28.2264)  acc5: 66.6667 (64.3547)  time: 0.3240  data: 0.1162  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3077 (3.3765)  acc1: 27.7778 (28.0702)  acc5: 66.6667 (64.1001)  time: 0.3380  data: 0.1280  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3077 (3.3715)  acc1: 22.2222 (27.7778)  acc5: 66.6667 (64.4260)  time: 0.3088  data: 0.0971  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.3408 (3.3914)  acc1: 16.6667 (27.3706)  acc5: 61.1111 (63.3799)  time: 0.2562  data: 0.0452  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.3980 (3.3906)  acc1: 11.1111 (26.8380)  acc5: 55.5556 (63.2117)  time: 0.2849  data: 0.0747  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.3218 (3.3945)  acc1: 22.2222 (26.9089)  acc5: 61.1111 (63.2175)  time: 0.3322  data: 0.1149  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5048 (3.3968)  acc1: 27.7778 (26.8225)  acc5: 61.1111 (62.9462)  time: 0.3149  data: 0.1104  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2418 (3.3890)  acc1: 38.8889 (27.5373)  acc5: 66.6667 (63.4921)  time: 0.3075  data: 0.1174  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1186 (3.3779)  acc1: 27.7778 (27.5012)  acc5: 77.7778 (63.9696)  time: 0.3074  data: 0.1152  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2462 (3.3807)  acc1: 22.2222 (27.3130)  acc5: 77.7778 (63.9000)  time: 0.2791  data: 0.0838  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.2057 (3.3689)  acc1: 33.3333 (27.9268)  acc5: 77.7778 (64.6232)  time: 0.2799  data: 0.0914  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2911 (3.3708)  acc1: 33.3333 (27.8188)  acc5: 72.2222 (64.4526)  time: 0.2652  data: 0.0938  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2911 (3.3722)  acc1: 33.3333 (27.8108)  acc5: 72.2222 (64.4481)  time: 0.2584  data: 0.0937  max mem: 15572
Val: Total time: 0:01:26 (0.3194 s / it)
* Acc@1 27.811 Acc@5 64.448 loss 3.372
Accuracy of the network on the 4883 val videos: 27.8%
Max accuracy: 29.31%
Epoch: [32]  [ 0/99]  eta: 0:10:47  lr: 0.000006  min_lr: 0.000000  loss: 4.7934 (4.7934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.5382  data: 6.0303  max mem: 15572
Epoch: [32]  [10/99]  eta: 0:01:45  lr: 0.000006  min_lr: 0.000000  loss: 4.7934 (4.7394)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1822  data: 0.7301  max mem: 15572
Epoch: [32]  [20/99]  eta: 0:01:07  lr: 0.000006  min_lr: 0.000000  loss: 4.6178 (4.6654)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1324  max mem: 15572
[2025-01-13 03:00:46,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:00:46,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:00:46,495] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29635
[2025-01-13 03:00:46,495] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:00:46,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [30/99]  eta: 0:00:54  lr: 0.000005  min_lr: 0.000000  loss: 4.6674 (4.7056)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1149  max mem: 15572
Epoch: [32]  [40/99]  eta: 0:00:43  lr: 0.000005  min_lr: 0.000000  loss: 4.7195 (4.7135)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1627  max mem: 15572
Epoch: [32]  [50/99]  eta: 0:00:35  lr: 0.000005  min_lr: 0.000000  loss: 4.6816 (4.6970)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1608  max mem: 15572
Epoch: [32]  [60/99]  eta: 0:00:26  lr: 0.000005  min_lr: 0.000000  loss: 4.6583 (4.7188)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1011  max mem: 15572
Epoch: [32]  [70/99]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000000  loss: 4.7777 (4.7185)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0719  max mem: 15572
Epoch: [32]  [80/99]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 4.7735 (4.7307)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1344  max mem: 15572
Epoch: [32]  [90/99]  eta: 0:00:05  lr: 0.000005  min_lr: 0.000000  loss: 4.7735 (4.7308)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1317  max mem: 15572
Epoch: [32]  [98/99]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 4.7391 (4.7287)  loss_scale: 65536.0000 (66197.9798)  weight_decay: 0.0500 (0.0500)  time: 0.4760  data: 0.0689  max mem: 15572
Epoch: [32] Total time: 0:01:02 (0.6313 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 4.7391 (4.7287)  loss_scale: 65536.0000 (66197.9798)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 92
Indices to remove: tensor([  299,   399,   428,   456,   501,   919,  1174,  1410,  1606,  1941,
         2029,  2242,  2342,  2417,  2460,  2666,  2727,  2825,  2994,  3224,
         3946,  4655,  4723,  5008,  5681,  6477,  6860,  7174,  7871,  8676,
        10305, 10451, 10940, 11005, 11677, 11694, 12138, 12265, 12267, 12674,
        12871, 12944, 13024, 13190, 13296, 15148, 15404, 15629, 16556, 16673,
        16676, 16803, 17598, 17908, 17930, 17975, 18053, 19384, 19948, 21809,
        21845, 21859, 21860, 22046, 22177, 22211, 22263, 22608, 22657, 23900,
        24478, 24828, 24838, 24840, 24863, 25163, 25594, 25717, 26326, 27370,
        27449, 27550, 27592, 28275, 29792, 30526, 30536, 30686, 31002, 31383,
        31589, 31763], device='cuda:0')
length of data loader train is: 91
num_training_steps_per_epoch is: 91
Change step level LR scheduler!
Set warmup steps = 455
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:24:34  loss: 2.8505 (2.8505)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 5.4215  data: 5.2437  max mem: 15572
Val:  [ 10/272]  eta: 0:03:35  loss: 3.4460 (3.4752)  acc1: 22.2222 (23.2323)  acc5: 66.6667 (64.1414)  time: 0.8208  data: 0.6345  max mem: 15572
Val:  [ 20/272]  eta: 0:02:24  loss: 3.3882 (3.4083)  acc1: 27.7778 (29.1005)  acc5: 61.1111 (65.3439)  time: 0.3311  data: 0.1437  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 3.3830 (3.4501)  acc1: 27.7778 (27.4194)  acc5: 61.1111 (62.5448)  time: 0.2599  data: 0.0573  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 3.5537 (3.4522)  acc1: 22.2222 (26.2873)  acc5: 61.1111 (62.4661)  time: 0.2533  data: 0.0365  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 3.4656 (3.4095)  acc1: 22.2222 (27.1242)  acc5: 66.6667 (63.3987)  time: 0.3043  data: 0.0935  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 3.0642 (3.3738)  acc1: 22.2222 (27.1403)  acc5: 72.2222 (64.1166)  time: 0.3312  data: 0.1186  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.1431 (3.3514)  acc1: 33.3333 (29.4210)  acc5: 72.2222 (65.7277)  time: 0.3188  data: 0.1119  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.2753 (3.3548)  acc1: 27.7778 (29.0809)  acc5: 66.6667 (65.2263)  time: 0.3075  data: 0.1028  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.5272 (3.3820)  acc1: 16.6667 (27.6557)  acc5: 55.5556 (64.1026)  time: 0.3034  data: 0.1050  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.5921 (3.4056)  acc1: 16.6667 (26.3476)  acc5: 61.1111 (63.3113)  time: 0.3232  data: 0.1266  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6451 (3.4292)  acc1: 11.1111 (25.5756)  acc5: 55.5556 (62.2623)  time: 0.3341  data: 0.1323  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5642 (3.4340)  acc1: 22.2222 (26.3545)  acc5: 55.5556 (62.2130)  time: 0.2928  data: 0.0986  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2593 (3.4078)  acc1: 33.3333 (27.6930)  acc5: 72.2222 (63.1891)  time: 0.2891  data: 0.0933  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1549 (3.3980)  acc1: 38.8889 (28.2112)  acc5: 72.2222 (63.4752)  time: 0.3017  data: 0.0972  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2254 (3.3842)  acc1: 33.3333 (28.4400)  acc5: 72.2222 (63.9441)  time: 0.2886  data: 0.0911  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2072 (3.3796)  acc1: 22.2222 (28.2954)  acc5: 72.2222 (64.2167)  time: 0.3123  data: 0.1199  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3438 (3.3822)  acc1: 22.2222 (28.0702)  acc5: 66.6667 (63.9051)  time: 0.3115  data: 0.1150  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3362 (3.3770)  acc1: 16.6667 (27.8392)  acc5: 66.6667 (64.2419)  time: 0.2993  data: 0.1121  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3362 (3.3957)  acc1: 16.6667 (27.3997)  acc5: 61.1111 (63.1472)  time: 0.3571  data: 0.1584  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.4374 (3.3944)  acc1: 16.6667 (26.9486)  acc5: 50.0000 (63.0182)  time: 0.3384  data: 0.1298  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2255 (3.4000)  acc1: 22.2222 (27.0669)  acc5: 61.1111 (62.9279)  time: 0.3139  data: 0.1135  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5449 (3.4020)  acc1: 27.7778 (27.0236)  acc5: 61.1111 (62.7702)  time: 0.3062  data: 0.1138  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2247 (3.3953)  acc1: 38.8889 (27.6575)  acc5: 72.2222 (63.2756)  time: 0.3001  data: 0.1090  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2031 (3.3845)  acc1: 27.7778 (27.5012)  acc5: 72.2222 (63.6699)  time: 0.3273  data: 0.1293  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2254 (3.3853)  acc1: 22.2222 (27.4679)  acc5: 72.2222 (63.6122)  time: 0.2945  data: 0.1031  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1648 (3.3728)  acc1: 38.8889 (28.1609)  acc5: 77.7778 (64.3678)  time: 0.2895  data: 0.1035  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3102 (3.3749)  acc1: 38.8889 (28.1468)  acc5: 77.7778 (64.1041)  time: 0.2343  data: 0.0624  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3102 (3.3765)  acc1: 38.8889 (28.1589)  acc5: 77.7778 (64.1204)  time: 0.2280  data: 0.0624  max mem: 15572
Val: Total time: 0:01:27 (0.3222 s / it)
* Acc@1 28.159 Acc@5 64.120 loss 3.377
Accuracy of the network on the 4883 val videos: 28.2%
Max accuracy: 29.31%
Epoch: [33]  [ 0/91]  eta: 0:12:06  lr: 0.000005  min_lr: 0.000000  loss: 4.5665 (4.5665)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9826  data: 7.5480  max mem: 15572
Epoch: [33]  [10/91]  eta: 0:01:52  lr: 0.000004  min_lr: 0.000000  loss: 4.6390 (4.6776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3942  data: 0.9544  max mem: 15572
Epoch: [33]  [20/91]  eta: 0:01:10  lr: 0.000004  min_lr: 0.000000  loss: 4.6450 (4.6950)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.2007  max mem: 15572
Epoch: [33]  [30/91]  eta: 0:00:52  lr: 0.000004  min_lr: 0.000000  loss: 4.6842 (4.7024)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1134  max mem: 15572
Epoch: [33]  [40/91]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000000  loss: 4.6621 (4.6980)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0901  max mem: 15572
Epoch: [33]  [50/91]  eta: 0:00:28  lr: 0.000004  min_lr: 0.000000  loss: 4.7131 (4.7020)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4640  data: 0.0302  max mem: 15572
[2025-01-13 03:03:32,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:03:32,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:03:33,651] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29765
[2025-01-13 03:03:33,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:03:33,652] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [60/91]  eta: 0:00:20  lr: 0.000004  min_lr: 0.000000  loss: 4.6932 (4.6905)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.4684  data: 0.0410  max mem: 15572
Epoch: [33]  [70/91]  eta: 0:00:13  lr: 0.000004  min_lr: 0.000000  loss: 4.7360 (4.6999)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1357  max mem: 15572
Epoch: [33]  [80/91]  eta: 0:00:07  lr: 0.000003  min_lr: 0.000000  loss: 4.7360 (4.6966)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1673  max mem: 15572
Epoch: [33]  [90/91]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 4.6999 (4.6996)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.4873  data: 0.0760  max mem: 15572
Epoch: [33] Total time: 0:00:57 (0.6278 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 4.6999 (4.6996)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 78
Indices to remove: tensor([  341,   400,   518,  1638,  1968,  1998,  2000,  2097,  2319,  2630,
         3262,  3892,  4400,  4445,  4922,  5250,  6914,  7180,  7994,  8024,
         8464, 10404, 10640, 10867, 11781, 11803, 11836, 12139, 12305, 12445,
        12721, 12881, 12927, 13508, 13779, 14292, 14489, 14590, 14766, 14882,
        15218, 15659, 16445, 17847, 18181, 18583, 18914, 19278, 20206, 20366,
        21768, 21825, 22049, 22126, 22766, 22812, 23319, 23928, 24508, 24702,
        24864, 25549, 25614, 26183, 26264, 26378, 26438, 26552, 26769, 27606,
        29865, 30169, 30580, 30660, 30690, 31366, 31697, 33624],
       device='cuda:0')
length of data loader train is: 85
num_training_steps_per_epoch is: 85
Change step level LR scheduler!
Set warmup steps = 425
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:25:22  loss: 2.8577 (2.8577)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 5.5977  data: 5.4216  max mem: 15572
Val:  [ 10/272]  eta: 0:03:29  loss: 3.4453 (3.4843)  acc1: 16.6667 (19.1919)  acc5: 66.6667 (62.6263)  time: 0.7995  data: 0.5931  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 3.4453 (3.4403)  acc1: 22.2222 (25.3968)  acc5: 61.1111 (63.4921)  time: 0.2870  data: 0.0927  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 3.5388 (3.4703)  acc1: 27.7778 (25.6272)  acc5: 61.1111 (61.6487)  time: 0.2472  data: 0.0653  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.5591 (3.4682)  acc1: 22.2222 (24.1192)  acc5: 61.1111 (62.0596)  time: 0.2626  data: 0.0660  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.4657 (3.4312)  acc1: 22.2222 (24.8366)  acc5: 61.1111 (62.6362)  time: 0.2997  data: 0.1063  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 3.1629 (3.3907)  acc1: 27.7778 (25.6831)  acc5: 72.2222 (63.5701)  time: 0.3292  data: 0.1483  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 3.1629 (3.3670)  acc1: 33.3333 (28.3255)  acc5: 72.2222 (65.3365)  time: 0.3021  data: 0.1182  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.2837 (3.3711)  acc1: 27.7778 (27.8464)  acc5: 66.6667 (64.8148)  time: 0.2978  data: 0.0975  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.4936 (3.3906)  acc1: 22.2222 (26.9841)  acc5: 55.5556 (63.6752)  time: 0.3354  data: 0.1266  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.4953 (3.4108)  acc1: 16.6667 (25.9076)  acc5: 61.1111 (62.9813)  time: 0.3566  data: 0.1509  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6140 (3.4349)  acc1: 16.6667 (24.9249)  acc5: 55.5556 (61.8118)  time: 0.3298  data: 0.1297  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5969 (3.4401)  acc1: 22.2222 (25.7576)  acc5: 55.5556 (61.6621)  time: 0.3046  data: 0.1147  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.3265 (3.4158)  acc1: 38.8889 (27.1416)  acc5: 66.6667 (62.4682)  time: 0.3222  data: 0.1138  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.1568 (3.4074)  acc1: 38.8889 (27.7384)  acc5: 72.2222 (62.8842)  time: 0.3092  data: 0.0898  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2323 (3.3927)  acc1: 38.8889 (28.4768)  acc5: 66.6667 (63.3922)  time: 0.2944  data: 0.0971  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2323 (3.3888)  acc1: 22.2222 (28.2264)  acc5: 66.6667 (63.6991)  time: 0.2904  data: 0.1101  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3241 (3.3925)  acc1: 22.2222 (28.1027)  acc5: 66.6667 (63.3853)  time: 0.3264  data: 0.1361  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3238 (3.3854)  acc1: 22.2222 (27.8085)  acc5: 66.6667 (63.7508)  time: 0.3344  data: 0.1443  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3238 (3.4042)  acc1: 16.6667 (27.3706)  acc5: 55.5556 (62.7109)  time: 0.3337  data: 0.1511  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.3811 (3.4032)  acc1: 16.6667 (26.8657)  acc5: 55.5556 (62.6313)  time: 0.3180  data: 0.1294  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2999 (3.4079)  acc1: 16.6667 (26.9089)  acc5: 61.1111 (62.6382)  time: 0.2816  data: 0.0933  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5476 (3.4096)  acc1: 22.2222 (26.8225)  acc5: 61.1111 (62.4434)  time: 0.3054  data: 0.1148  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2355 (3.4034)  acc1: 38.8889 (27.4170)  acc5: 72.2222 (62.8668)  time: 0.3327  data: 0.1459  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2355 (3.3939)  acc1: 27.7778 (27.2245)  acc5: 72.2222 (63.4163)  time: 0.3112  data: 0.1278  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2622 (3.3942)  acc1: 22.2222 (27.1138)  acc5: 77.7778 (63.4573)  time: 0.2975  data: 0.1077  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.2110 (3.3821)  acc1: 33.3333 (27.6288)  acc5: 77.7778 (64.0485)  time: 0.2922  data: 0.1064  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2890 (3.3845)  acc1: 33.3333 (27.6753)  acc5: 72.2222 (63.9196)  time: 0.2205  data: 0.0537  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2890 (3.3858)  acc1: 33.3333 (27.6674)  acc5: 72.2222 (63.9156)  time: 0.2136  data: 0.0537  max mem: 15572
Val: Total time: 0:01:27 (0.3224 s / it)
* Acc@1 27.667 Acc@5 63.916 loss 3.386
Accuracy of the network on the 4883 val videos: 27.7%
Max accuracy: 29.31%
Epoch: [34]  [ 0/85]  eta: 0:10:07  lr: 0.000003  min_lr: 0.000000  loss: 4.7979 (4.7979)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1509  data: 6.4533  max mem: 15572
Epoch: [34]  [10/85]  eta: 0:01:24  lr: 0.000003  min_lr: 0.000000  loss: 4.6943 (4.7571)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1220  data: 0.6739  max mem: 15572
Epoch: [34]  [20/85]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 4.6943 (4.7140)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1178  max mem: 15572
Epoch: [34]  [30/85]  eta: 0:00:42  lr: 0.000003  min_lr: 0.000000  loss: 4.8231 (4.7606)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1627  max mem: 15572
Epoch: [34]  [40/85]  eta: 0:00:33  lr: 0.000003  min_lr: 0.000000  loss: 4.7846 (4.7596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1773  max mem: 15572
Epoch: [34]  [50/85]  eta: 0:00:24  lr: 0.000003  min_lr: 0.000000  loss: 4.7171 (4.7424)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1569  max mem: 15572
Epoch: [34]  [60/85]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 4.7171 (4.7307)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1384  max mem: 15572
Epoch: [34]  [70/85]  eta: 0:00:10  lr: 0.000003  min_lr: 0.000000  loss: 4.7006 (4.7328)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1536  max mem: 15572
Epoch: [34]  [80/85]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000000  loss: 4.7414 (4.7353)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0879  max mem: 15572
Epoch: [34]  [84/85]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 4.7032 (4.7320)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0878  max mem: 15572
Epoch: [34] Total time: 0:00:53 (0.6326 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 4.7032 (4.7320)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 65
Indices to remove: tensor([  386,  1697,  2180,  2206,  2783,  4093,  4204,  4501,  7053,  7386,
         8084,  9039,  9970, 10300, 10373, 10673, 11004, 11820, 12470, 12641,
        13291, 13341, 13604, 13885, 14055, 14375, 15480, 17879, 17902, 17984,
        18288, 18466, 19628, 20136, 21231, 21823, 22381, 22409, 22933, 23758,
        23927, 24825, 24868, 25586, 25670, 25956, 26348, 26587, 27035, 27517,
        28510, 28517, 28575, 28584, 28628, 28912, 28920, 29460, 29717, 29915,
        31392, 31676, 32763, 33617, 33633], device='cuda:0')
length of data loader train is: 79
num_training_steps_per_epoch is: 79
Change step level LR scheduler!
Set warmup steps = 395
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:22:14  loss: 2.8575 (2.8575)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 4.9073  data: 4.7302  max mem: 15572
Val:  [ 10/272]  eta: 0:03:24  loss: 3.4615 (3.5021)  acc1: 16.6667 (18.1818)  acc5: 61.1111 (62.6263)  time: 0.7790  data: 0.5689  max mem: 15572
Val:  [ 20/272]  eta: 0:02:21  loss: 3.4218 (3.4476)  acc1: 27.7778 (25.3968)  acc5: 61.1111 (63.7566)  time: 0.3432  data: 0.1347  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 3.4218 (3.4804)  acc1: 27.7778 (24.7312)  acc5: 55.5556 (61.6487)  time: 0.2547  data: 0.0616  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 3.4928 (3.4719)  acc1: 22.2222 (24.1192)  acc5: 55.5556 (61.6531)  time: 0.2508  data: 0.0681  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.4530 (3.4235)  acc1: 22.2222 (25.1634)  acc5: 66.6667 (63.0719)  time: 0.3001  data: 0.1125  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 3.1010 (3.3845)  acc1: 27.7778 (26.2295)  acc5: 72.2222 (63.6612)  time: 0.3166  data: 0.1256  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.1225 (3.3608)  acc1: 33.3333 (28.4820)  acc5: 72.2222 (65.6495)  time: 0.3405  data: 0.1444  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.3122 (3.3642)  acc1: 27.7778 (28.1207)  acc5: 66.6667 (65.0206)  time: 0.3330  data: 0.1145  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 3.4993 (3.3858)  acc1: 22.2222 (27.0452)  acc5: 61.1111 (64.0415)  time: 0.3484  data: 0.1315  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.5669 (3.4062)  acc1: 16.6667 (25.9626)  acc5: 61.1111 (63.7514)  time: 0.3328  data: 0.1355  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.5790 (3.4290)  acc1: 11.1111 (25.1251)  acc5: 55.5556 (62.3624)  time: 0.3057  data: 0.1043  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5790 (3.4335)  acc1: 22.2222 (26.0790)  acc5: 55.5556 (62.4885)  time: 0.3060  data: 0.0990  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2758 (3.4105)  acc1: 38.8889 (27.4385)  acc5: 72.2222 (63.3164)  time: 0.3228  data: 0.1308  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.2169 (3.4024)  acc1: 38.8889 (27.8566)  acc5: 77.7778 (63.4752)  time: 0.3275  data: 0.1376  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.2169 (3.3893)  acc1: 33.3333 (28.0721)  acc5: 66.6667 (63.7601)  time: 0.3288  data: 0.1255  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.1947 (3.3860)  acc1: 27.7778 (27.7433)  acc5: 72.2222 (64.2167)  time: 0.3211  data: 0.1263  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.3589 (3.3888)  acc1: 27.7778 (27.6478)  acc5: 66.6667 (63.8726)  time: 0.3064  data: 0.1115  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3180 (3.3833)  acc1: 27.7778 (27.3481)  acc5: 66.6667 (64.2726)  time: 0.3267  data: 0.1238  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3051 (3.4016)  acc1: 16.6667 (26.9634)  acc5: 61.1111 (63.2635)  time: 0.3071  data: 0.1180  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.3517 (3.3993)  acc1: 16.6667 (26.6446)  acc5: 61.1111 (63.3499)  time: 0.3171  data: 0.1335  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2038 (3.4031)  acc1: 22.2222 (26.6719)  acc5: 66.6667 (63.2701)  time: 0.3295  data: 0.1419  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5585 (3.4056)  acc1: 22.2222 (26.7220)  acc5: 61.1111 (63.0468)  time: 0.2754  data: 0.0965  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.1885 (3.3983)  acc1: 38.8889 (27.3930)  acc5: 72.2222 (63.6364)  time: 0.2946  data: 0.1058  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.1885 (3.3886)  acc1: 22.2222 (27.1093)  acc5: 77.7778 (64.1309)  time: 0.3386  data: 0.1265  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2128 (3.3893)  acc1: 22.2222 (27.0474)  acc5: 77.7778 (64.1656)  time: 0.2884  data: 0.0727  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1736 (3.3772)  acc1: 33.3333 (27.6075)  acc5: 77.7778 (64.9212)  time: 0.2923  data: 0.0987  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3320 (3.3808)  acc1: 33.3333 (27.5113)  acc5: 72.2222 (64.5961)  time: 0.2636  data: 0.1021  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3320 (3.3819)  acc1: 33.3333 (27.5241)  acc5: 72.2222 (64.6119)  time: 0.2563  data: 0.1021  max mem: 15572
Val: Total time: 0:01:28 (0.3266 s / it)
* Acc@1 27.524 Acc@5 64.612 loss 3.382
Accuracy of the network on the 4883 val videos: 27.5%
Max accuracy: 29.31%
Epoch: [35]  [ 0/79]  eta: 0:11:32  lr: 0.000002  min_lr: 0.000000  loss: 4.4791 (4.4791)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7663  data: 8.3505  max mem: 15572
Epoch: [35]  [10/79]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 4.6661 (4.6823)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3181  data: 0.8868  max mem: 15572
[2025-01-13 03:07:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:07:56,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:07:57,748] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29897
[2025-01-13 03:07:57,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:07:57,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [20/79]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 4.7066 (4.6905)  loss_scale: 65536.0000 (74898.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1348  max mem: 15572
Epoch: [35]  [30/79]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 4.7454 (4.6990)  loss_scale: 65536.0000 (71878.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1274  max mem: 15572
Epoch: [35]  [40/79]  eta: 0:00:29  lr: 0.000002  min_lr: 0.000000  loss: 4.7232 (4.6833)  loss_scale: 65536.0000 (70331.3171)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0949  max mem: 15572
Epoch: [35]  [50/79]  eta: 0:00:20  lr: 0.000002  min_lr: 0.000000  loss: 4.7232 (4.6962)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0634  max mem: 15572
Epoch: [35]  [60/79]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 4.7689 (4.7119)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0493  max mem: 15572
Epoch: [35]  [70/79]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000000  loss: 4.8123 (4.7276)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0799  max mem: 15572
Epoch: [35]  [78/79]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 4.8774 (4.7376)  loss_scale: 65536.0000 (68024.7089)  weight_decay: 0.0500 (0.0500)  time: 0.4652  data: 0.0622  max mem: 15572
Epoch: [35] Total time: 0:00:50 (0.6360 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 4.8774 (4.7376)  loss_scale: 65536.0000 (68024.7089)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 62
Indices to remove: tensor([  472,  1841,  2146,  2450,  2771,  3077,  3467,  3588,  3979,  4656,
         4830,  4923,  6457,  7342,  8569, 10387, 11057, 11830, 11958, 12330,
        12687, 14187, 14196, 14204, 14224, 16619, 17867, 17957, 17974, 18006,
        18157, 18432, 18578, 18844, 18946, 19454, 19999, 20518, 20907, 21164,
        22315, 22489, 22762, 23113, 24985, 25058, 25293, 26353, 26487, 26862,
        27290, 27608, 27720, 29997, 30045, 30085, 30474, 30648, 30897, 31359,
        31365, 32941], device='cuda:0')
length of data loader train is: 74
num_training_steps_per_epoch is: 74
Change step level LR scheduler!
Set warmup steps = 370
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:26:36  loss: 2.8577 (2.8577)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 5.8684  data: 5.6325  max mem: 15572
Val:  [ 10/272]  eta: 0:03:47  loss: 3.4236 (3.4692)  acc1: 16.6667 (21.7172)  acc5: 66.6667 (63.6364)  time: 0.8677  data: 0.6749  max mem: 15572
Val:  [ 20/272]  eta: 0:02:23  loss: 3.4236 (3.4367)  acc1: 22.2222 (25.3968)  acc5: 66.6667 (64.5503)  time: 0.3043  data: 0.0899  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 3.4546 (3.4762)  acc1: 22.2222 (23.8351)  acc5: 55.5556 (61.8280)  time: 0.2245  data: 0.0078  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 3.5419 (3.4740)  acc1: 22.2222 (23.1707)  acc5: 55.5556 (61.1111)  time: 0.2335  data: 0.0364  max mem: 15572
Val:  [ 50/272]  eta: 0:01:22  loss: 3.4774 (3.4291)  acc1: 22.2222 (24.1830)  acc5: 61.1111 (62.5272)  time: 0.2507  data: 0.0449  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 3.0807 (3.3883)  acc1: 27.7778 (25.1366)  acc5: 66.6667 (63.0237)  time: 0.2917  data: 0.0968  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 3.1325 (3.3652)  acc1: 33.3333 (27.5430)  acc5: 77.7778 (64.7887)  time: 0.3535  data: 0.1615  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 3.2594 (3.3640)  acc1: 27.7778 (27.5720)  acc5: 72.2222 (64.6776)  time: 0.3252  data: 0.1314  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.4766 (3.3849)  acc1: 22.2222 (26.7399)  acc5: 61.1111 (63.4921)  time: 0.3254  data: 0.1379  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.5138 (3.4086)  acc1: 16.6667 (25.5226)  acc5: 55.5556 (62.9263)  time: 0.3389  data: 0.1485  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6229 (3.4315)  acc1: 11.1111 (24.6747)  acc5: 55.5556 (62.0120)  time: 0.3386  data: 0.1449  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5972 (3.4355)  acc1: 16.6667 (25.6198)  acc5: 55.5556 (62.1212)  time: 0.3263  data: 0.1309  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.2846 (3.4107)  acc1: 38.8889 (27.0992)  acc5: 66.6667 (62.9347)  time: 0.2778  data: 0.0749  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 3.2146 (3.4018)  acc1: 38.8889 (27.6596)  acc5: 66.6667 (63.1206)  time: 0.2800  data: 0.0691  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2146 (3.3872)  acc1: 33.3333 (28.1089)  acc5: 66.6667 (63.7233)  time: 0.2956  data: 0.0840  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2092 (3.3838)  acc1: 33.3333 (28.0193)  acc5: 72.2222 (64.0442)  time: 0.3123  data: 0.0990  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3575 (3.3882)  acc1: 22.2222 (27.7128)  acc5: 72.2222 (63.9376)  time: 0.3282  data: 0.1115  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3432 (3.3819)  acc1: 22.2222 (27.5015)  acc5: 66.6667 (64.3033)  time: 0.3118  data: 0.0984  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2772 (3.4009)  acc1: 16.6667 (27.0506)  acc5: 61.1111 (63.2054)  time: 0.2870  data: 0.0959  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4372 (3.4009)  acc1: 16.6667 (26.5064)  acc5: 55.5556 (62.9906)  time: 0.3022  data: 0.1176  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2835 (3.4049)  acc1: 16.6667 (26.6456)  acc5: 61.1111 (62.8752)  time: 0.3152  data: 0.1127  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4817 (3.4066)  acc1: 27.7778 (26.5209)  acc5: 61.1111 (62.5691)  time: 0.3154  data: 0.1054  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.1929 (3.4006)  acc1: 38.8889 (27.3208)  acc5: 72.2222 (63.0351)  time: 0.3527  data: 0.1365  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2052 (3.3907)  acc1: 27.7778 (27.1784)  acc5: 72.2222 (63.5546)  time: 0.3272  data: 0.1031  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2247 (3.3911)  acc1: 22.2222 (27.1580)  acc5: 77.7778 (63.6344)  time: 0.2957  data: 0.0852  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1329 (3.3784)  acc1: 38.8889 (27.7991)  acc5: 77.7778 (64.3465)  time: 0.3109  data: 0.1084  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2223 (3.3802)  acc1: 33.3333 (27.8393)  acc5: 72.2222 (64.1656)  time: 0.2312  data: 0.0479  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2223 (3.3821)  acc1: 33.3333 (27.8517)  acc5: 77.7778 (64.1819)  time: 0.2241  data: 0.0477  max mem: 15572
Val: Total time: 0:01:27 (0.3216 s / it)
* Acc@1 27.852 Acc@5 64.182 loss 3.382
Accuracy of the network on the 4883 val videos: 27.9%
Max accuracy: 29.31%
Epoch: [36]  [ 0/74]  eta: 0:06:45  lr: 0.000002  min_lr: 0.000000  loss: 4.9424 (4.9424)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.4777  data: 5.0368  max mem: 15572
Epoch: [36]  [10/74]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 4.8681 (4.7907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8629  data: 0.4702  max mem: 15572
Epoch: [36]  [20/74]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 4.7808 (4.7311)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4481  data: 0.0072  max mem: 15572
Epoch: [36]  [30/74]  eta: 0:00:27  lr: 0.000001  min_lr: 0.000000  loss: 4.6985 (4.7308)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0316  max mem: 15572
[2025-01-13 03:10:23,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=196, lr=[1.1387886840671411e-08, 1.1387886840671411e-08, 1.6268409772387734e-08, 1.6268409772387734e-08, 2.3240585389125334e-08, 2.3240585389125334e-08, 3.3200836270179053e-08, 3.3200836270179053e-08, 4.742976610025579e-08, 4.742976610025579e-08, 6.775680871465113e-08, 6.775680871465113e-08, 9.67954410209302e-08, 9.67954410209302e-08, 1.3827920145847173e-07, 1.3827920145847173e-07, 1.975417163692453e-07, 1.975417163692453e-07, 2.8220245195606474e-07, 2.8220245195606474e-07, 4.0314635993723537e-07, 4.0314635993723537e-07, 5.759233713389078e-07, 5.759233713389078e-07, 8.227476733412968e-07, 8.227476733412968e-07, 1.1753538190589955e-06, 1.1753538190589955e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-13 03:10:23,786] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=27.981992963704304, CurrSamplesPerSec=24.17015849111693, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [40/74]  eta: 0:00:21  lr: 0.000001  min_lr: 0.000000  loss: 4.7045 (4.7505)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1151  max mem: 15572
Epoch: [36]  [50/74]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 4.7696 (4.7472)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.1870  max mem: 15572
Epoch: [36]  [60/74]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000000  loss: 4.7228 (4.7423)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1642  max mem: 15572
[2025-01-13 03:10:41,679] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:10:41,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:10:42,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30027
[2025-01-13 03:10:42,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:10:42,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [70/74]  eta: 0:00:02  lr: 0.000001  min_lr: 0.000000  loss: 4.6719 (4.7373)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1726  max mem: 15572
Epoch: [36]  [73/74]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 4.7074 (4.7443)  loss_scale: 65536.0000 (66421.6216)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.1119  max mem: 15572
Epoch: [36] Total time: 0:00:46 (0.6321 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 4.7074 (4.7443)  loss_scale: 65536.0000 (66421.6216)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 45
Indices to remove: tensor([  932,  3045,  4357,  4431,  7112,  7116,  7980, 10928, 11001, 12189,
        13118, 13456, 13536, 13649, 14889, 16394, 16680, 17990, 18088, 18531,
        19282, 19983, 20019, 21722, 21862, 22192, 22748, 22809, 22942, 23115,
        24845, 25502, 26343, 26678, 27022, 28170, 28214, 30021, 30434, 30830,
        30921, 31285, 31421, 31749, 32866], device='cuda:0')
length of data loader train is: 71
num_training_steps_per_epoch is: 71
Change step level LR scheduler!
Set warmup steps = 355
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:27:47  loss: 2.8547 (2.8547)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 6.1319  data: 5.8438  max mem: 15572
Val:  [ 10/272]  eta: 0:03:38  loss: 3.4408 (3.4968)  acc1: 16.6667 (21.7172)  acc5: 66.6667 (62.6263)  time: 0.8329  data: 0.6216  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 3.4408 (3.4424)  acc1: 22.2222 (26.4550)  acc5: 66.6667 (63.7566)  time: 0.2719  data: 0.0687  max mem: 15572
Val:  [ 30/272]  eta: 0:01:58  loss: 3.5057 (3.4868)  acc1: 27.7778 (25.0896)  acc5: 55.5556 (60.5735)  time: 0.3000  data: 0.0911  max mem: 15572
Val:  [ 40/272]  eta: 0:01:48  loss: 3.5556 (3.4787)  acc1: 22.2222 (24.2547)  acc5: 55.5556 (60.4336)  time: 0.3787  data: 0.1740  max mem: 15572
Val:  [ 50/272]  eta: 0:01:39  loss: 3.4275 (3.4316)  acc1: 22.2222 (24.6187)  acc5: 61.1111 (61.7647)  time: 0.3835  data: 0.1814  max mem: 15572
Val:  [ 60/272]  eta: 0:01:31  loss: 3.1029 (3.3909)  acc1: 27.7778 (25.2277)  acc5: 72.2222 (62.8415)  time: 0.3609  data: 0.1475  max mem: 15572
Val:  [ 70/272]  eta: 0:01:25  loss: 3.1417 (3.3671)  acc1: 33.3333 (27.4648)  acc5: 77.7778 (64.6322)  time: 0.3563  data: 0.1353  max mem: 15572
Val:  [ 80/272]  eta: 0:01:18  loss: 3.2906 (3.3669)  acc1: 27.7778 (27.5720)  acc5: 66.6667 (64.1975)  time: 0.3415  data: 0.1079  max mem: 15572
Val:  [ 90/272]  eta: 0:01:12  loss: 3.4612 (3.3875)  acc1: 22.2222 (26.7399)  acc5: 61.1111 (63.3700)  time: 0.3258  data: 0.0968  max mem: 15572
Val:  [100/272]  eta: 0:01:07  loss: 3.5755 (3.4089)  acc1: 16.6667 (25.7976)  acc5: 61.1111 (62.5963)  time: 0.3250  data: 0.1153  max mem: 15572
Val:  [110/272]  eta: 0:01:03  loss: 3.6175 (3.4321)  acc1: 16.6667 (24.9750)  acc5: 55.5556 (61.5115)  time: 0.3368  data: 0.1293  max mem: 15572
Val:  [120/272]  eta: 0:00:59  loss: 3.5650 (3.4352)  acc1: 22.2222 (25.5280)  acc5: 55.5556 (61.8457)  time: 0.3859  data: 0.1739  max mem: 15572
Val:  [130/272]  eta: 0:00:54  loss: 3.2764 (3.4099)  acc1: 33.3333 (27.0144)  acc5: 72.2222 (62.8923)  time: 0.3467  data: 0.1383  max mem: 15572
Val:  [140/272]  eta: 0:00:48  loss: 3.2183 (3.4007)  acc1: 38.8889 (27.4626)  acc5: 72.2222 (63.1600)  time: 0.2389  data: 0.0525  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.2376 (3.3861)  acc1: 33.3333 (28.1089)  acc5: 66.6667 (63.6497)  time: 0.1986  data: 0.0183  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.2109 (3.3828)  acc1: 33.3333 (27.9848)  acc5: 72.2222 (63.8026)  time: 0.2341  data: 0.0008  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.3567 (3.3874)  acc1: 22.2222 (27.7128)  acc5: 66.6667 (63.4178)  time: 0.3457  data: 0.1091  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.3307 (3.3822)  acc1: 22.2222 (27.4708)  acc5: 66.6667 (63.8122)  time: 0.3972  data: 0.2083  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 3.3307 (3.4017)  acc1: 16.6667 (26.9924)  acc5: 66.6667 (62.8272)  time: 0.3306  data: 0.1401  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.4218 (3.4017)  acc1: 16.6667 (26.5616)  acc5: 55.5556 (62.7418)  time: 0.3121  data: 0.1109  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.3077 (3.4057)  acc1: 22.2222 (26.7509)  acc5: 66.6667 (62.6646)  time: 0.3240  data: 0.1092  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 3.5386 (3.4082)  acc1: 27.7778 (26.7974)  acc5: 55.5556 (62.4686)  time: 0.3454  data: 0.1305  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.2394 (3.4017)  acc1: 38.8889 (27.4892)  acc5: 72.2222 (62.9870)  time: 0.3363  data: 0.1271  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 3.2092 (3.3940)  acc1: 27.7778 (27.4089)  acc5: 72.2222 (63.3933)  time: 0.3246  data: 0.1063  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2888 (3.3943)  acc1: 22.2222 (27.3572)  acc5: 72.2222 (63.4573)  time: 0.3370  data: 0.1291  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.1408 (3.3821)  acc1: 44.4444 (27.9906)  acc5: 77.7778 (64.0698)  time: 0.2817  data: 0.0977  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3691 (3.3859)  acc1: 38.8889 (27.9008)  acc5: 72.2222 (63.9401)  time: 0.2064  data: 0.0433  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3691 (3.3881)  acc1: 33.3333 (27.8927)  acc5: 72.2222 (63.9361)  time: 0.1990  data: 0.0431  max mem: 15572
Val: Total time: 0:01:32 (0.3388 s / it)
* Acc@1 27.893 Acc@5 63.936 loss 3.388
Accuracy of the network on the 4883 val videos: 27.9%
Max accuracy: 29.31%
Epoch: [37]  [ 0/71]  eta: 0:08:42  lr: 0.000001  min_lr: 0.000000  loss: 4.4529 (4.4529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3653  data: 6.8635  max mem: 15572
Epoch: [37]  [10/71]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000000  loss: 4.5618 (4.6837)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3028  data: 0.8201  max mem: 15572
Epoch: [37]  [20/71]  eta: 0:00:48  lr: 0.000001  min_lr: 0.000000  loss: 4.8486 (4.7525)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1800  max mem: 15572
Epoch: [37]  [30/71]  eta: 0:00:33  lr: 0.000001  min_lr: 0.000000  loss: 4.8406 (4.7545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1456  max mem: 15572
Epoch: [37]  [40/71]  eta: 0:00:23  lr: 0.000001  min_lr: 0.000000  loss: 4.7233 (4.7751)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1239  max mem: 15572
Epoch: [37]  [50/71]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 4.7233 (4.7615)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1035  max mem: 15572
Epoch: [37]  [60/71]  eta: 0:00:07  lr: 0.000000  min_lr: 0.000000  loss: 4.7642 (4.7685)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1056  max mem: 15572
Epoch: [37]  [70/71]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.8098 (4.7716)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4591  data: 0.0525  max mem: 15572
Epoch: [37] Total time: 0:00:46 (0.6527 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.8098 (4.7716)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 57
Indices to remove: tensor([  686,  1632,  2168,  2404,  3555,  4590,  5466,  5845,  6413,  7007,
         8007,  8201,  8554,  9986, 10930, 11043, 11705, 12283, 13932, 13952,
        14749, 15329, 15673, 16366, 16398, 16504, 16637, 18061, 18180, 19164,
        19260, 20062, 20141, 20253, 20834, 21674, 22212, 22214, 22308, 24826,
        25499, 26287, 26316, 26342, 27172, 28269, 28508, 29559, 30440, 30641,
        31268, 31303, 31506, 31908, 32215, 32790, 32842], device='cuda:0')
length of data loader train is: 66
num_training_steps_per_epoch is: 66
Change step level LR scheduler!
Set warmup steps = 330
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:19:48  loss: 2.8549 (2.8549)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 4.3696  data: 4.1946  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 3.4353 (3.4737)  acc1: 16.6667 (19.1919)  acc5: 66.6667 (61.6162)  time: 0.7463  data: 0.5519  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 3.4097 (3.4309)  acc1: 22.2222 (26.4550)  acc5: 66.6667 (64.2857)  time: 0.3395  data: 0.1461  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 3.4295 (3.4797)  acc1: 22.2222 (25.2688)  acc5: 61.1111 (61.6487)  time: 0.3065  data: 0.1108  max mem: 15572
Val:  [ 40/272]  eta: 0:01:48  loss: 3.5481 (3.4733)  acc1: 22.2222 (24.1192)  acc5: 61.1111 (62.0596)  time: 0.3990  data: 0.2006  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 3.4281 (3.4321)  acc1: 22.2222 (24.8366)  acc5: 61.1111 (62.7451)  time: 0.4029  data: 0.2130  max mem: 15572
Val:  [ 60/272]  eta: 0:01:27  loss: 3.0837 (3.3901)  acc1: 27.7778 (25.6831)  acc5: 66.6667 (63.3880)  time: 0.3034  data: 0.1199  max mem: 15572
Val:  [ 70/272]  eta: 0:01:21  loss: 3.0837 (3.3640)  acc1: 38.8889 (28.4038)  acc5: 72.2222 (65.3365)  time: 0.3092  data: 0.1238  max mem: 15572
Val:  [ 80/272]  eta: 0:01:15  loss: 3.2866 (3.3651)  acc1: 33.3333 (28.2579)  acc5: 72.2222 (64.6091)  time: 0.3301  data: 0.1365  max mem: 15572
Val:  [ 90/272]  eta: 0:01:09  loss: 3.4690 (3.3897)  acc1: 22.2222 (27.3504)  acc5: 50.0000 (63.4921)  time: 0.3067  data: 0.1008  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.5447 (3.4105)  acc1: 16.6667 (26.5127)  acc5: 61.1111 (62.9813)  time: 0.2877  data: 0.0825  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.6415 (3.4324)  acc1: 11.1111 (25.4254)  acc5: 55.5556 (62.0621)  time: 0.3088  data: 0.1203  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.5700 (3.4369)  acc1: 22.2222 (26.2167)  acc5: 55.5556 (62.1671)  time: 0.2921  data: 0.0957  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.3231 (3.4128)  acc1: 38.8889 (27.4809)  acc5: 66.6667 (62.9347)  time: 0.2776  data: 0.0643  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 3.2414 (3.4050)  acc1: 38.8889 (28.0536)  acc5: 66.6667 (63.1994)  time: 0.3014  data: 0.1003  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.2597 (3.3908)  acc1: 33.3333 (28.6240)  acc5: 66.6667 (63.8705)  time: 0.2946  data: 0.1020  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.2026 (3.3876)  acc1: 27.7778 (28.3644)  acc5: 72.2222 (64.1132)  time: 0.2977  data: 0.0959  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.3870 (3.3923)  acc1: 27.7778 (28.2001)  acc5: 66.6667 (63.7102)  time: 0.3378  data: 0.1392  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3415 (3.3861)  acc1: 22.2222 (27.8699)  acc5: 66.6667 (64.1805)  time: 0.3512  data: 0.1460  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3334 (3.4050)  acc1: 16.6667 (27.4578)  acc5: 61.1111 (63.1472)  time: 0.3043  data: 0.0884  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.4315 (3.4047)  acc1: 16.6667 (26.9486)  acc5: 50.0000 (63.0459)  time: 0.2692  data: 0.0529  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2904 (3.4093)  acc1: 22.2222 (26.9616)  acc5: 66.6667 (63.0332)  time: 0.2981  data: 0.0975  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5673 (3.4116)  acc1: 22.2222 (26.9482)  acc5: 66.6667 (62.8205)  time: 0.3062  data: 0.1013  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2666 (3.4054)  acc1: 27.7778 (27.5613)  acc5: 72.2222 (63.2997)  time: 0.2876  data: 0.0749  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2666 (3.3972)  acc1: 27.7778 (27.4320)  acc5: 72.2222 (63.7160)  time: 0.2983  data: 0.0973  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2747 (3.3971)  acc1: 22.2222 (27.4679)  acc5: 72.2222 (63.7450)  time: 0.2612  data: 0.0601  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1964 (3.3849)  acc1: 44.4444 (28.1183)  acc5: 77.7778 (64.4955)  time: 0.2615  data: 0.0722  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2650 (3.3871)  acc1: 38.8889 (28.1058)  acc5: 72.2222 (64.2476)  time: 0.2472  data: 0.0794  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2650 (3.3884)  acc1: 38.8889 (28.1384)  acc5: 72.2222 (64.2433)  time: 0.2370  data: 0.0751  max mem: 15572
Val: Total time: 0:01:27 (0.3215 s / it)
* Acc@1 28.138 Acc@5 64.243 loss 3.388
Accuracy of the network on the 4883 val videos: 28.1%
Max accuracy: 29.31%
Epoch: [38]  [ 0/66]  eta: 0:08:03  lr: 0.000000  min_lr: 0.000000  loss: 4.8523 (4.8523)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.3333  data: 6.9042  max mem: 15572
Epoch: [38]  [10/66]  eta: 0:01:09  lr: 0.000000  min_lr: 0.000000  loss: 4.7781 (4.7531)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2493  data: 0.7788  max mem: 15572
Epoch: [38]  [20/66]  eta: 0:00:45  lr: 0.000000  min_lr: 0.000000  loss: 4.7771 (4.7727)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.2272  max mem: 15572
Epoch: [38]  [30/66]  eta: 0:00:30  lr: 0.000000  min_lr: 0.000000  loss: 4.7958 (4.7907)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1983  max mem: 15572
Epoch: [38]  [40/66]  eta: 0:00:20  lr: 0.000000  min_lr: 0.000000  loss: 4.7695 (4.7487)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1106  max mem: 15572
Epoch: [38]  [50/66]  eta: 0:00:11  lr: 0.000000  min_lr: 0.000000  loss: 4.7721 (4.7576)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1219  max mem: 15572
[2025-01-13 03:15:09,052] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-13 03:15:09,053] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-13 03:15:09,490] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30157
[2025-01-13 03:15:09,491] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-13 03:15:09,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [60/66]  eta: 0:00:04  lr: 0.000000  min_lr: 0.000000  loss: 4.8097 (4.7589)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0657  max mem: 15572
Epoch: [38]  [65/66]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.7837 (4.7646)  loss_scale: 65536.0000 (66528.9697)  weight_decay: 0.0500 (0.0500)  time: 0.4783  data: 0.0656  max mem: 15572
Epoch: [38] Total time: 0:00:44 (0.6696 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.7837 (4.7646)  loss_scale: 65536.0000 (66528.9697)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 55
Indices to remove: tensor([  507,   613,  1543,  1905,  2533,  4037,  4144,  4211,  4353,  4405,
         7207,  7220,  9000,  9095, 12033, 13582, 14444, 14686, 14933, 14953,
        15259, 15534, 16565, 16694, 17591, 17866, 19439, 19475, 19775, 21009,
        22186, 22344, 22813, 22928, 23021, 23523, 23690, 24167, 24686, 25247,
        25634, 25640, 25696, 25827, 26215, 26369, 26993, 27308, 28704, 29250,
        30462, 30878, 31376, 31584, 33125], device='cuda:0')
length of data loader train is: 61
num_training_steps_per_epoch is: 61
Change step level LR scheduler!
Set warmup steps = 305
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Val:  [  0/272]  eta: 0:27:22  loss: 2.8864 (2.8864)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 6.0377  data: 5.8048  max mem: 15572
Val:  [ 10/272]  eta: 0:03:30  loss: 3.4884 (3.4721)  acc1: 22.2222 (20.2020)  acc5: 66.6667 (64.6465)  time: 0.8040  data: 0.6041  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 3.4536 (3.4323)  acc1: 22.2222 (26.1905)  acc5: 66.6667 (65.6085)  time: 0.2688  data: 0.0640  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 3.4536 (3.4779)  acc1: 22.2222 (24.7312)  acc5: 61.1111 (63.0824)  time: 0.3099  data: 0.1056  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 3.5410 (3.4687)  acc1: 22.2222 (23.9837)  acc5: 55.5556 (62.6016)  time: 0.3283  data: 0.1208  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 3.3910 (3.4270)  acc1: 22.2222 (25.0545)  acc5: 61.1111 (63.2898)  time: 0.2677  data: 0.0545  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 3.1191 (3.3845)  acc1: 27.7778 (25.5009)  acc5: 66.6667 (64.2077)  time: 0.2378  data: 0.0339  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 3.1191 (3.3588)  acc1: 33.3333 (27.8560)  acc5: 77.7778 (65.8059)  time: 0.2869  data: 0.0880  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 3.2677 (3.3618)  acc1: 27.7778 (27.9150)  acc5: 66.6667 (65.2263)  time: 0.3213  data: 0.1306  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 3.4951 (3.3835)  acc1: 22.2222 (27.1062)  acc5: 55.5556 (63.7363)  time: 0.3170  data: 0.1278  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.5552 (3.4050)  acc1: 16.6667 (25.7976)  acc5: 55.5556 (63.0913)  time: 0.3150  data: 0.1223  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.6211 (3.4296)  acc1: 11.1111 (24.7247)  acc5: 50.0000 (61.9119)  time: 0.3235  data: 0.1247  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.5796 (3.4353)  acc1: 16.6667 (25.3903)  acc5: 55.5556 (61.8457)  time: 0.2950  data: 0.0995  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2307 (3.4102)  acc1: 33.3333 (26.6327)  acc5: 72.2222 (62.9771)  time: 0.3179  data: 0.1336  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.2191 (3.4017)  acc1: 33.3333 (27.2262)  acc5: 72.2222 (63.1994)  time: 0.3649  data: 0.1715  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2575 (3.3880)  acc1: 33.3333 (27.6306)  acc5: 66.6667 (63.6497)  time: 0.2906  data: 0.0883  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.2550 (3.3860)  acc1: 27.7778 (27.6052)  acc5: 66.6667 (63.8371)  time: 0.2864  data: 0.0798  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3834 (3.3904)  acc1: 22.2222 (27.5504)  acc5: 66.6667 (63.5478)  time: 0.3293  data: 0.1202  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 3.3196 (3.3848)  acc1: 22.2222 (27.2560)  acc5: 66.6667 (63.8735)  time: 0.3070  data: 0.0994  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.2944 (3.4038)  acc1: 16.6667 (26.8470)  acc5: 61.1111 (62.7981)  time: 0.3081  data: 0.1039  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4824 (3.4033)  acc1: 11.1111 (26.2576)  acc5: 55.5556 (62.6589)  time: 0.3055  data: 0.1000  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.3967 (3.4071)  acc1: 16.6667 (26.2243)  acc5: 61.1111 (62.6646)  time: 0.2719  data: 0.0647  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4783 (3.4105)  acc1: 22.2222 (26.1941)  acc5: 61.1111 (62.4183)  time: 0.2992  data: 0.1045  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.2564 (3.4045)  acc1: 38.8889 (26.7677)  acc5: 72.2222 (62.9389)  time: 0.3182  data: 0.1177  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2485 (3.3956)  acc1: 27.7778 (26.6943)  acc5: 77.7778 (63.4624)  time: 0.3381  data: 0.1271  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2485 (3.3960)  acc1: 22.2222 (26.6711)  acc5: 77.7778 (63.4351)  time: 0.3313  data: 0.1277  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1997 (3.3840)  acc1: 38.8889 (27.2882)  acc5: 77.7778 (64.1762)  time: 0.2714  data: 0.0808  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3371 (3.3864)  acc1: 38.8889 (27.2243)  acc5: 72.2222 (64.0016)  time: 0.2013  data: 0.0356  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3371 (3.3878)  acc1: 38.8889 (27.2578)  acc5: 66.6667 (63.9975)  time: 0.1938  data: 0.0355  max mem: 15572
Val: Total time: 0:01:26 (0.3192 s / it)
* Acc@1 27.258 Acc@5 63.998 loss 3.388
Accuracy of the network on the 4883 val videos: 27.3%
Max accuracy: 29.31%
Epoch: [39]  [ 0/61]  eta: 0:08:24  lr: 0.000000  min_lr: 0.000000  loss: 4.7022 (4.7022)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.2745  data: 7.7803  max mem: 15572
Epoch: [39]  [10/61]  eta: 0:01:06  lr: 0.000000  min_lr: 0.000000  loss: 4.9737 (4.8585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2952  data: 0.8524  max mem: 15572
Epoch: [39]  [20/61]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 4.9187 (4.8443)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1391  max mem: 15572
Epoch: [39]  [30/61]  eta: 0:00:25  lr: 0.000000  min_lr: 0.000000  loss: 4.8424 (4.8232)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1505  max mem: 15572
Epoch: [39]  [40/61]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 4.7962 (4.8343)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1694  max mem: 15572
Epoch: [39]  [50/61]  eta: 0:00:08  lr: 0.000000  min_lr: 0.000000  loss: 4.7535 (4.7988)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1367  max mem: 15572
Epoch: [39]  [60/61]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 4.5615 (4.7536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4690  data: 0.0587  max mem: 15572
Epoch: [39] Total time: 0:00:41 (0.6811 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 4.5615 (4.7536)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)
Number of samples to remove: 35
Indices to remove: tensor([ 1781,  4635,  9300, 10208, 10382, 10967, 11038, 11141, 11631, 12194,
        12976, 13352, 13441, 13577, 14246, 14928, 15660, 15769, 16527, 17840,
        21857, 21897, 22513, 23020, 23048, 26255, 26725, 27798, 27839, 28760,
        30538, 31142, 31562, 33080, 33707], device='cuda:0')
length of data loader train is: 58
num_training_steps_per_epoch is: 58
Change step level LR scheduler!
Set warmup steps = 290
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
[2025-01-13 03:17:23,336] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2025-01-13 03:17:23,338] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-39/mp_rank_00_model_states.pt
[2025-01-13 03:17:23,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-39/mp_rank_00_model_states.pt...
[2025-01-13 03:17:23,672] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_train_wrong_samples/checkpoint-39/mp_rank_00_model_states.pt.
[2025-01-13 03:17:23,672] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [  0/272]  eta: 0:25:43  loss: 2.8747 (2.8747)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 5.6749  data: 5.4748  max mem: 15572
Val:  [ 10/272]  eta: 0:03:16  loss: 3.5062 (3.4852)  acc1: 22.2222 (19.1919)  acc5: 61.1111 (61.1111)  time: 0.7502  data: 0.5636  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 3.4822 (3.4253)  acc1: 22.2222 (25.6614)  acc5: 61.1111 (62.6984)  time: 0.2886  data: 0.1056  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 3.4822 (3.4665)  acc1: 27.7778 (25.0896)  acc5: 61.1111 (61.4695)  time: 0.2733  data: 0.0851  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 3.5693 (3.4640)  acc1: 22.2222 (24.3902)  acc5: 61.1111 (61.5176)  time: 0.2574  data: 0.0588  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 3.4336 (3.4259)  acc1: 22.2222 (25.2723)  acc5: 61.1111 (62.2004)  time: 0.3204  data: 0.1273  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 3.1370 (3.3834)  acc1: 27.7778 (25.8652)  acc5: 72.2222 (63.4791)  time: 0.3495  data: 0.1525  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 3.1370 (3.3604)  acc1: 33.3333 (28.1690)  acc5: 77.7778 (65.4147)  time: 0.3153  data: 0.1075  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 3.2925 (3.3610)  acc1: 27.7778 (28.3265)  acc5: 66.6667 (65.0892)  time: 0.3010  data: 0.1012  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 3.4564 (3.3854)  acc1: 22.2222 (27.4115)  acc5: 55.5556 (63.8584)  time: 0.3306  data: 0.1301  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 3.5532 (3.4085)  acc1: 16.6667 (26.0726)  acc5: 55.5556 (63.2013)  time: 0.3157  data: 0.1111  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6150 (3.4323)  acc1: 11.1111 (25.0751)  acc5: 55.5556 (62.2623)  time: 0.3181  data: 0.1165  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.5803 (3.4349)  acc1: 22.2222 (26.0790)  acc5: 55.5556 (62.4426)  time: 0.3294  data: 0.1330  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.2619 (3.4105)  acc1: 33.3333 (27.3113)  acc5: 72.2222 (63.3588)  time: 0.3150  data: 0.1198  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 3.2406 (3.4023)  acc1: 33.3333 (27.8172)  acc5: 72.2222 (63.6328)  time: 0.2878  data: 0.0950  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 3.2408 (3.3878)  acc1: 33.3333 (28.4400)  acc5: 66.6667 (64.0912)  time: 0.3023  data: 0.1083  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 3.2408 (3.3849)  acc1: 22.2222 (28.1228)  acc5: 66.6667 (64.2512)  time: 0.3474  data: 0.1571  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.3841 (3.3899)  acc1: 22.2222 (27.7453)  acc5: 66.6667 (63.9376)  time: 0.3212  data: 0.1231  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.3395 (3.3838)  acc1: 22.2222 (27.4708)  acc5: 66.6667 (64.2419)  time: 0.2909  data: 0.0873  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 3.3227 (3.4032)  acc1: 16.6667 (27.0797)  acc5: 55.5556 (63.0890)  time: 0.3332  data: 0.1286  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.4617 (3.4028)  acc1: 22.2222 (26.7275)  acc5: 50.0000 (62.9077)  time: 0.3396  data: 0.1243  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.2878 (3.4064)  acc1: 22.2222 (26.8299)  acc5: 66.6667 (62.9542)  time: 0.3078  data: 0.0974  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.5661 (3.4086)  acc1: 22.2222 (26.7722)  acc5: 66.6667 (62.6948)  time: 0.3065  data: 0.0993  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 3.1957 (3.4026)  acc1: 38.8889 (27.3449)  acc5: 72.2222 (63.1794)  time: 0.2990  data: 0.1001  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.2084 (3.3943)  acc1: 27.7778 (27.1554)  acc5: 72.2222 (63.6699)  time: 0.3027  data: 0.1132  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2696 (3.3947)  acc1: 22.2222 (27.1138)  acc5: 72.2222 (63.6786)  time: 0.3378  data: 0.1441  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 3.1462 (3.3829)  acc1: 38.8889 (27.6501)  acc5: 77.7778 (64.3252)  time: 0.3078  data: 0.1181  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.2995 (3.3858)  acc1: 33.3333 (27.7163)  acc5: 66.6667 (64.1656)  time: 0.1972  data: 0.0323  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.2995 (3.3878)  acc1: 33.3333 (27.7289)  acc5: 66.6667 (64.1409)  time: 0.1891  data: 0.0323  max mem: 15572
Val: Total time: 0:01:28 (0.3243 s / it)
* Acc@1 27.729 Acc@5 64.141 loss 3.388
Accuracy of the network on the 4883 val videos: 27.7%
Max accuracy: 29.31%
Test:  [   0/2442]  eta: 2:45:17  loss: 3.0671 (3.0671)  acc1: 16.6667 (16.6667)  acc5: 83.3333 (83.3333)  time: 4.0614  data: 3.6965  max mem: 15572
Test:  [  10/2442]  eta: 0:23:33  loss: 3.5161 (3.5300)  acc1: 16.6667 (16.6667)  acc5: 50.0000 (59.0909)  time: 0.5810  data: 0.4184  max mem: 15572
Test:  [  20/2442]  eta: 0:17:12  loss: 3.5161 (3.5543)  acc1: 25.0000 (22.6190)  acc5: 50.0000 (55.5556)  time: 0.2447  data: 0.1050  max mem: 15572
Test:  [  30/2442]  eta: 0:14:09  loss: 3.3800 (3.5066)  acc1: 25.0000 (24.1935)  acc5: 58.3333 (58.6022)  time: 0.2261  data: 0.0859  max mem: 15572
Test:  [  40/2442]  eta: 0:12:55  loss: 3.3743 (3.4981)  acc1: 25.0000 (25.2033)  acc5: 58.3333 (58.5366)  time: 0.2144  data: 0.0833  max mem: 15572
Test:  [  50/2442]  eta: 0:12:32  loss: 3.6418 (3.5552)  acc1: 16.6667 (22.7124)  acc5: 50.0000 (55.8824)  time: 0.2561  data: 0.1293  max mem: 15572
Test:  [  60/2442]  eta: 0:11:59  loss: 3.6712 (3.5554)  acc1: 16.6667 (21.4481)  acc5: 50.0000 (55.6011)  time: 0.2588  data: 0.1195  max mem: 15572
Test:  [  70/2442]  eta: 0:11:22  loss: 3.4471 (3.5400)  acc1: 16.6667 (22.8873)  acc5: 58.3333 (55.7512)  time: 0.2197  data: 0.0813  max mem: 15572
Test:  [  80/2442]  eta: 0:11:22  loss: 3.3050 (3.4959)  acc1: 25.0000 (24.0741)  acc5: 66.6667 (58.0247)  time: 0.2496  data: 0.1153  max mem: 15572
Test:  [  90/2442]  eta: 0:10:57  loss: 3.2223 (3.4855)  acc1: 25.0000 (23.8095)  acc5: 66.6667 (57.8755)  time: 0.2514  data: 0.1210  max mem: 15572
Test:  [ 100/2442]  eta: 0:10:47  loss: 3.2186 (3.4641)  acc1: 33.3333 (24.7525)  acc5: 66.6667 (58.4158)  time: 0.2263  data: 0.0952  max mem: 15572
Test:  [ 110/2442]  eta: 0:10:47  loss: 3.2177 (3.4462)  acc1: 33.3333 (26.2012)  acc5: 66.6667 (59.3844)  time: 0.2675  data: 0.1354  max mem: 15572
Test:  [ 120/2442]  eta: 0:10:41  loss: 3.3859 (3.4638)  acc1: 25.0000 (25.6198)  acc5: 58.3333 (58.3333)  time: 0.2756  data: 0.1468  max mem: 15572
Test:  [ 130/2442]  eta: 0:10:34  loss: 3.6183 (3.4667)  acc1: 16.6667 (25.0636)  acc5: 41.6667 (58.3333)  time: 0.2578  data: 0.1262  max mem: 15572
Test:  [ 140/2442]  eta: 0:10:16  loss: 3.6899 (3.4841)  acc1: 16.6667 (24.8227)  acc5: 50.0000 (57.6832)  time: 0.2162  data: 0.0824  max mem: 15572
Test:  [ 150/2442]  eta: 0:10:20  loss: 3.6914 (3.4974)  acc1: 8.3333 (24.1170)  acc5: 50.0000 (57.5055)  time: 0.2447  data: 0.1100  max mem: 15572
Test:  [ 160/2442]  eta: 0:10:14  loss: 3.7850 (3.5253)  acc1: 8.3333 (23.3437)  acc5: 41.6667 (56.2112)  time: 0.2792  data: 0.1388  max mem: 15572
Test:  [ 170/2442]  eta: 0:10:04  loss: 3.7217 (3.5286)  acc1: 16.6667 (23.0020)  acc5: 41.6667 (56.3353)  time: 0.2321  data: 0.0839  max mem: 15572
Test:  [ 180/2442]  eta: 0:10:00  loss: 3.5145 (3.5268)  acc1: 16.6667 (23.6188)  acc5: 66.6667 (56.5378)  time: 0.2336  data: 0.0865  max mem: 15572
Test:  [ 190/2442]  eta: 0:09:54  loss: 3.2031 (3.5159)  acc1: 25.0000 (23.8656)  acc5: 66.6667 (56.9372)  time: 0.2458  data: 0.1033  max mem: 15572
Test:  [ 200/2442]  eta: 0:09:49  loss: 3.0706 (3.4881)  acc1: 33.3333 (24.6683)  acc5: 75.0000 (57.7944)  time: 0.2400  data: 0.0975  max mem: 15572
Test:  [ 210/2442]  eta: 0:09:45  loss: 3.3063 (3.4997)  acc1: 25.0000 (24.5261)  acc5: 58.3333 (57.3855)  time: 0.2473  data: 0.1098  max mem: 15572
Test:  [ 220/2442]  eta: 0:09:43  loss: 3.4080 (3.4913)  acc1: 25.0000 (24.5852)  acc5: 50.0000 (57.8054)  time: 0.2615  data: 0.1383  max mem: 15572
Test:  [ 230/2442]  eta: 0:09:38  loss: 3.2479 (3.4825)  acc1: 25.0000 (25.1082)  acc5: 66.6667 (58.2612)  time: 0.2503  data: 0.1227  max mem: 15572
Test:  [ 240/2442]  eta: 0:09:33  loss: 3.3457 (3.4770)  acc1: 25.0000 (25.0692)  acc5: 66.6667 (58.6100)  time: 0.2367  data: 0.1041  max mem: 15572
Test:  [ 250/2442]  eta: 0:09:32  loss: 3.3656 (3.4766)  acc1: 25.0000 (24.9668)  acc5: 66.6667 (58.7649)  time: 0.2597  data: 0.1294  max mem: 15572
Test:  [ 260/2442]  eta: 0:09:29  loss: 3.3919 (3.4713)  acc1: 16.6667 (24.8404)  acc5: 66.6667 (58.8761)  time: 0.2644  data: 0.1205  max mem: 15572
Test:  [ 270/2442]  eta: 0:09:25  loss: 3.3678 (3.4723)  acc1: 16.6667 (24.6310)  acc5: 66.6667 (59.2251)  time: 0.2473  data: 0.0938  max mem: 15572
Test:  [ 280/2442]  eta: 0:09:18  loss: 3.4878 (3.4819)  acc1: 16.6667 (24.5552)  acc5: 66.6667 (58.6002)  time: 0.2272  data: 0.0809  max mem: 15572
Test:  [ 290/2442]  eta: 0:09:15  loss: 3.4878 (3.4826)  acc1: 16.6667 (24.3127)  acc5: 50.0000 (58.6197)  time: 0.2319  data: 0.0942  max mem: 15572
Test:  [ 300/2442]  eta: 0:09:12  loss: 3.3219 (3.4808)  acc1: 16.6667 (24.1141)  acc5: 58.3333 (58.5548)  time: 0.2533  data: 0.1186  max mem: 15572
Test:  [ 310/2442]  eta: 0:09:08  loss: 3.2165 (3.4814)  acc1: 16.6667 (24.3569)  acc5: 66.6667 (58.7353)  time: 0.2461  data: 0.1192  max mem: 15572
Test:  [ 320/2442]  eta: 0:09:06  loss: 3.5609 (3.4891)  acc1: 16.6667 (24.1433)  acc5: 58.3333 (58.4112)  time: 0.2510  data: 0.1223  max mem: 15572
Test:  [ 330/2442]  eta: 0:09:09  loss: 3.5781 (3.4889)  acc1: 16.6667 (24.2195)  acc5: 50.0000 (58.3837)  time: 0.3022  data: 0.1725  max mem: 15572
Test:  [ 340/2442]  eta: 0:09:05  loss: 3.4956 (3.4885)  acc1: 25.0000 (24.3157)  acc5: 58.3333 (58.5533)  time: 0.2863  data: 0.1577  max mem: 15572
Test:  [ 350/2442]  eta: 0:09:00  loss: 3.3073 (3.4821)  acc1: 25.0000 (24.4777)  acc5: 75.0000 (59.1880)  time: 0.2275  data: 0.0893  max mem: 15572
Test:  [ 360/2442]  eta: 0:08:56  loss: 3.1730 (3.4735)  acc1: 25.0000 (24.5845)  acc5: 75.0000 (59.4414)  time: 0.2287  data: 0.0938  max mem: 15572
Test:  [ 370/2442]  eta: 0:08:50  loss: 3.3145 (3.4759)  acc1: 25.0000 (24.6631)  acc5: 58.3333 (59.4789)  time: 0.2124  data: 0.0915  max mem: 15572
Test:  [ 380/2442]  eta: 0:08:47  loss: 3.3859 (3.4768)  acc1: 16.6667 (24.6282)  acc5: 58.3333 (59.4051)  time: 0.2251  data: 0.1092  max mem: 15572
Test:  [ 390/2442]  eta: 0:08:50  loss: 3.0708 (3.4645)  acc1: 25.0000 (25.0000)  acc5: 75.0000 (60.0810)  time: 0.3112  data: 0.1874  max mem: 15572
Test:  [ 400/2442]  eta: 0:08:44  loss: 3.3133 (3.4661)  acc1: 25.0000 (24.8961)  acc5: 75.0000 (59.9127)  time: 0.2731  data: 0.1391  max mem: 15572
Test:  [ 410/2442]  eta: 0:08:40  loss: 3.4889 (3.4643)  acc1: 16.6667 (24.9797)  acc5: 58.3333 (60.1379)  time: 0.2111  data: 0.0735  max mem: 15572
Test:  [ 420/2442]  eta: 0:08:40  loss: 3.5207 (3.4673)  acc1: 16.6667 (24.7625)  acc5: 66.6667 (60.1346)  time: 0.2690  data: 0.1356  max mem: 15572
Test:  [ 430/2442]  eta: 0:08:36  loss: 3.4014 (3.4642)  acc1: 25.0000 (24.9227)  acc5: 58.3333 (60.2668)  time: 0.2600  data: 0.1350  max mem: 15572
Test:  [ 440/2442]  eta: 0:08:32  loss: 3.2305 (3.4580)  acc1: 25.0000 (25.1323)  acc5: 66.6667 (60.5253)  time: 0.2289  data: 0.0996  max mem: 15572
Test:  [ 450/2442]  eta: 0:08:30  loss: 3.2531 (3.4595)  acc1: 33.3333 (25.2033)  acc5: 66.6667 (60.5876)  time: 0.2480  data: 0.1149  max mem: 15572
Test:  [ 460/2442]  eta: 0:08:28  loss: 3.5208 (3.4615)  acc1: 25.0000 (25.0904)  acc5: 58.3333 (60.4664)  time: 0.2691  data: 0.1358  max mem: 15572
Test:  [ 470/2442]  eta: 0:08:23  loss: 3.4505 (3.4605)  acc1: 16.6667 (24.9292)  acc5: 58.3333 (60.5980)  time: 0.2419  data: 0.0986  max mem: 15572
Test:  [ 480/2442]  eta: 0:08:19  loss: 3.3294 (3.4541)  acc1: 16.6667 (25.0347)  acc5: 75.0000 (60.8628)  time: 0.2049  data: 0.0556  max mem: 15572
Test:  [ 490/2442]  eta: 0:08:14  loss: 3.1652 (3.4517)  acc1: 25.0000 (25.1018)  acc5: 66.6667 (60.7943)  time: 0.2071  data: 0.0678  max mem: 15572
Test:  [ 500/2442]  eta: 0:08:14  loss: 3.0181 (3.4429)  acc1: 33.3333 (25.2994)  acc5: 66.6667 (61.1111)  time: 0.2582  data: 0.1138  max mem: 15572
Test:  [ 510/2442]  eta: 0:08:10  loss: 2.9497 (3.4366)  acc1: 41.6667 (25.7175)  acc5: 83.3333 (61.4318)  time: 0.2577  data: 0.1045  max mem: 15572
Test:  [ 520/2442]  eta: 0:08:06  loss: 3.0804 (3.4317)  acc1: 41.6667 (25.8957)  acc5: 75.0000 (61.5803)  time: 0.2163  data: 0.0739  max mem: 15572
Test:  [ 530/2442]  eta: 0:08:04  loss: 3.2339 (3.4335)  acc1: 25.0000 (25.8945)  acc5: 58.3333 (61.5819)  time: 0.2430  data: 0.1048  max mem: 15572
Test:  [ 540/2442]  eta: 0:08:00  loss: 3.5688 (3.4357)  acc1: 16.6667 (25.7856)  acc5: 50.0000 (61.3832)  time: 0.2402  data: 0.1128  max mem: 15572
Test:  [ 550/2442]  eta: 0:08:00  loss: 3.4416 (3.4369)  acc1: 16.6667 (25.7864)  acc5: 58.3333 (61.3430)  time: 0.2702  data: 0.1498  max mem: 15572
Test:  [ 560/2442]  eta: 0:07:56  loss: 3.5874 (3.4416)  acc1: 16.6667 (25.5942)  acc5: 58.3333 (61.2894)  time: 0.2603  data: 0.1247  max mem: 15572
Test:  [ 570/2442]  eta: 0:07:53  loss: 3.6893 (3.4476)  acc1: 8.3333 (25.3649)  acc5: 50.0000 (60.8873)  time: 0.2281  data: 0.0883  max mem: 15572
Test:  [ 580/2442]  eta: 0:07:50  loss: 3.6289 (3.4436)  acc1: 16.6667 (25.5450)  acc5: 50.0000 (61.0155)  time: 0.2418  data: 0.1187  max mem: 15572
Test:  [ 590/2442]  eta: 0:07:46  loss: 3.3979 (3.4448)  acc1: 33.3333 (25.6627)  acc5: 58.3333 (60.9983)  time: 0.2118  data: 0.0864  max mem: 15572
Test:  [ 600/2442]  eta: 0:07:44  loss: 3.3331 (3.4407)  acc1: 33.3333 (25.8874)  acc5: 66.6667 (61.1758)  time: 0.2432  data: 0.1046  max mem: 15572
Test:  [ 610/2442]  eta: 0:07:40  loss: 3.0897 (3.4367)  acc1: 41.6667 (26.0911)  acc5: 75.0000 (61.3339)  time: 0.2490  data: 0.1121  max mem: 15572
Test:  [ 620/2442]  eta: 0:07:38  loss: 3.2508 (3.4364)  acc1: 33.3333 (26.1004)  acc5: 58.3333 (61.2856)  time: 0.2309  data: 0.0966  max mem: 15572
Test:  [ 630/2442]  eta: 0:07:34  loss: 3.2109 (3.4291)  acc1: 33.3333 (26.4395)  acc5: 66.6667 (61.5821)  time: 0.2227  data: 0.0899  max mem: 15572
Test:  [ 640/2442]  eta: 0:07:32  loss: 3.1590 (3.4278)  acc1: 33.3333 (26.4041)  acc5: 75.0000 (61.6485)  time: 0.2419  data: 0.1065  max mem: 15572
Test:  [ 650/2442]  eta: 0:07:30  loss: 3.2428 (3.4251)  acc1: 25.0000 (26.3697)  acc5: 75.0000 (61.7384)  time: 0.2755  data: 0.1355  max mem: 15572
Test:  [ 660/2442]  eta: 0:07:28  loss: 3.2428 (3.4236)  acc1: 25.0000 (26.3616)  acc5: 66.6667 (61.7877)  time: 0.2694  data: 0.1315  max mem: 15572
Test:  [ 670/2442]  eta: 0:07:27  loss: 3.1579 (3.4191)  acc1: 25.0000 (26.4282)  acc5: 66.6667 (61.9846)  time: 0.2894  data: 0.1375  max mem: 15572
Test:  [ 680/2442]  eta: 0:07:23  loss: 3.1579 (3.4193)  acc1: 16.6667 (26.3338)  acc5: 66.6667 (62.0044)  time: 0.2569  data: 0.1034  max mem: 15572
Test:  [ 690/2442]  eta: 0:07:21  loss: 3.5270 (3.4259)  acc1: 16.6667 (26.2301)  acc5: 66.6667 (61.6860)  time: 0.2369  data: 0.0830  max mem: 15572
Test:  [ 700/2442]  eta: 0:07:18  loss: 3.3511 (3.4232)  acc1: 16.6667 (26.1056)  acc5: 66.6667 (61.7451)  time: 0.2385  data: 0.0886  max mem: 15572
Test:  [ 710/2442]  eta: 0:07:16  loss: 3.1499 (3.4242)  acc1: 16.6667 (26.0080)  acc5: 66.6667 (61.6737)  time: 0.2485  data: 0.1099  max mem: 15572
Test:  [ 720/2442]  eta: 0:07:12  loss: 3.3331 (3.4256)  acc1: 25.0000 (26.0518)  acc5: 66.6667 (61.6967)  time: 0.2400  data: 0.0993  max mem: 15572
Test:  [ 730/2442]  eta: 0:07:11  loss: 3.4963 (3.4285)  acc1: 16.6667 (25.8664)  acc5: 66.6667 (61.5823)  time: 0.2538  data: 0.1154  max mem: 15572
Test:  [ 740/2442]  eta: 0:07:10  loss: 3.4066 (3.4265)  acc1: 16.6667 (26.0234)  acc5: 58.3333 (61.6284)  time: 0.3106  data: 0.1649  max mem: 15572
Test:  [ 750/2442]  eta: 0:07:07  loss: 3.2300 (3.4242)  acc1: 41.6667 (26.2317)  acc5: 66.6667 (61.7843)  time: 0.2706  data: 0.1286  max mem: 15572
Test:  [ 760/2442]  eta: 0:07:04  loss: 3.1641 (3.4216)  acc1: 25.0000 (26.1608)  acc5: 75.0000 (61.9689)  time: 0.2409  data: 0.1065  max mem: 15572
Test:  [ 770/2442]  eta: 0:07:01  loss: 3.1641 (3.4180)  acc1: 25.0000 (26.2106)  acc5: 75.0000 (62.1055)  time: 0.2411  data: 0.1004  max mem: 15572
Test:  [ 780/2442]  eta: 0:06:59  loss: 3.2834 (3.4189)  acc1: 33.3333 (26.2804)  acc5: 66.6667 (62.0572)  time: 0.2392  data: 0.0927  max mem: 15572
Test:  [ 790/2442]  eta: 0:06:56  loss: 3.2329 (3.4167)  acc1: 33.3333 (26.3801)  acc5: 66.6667 (62.1787)  time: 0.2508  data: 0.1093  max mem: 15572
Test:  [ 800/2442]  eta: 0:06:53  loss: 3.1493 (3.4129)  acc1: 33.3333 (26.4045)  acc5: 83.3333 (62.3283)  time: 0.2459  data: 0.1109  max mem: 15572
Test:  [ 810/2442]  eta: 0:06:51  loss: 3.1649 (3.4118)  acc1: 33.3333 (26.4797)  acc5: 83.3333 (62.4229)  time: 0.2404  data: 0.1051  max mem: 15572
Test:  [ 820/2442]  eta: 0:06:48  loss: 3.3319 (3.4122)  acc1: 25.0000 (26.3906)  acc5: 75.0000 (62.4340)  time: 0.2482  data: 0.1063  max mem: 15572
Test:  [ 830/2442]  eta: 0:06:45  loss: 3.4976 (3.4143)  acc1: 25.0000 (26.3939)  acc5: 58.3333 (62.3646)  time: 0.2504  data: 0.1102  max mem: 15572
Test:  [ 840/2442]  eta: 0:06:43  loss: 3.4012 (3.4142)  acc1: 25.0000 (26.4170)  acc5: 66.6667 (62.4356)  time: 0.2463  data: 0.1066  max mem: 15572
Test:  [ 850/2442]  eta: 0:06:40  loss: 3.3931 (3.4144)  acc1: 25.0000 (26.4493)  acc5: 66.6667 (62.4364)  time: 0.2382  data: 0.0874  max mem: 15572
Test:  [ 860/2442]  eta: 0:06:38  loss: 3.5347 (3.4185)  acc1: 16.6667 (26.3163)  acc5: 58.3333 (62.2822)  time: 0.2519  data: 0.1055  max mem: 15572
Test:  [ 870/2442]  eta: 0:06:35  loss: 3.7119 (3.4211)  acc1: 16.6667 (26.1959)  acc5: 50.0000 (62.1125)  time: 0.2517  data: 0.1202  max mem: 15572
Test:  [ 880/2442]  eta: 0:06:32  loss: 3.5545 (3.4218)  acc1: 8.3333 (26.1067)  acc5: 58.3333 (62.1358)  time: 0.2315  data: 0.0946  max mem: 15572
Test:  [ 890/2442]  eta: 0:06:29  loss: 3.3022 (3.4183)  acc1: 16.6667 (26.1691)  acc5: 66.6667 (62.2709)  time: 0.2341  data: 0.0967  max mem: 15572
Test:  [ 900/2442]  eta: 0:06:27  loss: 3.2480 (3.4187)  acc1: 25.0000 (26.1746)  acc5: 66.6667 (62.1624)  time: 0.2428  data: 0.1100  max mem: 15572
Test:  [ 910/2442]  eta: 0:06:24  loss: 3.2772 (3.4175)  acc1: 33.3333 (26.2532)  acc5: 58.3333 (62.2119)  time: 0.2486  data: 0.1167  max mem: 15572
Test:  [ 920/2442]  eta: 0:06:22  loss: 3.2772 (3.4173)  acc1: 41.6667 (26.3572)  acc5: 66.6667 (62.2240)  time: 0.2486  data: 0.1181  max mem: 15572
Test:  [ 930/2442]  eta: 0:06:19  loss: 3.3595 (3.4178)  acc1: 25.0000 (26.2710)  acc5: 58.3333 (62.1912)  time: 0.2344  data: 0.1057  max mem: 15572
Test:  [ 940/2442]  eta: 0:06:16  loss: 3.4971 (3.4188)  acc1: 16.6667 (26.2132)  acc5: 58.3333 (62.1945)  time: 0.2324  data: 0.1024  max mem: 15572
Test:  [ 950/2442]  eta: 0:06:14  loss: 3.5877 (3.4221)  acc1: 16.6667 (26.1654)  acc5: 58.3333 (62.0487)  time: 0.2641  data: 0.1325  max mem: 15572
Test:  [ 960/2442]  eta: 0:06:11  loss: 3.5877 (3.4247)  acc1: 16.6667 (26.0059)  acc5: 50.0000 (61.9667)  time: 0.2610  data: 0.1316  max mem: 15572
Test:  [ 970/2442]  eta: 0:06:09  loss: 3.7214 (3.4291)  acc1: 8.3333 (25.8926)  acc5: 50.0000 (61.7748)  time: 0.2485  data: 0.1172  max mem: 15572
Test:  [ 980/2442]  eta: 0:06:06  loss: 3.7795 (3.4319)  acc1: 16.6667 (25.7475)  acc5: 50.0000 (61.6463)  time: 0.2455  data: 0.1129  max mem: 15572
Test:  [ 990/2442]  eta: 0:06:04  loss: 3.7140 (3.4329)  acc1: 16.6667 (25.7904)  acc5: 50.0000 (61.5624)  time: 0.2444  data: 0.1091  max mem: 15572
Test:  [1000/2442]  eta: 0:06:01  loss: 3.4465 (3.4331)  acc1: 33.3333 (25.9657)  acc5: 58.3333 (61.5967)  time: 0.2552  data: 0.1119  max mem: 15572
Test:  [1010/2442]  eta: 0:05:59  loss: 3.3770 (3.4320)  acc1: 41.6667 (26.0715)  acc5: 66.6667 (61.6057)  time: 0.2455  data: 0.0903  max mem: 15572
Test:  [1020/2442]  eta: 0:05:56  loss: 3.2511 (3.4326)  acc1: 33.3333 (26.1345)  acc5: 58.3333 (61.5818)  time: 0.2473  data: 0.0961  max mem: 15572
Test:  [1030/2442]  eta: 0:05:54  loss: 3.3953 (3.4336)  acc1: 16.6667 (26.0750)  acc5: 58.3333 (61.4775)  time: 0.2598  data: 0.1231  max mem: 15572
Test:  [1040/2442]  eta: 0:05:51  loss: 3.3384 (3.4301)  acc1: 25.0000 (26.2008)  acc5: 66.6667 (61.5674)  time: 0.2453  data: 0.1026  max mem: 15572
Test:  [1050/2442]  eta: 0:05:48  loss: 3.1945 (3.4303)  acc1: 25.0000 (26.1497)  acc5: 66.6667 (61.5842)  time: 0.2266  data: 0.0761  max mem: 15572
Test:  [1060/2442]  eta: 0:05:46  loss: 3.4725 (3.4314)  acc1: 8.3333 (25.9739)  acc5: 58.3333 (61.4515)  time: 0.2316  data: 0.0903  max mem: 15572
Test:  [1070/2442]  eta: 0:05:43  loss: 3.4683 (3.4309)  acc1: 16.6667 (26.0660)  acc5: 58.3333 (61.4768)  time: 0.2463  data: 0.1101  max mem: 15572
Test:  [1080/2442]  eta: 0:05:40  loss: 3.2384 (3.4303)  acc1: 25.0000 (26.0099)  acc5: 66.6667 (61.5248)  time: 0.2371  data: 0.1053  max mem: 15572
Test:  [1090/2442]  eta: 0:05:38  loss: 3.2412 (3.4336)  acc1: 8.3333 (25.9090)  acc5: 58.3333 (61.3504)  time: 0.2317  data: 0.0908  max mem: 15572
Test:  [1100/2442]  eta: 0:05:35  loss: 3.4526 (3.4351)  acc1: 25.0000 (25.9007)  acc5: 58.3333 (61.2776)  time: 0.2599  data: 0.1137  max mem: 15572
Test:  [1110/2442]  eta: 0:05:33  loss: 3.4227 (3.4335)  acc1: 25.0000 (25.8701)  acc5: 58.3333 (61.2886)  time: 0.2503  data: 0.1091  max mem: 15572
Test:  [1120/2442]  eta: 0:05:30  loss: 3.2479 (3.4347)  acc1: 16.6667 (25.8103)  acc5: 66.6667 (61.2474)  time: 0.2369  data: 0.1010  max mem: 15572
Test:  [1130/2442]  eta: 0:05:28  loss: 3.5322 (3.4365)  acc1: 16.6667 (25.8031)  acc5: 66.6667 (61.2216)  time: 0.2585  data: 0.1192  max mem: 15572
Test:  [1140/2442]  eta: 0:05:25  loss: 3.6388 (3.4390)  acc1: 16.6667 (25.7157)  acc5: 50.0000 (61.0576)  time: 0.2429  data: 0.0937  max mem: 15572
Test:  [1150/2442]  eta: 0:05:22  loss: 3.6156 (3.4398)  acc1: 16.6667 (25.7095)  acc5: 50.0000 (61.0411)  time: 0.2172  data: 0.0717  max mem: 15572
Test:  [1160/2442]  eta: 0:05:20  loss: 3.3634 (3.4392)  acc1: 25.0000 (25.8254)  acc5: 66.6667 (61.1542)  time: 0.2467  data: 0.1087  max mem: 15572
Test:  [1170/2442]  eta: 0:05:17  loss: 3.3525 (3.4392)  acc1: 33.3333 (25.7543)  acc5: 66.6667 (61.1443)  time: 0.2465  data: 0.1007  max mem: 15572
Test:  [1180/2442]  eta: 0:05:15  loss: 3.4092 (3.4378)  acc1: 25.0000 (25.8185)  acc5: 66.6667 (61.1981)  time: 0.2443  data: 0.1046  max mem: 15572
Test:  [1190/2442]  eta: 0:05:12  loss: 3.4079 (3.4395)  acc1: 25.0000 (25.7767)  acc5: 66.6667 (61.1461)  time: 0.2560  data: 0.1288  max mem: 15572
Test:  [1200/2442]  eta: 0:05:09  loss: 3.4017 (3.4371)  acc1: 25.0000 (25.8396)  acc5: 66.6667 (61.3031)  time: 0.2169  data: 0.0780  max mem: 15572
Test:  [1210/2442]  eta: 0:05:07  loss: 3.3643 (3.4372)  acc1: 25.0000 (25.7845)  acc5: 75.0000 (61.2648)  time: 0.2240  data: 0.0839  max mem: 15572
Test:  [1220/2442]  eta: 0:05:04  loss: 3.3564 (3.4371)  acc1: 16.6667 (25.8054)  acc5: 66.6667 (61.3022)  time: 0.2546  data: 0.1195  max mem: 15572
Test:  [1230/2442]  eta: 0:05:01  loss: 3.4508 (3.4380)  acc1: 16.6667 (25.6905)  acc5: 66.6667 (61.2849)  time: 0.2377  data: 0.1087  max mem: 15572
Test:  [1240/2442]  eta: 0:04:59  loss: 3.5506 (3.4392)  acc1: 16.6667 (25.6916)  acc5: 50.0000 (61.2141)  time: 0.2449  data: 0.1160  max mem: 15572
Test:  [1250/2442]  eta: 0:04:57  loss: 3.4704 (3.4396)  acc1: 16.6667 (25.6795)  acc5: 58.3333 (61.2244)  time: 0.2904  data: 0.1493  max mem: 15572
Test:  [1260/2442]  eta: 0:04:54  loss: 3.4704 (3.4403)  acc1: 16.6667 (25.6542)  acc5: 58.3333 (61.2080)  time: 0.2661  data: 0.1216  max mem: 15572
Test:  [1270/2442]  eta: 0:04:52  loss: 3.6631 (3.4432)  acc1: 16.6667 (25.5770)  acc5: 50.0000 (61.0674)  time: 0.2462  data: 0.1022  max mem: 15572
Test:  [1280/2442]  eta: 0:04:49  loss: 3.6852 (3.4443)  acc1: 16.6667 (25.5074)  acc5: 50.0000 (61.0330)  time: 0.2404  data: 0.0991  max mem: 15572
Test:  [1290/2442]  eta: 0:04:46  loss: 3.5579 (3.4443)  acc1: 16.6667 (25.4906)  acc5: 58.3333 (60.9928)  time: 0.2036  data: 0.0658  max mem: 15572
Test:  [1300/2442]  eta: 0:04:44  loss: 3.2993 (3.4430)  acc1: 25.0000 (25.5188)  acc5: 58.3333 (61.0748)  time: 0.2518  data: 0.1145  max mem: 15572
Test:  [1310/2442]  eta: 0:04:42  loss: 3.1619 (3.4428)  acc1: 25.0000 (25.4958)  acc5: 75.0000 (61.0475)  time: 0.2923  data: 0.1542  max mem: 15572
Test:  [1320/2442]  eta: 0:04:39  loss: 3.2297 (3.4416)  acc1: 33.3333 (25.5488)  acc5: 66.6667 (61.0712)  time: 0.2491  data: 0.1010  max mem: 15572
Test:  [1330/2442]  eta: 0:04:37  loss: 3.2028 (3.4406)  acc1: 33.3333 (25.6574)  acc5: 66.6667 (61.1007)  time: 0.2443  data: 0.0970  max mem: 15572
Test:  [1340/2442]  eta: 0:04:34  loss: 3.3750 (3.4421)  acc1: 25.0000 (25.5841)  acc5: 58.3333 (60.9993)  time: 0.2345  data: 0.0962  max mem: 15572
Test:  [1350/2442]  eta: 0:04:32  loss: 3.6081 (3.4424)  acc1: 16.6667 (25.5243)  acc5: 41.6667 (60.9610)  time: 0.2513  data: 0.1207  max mem: 15572
Test:  [1360/2442]  eta: 0:04:30  loss: 3.6081 (3.4448)  acc1: 16.6667 (25.4592)  acc5: 50.0000 (60.8560)  time: 0.2791  data: 0.1439  max mem: 15572
Test:  [1370/2442]  eta: 0:04:27  loss: 3.5999 (3.4462)  acc1: 8.3333 (25.3586)  acc5: 50.0000 (60.8376)  time: 0.2655  data: 0.1201  max mem: 15572
Test:  [1380/2442]  eta: 0:04:25  loss: 3.8927 (3.4497)  acc1: 8.3333 (25.2534)  acc5: 41.6667 (60.6565)  time: 0.2631  data: 0.1195  max mem: 15572
Test:  [1390/2442]  eta: 0:04:22  loss: 3.7718 (3.4506)  acc1: 8.3333 (25.2217)  acc5: 41.6667 (60.6458)  time: 0.2467  data: 0.1095  max mem: 15572
Test:  [1400/2442]  eta: 0:04:20  loss: 3.6642 (3.4510)  acc1: 25.0000 (25.3093)  acc5: 58.3333 (60.6234)  time: 0.2467  data: 0.1157  max mem: 15572
Test:  [1410/2442]  eta: 0:04:18  loss: 3.2443 (3.4502)  acc1: 33.3333 (25.3721)  acc5: 58.3333 (60.6189)  time: 0.2950  data: 0.1602  max mem: 15572
Test:  [1420/2442]  eta: 0:04:15  loss: 3.1009 (3.4469)  acc1: 41.6667 (25.4809)  acc5: 75.0000 (60.7436)  time: 0.2756  data: 0.1460  max mem: 15572
Test:  [1430/2442]  eta: 0:04:13  loss: 3.2051 (3.4491)  acc1: 33.3333 (25.4542)  acc5: 58.3333 (60.6569)  time: 0.2452  data: 0.1140  max mem: 15572
Test:  [1440/2442]  eta: 0:04:10  loss: 3.5143 (3.4483)  acc1: 25.0000 (25.4626)  acc5: 50.0000 (60.6986)  time: 0.2651  data: 0.1261  max mem: 15572
Test:  [1450/2442]  eta: 0:04:08  loss: 3.2795 (3.4472)  acc1: 25.0000 (25.5226)  acc5: 66.6667 (60.7225)  time: 0.2435  data: 0.1012  max mem: 15572
Test:  [1460/2442]  eta: 0:04:05  loss: 3.4108 (3.4469)  acc1: 25.0000 (25.5362)  acc5: 58.3333 (60.7404)  time: 0.2369  data: 0.0915  max mem: 15572
Test:  [1470/2442]  eta: 0:04:02  loss: 3.4425 (3.4472)  acc1: 25.0000 (25.5042)  acc5: 66.6667 (60.7580)  time: 0.2179  data: 0.0758  max mem: 15572
Test:  [1480/2442]  eta: 0:04:00  loss: 3.3529 (3.4461)  acc1: 25.0000 (25.4614)  acc5: 66.6667 (60.8316)  time: 0.2155  data: 0.0865  max mem: 15572
Test:  [1490/2442]  eta: 0:03:57  loss: 3.3488 (3.4467)  acc1: 16.6667 (25.4080)  acc5: 66.6667 (60.8317)  time: 0.2306  data: 0.1027  max mem: 15572
Test:  [1500/2442]  eta: 0:03:55  loss: 3.5409 (3.4485)  acc1: 16.6667 (25.3997)  acc5: 50.0000 (60.7151)  time: 0.2407  data: 0.1015  max mem: 15572
Test:  [1510/2442]  eta: 0:03:52  loss: 3.4715 (3.4493)  acc1: 16.6667 (25.3695)  acc5: 50.0000 (60.6717)  time: 0.2676  data: 0.1377  max mem: 15572
Test:  [1520/2442]  eta: 0:03:50  loss: 3.3477 (3.4488)  acc1: 16.6667 (25.3013)  acc5: 58.3333 (60.6509)  time: 0.2487  data: 0.1250  max mem: 15572
Test:  [1530/2442]  eta: 0:03:47  loss: 3.2456 (3.4490)  acc1: 25.0000 (25.3375)  acc5: 58.3333 (60.6466)  time: 0.2327  data: 0.1070  max mem: 15572
Test:  [1540/2442]  eta: 0:03:45  loss: 3.3551 (3.4509)  acc1: 25.0000 (25.3191)  acc5: 58.3333 (60.5721)  time: 0.2455  data: 0.1177  max mem: 15572
Test:  [1550/2442]  eta: 0:03:42  loss: 3.7448 (3.4518)  acc1: 25.0000 (25.3224)  acc5: 50.0000 (60.5147)  time: 0.2629  data: 0.1321  max mem: 15572
Test:  [1560/2442]  eta: 0:03:40  loss: 3.5936 (3.4523)  acc1: 25.0000 (25.3203)  acc5: 50.0000 (60.5168)  time: 0.2749  data: 0.1426  max mem: 15572
Test:  [1570/2442]  eta: 0:03:37  loss: 3.4181 (3.4512)  acc1: 25.0000 (25.3713)  acc5: 66.6667 (60.6090)  time: 0.2426  data: 0.1011  max mem: 15572
Test:  [1580/2442]  eta: 0:03:35  loss: 3.2051 (3.4493)  acc1: 25.0000 (25.3479)  acc5: 66.6667 (60.6684)  time: 0.2404  data: 0.1053  max mem: 15572
Test:  [1590/2442]  eta: 0:03:32  loss: 3.2432 (3.4497)  acc1: 25.0000 (25.3771)  acc5: 66.6667 (60.7061)  time: 0.2292  data: 0.1000  max mem: 15572
Test:  [1600/2442]  eta: 0:03:29  loss: 3.5365 (3.4506)  acc1: 25.0000 (25.3487)  acc5: 58.3333 (60.6756)  time: 0.2053  data: 0.0683  max mem: 15572
Test:  [1610/2442]  eta: 0:03:27  loss: 3.1445 (3.4480)  acc1: 25.0000 (25.4138)  acc5: 75.0000 (60.8111)  time: 0.2596  data: 0.1248  max mem: 15572
Test:  [1620/2442]  eta: 0:03:24  loss: 3.2576 (3.4488)  acc1: 25.0000 (25.3701)  acc5: 66.6667 (60.7650)  time: 0.2597  data: 0.1259  max mem: 15572
Test:  [1630/2442]  eta: 0:03:22  loss: 3.4508 (3.4485)  acc1: 16.6667 (25.3934)  acc5: 58.3333 (60.8165)  time: 0.2421  data: 0.1101  max mem: 15572
Test:  [1640/2442]  eta: 0:03:19  loss: 3.5894 (3.4498)  acc1: 16.6667 (25.3352)  acc5: 58.3333 (60.7506)  time: 0.2380  data: 0.1058  max mem: 15572
Test:  [1650/2442]  eta: 0:03:17  loss: 3.5301 (3.4496)  acc1: 25.0000 (25.3735)  acc5: 58.3333 (60.7612)  time: 0.2452  data: 0.1125  max mem: 15572
Test:  [1660/2442]  eta: 0:03:14  loss: 3.4053 (3.4486)  acc1: 25.0000 (25.4164)  acc5: 66.6667 (60.8218)  time: 0.2316  data: 0.0949  max mem: 15572
Test:  [1670/2442]  eta: 0:03:12  loss: 3.2988 (3.4490)  acc1: 25.0000 (25.4339)  acc5: 66.6667 (60.8169)  time: 0.2295  data: 0.0834  max mem: 15572
Test:  [1680/2442]  eta: 0:03:09  loss: 3.5942 (3.4501)  acc1: 25.0000 (25.3817)  acc5: 50.0000 (60.7624)  time: 0.2560  data: 0.1078  max mem: 15572
Test:  [1690/2442]  eta: 0:03:07  loss: 3.5664 (3.4496)  acc1: 16.6667 (25.3548)  acc5: 58.3333 (60.8171)  time: 0.2386  data: 0.0924  max mem: 15572
Test:  [1700/2442]  eta: 0:03:04  loss: 3.3278 (3.4482)  acc1: 16.6667 (25.4017)  acc5: 66.6667 (60.8368)  time: 0.2481  data: 0.1081  max mem: 15572
Test:  [1710/2442]  eta: 0:03:02  loss: 3.1916 (3.4476)  acc1: 25.0000 (25.3994)  acc5: 66.6667 (60.8562)  time: 0.2376  data: 0.1039  max mem: 15572
Test:  [1720/2442]  eta: 0:02:59  loss: 3.0732 (3.4454)  acc1: 25.0000 (25.4455)  acc5: 66.6667 (60.9287)  time: 0.2193  data: 0.0828  max mem: 15572
Test:  [1730/2442]  eta: 0:02:56  loss: 2.8963 (3.4435)  acc1: 33.3333 (25.5536)  acc5: 83.3333 (61.0389)  time: 0.2149  data: 0.0839  max mem: 15572
Test:  [1740/2442]  eta: 0:02:54  loss: 2.9800 (3.4419)  acc1: 41.6667 (25.5887)  acc5: 75.0000 (61.0856)  time: 0.2304  data: 0.0997  max mem: 15572
Test:  [1750/2442]  eta: 0:02:52  loss: 3.3392 (3.4425)  acc1: 25.0000 (25.5806)  acc5: 66.6667 (61.0651)  time: 0.2624  data: 0.1239  max mem: 15572
Test:  [1760/2442]  eta: 0:02:49  loss: 3.5684 (3.4431)  acc1: 16.6667 (25.5347)  acc5: 50.0000 (61.0259)  time: 0.2436  data: 0.1023  max mem: 15572
Test:  [1770/2442]  eta: 0:02:46  loss: 3.4715 (3.4436)  acc1: 25.0000 (25.5411)  acc5: 58.3333 (61.0107)  time: 0.2334  data: 0.0793  max mem: 15572
Test:  [1780/2442]  eta: 0:02:44  loss: 3.4990 (3.4450)  acc1: 16.6667 (25.4632)  acc5: 58.3333 (61.0004)  time: 0.2489  data: 0.0999  max mem: 15572
Test:  [1790/2442]  eta: 0:02:42  loss: 3.7430 (3.4467)  acc1: 8.3333 (25.3955)  acc5: 50.0000 (60.8692)  time: 0.2534  data: 0.1210  max mem: 15572
Test:  [1800/2442]  eta: 0:02:39  loss: 3.6024 (3.4456)  acc1: 16.6667 (25.4488)  acc5: 50.0000 (60.9060)  time: 0.2627  data: 0.1279  max mem: 15572
Test:  [1810/2442]  eta: 0:02:37  loss: 3.4119 (3.4461)  acc1: 33.3333 (25.4832)  acc5: 58.3333 (60.8964)  time: 0.2482  data: 0.1129  max mem: 15572
Test:  [1820/2442]  eta: 0:02:34  loss: 3.4012 (3.4449)  acc1: 33.3333 (25.5812)  acc5: 66.6667 (60.9418)  time: 0.2624  data: 0.1258  max mem: 15572
Test:  [1830/2442]  eta: 0:02:32  loss: 3.0820 (3.4432)  acc1: 41.6667 (25.6599)  acc5: 75.0000 (60.9958)  time: 0.2759  data: 0.1255  max mem: 15572
Test:  [1840/2442]  eta: 0:02:29  loss: 3.3674 (3.4436)  acc1: 33.3333 (25.6699)  acc5: 58.3333 (60.9723)  time: 0.2297  data: 0.0778  max mem: 15572
Test:  [1850/2442]  eta: 0:02:27  loss: 3.2336 (3.4414)  acc1: 33.3333 (25.7609)  acc5: 66.6667 (61.0436)  time: 0.2383  data: 0.0966  max mem: 15572
Test:  [1860/2442]  eta: 0:02:24  loss: 3.1066 (3.4412)  acc1: 33.3333 (25.7478)  acc5: 66.6667 (61.0559)  time: 0.2459  data: 0.1025  max mem: 15572
Test:  [1870/2442]  eta: 0:02:22  loss: 3.2326 (3.4399)  acc1: 25.0000 (25.7705)  acc5: 66.6667 (61.1081)  time: 0.2460  data: 0.1043  max mem: 15572
Test:  [1880/2442]  eta: 0:02:19  loss: 3.2201 (3.4397)  acc1: 25.0000 (25.7487)  acc5: 66.6667 (61.1023)  time: 0.2477  data: 0.1096  max mem: 15572
Test:  [1890/2442]  eta: 0:02:17  loss: 3.1984 (3.4380)  acc1: 16.6667 (25.7536)  acc5: 66.6667 (61.1802)  time: 0.2199  data: 0.0785  max mem: 15572
Test:  [1900/2442]  eta: 0:02:14  loss: 3.1639 (3.4376)  acc1: 16.6667 (25.7233)  acc5: 75.0000 (61.2309)  time: 0.2520  data: 0.1037  max mem: 15572
Test:  [1910/2442]  eta: 0:02:12  loss: 3.4160 (3.4396)  acc1: 25.0000 (25.6803)  acc5: 58.3333 (61.1024)  time: 0.2475  data: 0.1061  max mem: 15572
Test:  [1920/2442]  eta: 0:02:09  loss: 3.4160 (3.4396)  acc1: 16.6667 (25.6117)  acc5: 58.3333 (61.0880)  time: 0.2448  data: 0.1055  max mem: 15572
Test:  [1930/2442]  eta: 0:02:07  loss: 3.2367 (3.4398)  acc1: 16.6667 (25.5783)  acc5: 66.6667 (61.0737)  time: 0.2360  data: 0.0868  max mem: 15572
Test:  [1940/2442]  eta: 0:02:04  loss: 3.2685 (3.4396)  acc1: 25.0000 (25.6054)  acc5: 58.3333 (61.0811)  time: 0.2223  data: 0.0787  max mem: 15572
Test:  [1950/2442]  eta: 0:02:02  loss: 3.3953 (3.4405)  acc1: 25.0000 (25.5553)  acc5: 58.3333 (61.0371)  time: 0.2497  data: 0.1035  max mem: 15572
Test:  [1960/2442]  eta: 0:01:59  loss: 3.4591 (3.4402)  acc1: 16.6667 (25.5907)  acc5: 58.3333 (61.0403)  time: 0.2439  data: 0.0963  max mem: 15572
Test:  [1970/2442]  eta: 0:01:57  loss: 3.2930 (3.4395)  acc1: 41.6667 (25.6680)  acc5: 75.0000 (61.0900)  time: 0.2526  data: 0.1093  max mem: 15572
Test:  [1980/2442]  eta: 0:01:54  loss: 3.2288 (3.4383)  acc1: 33.3333 (25.6731)  acc5: 75.0000 (61.1896)  time: 0.2611  data: 0.1195  max mem: 15572
Test:  [1990/2442]  eta: 0:01:52  loss: 3.1066 (3.4365)  acc1: 25.0000 (25.6822)  acc5: 75.0000 (61.2716)  time: 0.2258  data: 0.0786  max mem: 15572
Test:  [2000/2442]  eta: 0:01:49  loss: 3.2746 (3.4365)  acc1: 16.6667 (25.6997)  acc5: 66.6667 (61.2735)  time: 0.2335  data: 0.0950  max mem: 15572
Test:  [2010/2442]  eta: 0:01:47  loss: 3.3317 (3.4362)  acc1: 25.0000 (25.7210)  acc5: 66.6667 (61.2962)  time: 0.2691  data: 0.1364  max mem: 15572
Test:  [2020/2442]  eta: 0:01:44  loss: 3.1634 (3.4347)  acc1: 33.3333 (25.7463)  acc5: 75.0000 (61.3516)  time: 0.2591  data: 0.1260  max mem: 15572
Test:  [2030/2442]  eta: 0:01:42  loss: 3.2038 (3.4345)  acc1: 33.3333 (25.7714)  acc5: 75.0000 (61.3737)  time: 0.2354  data: 0.1120  max mem: 15572
Test:  [2040/2442]  eta: 0:01:39  loss: 3.4049 (3.4345)  acc1: 16.6667 (25.7227)  acc5: 66.6667 (61.3915)  time: 0.2427  data: 0.1085  max mem: 15572
Test:  [2050/2442]  eta: 0:01:37  loss: 3.6050 (3.4355)  acc1: 16.6667 (25.6867)  acc5: 58.3333 (61.3563)  time: 0.2321  data: 0.0954  max mem: 15572
Test:  [2060/2442]  eta: 0:01:34  loss: 3.6050 (3.4359)  acc1: 16.6667 (25.6914)  acc5: 58.3333 (61.3618)  time: 0.2250  data: 0.0893  max mem: 15572
Test:  [2070/2442]  eta: 0:01:32  loss: 3.4648 (3.4360)  acc1: 25.0000 (25.7042)  acc5: 58.3333 (61.3673)  time: 0.2430  data: 0.1015  max mem: 15572
Test:  [2080/2442]  eta: 0:01:29  loss: 3.6182 (3.4376)  acc1: 25.0000 (25.6567)  acc5: 58.3333 (61.3047)  time: 0.2248  data: 0.0929  max mem: 15572
Test:  [2090/2442]  eta: 0:01:27  loss: 3.6603 (3.4386)  acc1: 16.6667 (25.6416)  acc5: 41.6667 (61.2546)  time: 0.2374  data: 0.1104  max mem: 15572
Test:  [2100/2442]  eta: 0:01:24  loss: 3.6289 (3.4389)  acc1: 16.6667 (25.6148)  acc5: 58.3333 (61.2367)  time: 0.2334  data: 0.0984  max mem: 15572
Test:  [2110/2442]  eta: 0:01:22  loss: 3.3105 (3.4376)  acc1: 25.0000 (25.6592)  acc5: 66.6667 (61.3019)  time: 0.2590  data: 0.1163  max mem: 15572
Test:  [2120/2442]  eta: 0:01:19  loss: 3.2733 (3.4378)  acc1: 25.0000 (25.6444)  acc5: 66.6667 (61.2840)  time: 0.2488  data: 0.1085  max mem: 15572
Test:  [2130/2442]  eta: 0:01:17  loss: 3.2931 (3.4371)  acc1: 25.0000 (25.6687)  acc5: 58.3333 (61.3092)  time: 0.2253  data: 0.0812  max mem: 15572
Test:  [2140/2442]  eta: 0:01:14  loss: 3.3005 (3.4373)  acc1: 33.3333 (25.7045)  acc5: 66.6667 (61.3109)  time: 0.2624  data: 0.1141  max mem: 15572
Test:  [2150/2442]  eta: 0:01:12  loss: 3.3403 (3.4371)  acc1: 33.3333 (25.7051)  acc5: 66.6667 (61.3203)  time: 0.2360  data: 0.0961  max mem: 15572
Test:  [2160/2442]  eta: 0:01:09  loss: 3.3903 (3.4377)  acc1: 16.6667 (25.6633)  acc5: 58.3333 (61.3065)  time: 0.2355  data: 0.0974  max mem: 15572
Test:  [2170/2442]  eta: 0:01:07  loss: 3.6779 (3.4386)  acc1: 16.6667 (25.6525)  acc5: 50.0000 (61.2659)  time: 0.2595  data: 0.1189  max mem: 15572
Test:  [2180/2442]  eta: 0:01:04  loss: 3.6035 (3.4400)  acc1: 8.3333 (25.5770)  acc5: 58.3333 (61.2334)  time: 0.2598  data: 0.1165  max mem: 15572
Test:  [2190/2442]  eta: 0:01:02  loss: 3.6458 (3.4415)  acc1: 8.3333 (25.5135)  acc5: 58.3333 (61.1821)  time: 0.2450  data: 0.1118  max mem: 15572
Test:  [2200/2442]  eta: 0:01:00  loss: 3.7493 (3.4429)  acc1: 8.3333 (25.4430)  acc5: 41.6667 (61.1010)  time: 0.2775  data: 0.1513  max mem: 15572
Test:  [2210/2442]  eta: 0:00:57  loss: 3.7205 (3.4434)  acc1: 16.6667 (25.4598)  acc5: 50.0000 (61.0659)  time: 0.2559  data: 0.1289  max mem: 15572
Test:  [2220/2442]  eta: 0:00:55  loss: 3.5244 (3.4438)  acc1: 33.3333 (25.5103)  acc5: 58.3333 (61.0648)  time: 0.2202  data: 0.0918  max mem: 15572
Test:  [2230/2442]  eta: 0:00:52  loss: 3.3538 (3.4433)  acc1: 33.3333 (25.5566)  acc5: 66.6667 (61.0750)  time: 0.2576  data: 0.1197  max mem: 15572
Test:  [2240/2442]  eta: 0:00:50  loss: 3.3538 (3.4433)  acc1: 25.0000 (25.5838)  acc5: 58.3333 (61.0888)  time: 0.2533  data: 0.1152  max mem: 15572
Test:  [2250/2442]  eta: 0:00:47  loss: 3.5438 (3.4439)  acc1: 25.0000 (25.5701)  acc5: 58.3333 (61.0469)  time: 0.2545  data: 0.1166  max mem: 15572
Test:  [2260/2442]  eta: 0:00:45  loss: 3.3411 (3.4426)  acc1: 25.0000 (25.6118)  acc5: 58.3333 (61.0829)  time: 0.2336  data: 0.0947  max mem: 15572
Test:  [2270/2442]  eta: 0:00:42  loss: 3.2956 (3.4428)  acc1: 25.0000 (25.5908)  acc5: 58.3333 (61.0634)  time: 0.2395  data: 0.1080  max mem: 15572
Test:  [2280/2442]  eta: 0:00:40  loss: 3.4862 (3.4432)  acc1: 8.3333 (25.5224)  acc5: 50.0000 (61.0113)  time: 0.2852  data: 0.1491  max mem: 15572
Test:  [2290/2442]  eta: 0:00:37  loss: 3.4720 (3.4434)  acc1: 16.6667 (25.5347)  acc5: 50.0000 (60.9959)  time: 0.2591  data: 0.1163  max mem: 15572
Test:  [2300/2442]  eta: 0:00:35  loss: 3.2549 (3.4429)  acc1: 25.0000 (25.5215)  acc5: 66.6667 (61.0278)  time: 0.2372  data: 0.1032  max mem: 15572
Test:  [2310/2442]  eta: 0:00:32  loss: 3.3426 (3.4438)  acc1: 8.3333 (25.4796)  acc5: 66.6667 (60.9873)  time: 0.2616  data: 0.1316  max mem: 15572
Test:  [2320/2442]  eta: 0:00:30  loss: 3.4486 (3.4447)  acc1: 16.6667 (25.4596)  acc5: 58.3333 (60.9292)  time: 0.2556  data: 0.1154  max mem: 15572
Test:  [2330/2442]  eta: 0:00:27  loss: 3.4131 (3.4444)  acc1: 16.6667 (25.4326)  acc5: 58.3333 (60.9359)  time: 0.2257  data: 0.0786  max mem: 15572
Test:  [2340/2442]  eta: 0:00:25  loss: 3.3171 (3.4448)  acc1: 25.0000 (25.4129)  acc5: 66.6667 (60.9213)  time: 0.2204  data: 0.0799  max mem: 15572
Test:  [2350/2442]  eta: 0:00:22  loss: 3.3376 (3.4455)  acc1: 25.0000 (25.4041)  acc5: 66.6667 (60.8996)  time: 0.2538  data: 0.1124  max mem: 15572
Test:  [2360/2442]  eta: 0:00:20  loss: 3.5838 (3.4469)  acc1: 8.3333 (25.3530)  acc5: 41.6667 (60.8252)  time: 0.2302  data: 0.0812  max mem: 15572
Test:  [2370/2442]  eta: 0:00:17  loss: 3.5203 (3.4473)  acc1: 16.6667 (25.3480)  acc5: 50.0000 (60.7971)  time: 0.2327  data: 0.0870  max mem: 15572
Test:  [2380/2442]  eta: 0:00:15  loss: 3.4932 (3.4470)  acc1: 33.3333 (25.4130)  acc5: 66.6667 (60.8533)  time: 0.2624  data: 0.1252  max mem: 15572
Test:  [2390/2442]  eta: 0:00:12  loss: 3.4455 (3.4469)  acc1: 33.3333 (25.3834)  acc5: 66.6667 (60.8637)  time: 0.2607  data: 0.1273  max mem: 15572
Test:  [2400/2442]  eta: 0:00:10  loss: 3.3994 (3.4460)  acc1: 25.0000 (25.4269)  acc5: 66.6667 (60.9052)  time: 0.2661  data: 0.1290  max mem: 15572
Test:  [2410/2442]  eta: 0:00:07  loss: 3.4365 (3.4469)  acc1: 25.0000 (25.4113)  acc5: 66.6667 (60.8738)  time: 0.2439  data: 0.1103  max mem: 15572
Test:  [2420/2442]  eta: 0:00:05  loss: 3.4847 (3.4462)  acc1: 25.0000 (25.4199)  acc5: 66.6667 (60.9184)  time: 0.2533  data: 0.1138  max mem: 15572
Test:  [2430/2442]  eta: 0:00:02  loss: 3.4847 (3.4461)  acc1: 16.6667 (25.3736)  acc5: 66.6667 (60.9077)  time: 0.2519  data: 0.1050  max mem: 15572
Test:  [2440/2442]  eta: 0:00:00  loss: 3.5229 (3.4461)  acc1: 16.6667 (25.3858)  acc5: 58.3333 (60.9279)  time: 0.1835  data: 0.0605  max mem: 15572
Test:  [2441/2442]  eta: 0:00:00  loss: 3.5339 (3.4463)  acc1: 16.6667 (25.3840)  acc5: 58.3333 (60.9257)  time: 0.1554  data: 0.0360  max mem: 15572
Test: Total time: 0:10:04 (0.2477 s / it)
* Acc@1 25.384 Acc@5 60.926 loss 3.446
Start merging results...
Reading individual output files
Computing final results
4883
Accuracy of the network on the 29298 test videos: Top-1: 28.30%, Top-5: 65.74%
Training time 6:07:00
