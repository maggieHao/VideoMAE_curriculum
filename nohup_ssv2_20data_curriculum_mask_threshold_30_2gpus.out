/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-17 07:51:43,124] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 07:51:43,124] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=30, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa34814c3d0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-17 07:51:45,844] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-17 07:51:45,844] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-17 07:51:45,856] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-17 07:51:45,856] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-17 07:51:45,898] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.038413286209106445 seconds
[2025-01-17 07:51:46,100] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-17 07:51:46,100] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-17 07:51:46,102] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-17 07:51:46,102] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-17 07:51:46,108] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-17 07:51:46,108] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-17 07:51:46,108] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-17 07:51:46,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 07:51:46,108] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-17 07:51:46,108] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-17 07:51:46,108] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-17 07:51:46,108] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-17 07:51:46,108] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-17 07:51:46,108] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa34544ee60>
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-17 07:51:46,109] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-17 07:51:46,110] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-17 07:51:46,110] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-17 07:51:46,110] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-17 07:51:46,110] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 30
Epoch: [0]  [   0/1404]  eta: 5:39:57  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 14.5285  data: 5.0714  max mem: 15572
Epoch: [0]  [  10/1404]  eta: 0:39:27  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0038)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6982  data: 0.4614  max mem: 15572
Epoch: [0]  [  20/1404]  eta: 0:25:30  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0040)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4349  data: 0.0005  max mem: 15572
Epoch: [0]  [  30/1404]  eta: 0:20:55  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0054)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0007  max mem: 15572
Epoch: [0]  [  40/1404]  eta: 0:18:21  lr: 0.000001  min_lr: 0.000000  loss: 5.1599 (5.1600)  class_acc: 0.0000 (0.0051)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0008  max mem: 15572
Epoch: [0]  [  50/1404]  eta: 0:16:51  lr: 0.000001  min_lr: 0.000000  loss: 5.1593 (5.1598)  class_acc: 0.0000 (0.0090)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0008  max mem: 15572
Epoch: [0]  [  60/1404]  eta: 0:16:36  lr: 0.000001  min_lr: 0.000000  loss: 5.1588 (5.1596)  class_acc: 0.0000 (0.0096)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0434  max mem: 15572
Epoch: [0]  [  70/1404]  eta: 0:15:46  lr: 0.000001  min_lr: 0.000000  loss: 5.1585 (5.1594)  class_acc: 0.0000 (0.0123)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.0559  max mem: 15572
Epoch: [0]  [  80/1404]  eta: 0:15:16  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  class_acc: 0.0000 (0.0123)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0649  max mem: 15572
Epoch: [0]  [  90/1404]  eta: 0:15:06  lr: 0.000001  min_lr: 0.000000  loss: 5.1579 (5.1591)  class_acc: 0.0000 (0.0133)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1334  max mem: 15572
Epoch: [0]  [ 100/1404]  eta: 0:14:48  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1591)  class_acc: 0.0000 (0.0153)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1446  max mem: 15572
Epoch: [0]  [ 110/1404]  eta: 0:14:42  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1589)  class_acc: 0.0000 (0.0146)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1497  max mem: 15572
Epoch: [0]  [ 120/1404]  eta: 0:14:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1574 (5.1588)  class_acc: 0.0000 (0.0148)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1152  max mem: 15572
[2025-01-17 07:53:12,436] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:53:12,436] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:53:12,436] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
[2025-01-17 07:53:12,436] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/1404]  eta: 0:14:07  lr: 0.000002  min_lr: 0.000000  loss: 5.1565 (5.1586)  class_acc: 0.0000 (0.0143)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0893  max mem: 15572
Epoch: [0]  [ 140/1404]  eta: 0:14:00  lr: 0.000002  min_lr: 0.000000  loss: 5.1563 (5.1584)  class_acc: 0.0000 (0.0136)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1424  max mem: 15572
Epoch: [0]  [ 150/1404]  eta: 0:13:52  lr: 0.000002  min_lr: 0.000000  loss: 5.1554 (5.1582)  class_acc: 0.0000 (0.0135)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.1410  max mem: 15572
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4045940 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4045941 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 4045937 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-17 07:53:32,004] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 07:53:32,009] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=30, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f4a843403d0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
[2025-01-17 07:53:34,883] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-17 07:53:34,883] [INFO] [comm.py:637:init_distributed] cdb=None
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00009375
Batch size = 24
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 1404
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-17 07:53:34,900] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-17 07:53:34,900] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-17 07:53:34,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.03939199447631836 seconds
[2025-01-17 07:53:35,138] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-17 07:53:35,138] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-17 07:53:35,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-17 07:53:35,141] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-17 07:53:35,147] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-17 07:53:35,147] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-17 07:53:35,147] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-17 07:53:35,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 07:53:35,147] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4a7a17ae60>
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-17 07:53:35,147] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   train_batch_size ............. 24
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   world_size ................... 2
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-17 07:53:35,148] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-17 07:53:35,148] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 7020
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 30
Epoch: [0]  [   0/1404]  eta: 5:36:06  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 14.3639  data: 9.0715  max mem: 15572
Epoch: [0]  [  10/1404]  eta: 0:39:10  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0038)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6863  data: 0.8251  max mem: 15572
Epoch: [0]  [  20/1404]  eta: 0:25:35  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0040)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4467  data: 0.0006  max mem: 15572
Epoch: [0]  [  30/1404]  eta: 0:20:51  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0054)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0007  max mem: 15572
Epoch: [0]  [  40/1404]  eta: 0:18:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1599 (5.1600)  class_acc: 0.0000 (0.0051)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0008  max mem: 15572
Epoch: [0]  [  50/1404]  eta: 0:16:42  lr: 0.000001  min_lr: 0.000000  loss: 5.1593 (5.1598)  class_acc: 0.0000 (0.0090)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4763  data: 0.0011  max mem: 15572
Epoch: [0]  [  60/1404]  eta: 0:16:09  lr: 0.000001  min_lr: 0.000000  loss: 5.1588 (5.1596)  class_acc: 0.0000 (0.0096)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0009  max mem: 15572
Epoch: [0]  [  70/1404]  eta: 0:15:50  lr: 0.000001  min_lr: 0.000000  loss: 5.1585 (5.1594)  class_acc: 0.0000 (0.0123)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0245  max mem: 15572
Epoch: [0]  [  80/1404]  eta: 0:15:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  class_acc: 0.0000 (0.0123)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.0908  max mem: 15572
Epoch: [0]  [  90/1404]  eta: 0:15:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1579 (5.1591)  class_acc: 0.0000 (0.0133)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1408  max mem: 15572
Epoch: [0]  [ 100/1404]  eta: 0:14:54  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1591)  class_acc: 0.0000 (0.0153)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1219  max mem: 15572
Epoch: [0]  [ 110/1404]  eta: 0:14:46  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1589)  class_acc: 0.0000 (0.0146)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.0726  max mem: 15572
Epoch: [0]  [ 120/1404]  eta: 0:14:23  lr: 0.000002  min_lr: 0.000000  loss: 5.1574 (5.1588)  class_acc: 0.0000 (0.0148)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0422  max mem: 15572
[2025-01-17 07:55:02,470] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:55:02,470] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
[2025-01-17 07:55:02,490] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:55:02,491] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/1404]  eta: 0:14:17  lr: 0.000002  min_lr: 0.000000  loss: 5.1565 (5.1586)  class_acc: 0.0000 (0.0143)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0610  max mem: 15572
Epoch: [0]  [ 140/1404]  eta: 0:14:06  lr: 0.000002  min_lr: 0.000000  loss: 5.1563 (5.1584)  class_acc: 0.0000 (0.0136)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0569  max mem: 15572
Epoch: [0]  [ 150/1404]  eta: 0:13:50  lr: 0.000002  min_lr: 0.000000  loss: 5.1554 (5.1582)  class_acc: 0.0000 (0.0135)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0250  max mem: 15572
Epoch: [0]  [ 160/1404]  eta: 0:13:45  lr: 0.000002  min_lr: 0.000000  loss: 5.1550 (5.1580)  class_acc: 0.0000 (0.0137)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0334  max mem: 15572
Epoch: [0]  [ 170/1404]  eta: 0:13:39  lr: 0.000002  min_lr: 0.000000  loss: 5.1546 (5.1577)  class_acc: 0.0000 (0.0139)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.0219  max mem: 15572
Epoch: [0]  [ 180/1404]  eta: 0:13:25  lr: 0.000002  min_lr: 0.000000  loss: 5.1536 (5.1575)  class_acc: 0.0000 (0.0143)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0007  max mem: 15572
Epoch: [0]  [ 190/1404]  eta: 0:13:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1531 (5.1572)  class_acc: 0.0000 (0.0142)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0008  max mem: 15572
Epoch: [0]  [ 200/1404]  eta: 0:13:06  lr: 0.000003  min_lr: 0.000000  loss: 5.1512 (5.1568)  class_acc: 0.0000 (0.0139)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0009  max mem: 15572
Epoch: [0]  [ 210/1404]  eta: 0:12:51  lr: 0.000003  min_lr: 0.000000  loss: 5.1500 (5.1565)  class_acc: 0.0000 (0.0140)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0007  max mem: 15572
Epoch: [0]  [ 220/1404]  eta: 0:12:42  lr: 0.000003  min_lr: 0.000000  loss: 5.1483 (5.1562)  class_acc: 0.0000 (0.0147)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0202  max mem: 15572
Epoch: [0]  [ 230/1404]  eta: 0:12:36  lr: 0.000003  min_lr: 0.000000  loss: 5.1476 (5.1558)  class_acc: 0.0000 (0.0148)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0961  max mem: 15572
Epoch: [0]  [ 240/1404]  eta: 0:12:28  lr: 0.000003  min_lr: 0.000000  loss: 5.1428 (5.1552)  class_acc: 0.0000 (0.0145)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1300  max mem: 15572
Epoch: [0]  [ 250/1404]  eta: 0:12:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1423 (5.1547)  class_acc: 0.0000 (0.0146)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1076  max mem: 15572
[2025-01-17 07:56:20,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:56:20,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
[2025-01-17 07:56:20,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:56:20,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/1404]  eta: 0:12:14  lr: 0.000003  min_lr: 0.000000  loss: 5.1404 (5.1541)  class_acc: 0.0000 (0.0144)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0844  max mem: 15572
Epoch: [0]  [ 270/1404]  eta: 0:12:04  lr: 0.000004  min_lr: 0.000000  loss: 5.1373 (5.1535)  class_acc: 0.0000 (0.0145)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0560  max mem: 15572
Epoch: [0]  [ 280/1404]  eta: 0:11:57  lr: 0.000004  min_lr: 0.000000  loss: 5.1362 (5.1531)  class_acc: 0.0000 (0.0151)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0287  max mem: 15572
Epoch: [0]  [ 290/1404]  eta: 0:11:51  lr: 0.000004  min_lr: 0.000000  loss: 5.1402 (5.1526)  class_acc: 0.0000 (0.0146)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.0038  max mem: 15572
Epoch: [0]  [ 300/1404]  eta: 0:11:43  lr: 0.000004  min_lr: 0.000000  loss: 5.1323 (5.1518)  class_acc: 0.0000 (0.0147)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0009  max mem: 15572
Epoch: [0]  [ 310/1404]  eta: 0:11:35  lr: 0.000004  min_lr: 0.000000  loss: 5.1250 (5.1511)  class_acc: 0.0000 (0.0147)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0007  max mem: 15572
Epoch: [0]  [ 320/1404]  eta: 0:11:27  lr: 0.000004  min_lr: 0.000000  loss: 5.1309 (5.1504)  class_acc: 0.0000 (0.0148)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0006  max mem: 15572
Epoch: [0]  [ 330/1404]  eta: 0:11:23  lr: 0.000004  min_lr: 0.000000  loss: 5.1295 (5.1496)  class_acc: 0.0000 (0.0162)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0324  max mem: 15572
Epoch: [0]  [ 340/1404]  eta: 0:11:17  lr: 0.000005  min_lr: 0.000000  loss: 5.1108 (5.1484)  class_acc: 0.0000 (0.0167)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.1007  max mem: 15572
Epoch: [0]  [ 350/1404]  eta: 0:11:07  lr: 0.000005  min_lr: 0.000000  loss: 5.1158 (5.1477)  class_acc: 0.0000 (0.0163)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.0691  max mem: 15572
Epoch: [0]  [ 360/1404]  eta: 0:10:58  lr: 0.000005  min_lr: 0.000000  loss: 5.1197 (5.1468)  class_acc: 0.0000 (0.0165)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0006  max mem: 15572
Epoch: [0]  [ 370/1404]  eta: 0:10:50  lr: 0.000005  min_lr: 0.000000  loss: 5.1111 (5.1458)  class_acc: 0.0000 (0.0163)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0242  max mem: 15572
Epoch: [0]  [ 380/1404]  eta: 0:10:46  lr: 0.000005  min_lr: 0.000000  loss: 5.1024 (5.1447)  class_acc: 0.0000 (0.0159)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0567  max mem: 15572
[2025-01-17 07:57:39,198] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:57:39,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2025-01-17 07:57:39,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:57:39,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/1404]  eta: 0:10:40  lr: 0.000005  min_lr: 0.000000  loss: 5.1010 (5.1436)  class_acc: 0.0000 (0.0163)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.0822  max mem: 15572
Epoch: [0]  [ 400/1404]  eta: 0:10:31  lr: 0.000005  min_lr: 0.000000  loss: 5.1010 (5.1426)  class_acc: 0.0000 (0.0162)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0498  max mem: 15572
Epoch: [0]  [ 410/1404]  eta: 0:10:25  lr: 0.000005  min_lr: 0.000000  loss: 5.1009 (5.1419)  class_acc: 0.0000 (0.0160)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0326  max mem: 15572
Epoch: [0]  [ 420/1404]  eta: 0:10:21  lr: 0.000006  min_lr: 0.000000  loss: 5.1009 (5.1407)  class_acc: 0.0000 (0.0162)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.6779  data: 0.1183  max mem: 15572
Epoch: [0]  [ 430/1404]  eta: 0:10:13  lr: 0.000006  min_lr: 0.000000  loss: 5.0888 (5.1395)  class_acc: 0.0000 (0.0164)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.1353  max mem: 15572
Epoch: [0]  [ 440/1404]  eta: 0:10:07  lr: 0.000006  min_lr: 0.000000  loss: 5.0852 (5.1385)  class_acc: 0.0000 (0.0172)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1130  max mem: 15572
Epoch: [0]  [ 450/1404]  eta: 0:10:01  lr: 0.000006  min_lr: 0.000000  loss: 5.0882 (5.1376)  class_acc: 0.0000 (0.0176)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1112  max mem: 15572
Epoch: [0]  [ 460/1404]  eta: 0:09:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0989 (5.1369)  class_acc: 0.0000 (0.0175)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1006  max mem: 15572
Epoch: [0]  [ 470/1404]  eta: 0:09:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0989 (5.1359)  class_acc: 0.0000 (0.0179)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1230  max mem: 15572
Epoch: [0]  [ 480/1404]  eta: 0:09:41  lr: 0.000006  min_lr: 0.000000  loss: 5.0973 (5.1350)  class_acc: 0.0000 (0.0178)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1203  max mem: 15572
Epoch: [0]  [ 490/1404]  eta: 0:09:33  lr: 0.000007  min_lr: 0.000000  loss: 5.0796 (5.1340)  class_acc: 0.0000 (0.0175)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0987  max mem: 15572
Epoch: [0]  [ 500/1404]  eta: 0:09:28  lr: 0.000007  min_lr: 0.000000  loss: 5.0696 (5.1327)  class_acc: 0.0000 (0.0175)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1532  max mem: 15572
Epoch: [0]  [ 510/1404]  eta: 0:09:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0646 (5.1316)  class_acc: 0.0000 (0.0176)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1054  max mem: 15572
[2025-01-17 07:58:56,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:58:56,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2025-01-17 07:58:56,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 07:58:56,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/1404]  eta: 0:09:11  lr: 0.000007  min_lr: 0.000000  loss: 5.0646 (5.1304)  class_acc: 0.0000 (0.0174)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4885  data: 0.0125  max mem: 15572
Epoch: [0]  [ 530/1404]  eta: 0:09:05  lr: 0.000007  min_lr: 0.000000  loss: 5.0805 (5.1298)  class_acc: 0.0000 (0.0176)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0952  max mem: 15572
Epoch: [0]  [ 540/1404]  eta: 0:08:58  lr: 0.000007  min_lr: 0.000000  loss: 5.0629 (5.1283)  class_acc: 0.0000 (0.0177)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1143  max mem: 15572
Epoch: [0]  [ 550/1404]  eta: 0:08:52  lr: 0.000007  min_lr: 0.000000  loss: 5.0627 (5.1274)  class_acc: 0.0000 (0.0178)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1031  max mem: 15572
Epoch: [0]  [ 560/1404]  eta: 0:08:45  lr: 0.000007  min_lr: 0.000000  loss: 5.0749 (5.1264)  class_acc: 0.0000 (0.0177)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0854  max mem: 15572
Epoch: [0]  [ 570/1404]  eta: 0:08:39  lr: 0.000008  min_lr: 0.000000  loss: 5.0551 (5.1254)  class_acc: 0.0000 (0.0177)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0460  max mem: 15572
Epoch: [0]  [ 580/1404]  eta: 0:08:33  lr: 0.000008  min_lr: 0.000000  loss: 5.0525 (5.1244)  class_acc: 0.0000 (0.0175)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0330  max mem: 15572
Epoch: [0]  [ 590/1404]  eta: 0:08:26  lr: 0.000008  min_lr: 0.000000  loss: 5.0562 (5.1232)  class_acc: 0.0000 (0.0179)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0081  max mem: 15572
Epoch: [0]  [ 600/1404]  eta: 0:08:21  lr: 0.000008  min_lr: 0.000000  loss: 5.0506 (5.1219)  class_acc: 0.0000 (0.0177)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.0081  max mem: 15572
Epoch: [0]  [ 610/1404]  eta: 0:08:13  lr: 0.000008  min_lr: 0.000000  loss: 5.0544 (5.1212)  class_acc: 0.0000 (0.0175)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0008  max mem: 15572
Epoch: [0]  [ 620/1404]  eta: 0:08:07  lr: 0.000008  min_lr: 0.000000  loss: 5.0772 (5.1204)  class_acc: 0.0000 (0.0176)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0007  max mem: 15572
Epoch: [0]  [ 630/1404]  eta: 0:08:01  lr: 0.000008  min_lr: 0.000000  loss: 5.0539 (5.1193)  class_acc: 0.0000 (0.0173)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0139  max mem: 15572
[2025-01-17 08:00:14,261] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:00:14,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/1404]  eta: 0:07:55  lr: 0.000009  min_lr: 0.000000  loss: 5.0800 (5.1187)  class_acc: 0.0000 (0.0174)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0346  max mem: 15572
[2025-01-17 08:00:14,289] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:00:14,291] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 650/1404]  eta: 0:07:49  lr: 0.000009  min_lr: 0.000000  loss: 5.0812 (5.1178)  class_acc: 0.0000 (0.0174)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.0215  max mem: 15572
Epoch: [0]  [ 660/1404]  eta: 0:07:43  lr: 0.000009  min_lr: 0.000000  loss: 5.0291 (5.1162)  class_acc: 0.0000 (0.0177)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0007  max mem: 15572
Epoch: [0]  [ 670/1404]  eta: 0:07:36  lr: 0.000009  min_lr: 0.000000  loss: 5.0094 (5.1152)  class_acc: 0.0000 (0.0176)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0006  max mem: 15572
Epoch: [0]  [ 680/1404]  eta: 0:07:30  lr: 0.000009  min_lr: 0.000000  loss: 5.0184 (5.1140)  class_acc: 0.0000 (0.0177)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.0006  max mem: 15572
Epoch: [0]  [ 690/1404]  eta: 0:07:23  lr: 0.000009  min_lr: 0.000000  loss: 5.0184 (5.1130)  class_acc: 0.0000 (0.0180)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0007  max mem: 15572
Epoch: [0]  [ 700/1404]  eta: 0:07:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.1121)  class_acc: 0.0000 (0.0178)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0008  max mem: 15572
Epoch: [0]  [ 710/1404]  eta: 0:07:10  lr: 0.000009  min_lr: 0.000000  loss: 5.0339 (5.1110)  class_acc: 0.0000 (0.0182)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.0007  max mem: 15572
Epoch: [0]  [ 720/1404]  eta: 0:07:04  lr: 0.000010  min_lr: 0.000000  loss: 5.0251 (5.1100)  class_acc: 0.0000 (0.0181)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0007  max mem: 15572
Epoch: [0]  [ 730/1404]  eta: 0:06:58  lr: 0.000010  min_lr: 0.000000  loss: 5.0358 (5.1094)  class_acc: 0.0000 (0.0182)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0013  max mem: 15572
Epoch: [0]  [ 740/1404]  eta: 0:06:52  lr: 0.000010  min_lr: 0.000000  loss: 5.0506 (5.1088)  class_acc: 0.0000 (0.0181)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0013  max mem: 15572
Epoch: [0]  [ 750/1404]  eta: 0:06:46  lr: 0.000010  min_lr: 0.000000  loss: 5.0349 (5.1073)  class_acc: 0.0000 (0.0182)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0006  max mem: 15572
Epoch: [0]  [ 760/1404]  eta: 0:06:40  lr: 0.000010  min_lr: 0.000000  loss: 5.0390 (5.1068)  class_acc: 0.0000 (0.0181)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.0005  max mem: 15572
[2025-01-17 08:01:33,673] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:01:33,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2025-01-17 08:01:33,673] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:01:33,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/1404]  eta: 0:06:33  lr: 0.000010  min_lr: 0.000000  loss: 5.0466 (5.1059)  class_acc: 0.0000 (0.0181)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0006  max mem: 15572
Epoch: [0]  [ 780/1404]  eta: 0:06:27  lr: 0.000010  min_lr: 0.000000  loss: 5.0185 (5.1047)  class_acc: 0.0000 (0.0178)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.0007  max mem: 15572
Epoch: [0]  [ 790/1404]  eta: 0:06:21  lr: 0.000011  min_lr: 0.000000  loss: 5.0088 (5.1036)  class_acc: 0.0000 (0.0178)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0010  max mem: 15572
Epoch: [0]  [ 800/1404]  eta: 0:06:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9854 (5.1023)  class_acc: 0.0000 (0.0179)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.0009  max mem: 15572
Epoch: [0]  [ 810/1404]  eta: 0:06:08  lr: 0.000011  min_lr: 0.000000  loss: 5.0443 (5.1017)  class_acc: 0.0000 (0.0178)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0006  max mem: 15572
Epoch: [0]  [ 820/1404]  eta: 0:06:02  lr: 0.000011  min_lr: 0.000000  loss: 5.0386 (5.1006)  class_acc: 0.0000 (0.0176)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0009  max mem: 15572
Epoch: [0]  [ 830/1404]  eta: 0:05:55  lr: 0.000011  min_lr: 0.000000  loss: 4.9705 (5.0993)  class_acc: 0.0000 (0.0175)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0009  max mem: 15572
Epoch: [0]  [ 840/1404]  eta: 0:05:49  lr: 0.000011  min_lr: 0.000000  loss: 5.0180 (5.0986)  class_acc: 0.0000 (0.0176)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0006  max mem: 15572
Epoch: [0]  [ 850/1404]  eta: 0:05:43  lr: 0.000011  min_lr: 0.000000  loss: 5.0180 (5.0978)  class_acc: 0.0000 (0.0177)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0006  max mem: 15572
Epoch: [0]  [ 860/1404]  eta: 0:05:36  lr: 0.000011  min_lr: 0.000000  loss: 5.0143 (5.0966)  class_acc: 0.0000 (0.0178)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0005  max mem: 15572
Epoch: [0]  [ 870/1404]  eta: 0:05:30  lr: 0.000012  min_lr: 0.000000  loss: 5.0040 (5.0955)  class_acc: 0.0000 (0.0177)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0004  max mem: 15572
Epoch: [0]  [ 880/1404]  eta: 0:05:24  lr: 0.000012  min_lr: 0.000000  loss: 4.9828 (5.0947)  class_acc: 0.0000 (0.0176)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.0005  max mem: 15572
Epoch: [0]  [ 890/1404]  eta: 0:05:18  lr: 0.000012  min_lr: 0.000000  loss: 4.9824 (5.0936)  class_acc: 0.0000 (0.0178)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0008  max mem: 15572
[2025-01-17 08:02:51,079] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:02:51,079] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2025-01-17 08:02:51,138] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:02:51,139] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/1404]  eta: 0:05:11  lr: 0.000012  min_lr: 0.000000  loss: 5.0013 (5.0932)  class_acc: 0.0000 (0.0177)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0009  max mem: 15572
Epoch: [0]  [ 910/1404]  eta: 0:05:05  lr: 0.000012  min_lr: 0.000000  loss: 5.0432 (5.0926)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0007  max mem: 15572
Epoch: [0]  [ 920/1404]  eta: 0:04:58  lr: 0.000012  min_lr: 0.000000  loss: 5.0424 (5.0924)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0007  max mem: 15572
Epoch: [0]  [ 930/1404]  eta: 0:04:52  lr: 0.000012  min_lr: 0.000000  loss: 5.0337 (5.0920)  class_acc: 0.0000 (0.0174)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0007  max mem: 15572
Epoch: [0]  [ 940/1404]  eta: 0:04:46  lr: 0.000013  min_lr: 0.000000  loss: 5.0094 (5.0910)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0007  max mem: 15572
Epoch: [0]  [ 950/1404]  eta: 0:04:39  lr: 0.000013  min_lr: 0.000000  loss: 5.0094 (5.0905)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0008  max mem: 15572
Epoch: [0]  [ 960/1404]  eta: 0:04:33  lr: 0.000013  min_lr: 0.000000  loss: 5.0361 (5.0899)  class_acc: 0.0000 (0.0174)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0009  max mem: 15572
Epoch: [0]  [ 970/1404]  eta: 0:04:27  lr: 0.000013  min_lr: 0.000000  loss: 5.0396 (5.0890)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0008  max mem: 15572
Epoch: [0]  [ 980/1404]  eta: 0:04:21  lr: 0.000013  min_lr: 0.000000  loss: 5.0270 (5.0883)  class_acc: 0.0000 (0.0176)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0006  max mem: 15572
Epoch: [0]  [ 990/1404]  eta: 0:04:15  lr: 0.000013  min_lr: 0.000000  loss: 5.0270 (5.0879)  class_acc: 0.0000 (0.0176)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.0005  max mem: 15572
[2025-01-17 08:03:53,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.2928139878801235e-07, 1.2928139878801235e-07, 1.8468771255430337e-07, 1.8468771255430337e-07, 2.638395893632906e-07, 2.638395893632906e-07, 3.7691369909041513e-07, 3.7691369909041513e-07, 5.384481415577359e-07, 5.384481415577359e-07, 7.692116307967656e-07, 7.692116307967656e-07, 1.0988737582810937e-06, 1.0988737582810937e-06, 1.569819654687277e-06, 1.569819654687277e-06, 2.24259950669611e-06, 2.24259950669611e-06, 3.2037135809944434e-06, 3.2037135809944434e-06, 4.5767336871349184e-06, 4.5767336871349184e-06, 6.538190981621313e-06, 6.538190981621313e-06, 9.34027283088759e-06, 9.34027283088759e-06, 1.3343246901267988e-05, 1.3343246901267988e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:03:53,137] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=44.51082390169783, CurrSamplesPerSec=44.189524975965654, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/1404]  eta: 0:04:09  lr: 0.000013  min_lr: 0.000000  loss: 5.0556 (5.0875)  class_acc: 0.0000 (0.0174)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0006  max mem: 15572
Epoch: [0]  [1010/1404]  eta: 0:04:03  lr: 0.000013  min_lr: 0.000000  loss: 5.0640 (5.0872)  class_acc: 0.0000 (0.0174)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0007  max mem: 15572
Epoch: [0]  [1020/1404]  eta: 0:03:57  lr: 0.000014  min_lr: 0.000000  loss: 5.0430 (5.0863)  class_acc: 0.0000 (0.0176)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0007  max mem: 15572
[2025-01-17 08:04:08,132] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:04:08,132] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2025-01-17 08:04:08,132] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:04:08,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/1404]  eta: 0:03:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9590 (5.0852)  class_acc: 0.0000 (0.0175)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.0008  max mem: 15572
Epoch: [0]  [1040/1404]  eta: 0:03:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9700 (5.0848)  class_acc: 0.0000 (0.0175)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0068  max mem: 15572
Epoch: [0]  [1050/1404]  eta: 0:03:38  lr: 0.000014  min_lr: 0.000000  loss: 5.0628 (5.0842)  class_acc: 0.0000 (0.0173)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0068  max mem: 15572
Epoch: [0]  [1060/1404]  eta: 0:03:31  lr: 0.000014  min_lr: 0.000000  loss: 5.0158 (5.0834)  class_acc: 0.0000 (0.0172)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0076  max mem: 15572
Epoch: [0]  [1070/1404]  eta: 0:03:25  lr: 0.000014  min_lr: 0.000000  loss: 4.9794 (5.0826)  class_acc: 0.0000 (0.0174)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0367  max mem: 15572
Epoch: [0]  [1080/1404]  eta: 0:03:19  lr: 0.000014  min_lr: 0.000000  loss: 5.0309 (5.0819)  class_acc: 0.0000 (0.0173)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.0375  max mem: 15572
Epoch: [0]  [1090/1404]  eta: 0:03:13  lr: 0.000015  min_lr: 0.000000  loss: 5.0742 (5.0823)  class_acc: 0.0000 (0.0172)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0084  max mem: 15572
Epoch: [0]  [1100/1404]  eta: 0:03:07  lr: 0.000015  min_lr: 0.000000  loss: 5.0284 (5.0813)  class_acc: 0.0000 (0.0173)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0006  max mem: 15572
Epoch: [0]  [1110/1404]  eta: 0:03:01  lr: 0.000015  min_lr: 0.000000  loss: 4.9885 (5.0801)  class_acc: 0.0000 (0.0176)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0008  max mem: 15572
Epoch: [0]  [1120/1404]  eta: 0:02:54  lr: 0.000015  min_lr: 0.000000  loss: 4.9556 (5.0791)  class_acc: 0.0000 (0.0176)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.0007  max mem: 15572
Epoch: [0]  [1130/1404]  eta: 0:02:48  lr: 0.000015  min_lr: 0.000000  loss: 4.9694 (5.0784)  class_acc: 0.0000 (0.0175)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0005  max mem: 15572
Epoch: [0]  [1140/1404]  eta: 0:02:42  lr: 0.000015  min_lr: 0.000000  loss: 4.9774 (5.0776)  class_acc: 0.0000 (0.0177)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0005  max mem: 15572
Epoch: [0]  [1150/1404]  eta: 0:02:36  lr: 0.000015  min_lr: 0.000000  loss: 4.9774 (5.0766)  class_acc: 0.0000 (0.0177)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0008  max mem: 15572
[2025-01-17 08:05:25,475] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:05:25,476] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2025-01-17 08:05:25,476] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:05:25,476] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/1404]  eta: 0:02:30  lr: 0.000015  min_lr: 0.000000  loss: 4.9715 (5.0759)  class_acc: 0.0000 (0.0176)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.0008  max mem: 15572
Epoch: [0]  [1170/1404]  eta: 0:02:23  lr: 0.000016  min_lr: 0.000000  loss: 5.0212 (5.0756)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0006  max mem: 15572
Epoch: [0]  [1180/1404]  eta: 0:02:17  lr: 0.000016  min_lr: 0.000000  loss: 5.0263 (5.0750)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0028  max mem: 15572
Epoch: [0]  [1190/1404]  eta: 0:02:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0165 (5.0745)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5144  data: 0.0028  max mem: 15572
Epoch: [0]  [1200/1404]  eta: 0:02:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9971 (5.0739)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0007  max mem: 15572
Epoch: [0]  [1210/1404]  eta: 0:01:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9716 (5.0730)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0006  max mem: 15572
Epoch: [0]  [1220/1404]  eta: 0:01:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9630 (5.0724)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.0007  max mem: 15572
Epoch: [0]  [1230/1404]  eta: 0:01:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9569 (5.0718)  class_acc: 0.0000 (0.0175)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0007  max mem: 15572
Epoch: [0]  [1240/1404]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 4.9424 (5.0710)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0007  max mem: 15572
Epoch: [0]  [1250/1404]  eta: 0:01:34  lr: 0.000017  min_lr: 0.000000  loss: 4.9991 (5.0704)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.0007  max mem: 15572
Epoch: [0]  [1260/1404]  eta: 0:01:28  lr: 0.000017  min_lr: 0.000000  loss: 4.9991 (5.0696)  class_acc: 0.0000 (0.0178)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0008  max mem: 15572
Epoch: [0]  [1270/1404]  eta: 0:01:22  lr: 0.000017  min_lr: 0.000000  loss: 4.9727 (5.0689)  class_acc: 0.0000 (0.0178)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0008  max mem: 15572
[2025-01-17 08:06:42,578] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:06:42,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2025-01-17 08:06:42,586] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:06:42,586] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/1404]  eta: 0:01:16  lr: 0.000017  min_lr: 0.000000  loss: 4.9929 (5.0689)  class_acc: 0.0000 (0.0178)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0007  max mem: 15572
Epoch: [0]  [1290/1404]  eta: 0:01:10  lr: 0.000017  min_lr: 0.000000  loss: 5.0127 (5.0682)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0006  max mem: 15572
Epoch: [0]  [1300/1404]  eta: 0:01:03  lr: 0.000017  min_lr: 0.000000  loss: 5.0070 (5.0680)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0006  max mem: 15572
Epoch: [0]  [1310/1404]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 5.0401 (5.0674)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0006  max mem: 15572
Epoch: [0]  [1320/1404]  eta: 0:00:51  lr: 0.000018  min_lr: 0.000000  loss: 5.0500 (5.0672)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0006  max mem: 15572
Epoch: [0]  [1330/1404]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 5.0718 (5.0670)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0069  max mem: 15572
Epoch: [0]  [1340/1404]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 5.0077 (5.0666)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0176  max mem: 15572
Epoch: [0]  [1350/1404]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 5.0077 (5.0663)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0347  max mem: 15572
Epoch: [0]  [1360/1404]  eta: 0:00:26  lr: 0.000018  min_lr: 0.000000  loss: 5.0087 (5.0660)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.0266  max mem: 15572
Epoch: [0]  [1370/1404]  eta: 0:00:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9893 (5.0653)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.0186  max mem: 15572
Epoch: [0]  [1380/1404]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9963 (5.0649)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.7007  data: 0.1023  max mem: 15572
Epoch: [0]  [1390/1404]  eta: 0:00:08  lr: 0.000019  min_lr: 0.000000  loss: 5.0110 (5.0645)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1012  max mem: 15572
Epoch: [0]  [1400/1404]  eta: 0:00:02  lr: 0.000019  min_lr: 0.000000  loss: 4.9658 (5.0638)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.4506  data: 0.0146  max mem: 15572
Epoch: [0]  [1403/1404]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 5.0054 (5.0638)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)  time: 0.4359  data: 0.0146  max mem: 15572
Epoch: [0] Total time: 0:14:19 (0.6120 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 5.0054 (5.0628)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (23514.0741)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:53  loss: 5.1042 (5.1042)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 6.5698  data: 6.1787  max mem: 15572
Val:  [ 10/136]  eta: 0:01:50  loss: 5.1506 (5.0705)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8809  data: 0.6556  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 5.0167 (5.0084)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2936  data: 0.0860  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 4.8438 (5.0142)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3055  data: 0.0928  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 4.9481 (4.9528)  acc1: 0.0000 (4.7425)  acc5: 0.0000 (8.9431)  time: 0.3038  data: 0.0941  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 5.0473 (5.0219)  acc1: 0.0000 (3.8126)  acc5: 0.0000 (7.1895)  time: 0.3541  data: 0.1476  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 5.2331 (5.0668)  acc1: 0.0000 (3.1876)  acc5: 0.0000 (6.0109)  time: 0.3798  data: 0.1664  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 5.0510 (5.0418)  acc1: 0.0000 (2.7387)  acc5: 0.0000 (8.9984)  time: 0.3532  data: 0.1468  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 4.7027 (5.0005)  acc1: 0.0000 (2.4005)  acc5: 0.0000 (9.9451)  time: 0.3773  data: 0.1787  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 4.9924 (5.0136)  acc1: 0.0000 (2.1368)  acc5: 0.0000 (8.8523)  time: 0.3864  data: 0.1760  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 5.0938 (5.0343)  acc1: 0.0000 (1.9252)  acc5: 0.0000 (7.9758)  time: 0.3730  data: 0.1701  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 5.0312 (5.0209)  acc1: 0.0000 (1.7518)  acc5: 0.0000 (7.2573)  time: 0.3402  data: 0.1582  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7658 (5.0096)  acc1: 0.0000 (1.6070)  acc5: 0.0000 (6.6575)  time: 0.3826  data: 0.1886  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.9219 (5.0257)  acc1: 0.0000 (1.4843)  acc5: 0.0000 (6.1493)  time: 0.3304  data: 0.1494  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 5.2168 (5.0267)  acc1: 0.0000 (1.4333)  acc5: 0.0000 (5.9378)  time: 0.1954  data: 0.0386  max mem: 15572
Val: Total time: 0:00:51 (0.3816 s / it)
* Acc@1 1.454 Acc@5 5.938 loss 5.026
Accuracy of the network on the 4883 val videos: 1.5%
[2025-01-17 08:08:46,399] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-17 08:08:46,402] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 08:08:46,402] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 08:08:46,402] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 08:08:46,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 08:08:46,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.45%
Epoch: [1]  [   0/1404]  eta: 3:04:32  lr: 0.000019  min_lr: 0.000000  loss: 5.0225 (5.0225)  class_acc: 0.0000 (0.0000)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 7.8867  data: 7.4080  max mem: 15572
[2025-01-17 08:08:57,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:08:57,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
[2025-01-17 08:08:57,092] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:08:57,093] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [1]  [  10/1404]  eta: 0:30:33  lr: 0.000019  min_lr: 0.000000  loss: 5.0225 (5.0036)  class_acc: 0.0000 (0.0303)  loss_scale: 262144.0000 (214481.4545)  weight_decay: 0.0500 (0.0500)  time: 1.3151  data: 0.8425  max mem: 15572
[2025-01-17 08:09:05,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-17 08:09:05,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-17 08:09:05,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
[2025-01-17 08:09:05,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1422
[2025-01-17 08:09:05,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
Epoch: [1]  [  20/1404]  eta: 0:22:51  lr: 0.000019  min_lr: 0.000000  loss: 4.9886 (4.9903)  class_acc: 0.0000 (0.0238)  loss_scale: 262144.0000 (218453.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1627  max mem: 15572
Epoch: [1]  [  30/1404]  eta: 0:19:50  lr: 0.000019  min_lr: 0.000000  loss: 4.9750 (4.9995)  class_acc: 0.0000 (0.0215)  loss_scale: 131072.0000 (190265.8065)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1068  max mem: 15572
Epoch: [1]  [  40/1404]  eta: 0:17:53  lr: 0.000019  min_lr: 0.000000  loss: 4.9750 (5.0030)  class_acc: 0.0000 (0.0244)  loss_scale: 131072.0000 (175828.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.0619  max mem: 15572
Epoch: [1]  [  50/1404]  eta: 0:17:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9686 (4.9894)  class_acc: 0.0000 (0.0261)  loss_scale: 131072.0000 (167052.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1184  max mem: 15572
Epoch: [1]  [  60/1404]  eta: 0:16:36  lr: 0.000020  min_lr: 0.000000  loss: 4.9665 (4.9843)  class_acc: 0.0000 (0.0232)  loss_scale: 131072.0000 (161154.0984)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.1721  max mem: 15572
Epoch: [1]  [  70/1404]  eta: 0:16:04  lr: 0.000020  min_lr: 0.000000  loss: 4.9987 (4.9955)  class_acc: 0.0000 (0.0223)  loss_scale: 131072.0000 (156917.1831)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1319  max mem: 15572
Epoch: [1]  [  80/1404]  eta: 0:15:37  lr: 0.000020  min_lr: 0.000000  loss: 5.0141 (4.9980)  class_acc: 0.0000 (0.0216)  loss_scale: 131072.0000 (153726.4198)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1196  max mem: 15572
Epoch: [1]  [  90/1404]  eta: 0:15:19  lr: 0.000020  min_lr: 0.000000  loss: 4.9973 (5.0018)  class_acc: 0.0000 (0.0211)  loss_scale: 131072.0000 (151236.9231)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1355  max mem: 15572
Epoch: [1]  [ 100/1404]  eta: 0:14:53  lr: 0.000020  min_lr: 0.000000  loss: 5.0167 (5.0014)  class_acc: 0.0000 (0.0206)  loss_scale: 131072.0000 (149240.3960)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0698  max mem: 15572
Epoch: [1]  [ 110/1404]  eta: 0:14:37  lr: 0.000020  min_lr: 0.000000  loss: 4.9969 (4.9965)  class_acc: 0.0000 (0.0218)  loss_scale: 131072.0000 (147603.6036)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0061  max mem: 15572
Epoch: [1]  [ 120/1404]  eta: 0:14:22  lr: 0.000020  min_lr: 0.000000  loss: 5.0227 (5.0042)  class_acc: 0.0000 (0.0207)  loss_scale: 131072.0000 (146237.3554)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0466  max mem: 15572
Epoch: [1]  [ 130/1404]  eta: 0:14:17  lr: 0.000020  min_lr: 0.000000  loss: 5.0782 (5.0070)  class_acc: 0.0000 (0.0204)  loss_scale: 131072.0000 (145079.6947)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.0971  max mem: 15572
Epoch: [1]  [ 140/1404]  eta: 0:14:16  lr: 0.000021  min_lr: 0.000000  loss: 5.0786 (5.0104)  class_acc: 0.0000 (0.0207)  loss_scale: 131072.0000 (144086.2411)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.0690  max mem: 15572
[2025-01-17 08:10:26,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:10:26,228] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-17 08:10:26,243] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:10:26,244] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-17 08:10:26,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-17 08:10:26,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-17 08:10:26,753] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1552
[2025-01-17 08:10:26,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-17 08:10:26,753] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [1]  [ 150/1404]  eta: 0:13:59  lr: 0.000021  min_lr: 0.000000  loss: 5.0652 (5.0137)  class_acc: 0.0000 (0.0215)  loss_scale: 131072.0000 (144092.3974)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0327  max mem: 15572
Epoch: [1]  [ 160/1404]  eta: 0:13:39  lr: 0.000021  min_lr: 0.000000  loss: 4.9734 (5.0100)  class_acc: 0.0000 (0.0217)  loss_scale: 131072.0000 (143283.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0200  max mem: 15572
Epoch: [1]  [ 170/1404]  eta: 0:13:31  lr: 0.000021  min_lr: 0.000000  loss: 4.9148 (5.0034)  class_acc: 0.0000 (0.0219)  loss_scale: 131072.0000 (142569.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0428  max mem: 15572
Epoch: [1]  [ 180/1404]  eta: 0:13:19  lr: 0.000021  min_lr: 0.000000  loss: 4.9361 (5.0030)  class_acc: 0.0000 (0.0244)  loss_scale: 131072.0000 (141934.3204)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0687  max mem: 15572
Epoch: [1]  [ 190/1404]  eta: 0:13:14  lr: 0.000021  min_lr: 0.000000  loss: 5.0448 (5.0038)  class_acc: 0.0000 (0.0240)  loss_scale: 131072.0000 (141365.6126)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0553  max mem: 15572
[2025-01-17 08:10:53,276] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1597
[2025-01-17 08:10:53,276] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:10:53,276] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:10:53,293] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1597
[2025-01-17 08:10:53,294] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [1]  [ 200/1404]  eta: 0:13:03  lr: 0.000021  min_lr: 0.000000  loss: 5.0127 (5.0029)  class_acc: 0.0000 (0.0232)  loss_scale: 131072.0000 (138245.0945)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0299  max mem: 15572
Epoch: [1]  [ 210/1404]  eta: 0:12:59  lr: 0.000022  min_lr: 0.000000  loss: 5.0090 (5.0026)  class_acc: 0.0000 (0.0233)  loss_scale: 65536.0000 (134799.1659)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.0717  max mem: 15572
Epoch: [1]  [ 220/1404]  eta: 0:12:49  lr: 0.000022  min_lr: 0.000000  loss: 5.0197 (5.0030)  class_acc: 0.0000 (0.0222)  loss_scale: 65536.0000 (131665.0860)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0789  max mem: 15572
Epoch: [1]  [ 230/1404]  eta: 0:12:43  lr: 0.000022  min_lr: 0.000000  loss: 5.0061 (5.0020)  class_acc: 0.0000 (0.0224)  loss_scale: 65536.0000 (128802.3550)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0265  max mem: 15572
Epoch: [1]  [ 240/1404]  eta: 0:12:37  lr: 0.000022  min_lr: 0.000000  loss: 5.0029 (5.0032)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (126177.1950)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0191  max mem: 15572
Epoch: [1]  [ 250/1404]  eta: 0:12:26  lr: 0.000022  min_lr: 0.000000  loss: 4.9444 (5.0010)  class_acc: 0.0000 (0.0227)  loss_scale: 65536.0000 (123761.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0008  max mem: 15572
Epoch: [1]  [ 260/1404]  eta: 0:12:15  lr: 0.000022  min_lr: 0.000000  loss: 4.9385 (4.9999)  class_acc: 0.0000 (0.0225)  loss_scale: 65536.0000 (121530.3602)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0009  max mem: 15572
Epoch: [1]  [ 270/1404]  eta: 0:12:09  lr: 0.000022  min_lr: 0.000000  loss: 5.0329 (5.0011)  class_acc: 0.0000 (0.0223)  loss_scale: 65536.0000 (119464.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0011  max mem: 15572
Epoch: [1]  [ 280/1404]  eta: 0:12:00  lr: 0.000022  min_lr: 0.000000  loss: 5.0567 (5.0048)  class_acc: 0.0000 (0.0215)  loss_scale: 65536.0000 (117544.9964)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.0008  max mem: 15572
Epoch: [1]  [ 290/1404]  eta: 0:11:55  lr: 0.000023  min_lr: 0.000000  loss: 5.0581 (5.0036)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (115757.7457)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.0009  max mem: 15572
Epoch: [1]  [ 300/1404]  eta: 0:11:47  lr: 0.000023  min_lr: 0.000000  loss: 5.0011 (5.0031)  class_acc: 0.0000 (0.0215)  loss_scale: 65536.0000 (114089.2492)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.0008  max mem: 15572
Epoch: [1]  [ 310/1404]  eta: 0:11:44  lr: 0.000023  min_lr: 0.000000  loss: 4.9791 (5.0022)  class_acc: 0.0000 (0.0216)  loss_scale: 65536.0000 (112528.0514)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.0005  max mem: 15572
Epoch: [1]  [ 320/1404]  eta: 0:11:34  lr: 0.000023  min_lr: 0.000000  loss: 5.0287 (5.0052)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (111064.1246)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0006  max mem: 15572
[2025-01-17 08:12:13,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:12:13,341] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:12:13,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:12:13,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 330/1404]  eta: 0:11:24  lr: 0.000023  min_lr: 0.000000  loss: 5.0270 (5.0052)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (111470.5982)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0008  max mem: 15572
[2025-01-17 08:12:20,850] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1740
[2025-01-17 08:12:20,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1740
[2025-01-17 08:12:20,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:12:20,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:12:20,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 340/1404]  eta: 0:11:17  lr: 0.000023  min_lr: 0.000000  loss: 5.0007 (5.0073)  class_acc: 0.0000 (0.0207)  loss_scale: 131072.0000 (111084.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0008  max mem: 15572
Epoch: [1]  [ 350/1404]  eta: 0:11:10  lr: 0.000023  min_lr: 0.000000  loss: 5.0609 (5.0087)  class_acc: 0.0000 (0.0203)  loss_scale: 65536.0000 (109786.8034)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0008  max mem: 15572
Epoch: [1]  [ 360/1404]  eta: 0:11:03  lr: 0.000024  min_lr: 0.000000  loss: 4.9739 (5.0072)  class_acc: 0.0000 (0.0203)  loss_scale: 65536.0000 (108561.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0009  max mem: 15572
Epoch: [1]  [ 370/1404]  eta: 0:10:56  lr: 0.000024  min_lr: 0.000000  loss: 4.9379 (5.0053)  class_acc: 0.0000 (0.0208)  loss_scale: 65536.0000 (107401.3154)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0010  max mem: 15572
Epoch: [1]  [ 380/1404]  eta: 0:10:49  lr: 0.000024  min_lr: 0.000000  loss: 4.9764 (5.0055)  class_acc: 0.0000 (0.0208)  loss_scale: 65536.0000 (106302.4882)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0010  max mem: 15572
Epoch: [1]  [ 390/1404]  eta: 0:10:42  lr: 0.000024  min_lr: 0.000000  loss: 5.0081 (5.0041)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (105259.8670)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0013  max mem: 15572
Epoch: [1]  [ 400/1404]  eta: 0:10:33  lr: 0.000024  min_lr: 0.000000  loss: 4.9609 (5.0031)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (104269.2469)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.0014  max mem: 15572
Epoch: [1]  [ 410/1404]  eta: 0:10:27  lr: 0.000024  min_lr: 0.000000  loss: 4.9933 (5.0039)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (103326.8321)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0010  max mem: 15572
Epoch: [1]  [ 420/1404]  eta: 0:10:20  lr: 0.000024  min_lr: 0.000000  loss: 5.0246 (5.0049)  class_acc: 0.0000 (0.0216)  loss_scale: 65536.0000 (102429.1876)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0009  max mem: 15572
Epoch: [1]  [ 430/1404]  eta: 0:10:13  lr: 0.000024  min_lr: 0.000000  loss: 5.0231 (5.0044)  class_acc: 0.0000 (0.0214)  loss_scale: 65536.0000 (101573.1972)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.0007  max mem: 15572
Epoch: [1]  [ 440/1404]  eta: 0:10:05  lr: 0.000025  min_lr: 0.000000  loss: 5.0231 (5.0042)  class_acc: 0.0000 (0.0214)  loss_scale: 65536.0000 (100756.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0007  max mem: 15572
Epoch: [1]  [ 450/1404]  eta: 0:09:58  lr: 0.000025  min_lr: 0.000000  loss: 5.0179 (5.0043)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (99975.0953)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.0007  max mem: 15572
Epoch: [1]  [ 460/1404]  eta: 0:09:53  lr: 0.000025  min_lr: 0.000000  loss: 4.9907 (5.0038)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (99228.0434)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0005  max mem: 15572
[2025-01-17 08:13:39,026] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:13:39,026] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:13:39,052] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:13:39,052] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:13:39,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1870
[2025-01-17 08:13:39,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:13:39,494] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1870
[2025-01-17 08:13:39,494] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:13:39,494] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 470/1404]  eta: 0:09:47  lr: 0.000025  min_lr: 0.000000  loss: 4.9798 (5.0036)  class_acc: 0.0000 (0.0211)  loss_scale: 65536.0000 (98651.8556)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0006  max mem: 15572
Epoch: [1]  [ 480/1404]  eta: 0:09:41  lr: 0.000025  min_lr: 0.000000  loss: 4.9741 (5.0023)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (97963.3763)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.0009  max mem: 15572
Epoch: [1]  [ 490/1404]  eta: 0:09:35  lr: 0.000025  min_lr: 0.000000  loss: 4.9147 (5.0017)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (97302.9409)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.0009  max mem: 15572
Epoch: [1]  [ 500/1404]  eta: 0:09:28  lr: 0.000025  min_lr: 0.000000  loss: 5.0143 (5.0010)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (96668.8703)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0008  max mem: 15572
Epoch: [1]  [ 510/1404]  eta: 0:09:20  lr: 0.000026  min_lr: 0.000000  loss: 5.0493 (5.0019)  class_acc: 0.0000 (0.0211)  loss_scale: 65536.0000 (96059.6164)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0008  max mem: 15572
Epoch: [1]  [ 520/1404]  eta: 0:09:14  lr: 0.000026  min_lr: 0.000000  loss: 4.9827 (5.0006)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (95473.7505)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0008  max mem: 15572
Epoch: [1]  [ 530/1404]  eta: 0:09:08  lr: 0.000026  min_lr: 0.000000  loss: 4.9491 (4.9996)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (94909.9510)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0006  max mem: 15572
Epoch: [1]  [ 540/1404]  eta: 0:09:00  lr: 0.000026  min_lr: 0.000000  loss: 4.9554 (4.9989)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (94366.9945)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0005  max mem: 15572
Epoch: [1]  [ 550/1404]  eta: 0:08:54  lr: 0.000026  min_lr: 0.000000  loss: 4.9582 (4.9982)  class_acc: 0.0000 (0.0214)  loss_scale: 65536.0000 (93843.7459)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0007  max mem: 15572
Epoch: [1]  [ 560/1404]  eta: 0:08:48  lr: 0.000026  min_lr: 0.000000  loss: 4.9919 (4.9992)  class_acc: 0.0000 (0.0215)  loss_scale: 65536.0000 (93339.1515)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0008  max mem: 15572
Epoch: [1]  [ 570/1404]  eta: 0:08:42  lr: 0.000026  min_lr: 0.000000  loss: 4.9462 (4.9978)  class_acc: 0.0000 (0.0215)  loss_scale: 65536.0000 (92852.2312)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0008  max mem: 15572
Epoch: [1]  [ 580/1404]  eta: 0:08:35  lr: 0.000026  min_lr: 0.000000  loss: 4.9225 (4.9979)  class_acc: 0.0000 (0.0221)  loss_scale: 65536.0000 (92382.0723)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0008  max mem: 15572
Epoch: [1]  [ 590/1404]  eta: 0:08:30  lr: 0.000027  min_lr: 0.000000  loss: 4.9867 (4.9973)  class_acc: 0.0000 (0.0222)  loss_scale: 65536.0000 (91927.8240)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0009  max mem: 15572
[2025-01-17 08:14:59,772] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:14:59,772] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:14:59,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[2.586922083856223e-07, 2.586922083856223e-07, 3.695602976937462e-07, 3.695602976937462e-07, 5.279432824196375e-07, 5.279432824196375e-07, 7.542046891709107e-07, 7.542046891709107e-07, 1.0774352702441582e-06, 1.0774352702441582e-06, 1.5391932432059404e-06, 1.5391932432059404e-06, 2.1988474902942005e-06, 2.1988474902942005e-06, 3.141210700420287e-06, 3.141210700420287e-06, 4.487443857743267e-06, 4.487443857743267e-06, 6.410634082490383e-06, 6.410634082490383e-06, 9.158048689271974e-06, 9.158048689271974e-06, 1.3082926698959966e-05, 1.3082926698959966e-05, 1.8689895284228524e-05, 1.8689895284228524e-05, 2.669985040604075e-05, 2.669985040604075e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:14:59,782] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=43.90775413778925, CurrSamplesPerSec=46.730306810210884, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-17 08:14:59,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:14:59,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:15:00,337] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2000
[2025-01-17 08:15:00,337] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2000
[2025-01-17 08:15:00,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:15:00,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:15:00,338] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:15:01,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2003
[2025-01-17 08:15:01,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:15:01,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2003
[2025-01-17 08:15:01,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:15:01,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [ 600/1404]  eta: 0:08:22  lr: 0.000027  min_lr: 0.000000  loss: 4.9443 (4.9963)  class_acc: 0.0000 (0.0229)  loss_scale: 65536.0000 (91488.6922)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0008  max mem: 15572
Epoch: [1]  [ 610/1404]  eta: 0:08:15  lr: 0.000027  min_lr: 0.000000  loss: 4.9447 (4.9956)  class_acc: 0.0417 (0.0232)  loss_scale: 32768.0000 (90527.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0009  max mem: 15572
Epoch: [1]  [ 620/1404]  eta: 0:08:09  lr: 0.000027  min_lr: 0.000000  loss: 4.9940 (4.9962)  class_acc: 0.0000 (0.0232)  loss_scale: 32768.0000 (89597.5266)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0009  max mem: 15572
Epoch: [1]  [ 630/1404]  eta: 0:08:02  lr: 0.000027  min_lr: 0.000000  loss: 4.9839 (4.9956)  class_acc: 0.0000 (0.0233)  loss_scale: 32768.0000 (88696.9002)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0007  max mem: 15572
Epoch: [1]  [ 640/1404]  eta: 0:07:57  lr: 0.000027  min_lr: 0.000000  loss: 4.9653 (4.9950)  class_acc: 0.0000 (0.0232)  loss_scale: 32768.0000 (87824.3744)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.0007  max mem: 15572
Epoch: [1]  [ 650/1404]  eta: 0:07:50  lr: 0.000027  min_lr: 0.000000  loss: 4.9860 (4.9963)  class_acc: 0.0000 (0.0230)  loss_scale: 32768.0000 (86978.6544)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0009  max mem: 15572
Epoch: [1]  [ 660/1404]  eta: 0:07:43  lr: 0.000028  min_lr: 0.000000  loss: 4.9823 (4.9952)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (86158.5234)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0010  max mem: 15572
Epoch: [1]  [ 670/1404]  eta: 0:07:36  lr: 0.000028  min_lr: 0.000000  loss: 4.9638 (4.9950)  class_acc: 0.0000 (0.0234)  loss_scale: 32768.0000 (85362.8376)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0009  max mem: 15572
Epoch: [1]  [ 680/1404]  eta: 0:07:30  lr: 0.000028  min_lr: 0.000000  loss: 4.9965 (4.9945)  class_acc: 0.0000 (0.0233)  loss_scale: 32768.0000 (84590.5198)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0010  max mem: 15572
Epoch: [1]  [ 690/1404]  eta: 0:07:23  lr: 0.000028  min_lr: 0.000000  loss: 4.9848 (4.9932)  class_acc: 0.0000 (0.0236)  loss_scale: 32768.0000 (83840.5557)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.0010  max mem: 15572
Epoch: [1]  [ 700/1404]  eta: 0:07:16  lr: 0.000028  min_lr: 0.000000  loss: 4.9248 (4.9924)  class_acc: 0.0000 (0.0237)  loss_scale: 32768.0000 (83111.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0010  max mem: 15572
Epoch: [1]  [ 710/1404]  eta: 0:07:10  lr: 0.000028  min_lr: 0.000000  loss: 4.9636 (4.9914)  class_acc: 0.0000 (0.0237)  loss_scale: 32768.0000 (82403.9156)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0010  max mem: 15572
Epoch: [1]  [ 720/1404]  eta: 0:07:04  lr: 0.000028  min_lr: 0.000000  loss: 5.0284 (4.9930)  class_acc: 0.0000 (0.0236)  loss_scale: 32768.0000 (81715.4840)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0009  max mem: 15572
[2025-01-17 08:16:19,979] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:16:19,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:16:20,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:16:20,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 730/1404]  eta: 0:06:58  lr: 0.000029  min_lr: 0.000000  loss: 5.0066 (4.9921)  class_acc: 0.0000 (0.0239)  loss_scale: 32768.0000 (81180.3666)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0007  max mem: 15572
Epoch: [1]  [ 740/1404]  eta: 0:06:52  lr: 0.000029  min_lr: 0.000000  loss: 4.8804 (4.9907)  class_acc: 0.0417 (0.0243)  loss_scale: 65536.0000 (80969.2416)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.0007  max mem: 15572
Epoch: [1]  [ 750/1404]  eta: 0:06:46  lr: 0.000029  min_lr: 0.000000  loss: 4.9335 (4.9903)  class_acc: 0.0000 (0.0242)  loss_scale: 65536.0000 (80763.7390)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0008  max mem: 15572
Epoch: [1]  [ 760/1404]  eta: 0:06:40  lr: 0.000029  min_lr: 0.000000  loss: 4.9335 (4.9891)  class_acc: 0.0000 (0.0245)  loss_scale: 65536.0000 (80563.6373)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0009  max mem: 15572
Epoch: [1]  [ 770/1404]  eta: 0:06:33  lr: 0.000029  min_lr: 0.000000  loss: 4.9119 (4.9884)  class_acc: 0.0000 (0.0245)  loss_scale: 65536.0000 (80368.7263)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.0009  max mem: 15572
Epoch: [1]  [ 780/1404]  eta: 0:06:28  lr: 0.000029  min_lr: 0.000000  loss: 4.9553 (4.9886)  class_acc: 0.0000 (0.0245)  loss_scale: 65536.0000 (80178.8067)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0008  max mem: 15572
Epoch: [1]  [ 790/1404]  eta: 0:06:21  lr: 0.000029  min_lr: 0.000000  loss: 4.9893 (4.9881)  class_acc: 0.0000 (0.0248)  loss_scale: 65536.0000 (79993.6890)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0009  max mem: 15572
Epoch: [1]  [ 800/1404]  eta: 0:06:14  lr: 0.000029  min_lr: 0.000000  loss: 4.9893 (4.9884)  class_acc: 0.0000 (0.0249)  loss_scale: 65536.0000 (79813.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0009  max mem: 15572
Epoch: [1]  [ 810/1404]  eta: 0:06:08  lr: 0.000030  min_lr: 0.000000  loss: 4.9249 (4.9874)  class_acc: 0.0000 (0.0251)  loss_scale: 65536.0000 (79637.1492)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0008  max mem: 15572
Epoch: [1]  [ 820/1404]  eta: 0:06:02  lr: 0.000030  min_lr: 0.000000  loss: 4.8918 (4.9865)  class_acc: 0.0000 (0.0251)  loss_scale: 65536.0000 (79465.3934)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0008  max mem: 15572
Epoch: [1]  [ 830/1404]  eta: 0:05:56  lr: 0.000030  min_lr: 0.000000  loss: 4.9106 (4.9865)  class_acc: 0.0000 (0.0250)  loss_scale: 65536.0000 (79297.7714)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0007  max mem: 15572
Epoch: [1]  [ 840/1404]  eta: 0:05:49  lr: 0.000030  min_lr: 0.000000  loss: 4.9254 (4.9855)  class_acc: 0.0000 (0.0252)  loss_scale: 65536.0000 (79134.1356)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0007  max mem: 15572
Epoch: [1]  [ 850/1404]  eta: 0:05:43  lr: 0.000030  min_lr: 0.000000  loss: 4.8944 (4.9845)  class_acc: 0.0000 (0.0252)  loss_scale: 65536.0000 (78974.3455)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0007  max mem: 15572
[2025-01-17 08:17:38,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:17:38,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:17:38,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:17:38,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:17:38,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2261
[2025-01-17 08:17:38,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:17:38,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2261
[2025-01-17 08:17:38,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:17:38,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:17:41,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2264
[2025-01-17 08:17:41,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:17:41,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:17:41,022] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2264
[2025-01-17 08:17:41,022] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [ 860/1404]  eta: 0:05:37  lr: 0.000030  min_lr: 0.000000  loss: 4.9043 (4.9837)  class_acc: 0.0000 (0.0254)  loss_scale: 65536.0000 (78856.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0008  max mem: 15572
Epoch: [1]  [ 870/1404]  eta: 0:05:31  lr: 0.000030  min_lr: 0.000000  loss: 4.9264 (4.9826)  class_acc: 0.0000 (0.0257)  loss_scale: 32768.0000 (78327.1825)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0006  max mem: 15572
Epoch: [1]  [ 880/1404]  eta: 0:05:24  lr: 0.000031  min_lr: 0.000000  loss: 4.9500 (4.9828)  class_acc: 0.0000 (0.0255)  loss_scale: 32768.0000 (77810.0522)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0008  max mem: 15572
Epoch: [1]  [ 890/1404]  eta: 0:05:17  lr: 0.000031  min_lr: 0.000000  loss: 4.9629 (4.9829)  class_acc: 0.0000 (0.0255)  loss_scale: 32768.0000 (77304.5297)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0010  max mem: 15572
Epoch: [1]  [ 900/1404]  eta: 0:05:11  lr: 0.000031  min_lr: 0.000000  loss: 4.9980 (4.9826)  class_acc: 0.0000 (0.0254)  loss_scale: 32768.0000 (76810.2286)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0009  max mem: 15572
Epoch: [1]  [ 910/1404]  eta: 0:05:05  lr: 0.000031  min_lr: 0.000000  loss: 5.0009 (4.9831)  class_acc: 0.0000 (0.0254)  loss_scale: 32768.0000 (76326.7794)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0008  max mem: 15572
Epoch: [1]  [ 920/1404]  eta: 0:04:59  lr: 0.000031  min_lr: 0.000000  loss: 5.0009 (4.9830)  class_acc: 0.0000 (0.0255)  loss_scale: 32768.0000 (75853.8284)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0006  max mem: 15572
Epoch: [1]  [ 930/1404]  eta: 0:04:53  lr: 0.000031  min_lr: 0.000000  loss: 4.9032 (4.9828)  class_acc: 0.0000 (0.0255)  loss_scale: 32768.0000 (75391.0376)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0006  max mem: 15572
Epoch: [1]  [ 940/1404]  eta: 0:04:46  lr: 0.000031  min_lr: 0.000000  loss: 4.9016 (4.9823)  class_acc: 0.0417 (0.0259)  loss_scale: 32768.0000 (74938.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0008  max mem: 15572
Epoch: [1]  [ 950/1404]  eta: 0:04:40  lr: 0.000031  min_lr: 0.000000  loss: 4.9107 (4.9817)  class_acc: 0.0000 (0.0258)  loss_scale: 32768.0000 (74494.6540)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.0007  max mem: 15572
Epoch: [1]  [ 960/1404]  eta: 0:04:34  lr: 0.000032  min_lr: 0.000000  loss: 4.9071 (4.9814)  class_acc: 0.0000 (0.0263)  loss_scale: 32768.0000 (74060.4537)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0006  max mem: 15572
Epoch: [1]  [ 970/1404]  eta: 0:04:28  lr: 0.000032  min_lr: 0.000000  loss: 4.9519 (4.9809)  class_acc: 0.0000 (0.0263)  loss_scale: 32768.0000 (73635.1967)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0006  max mem: 15572
Epoch: [1]  [ 980/1404]  eta: 0:04:22  lr: 0.000032  min_lr: 0.000000  loss: 4.9594 (4.9805)  class_acc: 0.0000 (0.0263)  loss_scale: 32768.0000 (73218.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0006  max mem: 15572
[2025-01-17 08:18:58,774] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:18:58,775] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:18:58,783] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:18:58,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [ 990/1404]  eta: 0:04:15  lr: 0.000032  min_lr: 0.000000  loss: 4.9044 (4.9797)  class_acc: 0.0000 (0.0266)  loss_scale: 32768.0000 (72876.5610)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0008  max mem: 15572
Epoch: [1]  [1000/1404]  eta: 0:04:09  lr: 0.000032  min_lr: 0.000000  loss: 4.9309 (4.9796)  class_acc: 0.0000 (0.0266)  loss_scale: 65536.0000 (72803.2288)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0009  max mem: 15572
Epoch: [1]  [1010/1404]  eta: 0:04:03  lr: 0.000032  min_lr: 0.000000  loss: 4.9566 (4.9793)  class_acc: 0.0000 (0.0265)  loss_scale: 65536.0000 (72731.3472)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0007  max mem: 15572
[2025-01-17 08:19:13,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-17 08:19:13,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:19:13,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:19:13,523] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2419
[2025-01-17 08:19:13,523] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [1]  [1020/1404]  eta: 0:03:56  lr: 0.000032  min_lr: 0.000000  loss: 4.9159 (4.9786)  class_acc: 0.0000 (0.0266)  loss_scale: 65536.0000 (72468.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0007  max mem: 15572
Epoch: [1]  [1030/1404]  eta: 0:03:50  lr: 0.000033  min_lr: 0.000000  loss: 4.8925 (4.9782)  class_acc: 0.0000 (0.0265)  loss_scale: 32768.0000 (72083.2435)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0009  max mem: 15572
Epoch: [1]  [1040/1404]  eta: 0:03:44  lr: 0.000033  min_lr: 0.000000  loss: 4.9772 (4.9784)  class_acc: 0.0000 (0.0265)  loss_scale: 32768.0000 (71705.5754)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.0009  max mem: 15572
Epoch: [1]  [1050/1404]  eta: 0:03:38  lr: 0.000033  min_lr: 0.000000  loss: 4.9853 (4.9782)  class_acc: 0.0000 (0.0264)  loss_scale: 32768.0000 (71335.0942)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.0467  max mem: 15572
Epoch: [1]  [1060/1404]  eta: 0:03:32  lr: 0.000033  min_lr: 0.000000  loss: 4.9292 (4.9776)  class_acc: 0.0000 (0.0265)  loss_scale: 32768.0000 (70971.5966)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.0575  max mem: 15572
Epoch: [1]  [1070/1404]  eta: 0:03:26  lr: 0.000033  min_lr: 0.000000  loss: 4.8841 (4.9771)  class_acc: 0.0000 (0.0266)  loss_scale: 32768.0000 (70614.8870)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.0118  max mem: 15572
Epoch: [1]  [1080/1404]  eta: 0:03:20  lr: 0.000033  min_lr: 0.000000  loss: 4.9073 (4.9769)  class_acc: 0.0000 (0.0265)  loss_scale: 32768.0000 (70264.7771)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0010  max mem: 15572
Epoch: [1]  [1090/1404]  eta: 0:03:13  lr: 0.000033  min_lr: 0.000000  loss: 4.9537 (4.9766)  class_acc: 0.0000 (0.0265)  loss_scale: 32768.0000 (69921.0852)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0007  max mem: 15572
Epoch: [1]  [1100/1404]  eta: 0:03:07  lr: 0.000033  min_lr: 0.000000  loss: 4.9858 (4.9761)  class_acc: 0.0000 (0.0266)  loss_scale: 32768.0000 (69583.6367)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0010  max mem: 15572
Epoch: [1]  [1110/1404]  eta: 0:03:01  lr: 0.000034  min_lr: 0.000000  loss: 4.9050 (4.9752)  class_acc: 0.0417 (0.0270)  loss_scale: 32768.0000 (69252.2628)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0011  max mem: 15572
Epoch: [1]  [1120/1404]  eta: 0:02:55  lr: 0.000034  min_lr: 0.000000  loss: 4.9036 (4.9750)  class_acc: 0.0417 (0.0271)  loss_scale: 32768.0000 (68926.8011)  weight_decay: 0.0500 (0.0500)  time: 0.6540  data: 0.0007  max mem: 15572
Epoch: [1]  [1130/1404]  eta: 0:02:49  lr: 0.000034  min_lr: 0.000000  loss: 4.9461 (4.9746)  class_acc: 0.0000 (0.0271)  loss_scale: 32768.0000 (68607.0946)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.0006  max mem: 15572
Epoch: [1]  [1140/1404]  eta: 0:02:42  lr: 0.000034  min_lr: 0.000000  loss: 4.8846 (4.9732)  class_acc: 0.0000 (0.0273)  loss_scale: 32768.0000 (68292.9921)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0007  max mem: 15572
[2025-01-17 08:20:33,203] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:20:33,203] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:20:33,204] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:20:33,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1150/1404]  eta: 0:02:36  lr: 0.000034  min_lr: 0.000000  loss: 4.8366 (4.9727)  class_acc: 0.0000 (0.0274)  loss_scale: 32768.0000 (68183.6316)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0008  max mem: 15572
Epoch: [1]  [1160/1404]  eta: 0:02:30  lr: 0.000034  min_lr: 0.000000  loss: 4.9681 (4.9723)  class_acc: 0.0000 (0.0273)  loss_scale: 65536.0000 (68160.8269)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0007  max mem: 15572
Epoch: [1]  [1170/1404]  eta: 0:02:24  lr: 0.000034  min_lr: 0.000000  loss: 4.9356 (4.9719)  class_acc: 0.0000 (0.0274)  loss_scale: 65536.0000 (68138.4116)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0006  max mem: 15572
Epoch: [1]  [1180/1404]  eta: 0:02:18  lr: 0.000035  min_lr: 0.000000  loss: 4.8740 (4.9718)  class_acc: 0.0000 (0.0274)  loss_scale: 65536.0000 (68116.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0007  max mem: 15572
Epoch: [1]  [1190/1404]  eta: 0:02:11  lr: 0.000035  min_lr: 0.000000  loss: 4.9115 (4.9716)  class_acc: 0.0000 (0.0273)  loss_scale: 65536.0000 (68094.7103)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0007  max mem: 15572
Epoch: [1]  [1200/1404]  eta: 0:02:05  lr: 0.000035  min_lr: 0.000000  loss: 4.8332 (4.9700)  class_acc: 0.0000 (0.0274)  loss_scale: 65536.0000 (68073.4055)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.0006  max mem: 15572
Epoch: [1]  [1210/1404]  eta: 0:01:59  lr: 0.000035  min_lr: 0.000000  loss: 4.8932 (4.9697)  class_acc: 0.0000 (0.0274)  loss_scale: 65536.0000 (68052.4525)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.0006  max mem: 15572
Epoch: [1]  [1220/1404]  eta: 0:01:53  lr: 0.000035  min_lr: 0.000000  loss: 4.9163 (4.9692)  class_acc: 0.0000 (0.0276)  loss_scale: 65536.0000 (68031.8428)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0006  max mem: 15572
Epoch: [1]  [1230/1404]  eta: 0:01:47  lr: 0.000035  min_lr: 0.000000  loss: 4.9163 (4.9692)  class_acc: 0.0000 (0.0275)  loss_scale: 65536.0000 (68011.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0032  max mem: 15572
[2025-01-17 08:21:31,633] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2643
[2025-01-17 08:21:31,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:21:31,643] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2643
[2025-01-17 08:21:31,643] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:21:31,643] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [1240/1404]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000000  loss: 4.9221 (4.9683)  class_acc: 0.0000 (0.0274)  loss_scale: 65536.0000 (67938.8106)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0034  max mem: 15572
Epoch: [1]  [1250/1404]  eta: 0:01:34  lr: 0.000035  min_lr: 0.000000  loss: 4.8717 (4.9673)  class_acc: 0.0000 (0.0276)  loss_scale: 32768.0000 (67657.6691)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0008  max mem: 15572
Epoch: [1]  [1260/1404]  eta: 0:01:28  lr: 0.000036  min_lr: 0.000000  loss: 4.8717 (4.9664)  class_acc: 0.0000 (0.0278)  loss_scale: 32768.0000 (67380.9865)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.0006  max mem: 15572
Epoch: [1]  [1270/1404]  eta: 0:01:22  lr: 0.000036  min_lr: 0.000000  loss: 4.8891 (4.9663)  class_acc: 0.0000 (0.0276)  loss_scale: 32768.0000 (67108.6577)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0034  max mem: 15572
Epoch: [1]  [1280/1404]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000000  loss: 4.9313 (4.9660)  class_acc: 0.0000 (0.0278)  loss_scale: 32768.0000 (66840.5808)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0034  max mem: 15572
Epoch: [1]  [1290/1404]  eta: 0:01:10  lr: 0.000036  min_lr: 0.000000  loss: 4.8863 (4.9651)  class_acc: 0.0417 (0.0280)  loss_scale: 32768.0000 (66576.6569)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.0006  max mem: 15572
Epoch: [1]  [1300/1404]  eta: 0:01:04  lr: 0.000036  min_lr: 0.000000  loss: 4.9086 (4.9652)  class_acc: 0.0417 (0.0284)  loss_scale: 32768.0000 (66316.7902)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0006  max mem: 15572
Epoch: [1]  [1310/1404]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000000  loss: 4.9140 (4.9646)  class_acc: 0.0417 (0.0285)  loss_scale: 32768.0000 (66060.8879)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0007  max mem: 15572
Epoch: [1]  [1320/1404]  eta: 0:00:51  lr: 0.000036  min_lr: 0.000000  loss: 4.8394 (4.9633)  class_acc: 0.0417 (0.0288)  loss_scale: 32768.0000 (65808.8600)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0006  max mem: 15572
Epoch: [1]  [1330/1404]  eta: 0:00:45  lr: 0.000037  min_lr: 0.000000  loss: 4.8046 (4.9623)  class_acc: 0.0417 (0.0290)  loss_scale: 32768.0000 (65560.6191)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.0006  max mem: 15572
Epoch: [1]  [1340/1404]  eta: 0:00:39  lr: 0.000037  min_lr: 0.000000  loss: 4.8235 (4.9621)  class_acc: 0.0000 (0.0289)  loss_scale: 32768.0000 (65316.0805)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0007  max mem: 15572
Epoch: [1]  [1350/1404]  eta: 0:00:33  lr: 0.000037  min_lr: 0.000000  loss: 4.8993 (4.9608)  class_acc: 0.0000 (0.0291)  loss_scale: 32768.0000 (65075.1621)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0006  max mem: 15572
Epoch: [1]  [1360/1404]  eta: 0:00:27  lr: 0.000037  min_lr: 0.000000  loss: 4.8995 (4.9606)  class_acc: 0.0417 (0.0291)  loss_scale: 32768.0000 (64837.7840)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0007  max mem: 15572
[2025-01-17 08:22:49,715] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:22:49,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:22:49,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:22:49,769] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [1370/1404]  eta: 0:00:20  lr: 0.000037  min_lr: 0.000000  loss: 4.9321 (4.9606)  class_acc: 0.0417 (0.0293)  loss_scale: 32768.0000 (64675.5711)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0009  max mem: 15572
Epoch: [1]  [1380/1404]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000000  loss: 4.8967 (4.9596)  class_acc: 0.0417 (0.0293)  loss_scale: 65536.0000 (64681.8016)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0009  max mem: 15572
Epoch: [1]  [1390/1404]  eta: 0:00:08  lr: 0.000037  min_lr: 0.000000  loss: 4.8490 (4.9584)  class_acc: 0.0417 (0.0295)  loss_scale: 65536.0000 (64687.9425)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0007  max mem: 15572
Epoch: [1]  [1400/1404]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000000  loss: 4.8173 (4.9577)  class_acc: 0.0417 (0.0296)  loss_scale: 65536.0000 (64693.9957)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0004  max mem: 15572
Epoch: [1]  [1403/1404]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.8173 (4.9576)  class_acc: 0.0417 (0.0297)  loss_scale: 65536.0000 (64695.7949)  weight_decay: 0.0500 (0.0500)  time: 0.4661  data: 0.0004  max mem: 15572
Epoch: [1] Total time: 0:14:21 (0.6134 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.8173 (4.9560)  class_acc: 0.0417 (0.0290)  loss_scale: 65536.0000 (64695.7949)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:13  loss: 5.0632 (5.0632)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.3916  data: 5.1743  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 4.9768 (4.8298)  acc1: 0.0000 (9.0909)  acc5: 0.0000 (16.6667)  time: 0.8135  data: 0.6189  max mem: 15572
Val:  [ 20/136]  eta: 0:01:08  loss: 4.6567 (4.6681)  acc1: 0.0000 (10.5820)  acc5: 5.5556 (28.3069)  time: 0.3481  data: 0.1506  max mem: 15572
Val:  [ 30/136]  eta: 0:00:54  loss: 4.2990 (4.6589)  acc1: 0.0000 (7.1685)  acc5: 16.6667 (26.8817)  time: 0.3505  data: 0.1431  max mem: 15572
Val:  [ 40/136]  eta: 0:00:47  loss: 4.4284 (4.6164)  acc1: 0.0000 (7.8591)  acc5: 11.1111 (27.2358)  time: 0.3930  data: 0.1893  max mem: 15572
Val:  [ 50/136]  eta: 0:00:39  loss: 4.8405 (4.6958)  acc1: 0.0000 (6.4270)  acc5: 0.0000 (23.3115)  time: 0.3782  data: 0.1699  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 4.9178 (4.7504)  acc1: 0.0000 (5.4645)  acc5: 0.0000 (19.9454)  time: 0.3299  data: 0.1121  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 4.7986 (4.6985)  acc1: 0.0000 (7.4335)  acc5: 0.0000 (23.0829)  time: 0.3696  data: 0.1667  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 4.5768 (4.6903)  acc1: 0.0000 (7.7503)  acc5: 0.0000 (22.7709)  time: 0.4292  data: 0.2231  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 4.7389 (4.7109)  acc1: 0.0000 (6.8987)  acc5: 0.0000 (20.3907)  time: 0.3844  data: 0.1735  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 4.9164 (4.7456)  acc1: 0.0000 (6.2156)  acc5: 0.0000 (18.3718)  time: 0.3355  data: 0.1252  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.8813 (4.7522)  acc1: 0.0000 (5.6557)  acc5: 0.0000 (17.5175)  time: 0.3573  data: 0.1379  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.7201 (4.7510)  acc1: 0.0000 (5.1882)  acc5: 5.5556 (16.8503)  time: 0.3965  data: 0.1775  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.6973 (4.7574)  acc1: 0.0000 (4.8346)  acc5: 11.1111 (17.0059)  time: 0.2907  data: 0.1051  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.8783 (4.7578)  acc1: 0.0000 (4.6683)  acc5: 5.5556 (17.3219)  time: 0.2070  data: 0.0471  max mem: 15572
Val: Total time: 0:00:52 (0.3851 s / it)
* Acc@1 4.566 Acc@5 16.790 loss 4.767
Accuracy of the network on the 4883 val videos: 4.6%
[2025-01-17 08:24:00,406] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 08:24:00,408] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 08:24:00,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 08:24:00,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 08:24:03,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 08:24:03,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 4.57%
Epoch: [2]  [   0/1404]  eta: 3:01:28  lr: 0.000038  min_lr: 0.000000  loss: 4.8350 (4.8350)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7556  data: 5.5713  max mem: 15572
[2025-01-17 08:24:14,584] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2816
[2025-01-17 08:24:14,586] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:24:14,587] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:24:14,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2816
[2025-01-17 08:24:14,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [  10/1404]  eta: 0:30:16  lr: 0.000038  min_lr: 0.000000  loss: 4.8886 (4.9028)  class_acc: 0.0833 (0.0682)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 1.3029  data: 0.5161  max mem: 15572
Epoch: [2]  [  20/1404]  eta: 0:22:34  lr: 0.000038  min_lr: 0.000000  loss: 4.8886 (4.9045)  class_acc: 0.0417 (0.0615)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.0056  max mem: 15572
Epoch: [2]  [  30/1404]  eta: 0:19:22  lr: 0.000038  min_lr: 0.000000  loss: 4.8779 (4.9020)  class_acc: 0.0000 (0.0511)  loss_scale: 32768.0000 (41224.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0008  max mem: 15572
Epoch: [2]  [  40/1404]  eta: 0:17:59  lr: 0.000038  min_lr: 0.000000  loss: 4.9607 (4.9086)  class_acc: 0.0000 (0.0467)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0009  max mem: 15572
Epoch: [2]  [  50/1404]  eta: 0:16:44  lr: 0.000038  min_lr: 0.000000  loss: 4.8115 (4.8824)  class_acc: 0.0000 (0.0474)  loss_scale: 32768.0000 (37908.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0007  max mem: 15572
Epoch: [2]  [  60/1404]  eta: 0:16:19  lr: 0.000038  min_lr: 0.000000  loss: 4.7800 (4.8779)  class_acc: 0.0000 (0.0444)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0007  max mem: 15572
Epoch: [2]  [  70/1404]  eta: 0:15:39  lr: 0.000038  min_lr: 0.000000  loss: 4.9314 (4.8801)  class_acc: 0.0000 (0.0446)  loss_scale: 32768.0000 (36460.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0008  max mem: 15572
Epoch: [2]  [  80/1404]  eta: 0:15:11  lr: 0.000039  min_lr: 0.000000  loss: 4.8547 (4.8674)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0010  max mem: 15572
Epoch: [2]  [  90/1404]  eta: 0:14:46  lr: 0.000039  min_lr: 0.000000  loss: 4.8318 (4.8627)  class_acc: 0.0000 (0.0440)  loss_scale: 32768.0000 (35648.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0080  max mem: 15572
Epoch: [2]  [ 100/1404]  eta: 0:14:43  lr: 0.000039  min_lr: 0.000000  loss: 4.8328 (4.8527)  class_acc: 0.0000 (0.0454)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0076  max mem: 15572
Epoch: [2]  [ 110/1404]  eta: 0:14:21  lr: 0.000039  min_lr: 0.000000  loss: 4.8691 (4.8554)  class_acc: 0.0000 (0.0447)  loss_scale: 32768.0000 (35129.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0005  max mem: 15572
Epoch: [2]  [ 120/1404]  eta: 0:14:22  lr: 0.000039  min_lr: 0.000000  loss: 4.8019 (4.8492)  class_acc: 0.0000 (0.0458)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0006  max mem: 15572
Epoch: [2]  [ 130/1404]  eta: 0:14:06  lr: 0.000039  min_lr: 0.000000  loss: 4.8521 (4.8576)  class_acc: 0.0000 (0.0458)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.0006  max mem: 15572
[2025-01-17 08:25:33,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:25:33,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:25:33,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:25:33,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 140/1404]  eta: 0:13:59  lr: 0.000039  min_lr: 0.000000  loss: 4.9151 (4.8557)  class_acc: 0.0000 (0.0443)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0006  max mem: 15572
Epoch: [2]  [ 150/1404]  eta: 0:13:41  lr: 0.000040  min_lr: 0.000000  loss: 4.8326 (4.8535)  class_acc: 0.0000 (0.0430)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0128  max mem: 15572
Epoch: [2]  [ 160/1404]  eta: 0:13:35  lr: 0.000040  min_lr: 0.000000  loss: 4.8326 (4.8563)  class_acc: 0.0000 (0.0419)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0611  max mem: 15572
Epoch: [2]  [ 170/1404]  eta: 0:13:25  lr: 0.000040  min_lr: 0.000000  loss: 4.8179 (4.8552)  class_acc: 0.0000 (0.0404)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0491  max mem: 15572
Epoch: [2]  [ 180/1404]  eta: 0:13:16  lr: 0.000040  min_lr: 0.000000  loss: 4.8007 (4.8537)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0006  max mem: 15572
Epoch: [2]  [ 190/1404]  eta: 0:13:05  lr: 0.000040  min_lr: 0.000000  loss: 4.7994 (4.8510)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0006  max mem: 15572
[2025-01-17 08:26:07,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[3.8810301798323227e-07, 3.8810301798323227e-07, 5.54432882833189e-07, 5.54432882833189e-07, 7.920469754759844e-07, 7.920469754759844e-07, 1.1314956792514063e-06, 1.1314956792514063e-06, 1.6164223989305805e-06, 1.6164223989305805e-06, 2.309174855615115e-06, 2.309174855615115e-06, 3.2988212223073076e-06, 3.2988212223073076e-06, 4.712601746153297e-06, 4.712601746153297e-06, 6.732288208790424e-06, 6.732288208790424e-06, 9.617554583986322e-06, 9.617554583986322e-06, 1.373936369140903e-05, 1.373936369140903e-05, 1.9627662416298616e-05, 1.9627662416298616e-05, 2.8039517737569454e-05, 2.8039517737569454e-05, 4.005645391081351e-05, 4.005645391081351e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:26:07,440] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=43.222412129686205, CurrSamplesPerSec=40.228276203272024, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 200/1404]  eta: 0:12:53  lr: 0.000040  min_lr: 0.000000  loss: 4.8526 (4.8552)  class_acc: 0.0000 (0.0413)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0006  max mem: 15572
[2025-01-17 08:26:15,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-17 08:26:15,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:26:15,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:26:15,465] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3013
[2025-01-17 08:26:15,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 210/1404]  eta: 0:12:42  lr: 0.000040  min_lr: 0.000000  loss: 4.9057 (4.8546)  class_acc: 0.0000 (0.0417)  loss_scale: 65536.0000 (44570.6919)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0428  max mem: 15572
Epoch: [2]  [ 220/1404]  eta: 0:12:32  lr: 0.000040  min_lr: 0.000000  loss: 4.7853 (4.8504)  class_acc: 0.0417 (0.0422)  loss_scale: 32768.0000 (44036.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0603  max mem: 15572
Epoch: [2]  [ 230/1404]  eta: 0:12:23  lr: 0.000041  min_lr: 0.000000  loss: 4.8055 (4.8511)  class_acc: 0.0000 (0.0417)  loss_scale: 32768.0000 (43548.8139)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.0319  max mem: 15572
Epoch: [2]  [ 240/1404]  eta: 0:12:16  lr: 0.000041  min_lr: 0.000000  loss: 4.8275 (4.8502)  class_acc: 0.0417 (0.0427)  loss_scale: 32768.0000 (43101.4772)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0219  max mem: 15572
Epoch: [2]  [ 250/1404]  eta: 0:12:17  lr: 0.000041  min_lr: 0.000000  loss: 4.8746 (4.8526)  class_acc: 0.0417 (0.0425)  loss_scale: 32768.0000 (42689.7849)  weight_decay: 0.0500 (0.0500)  time: 0.7008  data: 0.0134  max mem: 15572
Epoch: [2]  [ 260/1404]  eta: 0:12:07  lr: 0.000041  min_lr: 0.000000  loss: 4.8227 (4.8491)  class_acc: 0.0833 (0.0444)  loss_scale: 32768.0000 (42309.6398)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.0061  max mem: 15572
Epoch: [2]  [ 270/1404]  eta: 0:11:58  lr: 0.000041  min_lr: 0.000000  loss: 4.7861 (4.8503)  class_acc: 0.0833 (0.0450)  loss_scale: 32768.0000 (41957.5498)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.0031  max mem: 15572
Epoch: [2]  [ 280/1404]  eta: 0:11:48  lr: 0.000041  min_lr: 0.000000  loss: 4.8630 (4.8495)  class_acc: 0.0417 (0.0452)  loss_scale: 32768.0000 (41630.5196)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0031  max mem: 15572
Epoch: [2]  [ 290/1404]  eta: 0:11:38  lr: 0.000041  min_lr: 0.000000  loss: 4.8396 (4.8477)  class_acc: 0.0833 (0.0457)  loss_scale: 32768.0000 (41325.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0009  max mem: 15572
Epoch: [2]  [ 300/1404]  eta: 0:11:31  lr: 0.000042  min_lr: 0.000000  loss: 4.8581 (4.8492)  class_acc: 0.0417 (0.0453)  loss_scale: 32768.0000 (41041.6478)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0084  max mem: 15572
Epoch: [2]  [ 310/1404]  eta: 0:11:23  lr: 0.000042  min_lr: 0.000000  loss: 4.8772 (4.8510)  class_acc: 0.0000 (0.0447)  loss_scale: 32768.0000 (40775.6141)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0082  max mem: 15572
Epoch: [2]  [ 320/1404]  eta: 0:11:16  lr: 0.000042  min_lr: 0.000000  loss: 4.8048 (4.8506)  class_acc: 0.0000 (0.0449)  loss_scale: 32768.0000 (40526.1558)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0288  max mem: 15572
Epoch: [2]  [ 330/1404]  eta: 0:11:10  lr: 0.000042  min_lr: 0.000000  loss: 4.8210 (4.8523)  class_acc: 0.0000 (0.0449)  loss_scale: 32768.0000 (40291.7704)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0941  max mem: 15572
[2025-01-17 08:27:33,203] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:27:33,204] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:27:33,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:27:33,207] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 340/1404]  eta: 0:11:05  lr: 0.000042  min_lr: 0.000000  loss: 4.9002 (4.8536)  class_acc: 0.0000 (0.0445)  loss_scale: 32768.0000 (40743.7889)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.0870  max mem: 15572
Epoch: [2]  [ 350/1404]  eta: 0:11:00  lr: 0.000042  min_lr: 0.000000  loss: 4.7796 (4.8515)  class_acc: 0.0000 (0.0442)  loss_scale: 65536.0000 (41450.1197)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.0844  max mem: 15572
Epoch: [2]  [ 360/1404]  eta: 0:10:56  lr: 0.000042  min_lr: 0.000000  loss: 4.8099 (4.8504)  class_acc: 0.0000 (0.0448)  loss_scale: 65536.0000 (42117.3186)  weight_decay: 0.0500 (0.0500)  time: 0.6743  data: 0.0815  max mem: 15572
Epoch: [2]  [ 370/1404]  eta: 0:10:49  lr: 0.000042  min_lr: 0.000000  loss: 4.8373 (4.8519)  class_acc: 0.0417 (0.0450)  loss_scale: 65536.0000 (42748.5499)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.0187  max mem: 15572
Epoch: [2]  [ 380/1404]  eta: 0:10:42  lr: 0.000043  min_lr: 0.000000  loss: 4.8086 (4.8489)  class_acc: 0.0833 (0.0459)  loss_scale: 65536.0000 (43346.6457)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0007  max mem: 15572
Epoch: [2]  [ 390/1404]  eta: 0:10:34  lr: 0.000043  min_lr: 0.000000  loss: 4.6841 (4.8463)  class_acc: 0.0833 (0.0465)  loss_scale: 65536.0000 (43914.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0007  max mem: 15572
Epoch: [2]  [ 400/1404]  eta: 0:10:29  lr: 0.000043  min_lr: 0.000000  loss: 4.6869 (4.8453)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (44453.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0008  max mem: 15572
Epoch: [2]  [ 410/1404]  eta: 0:10:22  lr: 0.000043  min_lr: 0.000000  loss: 4.7219 (4.8439)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (44966.3066)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0008  max mem: 15572
[2025-01-17 08:28:21,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3219
[2025-01-17 08:28:21,315] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:28:21,323] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3219
[2025-01-17 08:28:21,324] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:28:21,324] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 420/1404]  eta: 0:10:15  lr: 0.000043  min_lr: 0.000000  loss: 4.7481 (4.8424)  class_acc: 0.0417 (0.0472)  loss_scale: 32768.0000 (44676.5606)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0006  max mem: 15572
Epoch: [2]  [ 430/1404]  eta: 0:10:10  lr: 0.000043  min_lr: 0.000000  loss: 4.7685 (4.8420)  class_acc: 0.0417 (0.0470)  loss_scale: 32768.0000 (44400.2599)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0009  max mem: 15572
Epoch: [2]  [ 440/1404]  eta: 0:10:02  lr: 0.000043  min_lr: 0.000000  loss: 4.8431 (4.8421)  class_acc: 0.0000 (0.0467)  loss_scale: 32768.0000 (44136.4898)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.0012  max mem: 15572
Epoch: [2]  [ 450/1404]  eta: 0:09:57  lr: 0.000044  min_lr: 0.000000  loss: 4.8192 (4.8420)  class_acc: 0.0417 (0.0463)  loss_scale: 32768.0000 (43884.4169)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0011  max mem: 15572
Epoch: [2]  [ 460/1404]  eta: 0:09:51  lr: 0.000044  min_lr: 0.000000  loss: 4.8654 (4.8435)  class_acc: 0.0417 (0.0460)  loss_scale: 32768.0000 (43643.2798)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.0008  max mem: 15572
Epoch: [2]  [ 470/1404]  eta: 0:09:42  lr: 0.000044  min_lr: 0.000000  loss: 4.8260 (4.8419)  class_acc: 0.0417 (0.0472)  loss_scale: 32768.0000 (43412.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0006  max mem: 15572
Epoch: [2]  [ 480/1404]  eta: 0:09:35  lr: 0.000044  min_lr: 0.000000  loss: 4.7420 (4.8398)  class_acc: 0.0833 (0.0476)  loss_scale: 32768.0000 (43191.0852)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0007  max mem: 15572
Epoch: [2]  [ 490/1404]  eta: 0:09:28  lr: 0.000044  min_lr: 0.000000  loss: 4.7799 (4.8391)  class_acc: 0.0833 (0.0480)  loss_scale: 32768.0000 (42978.8024)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0006  max mem: 15572
Epoch: [2]  [ 500/1404]  eta: 0:09:22  lr: 0.000044  min_lr: 0.000000  loss: 4.8483 (4.8393)  class_acc: 0.0417 (0.0481)  loss_scale: 32768.0000 (42774.9940)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0006  max mem: 15572
Epoch: [2]  [ 510/1404]  eta: 0:09:16  lr: 0.000044  min_lr: 0.000000  loss: 4.7889 (4.8378)  class_acc: 0.0417 (0.0478)  loss_scale: 32768.0000 (42579.1624)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0299  max mem: 15572
Epoch: [2]  [ 520/1404]  eta: 0:09:09  lr: 0.000044  min_lr: 0.000000  loss: 4.7420 (4.8368)  class_acc: 0.0417 (0.0481)  loss_scale: 32768.0000 (42390.8484)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0495  max mem: 15572
Epoch: [2]  [ 530/1404]  eta: 0:09:02  lr: 0.000045  min_lr: 0.000000  loss: 4.7503 (4.8355)  class_acc: 0.0417 (0.0486)  loss_scale: 32768.0000 (42209.6271)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0202  max mem: 15572
[2025-01-17 08:29:40,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:29:40,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:29:40,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:29:40,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 540/1404]  eta: 0:08:57  lr: 0.000045  min_lr: 0.000000  loss: 4.7812 (4.8354)  class_acc: 0.0417 (0.0491)  loss_scale: 32768.0000 (42095.6747)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0007  max mem: 15572
Epoch: [2]  [ 550/1404]  eta: 0:08:50  lr: 0.000045  min_lr: 0.000000  loss: 4.8269 (4.8343)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (42521.0889)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0008  max mem: 15572
Epoch: [2]  [ 560/1404]  eta: 0:08:43  lr: 0.000045  min_lr: 0.000000  loss: 4.7489 (4.8331)  class_acc: 0.0417 (0.0499)  loss_scale: 65536.0000 (42931.3369)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0010  max mem: 15572
[2025-01-17 08:29:57,625] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3378
[2025-01-17 08:29:57,626] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:29:57,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3378
[2025-01-17 08:29:57,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:29:57,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 570/1404]  eta: 0:08:37  lr: 0.000045  min_lr: 0.000000  loss: 4.7982 (4.8333)  class_acc: 0.0417 (0.0495)  loss_scale: 65536.0000 (43269.8284)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0009  max mem: 15572
Epoch: [2]  [ 580/1404]  eta: 0:08:32  lr: 0.000045  min_lr: 0.000000  loss: 4.8440 (4.8337)  class_acc: 0.0417 (0.0501)  loss_scale: 32768.0000 (43089.0740)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0006  max mem: 15572
Epoch: [2]  [ 590/1404]  eta: 0:08:24  lr: 0.000045  min_lr: 0.000000  loss: 4.8296 (4.8323)  class_acc: 0.0833 (0.0505)  loss_scale: 32768.0000 (42914.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0007  max mem: 15572
Epoch: [2]  [ 600/1404]  eta: 0:08:17  lr: 0.000046  min_lr: 0.000000  loss: 4.7641 (4.8311)  class_acc: 0.0417 (0.0507)  loss_scale: 32768.0000 (42745.6106)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0008  max mem: 15572
Epoch: [2]  [ 610/1404]  eta: 0:08:10  lr: 0.000046  min_lr: 0.000000  loss: 4.7913 (4.8312)  class_acc: 0.0417 (0.0505)  loss_scale: 32768.0000 (42582.3110)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0007  max mem: 15572
Epoch: [2]  [ 620/1404]  eta: 0:08:04  lr: 0.000046  min_lr: 0.000000  loss: 4.7924 (4.8297)  class_acc: 0.0417 (0.0503)  loss_scale: 32768.0000 (42424.2705)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0010  max mem: 15572
Epoch: [2]  [ 630/1404]  eta: 0:07:59  lr: 0.000046  min_lr: 0.000000  loss: 4.7773 (4.8296)  class_acc: 0.0417 (0.0506)  loss_scale: 32768.0000 (42271.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.0012  max mem: 15572
Epoch: [2]  [ 640/1404]  eta: 0:07:54  lr: 0.000046  min_lr: 0.000000  loss: 4.8402 (4.8289)  class_acc: 0.0417 (0.0508)  loss_scale: 32768.0000 (42122.9828)  weight_decay: 0.0500 (0.0500)  time: 0.7084  data: 0.0009  max mem: 15572
Epoch: [2]  [ 650/1404]  eta: 0:07:47  lr: 0.000046  min_lr: 0.000000  loss: 4.7722 (4.8262)  class_acc: 0.0833 (0.0515)  loss_scale: 32768.0000 (41979.2811)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0094  max mem: 15572
Epoch: [2]  [ 660/1404]  eta: 0:07:40  lr: 0.000046  min_lr: 0.000000  loss: 4.6879 (4.8243)  class_acc: 0.0833 (0.0514)  loss_scale: 32768.0000 (41839.9274)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0306  max mem: 15572
Epoch: [2]  [ 670/1404]  eta: 0:07:35  lr: 0.000046  min_lr: 0.000000  loss: 4.7453 (4.8235)  class_acc: 0.0417 (0.0512)  loss_scale: 32768.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0557  max mem: 15572
Epoch: [2]  [ 680/1404]  eta: 0:07:29  lr: 0.000047  min_lr: 0.000000  loss: 4.7625 (4.8221)  class_acc: 0.0417 (0.0515)  loss_scale: 32768.0000 (41573.4978)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0693  max mem: 15572
Epoch: [2]  [ 690/1404]  eta: 0:07:23  lr: 0.000047  min_lr: 0.000000  loss: 4.7590 (4.8208)  class_acc: 0.0417 (0.0518)  loss_scale: 32768.0000 (41446.0666)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0786  max mem: 15572
[2025-01-17 08:31:18,372] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:31:18,372] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:31:18,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:31:18,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 700/1404]  eta: 0:07:17  lr: 0.000047  min_lr: 0.000000  loss: 4.7976 (4.8215)  class_acc: 0.0833 (0.0518)  loss_scale: 32768.0000 (41415.7603)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1038  max mem: 15572
Epoch: [2]  [ 710/1404]  eta: 0:07:10  lr: 0.000047  min_lr: 0.000000  loss: 4.7344 (4.8191)  class_acc: 0.0833 (0.0524)  loss_scale: 65536.0000 (41755.0042)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0608  max mem: 15572
Epoch: [2]  [ 720/1404]  eta: 0:07:04  lr: 0.000047  min_lr: 0.000000  loss: 4.6830 (4.8173)  class_acc: 0.0833 (0.0524)  loss_scale: 65536.0000 (42084.8377)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.0009  max mem: 15572
Epoch: [2]  [ 730/1404]  eta: 0:06:58  lr: 0.000047  min_lr: 0.000000  loss: 4.7420 (4.8156)  class_acc: 0.0417 (0.0526)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0186  max mem: 15572
Epoch: [2]  [ 740/1404]  eta: 0:06:52  lr: 0.000047  min_lr: 0.000000  loss: 4.8320 (4.8162)  class_acc: 0.0417 (0.0526)  loss_scale: 65536.0000 (42717.7976)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.0289  max mem: 15572
Epoch: [2]  [ 750/1404]  eta: 0:06:46  lr: 0.000048  min_lr: 0.000000  loss: 4.8382 (4.8136)  class_acc: 0.0417 (0.0528)  loss_scale: 65536.0000 (43021.6352)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.0112  max mem: 15572
Epoch: [2]  [ 760/1404]  eta: 0:06:40  lr: 0.000048  min_lr: 0.000000  loss: 4.7329 (4.8144)  class_acc: 0.0417 (0.0528)  loss_scale: 65536.0000 (43317.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0008  max mem: 15572
Epoch: [2]  [ 770/1404]  eta: 0:06:33  lr: 0.000048  min_lr: 0.000000  loss: 4.7374 (4.8141)  class_acc: 0.0417 (0.0527)  loss_scale: 65536.0000 (43605.6654)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0009  max mem: 15572
Epoch: [2]  [ 780/1404]  eta: 0:06:27  lr: 0.000048  min_lr: 0.000000  loss: 4.8164 (4.8144)  class_acc: 0.0417 (0.0530)  loss_scale: 65536.0000 (43886.4635)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.0008  max mem: 15572
Epoch: [2]  [ 790/1404]  eta: 0:06:21  lr: 0.000048  min_lr: 0.000000  loss: 4.8169 (4.8139)  class_acc: 0.0833 (0.0532)  loss_scale: 65536.0000 (44160.1618)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.0005  max mem: 15572
Epoch: [2]  [ 800/1404]  eta: 0:06:15  lr: 0.000048  min_lr: 0.000000  loss: 4.8146 (4.8141)  class_acc: 0.0417 (0.0533)  loss_scale: 65536.0000 (44427.0262)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0005  max mem: 15572
Epoch: [2]  [ 810/1404]  eta: 0:06:09  lr: 0.000048  min_lr: 0.000000  loss: 4.8146 (4.8134)  class_acc: 0.0417 (0.0534)  loss_scale: 65536.0000 (44687.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0007  max mem: 15572
Epoch: [2]  [ 820/1404]  eta: 0:06:02  lr: 0.000048  min_lr: 0.000000  loss: 4.7511 (4.8126)  class_acc: 0.0417 (0.0534)  loss_scale: 65536.0000 (44941.2521)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0121  max mem: 15572
[2025-01-17 08:32:38,112] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:32:38,112] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:32:38,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:32:38,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:32:38,694] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3636
[2025-01-17 08:32:38,694] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:32:38,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:32:38,724] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3636
[2025-01-17 08:32:38,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [2]  [ 830/1404]  eta: 0:05:56  lr: 0.000049  min_lr: 0.000000  loss: 4.7502 (4.8117)  class_acc: 0.0417 (0.0533)  loss_scale: 65536.0000 (45267.9471)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.0656  max mem: 15572
Epoch: [2]  [ 840/1404]  eta: 0:05:49  lr: 0.000049  min_lr: 0.000000  loss: 4.7665 (4.8111)  class_acc: 0.0417 (0.0534)  loss_scale: 65536.0000 (45508.9465)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0544  max mem: 15572
Epoch: [2]  [ 850/1404]  eta: 0:05:43  lr: 0.000049  min_lr: 0.000000  loss: 4.7415 (4.8110)  class_acc: 0.0417 (0.0533)  loss_scale: 65536.0000 (45744.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0008  max mem: 15572
Epoch: [2]  [ 860/1404]  eta: 0:05:36  lr: 0.000049  min_lr: 0.000000  loss: 4.7545 (4.8112)  class_acc: 0.0417 (0.0536)  loss_scale: 65536.0000 (45974.1510)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0007  max mem: 15572
Epoch: [2]  [ 870/1404]  eta: 0:05:31  lr: 0.000049  min_lr: 0.000000  loss: 4.8117 (4.8108)  class_acc: 0.0417 (0.0536)  loss_scale: 65536.0000 (46198.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0006  max mem: 15572
Epoch: [2]  [ 880/1404]  eta: 0:05:24  lr: 0.000049  min_lr: 0.000000  loss: 4.8117 (4.8111)  class_acc: 0.0417 (0.0536)  loss_scale: 65536.0000 (46418.2338)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.0006  max mem: 15572
Epoch: [2]  [ 890/1404]  eta: 0:05:18  lr: 0.000049  min_lr: 0.000000  loss: 4.8139 (4.8108)  class_acc: 0.0417 (0.0542)  loss_scale: 65536.0000 (46632.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0005  max mem: 15572
Epoch: [2]  [ 900/1404]  eta: 0:05:12  lr: 0.000050  min_lr: 0.000000  loss: 4.8129 (4.8103)  class_acc: 0.0417 (0.0541)  loss_scale: 65536.0000 (46842.6016)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0006  max mem: 15572
Epoch: [2]  [ 910/1404]  eta: 0:05:05  lr: 0.000050  min_lr: 0.000000  loss: 4.8267 (4.8107)  class_acc: 0.0417 (0.0543)  loss_scale: 65536.0000 (47047.7980)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.0008  max mem: 15572
[2025-01-17 08:33:29,177] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-17 08:33:29,177] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:33:29,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:33:29,230] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3721
[2025-01-17 08:33:29,230] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [ 920/1404]  eta: 0:04:59  lr: 0.000050  min_lr: 0.000000  loss: 4.7632 (4.8089)  class_acc: 0.0833 (0.0546)  loss_scale: 65536.0000 (46963.9088)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0012  max mem: 15572
Epoch: [2]  [ 930/1404]  eta: 0:04:53  lr: 0.000050  min_lr: 0.000000  loss: 4.6053 (4.8074)  class_acc: 0.0833 (0.0547)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0011  max mem: 15572
Epoch: [2]  [ 940/1404]  eta: 0:04:47  lr: 0.000050  min_lr: 0.000000  loss: 4.7197 (4.8070)  class_acc: 0.0417 (0.0548)  loss_scale: 32768.0000 (46662.1892)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0006  max mem: 15572
Epoch: [2]  [ 950/1404]  eta: 0:04:41  lr: 0.000050  min_lr: 0.000000  loss: 4.7561 (4.8058)  class_acc: 0.0833 (0.0553)  loss_scale: 32768.0000 (46516.0883)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0006  max mem: 15572
Epoch: [2]  [ 960/1404]  eta: 0:04:35  lr: 0.000050  min_lr: 0.000000  loss: 4.7714 (4.8064)  class_acc: 0.0417 (0.0552)  loss_scale: 32768.0000 (46373.0281)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.0008  max mem: 15572
Epoch: [2]  [ 970/1404]  eta: 0:04:28  lr: 0.000050  min_lr: 0.000000  loss: 4.8498 (4.8055)  class_acc: 0.0417 (0.0556)  loss_scale: 32768.0000 (46232.9145)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0010  max mem: 15572
Epoch: [2]  [ 980/1404]  eta: 0:04:22  lr: 0.000051  min_lr: 0.000000  loss: 4.7423 (4.8050)  class_acc: 0.0833 (0.0559)  loss_scale: 32768.0000 (46095.6575)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.0008  max mem: 15572
Epoch: [2]  [ 990/1404]  eta: 0:04:15  lr: 0.000051  min_lr: 0.000000  loss: 4.7423 (4.8045)  class_acc: 0.0833 (0.0560)  loss_scale: 32768.0000 (45961.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0008  max mem: 15572
Epoch: [2]  [1000/1404]  eta: 0:04:09  lr: 0.000051  min_lr: 0.000000  loss: 4.7556 (4.8042)  class_acc: 0.0417 (0.0562)  loss_scale: 32768.0000 (45829.3706)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0007  max mem: 15572
Epoch: [2]  [1010/1404]  eta: 0:04:03  lr: 0.000051  min_lr: 0.000000  loss: 4.7389 (4.8036)  class_acc: 0.0833 (0.0562)  loss_scale: 32768.0000 (45700.1780)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0006  max mem: 15572
Epoch: [2]  [1020/1404]  eta: 0:03:57  lr: 0.000051  min_lr: 0.000000  loss: 4.7389 (4.8034)  class_acc: 0.0833 (0.0561)  loss_scale: 32768.0000 (45573.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0006  max mem: 15572
Epoch: [2]  [1030/1404]  eta: 0:03:51  lr: 0.000051  min_lr: 0.000000  loss: 4.7515 (4.8023)  class_acc: 0.0417 (0.0563)  loss_scale: 32768.0000 (45449.3113)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.0007  max mem: 15572
Epoch: [2]  [1040/1404]  eta: 0:03:44  lr: 0.000051  min_lr: 0.000000  loss: 4.5824 (4.8005)  class_acc: 0.0833 (0.0568)  loss_scale: 32768.0000 (45327.4928)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0007  max mem: 15572
[2025-01-17 08:34:47,926] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:34:47,926] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:34:47,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:34:47,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1050/1404]  eta: 0:03:38  lr: 0.000052  min_lr: 0.000000  loss: 4.6856 (4.8002)  class_acc: 0.0833 (0.0570)  loss_scale: 32768.0000 (45488.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0007  max mem: 15572
Epoch: [2]  [1060/1404]  eta: 0:03:32  lr: 0.000052  min_lr: 0.000001  loss: 4.7502 (4.7985)  class_acc: 0.0833 (0.0575)  loss_scale: 65536.0000 (45677.5419)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.0007  max mem: 15572
Epoch: [2]  [1070/1404]  eta: 0:03:26  lr: 0.000052  min_lr: 0.000001  loss: 4.5940 (4.7977)  class_acc: 0.0833 (0.0577)  loss_scale: 65536.0000 (45862.9617)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0010  max mem: 15572
Epoch: [2]  [1080/1404]  eta: 0:03:20  lr: 0.000052  min_lr: 0.000001  loss: 4.7802 (4.7973)  class_acc: 0.0833 (0.0580)  loss_scale: 65536.0000 (46044.9510)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0011  max mem: 15572
Epoch: [2]  [1090/1404]  eta: 0:03:14  lr: 0.000052  min_lr: 0.000001  loss: 4.6960 (4.7961)  class_acc: 0.0833 (0.0582)  loss_scale: 65536.0000 (46223.6040)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0009  max mem: 15572
Epoch: [2]  [1100/1404]  eta: 0:03:07  lr: 0.000052  min_lr: 0.000001  loss: 4.6562 (4.7955)  class_acc: 0.0833 (0.0582)  loss_scale: 65536.0000 (46399.0118)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0007  max mem: 15572
[2025-01-17 08:35:29,025] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-17 08:35:29,026] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:35:29,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-17 08:35:29,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:35:29,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [1110/1404]  eta: 0:03:01  lr: 0.000052  min_lr: 0.000001  loss: 4.6823 (4.7947)  class_acc: 0.0417 (0.0582)  loss_scale: 65536.0000 (46512.2736)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0007  max mem: 15572
Epoch: [2]  [1120/1404]  eta: 0:02:55  lr: 0.000052  min_lr: 0.000001  loss: 4.7437 (4.7934)  class_acc: 0.0833 (0.0586)  loss_scale: 32768.0000 (46389.6664)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0008  max mem: 15572
Epoch: [2]  [1130/1404]  eta: 0:02:49  lr: 0.000053  min_lr: 0.000001  loss: 4.7568 (4.7927)  class_acc: 0.0833 (0.0587)  loss_scale: 32768.0000 (46269.2272)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.0009  max mem: 15572
Epoch: [2]  [1140/1404]  eta: 0:02:43  lr: 0.000053  min_lr: 0.000001  loss: 4.6822 (4.7910)  class_acc: 0.0833 (0.0590)  loss_scale: 32768.0000 (46150.8992)  weight_decay: 0.0500 (0.0500)  time: 0.6911  data: 0.0008  max mem: 15572
Epoch: [2]  [1150/1404]  eta: 0:02:37  lr: 0.000053  min_lr: 0.000001  loss: 4.7050 (4.7908)  class_acc: 0.0417 (0.0592)  loss_scale: 32768.0000 (46034.6273)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0007  max mem: 15572
Epoch: [2]  [1160/1404]  eta: 0:02:31  lr: 0.000053  min_lr: 0.000001  loss: 4.7539 (4.7902)  class_acc: 0.0417 (0.0592)  loss_scale: 32768.0000 (45920.3583)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0012  max mem: 15572
Epoch: [2]  [1170/1404]  eta: 0:02:24  lr: 0.000053  min_lr: 0.000001  loss: 4.7487 (4.7906)  class_acc: 0.0417 (0.0592)  loss_scale: 32768.0000 (45808.0410)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.0011  max mem: 15572
Epoch: [2]  [1180/1404]  eta: 0:02:18  lr: 0.000053  min_lr: 0.000001  loss: 4.7419 (4.7906)  class_acc: 0.0417 (0.0593)  loss_scale: 32768.0000 (45697.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0007  max mem: 15572
Epoch: [2]  [1190/1404]  eta: 0:02:12  lr: 0.000053  min_lr: 0.000001  loss: 4.7419 (4.7903)  class_acc: 0.0417 (0.0594)  loss_scale: 32768.0000 (45589.0647)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0009  max mem: 15572
[2025-01-17 08:36:21,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[5.175138275808422e-07, 5.175138275808422e-07, 7.393054679726318e-07, 7.393054679726318e-07, 1.0561506685323313e-06, 1.0561506685323313e-06, 1.508786669331902e-06, 1.508786669331902e-06, 2.1554095276170026e-06, 2.1554095276170026e-06, 3.0791564680242897e-06, 3.0791564680242897e-06, 4.398794954320414e-06, 4.398794954320414e-06, 6.283992791886307e-06, 6.283992791886307e-06, 8.97713255983758e-06, 8.97713255983758e-06, 1.282447508548226e-05, 1.282447508548226e-05, 1.8320678693546087e-05, 1.8320678693546087e-05, 2.6172398133637267e-05, 2.6172398133637267e-05, 3.7389140190910385e-05, 3.7389140190910385e-05, 5.341305741558627e-05, 5.341305741558627e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:36:21,411] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=42.93158195646454, CurrSamplesPerSec=44.354755929048814, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [1200/1404]  eta: 0:02:06  lr: 0.000054  min_lr: 0.000001  loss: 4.7031 (4.7891)  class_acc: 0.0417 (0.0596)  loss_scale: 32768.0000 (45482.3114)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0009  max mem: 15572
Epoch: [2]  [1210/1404]  eta: 0:02:00  lr: 0.000054  min_lr: 0.000001  loss: 4.6244 (4.7878)  class_acc: 0.0833 (0.0598)  loss_scale: 32768.0000 (45377.3212)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0009  max mem: 15572
Epoch: [2]  [1220/1404]  eta: 0:01:53  lr: 0.000054  min_lr: 0.000001  loss: 4.7074 (4.7873)  class_acc: 0.0833 (0.0598)  loss_scale: 32768.0000 (45274.0508)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0008  max mem: 15572
Epoch: [2]  [1230/1404]  eta: 0:01:47  lr: 0.000054  min_lr: 0.000001  loss: 4.7398 (4.7870)  class_acc: 0.0417 (0.0599)  loss_scale: 32768.0000 (45172.4582)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0009  max mem: 15572
[2025-01-17 08:36:51,701] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:36:51,701] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:36:51,706] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:36:51,706] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [1240/1404]  eta: 0:01:41  lr: 0.000054  min_lr: 0.000001  loss: 4.7671 (4.7868)  class_acc: 0.0417 (0.0598)  loss_scale: 32768.0000 (45151.7164)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0009  max mem: 15572
Epoch: [2]  [1250/1404]  eta: 0:01:35  lr: 0.000054  min_lr: 0.000001  loss: 4.7643 (4.7867)  class_acc: 0.0417 (0.0600)  loss_scale: 65536.0000 (45314.6603)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0010  max mem: 15572
Epoch: [2]  [1260/1404]  eta: 0:01:29  lr: 0.000054  min_lr: 0.000001  loss: 4.6926 (4.7855)  class_acc: 0.0833 (0.0601)  loss_scale: 65536.0000 (45475.0198)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.0009  max mem: 15572
Epoch: [2]  [1270/1404]  eta: 0:01:23  lr: 0.000054  min_lr: 0.000001  loss: 4.6619 (4.7847)  class_acc: 0.0833 (0.0603)  loss_scale: 65536.0000 (45632.8560)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.0008  max mem: 15572
Epoch: [2]  [1280/1404]  eta: 0:01:16  lr: 0.000055  min_lr: 0.000001  loss: 4.6244 (4.7833)  class_acc: 0.0833 (0.0605)  loss_scale: 65536.0000 (45788.2279)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0009  max mem: 15572
Epoch: [2]  [1290/1404]  eta: 0:01:10  lr: 0.000055  min_lr: 0.000001  loss: 4.6652 (4.7829)  class_acc: 0.0833 (0.0606)  loss_scale: 65536.0000 (45941.1929)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0009  max mem: 15572
Epoch: [2]  [1300/1404]  eta: 0:01:04  lr: 0.000055  min_lr: 0.000001  loss: 4.6652 (4.7813)  class_acc: 0.0417 (0.0608)  loss_scale: 65536.0000 (46091.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0010  max mem: 15572
Epoch: [2]  [1310/1404]  eta: 0:00:58  lr: 0.000055  min_lr: 0.000001  loss: 4.7254 (4.7812)  class_acc: 0.0417 (0.0608)  loss_scale: 65536.0000 (46240.1220)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0190  max mem: 15572
Epoch: [2]  [1320/1404]  eta: 0:00:52  lr: 0.000055  min_lr: 0.000001  loss: 4.6900 (4.7805)  class_acc: 0.0417 (0.0609)  loss_scale: 65536.0000 (46386.1923)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0187  max mem: 15572
Epoch: [2]  [1330/1404]  eta: 0:00:45  lr: 0.000055  min_lr: 0.000001  loss: 4.6540 (4.7803)  class_acc: 0.0417 (0.0607)  loss_scale: 65536.0000 (46530.0676)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0391  max mem: 15572
Epoch: [2]  [1340/1404]  eta: 0:00:39  lr: 0.000055  min_lr: 0.000001  loss: 4.7016 (4.7801)  class_acc: 0.0417 (0.0608)  loss_scale: 65536.0000 (46671.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0733  max mem: 15572
Epoch: [2]  [1350/1404]  eta: 0:00:33  lr: 0.000056  min_lr: 0.000001  loss: 4.5640 (4.7783)  class_acc: 0.0833 (0.0610)  loss_scale: 65536.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0876  max mem: 15572
Epoch: [2]  [1360/1404]  eta: 0:00:27  lr: 0.000056  min_lr: 0.000001  loss: 4.5782 (4.7773)  class_acc: 0.0833 (0.0610)  loss_scale: 65536.0000 (46949.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0996  max mem: 15572
[2025-01-17 08:38:09,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:38:09,484] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:38:09,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:38:09,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:38:11,989] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4175
[2025-01-17 08:38:11,989] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4175
[2025-01-17 08:38:11,989] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:38:11,989] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:38:11,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:38:12,882] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4177
[2025-01-17 08:38:12,882] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:38:12,882] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:38:12,882] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4177
[2025-01-17 08:38:12,882] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [2]  [1370/1404]  eta: 0:00:21  lr: 0.000056  min_lr: 0.000001  loss: 4.7781 (4.7774)  class_acc: 0.0417 (0.0612)  loss_scale: 65536.0000 (47084.5806)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.0807  max mem: 15572
Epoch: [2]  [1380/1404]  eta: 0:00:14  lr: 0.000056  min_lr: 0.000001  loss: 4.8010 (4.7777)  class_acc: 0.0417 (0.0611)  loss_scale: 32768.0000 (46980.9124)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0345  max mem: 15572
Epoch: [2]  [1390/1404]  eta: 0:00:08  lr: 0.000056  min_lr: 0.000001  loss: 4.6076 (4.7766)  class_acc: 0.0417 (0.0613)  loss_scale: 32768.0000 (46878.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.0007  max mem: 15572
Epoch: [2]  [1400/1404]  eta: 0:00:02  lr: 0.000056  min_lr: 0.000001  loss: 4.6076 (4.7764)  class_acc: 0.0833 (0.0614)  loss_scale: 32768.0000 (46778.0157)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0005  max mem: 15572
Epoch: [2]  [1403/1404]  eta: 0:00:00  lr: 0.000056  min_lr: 0.000001  loss: 4.6322 (4.7759)  class_acc: 0.0833 (0.0616)  loss_scale: 32768.0000 (46748.0798)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0005  max mem: 15572
Epoch: [2] Total time: 0:14:27 (0.6175 s / it)
Averaged stats: lr: 0.000056  min_lr: 0.000001  loss: 4.6322 (4.7789)  class_acc: 0.0833 (0.0619)  loss_scale: 32768.0000 (46748.0798)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:38  loss: 3.9299 (3.9299)  acc1: 0.0000 (0.0000)  acc5: 55.5556 (55.5556)  time: 6.0170  data: 5.8302  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 4.6445 (4.5214)  acc1: 0.0000 (6.0606)  acc5: 0.0000 (20.2020)  time: 0.8118  data: 0.6185  max mem: 15572
Val:  [ 20/136]  eta: 0:01:10  loss: 4.4398 (4.3727)  acc1: 0.0000 (7.9365)  acc5: 11.1111 (26.7196)  time: 0.3402  data: 0.1547  max mem: 15572
Val:  [ 30/136]  eta: 0:00:58  loss: 3.8193 (4.2508)  acc1: 0.0000 (7.8853)  acc5: 44.4444 (36.0215)  time: 0.4156  data: 0.2277  max mem: 15572
Val:  [ 40/136]  eta: 0:00:46  loss: 3.6297 (4.1989)  acc1: 0.0000 (11.2466)  acc5: 55.5556 (34.5528)  time: 0.3559  data: 0.1671  max mem: 15572
Val:  [ 50/136]  eta: 0:00:41  loss: 4.5133 (4.3170)  acc1: 0.0000 (9.5861)  acc5: 0.0000 (29.0850)  time: 0.3628  data: 0.1757  max mem: 15572
Val:  [ 60/136]  eta: 0:00:35  loss: 4.7378 (4.3987)  acc1: 0.0000 (8.0146)  acc5: 0.0000 (25.3188)  time: 0.4157  data: 0.2183  max mem: 15572
Val:  [ 70/136]  eta: 0:00:30  loss: 4.5992 (4.3081)  acc1: 0.0000 (10.4851)  acc5: 5.5556 (28.1690)  time: 0.3988  data: 0.1990  max mem: 15572
Val:  [ 80/136]  eta: 0:00:25  loss: 4.1798 (4.3108)  acc1: 0.0000 (10.8368)  acc5: 27.7778 (29.0809)  time: 0.4060  data: 0.2091  max mem: 15572
Val:  [ 90/136]  eta: 0:00:20  loss: 4.3128 (4.3344)  acc1: 0.0000 (9.6459)  acc5: 11.1111 (27.2283)  time: 0.3726  data: 0.1666  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 4.6582 (4.3889)  acc1: 0.0000 (8.6909)  acc5: 0.0000 (24.8625)  time: 0.3219  data: 0.1142  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.5651 (4.3985)  acc1: 0.0000 (8.3083)  acc5: 5.5556 (24.8248)  time: 0.2995  data: 0.1033  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.3287 (4.3871)  acc1: 5.5556 (9.0909)  acc5: 16.6667 (25.9412)  time: 0.3512  data: 0.1645  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 4.2177 (4.3756)  acc1: 11.1111 (9.4148)  acc5: 33.3333 (26.9720)  time: 0.2657  data: 0.1069  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 4.4580 (4.3793)  acc1: 5.5556 (9.5823)  acc5: 33.3333 (27.3137)  time: 0.2454  data: 0.1004  max mem: 15572
Val: Total time: 0:00:52 (0.3827 s / it)
* Acc@1 9.234 Acc@5 26.658 loss 4.392
Accuracy of the network on the 4883 val videos: 9.2%
[2025-01-17 08:39:22,202] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 08:39:22,218] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 08:39:22,218] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 08:39:22,219] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 08:39:24,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 08:39:24,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 9.23%
Epoch: [3]  [   0/1404]  eta: 2:53:32  lr: 0.000056  min_lr: 0.000001  loss: 4.4238 (4.4238)  class_acc: 0.0417 (0.0417)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.4164  data: 5.3250  max mem: 15572
Epoch: [3]  [  10/1404]  eta: 0:29:00  lr: 0.000056  min_lr: 0.000001  loss: 4.6633 (4.6979)  class_acc: 0.0417 (0.0455)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2489  data: 0.5619  max mem: 15572
Epoch: [3]  [  20/1404]  eta: 0:22:18  lr: 0.000057  min_lr: 0.000001  loss: 4.6502 (4.6122)  class_acc: 0.0417 (0.0615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.0587  max mem: 15572
Epoch: [3]  [  30/1404]  eta: 0:19:13  lr: 0.000057  min_lr: 0.000001  loss: 4.5596 (4.6019)  class_acc: 0.0833 (0.0753)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0164  max mem: 15572
Epoch: [3]  [  40/1404]  eta: 0:17:54  lr: 0.000057  min_lr: 0.000001  loss: 4.5598 (4.6251)  class_acc: 0.0833 (0.0823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0555  max mem: 15572
Epoch: [3]  [  50/1404]  eta: 0:17:09  lr: 0.000057  min_lr: 0.000001  loss: 4.5757 (4.6241)  class_acc: 0.0833 (0.0850)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0999  max mem: 15572
Epoch: [3]  [  60/1404]  eta: 0:16:33  lr: 0.000057  min_lr: 0.000001  loss: 4.6040 (4.6379)  class_acc: 0.0833 (0.0813)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0685  max mem: 15572
Epoch: [3]  [  70/1404]  eta: 0:15:51  lr: 0.000057  min_lr: 0.000001  loss: 4.7576 (4.6568)  class_acc: 0.0417 (0.0745)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0507  max mem: 15572
Epoch: [3]  [  80/1404]  eta: 0:15:21  lr: 0.000057  min_lr: 0.000001  loss: 4.7576 (4.6546)  class_acc: 0.0000 (0.0730)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0729  max mem: 15572
Epoch: [3]  [  90/1404]  eta: 0:15:12  lr: 0.000057  min_lr: 0.000001  loss: 4.6718 (4.6650)  class_acc: 0.0833 (0.0746)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0896  max mem: 15572
[2025-01-17 08:40:30,038] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:40:30,038] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:40:30,039] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:40:30,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 100/1404]  eta: 0:14:53  lr: 0.000058  min_lr: 0.000001  loss: 4.7334 (4.6725)  class_acc: 0.0833 (0.0751)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0442  max mem: 15572
Epoch: [3]  [ 110/1404]  eta: 0:14:40  lr: 0.000058  min_lr: 0.000001  loss: 4.7310 (4.6757)  class_acc: 0.0833 (0.0766)  loss_scale: 65536.0000 (37786.5225)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0010  max mem: 15572
Epoch: [3]  [ 120/1404]  eta: 0:14:21  lr: 0.000058  min_lr: 0.000001  loss: 4.7241 (4.6811)  class_acc: 0.0833 (0.0740)  loss_scale: 65536.0000 (40079.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0012  max mem: 15572
[2025-01-17 08:40:46,847] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4333
[2025-01-17 08:40:46,847] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:40:46,847] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:40:46,849] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4333
[2025-01-17 08:40:46,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 130/1404]  eta: 0:14:05  lr: 0.000058  min_lr: 0.000001  loss: 4.6891 (4.6783)  class_acc: 0.0417 (0.0763)  loss_scale: 32768.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.0010  max mem: 15572
Epoch: [3]  [ 140/1404]  eta: 0:13:57  lr: 0.000058  min_lr: 0.000001  loss: 4.6796 (4.6793)  class_acc: 0.0833 (0.0765)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0009  max mem: 15572
Epoch: [3]  [ 150/1404]  eta: 0:13:46  lr: 0.000058  min_lr: 0.000001  loss: 4.7563 (4.6802)  class_acc: 0.0417 (0.0770)  loss_scale: 32768.0000 (38627.1788)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.0012  max mem: 15572
Epoch: [3]  [ 160/1404]  eta: 0:13:40  lr: 0.000058  min_lr: 0.000001  loss: 4.7304 (4.6808)  class_acc: 0.0417 (0.0789)  loss_scale: 32768.0000 (38263.2547)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0011  max mem: 15572
Epoch: [3]  [ 170/1404]  eta: 0:13:33  lr: 0.000059  min_lr: 0.000001  loss: 4.5444 (4.6758)  class_acc: 0.0833 (0.0792)  loss_scale: 32768.0000 (37941.8947)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0009  max mem: 15572
Epoch: [3]  [ 180/1404]  eta: 0:13:25  lr: 0.000059  min_lr: 0.000001  loss: 4.6586 (4.6768)  class_acc: 0.0417 (0.0792)  loss_scale: 32768.0000 (37656.0442)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.0008  max mem: 15572
Epoch: [3]  [ 190/1404]  eta: 0:13:09  lr: 0.000059  min_lr: 0.000001  loss: 4.6919 (4.6789)  class_acc: 0.0417 (0.0781)  loss_scale: 32768.0000 (37400.1257)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0007  max mem: 15572
Epoch: [3]  [ 200/1404]  eta: 0:12:54  lr: 0.000059  min_lr: 0.000001  loss: 4.6919 (4.6789)  class_acc: 0.0417 (0.0771)  loss_scale: 32768.0000 (37169.6716)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0008  max mem: 15572
Epoch: [3]  [ 210/1404]  eta: 0:12:46  lr: 0.000059  min_lr: 0.000001  loss: 4.7861 (4.6850)  class_acc: 0.0417 (0.0762)  loss_scale: 32768.0000 (36961.0616)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0009  max mem: 15572
Epoch: [3]  [ 220/1404]  eta: 0:12:37  lr: 0.000059  min_lr: 0.000001  loss: 4.7873 (4.6836)  class_acc: 0.0417 (0.0777)  loss_scale: 32768.0000 (36771.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0009  max mem: 15572
Epoch: [3]  [ 230/1404]  eta: 0:12:31  lr: 0.000059  min_lr: 0.000001  loss: 4.6834 (4.6843)  class_acc: 0.0417 (0.0761)  loss_scale: 32768.0000 (36598.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0007  max mem: 15572
Epoch: [3]  [ 240/1404]  eta: 0:12:26  lr: 0.000059  min_lr: 0.000001  loss: 4.6829 (4.6820)  class_acc: 0.0417 (0.0769)  loss_scale: 32768.0000 (36439.1037)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.0007  max mem: 15572
[2025-01-17 08:42:05,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:42:05,482] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:42:05,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:42:05,482] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 250/1404]  eta: 0:12:18  lr: 0.000060  min_lr: 0.000001  loss: 4.5765 (4.6807)  class_acc: 0.0833 (0.0777)  loss_scale: 32768.0000 (36423.3944)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.0012  max mem: 15572
Epoch: [3]  [ 260/1404]  eta: 0:12:10  lr: 0.000060  min_lr: 0.000001  loss: 4.6568 (4.6798)  class_acc: 0.0833 (0.0795)  loss_scale: 65536.0000 (37538.8199)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0014  max mem: 15572
Epoch: [3]  [ 270/1404]  eta: 0:12:02  lr: 0.000060  min_lr: 0.000001  loss: 4.7179 (4.6793)  class_acc: 0.0833 (0.0792)  loss_scale: 65536.0000 (38571.9262)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0011  max mem: 15572
Epoch: [3]  [ 280/1404]  eta: 0:11:55  lr: 0.000060  min_lr: 0.000001  loss: 4.5547 (4.6736)  class_acc: 0.0833 (0.0804)  loss_scale: 65536.0000 (39531.5018)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0011  max mem: 15572
Epoch: [3]  [ 290/1404]  eta: 0:11:47  lr: 0.000060  min_lr: 0.000001  loss: 4.5414 (4.6744)  class_acc: 0.0833 (0.0798)  loss_scale: 65536.0000 (40425.1271)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0008  max mem: 15572
Epoch: [3]  [ 300/1404]  eta: 0:11:43  lr: 0.000060  min_lr: 0.000001  loss: 4.6791 (4.6759)  class_acc: 0.0833 (0.0796)  loss_scale: 65536.0000 (41259.3754)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.0206  max mem: 15572
Epoch: [3]  [ 310/1404]  eta: 0:11:33  lr: 0.000060  min_lr: 0.000001  loss: 4.6804 (4.6760)  class_acc: 0.0833 (0.0793)  loss_scale: 65536.0000 (42039.9743)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0207  max mem: 15572
Epoch: [3]  [ 320/1404]  eta: 0:11:28  lr: 0.000061  min_lr: 0.000001  loss: 4.6652 (4.6753)  class_acc: 0.0833 (0.0800)  loss_scale: 65536.0000 (42771.9377)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0752  max mem: 15572
Epoch: [3]  [ 330/1404]  eta: 0:11:20  lr: 0.000061  min_lr: 0.000001  loss: 4.6517 (4.6747)  class_acc: 0.0833 (0.0797)  loss_scale: 65536.0000 (43459.6737)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1328  max mem: 15572
Epoch: [3]  [ 340/1404]  eta: 0:11:14  lr: 0.000061  min_lr: 0.000001  loss: 4.6190 (4.6743)  class_acc: 0.0833 (0.0794)  loss_scale: 65536.0000 (44107.0733)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1102  max mem: 15572
[2025-01-17 08:43:06,451] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-17 08:43:06,451] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:43:06,451] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:43:06,547] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4560
[2025-01-17 08:43:06,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 350/1404]  eta: 0:11:08  lr: 0.000061  min_lr: 0.000001  loss: 4.5947 (4.6734)  class_acc: 0.0417 (0.0792)  loss_scale: 65536.0000 (44437.5157)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1272  max mem: 15572
Epoch: [3]  [ 360/1404]  eta: 0:11:01  lr: 0.000061  min_lr: 0.000001  loss: 4.6801 (4.6756)  class_acc: 0.0417 (0.0789)  loss_scale: 32768.0000 (44114.2604)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1324  max mem: 15572
Epoch: [3]  [ 370/1404]  eta: 0:10:55  lr: 0.000061  min_lr: 0.000001  loss: 4.7351 (4.6789)  class_acc: 0.0417 (0.0786)  loss_scale: 32768.0000 (43808.4313)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1337  max mem: 15572
Epoch: [3]  [ 380/1404]  eta: 0:10:45  lr: 0.000061  min_lr: 0.000001  loss: 4.6617 (4.6774)  class_acc: 0.0833 (0.0788)  loss_scale: 32768.0000 (43518.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0893  max mem: 15572
Epoch: [3]  [ 390/1404]  eta: 0:10:40  lr: 0.000061  min_lr: 0.000001  loss: 4.5510 (4.6743)  class_acc: 0.0833 (0.0795)  loss_scale: 32768.0000 (43243.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1076  max mem: 15572
Epoch: [3]  [ 400/1404]  eta: 0:10:33  lr: 0.000062  min_lr: 0.000001  loss: 4.6111 (4.6735)  class_acc: 0.0833 (0.0792)  loss_scale: 32768.0000 (42982.4638)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.1705  max mem: 15572
Epoch: [3]  [ 410/1404]  eta: 0:10:26  lr: 0.000062  min_lr: 0.000001  loss: 4.7178 (4.6736)  class_acc: 0.0417 (0.0788)  loss_scale: 32768.0000 (42733.9367)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1037  max mem: 15572
Epoch: [3]  [ 420/1404]  eta: 0:10:20  lr: 0.000062  min_lr: 0.000001  loss: 4.6884 (4.6733)  class_acc: 0.0417 (0.0782)  loss_scale: 32768.0000 (42497.2162)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0764  max mem: 15572
Epoch: [3]  [ 430/1404]  eta: 0:10:12  lr: 0.000062  min_lr: 0.000001  loss: 4.6619 (4.6734)  class_acc: 0.0417 (0.0787)  loss_scale: 32768.0000 (42271.4803)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0734  max mem: 15572
Epoch: [3]  [ 440/1404]  eta: 0:10:05  lr: 0.000062  min_lr: 0.000001  loss: 4.6333 (4.6731)  class_acc: 0.0833 (0.0791)  loss_scale: 32768.0000 (42055.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0844  max mem: 15572
Epoch: [3]  [ 450/1404]  eta: 0:10:00  lr: 0.000062  min_lr: 0.000001  loss: 4.6874 (4.6746)  class_acc: 0.0833 (0.0787)  loss_scale: 32768.0000 (41850.0399)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1432  max mem: 15572
Epoch: [3]  [ 460/1404]  eta: 0:09:54  lr: 0.000062  min_lr: 0.000001  loss: 4.6777 (4.6725)  class_acc: 0.0833 (0.0794)  loss_scale: 32768.0000 (41653.0325)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.1639  max mem: 15572
Epoch: [3]  [ 470/1404]  eta: 0:09:48  lr: 0.000063  min_lr: 0.000001  loss: 4.6443 (4.6735)  class_acc: 0.0833 (0.0793)  loss_scale: 32768.0000 (41464.3907)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1575  max mem: 15572
[2025-01-17 08:44:25,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:44:25,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:44:25,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:44:25,110] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 480/1404]  eta: 0:09:39  lr: 0.000063  min_lr: 0.000001  loss: 4.6728 (4.6739)  class_acc: 0.0417 (0.0791)  loss_scale: 32768.0000 (41556.0915)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.0867  max mem: 15572
Epoch: [3]  [ 490/1404]  eta: 0:09:33  lr: 0.000063  min_lr: 0.000001  loss: 4.6953 (4.6754)  class_acc: 0.0417 (0.0788)  loss_scale: 65536.0000 (42044.4807)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0551  max mem: 15572
Epoch: [3]  [ 500/1404]  eta: 0:09:26  lr: 0.000063  min_lr: 0.000001  loss: 4.6617 (4.6736)  class_acc: 0.0833 (0.0789)  loss_scale: 65536.0000 (42513.3733)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0800  max mem: 15572
Epoch: [3]  [ 510/1404]  eta: 0:09:20  lr: 0.000063  min_lr: 0.000001  loss: 4.6824 (4.6737)  class_acc: 0.0833 (0.0790)  loss_scale: 65536.0000 (42963.9139)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0584  max mem: 15572
Epoch: [3]  [ 520/1404]  eta: 0:09:13  lr: 0.000063  min_lr: 0.000001  loss: 4.6798 (4.6726)  class_acc: 0.0833 (0.0797)  loss_scale: 65536.0000 (43397.1593)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0873  max mem: 15572
Epoch: [3]  [ 530/1404]  eta: 0:09:05  lr: 0.000063  min_lr: 0.000001  loss: 4.5680 (4.6716)  class_acc: 0.0833 (0.0800)  loss_scale: 65536.0000 (43814.0866)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0634  max mem: 15572
Epoch: [3]  [ 540/1404]  eta: 0:09:00  lr: 0.000063  min_lr: 0.000001  loss: 4.6015 (4.6711)  class_acc: 0.0417 (0.0793)  loss_scale: 65536.0000 (44215.6007)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0690  max mem: 15572
Epoch: [3]  [ 550/1404]  eta: 0:08:53  lr: 0.000064  min_lr: 0.000001  loss: 4.6736 (4.6727)  class_acc: 0.0417 (0.0790)  loss_scale: 65536.0000 (44602.5408)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0689  max mem: 15572
Epoch: [3]  [ 560/1404]  eta: 0:08:46  lr: 0.000064  min_lr: 0.000001  loss: 4.6708 (4.6722)  class_acc: 0.0833 (0.0790)  loss_scale: 65536.0000 (44975.6863)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0143  max mem: 15572
Epoch: [3]  [ 570/1404]  eta: 0:08:40  lr: 0.000064  min_lr: 0.000001  loss: 4.6471 (4.6706)  class_acc: 0.0833 (0.0799)  loss_scale: 65536.0000 (45335.7618)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0612  max mem: 15572
Epoch: [3]  [ 580/1404]  eta: 0:08:33  lr: 0.000064  min_lr: 0.000001  loss: 4.6471 (4.6685)  class_acc: 0.0833 (0.0802)  loss_scale: 65536.0000 (45683.4423)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.0476  max mem: 15572
[2025-01-17 08:45:30,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4796
[2025-01-17 08:45:30,187] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:45:30,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4796
[2025-01-17 08:45:30,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:45:30,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [3]  [ 590/1404]  eta: 0:08:27  lr: 0.000064  min_lr: 0.000001  loss: 4.6406 (4.6681)  class_acc: 0.0417 (0.0807)  loss_scale: 65536.0000 (45631.2420)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0008  max mem: 15572
Epoch: [3]  [ 600/1404]  eta: 0:08:20  lr: 0.000064  min_lr: 0.000001  loss: 4.6352 (4.6675)  class_acc: 0.0833 (0.0805)  loss_scale: 32768.0000 (45417.2113)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0008  max mem: 15572
Epoch: [3]  [ 610/1404]  eta: 0:08:14  lr: 0.000064  min_lr: 0.000001  loss: 4.6334 (4.6667)  class_acc: 0.0833 (0.0807)  loss_scale: 32768.0000 (45210.1866)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0008  max mem: 15572
Epoch: [3]  [ 620/1404]  eta: 0:08:07  lr: 0.000065  min_lr: 0.000001  loss: 4.7512 (4.6684)  class_acc: 0.0833 (0.0806)  loss_scale: 32768.0000 (45009.8293)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0008  max mem: 15572
Epoch: [3]  [ 630/1404]  eta: 0:08:00  lr: 0.000065  min_lr: 0.000001  loss: 4.7031 (4.6683)  class_acc: 0.0417 (0.0804)  loss_scale: 32768.0000 (44815.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0007  max mem: 15572
Epoch: [3]  [ 640/1404]  eta: 0:07:54  lr: 0.000065  min_lr: 0.000001  loss: 4.6696 (4.6675)  class_acc: 0.0833 (0.0803)  loss_scale: 32768.0000 (44627.8690)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0008  max mem: 15572
Epoch: [3]  [ 650/1404]  eta: 0:07:47  lr: 0.000065  min_lr: 0.000001  loss: 4.6085 (4.6671)  class_acc: 0.0833 (0.0802)  loss_scale: 32768.0000 (44445.6897)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0010  max mem: 15572
Epoch: [3]  [ 660/1404]  eta: 0:07:41  lr: 0.000065  min_lr: 0.000001  loss: 4.5990 (4.6658)  class_acc: 0.0417 (0.0801)  loss_scale: 32768.0000 (44269.0227)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0009  max mem: 15572
Epoch: [3]  [ 670/1404]  eta: 0:07:35  lr: 0.000065  min_lr: 0.000001  loss: 4.6032 (4.6671)  class_acc: 0.0833 (0.0804)  loss_scale: 32768.0000 (44097.6215)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.0008  max mem: 15572
Epoch: [3]  [ 680/1404]  eta: 0:07:28  lr: 0.000065  min_lr: 0.000001  loss: 4.7018 (4.6664)  class_acc: 0.0833 (0.0806)  loss_scale: 32768.0000 (43931.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0008  max mem: 15572
Epoch: [3]  [ 690/1404]  eta: 0:07:22  lr: 0.000065  min_lr: 0.000001  loss: 4.6634 (4.6673)  class_acc: 0.0833 (0.0806)  loss_scale: 32768.0000 (43769.7019)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0462  max mem: 15572
Epoch: [3]  [ 700/1404]  eta: 0:07:16  lr: 0.000066  min_lr: 0.000001  loss: 4.6342 (4.6655)  class_acc: 0.0833 (0.0810)  loss_scale: 32768.0000 (43612.7589)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0984  max mem: 15572
Epoch: [3]  [ 710/1404]  eta: 0:07:10  lr: 0.000066  min_lr: 0.000001  loss: 4.6182 (4.6657)  class_acc: 0.0833 (0.0812)  loss_scale: 32768.0000 (43460.2307)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0682  max mem: 15572
[2025-01-17 08:46:48,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:46:48,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:46:48,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:46:48,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 720/1404]  eta: 0:07:04  lr: 0.000066  min_lr: 0.000001  loss: 4.7124 (4.6670)  class_acc: 0.0833 (0.0812)  loss_scale: 32768.0000 (43675.5173)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.0718  max mem: 15572
Epoch: [3]  [ 730/1404]  eta: 0:06:58  lr: 0.000066  min_lr: 0.000001  loss: 4.6790 (4.6674)  class_acc: 0.0833 (0.0813)  loss_scale: 65536.0000 (43974.5663)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1191  max mem: 15572
Epoch: [3]  [ 740/1404]  eta: 0:06:51  lr: 0.000066  min_lr: 0.000001  loss: 4.5829 (4.6656)  class_acc: 0.0833 (0.0818)  loss_scale: 65536.0000 (44265.5439)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0862  max mem: 15572
[2025-01-17 08:47:09,186] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-17 08:47:09,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:47:09,197] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4959
[2025-01-17 08:47:09,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:47:09,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [ 750/1404]  eta: 0:06:45  lr: 0.000066  min_lr: 0.000001  loss: 4.4713 (4.6632)  class_acc: 0.1250 (0.0824)  loss_scale: 65536.0000 (44374.2423)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1140  max mem: 15572
Epoch: [3]  [ 760/1404]  eta: 0:06:39  lr: 0.000066  min_lr: 0.000001  loss: 4.5803 (4.6627)  class_acc: 0.1250 (0.0825)  loss_scale: 32768.0000 (44221.7293)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.1454  max mem: 15572
Epoch: [3]  [ 770/1404]  eta: 0:06:33  lr: 0.000067  min_lr: 0.000001  loss: 4.6195 (4.6628)  class_acc: 0.0417 (0.0823)  loss_scale: 32768.0000 (44073.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1497  max mem: 15572
Epoch: [3]  [ 780/1404]  eta: 0:06:26  lr: 0.000067  min_lr: 0.000001  loss: 4.6195 (4.6626)  class_acc: 0.0833 (0.0826)  loss_scale: 32768.0000 (43928.4200)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1341  max mem: 15572
[2025-01-17 08:47:33,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[6.469246371784522e-07, 6.469246371784522e-07, 9.241780531120747e-07, 9.241780531120747e-07, 1.3202543615886782e-06, 1.3202543615886782e-06, 1.8860776594123977e-06, 1.8860776594123977e-06, 2.694396656303425e-06, 2.694396656303425e-06, 3.849138080433465e-06, 3.849138080433465e-06, 5.498768686333522e-06, 5.498768686333522e-06, 7.855383837619317e-06, 7.855383837619317e-06, 1.1221976910884738e-05, 1.1221976910884738e-05, 1.60313955869782e-05, 1.60313955869782e-05, 2.290199369568314e-05, 2.290199369568314e-05, 3.271713385097592e-05, 3.271713385097592e-05, 4.673876264425132e-05, 4.673876264425132e-05, 6.676966092035903e-05, 6.676966092035903e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:47:33,493] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=43.33505875213173, CurrSamplesPerSec=54.50474965238522, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 790/1404]  eta: 0:06:20  lr: 0.000067  min_lr: 0.000001  loss: 4.6398 (4.6623)  class_acc: 0.0833 (0.0827)  loss_scale: 32768.0000 (43787.3274)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0932  max mem: 15572
Epoch: [3]  [ 800/1404]  eta: 0:06:13  lr: 0.000067  min_lr: 0.000001  loss: 4.6134 (4.6618)  class_acc: 0.0833 (0.0828)  loss_scale: 32768.0000 (43649.7578)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0669  max mem: 15572
Epoch: [3]  [ 810/1404]  eta: 0:06:07  lr: 0.000067  min_lr: 0.000001  loss: 4.6100 (4.6611)  class_acc: 0.0833 (0.0830)  loss_scale: 32768.0000 (43515.5808)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0618  max mem: 15572
Epoch: [3]  [ 820/1404]  eta: 0:06:01  lr: 0.000067  min_lr: 0.000001  loss: 4.6192 (4.6592)  class_acc: 0.1250 (0.0835)  loss_scale: 32768.0000 (43384.6724)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1171  max mem: 15572
Epoch: [3]  [ 830/1404]  eta: 0:05:55  lr: 0.000067  min_lr: 0.000001  loss: 4.6192 (4.6597)  class_acc: 0.0833 (0.0835)  loss_scale: 32768.0000 (43256.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.1104  max mem: 15572
Epoch: [3]  [ 840/1404]  eta: 0:05:48  lr: 0.000067  min_lr: 0.000001  loss: 4.6177 (4.6596)  class_acc: 0.0833 (0.0833)  loss_scale: 32768.0000 (43132.1950)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.0630  max mem: 15572
Epoch: [3]  [ 850/1404]  eta: 0:05:42  lr: 0.000068  min_lr: 0.000001  loss: 4.6639 (4.6605)  class_acc: 0.0417 (0.0834)  loss_scale: 32768.0000 (43010.4066)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1052  max mem: 15572
Epoch: [3]  [ 860/1404]  eta: 0:05:35  lr: 0.000068  min_lr: 0.000001  loss: 4.6429 (4.6596)  class_acc: 0.0833 (0.0836)  loss_scale: 32768.0000 (42891.4472)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1025  max mem: 15572
Epoch: [3]  [ 870/1404]  eta: 0:05:30  lr: 0.000068  min_lr: 0.000001  loss: 4.5391 (4.6599)  class_acc: 0.0833 (0.0836)  loss_scale: 32768.0000 (42775.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1342  max mem: 15572
[2025-01-17 08:48:28,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:48:28,886] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:48:28,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:48:28,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [ 880/1404]  eta: 0:05:24  lr: 0.000068  min_lr: 0.000001  loss: 4.4690 (4.6574)  class_acc: 0.0833 (0.0840)  loss_scale: 32768.0000 (42847.6005)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.2080  max mem: 15572
Epoch: [3]  [ 890/1404]  eta: 0:05:17  lr: 0.000068  min_lr: 0.000001  loss: 4.5829 (4.6575)  class_acc: 0.0833 (0.0840)  loss_scale: 65536.0000 (43102.2402)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1103  max mem: 15572
Epoch: [3]  [ 900/1404]  eta: 0:05:12  lr: 0.000068  min_lr: 0.000001  loss: 4.6836 (4.6586)  class_acc: 0.0833 (0.0839)  loss_scale: 65536.0000 (43351.2275)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1487  max mem: 15572
Epoch: [3]  [ 910/1404]  eta: 0:05:05  lr: 0.000068  min_lr: 0.000001  loss: 4.6953 (4.6575)  class_acc: 0.0417 (0.0843)  loss_scale: 65536.0000 (43594.7486)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1546  max mem: 15572
Epoch: [3]  [ 920/1404]  eta: 0:04:59  lr: 0.000069  min_lr: 0.000001  loss: 4.6522 (4.6580)  class_acc: 0.0833 (0.0842)  loss_scale: 65536.0000 (43832.9815)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1360  max mem: 15572
Epoch: [3]  [ 930/1404]  eta: 0:04:53  lr: 0.000069  min_lr: 0.000001  loss: 4.4699 (4.6564)  class_acc: 0.0833 (0.0847)  loss_scale: 65536.0000 (44066.0967)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1449  max mem: 15572
Epoch: [3]  [ 940/1404]  eta: 0:04:47  lr: 0.000069  min_lr: 0.000001  loss: 4.6286 (4.6565)  class_acc: 0.0833 (0.0847)  loss_scale: 65536.0000 (44294.2572)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0649  max mem: 15572
Epoch: [3]  [ 950/1404]  eta: 0:04:41  lr: 0.000069  min_lr: 0.000001  loss: 4.6788 (4.6561)  class_acc: 0.0833 (0.0852)  loss_scale: 65536.0000 (44517.6193)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0926  max mem: 15572
Epoch: [3]  [ 960/1404]  eta: 0:04:35  lr: 0.000069  min_lr: 0.000001  loss: 4.6123 (4.6550)  class_acc: 0.1250 (0.0855)  loss_scale: 65536.0000 (44736.3330)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.1203  max mem: 15572
Epoch: [3]  [ 970/1404]  eta: 0:04:29  lr: 0.000069  min_lr: 0.000001  loss: 4.7231 (4.6563)  class_acc: 0.0833 (0.0854)  loss_scale: 65536.0000 (44950.5417)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.1627  max mem: 15572
Epoch: [3]  [ 980/1404]  eta: 0:04:22  lr: 0.000069  min_lr: 0.000001  loss: 4.7365 (4.6562)  class_acc: 0.0833 (0.0853)  loss_scale: 65536.0000 (45160.3833)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1039  max mem: 15572
Epoch: [3]  [ 990/1404]  eta: 0:04:16  lr: 0.000069  min_lr: 0.000001  loss: 4.7290 (4.6568)  class_acc: 0.0833 (0.0854)  loss_scale: 65536.0000 (45365.9899)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0571  max mem: 15572
[2025-01-17 08:49:45,031] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5212
[2025-01-17 08:49:45,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:49:45,056] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5212
[2025-01-17 08:49:45,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:49:45,057] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1000/1404]  eta: 0:04:10  lr: 0.000070  min_lr: 0.000001  loss: 4.6480 (4.6555)  class_acc: 0.0833 (0.0857)  loss_scale: 65536.0000 (45534.7532)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0816  max mem: 15572
Epoch: [3]  [1010/1404]  eta: 0:04:03  lr: 0.000070  min_lr: 0.000001  loss: 4.5776 (4.6548)  class_acc: 0.0833 (0.0859)  loss_scale: 32768.0000 (45408.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0680  max mem: 15572
Epoch: [3]  [1020/1404]  eta: 0:03:57  lr: 0.000070  min_lr: 0.000001  loss: 4.5455 (4.6536)  class_acc: 0.0833 (0.0857)  loss_scale: 32768.0000 (45284.6699)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.0601  max mem: 15572
Epoch: [3]  [1030/1404]  eta: 0:03:51  lr: 0.000070  min_lr: 0.000001  loss: 4.5454 (4.6537)  class_acc: 0.0833 (0.0857)  loss_scale: 32768.0000 (45163.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0580  max mem: 15572
Epoch: [3]  [1040/1404]  eta: 0:03:44  lr: 0.000070  min_lr: 0.000001  loss: 4.5445 (4.6527)  class_acc: 0.0833 (0.0858)  loss_scale: 32768.0000 (45044.1960)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0412  max mem: 15572
Epoch: [3]  [1050/1404]  eta: 0:03:38  lr: 0.000070  min_lr: 0.000001  loss: 4.5358 (4.6525)  class_acc: 0.0833 (0.0855)  loss_scale: 32768.0000 (44927.3911)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.0775  max mem: 15572
Epoch: [3]  [1060/1404]  eta: 0:03:32  lr: 0.000070  min_lr: 0.000001  loss: 4.5358 (4.6516)  class_acc: 0.0833 (0.0855)  loss_scale: 32768.0000 (44812.7879)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.1515  max mem: 15572
Epoch: [3]  [1070/1404]  eta: 0:03:26  lr: 0.000071  min_lr: 0.000001  loss: 4.4678 (4.6503)  class_acc: 0.1250 (0.0859)  loss_scale: 32768.0000 (44700.3249)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1200  max mem: 15572
Epoch: [3]  [1080/1404]  eta: 0:03:20  lr: 0.000071  min_lr: 0.000001  loss: 4.5898 (4.6514)  class_acc: 0.0833 (0.0858)  loss_scale: 32768.0000 (44589.9426)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1247  max mem: 15572
Epoch: [3]  [1090/1404]  eta: 0:03:14  lr: 0.000071  min_lr: 0.000001  loss: 4.7799 (4.6525)  class_acc: 0.0417 (0.0856)  loss_scale: 32768.0000 (44481.5839)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1239  max mem: 15572
Epoch: [3]  [1100/1404]  eta: 0:03:08  lr: 0.000071  min_lr: 0.000001  loss: 4.6736 (4.6512)  class_acc: 0.0833 (0.0860)  loss_scale: 32768.0000 (44375.1935)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1113  max mem: 15572
Epoch: [3]  [1110/1404]  eta: 0:03:01  lr: 0.000071  min_lr: 0.000001  loss: 4.4416 (4.6498)  class_acc: 0.1250 (0.0864)  loss_scale: 32768.0000 (44270.7183)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.1280  max mem: 15572
Epoch: [3]  [1120/1404]  eta: 0:02:55  lr: 0.000071  min_lr: 0.000001  loss: 4.4767 (4.6500)  class_acc: 0.0833 (0.0864)  loss_scale: 32768.0000 (44168.1070)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0818  max mem: 15572
[2025-01-17 08:51:02,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:51:02,811] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:51:02,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:51:02,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1130/1404]  eta: 0:02:49  lr: 0.000071  min_lr: 0.000001  loss: 4.6863 (4.6500)  class_acc: 0.0833 (0.0862)  loss_scale: 32768.0000 (44125.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.0680  max mem: 15572
Epoch: [3]  [1140/1404]  eta: 0:02:44  lr: 0.000071  min_lr: 0.000001  loss: 4.6863 (4.6503)  class_acc: 0.0833 (0.0864)  loss_scale: 65536.0000 (44312.9045)  weight_decay: 0.0500 (0.0500)  time: 0.9850  data: 0.0482  max mem: 15572
[2025-01-17 08:51:22,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5361
[2025-01-17 08:51:22,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:51:22,286] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5361
[2025-01-17 08:51:22,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:51:22,400] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1150/1404]  eta: 0:02:38  lr: 0.000072  min_lr: 0.000001  loss: 4.6854 (4.6494)  class_acc: 0.0833 (0.0867)  loss_scale: 65536.0000 (44440.3545)  weight_decay: 0.0500 (0.0500)  time: 0.9268  data: 0.0005  max mem: 15572
Epoch: [3]  [1160/1404]  eta: 0:02:31  lr: 0.000072  min_lr: 0.000001  loss: 4.5118 (4.6470)  class_acc: 0.0417 (0.0867)  loss_scale: 32768.0000 (44339.8174)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0006  max mem: 15572
Epoch: [3]  [1170/1404]  eta: 0:02:25  lr: 0.000072  min_lr: 0.000001  loss: 4.5157 (4.6458)  class_acc: 0.0833 (0.0868)  loss_scale: 32768.0000 (44240.9974)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0006  max mem: 15572
Epoch: [3]  [1180/1404]  eta: 0:02:19  lr: 0.000072  min_lr: 0.000001  loss: 4.5157 (4.6452)  class_acc: 0.0833 (0.0869)  loss_scale: 32768.0000 (44143.8510)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0006  max mem: 15572
Epoch: [3]  [1190/1404]  eta: 0:02:12  lr: 0.000072  min_lr: 0.000001  loss: 4.7203 (4.6466)  class_acc: 0.0833 (0.0869)  loss_scale: 32768.0000 (44048.3359)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0007  max mem: 15572
Epoch: [3]  [1200/1404]  eta: 0:02:06  lr: 0.000072  min_lr: 0.000001  loss: 4.7310 (4.6464)  class_acc: 0.0833 (0.0871)  loss_scale: 32768.0000 (43954.4113)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.0008  max mem: 15572
Epoch: [3]  [1210/1404]  eta: 0:02:00  lr: 0.000072  min_lr: 0.000001  loss: 4.5601 (4.6450)  class_acc: 0.0833 (0.0873)  loss_scale: 32768.0000 (43862.0380)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0008  max mem: 15572
Epoch: [3]  [1220/1404]  eta: 0:01:54  lr: 0.000073  min_lr: 0.000001  loss: 4.5405 (4.6447)  class_acc: 0.0833 (0.0874)  loss_scale: 32768.0000 (43771.1777)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0007  max mem: 15572
Epoch: [3]  [1230/1404]  eta: 0:01:48  lr: 0.000073  min_lr: 0.000001  loss: 4.5417 (4.6440)  class_acc: 0.0833 (0.0877)  loss_scale: 32768.0000 (43681.7937)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0008  max mem: 15572
Epoch: [3]  [1240/1404]  eta: 0:01:41  lr: 0.000073  min_lr: 0.000001  loss: 4.5754 (4.6441)  class_acc: 0.0833 (0.0876)  loss_scale: 32768.0000 (43593.8501)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.0009  max mem: 15572
Epoch: [3]  [1250/1404]  eta: 0:01:35  lr: 0.000073  min_lr: 0.000001  loss: 4.6175 (4.6435)  class_acc: 0.0833 (0.0878)  loss_scale: 32768.0000 (43507.3125)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0008  max mem: 15572
Epoch: [3]  [1260/1404]  eta: 0:01:29  lr: 0.000073  min_lr: 0.000001  loss: 4.6391 (4.6431)  class_acc: 0.0833 (0.0878)  loss_scale: 32768.0000 (43422.1475)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.0007  max mem: 15572
Epoch: [3]  [1270/1404]  eta: 0:01:23  lr: 0.000073  min_lr: 0.000001  loss: 4.6558 (4.6436)  class_acc: 0.0833 (0.0878)  loss_scale: 32768.0000 (43338.3226)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.0136  max mem: 15572
[2025-01-17 08:52:41,775] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:52:41,775] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:52:41,824] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:52:41,824] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [1280/1404]  eta: 0:01:17  lr: 0.000073  min_lr: 0.000001  loss: 4.5006 (4.6418)  class_acc: 0.0833 (0.0879)  loss_scale: 32768.0000 (43332.5464)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.0829  max mem: 15572
Epoch: [3]  [1290/1404]  eta: 0:01:11  lr: 0.000073  min_lr: 0.000001  loss: 4.4444 (4.6404)  class_acc: 0.1250 (0.0885)  loss_scale: 65536.0000 (43504.5329)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.1340  max mem: 15572
Epoch: [3]  [1300/1404]  eta: 0:01:04  lr: 0.000074  min_lr: 0.000001  loss: 4.5075 (4.6393)  class_acc: 0.1250 (0.0885)  loss_scale: 65536.0000 (43673.8755)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0919  max mem: 15572
Epoch: [3]  [1310/1404]  eta: 0:00:58  lr: 0.000074  min_lr: 0.000001  loss: 4.5318 (4.6395)  class_acc: 0.0833 (0.0886)  loss_scale: 65536.0000 (43840.6346)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0963  max mem: 15572
[2025-01-17 08:53:04,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5528
[2025-01-17 08:53:04,606] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:53:04,763] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5528
[2025-01-17 08:53:04,763] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:53:04,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [1320/1404]  eta: 0:00:52  lr: 0.000074  min_lr: 0.000001  loss: 4.5627 (4.6388)  class_acc: 0.0833 (0.0888)  loss_scale: 65536.0000 (43880.8418)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1239  max mem: 15572
Epoch: [3]  [1330/1404]  eta: 0:00:46  lr: 0.000074  min_lr: 0.000001  loss: 4.5218 (4.6373)  class_acc: 0.0833 (0.0890)  loss_scale: 32768.0000 (43797.3494)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.0555  max mem: 15572
Epoch: [3]  [1340/1404]  eta: 0:00:39  lr: 0.000074  min_lr: 0.000001  loss: 4.5218 (4.6375)  class_acc: 0.0833 (0.0890)  loss_scale: 32768.0000 (43715.1022)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0611  max mem: 15572
Epoch: [3]  [1350/1404]  eta: 0:00:33  lr: 0.000074  min_lr: 0.000001  loss: 4.6679 (4.6377)  class_acc: 0.0833 (0.0889)  loss_scale: 32768.0000 (43634.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0993  max mem: 15572
Epoch: [3]  [1360/1404]  eta: 0:00:27  lr: 0.000074  min_lr: 0.000001  loss: 4.6099 (4.6368)  class_acc: 0.0833 (0.0890)  loss_scale: 32768.0000 (43554.2337)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.0574  max mem: 15572
Epoch: [3]  [1370/1404]  eta: 0:00:21  lr: 0.000075  min_lr: 0.000001  loss: 4.5294 (4.6357)  class_acc: 0.0833 (0.0890)  loss_scale: 32768.0000 (43475.5594)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.0618  max mem: 15572
Epoch: [3]  [1380/1404]  eta: 0:00:14  lr: 0.000075  min_lr: 0.000001  loss: 4.5682 (4.6351)  class_acc: 0.0833 (0.0892)  loss_scale: 32768.0000 (43398.0246)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.1108  max mem: 15572
Epoch: [3]  [1390/1404]  eta: 0:00:08  lr: 0.000075  min_lr: 0.000001  loss: 4.5198 (4.6344)  class_acc: 0.0833 (0.0892)  loss_scale: 32768.0000 (43321.6046)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1365  max mem: 15572
Epoch: [3]  [1400/1404]  eta: 0:00:02  lr: 0.000075  min_lr: 0.000001  loss: 4.5814 (4.6343)  class_acc: 0.0833 (0.0893)  loss_scale: 32768.0000 (43246.2755)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0689  max mem: 15572
Epoch: [3]  [1403/1404]  eta: 0:00:00  lr: 0.000075  min_lr: 0.000001  loss: 4.5814 (4.6340)  class_acc: 0.0833 (0.0892)  loss_scale: 32768.0000 (43223.8860)  weight_decay: 0.0500 (0.0500)  time: 0.4269  data: 0.0004  max mem: 15572
Epoch: [3] Total time: 0:14:30 (0.6201 s / it)
Averaged stats: lr: 0.000075  min_lr: 0.000001  loss: 4.5814 (4.6254)  class_acc: 0.0833 (0.0909)  loss_scale: 32768.0000 (43223.8860)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:30  loss: 2.6385 (2.6385)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 5.5220  data: 5.2941  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 4.1457 (4.0916)  acc1: 0.0000 (13.6364)  acc5: 16.6667 (27.7778)  time: 0.7958  data: 0.5540  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 4.1427 (4.0420)  acc1: 0.0000 (10.8466)  acc5: 16.6667 (32.0106)  time: 0.3319  data: 0.1040  max mem: 15572
Val:  [ 30/136]  eta: 0:00:49  loss: 3.4761 (3.8600)  acc1: 5.5556 (17.0251)  acc5: 55.5556 (40.8602)  time: 0.2919  data: 0.0753  max mem: 15572
Val:  [ 40/136]  eta: 0:00:42  loss: 3.1467 (3.8109)  acc1: 5.5556 (18.6992)  acc5: 72.2222 (41.4634)  time: 0.3101  data: 0.0979  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 4.3938 (3.9632)  acc1: 0.0000 (15.6863)  acc5: 0.0000 (34.8584)  time: 0.3889  data: 0.1831  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 4.5655 (4.0678)  acc1: 0.0000 (13.2058)  acc5: 0.0000 (31.4208)  time: 0.4157  data: 0.2123  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 4.4178 (4.0080)  acc1: 0.0000 (15.0235)  acc5: 22.2222 (33.8028)  time: 0.4346  data: 0.2329  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 4.1340 (4.0297)  acc1: 0.0000 (15.0206)  acc5: 33.3333 (33.8134)  time: 0.3701  data: 0.1603  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 4.1865 (4.0439)  acc1: 0.0000 (14.2247)  acc5: 33.3333 (34.0659)  time: 0.2990  data: 0.0943  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 4.1944 (4.0643)  acc1: 0.0000 (13.3113)  acc5: 27.7778 (33.8834)  time: 0.3049  data: 0.1143  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 4.3614 (4.0926)  acc1: 0.0000 (12.5125)  acc5: 11.1111 (32.4825)  time: 0.3232  data: 0.1260  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 4.1266 (4.0553)  acc1: 5.5556 (14.0496)  acc5: 22.2222 (35.1240)  time: 0.3579  data: 0.1542  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.6242 (4.0316)  acc1: 16.6667 (15.1399)  acc5: 44.4444 (35.6234)  time: 0.3483  data: 0.1659  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.8528 (4.0332)  acc1: 16.6667 (15.0287)  acc5: 44.4444 (36.1589)  time: 0.2558  data: 0.0937  max mem: 15572
Val: Total time: 0:00:51 (0.3772 s / it)
* Acc@1 15.192 Acc@5 35.831 loss 4.057
Accuracy of the network on the 4883 val videos: 15.2%
[2025-01-17 08:54:47,371] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 08:54:47,374] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 08:54:47,374] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 08:54:47,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 08:54:49,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 08:54:49,996] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 15.19%
Epoch: [4]  [   0/1404]  eta: 2:41:32  lr: 0.000075  min_lr: 0.000001  loss: 4.0623 (4.0623)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.9037  data: 6.4598  max mem: 15572
Epoch: [4]  [  10/1404]  eta: 0:26:57  lr: 0.000075  min_lr: 0.000001  loss: 4.6356 (4.5405)  class_acc: 0.0833 (0.1212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1602  data: 0.6613  max mem: 15572
Epoch: [4]  [  20/1404]  eta: 0:20:51  lr: 0.000075  min_lr: 0.000001  loss: 4.6356 (4.5655)  class_acc: 0.0833 (0.1250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0658  max mem: 15572
Epoch: [4]  [  30/1404]  eta: 0:18:54  lr: 0.000075  min_lr: 0.000001  loss: 4.6305 (4.5826)  class_acc: 0.0833 (0.1116)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0628  max mem: 15572
Epoch: [4]  [  40/1404]  eta: 0:17:22  lr: 0.000076  min_lr: 0.000001  loss: 4.5490 (4.5694)  class_acc: 0.0833 (0.1118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0384  max mem: 15572
[2025-01-17 08:55:22,415] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:55:22,416] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:55:22,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:55:22,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [  50/1404]  eta: 0:16:38  lr: 0.000076  min_lr: 0.000001  loss: 4.5441 (4.5555)  class_acc: 0.0833 (0.1144)  loss_scale: 32768.0000 (39193.0980)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0011  max mem: 15572
Epoch: [4]  [  60/1404]  eta: 0:16:00  lr: 0.000076  min_lr: 0.000001  loss: 4.5384 (4.5384)  class_acc: 0.0833 (0.1168)  loss_scale: 65536.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0325  max mem: 15572
Epoch: [4]  [  70/1404]  eta: 0:15:37  lr: 0.000076  min_lr: 0.000001  loss: 4.5384 (4.5383)  class_acc: 0.1250 (0.1191)  loss_scale: 65536.0000 (46613.6338)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0324  max mem: 15572
Epoch: [4]  [  80/1404]  eta: 0:15:19  lr: 0.000076  min_lr: 0.000001  loss: 4.5864 (4.5408)  class_acc: 0.0833 (0.1127)  loss_scale: 65536.0000 (48949.7284)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0630  max mem: 15572
Epoch: [4]  [  90/1404]  eta: 0:15:02  lr: 0.000076  min_lr: 0.000001  loss: 4.5579 (4.5475)  class_acc: 0.0833 (0.1122)  loss_scale: 65536.0000 (50772.3956)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1215  max mem: 15572
Epoch: [4]  [ 100/1404]  eta: 0:14:43  lr: 0.000076  min_lr: 0.000001  loss: 4.5931 (4.5511)  class_acc: 0.0833 (0.1101)  loss_scale: 65536.0000 (52234.1386)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0593  max mem: 15572
Epoch: [4]  [ 110/1404]  eta: 0:14:32  lr: 0.000076  min_lr: 0.000001  loss: 4.5931 (4.5462)  class_acc: 0.0833 (0.1164)  loss_scale: 65536.0000 (53432.5045)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.0008  max mem: 15572
Epoch: [4]  [ 120/1404]  eta: 0:14:24  lr: 0.000077  min_lr: 0.000001  loss: 4.5553 (4.5478)  class_acc: 0.0833 (0.1143)  loss_scale: 65536.0000 (54432.7934)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.0008  max mem: 15572
Epoch: [4]  [ 130/1404]  eta: 0:14:09  lr: 0.000077  min_lr: 0.000001  loss: 4.6338 (4.5550)  class_acc: 0.0833 (0.1145)  loss_scale: 65536.0000 (55280.3664)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0007  max mem: 15572
Epoch: [4]  [ 140/1404]  eta: 0:13:58  lr: 0.000077  min_lr: 0.000001  loss: 4.6532 (4.5597)  class_acc: 0.0833 (0.1141)  loss_scale: 65536.0000 (56007.7163)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.0007  max mem: 15572
Epoch: [4]  [ 150/1404]  eta: 0:13:48  lr: 0.000077  min_lr: 0.000001  loss: 4.5584 (4.5579)  class_acc: 0.0833 (0.1134)  loss_scale: 65536.0000 (56638.7285)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0008  max mem: 15572
Epoch: [4]  [ 160/1404]  eta: 0:13:37  lr: 0.000077  min_lr: 0.000001  loss: 4.5173 (4.5521)  class_acc: 0.1250 (0.1152)  loss_scale: 65536.0000 (57191.3540)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.0010  max mem: 15572
[2025-01-17 08:56:41,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:56:41,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 08:56:41,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:56:41,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 170/1404]  eta: 0:13:25  lr: 0.000077  min_lr: 0.000001  loss: 4.4633 (4.5529)  class_acc: 0.1250 (0.1148)  loss_scale: 65536.0000 (58445.8480)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0008  max mem: 15572
[2025-01-17 08:56:42,880] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5788
[2025-01-17 08:56:42,881] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 08:56:42,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 08:56:42,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5788
[2025-01-17 08:56:42,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [4]  [ 180/1404]  eta: 0:13:09  lr: 0.000077  min_lr: 0.000001  loss: 4.4834 (4.5506)  class_acc: 0.1250 (0.1153)  loss_scale: 65536.0000 (59199.6464)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0005  max mem: 15572
Epoch: [4]  [ 190/1404]  eta: 0:13:02  lr: 0.000078  min_lr: 0.000001  loss: 4.5838 (4.5529)  class_acc: 0.1250 (0.1147)  loss_scale: 65536.0000 (59531.3927)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.0270  max mem: 15572
Epoch: [4]  [ 200/1404]  eta: 0:12:56  lr: 0.000078  min_lr: 0.000001  loss: 4.4596 (4.5478)  class_acc: 0.1250 (0.1159)  loss_scale: 65536.0000 (59830.1294)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0584  max mem: 15572
Epoch: [4]  [ 210/1404]  eta: 0:12:52  lr: 0.000078  min_lr: 0.000001  loss: 4.5299 (4.5550)  class_acc: 0.0833 (0.1143)  loss_scale: 65536.0000 (60100.5498)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.1380  max mem: 15572
Epoch: [4]  [ 220/1404]  eta: 0:12:44  lr: 0.000078  min_lr: 0.000001  loss: 4.6513 (4.5569)  class_acc: 0.0833 (0.1137)  loss_scale: 65536.0000 (60346.4977)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.1657  max mem: 15572
Epoch: [4]  [ 230/1404]  eta: 0:12:33  lr: 0.000078  min_lr: 0.000001  loss: 4.6655 (4.5617)  class_acc: 0.1250 (0.1136)  loss_scale: 65536.0000 (60571.1515)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0992  max mem: 15572
[2025-01-17 08:57:21,076] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5851
[2025-01-17 08:57:21,076] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:57:21,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:57:21,078] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5851
[2025-01-17 08:57:21,078] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [ 240/1404]  eta: 0:12:28  lr: 0.000078  min_lr: 0.000001  loss: 4.5470 (4.5585)  class_acc: 0.0833 (0.1139)  loss_scale: 65536.0000 (59961.3610)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1128  max mem: 15572
Epoch: [4]  [ 250/1404]  eta: 0:12:17  lr: 0.000078  min_lr: 0.000001  loss: 4.5799 (4.5653)  class_acc: 0.0833 (0.1134)  loss_scale: 32768.0000 (58877.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0955  max mem: 15572
Epoch: [4]  [ 260/1404]  eta: 0:12:08  lr: 0.000078  min_lr: 0.000001  loss: 4.6521 (4.5686)  class_acc: 0.0833 (0.1129)  loss_scale: 32768.0000 (57877.5785)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0451  max mem: 15572
Epoch: [4]  [ 270/1404]  eta: 0:12:01  lr: 0.000079  min_lr: 0.000001  loss: 4.6045 (4.5678)  class_acc: 0.1250 (0.1127)  loss_scale: 32768.0000 (56951.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0722  max mem: 15572
Epoch: [4]  [ 280/1404]  eta: 0:11:53  lr: 0.000079  min_lr: 0.000001  loss: 4.5143 (4.5605)  class_acc: 0.1250 (0.1139)  loss_scale: 32768.0000 (56090.4199)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.0692  max mem: 15572
Epoch: [4]  [ 290/1404]  eta: 0:11:45  lr: 0.000079  min_lr: 0.000001  loss: 4.4240 (4.5541)  class_acc: 0.1250 (0.1160)  loss_scale: 32768.0000 (55288.9622)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0320  max mem: 15572
Epoch: [4]  [ 300/1404]  eta: 0:11:42  lr: 0.000079  min_lr: 0.000001  loss: 4.4416 (4.5539)  class_acc: 0.1250 (0.1154)  loss_scale: 32768.0000 (54540.7575)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.0305  max mem: 15572
Epoch: [4]  [ 310/1404]  eta: 0:11:34  lr: 0.000079  min_lr: 0.000001  loss: 4.4451 (4.5515)  class_acc: 0.1250 (0.1156)  loss_scale: 32768.0000 (53840.6688)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0184  max mem: 15572
Epoch: [4]  [ 320/1404]  eta: 0:11:27  lr: 0.000079  min_lr: 0.000001  loss: 4.5346 (4.5535)  class_acc: 0.0833 (0.1147)  loss_scale: 32768.0000 (53184.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0008  max mem: 15572
Epoch: [4]  [ 330/1404]  eta: 0:11:17  lr: 0.000079  min_lr: 0.000001  loss: 4.5492 (4.5515)  class_acc: 0.0833 (0.1149)  loss_scale: 32768.0000 (52567.3958)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0009  max mem: 15572
Epoch: [4]  [ 340/1404]  eta: 0:11:11  lr: 0.000080  min_lr: 0.000001  loss: 4.4988 (4.5528)  class_acc: 0.0833 (0.1145)  loss_scale: 32768.0000 (51986.7683)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.0008  max mem: 15572
Epoch: [4]  [ 350/1404]  eta: 0:11:05  lr: 0.000080  min_lr: 0.000001  loss: 4.5301 (4.5525)  class_acc: 0.0833 (0.1140)  loss_scale: 32768.0000 (51439.2251)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.0008  max mem: 15572
Epoch: [4]  [ 360/1404]  eta: 0:10:59  lr: 0.000080  min_lr: 0.000001  loss: 4.4708 (4.5490)  class_acc: 0.1250 (0.1140)  loss_scale: 32768.0000 (50922.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0010  max mem: 15572
[2025-01-17 08:58:40,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:58:40,486] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 08:58:40,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 08:58:40,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 370/1404]  eta: 0:10:51  lr: 0.000080  min_lr: 0.000001  loss: 4.4708 (4.5486)  class_acc: 0.1250 (0.1141)  loss_scale: 32768.0000 (51050.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0009  max mem: 15572
Epoch: [4]  [ 380/1404]  eta: 0:10:45  lr: 0.000080  min_lr: 0.000001  loss: 4.5874 (4.5500)  class_acc: 0.0833 (0.1144)  loss_scale: 65536.0000 (51431.1391)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0007  max mem: 15572
[2025-01-17 08:58:51,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=29, lr=[7.763354467760621e-07, 7.763354467760621e-07, 1.1090506382515173e-06, 1.1090506382515173e-06, 1.584358054645025e-06, 1.584358054645025e-06, 2.263368649492893e-06, 2.263368649492893e-06, 3.2333837849898472e-06, 3.2333837849898472e-06, 4.619119692842639e-06, 4.619119692842639e-06, 6.5987424183466274e-06, 6.5987424183466274e-06, 9.426774883352326e-06, 9.426774883352326e-06, 1.3466821261931895e-05, 1.3466821261931895e-05, 1.923831608847414e-05, 1.923831608847414e-05, 2.7483308697820196e-05, 2.7483308697820196e-05, 3.926186956831457e-05, 3.926186956831457e-05, 5.6088385097592245e-05, 5.6088385097592245e-05, 8.012626442513179e-05, 8.012626442513179e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 08:58:51,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=43.72268806965829, CurrSamplesPerSec=51.06128827886078, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [ 390/1404]  eta: 0:10:37  lr: 0.000080  min_lr: 0.000001  loss: 4.4331 (4.5469)  class_acc: 0.1250 (0.1143)  loss_scale: 65536.0000 (51791.8772)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0006  max mem: 15572
Epoch: [4]  [ 400/1404]  eta: 0:10:33  lr: 0.000080  min_lr: 0.000001  loss: 4.4735 (4.5475)  class_acc: 0.1250 (0.1146)  loss_scale: 65536.0000 (52134.6234)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0007  max mem: 15572
Epoch: [4]  [ 410/1404]  eta: 0:10:25  lr: 0.000080  min_lr: 0.000001  loss: 4.5348 (4.5444)  class_acc: 0.1250 (0.1154)  loss_scale: 65536.0000 (52460.6910)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.0008  max mem: 15572
Epoch: [4]  [ 420/1404]  eta: 0:10:19  lr: 0.000081  min_lr: 0.000001  loss: 4.4280 (4.5411)  class_acc: 0.0833 (0.1160)  loss_scale: 65536.0000 (52771.2684)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0007  max mem: 15572
Epoch: [4]  [ 430/1404]  eta: 0:10:13  lr: 0.000081  min_lr: 0.000001  loss: 4.4783 (4.5410)  class_acc: 0.1250 (0.1163)  loss_scale: 65536.0000 (53067.4339)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.0006  max mem: 15572
Epoch: [4]  [ 440/1404]  eta: 0:10:05  lr: 0.000081  min_lr: 0.000001  loss: 4.5842 (4.5388)  class_acc: 0.1250 (0.1169)  loss_scale: 65536.0000 (53350.1678)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0006  max mem: 15572
Epoch: [4]  [ 450/1404]  eta: 0:09:58  lr: 0.000081  min_lr: 0.000001  loss: 4.5942 (4.5415)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.0007  max mem: 15572
Epoch: [4]  [ 460/1404]  eta: 0:09:51  lr: 0.000081  min_lr: 0.000001  loss: 4.5566 (4.5415)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (53878.8373)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0008  max mem: 15572
Epoch: [4]  [ 470/1404]  eta: 0:09:44  lr: 0.000081  min_lr: 0.000001  loss: 4.5321 (4.5423)  class_acc: 0.0833 (0.1165)  loss_scale: 65536.0000 (54126.3355)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0009  max mem: 15572
[2025-01-17 08:59:47,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6090
[2025-01-17 08:59:47,074] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 08:59:47,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 08:59:47,104] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6090
[2025-01-17 08:59:47,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [4]  [ 480/1404]  eta: 0:09:37  lr: 0.000081  min_lr: 0.000001  loss: 4.5445 (4.5419)  class_acc: 0.0417 (0.1156)  loss_scale: 65536.0000 (53886.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0008  max mem: 15572
Epoch: [4]  [ 490/1404]  eta: 0:09:32  lr: 0.000082  min_lr: 0.000001  loss: 4.4977 (4.5395)  class_acc: 0.0833 (0.1160)  loss_scale: 32768.0000 (53456.5540)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0008  max mem: 15572
Epoch: [4]  [ 500/1404]  eta: 0:09:26  lr: 0.000082  min_lr: 0.000001  loss: 4.4843 (4.5376)  class_acc: 0.1250 (0.1164)  loss_scale: 32768.0000 (53043.6088)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0009  max mem: 15572
Epoch: [4]  [ 510/1404]  eta: 0:09:19  lr: 0.000082  min_lr: 0.000001  loss: 4.4376 (4.5356)  class_acc: 0.1250 (0.1170)  loss_scale: 32768.0000 (52646.8258)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0009  max mem: 15572
Epoch: [4]  [ 520/1404]  eta: 0:09:12  lr: 0.000082  min_lr: 0.000001  loss: 4.4197 (4.5343)  class_acc: 0.1667 (0.1177)  loss_scale: 32768.0000 (52265.2745)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0010  max mem: 15572
Epoch: [4]  [ 530/1404]  eta: 0:09:04  lr: 0.000082  min_lr: 0.000001  loss: 4.5000 (4.5355)  class_acc: 0.1667 (0.1179)  loss_scale: 32768.0000 (51898.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0011  max mem: 15572
Epoch: [4]  [ 540/1404]  eta: 0:08:59  lr: 0.000082  min_lr: 0.000001  loss: 4.5905 (4.5381)  class_acc: 0.1250 (0.1174)  loss_scale: 32768.0000 (51544.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0009  max mem: 15572
Epoch: [4]  [ 550/1404]  eta: 0:08:53  lr: 0.000082  min_lr: 0.000001  loss: 4.5346 (4.5375)  class_acc: 0.1250 (0.1180)  loss_scale: 32768.0000 (51203.7169)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0009  max mem: 15572
Epoch: [4]  [ 560/1404]  eta: 0:08:47  lr: 0.000082  min_lr: 0.000001  loss: 4.6439 (4.5411)  class_acc: 0.0833 (0.1175)  loss_scale: 32768.0000 (50875.0945)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0009  max mem: 15572
Epoch: [4]  [ 570/1404]  eta: 0:08:42  lr: 0.000083  min_lr: 0.000001  loss: 4.6439 (4.5419)  class_acc: 0.0833 (0.1172)  loss_scale: 32768.0000 (50557.9825)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.0008  max mem: 15572
Epoch: [4]  [ 580/1404]  eta: 0:08:35  lr: 0.000083  min_lr: 0.000001  loss: 4.6223 (4.5449)  class_acc: 0.0833 (0.1168)  loss_scale: 32768.0000 (50251.7866)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.0008  max mem: 15572
Epoch: [4]  [ 590/1404]  eta: 0:08:28  lr: 0.000083  min_lr: 0.000001  loss: 4.6198 (4.5450)  class_acc: 0.0833 (0.1174)  loss_scale: 32768.0000 (49955.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0013  max mem: 15572
Epoch: [4]  [ 600/1404]  eta: 0:08:24  lr: 0.000083  min_lr: 0.000001  loss: 4.4941 (4.5416)  class_acc: 0.1250 (0.1174)  loss_scale: 32768.0000 (49669.9634)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.1570  max mem: 15572
[2025-01-17 09:01:08,920] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:01:08,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:01:08,921] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:01:08,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 610/1404]  eta: 0:08:17  lr: 0.000083  min_lr: 0.000001  loss: 4.4941 (4.5418)  class_acc: 0.0833 (0.1172)  loss_scale: 32768.0000 (49822.3764)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.1567  max mem: 15572
Epoch: [4]  [ 620/1404]  eta: 0:08:09  lr: 0.000083  min_lr: 0.000001  loss: 4.4481 (4.5391)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (50075.4138)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0009  max mem: 15572
Epoch: [4]  [ 630/1404]  eta: 0:08:02  lr: 0.000083  min_lr: 0.000001  loss: 4.4481 (4.5402)  class_acc: 0.1250 (0.1167)  loss_scale: 65536.0000 (50320.4311)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0009  max mem: 15572
[2025-01-17 09:01:25,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6248
[2025-01-17 09:01:25,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6248
[2025-01-17 09:01:25,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:01:25,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:01:25,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 640/1404]  eta: 0:07:55  lr: 0.000084  min_lr: 0.000001  loss: 4.5448 (4.5411)  class_acc: 0.1250 (0.1165)  loss_scale: 65536.0000 (50097.7223)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0213  max mem: 15572
Epoch: [4]  [ 650/1404]  eta: 0:07:50  lr: 0.000084  min_lr: 0.000001  loss: 4.5528 (4.5420)  class_acc: 0.1250 (0.1167)  loss_scale: 32768.0000 (49831.5207)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0957  max mem: 15572
Epoch: [4]  [ 660/1404]  eta: 0:07:42  lr: 0.000084  min_lr: 0.000001  loss: 4.5094 (4.5407)  class_acc: 0.1250 (0.1170)  loss_scale: 32768.0000 (49573.3737)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0882  max mem: 15572
Epoch: [4]  [ 670/1404]  eta: 0:07:37  lr: 0.000084  min_lr: 0.000001  loss: 4.4355 (4.5400)  class_acc: 0.1250 (0.1173)  loss_scale: 32768.0000 (49322.9210)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0799  max mem: 15572
Epoch: [4]  [ 680/1404]  eta: 0:07:31  lr: 0.000084  min_lr: 0.000001  loss: 4.4429 (4.5393)  class_acc: 0.1250 (0.1174)  loss_scale: 32768.0000 (49079.8238)  weight_decay: 0.0500 (0.0500)  time: 0.6693  data: 0.1254  max mem: 15572
Epoch: [4]  [ 690/1404]  eta: 0:07:25  lr: 0.000084  min_lr: 0.000001  loss: 4.5694 (4.5418)  class_acc: 0.0833 (0.1174)  loss_scale: 32768.0000 (48843.7627)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1284  max mem: 15572
Epoch: [4]  [ 700/1404]  eta: 0:07:18  lr: 0.000084  min_lr: 0.000001  loss: 4.5694 (4.5413)  class_acc: 0.1250 (0.1178)  loss_scale: 32768.0000 (48614.4365)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0787  max mem: 15572
Epoch: [4]  [ 710/1404]  eta: 0:07:12  lr: 0.000084  min_lr: 0.000001  loss: 4.5633 (4.5433)  class_acc: 0.1250 (0.1174)  loss_scale: 32768.0000 (48391.5612)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0778  max mem: 15572
Epoch: [4]  [ 720/1404]  eta: 0:07:06  lr: 0.000085  min_lr: 0.000001  loss: 4.5177 (4.5434)  class_acc: 0.0833 (0.1174)  loss_scale: 32768.0000 (48174.8682)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.1134  max mem: 15572
Epoch: [4]  [ 730/1404]  eta: 0:06:58  lr: 0.000085  min_lr: 0.000001  loss: 4.5529 (4.5449)  class_acc: 0.0833 (0.1167)  loss_scale: 32768.0000 (47964.1040)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.0678  max mem: 15572
Epoch: [4]  [ 740/1404]  eta: 0:06:52  lr: 0.000085  min_lr: 0.000001  loss: 4.5644 (4.5435)  class_acc: 0.0417 (0.1167)  loss_scale: 32768.0000 (47759.0283)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0698  max mem: 15572
Epoch: [4]  [ 750/1404]  eta: 0:06:46  lr: 0.000085  min_lr: 0.000001  loss: 4.4825 (4.5417)  class_acc: 0.0833 (0.1173)  loss_scale: 32768.0000 (47559.4141)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0833  max mem: 15572
Epoch: [4]  [ 760/1404]  eta: 0:06:39  lr: 0.000085  min_lr: 0.000001  loss: 4.4667 (4.5419)  class_acc: 0.0833 (0.1169)  loss_scale: 32768.0000 (47365.0460)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0850  max mem: 15572
[2025-01-17 09:02:44,142] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:02:44,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:02:44,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:02:44,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [ 770/1404]  eta: 0:06:34  lr: 0.000085  min_lr: 0.000001  loss: 4.4667 (4.5416)  class_acc: 0.0833 (0.1171)  loss_scale: 32768.0000 (47600.7263)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.0841  max mem: 15572
Epoch: [4]  [ 780/1404]  eta: 0:06:28  lr: 0.000085  min_lr: 0.000001  loss: 4.6190 (4.5424)  class_acc: 0.0833 (0.1168)  loss_scale: 65536.0000 (47830.3713)  weight_decay: 0.0500 (0.0500)  time: 0.6996  data: 0.0453  max mem: 15572
Epoch: [4]  [ 790/1404]  eta: 0:06:22  lr: 0.000086  min_lr: 0.000001  loss: 4.6190 (4.5426)  class_acc: 0.0833 (0.1167)  loss_scale: 65536.0000 (48054.2099)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.0099  max mem: 15572
Epoch: [4]  [ 800/1404]  eta: 0:06:15  lr: 0.000086  min_lr: 0.000001  loss: 4.6280 (4.5438)  class_acc: 0.0833 (0.1168)  loss_scale: 65536.0000 (48272.4594)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0005  max mem: 15572
Epoch: [4]  [ 810/1404]  eta: 0:06:08  lr: 0.000086  min_lr: 0.000001  loss: 4.6280 (4.5431)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (48485.3268)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0006  max mem: 15572
Epoch: [4]  [ 820/1404]  eta: 0:06:02  lr: 0.000086  min_lr: 0.000001  loss: 4.5353 (4.5434)  class_acc: 0.0833 (0.1168)  loss_scale: 65536.0000 (48693.0085)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0008  max mem: 15572
Epoch: [4]  [ 830/1404]  eta: 0:05:55  lr: 0.000086  min_lr: 0.000001  loss: 4.5153 (4.5428)  class_acc: 0.0833 (0.1168)  loss_scale: 65536.0000 (48895.6919)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0008  max mem: 15572
Epoch: [4]  [ 840/1404]  eta: 0:05:49  lr: 0.000086  min_lr: 0.000001  loss: 4.4387 (4.5419)  class_acc: 0.1250 (0.1174)  loss_scale: 65536.0000 (49093.5553)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0008  max mem: 15572
Epoch: [4]  [ 850/1404]  eta: 0:05:43  lr: 0.000086  min_lr: 0.000001  loss: 4.4387 (4.5406)  class_acc: 0.0833 (0.1170)  loss_scale: 65536.0000 (49286.7685)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0300  max mem: 15572
Epoch: [4]  [ 860/1404]  eta: 0:05:37  lr: 0.000086  min_lr: 0.000001  loss: 4.2927 (4.5383)  class_acc: 0.0833 (0.1174)  loss_scale: 65536.0000 (49475.4936)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0343  max mem: 15572
Epoch: [4]  [ 870/1404]  eta: 0:05:31  lr: 0.000087  min_lr: 0.000001  loss: 4.4083 (4.5389)  class_acc: 0.1250 (0.1173)  loss_scale: 65536.0000 (49659.8852)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.0051  max mem: 15572
Epoch: [4]  [ 880/1404]  eta: 0:05:24  lr: 0.000087  min_lr: 0.000001  loss: 4.4419 (4.5382)  class_acc: 0.0833 (0.1175)  loss_scale: 65536.0000 (49840.0908)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
[2025-01-17 09:03:58,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6500
[2025-01-17 09:03:58,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:03:58,352] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6500
[2025-01-17 09:03:58,352] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:03:58,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [ 890/1404]  eta: 0:05:18  lr: 0.000087  min_lr: 0.000001  loss: 4.4419 (4.5378)  class_acc: 0.1250 (0.1180)  loss_scale: 65536.0000 (49758.8148)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.0012  max mem: 15572
Epoch: [4]  [ 900/1404]  eta: 0:05:11  lr: 0.000087  min_lr: 0.000001  loss: 4.5821 (4.5380)  class_acc: 0.1250 (0.1180)  loss_scale: 32768.0000 (49570.2375)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0015  max mem: 15572
Epoch: [4]  [ 910/1404]  eta: 0:05:05  lr: 0.000087  min_lr: 0.000001  loss: 4.5345 (4.5368)  class_acc: 0.0833 (0.1181)  loss_scale: 32768.0000 (49385.8002)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0010  max mem: 15572
Epoch: [4]  [ 920/1404]  eta: 0:04:59  lr: 0.000087  min_lr: 0.000001  loss: 4.4247 (4.5366)  class_acc: 0.0833 (0.1181)  loss_scale: 32768.0000 (49205.3681)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0008  max mem: 15572
Epoch: [4]  [ 930/1404]  eta: 0:04:53  lr: 0.000087  min_lr: 0.000001  loss: 4.5232 (4.5360)  class_acc: 0.1250 (0.1182)  loss_scale: 32768.0000 (49028.8120)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0008  max mem: 15572
Epoch: [4]  [ 940/1404]  eta: 0:04:46  lr: 0.000088  min_lr: 0.000001  loss: 4.4925 (4.5355)  class_acc: 0.1250 (0.1186)  loss_scale: 32768.0000 (48856.0085)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.0006  max mem: 15572
Epoch: [4]  [ 950/1404]  eta: 0:04:40  lr: 0.000088  min_lr: 0.000001  loss: 4.4925 (4.5361)  class_acc: 0.1250 (0.1186)  loss_scale: 32768.0000 (48686.8391)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0006  max mem: 15572
Epoch: [4]  [ 960/1404]  eta: 0:04:34  lr: 0.000088  min_lr: 0.000001  loss: 4.5395 (4.5354)  class_acc: 0.1250 (0.1188)  loss_scale: 32768.0000 (48521.1904)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.0006  max mem: 15572
Epoch: [4]  [ 970/1404]  eta: 0:04:28  lr: 0.000088  min_lr: 0.000001  loss: 4.4789 (4.5348)  class_acc: 0.1250 (0.1189)  loss_scale: 32768.0000 (48358.9537)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.0006  max mem: 15572
Epoch: [4]  [ 980/1404]  eta: 0:04:22  lr: 0.000088  min_lr: 0.000001  loss: 4.4789 (4.5340)  class_acc: 0.1667 (0.1196)  loss_scale: 32768.0000 (48200.0245)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0006  max mem: 15572
Epoch: [4]  [ 990/1404]  eta: 0:04:16  lr: 0.000088  min_lr: 0.000001  loss: 4.4506 (4.5331)  class_acc: 0.1667 (0.1195)  loss_scale: 32768.0000 (48044.3027)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0006  max mem: 15572
Epoch: [4]  [1000/1404]  eta: 0:04:10  lr: 0.000088  min_lr: 0.000001  loss: 4.5217 (4.5332)  class_acc: 0.0833 (0.1194)  loss_scale: 32768.0000 (47891.6923)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0006  max mem: 15572
Epoch: [4]  [1010/1404]  eta: 0:04:04  lr: 0.000089  min_lr: 0.000001  loss: 4.5353 (4.5336)  class_acc: 0.0833 (0.1194)  loss_scale: 32768.0000 (47742.1009)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0005  max mem: 15572
[2025-01-17 09:05:18,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:05:18,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:05:18,633] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:05:18,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1020/1404]  eta: 0:03:58  lr: 0.000089  min_lr: 0.000001  loss: 4.5323 (4.5323)  class_acc: 0.1250 (0.1197)  loss_scale: 32768.0000 (47852.1920)  weight_decay: 0.0500 (0.0500)  time: 0.6761  data: 0.0005  max mem: 15572
Epoch: [4]  [1030/1404]  eta: 0:03:51  lr: 0.000089  min_lr: 0.000001  loss: 4.5049 (4.5325)  class_acc: 0.1250 (0.1198)  loss_scale: 65536.0000 (48023.7129)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0006  max mem: 15572
Epoch: [4]  [1040/1404]  eta: 0:03:45  lr: 0.000089  min_lr: 0.000001  loss: 4.5125 (4.5331)  class_acc: 0.0833 (0.1196)  loss_scale: 65536.0000 (48191.9385)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0007  max mem: 15572
Epoch: [4]  [1050/1404]  eta: 0:03:39  lr: 0.000089  min_lr: 0.000001  loss: 4.4433 (4.5315)  class_acc: 0.0833 (0.1201)  loss_scale: 65536.0000 (48356.9629)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0007  max mem: 15572
Epoch: [4]  [1060/1404]  eta: 0:03:32  lr: 0.000089  min_lr: 0.000001  loss: 4.4415 (4.5302)  class_acc: 0.1667 (0.1209)  loss_scale: 65536.0000 (48518.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0007  max mem: 15572
Epoch: [4]  [1070/1404]  eta: 0:03:26  lr: 0.000089  min_lr: 0.000001  loss: 4.4967 (4.5294)  class_acc: 0.1667 (0.1212)  loss_scale: 65536.0000 (48677.7666)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0007  max mem: 15572
Epoch: [4]  [1080/1404]  eta: 0:03:20  lr: 0.000089  min_lr: 0.000001  loss: 4.5424 (4.5297)  class_acc: 0.1667 (0.1214)  loss_scale: 65536.0000 (48833.7169)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0007  max mem: 15572
Epoch: [4]  [1090/1404]  eta: 0:03:14  lr: 0.000090  min_lr: 0.000001  loss: 4.5918 (4.5305)  class_acc: 0.0833 (0.1210)  loss_scale: 65536.0000 (48986.8084)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.0009  max mem: 15572
Epoch: [4]  [1100/1404]  eta: 0:03:08  lr: 0.000090  min_lr: 0.000001  loss: 4.5634 (4.5309)  class_acc: 0.0833 (0.1210)  loss_scale: 65536.0000 (49137.1190)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0011  max mem: 15572
[2025-01-17 09:06:15,823] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6723
[2025-01-17 09:06:15,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:06:15,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6723
[2025-01-17 09:06:15,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:06:15,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1110/1404]  eta: 0:03:01  lr: 0.000090  min_lr: 0.000001  loss: 4.5046 (4.5316)  class_acc: 0.0833 (0.1209)  loss_scale: 65536.0000 (49166.7471)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.0010  max mem: 15572
Epoch: [4]  [1120/1404]  eta: 0:02:55  lr: 0.000090  min_lr: 0.000001  loss: 4.4937 (4.5311)  class_acc: 0.0833 (0.1209)  loss_scale: 32768.0000 (49020.4603)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0009  max mem: 15572
Epoch: [4]  [1130/1404]  eta: 0:02:49  lr: 0.000090  min_lr: 0.000001  loss: 4.4696 (4.5306)  class_acc: 0.1250 (0.1213)  loss_scale: 32768.0000 (48876.7604)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.0009  max mem: 15572
Epoch: [4]  [1140/1404]  eta: 0:02:43  lr: 0.000090  min_lr: 0.000001  loss: 4.4156 (4.5306)  class_acc: 0.1250 (0.1213)  loss_scale: 32768.0000 (48735.5793)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0008  max mem: 15572
Epoch: [4]  [1150/1404]  eta: 0:02:36  lr: 0.000090  min_lr: 0.000001  loss: 4.4156 (4.5301)  class_acc: 0.1250 (0.1216)  loss_scale: 32768.0000 (48596.8514)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0185  max mem: 15572
Epoch: [4]  [1160/1404]  eta: 0:02:30  lr: 0.000091  min_lr: 0.000001  loss: 4.3266 (4.5276)  class_acc: 0.1667 (0.1224)  loss_scale: 32768.0000 (48460.5134)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0190  max mem: 15572
Epoch: [4]  [1170/1404]  eta: 0:02:24  lr: 0.000091  min_lr: 0.000001  loss: 4.3539 (4.5273)  class_acc: 0.1667 (0.1225)  loss_scale: 32768.0000 (48326.5038)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0170  max mem: 15572
Epoch: [4]  [1180/1404]  eta: 0:02:18  lr: 0.000091  min_lr: 0.000001  loss: 4.6425 (4.5291)  class_acc: 0.0417 (0.1223)  loss_scale: 32768.0000 (48194.7638)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.1104  max mem: 15572
Epoch: [4]  [1190/1404]  eta: 0:02:12  lr: 0.000091  min_lr: 0.000001  loss: 4.6041 (4.5280)  class_acc: 0.1250 (0.1223)  loss_scale: 32768.0000 (48065.2359)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.1469  max mem: 15572
Epoch: [4]  [1200/1404]  eta: 0:02:06  lr: 0.000091  min_lr: 0.000001  loss: 4.4243 (4.5281)  class_acc: 0.0833 (0.1222)  loss_scale: 32768.0000 (47937.8651)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1167  max mem: 15572
Epoch: [4]  [1210/1404]  eta: 0:02:00  lr: 0.000091  min_lr: 0.000001  loss: 4.5272 (4.5278)  class_acc: 0.0833 (0.1223)  loss_scale: 32768.0000 (47812.5979)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1078  max mem: 15572
Epoch: [4]  [1220/1404]  eta: 0:01:53  lr: 0.000091  min_lr: 0.000001  loss: 4.5272 (4.5274)  class_acc: 0.1250 (0.1224)  loss_scale: 32768.0000 (47689.3825)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1026  max mem: 15572
Epoch: [4]  [1230/1404]  eta: 0:01:47  lr: 0.000091  min_lr: 0.000001  loss: 4.4808 (4.5264)  class_acc: 0.1667 (0.1226)  loss_scale: 32768.0000 (47568.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0962  max mem: 15572
[2025-01-17 09:07:36,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:07:36,718] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:07:36,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:07:36,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1240/1404]  eta: 0:01:41  lr: 0.000092  min_lr: 0.000001  loss: 4.4439 (4.5247)  class_acc: 0.1667 (0.1233)  loss_scale: 32768.0000 (47580.9315)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1130  max mem: 15572
Epoch: [4]  [1250/1404]  eta: 0:01:35  lr: 0.000092  min_lr: 0.000001  loss: 4.5288 (4.5249)  class_acc: 0.0833 (0.1231)  loss_scale: 65536.0000 (47724.4572)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1453  max mem: 15572
Epoch: [4]  [1260/1404]  eta: 0:01:29  lr: 0.000092  min_lr: 0.000001  loss: 4.5288 (4.5247)  class_acc: 0.0833 (0.1231)  loss_scale: 65536.0000 (47865.7066)  weight_decay: 0.0500 (0.0500)  time: 0.8644  data: 0.3120  max mem: 15572
Epoch: [4]  [1270/1404]  eta: 0:01:23  lr: 0.000092  min_lr: 0.000001  loss: 4.4849 (4.5247)  class_acc: 0.1250 (0.1233)  loss_scale: 65536.0000 (48004.7333)  weight_decay: 0.0500 (0.0500)  time: 0.8127  data: 0.2431  max mem: 15572
Epoch: [4]  [1280/1404]  eta: 0:01:17  lr: 0.000092  min_lr: 0.000001  loss: 4.5112 (4.5245)  class_acc: 0.1667 (0.1238)  loss_scale: 65536.0000 (48141.5894)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0013  max mem: 15572
Epoch: [4]  [1290/1404]  eta: 0:01:10  lr: 0.000092  min_lr: 0.000001  loss: 4.3611 (4.5231)  class_acc: 0.1250 (0.1240)  loss_scale: 65536.0000 (48276.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0563  max mem: 15572
Epoch: [4]  [1300/1404]  eta: 0:01:04  lr: 0.000092  min_lr: 0.000001  loss: 4.2473 (4.5223)  class_acc: 0.1250 (0.1243)  loss_scale: 65536.0000 (48408.9900)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1129  max mem: 15572
Epoch: [4]  [1310/1404]  eta: 0:00:58  lr: 0.000093  min_lr: 0.000001  loss: 4.5637 (4.5220)  class_acc: 0.1250 (0.1243)  loss_scale: 65536.0000 (48539.6308)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0744  max mem: 15572
Epoch: [4]  [1320/1404]  eta: 0:00:52  lr: 0.000093  min_lr: 0.000001  loss: 4.5217 (4.5217)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (48668.2937)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0229  max mem: 15572
Epoch: [4]  [1330/1404]  eta: 0:00:45  lr: 0.000093  min_lr: 0.000001  loss: 4.4323 (4.5210)  class_acc: 0.1667 (0.1249)  loss_scale: 65536.0000 (48795.0233)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0059  max mem: 15572
Epoch: [4]  [1340/1404]  eta: 0:00:39  lr: 0.000093  min_lr: 0.000001  loss: 4.4249 (4.5208)  class_acc: 0.1250 (0.1248)  loss_scale: 65536.0000 (48919.8628)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0322  max mem: 15572
Epoch: [4]  [1350/1404]  eta: 0:00:33  lr: 0.000093  min_lr: 0.000001  loss: 4.4239 (4.5197)  class_acc: 0.1250 (0.1251)  loss_scale: 65536.0000 (49042.8542)  weight_decay: 0.0500 (0.0500)  time: 0.7026  data: 0.0324  max mem: 15572
Epoch: [4]  [1360/1404]  eta: 0:00:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4355 (4.5192)  class_acc: 0.1667 (0.1254)  loss_scale: 65536.0000 (49164.0382)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0009  max mem: 15572
[2025-01-17 09:08:59,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:08:59,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:08:59,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:08:59,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:09:01,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6983
[2025-01-17 09:09:01,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6983
[2025-01-17 09:09:01,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:09:01,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:09:01,168] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1370/1404]  eta: 0:00:21  lr: 0.000093  min_lr: 0.000001  loss: 4.4010 (4.5177)  class_acc: 0.1250 (0.1256)  loss_scale: 65536.0000 (49426.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7088  data: 0.0007  max mem: 15572
Epoch: [4]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.4010 (4.5179)  class_acc: 0.1667 (0.1258)  loss_scale: 65536.0000 (49543.5076)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.0007  max mem: 15572
[2025-01-17 09:09:12,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=34, lr=[9.057462563736721e-07, 9.057462563736721e-07, 1.2939232233909602e-06, 1.2939232233909602e-06, 1.848461747701372e-06, 1.848461747701372e-06, 2.6406596395733886e-06, 2.6406596395733886e-06, 3.7723709136762698e-06, 3.7723709136762698e-06, 5.389101305251814e-06, 5.389101305251814e-06, 7.698716150359736e-06, 7.698716150359736e-06, 1.0998165929085337e-05, 1.0998165929085337e-05, 1.5711665612979053e-05, 1.5711665612979053e-05, 2.2445236589970078e-05, 2.2445236589970078e-05, 3.206462369995725e-05, 3.206462369995725e-05, 4.5806605285653225e-05, 4.5806605285653225e-05, 6.543800755093318e-05, 6.543800755093318e-05, 9.348286792990455e-05, 9.348286792990455e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 09:09:12,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=43.66457669406811, CurrSamplesPerSec=47.66593081895021, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.3785 (4.5163)  class_acc: 0.1667 (0.1263)  loss_scale: 65536.0000 (49658.4788)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0008  max mem: 15572
Epoch: [4]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2978 (4.5161)  class_acc: 0.1667 (0.1265)  loss_scale: 65536.0000 (49771.8087)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0006  max mem: 15572
Epoch: [4]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.3256 (4.5160)  class_acc: 0.1250 (0.1265)  loss_scale: 65536.0000 (49805.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0005  max mem: 15572
Epoch: [4] Total time: 0:14:32 (0.6217 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.3256 (4.5152)  class_acc: 0.1250 (0.1261)  loss_scale: 65536.0000 (49805.4929)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:10:09  loss: 2.3162 (2.3162)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.4810  data: 4.2580  max mem: 15572
Val:  [ 10/136]  eta: 0:01:52  loss: 3.8810 (3.7655)  acc1: 5.5556 (16.1616)  acc5: 27.7778 (32.8283)  time: 0.8904  data: 0.6625  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 3.8810 (3.8259)  acc1: 0.0000 (13.7566)  acc5: 27.7778 (34.9206)  time: 0.4006  data: 0.1809  max mem: 15572
Val:  [ 30/136]  eta: 0:00:55  loss: 3.4199 (3.5725)  acc1: 11.1111 (23.1183)  acc5: 50.0000 (43.3692)  time: 0.3197  data: 0.1144  max mem: 15572
Val:  [ 40/136]  eta: 0:00:46  loss: 2.7151 (3.5299)  acc1: 27.7778 (23.3062)  acc5: 66.6667 (44.5799)  time: 0.3764  data: 0.1793  max mem: 15572
Val:  [ 50/136]  eta: 0:00:40  loss: 4.0767 (3.6839)  acc1: 0.0000 (19.6078)  acc5: 16.6667 (39.3246)  time: 0.3919  data: 0.2014  max mem: 15572
Val:  [ 60/136]  eta: 0:00:34  loss: 4.2658 (3.7873)  acc1: 0.0000 (16.8488)  acc5: 11.1111 (36.2477)  time: 0.3884  data: 0.1904  max mem: 15572
Val:  [ 70/136]  eta: 0:00:29  loss: 4.1420 (3.7026)  acc1: 0.0000 (19.4053)  acc5: 22.2222 (38.4194)  time: 0.3961  data: 0.1859  max mem: 15572
Val:  [ 80/136]  eta: 0:00:24  loss: 3.6111 (3.6905)  acc1: 11.1111 (19.3416)  acc5: 50.0000 (39.5748)  time: 0.3910  data: 0.1881  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 3.6445 (3.7006)  acc1: 5.5556 (19.2308)  acc5: 44.4444 (39.6825)  time: 0.3550  data: 0.1653  max mem: 15572
Val:  [100/136]  eta: 0:00:15  loss: 3.7819 (3.7375)  acc1: 0.0000 (18.3168)  acc5: 44.4444 (39.3839)  time: 0.3846  data: 0.1855  max mem: 15572
Val:  [110/136]  eta: 0:00:11  loss: 3.7637 (3.7382)  acc1: 0.0000 (19.2192)  acc5: 38.8889 (40.2402)  time: 0.3989  data: 0.1991  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.4480 (3.6877)  acc1: 27.7778 (20.4775)  acc5: 61.1111 (43.3884)  time: 0.4117  data: 0.2139  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 3.0967 (3.6535)  acc1: 33.3333 (22.2222)  acc5: 66.6667 (44.4869)  time: 0.3368  data: 0.1454  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.4480 (3.6621)  acc1: 27.7778 (22.0311)  acc5: 61.1111 (44.7174)  time: 0.2449  data: 0.0784  max mem: 15572
Val: Total time: 0:00:54 (0.4014 s / it)
* Acc@1 21.581 Acc@5 44.431 loss 3.682
Accuracy of the network on the 4883 val videos: 21.6%
[2025-01-17 09:10:17,907] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 09:10:17,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 09:10:17,909] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 09:10:17,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 09:10:19,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 09:10:19,718] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.58%
Epoch: [5]  [   0/1404]  eta: 3:38:03  lr: 0.000094  min_lr: 0.000001  loss: 4.5972 (4.5972)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.3188  data: 8.7861  max mem: 15572
[2025-01-17 09:10:33,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7028
[2025-01-17 09:10:33,311] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:10:33,385] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7028
[2025-01-17 09:10:33,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:10:33,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [  10/1404]  eta: 0:32:51  lr: 0.000094  min_lr: 0.000001  loss: 4.4643 (4.4663)  class_acc: 0.1250 (0.1515)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 1.4140  data: 0.8928  max mem: 15572
Epoch: [5]  [  20/1404]  eta: 0:24:02  lr: 0.000094  min_lr: 0.000001  loss: 4.4152 (4.4192)  class_acc: 0.1667 (0.1766)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1239  max mem: 15572
Epoch: [5]  [  30/1404]  eta: 0:20:31  lr: 0.000094  min_lr: 0.000001  loss: 4.4282 (4.3878)  class_acc: 0.1667 (0.1774)  loss_scale: 32768.0000 (41224.2581)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1364  max mem: 15572
Epoch: [5]  [  40/1404]  eta: 0:18:47  lr: 0.000094  min_lr: 0.000001  loss: 4.4745 (4.4228)  class_acc: 0.1250 (0.1596)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1215  max mem: 15572
Epoch: [5]  [  50/1404]  eta: 0:18:16  lr: 0.000094  min_lr: 0.000001  loss: 4.4745 (4.4326)  class_acc: 0.0833 (0.1536)  loss_scale: 32768.0000 (37908.0784)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.1798  max mem: 15572
Epoch: [5]  [  60/1404]  eta: 0:17:24  lr: 0.000094  min_lr: 0.000001  loss: 4.4662 (4.4322)  class_acc: 0.1250 (0.1530)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.1796  max mem: 15572
Epoch: [5]  [  70/1404]  eta: 0:16:36  lr: 0.000094  min_lr: 0.000001  loss: 4.4654 (4.4474)  class_acc: 0.1250 (0.1491)  loss_scale: 32768.0000 (36460.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0796  max mem: 15572
Epoch: [5]  [  80/1404]  eta: 0:16:20  lr: 0.000094  min_lr: 0.000001  loss: 4.4844 (4.4507)  class_acc: 0.1250 (0.1466)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1204  max mem: 15572
Epoch: [5]  [  90/1404]  eta: 0:15:53  lr: 0.000094  min_lr: 0.000001  loss: 4.3794 (4.4453)  class_acc: 0.1250 (0.1419)  loss_scale: 32768.0000 (35648.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.1431  max mem: 15572
Epoch: [5]  [ 100/1404]  eta: 0:15:42  lr: 0.000094  min_lr: 0.000001  loss: 4.3869 (4.4560)  class_acc: 0.0833 (0.1399)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1445  max mem: 15572
Epoch: [5]  [ 110/1404]  eta: 0:15:16  lr: 0.000094  min_lr: 0.000001  loss: 4.4583 (4.4504)  class_acc: 0.1667 (0.1426)  loss_scale: 32768.0000 (35129.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1320  max mem: 15572
Epoch: [5]  [ 120/1404]  eta: 0:15:08  lr: 0.000094  min_lr: 0.000001  loss: 4.5440 (4.4578)  class_acc: 0.1667 (0.1408)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1136  max mem: 15572
Epoch: [5]  [ 130/1404]  eta: 0:15:00  lr: 0.000094  min_lr: 0.000001  loss: 4.5441 (4.4579)  class_acc: 0.1250 (0.1399)  loss_scale: 32768.0000 (34769.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1692  max mem: 15572
[2025-01-17 09:11:57,122] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:11:57,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:11:57,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:11:57,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 140/1404]  eta: 0:14:46  lr: 0.000094  min_lr: 0.000001  loss: 4.5458 (4.4697)  class_acc: 0.0833 (0.1359)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  time: 0.6677  data: 0.1380  max mem: 15572
Epoch: [5]  [ 150/1404]  eta: 0:14:37  lr: 0.000094  min_lr: 0.000001  loss: 4.6164 (4.4810)  class_acc: 0.0833 (0.1349)  loss_scale: 65536.0000 (37542.1457)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.1249  max mem: 15572
Epoch: [5]  [ 160/1404]  eta: 0:14:23  lr: 0.000094  min_lr: 0.000001  loss: 4.4570 (4.4626)  class_acc: 0.1250 (0.1392)  loss_scale: 65536.0000 (39280.8944)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.1201  max mem: 15572
Epoch: [5]  [ 170/1404]  eta: 0:14:14  lr: 0.000094  min_lr: 0.000001  loss: 4.4538 (4.4634)  class_acc: 0.2083 (0.1421)  loss_scale: 65536.0000 (40816.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0973  max mem: 15572
Epoch: [5]  [ 180/1404]  eta: 0:14:04  lr: 0.000094  min_lr: 0.000001  loss: 4.5120 (4.4675)  class_acc: 0.1250 (0.1411)  loss_scale: 65536.0000 (42182.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.1154  max mem: 15572
Epoch: [5]  [ 190/1404]  eta: 0:13:55  lr: 0.000094  min_lr: 0.000001  loss: 4.4107 (4.4598)  class_acc: 0.1250 (0.1407)  loss_scale: 65536.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1175  max mem: 15572
Epoch: [5]  [ 200/1404]  eta: 0:13:40  lr: 0.000094  min_lr: 0.000001  loss: 4.3543 (4.4556)  class_acc: 0.1250 (0.1424)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0552  max mem: 15572
Epoch: [5]  [ 210/1404]  eta: 0:13:31  lr: 0.000094  min_lr: 0.000001  loss: 4.3722 (4.4573)  class_acc: 0.1250 (0.1426)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0599  max mem: 15572
Epoch: [5]  [ 220/1404]  eta: 0:13:22  lr: 0.000094  min_lr: 0.000001  loss: 4.3963 (4.4552)  class_acc: 0.1250 (0.1422)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0790  max mem: 15572
Epoch: [5]  [ 230/1404]  eta: 0:13:13  lr: 0.000094  min_lr: 0.000001  loss: 4.4641 (4.4574)  class_acc: 0.1250 (0.1411)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0771  max mem: 15572
Epoch: [5]  [ 240/1404]  eta: 0:13:02  lr: 0.000094  min_lr: 0.000001  loss: 4.3793 (4.4572)  class_acc: 0.1250 (0.1416)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0821  max mem: 15572
[2025-01-17 09:13:02,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7261
[2025-01-17 09:13:02,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:13:02,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7261
[2025-01-17 09:13:02,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:13:02,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 250/1404]  eta: 0:12:56  lr: 0.000094  min_lr: 0.000001  loss: 4.3058 (4.4545)  class_acc: 0.1250 (0.1416)  loss_scale: 32768.0000 (47389.5777)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1009  max mem: 15572
Epoch: [5]  [ 260/1404]  eta: 0:12:43  lr: 0.000094  min_lr: 0.000001  loss: 4.3008 (4.4507)  class_acc: 0.1250 (0.1418)  loss_scale: 32768.0000 (46829.3640)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0764  max mem: 15572
Epoch: [5]  [ 270/1404]  eta: 0:12:37  lr: 0.000094  min_lr: 0.000001  loss: 4.3086 (4.4472)  class_acc: 0.1250 (0.1419)  loss_scale: 32768.0000 (46310.4945)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0522  max mem: 15572
Epoch: [5]  [ 280/1404]  eta: 0:12:24  lr: 0.000094  min_lr: 0.000001  loss: 4.4366 (4.4503)  class_acc: 0.0833 (0.1404)  loss_scale: 32768.0000 (45828.5552)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0524  max mem: 15572
Epoch: [5]  [ 290/1404]  eta: 0:12:14  lr: 0.000094  min_lr: 0.000001  loss: 4.4552 (4.4500)  class_acc: 0.1250 (0.1410)  loss_scale: 32768.0000 (45379.7388)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0107  max mem: 15572
Epoch: [5]  [ 300/1404]  eta: 0:12:08  lr: 0.000094  min_lr: 0.000001  loss: 4.3597 (4.4468)  class_acc: 0.1667 (0.1415)  loss_scale: 32768.0000 (44960.7442)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0910  max mem: 15572
Epoch: [5]  [ 310/1404]  eta: 0:12:02  lr: 0.000094  min_lr: 0.000001  loss: 4.3597 (4.4450)  class_acc: 0.1250 (0.1404)  loss_scale: 32768.0000 (44568.6945)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.1536  max mem: 15572
Epoch: [5]  [ 320/1404]  eta: 0:11:55  lr: 0.000094  min_lr: 0.000001  loss: 4.3297 (4.4416)  class_acc: 0.1250 (0.1410)  loss_scale: 32768.0000 (44201.0717)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.1280  max mem: 15572
Epoch: [5]  [ 330/1404]  eta: 0:11:47  lr: 0.000094  min_lr: 0.000001  loss: 4.4614 (4.4450)  class_acc: 0.0833 (0.1406)  loss_scale: 32768.0000 (43855.6616)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1100  max mem: 15572
Epoch: [5]  [ 340/1404]  eta: 0:11:40  lr: 0.000094  min_lr: 0.000001  loss: 4.5255 (4.4428)  class_acc: 0.1250 (0.1406)  loss_scale: 32768.0000 (43530.5103)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1260  max mem: 15572
Epoch: [5]  [ 350/1404]  eta: 0:11:36  lr: 0.000094  min_lr: 0.000001  loss: 4.5255 (4.4447)  class_acc: 0.1250 (0.1410)  loss_scale: 32768.0000 (43223.8860)  weight_decay: 0.0500 (0.0500)  time: 0.6883  data: 0.1809  max mem: 15572
Epoch: [5]  [ 360/1404]  eta: 0:11:27  lr: 0.000094  min_lr: 0.000001  loss: 4.3624 (4.4401)  class_acc: 0.1667 (0.1422)  loss_scale: 32768.0000 (42934.2493)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1523  max mem: 15572
[2025-01-17 09:14:24,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:14:24,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 370/1404]  eta: 0:11:21  lr: 0.000094  min_lr: 0.000001  loss: 4.3624 (4.4414)  class_acc: 0.1250 (0.1418)  loss_scale: 32768.0000 (42748.5499)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1231  max mem: 15572
[2025-01-17 09:14:24,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:14:24,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 380/1404]  eta: 0:11:13  lr: 0.000094  min_lr: 0.000001  loss: 4.4649 (4.4421)  class_acc: 0.1250 (0.1421)  loss_scale: 65536.0000 (43346.6457)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1453  max mem: 15572
Epoch: [5]  [ 390/1404]  eta: 0:11:08  lr: 0.000094  min_lr: 0.000001  loss: 4.4315 (4.4413)  class_acc: 0.1250 (0.1419)  loss_scale: 65536.0000 (43914.1483)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.1713  max mem: 15572
Epoch: [5]  [ 400/1404]  eta: 0:11:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4190 (4.4416)  class_acc: 0.1667 (0.1430)  loss_scale: 65536.0000 (44453.3466)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1561  max mem: 15572
Epoch: [5]  [ 410/1404]  eta: 0:10:54  lr: 0.000094  min_lr: 0.000001  loss: 4.4045 (4.4400)  class_acc: 0.1667 (0.1430)  loss_scale: 65536.0000 (44966.3066)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.1611  max mem: 15572
Epoch: [5]  [ 420/1404]  eta: 0:10:48  lr: 0.000094  min_lr: 0.000001  loss: 4.3570 (4.4389)  class_acc: 0.1250 (0.1435)  loss_scale: 65536.0000 (45454.8979)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.1914  max mem: 15572
Epoch: [5]  [ 430/1404]  eta: 0:10:40  lr: 0.000094  min_lr: 0.000001  loss: 4.3165 (4.4371)  class_acc: 0.1250 (0.1431)  loss_scale: 65536.0000 (45920.8167)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1162  max mem: 15572
Epoch: [5]  [ 440/1404]  eta: 0:10:34  lr: 0.000094  min_lr: 0.000001  loss: 4.3960 (4.4353)  class_acc: 0.1250 (0.1434)  loss_scale: 65536.0000 (46365.6054)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1289  max mem: 15572
Epoch: [5]  [ 450/1404]  eta: 0:10:25  lr: 0.000094  min_lr: 0.000001  loss: 4.4799 (4.4362)  class_acc: 0.1250 (0.1433)  loss_scale: 65536.0000 (46790.6696)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1143  max mem: 15572
Epoch: [5]  [ 460/1404]  eta: 0:10:19  lr: 0.000094  min_lr: 0.000001  loss: 4.5068 (4.4387)  class_acc: 0.1250 (0.1431)  loss_scale: 65536.0000 (47197.2928)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1104  max mem: 15572
Epoch: [5]  [ 470/1404]  eta: 0:10:12  lr: 0.000094  min_lr: 0.000001  loss: 4.5382 (4.4407)  class_acc: 0.1250 (0.1433)  loss_scale: 65536.0000 (47586.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.1366  max mem: 15572
Epoch: [5]  [ 480/1404]  eta: 0:10:04  lr: 0.000094  min_lr: 0.000001  loss: 4.4296 (4.4372)  class_acc: 0.1667 (0.1439)  loss_scale: 65536.0000 (47959.8170)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0765  max mem: 15572
Epoch: [5]  [ 490/1404]  eta: 0:09:58  lr: 0.000094  min_lr: 0.000001  loss: 4.1798 (4.4341)  class_acc: 0.1667 (0.1443)  loss_scale: 65536.0000 (48317.7841)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1119  max mem: 15572
[2025-01-17 09:15:45,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:15:45,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:15:45,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:15:45,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:15:46,431] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7519
[2025-01-17 09:15:46,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:15:46,432] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7519
[2025-01-17 09:15:46,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:15:46,432] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 500/1404]  eta: 0:09:50  lr: 0.000094  min_lr: 0.000001  loss: 4.4614 (4.4347)  class_acc: 0.1250 (0.1442)  loss_scale: 65536.0000 (48792.2715)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1020  max mem: 15572
Epoch: [5]  [ 510/1404]  eta: 0:09:43  lr: 0.000094  min_lr: 0.000001  loss: 4.4974 (4.4378)  class_acc: 0.1250 (0.1439)  loss_scale: 65536.0000 (49119.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0773  max mem: 15572
Epoch: [5]  [ 520/1404]  eta: 0:09:35  lr: 0.000094  min_lr: 0.000001  loss: 4.4876 (4.4371)  class_acc: 0.1667 (0.1444)  loss_scale: 65536.0000 (49435.0250)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0731  max mem: 15572
Epoch: [5]  [ 530/1404]  eta: 0:09:28  lr: 0.000094  min_lr: 0.000001  loss: 4.5212 (4.4386)  class_acc: 0.1250 (0.1438)  loss_scale: 65536.0000 (49738.2448)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0451  max mem: 15572
Epoch: [5]  [ 540/1404]  eta: 0:09:23  lr: 0.000094  min_lr: 0.000001  loss: 4.5212 (4.4358)  class_acc: 0.1250 (0.1443)  loss_scale: 65536.0000 (50030.2551)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.1237  max mem: 15572
Epoch: [5]  [ 550/1404]  eta: 0:09:14  lr: 0.000094  min_lr: 0.000001  loss: 4.3361 (4.4347)  class_acc: 0.1667 (0.1447)  loss_scale: 65536.0000 (50311.6661)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1068  max mem: 15572
Epoch: [5]  [ 560/1404]  eta: 0:09:07  lr: 0.000094  min_lr: 0.000001  loss: 4.4251 (4.4352)  class_acc: 0.1667 (0.1452)  loss_scale: 65536.0000 (50583.0446)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0925  max mem: 15572
Epoch: [5]  [ 570/1404]  eta: 0:09:02  lr: 0.000094  min_lr: 0.000001  loss: 4.4980 (4.4347)  class_acc: 0.1667 (0.1464)  loss_scale: 65536.0000 (50844.9177)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.1811  max mem: 15572
Epoch: [5]  [ 580/1404]  eta: 0:08:54  lr: 0.000094  min_lr: 0.000001  loss: 4.3694 (4.4321)  class_acc: 0.2083 (0.1470)  loss_scale: 65536.0000 (51097.7762)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1373  max mem: 15572
[2025-01-17 09:16:39,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7605
[2025-01-17 09:16:39,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:16:39,312] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7605
[2025-01-17 09:16:39,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:16:39,313] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 590/1404]  eta: 0:08:45  lr: 0.000094  min_lr: 0.000001  loss: 4.3498 (4.4316)  class_acc: 0.1250 (0.1474)  loss_scale: 65536.0000 (51009.4078)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0424  max mem: 15572
Epoch: [5]  [ 600/1404]  eta: 0:08:39  lr: 0.000094  min_lr: 0.000001  loss: 4.3427 (4.4305)  class_acc: 0.1250 (0.1474)  loss_scale: 32768.0000 (50705.8902)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0686  max mem: 15572
Epoch: [5]  [ 610/1404]  eta: 0:08:33  lr: 0.000094  min_lr: 0.000001  loss: 4.2120 (4.4276)  class_acc: 0.2083 (0.1485)  loss_scale: 32768.0000 (50412.3077)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.1648  max mem: 15572
Epoch: [5]  [ 620/1404]  eta: 0:08:25  lr: 0.000094  min_lr: 0.000001  loss: 4.2832 (4.4254)  class_acc: 0.2083 (0.1488)  loss_scale: 32768.0000 (50128.1804)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1106  max mem: 15572
Epoch: [5]  [ 630/1404]  eta: 0:08:19  lr: 0.000094  min_lr: 0.000001  loss: 4.3778 (4.4263)  class_acc: 0.1667 (0.1488)  loss_scale: 32768.0000 (49853.0586)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0969  max mem: 15572
Epoch: [5]  [ 640/1404]  eta: 0:08:12  lr: 0.000094  min_lr: 0.000001  loss: 4.4187 (4.4258)  class_acc: 0.1667 (0.1493)  loss_scale: 32768.0000 (49586.5211)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1090  max mem: 15572
Epoch: [5]  [ 650/1404]  eta: 0:08:05  lr: 0.000094  min_lr: 0.000001  loss: 4.4187 (4.4282)  class_acc: 0.1250 (0.1491)  loss_scale: 32768.0000 (49328.1720)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0881  max mem: 15572
Epoch: [5]  [ 660/1404]  eta: 0:07:58  lr: 0.000094  min_lr: 0.000001  loss: 4.3544 (4.4265)  class_acc: 0.1250 (0.1491)  loss_scale: 32768.0000 (49077.6399)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0964  max mem: 15572
Epoch: [5]  [ 670/1404]  eta: 0:07:51  lr: 0.000094  min_lr: 0.000001  loss: 4.3544 (4.4268)  class_acc: 0.1667 (0.1492)  loss_scale: 32768.0000 (48834.5753)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0474  max mem: 15572
Epoch: [5]  [ 680/1404]  eta: 0:07:43  lr: 0.000094  min_lr: 0.000001  loss: 4.4485 (4.4263)  class_acc: 0.1667 (0.1492)  loss_scale: 32768.0000 (48598.6490)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0204  max mem: 15572
Epoch: [5]  [ 690/1404]  eta: 0:07:38  lr: 0.000094  min_lr: 0.000001  loss: 4.4534 (4.4270)  class_acc: 0.1667 (0.1492)  loss_scale: 32768.0000 (48369.5514)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1037  max mem: 15572
Epoch: [5]  [ 700/1404]  eta: 0:07:31  lr: 0.000094  min_lr: 0.000001  loss: 4.3186 (4.4217)  class_acc: 0.1667 (0.1504)  loss_scale: 32768.0000 (48146.9900)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1523  max mem: 15572
Epoch: [5]  [ 710/1404]  eta: 0:07:24  lr: 0.000094  min_lr: 0.000001  loss: 4.2928 (4.4235)  class_acc: 0.1667 (0.1505)  loss_scale: 32768.0000 (47930.6892)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1181  max mem: 15572
[2025-01-17 09:17:57,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:17:57,984] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:17:58,001] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:17:58,001] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 720/1404]  eta: 0:07:18  lr: 0.000094  min_lr: 0.000001  loss: 4.4072 (4.4225)  class_acc: 0.2083 (0.1518)  loss_scale: 32768.0000 (48038.5243)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0962  max mem: 15572
Epoch: [5]  [ 730/1404]  eta: 0:07:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3642 (4.4220)  class_acc: 0.2083 (0.1522)  loss_scale: 65536.0000 (48277.8878)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0780  max mem: 15572
Epoch: [5]  [ 740/1404]  eta: 0:07:04  lr: 0.000094  min_lr: 0.000001  loss: 4.4220 (4.4231)  class_acc: 0.1250 (0.1515)  loss_scale: 65536.0000 (48510.7908)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0512  max mem: 15572
Epoch: [5]  [ 750/1404]  eta: 0:06:57  lr: 0.000094  min_lr: 0.000001  loss: 4.4897 (4.4235)  class_acc: 0.0833 (0.1511)  loss_scale: 65536.0000 (48737.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0269  max mem: 15572
Epoch: [5]  [ 760/1404]  eta: 0:06:51  lr: 0.000094  min_lr: 0.000001  loss: 4.5097 (4.4227)  class_acc: 0.1250 (0.1516)  loss_scale: 65536.0000 (48958.2339)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0698  max mem: 15572
Epoch: [5]  [ 770/1404]  eta: 0:06:44  lr: 0.000094  min_lr: 0.000001  loss: 4.4164 (4.4227)  class_acc: 0.1667 (0.1520)  loss_scale: 65536.0000 (49173.2503)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.0436  max mem: 15572
Epoch: [5]  [ 780/1404]  eta: 0:06:38  lr: 0.000094  min_lr: 0.000001  loss: 4.3519 (4.4206)  class_acc: 0.1667 (0.1522)  loss_scale: 65536.0000 (49382.7606)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0241  max mem: 15572
[2025-01-17 09:18:42,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7807
[2025-01-17 09:18:42,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:18:42,837] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7807
[2025-01-17 09:18:42,837] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:18:42,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [ 790/1404]  eta: 0:06:31  lr: 0.000094  min_lr: 0.000001  loss: 4.2642 (4.4195)  class_acc: 0.1667 (0.1530)  loss_scale: 65536.0000 (49421.2693)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0667  max mem: 15572
Epoch: [5]  [ 800/1404]  eta: 0:06:25  lr: 0.000094  min_lr: 0.000001  loss: 4.4347 (4.4183)  class_acc: 0.1667 (0.1532)  loss_scale: 32768.0000 (49213.3633)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1213  max mem: 15572
Epoch: [5]  [ 810/1404]  eta: 0:06:19  lr: 0.000094  min_lr: 0.000001  loss: 4.4347 (4.4174)  class_acc: 0.1667 (0.1536)  loss_scale: 32768.0000 (49010.5845)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.1674  max mem: 15572
Epoch: [5]  [ 820/1404]  eta: 0:06:12  lr: 0.000094  min_lr: 0.000001  loss: 4.2993 (4.4164)  class_acc: 0.1667 (0.1536)  loss_scale: 32768.0000 (48812.7454)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1123  max mem: 15572
Epoch: [5]  [ 830/1404]  eta: 0:06:05  lr: 0.000094  min_lr: 0.000001  loss: 4.2993 (4.4159)  class_acc: 0.1667 (0.1540)  loss_scale: 32768.0000 (48619.6679)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.0830  max mem: 15572
Epoch: [5]  [ 840/1404]  eta: 0:05:59  lr: 0.000094  min_lr: 0.000001  loss: 4.3132 (4.4145)  class_acc: 0.2083 (0.1544)  loss_scale: 32768.0000 (48431.1819)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0636  max mem: 15572
Epoch: [5]  [ 850/1404]  eta: 0:05:52  lr: 0.000094  min_lr: 0.000001  loss: 4.3737 (4.4151)  class_acc: 0.1667 (0.1545)  loss_scale: 32768.0000 (48247.1257)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0481  max mem: 15572
Epoch: [5]  [ 860/1404]  eta: 0:05:45  lr: 0.000094  min_lr: 0.000001  loss: 4.3241 (4.4138)  class_acc: 0.1667 (0.1547)  loss_scale: 32768.0000 (48067.3449)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0878  max mem: 15572
Epoch: [5]  [ 870/1404]  eta: 0:05:38  lr: 0.000094  min_lr: 0.000001  loss: 4.3241 (4.4146)  class_acc: 0.1667 (0.1547)  loss_scale: 32768.0000 (47891.6923)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.0515  max mem: 15572
Epoch: [5]  [ 880/1404]  eta: 0:05:32  lr: 0.000094  min_lr: 0.000001  loss: 4.4399 (4.4154)  class_acc: 0.1667 (0.1545)  loss_scale: 32768.0000 (47720.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0417  max mem: 15572
Epoch: [5]  [ 890/1404]  eta: 0:05:25  lr: 0.000094  min_lr: 0.000001  loss: 4.4640 (4.4170)  class_acc: 0.1250 (0.1541)  loss_scale: 32768.0000 (47552.2155)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0511  max mem: 15572
Epoch: [5]  [ 900/1404]  eta: 0:05:19  lr: 0.000094  min_lr: 0.000001  loss: 4.4671 (4.4164)  class_acc: 0.0833 (0.1537)  loss_scale: 32768.0000 (47388.1287)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1141  max mem: 15572
Epoch: [5]  [ 910/1404]  eta: 0:05:12  lr: 0.000094  min_lr: 0.000001  loss: 4.3459 (4.4156)  class_acc: 0.1250 (0.1543)  loss_scale: 32768.0000 (47227.6443)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1452  max mem: 15572
[2025-01-17 09:20:00,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:20:00,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:20:00,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:20:00,646] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [ 920/1404]  eta: 0:05:06  lr: 0.000094  min_lr: 0.000001  loss: 4.4598 (4.4168)  class_acc: 0.1667 (0.1540)  loss_scale: 32768.0000 (47248.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0581  max mem: 15572
Epoch: [5]  [ 930/1404]  eta: 0:05:00  lr: 0.000094  min_lr: 0.000001  loss: 4.4598 (4.4163)  class_acc: 0.1250 (0.1543)  loss_scale: 65536.0000 (47444.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.0977  max mem: 15572
Epoch: [5]  [ 940/1404]  eta: 0:04:53  lr: 0.000094  min_lr: 0.000001  loss: 4.3830 (4.4151)  class_acc: 0.1667 (0.1543)  loss_scale: 65536.0000 (47637.2200)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.1183  max mem: 15572
Epoch: [5]  [ 950/1404]  eta: 0:04:47  lr: 0.000094  min_lr: 0.000001  loss: 4.3830 (4.4146)  class_acc: 0.1250 (0.1542)  loss_scale: 65536.0000 (47825.4301)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.1098  max mem: 15572
Epoch: [5]  [ 960/1404]  eta: 0:04:41  lr: 0.000094  min_lr: 0.000001  loss: 4.4750 (4.4149)  class_acc: 0.1250 (0.1541)  loss_scale: 65536.0000 (48009.7232)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1107  max mem: 15572
Epoch: [5]  [ 970/1404]  eta: 0:04:34  lr: 0.000094  min_lr: 0.000001  loss: 4.5400 (4.4157)  class_acc: 0.1250 (0.1540)  loss_scale: 65536.0000 (48190.2204)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.0823  max mem: 15572
[2025-01-17 09:20:40,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=39, lr=[9.074460811791398e-07, 9.074460811791398e-07, 1.2963515445416284e-06, 1.2963515445416284e-06, 1.8519307779166122e-06, 1.8519307779166122e-06, 2.6456153970237317e-06, 2.6456153970237317e-06, 3.77945056717676e-06, 3.77945056717676e-06, 5.3992150959668e-06, 5.3992150959668e-06, 7.713164422809715e-06, 7.713164422809715e-06, 1.1018806318299594e-05, 1.1018806318299594e-05, 1.5741151883285134e-05, 1.5741151883285134e-05, 2.248735983326448e-05, 2.248735983326448e-05, 3.21247997618064e-05, 3.21247997618064e-05, 4.589257108829486e-05, 4.589257108829486e-05, 6.556081584042124e-05, 6.556081584042124e-05, 9.365830834345891e-05, 9.365830834345891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 09:20:40,409] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=44.262549700316185, CurrSamplesPerSec=58.12075973690045, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [ 980/1404]  eta: 0:04:28  lr: 0.000094  min_lr: 0.000001  loss: 4.4993 (4.4156)  class_acc: 0.1250 (0.1540)  loss_scale: 65536.0000 (48367.0377)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0918  max mem: 15572
Epoch: [5]  [ 990/1404]  eta: 0:04:21  lr: 0.000094  min_lr: 0.000001  loss: 4.2303 (4.4143)  class_acc: 0.1667 (0.1546)  loss_scale: 65536.0000 (48540.2866)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1100  max mem: 15572
Epoch: [5]  [1000/1404]  eta: 0:04:16  lr: 0.000094  min_lr: 0.000001  loss: 4.3048 (4.4140)  class_acc: 0.1667 (0.1549)  loss_scale: 65536.0000 (48710.0739)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.2449  max mem: 15572
Epoch: [5]  [1010/1404]  eta: 0:04:09  lr: 0.000094  min_lr: 0.000001  loss: 4.3292 (4.4129)  class_acc: 0.1667 (0.1550)  loss_scale: 65536.0000 (48876.5025)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.1756  max mem: 15572
Epoch: [5]  [1020/1404]  eta: 0:04:02  lr: 0.000094  min_lr: 0.000001  loss: 4.3567 (4.4137)  class_acc: 0.1250 (0.1548)  loss_scale: 65536.0000 (49039.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0490  max mem: 15572
Epoch: [5]  [1030/1404]  eta: 0:03:56  lr: 0.000094  min_lr: 0.000001  loss: 4.3298 (4.4109)  class_acc: 0.1667 (0.1556)  loss_scale: 65536.0000 (49199.6741)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0878  max mem: 15572
Epoch: [5]  [1040/1404]  eta: 0:03:49  lr: 0.000094  min_lr: 0.000001  loss: 4.3298 (4.4110)  class_acc: 0.2083 (0.1556)  loss_scale: 65536.0000 (49356.6033)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0820  max mem: 15572
[2025-01-17 09:21:19,930] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:21:19,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:21:19,932] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:21:19,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:21:20,933] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8066
[2025-01-17 09:21:20,933] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:21:20,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 09:21:20,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8066
[2025-01-17 09:21:20,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [5]  [1050/1404]  eta: 0:03:44  lr: 0.000094  min_lr: 0.000001  loss: 4.4306 (4.4115)  class_acc: 0.0833 (0.1552)  loss_scale: 65536.0000 (49635.2578)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.1651  max mem: 15572
Epoch: [5]  [1060/1404]  eta: 0:03:37  lr: 0.000094  min_lr: 0.000001  loss: 4.4116 (4.4106)  class_acc: 0.1250 (0.1553)  loss_scale: 65536.0000 (49785.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6870  data: 0.1518  max mem: 15572
Epoch: [5]  [1070/1404]  eta: 0:03:31  lr: 0.000094  min_lr: 0.000001  loss: 4.3614 (4.4101)  class_acc: 0.1250 (0.1555)  loss_scale: 65536.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0429  max mem: 15572
Epoch: [5]  [1080/1404]  eta: 0:03:24  lr: 0.000094  min_lr: 0.000001  loss: 4.3671 (4.4100)  class_acc: 0.1667 (0.1555)  loss_scale: 65536.0000 (50076.5365)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0508  max mem: 15572
Epoch: [5]  [1090/1404]  eta: 0:03:18  lr: 0.000094  min_lr: 0.000001  loss: 4.4185 (4.4110)  class_acc: 0.1250 (0.1559)  loss_scale: 65536.0000 (50218.2365)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1171  max mem: 15572
Epoch: [5]  [1100/1404]  eta: 0:03:11  lr: 0.000094  min_lr: 0.000001  loss: 4.4780 (4.4123)  class_acc: 0.1667 (0.1559)  loss_scale: 65536.0000 (50357.3624)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0872  max mem: 15572
Epoch: [5]  [1110/1404]  eta: 0:03:05  lr: 0.000094  min_lr: 0.000001  loss: 4.4780 (4.4123)  class_acc: 0.1667 (0.1561)  loss_scale: 65536.0000 (50493.9838)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0801  max mem: 15572
Epoch: [5]  [1120/1404]  eta: 0:02:59  lr: 0.000094  min_lr: 0.000001  loss: 4.3546 (4.4111)  class_acc: 0.1667 (0.1565)  loss_scale: 65536.0000 (50628.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0817  max mem: 15572
Epoch: [5]  [1130/1404]  eta: 0:02:52  lr: 0.000094  min_lr: 0.000001  loss: 4.3725 (4.4108)  class_acc: 0.2083 (0.1568)  loss_scale: 65536.0000 (50759.9788)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0268  max mem: 15572
[2025-01-17 09:22:16,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8156
[2025-01-17 09:22:16,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:22:16,423] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8156
[2025-01-17 09:22:16,423] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:22:16,423] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1140/1404]  eta: 0:02:46  lr: 0.000094  min_lr: 0.000001  loss: 4.3610 (4.4098)  class_acc: 0.1667 (0.1572)  loss_scale: 65536.0000 (50745.8861)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0822  max mem: 15572
Epoch: [5]  [1150/1404]  eta: 0:02:40  lr: 0.000094  min_lr: 0.000001  loss: 4.4397 (4.4101)  class_acc: 0.1667 (0.1574)  loss_scale: 32768.0000 (50589.6924)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1012  max mem: 15572
Epoch: [5]  [1160/1404]  eta: 0:02:33  lr: 0.000094  min_lr: 0.000001  loss: 4.3020 (4.4094)  class_acc: 0.1667 (0.1577)  loss_scale: 32768.0000 (50436.1895)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.0366  max mem: 15572
Epoch: [5]  [1170/1404]  eta: 0:02:27  lr: 0.000094  min_lr: 0.000001  loss: 4.2941 (4.4088)  class_acc: 0.1667 (0.1578)  loss_scale: 32768.0000 (50285.3083)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0399  max mem: 15572
Epoch: [5]  [1180/1404]  eta: 0:02:20  lr: 0.000094  min_lr: 0.000001  loss: 4.3085 (4.4088)  class_acc: 0.1250 (0.1578)  loss_scale: 32768.0000 (50136.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0811  max mem: 15572
Epoch: [5]  [1190/1404]  eta: 0:02:14  lr: 0.000094  min_lr: 0.000001  loss: 4.3842 (4.4090)  class_acc: 0.1667 (0.1581)  loss_scale: 32768.0000 (49991.1469)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1105  max mem: 15572
Epoch: [5]  [1200/1404]  eta: 0:02:08  lr: 0.000094  min_lr: 0.000001  loss: 4.5510 (4.4078)  class_acc: 0.1667 (0.1585)  loss_scale: 32768.0000 (49847.7402)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1141  max mem: 15572
Epoch: [5]  [1210/1404]  eta: 0:02:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2439 (4.4066)  class_acc: 0.1667 (0.1586)  loss_scale: 32768.0000 (49706.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1361  max mem: 15572
Epoch: [5]  [1220/1404]  eta: 0:01:55  lr: 0.000094  min_lr: 0.000001  loss: 4.3492 (4.4062)  class_acc: 0.1667 (0.1587)  loss_scale: 32768.0000 (49567.9738)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0963  max mem: 15572
Epoch: [5]  [1230/1404]  eta: 0:01:49  lr: 0.000094  min_lr: 0.000001  loss: 4.3492 (4.4055)  class_acc: 0.1667 (0.1592)  loss_scale: 32768.0000 (49431.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0704  max mem: 15572
Epoch: [5]  [1240/1404]  eta: 0:01:42  lr: 0.000094  min_lr: 0.000001  loss: 4.3636 (4.4058)  class_acc: 0.1667 (0.1590)  loss_scale: 32768.0000 (49297.2248)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0844  max mem: 15572
Epoch: [5]  [1250/1404]  eta: 0:01:36  lr: 0.000094  min_lr: 0.000001  loss: 4.3132 (4.4059)  class_acc: 0.1667 (0.1592)  loss_scale: 32768.0000 (49165.0967)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0986  max mem: 15572
Epoch: [5]  [1260/1404]  eta: 0:01:30  lr: 0.000094  min_lr: 0.000001  loss: 4.2432 (4.4051)  class_acc: 0.1667 (0.1590)  loss_scale: 32768.0000 (49035.0642)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1458  max mem: 15572
[2025-01-17 09:23:34,280] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:23:34,280] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:23:34,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:23:34,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1270/1404]  eta: 0:01:24  lr: 0.000094  min_lr: 0.000001  loss: 4.3925 (4.4057)  class_acc: 0.1250 (0.1587)  loss_scale: 32768.0000 (49061.7655)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.1659  max mem: 15572
Epoch: [5]  [1280/1404]  eta: 0:01:17  lr: 0.000094  min_lr: 0.000001  loss: 4.3154 (4.4046)  class_acc: 0.1667 (0.1587)  loss_scale: 65536.0000 (49190.3700)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1159  max mem: 15572
Epoch: [5]  [1290/1404]  eta: 0:01:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3154 (4.4049)  class_acc: 0.1667 (0.1587)  loss_scale: 65536.0000 (49316.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0363  max mem: 15572
Epoch: [5]  [1300/1404]  eta: 0:01:05  lr: 0.000094  min_lr: 0.000001  loss: 4.4340 (4.4050)  class_acc: 0.1250 (0.1588)  loss_scale: 65536.0000 (49441.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0897  max mem: 15572
Epoch: [5]  [1310/1404]  eta: 0:00:58  lr: 0.000094  min_lr: 0.000001  loss: 4.3662 (4.4055)  class_acc: 0.1250 (0.1587)  loss_scale: 65536.0000 (49564.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0911  max mem: 15572
Epoch: [5]  [1320/1404]  eta: 0:00:52  lr: 0.000094  min_lr: 0.000001  loss: 4.3182 (4.4052)  class_acc: 0.1250 (0.1586)  loss_scale: 65536.0000 (49685.3172)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0394  max mem: 15572
Epoch: [5]  [1330/1404]  eta: 0:00:46  lr: 0.000094  min_lr: 0.000001  loss: 4.3024 (4.4043)  class_acc: 0.1667 (0.1588)  loss_scale: 65536.0000 (49804.4057)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0561  max mem: 15572
Epoch: [5]  [1340/1404]  eta: 0:00:40  lr: 0.000094  min_lr: 0.000001  loss: 4.2481 (4.4045)  class_acc: 0.1667 (0.1586)  loss_scale: 65536.0000 (49921.7181)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0702  max mem: 15572
Epoch: [5]  [1350/1404]  eta: 0:00:33  lr: 0.000094  min_lr: 0.000001  loss: 4.3507 (4.4039)  class_acc: 0.1250 (0.1588)  loss_scale: 65536.0000 (50037.2939)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0328  max mem: 15572
Epoch: [5]  [1360/1404]  eta: 0:00:27  lr: 0.000094  min_lr: 0.000001  loss: 4.3082 (4.4039)  class_acc: 0.1250 (0.1586)  loss_scale: 65536.0000 (50151.1712)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0845  max mem: 15572
Epoch: [5]  [1370/1404]  eta: 0:00:21  lr: 0.000094  min_lr: 0.000001  loss: 4.3653 (4.4039)  class_acc: 0.1250 (0.1588)  loss_scale: 65536.0000 (50263.3873)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.1619  max mem: 15572
Epoch: [5]  [1380/1404]  eta: 0:00:15  lr: 0.000094  min_lr: 0.000001  loss: 4.4832 (4.4044)  class_acc: 0.1667 (0.1589)  loss_scale: 65536.0000 (50373.9783)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.1784  max mem: 15572
Epoch: [5]  [1390/1404]  eta: 0:00:08  lr: 0.000094  min_lr: 0.000001  loss: 4.4832 (4.4046)  class_acc: 0.1667 (0.1590)  loss_scale: 65536.0000 (50482.9792)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1224  max mem: 15572
[2025-01-17 09:24:53,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:24:53,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:24:53,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:24:53,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:24:53,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8414
[2025-01-17 09:24:53,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8414
[2025-01-17 09:24:53,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:24:53,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:24:53,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1400/1404]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000001  loss: 4.2690 (4.4029)  class_acc: 0.1667 (0.1592)  loss_scale: 65536.0000 (50637.2020)  weight_decay: 0.0500 (0.0500)  time: 0.4613  data: 0.0221  max mem: 15572
Epoch: [5]  [1403/1404]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000001  loss: 4.2944 (4.4037)  class_acc: 0.1667 (0.1591)  loss_scale: 65536.0000 (50669.0370)  weight_decay: 0.0500 (0.0500)  time: 0.4350  data: 0.0220  max mem: 15572
Epoch: [5] Total time: 0:14:37 (0.6250 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000001  loss: 4.2944 (4.4078)  class_acc: 0.1667 (0.1594)  loss_scale: 65536.0000 (50669.0370)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:16:07  loss: 2.1877 (2.1877)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 7.1127  data: 6.9132  max mem: 15572
Val:  [ 10/136]  eta: 0:01:46  loss: 3.8046 (3.6143)  acc1: 5.5556 (21.7172)  acc5: 44.4444 (38.3838)  time: 0.8460  data: 0.6435  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 3.6121 (3.6681)  acc1: 5.5556 (17.4603)  acc5: 44.4444 (41.2698)  time: 0.2771  data: 0.0710  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 3.3784 (3.3663)  acc1: 16.6667 (26.1649)  acc5: 55.5556 (48.9247)  time: 0.2888  data: 0.0708  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.2911 (3.3084)  acc1: 33.3333 (26.2873)  acc5: 77.7778 (52.7100)  time: 0.3104  data: 0.0904  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 3.8010 (3.4629)  acc1: 5.5556 (22.4401)  acc5: 44.4444 (47.0588)  time: 0.3721  data: 0.1566  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 3.9508 (3.5640)  acc1: 0.0000 (19.7632)  acc5: 22.2222 (44.4444)  time: 0.3753  data: 0.1510  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 3.9230 (3.4814)  acc1: 5.5556 (22.7700)  acc5: 44.4444 (46.5571)  time: 0.3946  data: 0.1650  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 3.2614 (3.4656)  acc1: 22.2222 (22.4966)  acc5: 55.5556 (47.1879)  time: 0.3768  data: 0.1609  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.6207 (3.4613)  acc1: 16.6667 (22.0391)  acc5: 50.0000 (48.4127)  time: 0.3462  data: 0.1376  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.5246 (3.5020)  acc1: 11.1111 (21.0671)  acc5: 50.0000 (47.8548)  time: 0.3575  data: 0.1362  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.5261 (3.5121)  acc1: 11.1111 (21.6717)  acc5: 44.4444 (47.7477)  time: 0.3766  data: 0.1529  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 3.2756 (3.4530)  acc1: 33.3333 (23.8751)  acc5: 61.1111 (50.3673)  time: 0.3719  data: 0.1554  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.8804 (3.4218)  acc1: 38.8889 (25.4029)  acc5: 72.2222 (51.1026)  time: 0.3056  data: 0.1228  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 3.0730 (3.4248)  acc1: 38.8889 (25.5119)  acc5: 61.1111 (51.1876)  time: 0.2111  data: 0.0492  max mem: 15572
Val: Total time: 0:00:51 (0.3809 s / it)
* Acc@1 24.734 Acc@5 49.939 loss 3.459
Accuracy of the network on the 4883 val videos: 24.7%
[2025-01-17 09:25:49,567] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 09:25:49,569] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 09:25:49,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 09:25:49,569] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 09:25:51,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 09:25:51,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.73%
Epoch: [6]  [   0/1404]  eta: 3:40:19  lr: 0.000094  min_lr: 0.000001  loss: 4.2712 (4.2712)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4159  data: 7.3369  max mem: 15572
Epoch: [6]  [  10/1404]  eta: 0:31:21  lr: 0.000094  min_lr: 0.000001  loss: 4.4054 (4.3339)  class_acc: 0.1667 (0.1705)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3500  data: 0.6727  max mem: 15572
Epoch: [6]  [  20/1404]  eta: 0:21:57  lr: 0.000094  min_lr: 0.000001  loss: 4.2832 (4.3814)  class_acc: 0.1250 (0.1647)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0036  max mem: 15572
Epoch: [6]  [  30/1404]  eta: 0:19:11  lr: 0.000094  min_lr: 0.000001  loss: 4.3870 (4.4205)  class_acc: 0.1250 (0.1505)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0171  max mem: 15572
Epoch: [6]  [  40/1404]  eta: 0:17:46  lr: 0.000094  min_lr: 0.000001  loss: 4.4853 (4.4397)  class_acc: 0.1250 (0.1524)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0778  max mem: 15572
Epoch: [6]  [  50/1404]  eta: 0:16:48  lr: 0.000094  min_lr: 0.000001  loss: 4.4611 (4.4094)  class_acc: 0.1667 (0.1552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1019  max mem: 15572
[2025-01-17 09:26:31,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8477
[2025-01-17 09:26:31,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:26:31,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:26:31,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8477
[2025-01-17 09:26:31,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [  60/1404]  eta: 0:15:53  lr: 0.000094  min_lr: 0.000001  loss: 4.2238 (4.3793)  class_acc: 0.1667 (0.1673)  loss_scale: 65536.0000 (61238.5574)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0624  max mem: 15572
Epoch: [6]  [  70/1404]  eta: 0:15:38  lr: 0.000094  min_lr: 0.000001  loss: 4.2812 (4.3814)  class_acc: 0.1667 (0.1702)  loss_scale: 32768.0000 (57228.6197)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1135  max mem: 15572
Epoch: [6]  [  80/1404]  eta: 0:15:26  lr: 0.000094  min_lr: 0.000001  loss: 4.3442 (4.3785)  class_acc: 0.1250 (0.1718)  loss_scale: 32768.0000 (54208.7901)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.1671  max mem: 15572
Epoch: [6]  [  90/1404]  eta: 0:15:01  lr: 0.000094  min_lr: 0.000001  loss: 4.3442 (4.3835)  class_acc: 0.1250 (0.1680)  loss_scale: 32768.0000 (51852.6593)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1105  max mem: 15572
Epoch: [6]  [ 100/1404]  eta: 0:14:50  lr: 0.000094  min_lr: 0.000001  loss: 4.4441 (4.3856)  class_acc: 0.1250 (0.1667)  loss_scale: 32768.0000 (49963.0891)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1198  max mem: 15572
Epoch: [6]  [ 110/1404]  eta: 0:14:38  lr: 0.000094  min_lr: 0.000001  loss: 4.4055 (4.3828)  class_acc: 0.1250 (0.1618)  loss_scale: 32768.0000 (48413.9820)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.0948  max mem: 15572
Epoch: [6]  [ 120/1404]  eta: 0:14:20  lr: 0.000094  min_lr: 0.000001  loss: 4.3553 (4.3818)  class_acc: 0.1250 (0.1639)  loss_scale: 32768.0000 (47120.9256)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0196  max mem: 15572
Epoch: [6]  [ 130/1404]  eta: 0:14:13  lr: 0.000094  min_lr: 0.000001  loss: 4.4194 (4.3875)  class_acc: 0.1667 (0.1638)  loss_scale: 32768.0000 (46025.2824)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0867  max mem: 15572
Epoch: [6]  [ 140/1404]  eta: 0:13:55  lr: 0.000094  min_lr: 0.000001  loss: 4.4635 (4.3888)  class_acc: 0.1250 (0.1628)  loss_scale: 32768.0000 (45085.0496)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0778  max mem: 15572
Epoch: [6]  [ 150/1404]  eta: 0:13:42  lr: 0.000094  min_lr: 0.000001  loss: 4.3869 (4.3871)  class_acc: 0.1667 (0.1658)  loss_scale: 32768.0000 (44269.3510)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0010  max mem: 15572
Epoch: [6]  [ 160/1404]  eta: 0:13:30  lr: 0.000094  min_lr: 0.000001  loss: 4.3066 (4.3805)  class_acc: 0.2083 (0.1685)  loss_scale: 32768.0000 (43554.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.0359  max mem: 15572
Epoch: [6]  [ 170/1404]  eta: 0:13:23  lr: 0.000094  min_lr: 0.000001  loss: 4.3594 (4.3847)  class_acc: 0.1250 (0.1667)  loss_scale: 32768.0000 (42924.1637)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0358  max mem: 15572
Epoch: [6]  [ 180/1404]  eta: 0:13:15  lr: 0.000094  min_lr: 0.000001  loss: 4.4635 (4.3902)  class_acc: 0.1250 (0.1639)  loss_scale: 32768.0000 (42363.0497)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0073  max mem: 15572
[2025-01-17 09:27:50,741] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:27:50,741] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:27:50,742] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:27:50,742] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 190/1404]  eta: 0:13:06  lr: 0.000094  min_lr: 0.000001  loss: 4.4242 (4.3816)  class_acc: 0.1250 (0.1651)  loss_scale: 32768.0000 (43404.7330)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0074  max mem: 15572
Epoch: [6]  [ 200/1404]  eta: 0:13:01  lr: 0.000094  min_lr: 0.000001  loss: 4.3941 (4.3822)  class_acc: 0.1667 (0.1681)  loss_scale: 65536.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.0038  max mem: 15572
Epoch: [6]  [ 210/1404]  eta: 0:12:52  lr: 0.000094  min_lr: 0.000001  loss: 4.4182 (4.3778)  class_acc: 0.1667 (0.1673)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0038  max mem: 15572
Epoch: [6]  [ 220/1404]  eta: 0:12:42  lr: 0.000093  min_lr: 0.000001  loss: 4.3920 (4.3780)  class_acc: 0.1250 (0.1665)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0147  max mem: 15572
Epoch: [6]  [ 230/1404]  eta: 0:12:35  lr: 0.000093  min_lr: 0.000001  loss: 4.3081 (4.3762)  class_acc: 0.1667 (0.1690)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0147  max mem: 15572
Epoch: [6]  [ 240/1404]  eta: 0:12:25  lr: 0.000093  min_lr: 0.000001  loss: 4.3081 (4.3743)  class_acc: 0.1667 (0.1682)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0400  max mem: 15572
Epoch: [6]  [ 250/1404]  eta: 0:12:21  lr: 0.000093  min_lr: 0.000001  loss: 4.3970 (4.3741)  class_acc: 0.1667 (0.1700)  loss_scale: 65536.0000 (48695.0757)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0643  max mem: 15572
Epoch: [6]  [ 260/1404]  eta: 0:12:11  lr: 0.000093  min_lr: 0.000001  loss: 4.1256 (4.3645)  class_acc: 0.2083 (0.1713)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.0491  max mem: 15572
Epoch: [6]  [ 270/1404]  eta: 0:12:04  lr: 0.000093  min_lr: 0.000001  loss: 4.1256 (4.3634)  class_acc: 0.1667 (0.1727)  loss_scale: 65536.0000 (49937.9483)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0494  max mem: 15572
Epoch: [6]  [ 280/1404]  eta: 0:11:54  lr: 0.000093  min_lr: 0.000001  loss: 4.3696 (4.3612)  class_acc: 0.1667 (0.1716)  loss_scale: 65536.0000 (50493.0391)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0255  max mem: 15572
[2025-01-17 09:28:54,662] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8710
[2025-01-17 09:28:54,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:28:54,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:28:54,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8710
[2025-01-17 09:28:54,666] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 290/1404]  eta: 0:11:49  lr: 0.000093  min_lr: 0.000001  loss: 4.3165 (4.3606)  class_acc: 0.1250 (0.1711)  loss_scale: 65536.0000 (50446.9553)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0324  max mem: 15572
Epoch: [6]  [ 300/1404]  eta: 0:11:44  lr: 0.000093  min_lr: 0.000001  loss: 4.3573 (4.3656)  class_acc: 0.1667 (0.1704)  loss_scale: 32768.0000 (49859.6146)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.0324  max mem: 15572
Epoch: [6]  [ 310/1404]  eta: 0:11:32  lr: 0.000093  min_lr: 0.000001  loss: 4.4095 (4.3677)  class_acc: 0.1667 (0.1702)  loss_scale: 32768.0000 (49310.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.0007  max mem: 15572
Epoch: [6]  [ 320/1404]  eta: 0:11:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4365 (4.3701)  class_acc: 0.1667 (0.1699)  loss_scale: 32768.0000 (48794.7165)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0009  max mem: 15572
Epoch: [6]  [ 330/1404]  eta: 0:11:20  lr: 0.000093  min_lr: 0.000001  loss: 4.4231 (4.3675)  class_acc: 0.1667 (0.1716)  loss_scale: 32768.0000 (48310.5257)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.0225  max mem: 15572
Epoch: [6]  [ 340/1404]  eta: 0:11:14  lr: 0.000093  min_lr: 0.000001  loss: 4.2130 (4.3678)  class_acc: 0.1667 (0.1717)  loss_scale: 32768.0000 (47854.7331)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0550  max mem: 15572
Epoch: [6]  [ 350/1404]  eta: 0:11:06  lr: 0.000093  min_lr: 0.000001  loss: 4.2530 (4.3645)  class_acc: 0.1667 (0.1741)  loss_scale: 32768.0000 (47424.9117)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0532  max mem: 15572
Epoch: [6]  [ 360/1404]  eta: 0:11:01  lr: 0.000093  min_lr: 0.000001  loss: 4.2915 (4.3630)  class_acc: 0.2083 (0.1751)  loss_scale: 32768.0000 (47018.9030)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0742  max mem: 15572
Epoch: [6]  [ 370/1404]  eta: 0:10:52  lr: 0.000093  min_lr: 0.000001  loss: 4.3248 (4.3628)  class_acc: 0.1667 (0.1745)  loss_scale: 32768.0000 (46634.7817)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0544  max mem: 15572
Epoch: [6]  [ 380/1404]  eta: 0:10:46  lr: 0.000093  min_lr: 0.000001  loss: 4.3248 (4.3622)  class_acc: 0.1667 (0.1752)  loss_scale: 32768.0000 (46270.8241)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0445  max mem: 15572
Epoch: [6]  [ 390/1404]  eta: 0:10:39  lr: 0.000093  min_lr: 0.000001  loss: 4.3014 (4.3627)  class_acc: 0.1667 (0.1749)  loss_scale: 32768.0000 (45925.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0593  max mem: 15572
Epoch: [6]  [ 400/1404]  eta: 0:10:33  lr: 0.000093  min_lr: 0.000001  loss: 4.3014 (4.3621)  class_acc: 0.1667 (0.1747)  loss_scale: 32768.0000 (45597.3666)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0244  max mem: 15572
Epoch: [6]  [ 410/1404]  eta: 0:10:25  lr: 0.000093  min_lr: 0.000001  loss: 4.4194 (4.3636)  class_acc: 0.2083 (0.1750)  loss_scale: 32768.0000 (45285.2165)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0097  max mem: 15572
[2025-01-17 09:30:14,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:30:14,029] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:30:14,036] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:30:14,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 420/1404]  eta: 0:10:18  lr: 0.000093  min_lr: 0.000001  loss: 4.4236 (4.3649)  class_acc: 0.1667 (0.1750)  loss_scale: 32768.0000 (45454.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0011  max mem: 15572
Epoch: [6]  [ 430/1404]  eta: 0:10:12  lr: 0.000093  min_lr: 0.000001  loss: 4.3625 (4.3635)  class_acc: 0.1667 (0.1752)  loss_scale: 65536.0000 (45920.8167)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0137  max mem: 15572
Epoch: [6]  [ 440/1404]  eta: 0:10:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3647 (4.3648)  class_acc: 0.1667 (0.1745)  loss_scale: 65536.0000 (46365.6054)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0135  max mem: 15572
Epoch: [6]  [ 450/1404]  eta: 0:09:59  lr: 0.000093  min_lr: 0.000001  loss: 4.3647 (4.3641)  class_acc: 0.1250 (0.1744)  loss_scale: 65536.0000 (46790.6696)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0822  max mem: 15572
Epoch: [6]  [ 460/1404]  eta: 0:09:52  lr: 0.000093  min_lr: 0.000001  loss: 4.4431 (4.3676)  class_acc: 0.1250 (0.1734)  loss_scale: 65536.0000 (47197.2928)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1424  max mem: 15572
Epoch: [6]  [ 470/1404]  eta: 0:09:48  lr: 0.000093  min_lr: 0.000001  loss: 4.4197 (4.3619)  class_acc: 0.1667 (0.1739)  loss_scale: 65536.0000 (47586.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6693  data: 0.1733  max mem: 15572
Epoch: [6]  [ 480/1404]  eta: 0:09:39  lr: 0.000093  min_lr: 0.000001  loss: 4.1933 (4.3577)  class_acc: 0.2083 (0.1747)  loss_scale: 65536.0000 (47959.8170)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1130  max mem: 15572
Epoch: [6]  [ 490/1404]  eta: 0:09:30  lr: 0.000093  min_lr: 0.000001  loss: 4.3846 (4.3589)  class_acc: 0.2083 (0.1751)  loss_scale: 65536.0000 (48317.7841)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0007  max mem: 15572
Epoch: [6]  [ 500/1404]  eta: 0:09:24  lr: 0.000093  min_lr: 0.000001  loss: 4.3846 (4.3587)  class_acc: 0.1667 (0.1748)  loss_scale: 65536.0000 (48661.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0496  max mem: 15572
Epoch: [6]  [ 510/1404]  eta: 0:09:18  lr: 0.000093  min_lr: 0.000001  loss: 4.3548 (4.3608)  class_acc: 0.1250 (0.1741)  loss_scale: 65536.0000 (48991.6869)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.0866  max mem: 15572
[2025-01-17 09:31:17,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-17 09:31:17,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:31:17,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8944
[2025-01-17 09:31:17,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:31:17,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 520/1404]  eta: 0:09:12  lr: 0.000093  min_lr: 0.000001  loss: 4.3932 (4.3599)  class_acc: 0.1250 (0.1743)  loss_scale: 65536.0000 (49246.3417)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.0661  max mem: 15572
Epoch: [6]  [ 530/1404]  eta: 0:09:05  lr: 0.000093  min_lr: 0.000001  loss: 4.2098 (4.3585)  class_acc: 0.1667 (0.1746)  loss_scale: 32768.0000 (48936.0151)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.0616  max mem: 15572
Epoch: [6]  [ 540/1404]  eta: 0:08:59  lr: 0.000093  min_lr: 0.000001  loss: 4.3240 (4.3609)  class_acc: 0.1667 (0.1738)  loss_scale: 32768.0000 (48637.1608)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0332  max mem: 15572
Epoch: [6]  [ 550/1404]  eta: 0:08:53  lr: 0.000093  min_lr: 0.000001  loss: 4.4655 (4.3591)  class_acc: 0.1250 (0.1745)  loss_scale: 32768.0000 (48349.1543)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.0510  max mem: 15572
Epoch: [6]  [ 560/1404]  eta: 0:08:48  lr: 0.000093  min_lr: 0.000001  loss: 4.4655 (4.3581)  class_acc: 0.1250 (0.1742)  loss_scale: 32768.0000 (48071.4153)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.1230  max mem: 15572
Epoch: [6]  [ 570/1404]  eta: 0:08:40  lr: 0.000093  min_lr: 0.000001  loss: 4.2316 (4.3561)  class_acc: 0.1667 (0.1752)  loss_scale: 32768.0000 (47803.4046)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0963  max mem: 15572
[2025-01-17 09:31:51,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=45, lr=[9.047079288391729e-07, 9.047079288391729e-07, 1.2924398983416757e-06, 1.2924398983416757e-06, 1.8463427119166798e-06, 1.8463427119166798e-06, 2.637632445595257e-06, 2.637632445595257e-06, 3.7680463508503673e-06, 3.7680463508503673e-06, 5.382923358357668e-06, 5.382923358357668e-06, 7.689890511939525e-06, 7.689890511939525e-06, 1.0985557874199324e-05, 1.0985557874199324e-05, 1.5693654105999034e-05, 1.5693654105999034e-05, 2.241950586571291e-05, 2.241950586571291e-05, 3.202786552244701e-05, 3.202786552244701e-05, 4.575409360349573e-05, 4.575409360349573e-05, 6.536299086213677e-05, 6.536299086213677e-05, 9.337570123162396e-05, 9.337570123162396e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 09:31:51,593] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=44.57071833585931, CurrSamplesPerSec=52.11990810710896, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 580/1404]  eta: 0:08:35  lr: 0.000093  min_lr: 0.000001  loss: 4.1843 (4.3529)  class_acc: 0.2083 (0.1756)  loss_scale: 32768.0000 (47544.6196)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0968  max mem: 15572
Epoch: [6]  [ 590/1404]  eta: 0:08:30  lr: 0.000093  min_lr: 0.000001  loss: 4.2532 (4.3534)  class_acc: 0.1667 (0.1760)  loss_scale: 32768.0000 (47294.5922)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1573  max mem: 15572
Epoch: [6]  [ 600/1404]  eta: 0:08:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3140 (4.3544)  class_acc: 0.2083 (0.1767)  loss_scale: 32768.0000 (47052.8852)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0847  max mem: 15572
Epoch: [6]  [ 610/1404]  eta: 0:08:16  lr: 0.000093  min_lr: 0.000001  loss: 4.2806 (4.3528)  class_acc: 0.2083 (0.1773)  loss_scale: 32768.0000 (46819.0900)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0488  max mem: 15572
Epoch: [6]  [ 620/1404]  eta: 0:08:09  lr: 0.000093  min_lr: 0.000001  loss: 4.2764 (4.3516)  class_acc: 0.2083 (0.1779)  loss_scale: 32768.0000 (46592.8245)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0893  max mem: 15572
Epoch: [6]  [ 630/1404]  eta: 0:08:03  lr: 0.000093  min_lr: 0.000001  loss: 4.3023 (4.3507)  class_acc: 0.1667 (0.1778)  loss_scale: 32768.0000 (46373.7306)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0492  max mem: 15572
Epoch: [6]  [ 640/1404]  eta: 0:07:57  lr: 0.000093  min_lr: 0.000001  loss: 4.3023 (4.3508)  class_acc: 0.1250 (0.1777)  loss_scale: 32768.0000 (46161.4727)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0505  max mem: 15572
[2025-01-17 09:32:38,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:32:38,942] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:32:38,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:32:38,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 650/1404]  eta: 0:07:51  lr: 0.000093  min_lr: 0.000001  loss: 4.3000 (4.3506)  class_acc: 0.1250 (0.1780)  loss_scale: 32768.0000 (46056.4055)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.1322  max mem: 15572
Epoch: [6]  [ 660/1404]  eta: 0:07:44  lr: 0.000093  min_lr: 0.000001  loss: 4.3199 (4.3498)  class_acc: 0.1667 (0.1776)  loss_scale: 65536.0000 (46351.1044)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1249  max mem: 15572
Epoch: [6]  [ 670/1404]  eta: 0:07:38  lr: 0.000093  min_lr: 0.000001  loss: 4.3704 (4.3522)  class_acc: 0.1667 (0.1773)  loss_scale: 65536.0000 (46637.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0983  max mem: 15572
Epoch: [6]  [ 680/1404]  eta: 0:07:31  lr: 0.000093  min_lr: 0.000001  loss: 4.4542 (4.3518)  class_acc: 0.1667 (0.1777)  loss_scale: 65536.0000 (46914.5374)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0839  max mem: 15572
Epoch: [6]  [ 690/1404]  eta: 0:07:26  lr: 0.000093  min_lr: 0.000001  loss: 4.4024 (4.3533)  class_acc: 0.1667 (0.1773)  loss_scale: 65536.0000 (47184.0232)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1277  max mem: 15572
Epoch: [6]  [ 700/1404]  eta: 0:07:19  lr: 0.000093  min_lr: 0.000001  loss: 4.3669 (4.3534)  class_acc: 0.1250 (0.1771)  loss_scale: 65536.0000 (47445.8203)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1397  max mem: 15572
[2025-01-17 09:33:11,039] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9127
[2025-01-17 09:33:11,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:33:11,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9127
[2025-01-17 09:33:11,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:33:11,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 710/1404]  eta: 0:07:13  lr: 0.000093  min_lr: 0.000001  loss: 4.3232 (4.3532)  class_acc: 0.1250 (0.1773)  loss_scale: 65536.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1260  max mem: 15572
Epoch: [6]  [ 720/1404]  eta: 0:07:06  lr: 0.000093  min_lr: 0.000001  loss: 4.3232 (4.3501)  class_acc: 0.1667 (0.1781)  loss_scale: 32768.0000 (47129.5645)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1249  max mem: 15572
Epoch: [6]  [ 730/1404]  eta: 0:07:00  lr: 0.000093  min_lr: 0.000001  loss: 4.2868 (4.3504)  class_acc: 0.1667 (0.1778)  loss_scale: 32768.0000 (46933.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1119  max mem: 15572
Epoch: [6]  [ 740/1404]  eta: 0:06:54  lr: 0.000093  min_lr: 0.000001  loss: 4.4102 (4.3495)  class_acc: 0.1667 (0.1781)  loss_scale: 32768.0000 (46741.9379)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1425  max mem: 15572
Epoch: [6]  [ 750/1404]  eta: 0:06:47  lr: 0.000093  min_lr: 0.000001  loss: 4.2150 (4.3486)  class_acc: 0.2083 (0.1787)  loss_scale: 32768.0000 (46555.8668)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0624  max mem: 15572
Epoch: [6]  [ 760/1404]  eta: 0:06:41  lr: 0.000093  min_lr: 0.000001  loss: 4.3561 (4.3491)  class_acc: 0.1667 (0.1782)  loss_scale: 32768.0000 (46374.6859)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0266  max mem: 15572
Epoch: [6]  [ 770/1404]  eta: 0:06:35  lr: 0.000093  min_lr: 0.000001  loss: 4.3742 (4.3489)  class_acc: 0.1250 (0.1780)  loss_scale: 32768.0000 (46198.2049)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0893  max mem: 15572
Epoch: [6]  [ 780/1404]  eta: 0:06:29  lr: 0.000093  min_lr: 0.000001  loss: 4.4243 (4.3495)  class_acc: 0.1250 (0.1779)  loss_scale: 32768.0000 (46026.2433)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.1496  max mem: 15572
Epoch: [6]  [ 790/1404]  eta: 0:06:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3684 (4.3480)  class_acc: 0.1250 (0.1779)  loss_scale: 32768.0000 (45858.6296)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1013  max mem: 15572
Epoch: [6]  [ 800/1404]  eta: 0:06:16  lr: 0.000093  min_lr: 0.000001  loss: 4.4338 (4.3516)  class_acc: 0.1250 (0.1772)  loss_scale: 32768.0000 (45695.2010)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0549  max mem: 15572
Epoch: [6]  [ 810/1404]  eta: 0:06:10  lr: 0.000093  min_lr: 0.000001  loss: 4.5016 (4.3510)  class_acc: 0.1250 (0.1773)  loss_scale: 32768.0000 (45535.8027)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1372  max mem: 15572
Epoch: [6]  [ 820/1404]  eta: 0:06:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3387 (4.3522)  class_acc: 0.1667 (0.1775)  loss_scale: 32768.0000 (45380.2875)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.1780  max mem: 15572
Epoch: [6]  [ 830/1404]  eta: 0:05:57  lr: 0.000093  min_lr: 0.000001  loss: 4.3432 (4.3536)  class_acc: 0.2083 (0.1778)  loss_scale: 32768.0000 (45228.5150)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0924  max mem: 15572
[2025-01-17 09:34:31,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:34:31,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:34:31,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:34:31,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:34:35,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9261
[2025-01-17 09:34:35,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9261
[2025-01-17 09:34:35,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:34:35,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:34:35,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 840/1404]  eta: 0:05:51  lr: 0.000093  min_lr: 0.000001  loss: 4.3952 (4.3547)  class_acc: 0.1667 (0.1778)  loss_scale: 32768.0000 (45275.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1154  max mem: 15572
Epoch: [6]  [ 850/1404]  eta: 0:05:45  lr: 0.000093  min_lr: 0.000001  loss: 4.3355 (4.3541)  class_acc: 0.2083 (0.1785)  loss_scale: 32768.0000 (45128.1974)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.1847  max mem: 15572
Epoch: [6]  [ 860/1404]  eta: 0:05:38  lr: 0.000093  min_lr: 0.000001  loss: 4.3547 (4.3548)  class_acc: 0.2083 (0.1784)  loss_scale: 32768.0000 (44984.6411)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0954  max mem: 15572
Epoch: [6]  [ 870/1404]  eta: 0:05:32  lr: 0.000093  min_lr: 0.000001  loss: 4.3429 (4.3537)  class_acc: 0.1667 (0.1786)  loss_scale: 32768.0000 (44844.3812)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1102  max mem: 15572
Epoch: [6]  [ 880/1404]  eta: 0:05:26  lr: 0.000093  min_lr: 0.000001  loss: 4.1603 (4.3529)  class_acc: 0.1667 (0.1788)  loss_scale: 32768.0000 (44707.3053)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.1625  max mem: 15572
Epoch: [6]  [ 890/1404]  eta: 0:05:20  lr: 0.000093  min_lr: 0.000001  loss: 4.1752 (4.3523)  class_acc: 0.1667 (0.1792)  loss_scale: 32768.0000 (44573.3064)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0671  max mem: 15572
Epoch: [6]  [ 900/1404]  eta: 0:05:14  lr: 0.000093  min_lr: 0.000001  loss: 4.2674 (4.3511)  class_acc: 0.2083 (0.1794)  loss_scale: 32768.0000 (44442.2819)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.0725  max mem: 15572
Epoch: [6]  [ 910/1404]  eta: 0:05:08  lr: 0.000093  min_lr: 0.000001  loss: 4.1575 (4.3489)  class_acc: 0.2083 (0.1801)  loss_scale: 32768.0000 (44314.1339)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.1697  max mem: 15572
Epoch: [6]  [ 920/1404]  eta: 0:05:01  lr: 0.000093  min_lr: 0.000001  loss: 4.1686 (4.3493)  class_acc: 0.1667 (0.1798)  loss_scale: 32768.0000 (44188.7687)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1399  max mem: 15572
Epoch: [6]  [ 930/1404]  eta: 0:04:55  lr: 0.000093  min_lr: 0.000001  loss: 4.4456 (4.3511)  class_acc: 0.1250 (0.1796)  loss_scale: 32768.0000 (44066.0967)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0654  max mem: 15572
Epoch: [6]  [ 940/1404]  eta: 0:04:49  lr: 0.000093  min_lr: 0.000001  loss: 4.4368 (4.3526)  class_acc: 0.1250 (0.1790)  loss_scale: 32768.0000 (43946.0319)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0854  max mem: 15572
Epoch: [6]  [ 950/1404]  eta: 0:04:43  lr: 0.000093  min_lr: 0.000001  loss: 4.3468 (4.3504)  class_acc: 0.1667 (0.1796)  loss_scale: 32768.0000 (43828.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1211  max mem: 15572
Epoch: [6]  [ 960/1404]  eta: 0:04:36  lr: 0.000093  min_lr: 0.000001  loss: 4.2782 (4.3511)  class_acc: 0.2083 (0.1795)  loss_scale: 32768.0000 (43713.3985)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.0592  max mem: 15572
[2025-01-17 09:35:54,021] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:35:54,021] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:35:54,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:35:54,052] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 970/1404]  eta: 0:04:29  lr: 0.000093  min_lr: 0.000001  loss: 4.4422 (4.3516)  class_acc: 0.1667 (0.1793)  loss_scale: 32768.0000 (43769.4089)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0193  max mem: 15572
Epoch: [6]  [ 980/1404]  eta: 0:04:23  lr: 0.000093  min_lr: 0.000001  loss: 4.4422 (4.3521)  class_acc: 0.1250 (0.1790)  loss_scale: 65536.0000 (43991.2905)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0322  max mem: 15572
Epoch: [6]  [ 990/1404]  eta: 0:04:16  lr: 0.000093  min_lr: 0.000001  loss: 4.3641 (4.3529)  class_acc: 0.1250 (0.1787)  loss_scale: 65536.0000 (44208.6942)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0253  max mem: 15572
[2025-01-17 09:36:09,690] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9418
[2025-01-17 09:36:09,691] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:36:09,691] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:36:09,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9418
[2025-01-17 09:36:09,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [1000/1404]  eta: 0:04:10  lr: 0.000093  min_lr: 0.000001  loss: 4.2622 (4.3528)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (44192.6074)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.0197  max mem: 15572
Epoch: [6]  [1010/1404]  eta: 0:04:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3366 (4.3525)  class_acc: 0.1667 (0.1792)  loss_scale: 32768.0000 (44079.6044)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0259  max mem: 15572
Epoch: [6]  [1020/1404]  eta: 0:03:57  lr: 0.000093  min_lr: 0.000001  loss: 4.3807 (4.3533)  class_acc: 0.1250 (0.1788)  loss_scale: 32768.0000 (43968.8149)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0189  max mem: 15572
Epoch: [6]  [1030/1404]  eta: 0:03:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2311 (4.3510)  class_acc: 0.1667 (0.1797)  loss_scale: 32768.0000 (43860.1746)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0011  max mem: 15572
Epoch: [6]  [1040/1404]  eta: 0:03:45  lr: 0.000093  min_lr: 0.000001  loss: 4.2102 (4.3512)  class_acc: 0.2083 (0.1798)  loss_scale: 32768.0000 (43753.6215)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0008  max mem: 15572
Epoch: [6]  [1050/1404]  eta: 0:03:39  lr: 0.000093  min_lr: 0.000001  loss: 4.3098 (4.3507)  class_acc: 0.1667 (0.1796)  loss_scale: 32768.0000 (43649.0961)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0009  max mem: 15572
Epoch: [6]  [1060/1404]  eta: 0:03:33  lr: 0.000093  min_lr: 0.000001  loss: 4.3786 (4.3525)  class_acc: 0.1667 (0.1793)  loss_scale: 32768.0000 (43546.5410)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0007  max mem: 15572
Epoch: [6]  [1070/1404]  eta: 0:03:27  lr: 0.000093  min_lr: 0.000001  loss: 4.4459 (4.3526)  class_acc: 0.1667 (0.1792)  loss_scale: 32768.0000 (43445.9010)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0006  max mem: 15572
Epoch: [6]  [1080/1404]  eta: 0:03:20  lr: 0.000093  min_lr: 0.000001  loss: 4.3270 (4.3509)  class_acc: 0.2083 (0.1797)  loss_scale: 32768.0000 (43347.1230)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0494  max mem: 15572
Epoch: [6]  [1090/1404]  eta: 0:03:14  lr: 0.000093  min_lr: 0.000001  loss: 4.2130 (4.3495)  class_acc: 0.2500 (0.1800)  loss_scale: 32768.0000 (43250.1558)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0997  max mem: 15572
Epoch: [6]  [1100/1404]  eta: 0:03:08  lr: 0.000093  min_lr: 0.000001  loss: 4.2918 (4.3497)  class_acc: 0.1667 (0.1798)  loss_scale: 32768.0000 (43154.9500)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0848  max mem: 15572
Epoch: [6]  [1110/1404]  eta: 0:03:02  lr: 0.000093  min_lr: 0.000001  loss: 4.4206 (4.3502)  class_acc: 0.1667 (0.1798)  loss_scale: 32768.0000 (43061.4581)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.1162  max mem: 15572
Epoch: [6]  [1120/1404]  eta: 0:02:56  lr: 0.000093  min_lr: 0.000001  loss: 4.3369 (4.3496)  class_acc: 0.1667 (0.1800)  loss_scale: 32768.0000 (42969.6343)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0972  max mem: 15572
[2025-01-17 09:37:29,387] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:37:29,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:37:29,389] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:37:29,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1130/1404]  eta: 0:02:50  lr: 0.000093  min_lr: 0.000001  loss: 4.2891 (4.3500)  class_acc: 0.1667 (0.1798)  loss_scale: 32768.0000 (43111.2149)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0818  max mem: 15572
Epoch: [6]  [1140/1404]  eta: 0:02:43  lr: 0.000093  min_lr: 0.000001  loss: 4.2867 (4.3499)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (43307.7511)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.0671  max mem: 15572
Epoch: [6]  [1150/1404]  eta: 0:02:37  lr: 0.000093  min_lr: 0.000001  loss: 4.3418 (4.3490)  class_acc: 0.1667 (0.1799)  loss_scale: 65536.0000 (43500.8723)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.0269  max mem: 15572
Epoch: [6]  [1160/1404]  eta: 0:02:31  lr: 0.000093  min_lr: 0.000001  loss: 4.2923 (4.3481)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0960  max mem: 15572
Epoch: [6]  [1170/1404]  eta: 0:02:24  lr: 0.000093  min_lr: 0.000001  loss: 4.2549 (4.3472)  class_acc: 0.2083 (0.1807)  loss_scale: 65536.0000 (43877.2195)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0916  max mem: 15572
Epoch: [6]  [1180/1404]  eta: 0:02:18  lr: 0.000093  min_lr: 0.000001  loss: 4.1355 (4.3458)  class_acc: 0.2083 (0.1808)  loss_scale: 65536.0000 (44060.6130)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0742  max mem: 15572
Epoch: [6]  [1190/1404]  eta: 0:02:12  lr: 0.000093  min_lr: 0.000001  loss: 4.2387 (4.3448)  class_acc: 0.1667 (0.1809)  loss_scale: 65536.0000 (44240.9270)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.0896  max mem: 15572
Epoch: [6]  [1200/1404]  eta: 0:02:06  lr: 0.000093  min_lr: 0.000001  loss: 4.2811 (4.3452)  class_acc: 0.2083 (0.1808)  loss_scale: 65536.0000 (44418.2381)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0897  max mem: 15572
Epoch: [6]  [1210/1404]  eta: 0:02:00  lr: 0.000093  min_lr: 0.000001  loss: 4.1144 (4.3433)  class_acc: 0.2083 (0.1812)  loss_scale: 65536.0000 (44592.6210)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.0943  max mem: 15572
Epoch: [6]  [1220/1404]  eta: 0:01:53  lr: 0.000093  min_lr: 0.000001  loss: 4.0655 (4.3420)  class_acc: 0.1667 (0.1811)  loss_scale: 65536.0000 (44764.1474)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.0704  max mem: 15572
[2025-01-17 09:38:30,211] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9648
[2025-01-17 09:38:30,211] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:38:30,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9648
[2025-01-17 09:38:30,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:38:30,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1230/1404]  eta: 0:01:47  lr: 0.000093  min_lr: 0.000001  loss: 4.2247 (4.3423)  class_acc: 0.1667 (0.1811)  loss_scale: 65536.0000 (44746.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.0526  max mem: 15572
Epoch: [6]  [1240/1404]  eta: 0:01:41  lr: 0.000093  min_lr: 0.000001  loss: 4.3434 (4.3427)  class_acc: 0.2083 (0.1813)  loss_scale: 32768.0000 (44650.0306)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.0568  max mem: 15572
Epoch: [6]  [1250/1404]  eta: 0:01:35  lr: 0.000093  min_lr: 0.000001  loss: 4.3816 (4.3437)  class_acc: 0.1667 (0.1810)  loss_scale: 32768.0000 (44555.0504)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0327  max mem: 15572
Epoch: [6]  [1260/1404]  eta: 0:01:29  lr: 0.000093  min_lr: 0.000001  loss: 4.4110 (4.3441)  class_acc: 0.1250 (0.1805)  loss_scale: 32768.0000 (44461.5765)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0005  max mem: 15572
Epoch: [6]  [1270/1404]  eta: 0:01:23  lr: 0.000093  min_lr: 0.000001  loss: 4.3984 (4.3440)  class_acc: 0.1250 (0.1806)  loss_scale: 32768.0000 (44369.5736)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0007  max mem: 15572
Epoch: [6]  [1280/1404]  eta: 0:01:16  lr: 0.000093  min_lr: 0.000001  loss: 4.3785 (4.3449)  class_acc: 0.1667 (0.1809)  loss_scale: 32768.0000 (44279.0070)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0111  max mem: 15572
Epoch: [6]  [1290/1404]  eta: 0:01:10  lr: 0.000093  min_lr: 0.000001  loss: 4.3865 (4.3449)  class_acc: 0.1667 (0.1809)  loss_scale: 32768.0000 (44189.8435)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0111  max mem: 15572
Epoch: [6]  [1300/1404]  eta: 0:01:04  lr: 0.000093  min_lr: 0.000001  loss: 4.3226 (4.3444)  class_acc: 0.1667 (0.1810)  loss_scale: 32768.0000 (44102.0507)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.0007  max mem: 15572
Epoch: [6]  [1310/1404]  eta: 0:00:58  lr: 0.000093  min_lr: 0.000001  loss: 4.2675 (4.3438)  class_acc: 0.2083 (0.1814)  loss_scale: 32768.0000 (44015.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0008  max mem: 15572
Epoch: [6]  [1320/1404]  eta: 0:00:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2652 (4.3427)  class_acc: 0.2083 (0.1816)  loss_scale: 32768.0000 (43930.4527)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0011  max mem: 15572
Epoch: [6]  [1330/1404]  eta: 0:00:45  lr: 0.000093  min_lr: 0.000001  loss: 4.1510 (4.3416)  class_acc: 0.2083 (0.1818)  loss_scale: 32768.0000 (43846.5875)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.0009  max mem: 15572
Epoch: [6]  [1340/1404]  eta: 0:00:39  lr: 0.000093  min_lr: 0.000001  loss: 4.0843 (4.3408)  class_acc: 0.2083 (0.1819)  loss_scale: 32768.0000 (43763.9732)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0012  max mem: 15572
Epoch: [6]  [1350/1404]  eta: 0:00:33  lr: 0.000093  min_lr: 0.000001  loss: 4.1796 (4.3399)  class_acc: 0.1667 (0.1822)  loss_scale: 32768.0000 (43682.5818)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0012  max mem: 15572
[2025-01-17 09:39:50,611] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:39:50,612] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:39:50,612] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:39:50,612] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1360/1404]  eta: 0:00:27  lr: 0.000093  min_lr: 0.000001  loss: 4.2751 (4.3411)  class_acc: 0.1667 (0.1820)  loss_scale: 32768.0000 (43794.9978)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0009  max mem: 15572
Epoch: [6]  [1370/1404]  eta: 0:00:21  lr: 0.000093  min_lr: 0.000001  loss: 4.4266 (4.3402)  class_acc: 0.1667 (0.1827)  loss_scale: 65536.0000 (43953.5755)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.0009  max mem: 15572
[2025-01-17 09:40:06,636] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9803
[2025-01-17 09:40:06,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:40:06,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9803
[2025-01-17 09:40:06,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:40:06,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1380/1404]  eta: 0:00:14  lr: 0.000093  min_lr: 0.000001  loss: 4.3366 (4.3403)  class_acc: 0.2083 (0.1829)  loss_scale: 65536.0000 (44062.4012)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.0008  max mem: 15572
Epoch: [6]  [1390/1404]  eta: 0:00:08  lr: 0.000093  min_lr: 0.000001  loss: 4.4059 (4.3407)  class_acc: 0.1667 (0.1827)  loss_scale: 32768.0000 (43981.2049)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0014  max mem: 15572
Epoch: [6]  [1400/1404]  eta: 0:00:02  lr: 0.000093  min_lr: 0.000001  loss: 4.3299 (4.3403)  class_acc: 0.1667 (0.1829)  loss_scale: 32768.0000 (43901.1677)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0011  max mem: 15572
Epoch: [6]  [1403/1404]  eta: 0:00:00  lr: 0.000093  min_lr: 0.000001  loss: 4.3299 (4.3404)  class_acc: 0.1667 (0.1830)  loss_scale: 32768.0000 (43877.3789)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0010  max mem: 15572
Epoch: [6] Total time: 0:14:26 (0.6170 s / it)
Averaged stats: lr: 0.000093  min_lr: 0.000001  loss: 4.3299 (4.3338)  class_acc: 0.1667 (0.1847)  loss_scale: 32768.0000 (43877.3789)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:02  loss: 2.1956 (2.1956)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 6.1953  data: 5.9909  max mem: 15572
Val:  [ 10/136]  eta: 0:01:39  loss: 3.4819 (3.3409)  acc1: 5.5556 (22.7273)  acc5: 50.0000 (44.4444)  time: 0.7861  data: 0.5940  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 3.4774 (3.3976)  acc1: 5.5556 (19.0476)  acc5: 50.0000 (48.4127)  time: 0.2903  data: 0.0928  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 3.2004 (3.1361)  acc1: 22.2222 (28.6738)  acc5: 61.1111 (54.6595)  time: 0.3456  data: 0.1433  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.3496 (3.0356)  acc1: 38.8889 (29.6748)  acc5: 77.7778 (59.4851)  time: 0.3436  data: 0.1384  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 3.1411 (3.1563)  acc1: 16.6667 (26.9063)  acc5: 66.6667 (57.9521)  time: 0.3435  data: 0.1371  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 3.5739 (3.2648)  acc1: 5.5556 (24.1348)  acc5: 50.0000 (55.0091)  time: 0.3627  data: 0.1660  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 3.5691 (3.2151)  acc1: 11.1111 (26.6823)  acc5: 50.0000 (55.5556)  time: 0.3713  data: 0.1775  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.9842 (3.1861)  acc1: 38.8889 (27.8464)  acc5: 66.6667 (57.3388)  time: 0.3617  data: 0.1573  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 3.0133 (3.1971)  acc1: 27.7778 (27.1673)  acc5: 72.2222 (57.6313)  time: 0.3503  data: 0.1485  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.4623 (3.2498)  acc1: 11.1111 (25.7426)  acc5: 55.5556 (56.1056)  time: 0.3640  data: 0.1701  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.2590 (3.2407)  acc1: 27.7778 (26.5766)  acc5: 50.0000 (56.4064)  time: 0.3846  data: 0.1761  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.9097 (3.1731)  acc1: 33.3333 (28.8338)  acc5: 72.2222 (58.9073)  time: 0.4069  data: 0.1925  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4224 (3.1346)  acc1: 55.5556 (30.5768)  acc5: 83.3333 (59.5420)  time: 0.3189  data: 0.1433  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.7882 (3.1433)  acc1: 38.8889 (30.2621)  acc5: 77.7778 (59.4185)  time: 0.1897  data: 0.0360  max mem: 15572
Val: Total time: 0:00:51 (0.3811 s / it)
* Acc@1 30.016 Acc@5 58.006 loss 3.171
Accuracy of the network on the 4883 val videos: 30.0%
[2025-01-17 09:41:10,121] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 09:41:10,123] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 09:41:10,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 09:41:10,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 09:41:13,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 09:41:13,077] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.02%
Epoch: [7]  [   0/1404]  eta: 2:34:24  lr: 0.000093  min_lr: 0.000001  loss: 4.1482 (4.1482)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.5987  data: 6.0945  max mem: 15572
Epoch: [7]  [  10/1404]  eta: 0:26:09  lr: 0.000093  min_lr: 0.000001  loss: 4.1266 (4.1463)  class_acc: 0.1667 (0.2121)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1262  data: 0.6221  max mem: 15572
Epoch: [7]  [  20/1404]  eta: 0:20:24  lr: 0.000093  min_lr: 0.000001  loss: 4.1265 (4.1478)  class_acc: 0.1667 (0.2024)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0732  max mem: 15572
Epoch: [7]  [  30/1404]  eta: 0:18:33  lr: 0.000093  min_lr: 0.000001  loss: 4.1265 (4.1439)  class_acc: 0.2083 (0.2003)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.0361  max mem: 15572
Epoch: [7]  [  40/1404]  eta: 0:17:05  lr: 0.000093  min_lr: 0.000001  loss: 4.2873 (4.1834)  class_acc: 0.1667 (0.1961)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0364  max mem: 15572
Epoch: [7]  [  50/1404]  eta: 0:16:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3152 (4.2138)  class_acc: 0.1667 (0.1969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0364  max mem: 15572
Epoch: [7]  [  60/1404]  eta: 0:15:49  lr: 0.000093  min_lr: 0.000001  loss: 4.2911 (4.2181)  class_acc: 0.2083 (0.2008)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0008  max mem: 15572
Epoch: [7]  [  70/1404]  eta: 0:15:24  lr: 0.000093  min_lr: 0.000001  loss: 4.1460 (4.1975)  class_acc: 0.2500 (0.2089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0007  max mem: 15572
Epoch: [7]  [  80/1404]  eta: 0:15:04  lr: 0.000093  min_lr: 0.000001  loss: 4.1545 (4.2150)  class_acc: 0.2083 (0.2027)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0006  max mem: 15572
Epoch: [7]  [  90/1404]  eta: 0:14:55  lr: 0.000093  min_lr: 0.000001  loss: 4.2386 (4.2205)  class_acc: 0.1667 (0.2042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.0007  max mem: 15572
Epoch: [7]  [ 100/1404]  eta: 0:14:38  lr: 0.000093  min_lr: 0.000001  loss: 4.2925 (4.2340)  class_acc: 0.1667 (0.1984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.0182  max mem: 15572
[2025-01-17 09:42:23,504] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:42:23,505] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:42:23,506] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:42:23,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 110/1404]  eta: 0:14:19  lr: 0.000093  min_lr: 0.000001  loss: 4.2896 (4.2408)  class_acc: 0.1667 (0.1986)  loss_scale: 32768.0000 (34834.4505)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.0387  max mem: 15572
Epoch: [7]  [ 120/1404]  eta: 0:14:09  lr: 0.000093  min_lr: 0.000001  loss: 4.3576 (4.2584)  class_acc: 0.1667 (0.1959)  loss_scale: 65536.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0543  max mem: 15572
Epoch: [7]  [ 130/1404]  eta: 0:13:55  lr: 0.000093  min_lr: 0.000001  loss: 4.3205 (4.2491)  class_acc: 0.1667 (0.2004)  loss_scale: 65536.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0338  max mem: 15572
Epoch: [7]  [ 140/1404]  eta: 0:13:50  lr: 0.000093  min_lr: 0.000001  loss: 4.2336 (4.2551)  class_acc: 0.2083 (0.1998)  loss_scale: 65536.0000 (41366.6950)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0586  max mem: 15572
Epoch: [7]  [ 150/1404]  eta: 0:13:45  lr: 0.000093  min_lr: 0.000001  loss: 4.2522 (4.2521)  class_acc: 0.2083 (0.2017)  loss_scale: 65536.0000 (42967.3113)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.1051  max mem: 15572
Epoch: [7]  [ 160/1404]  eta: 0:13:30  lr: 0.000093  min_lr: 0.000001  loss: 4.2930 (4.2557)  class_acc: 0.2083 (0.2013)  loss_scale: 65536.0000 (44369.0932)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0472  max mem: 15572
Epoch: [7]  [ 170/1404]  eta: 0:13:13  lr: 0.000093  min_lr: 0.000001  loss: 4.2320 (4.2491)  class_acc: 0.1667 (0.2030)  loss_scale: 65536.0000 (45606.9240)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0082  max mem: 15572
[2025-01-17 09:43:03,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=50, lr=[9.001307943001361e-07, 9.001307943001361e-07, 1.2859011347144805e-06, 1.2859011347144805e-06, 1.8370016210206865e-06, 1.8370016210206865e-06, 2.624288030029552e-06, 2.624288030029552e-06, 3.7489829000422176e-06, 3.7489829000422176e-06, 5.355689857203168e-06, 5.355689857203168e-06, 7.650985510290241e-06, 7.650985510290241e-06, 1.092997930041463e-05, 1.092997930041463e-05, 1.561425614344947e-05, 1.561425614344947e-05, 2.230608020492782e-05, 2.230608020492782e-05, 3.18658288641826e-05, 3.18658288641826e-05, 4.552261266311801e-05, 4.552261266311801e-05, 6.50323038044543e-05, 6.50323038044543e-05, 9.290329114922043e-05, 9.290329114922043e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 09:43:03,736] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=44.74101999426358, CurrSamplesPerSec=50.24113858883222, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 180/1404]  eta: 0:13:07  lr: 0.000093  min_lr: 0.000001  loss: 4.2027 (4.2475)  class_acc: 0.2083 (0.2053)  loss_scale: 65536.0000 (46707.9779)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0871  max mem: 15572
Epoch: [7]  [ 190/1404]  eta: 0:13:05  lr: 0.000093  min_lr: 0.000001  loss: 4.3119 (4.2566)  class_acc: 0.2083 (0.2031)  loss_scale: 65536.0000 (47693.7382)  weight_decay: 0.0500 (0.0500)  time: 0.6792  data: 0.1828  max mem: 15572
Epoch: [7]  [ 200/1404]  eta: 0:12:54  lr: 0.000093  min_lr: 0.000001  loss: 4.3788 (4.2607)  class_acc: 0.1667 (0.2011)  loss_scale: 65536.0000 (48581.4129)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1282  max mem: 15572
Epoch: [7]  [ 210/1404]  eta: 0:12:54  lr: 0.000093  min_lr: 0.000001  loss: 4.3681 (4.2646)  class_acc: 0.1250 (0.1994)  loss_scale: 65536.0000 (49384.9479)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.1047  max mem: 15572
Epoch: [7]  [ 220/1404]  eta: 0:12:43  lr: 0.000093  min_lr: 0.000001  loss: 4.3037 (4.2602)  class_acc: 0.1250 (0.2008)  loss_scale: 65536.0000 (50115.7647)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.1075  max mem: 15572
Epoch: [7]  [ 230/1404]  eta: 0:12:34  lr: 0.000093  min_lr: 0.000001  loss: 4.3207 (4.2644)  class_acc: 0.1250 (0.2002)  loss_scale: 65536.0000 (50783.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0601  max mem: 15572
[2025-01-17 09:43:42,653] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:43:42,654] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:43:42,677] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:43:42,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-17 09:43:43,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10061
[2025-01-17 09:43:43,428] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-17 09:43:43,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-17 09:43:43,430] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10061
[2025-01-17 09:43:43,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [7]  [ 240/1404]  eta: 0:12:27  lr: 0.000093  min_lr: 0.000001  loss: 4.3346 (4.2652)  class_acc: 0.1250 (0.2012)  loss_scale: 65536.0000 (51667.3859)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0891  max mem: 15572
Epoch: [7]  [ 250/1404]  eta: 0:12:16  lr: 0.000093  min_lr: 0.000001  loss: 4.3346 (4.2665)  class_acc: 0.2083 (0.2032)  loss_scale: 65536.0000 (52219.9203)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0830  max mem: 15572
Epoch: [7]  [ 260/1404]  eta: 0:12:12  lr: 0.000093  min_lr: 0.000001  loss: 4.2480 (4.2639)  class_acc: 0.2083 (0.2018)  loss_scale: 65536.0000 (52730.1149)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0270  max mem: 15572
Epoch: [7]  [ 270/1404]  eta: 0:12:09  lr: 0.000093  min_lr: 0.000001  loss: 4.2409 (4.2654)  class_acc: 0.1667 (0.2014)  loss_scale: 65536.0000 (53202.6568)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.0008  max mem: 15572
Epoch: [7]  [ 280/1404]  eta: 0:11:58  lr: 0.000093  min_lr: 0.000001  loss: 4.2409 (4.2641)  class_acc: 0.2500 (0.2033)  loss_scale: 65536.0000 (53641.5658)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.0008  max mem: 15572
Epoch: [7]  [ 290/1404]  eta: 0:11:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2745 (4.2650)  class_acc: 0.1667 (0.2029)  loss_scale: 65536.0000 (54050.3093)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0009  max mem: 15572
Epoch: [7]  [ 300/1404]  eta: 0:11:44  lr: 0.000093  min_lr: 0.000001  loss: 4.2999 (4.2646)  class_acc: 0.1667 (0.2029)  loss_scale: 65536.0000 (54431.8937)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0009  max mem: 15572
Epoch: [7]  [ 310/1404]  eta: 0:11:35  lr: 0.000093  min_lr: 0.000001  loss: 4.4087 (4.2677)  class_acc: 0.2083 (0.2032)  loss_scale: 65536.0000 (54788.9389)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0007  max mem: 15572
Epoch: [7]  [ 320/1404]  eta: 0:11:29  lr: 0.000093  min_lr: 0.000001  loss: 4.3542 (4.2655)  class_acc: 0.1667 (0.2028)  loss_scale: 65536.0000 (55123.7383)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0007  max mem: 15572
Epoch: [7]  [ 330/1404]  eta: 0:11:22  lr: 0.000093  min_lr: 0.000001  loss: 4.3001 (4.2617)  class_acc: 0.2083 (0.2048)  loss_scale: 65536.0000 (55438.3082)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0007  max mem: 15572
Epoch: [7]  [ 340/1404]  eta: 0:11:16  lr: 0.000093  min_lr: 0.000001  loss: 4.2911 (4.2635)  class_acc: 0.1667 (0.2037)  loss_scale: 65536.0000 (55734.4282)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0007  max mem: 15572
[2025-01-17 09:44:50,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10169
[2025-01-17 09:44:50,716] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:44:50,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:44:50,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10169
[2025-01-17 09:44:50,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 350/1404]  eta: 0:11:08  lr: 0.000093  min_lr: 0.000001  loss: 4.2911 (4.2667)  class_acc: 0.1250 (0.2030)  loss_scale: 32768.0000 (55080.1140)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0006  max mem: 15572
Epoch: [7]  [ 360/1404]  eta: 0:10:59  lr: 0.000093  min_lr: 0.000001  loss: 4.2641 (4.2654)  class_acc: 0.1667 (0.2036)  loss_scale: 32768.0000 (54462.0499)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0006  max mem: 15572
Epoch: [7]  [ 370/1404]  eta: 0:10:52  lr: 0.000093  min_lr: 0.000001  loss: 4.3116 (4.2730)  class_acc: 0.1250 (0.2007)  loss_scale: 32768.0000 (53877.3046)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0007  max mem: 15572
Epoch: [7]  [ 380/1404]  eta: 0:10:44  lr: 0.000093  min_lr: 0.000001  loss: 4.4332 (4.2747)  class_acc: 0.1250 (0.2007)  loss_scale: 32768.0000 (53323.2546)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.0007  max mem: 15572
Epoch: [7]  [ 390/1404]  eta: 0:10:36  lr: 0.000093  min_lr: 0.000001  loss: 4.2165 (4.2727)  class_acc: 0.2083 (0.2011)  loss_scale: 32768.0000 (52797.5448)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0008  max mem: 15572
Epoch: [7]  [ 400/1404]  eta: 0:10:30  lr: 0.000093  min_lr: 0.000001  loss: 4.1143 (4.2697)  class_acc: 0.2083 (0.2025)  loss_scale: 32768.0000 (52298.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0009  max mem: 15572
Epoch: [7]  [ 410/1404]  eta: 0:10:22  lr: 0.000093  min_lr: 0.000001  loss: 4.1064 (4.2681)  class_acc: 0.2083 (0.2036)  loss_scale: 32768.0000 (51822.8710)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0008  max mem: 15572
Epoch: [7]  [ 420/1404]  eta: 0:10:17  lr: 0.000093  min_lr: 0.000001  loss: 4.2904 (4.2695)  class_acc: 0.2083 (0.2035)  loss_scale: 32768.0000 (51370.2613)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0009  max mem: 15572
Epoch: [7]  [ 430/1404]  eta: 0:10:10  lr: 0.000093  min_lr: 0.000001  loss: 4.4863 (4.2745)  class_acc: 0.1667 (0.2025)  loss_scale: 32768.0000 (50938.6543)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0008  max mem: 15572
Epoch: [7]  [ 440/1404]  eta: 0:10:04  lr: 0.000093  min_lr: 0.000001  loss: 4.4030 (4.2753)  class_acc: 0.1667 (0.2029)  loss_scale: 32768.0000 (50526.6213)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0392  max mem: 15572
Epoch: [7]  [ 450/1404]  eta: 0:09:56  lr: 0.000093  min_lr: 0.000001  loss: 4.2625 (4.2752)  class_acc: 0.2083 (0.2024)  loss_scale: 32768.0000 (50132.8603)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.0716  max mem: 15572
Epoch: [7]  [ 460/1404]  eta: 0:09:51  lr: 0.000093  min_lr: 0.000001  loss: 4.2879 (4.2770)  class_acc: 0.2083 (0.2031)  loss_scale: 32768.0000 (49756.1822)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1293  max mem: 15572
[2025-01-17 09:46:08,477] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:46:08,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 470/1404]  eta: 0:09:45  lr: 0.000093  min_lr: 0.000001  loss: 4.2982 (4.2760)  class_acc: 0.2083 (0.2028)  loss_scale: 32768.0000 (49465.0701)  weight_decay: 0.0500 (0.0500)  time: 0.6649  data: 0.1432  max mem: 15572
[2025-01-17 09:46:08,489] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:46:08,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:46:11,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10301
[2025-01-17 09:46:11,515] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10301
[2025-01-17 09:46:11,516] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:46:11,516] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:46:11,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 480/1404]  eta: 0:09:39  lr: 0.000093  min_lr: 0.000001  loss: 4.3036 (4.2760)  class_acc: 0.1250 (0.2018)  loss_scale: 32768.0000 (49254.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1269  max mem: 15572
Epoch: [7]  [ 490/1404]  eta: 0:09:31  lr: 0.000093  min_lr: 0.000001  loss: 4.0996 (4.2690)  class_acc: 0.2083 (0.2032)  loss_scale: 32768.0000 (48918.4196)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0852  max mem: 15572
Epoch: [7]  [ 500/1404]  eta: 0:09:25  lr: 0.000093  min_lr: 0.000001  loss: 4.0794 (4.2680)  class_acc: 0.2500 (0.2033)  loss_scale: 32768.0000 (48596.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0556  max mem: 15572
Epoch: [7]  [ 510/1404]  eta: 0:09:20  lr: 0.000093  min_lr: 0.000001  loss: 4.2582 (4.2694)  class_acc: 0.2083 (0.2030)  loss_scale: 32768.0000 (48286.3092)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.1770  max mem: 15572
Epoch: [7]  [ 520/1404]  eta: 0:09:13  lr: 0.000093  min_lr: 0.000001  loss: 4.3900 (4.2719)  class_acc: 0.1667 (0.2022)  loss_scale: 32768.0000 (47988.4530)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1365  max mem: 15572
Epoch: [7]  [ 530/1404]  eta: 0:09:06  lr: 0.000093  min_lr: 0.000001  loss: 4.3434 (4.2711)  class_acc: 0.1250 (0.2019)  loss_scale: 32768.0000 (47701.8154)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0378  max mem: 15572
Epoch: [7]  [ 540/1404]  eta: 0:08:59  lr: 0.000093  min_lr: 0.000001  loss: 4.2395 (4.2702)  class_acc: 0.2083 (0.2030)  loss_scale: 32768.0000 (47425.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.0918  max mem: 15572
Epoch: [7]  [ 550/1404]  eta: 0:08:54  lr: 0.000093  min_lr: 0.000001  loss: 4.2122 (4.2663)  class_acc: 0.2500 (0.2043)  loss_scale: 32768.0000 (47159.7532)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.1492  max mem: 15572
Epoch: [7]  [ 560/1404]  eta: 0:08:47  lr: 0.000093  min_lr: 0.000001  loss: 4.2321 (4.2671)  class_acc: 0.2083 (0.2031)  loss_scale: 32768.0000 (46903.2157)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1267  max mem: 15572
Epoch: [7]  [ 570/1404]  eta: 0:08:42  lr: 0.000093  min_lr: 0.000001  loss: 4.2447 (4.2659)  class_acc: 0.1250 (0.2032)  loss_scale: 32768.0000 (46655.6637)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.1302  max mem: 15572
Epoch: [7]  [ 580/1404]  eta: 0:08:36  lr: 0.000093  min_lr: 0.000001  loss: 4.1042 (4.2618)  class_acc: 0.2083 (0.2039)  loss_scale: 32768.0000 (46416.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.1594  max mem: 15572
Epoch: [7]  [ 590/1404]  eta: 0:08:29  lr: 0.000093  min_lr: 0.000001  loss: 4.1732 (4.2608)  class_acc: 0.1667 (0.2038)  loss_scale: 32768.0000 (46185.6920)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1018  max mem: 15572
Epoch: [7]  [ 600/1404]  eta: 0:08:22  lr: 0.000093  min_lr: 0.000001  loss: 4.2731 (4.2620)  class_acc: 0.1667 (0.2035)  loss_scale: 32768.0000 (45962.4359)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.0689  max mem: 15572
[2025-01-17 09:47:29,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:47:29,875] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:47:29,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:47:29,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 610/1404]  eta: 0:08:15  lr: 0.000093  min_lr: 0.000001  loss: 4.1912 (4.2608)  class_acc: 0.2500 (0.2036)  loss_scale: 32768.0000 (46229.1588)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.0677  max mem: 15572
[2025-01-17 09:47:35,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10439
[2025-01-17 09:47:35,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:47:35,227] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10439
[2025-01-17 09:47:35,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:47:35,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 620/1404]  eta: 0:08:09  lr: 0.000093  min_lr: 0.000001  loss: 4.2697 (4.2642)  class_acc: 0.2083 (0.2030)  loss_scale: 32768.0000 (46012.3929)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1015  max mem: 15572
Epoch: [7]  [ 630/1404]  eta: 0:08:02  lr: 0.000093  min_lr: 0.000001  loss: 4.3322 (4.2656)  class_acc: 0.2083 (0.2029)  loss_scale: 32768.0000 (45802.4976)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1250  max mem: 15572
Epoch: [7]  [ 640/1404]  eta: 0:07:56  lr: 0.000093  min_lr: 0.000001  loss: 4.2875 (4.2646)  class_acc: 0.2083 (0.2038)  loss_scale: 32768.0000 (45599.1513)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.0805  max mem: 15572
Epoch: [7]  [ 650/1404]  eta: 0:07:48  lr: 0.000093  min_lr: 0.000001  loss: 4.3220 (4.2663)  class_acc: 0.2083 (0.2037)  loss_scale: 32768.0000 (45402.0522)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.0365  max mem: 15572
Epoch: [7]  [ 660/1404]  eta: 0:07:43  lr: 0.000093  min_lr: 0.000001  loss: 4.1103 (4.2645)  class_acc: 0.1667 (0.2036)  loss_scale: 32768.0000 (45210.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0632  max mem: 15572
Epoch: [7]  [ 670/1404]  eta: 0:07:36  lr: 0.000093  min_lr: 0.000001  loss: 4.0755 (4.2627)  class_acc: 0.2500 (0.2044)  loss_scale: 32768.0000 (45025.4784)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.0797  max mem: 15572
Epoch: [7]  [ 680/1404]  eta: 0:07:30  lr: 0.000093  min_lr: 0.000001  loss: 4.3730 (4.2655)  class_acc: 0.2500 (0.2041)  loss_scale: 32768.0000 (44845.4860)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0873  max mem: 15572
Epoch: [7]  [ 690/1404]  eta: 0:07:24  lr: 0.000093  min_lr: 0.000001  loss: 4.3487 (4.2655)  class_acc: 0.1667 (0.2036)  loss_scale: 32768.0000 (44670.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.1274  max mem: 15572
Epoch: [7]  [ 700/1404]  eta: 0:07:17  lr: 0.000093  min_lr: 0.000001  loss: 4.2924 (4.2666)  class_acc: 0.1667 (0.2033)  loss_scale: 32768.0000 (44500.9073)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0920  max mem: 15572
Epoch: [7]  [ 710/1404]  eta: 0:07:12  lr: 0.000093  min_lr: 0.000001  loss: 4.2830 (4.2651)  class_acc: 0.2083 (0.2039)  loss_scale: 32768.0000 (44335.8875)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1213  max mem: 15572
Epoch: [7]  [ 720/1404]  eta: 0:07:06  lr: 0.000093  min_lr: 0.000001  loss: 4.1780 (4.2647)  class_acc: 0.2083 (0.2041)  loss_scale: 32768.0000 (44175.4452)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.1606  max mem: 15572
Epoch: [7]  [ 730/1404]  eta: 0:07:01  lr: 0.000093  min_lr: 0.000001  loss: 4.2919 (4.2652)  class_acc: 0.2083 (0.2043)  loss_scale: 32768.0000 (44019.3926)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.1825  max mem: 15572
[2025-01-17 09:48:57,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:48:57,400] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 740/1404]  eta: 0:06:55  lr: 0.000093  min_lr: 0.000001  loss: 4.2712 (4.2625)  class_acc: 0.2083 (0.2041)  loss_scale: 32768.0000 (43911.7733)  weight_decay: 0.0500 (0.0500)  time: 0.7354  data: 0.2334  max mem: 15572
[2025-01-17 09:48:57,428] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:48:57,428] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 750/1404]  eta: 0:06:49  lr: 0.000093  min_lr: 0.000001  loss: 4.2248 (4.2649)  class_acc: 0.2083 (0.2044)  loss_scale: 65536.0000 (44199.7124)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1555  max mem: 15572
[2025-01-17 09:49:03,657] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10579
[2025-01-17 09:49:03,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:49:03,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:49:03,657] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10579
[2025-01-17 09:49:03,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [ 760/1404]  eta: 0:06:42  lr: 0.000093  min_lr: 0.000001  loss: 4.2472 (4.2645)  class_acc: 0.2083 (0.2044)  loss_scale: 32768.0000 (44049.4928)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1075  max mem: 15572
Epoch: [7]  [ 770/1404]  eta: 0:06:37  lr: 0.000093  min_lr: 0.000001  loss: 4.2434 (4.2639)  class_acc: 0.1667 (0.2041)  loss_scale: 32768.0000 (43903.1699)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.1602  max mem: 15572
Epoch: [7]  [ 780/1404]  eta: 0:06:30  lr: 0.000093  min_lr: 0.000001  loss: 4.3055 (4.2666)  class_acc: 0.1667 (0.2027)  loss_scale: 32768.0000 (43760.5941)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1407  max mem: 15572
Epoch: [7]  [ 790/1404]  eta: 0:06:24  lr: 0.000093  min_lr: 0.000001  loss: 4.3482 (4.2651)  class_acc: 0.1250 (0.2030)  loss_scale: 32768.0000 (43621.6233)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1013  max mem: 15572
Epoch: [7]  [ 800/1404]  eta: 0:06:17  lr: 0.000093  min_lr: 0.000001  loss: 4.1875 (4.2655)  class_acc: 0.1667 (0.2027)  loss_scale: 32768.0000 (43486.1223)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0798  max mem: 15572
Epoch: [7]  [ 810/1404]  eta: 0:06:12  lr: 0.000093  min_lr: 0.000001  loss: 4.2888 (4.2671)  class_acc: 0.1667 (0.2025)  loss_scale: 32768.0000 (43353.9630)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.1440  max mem: 15572
Epoch: [7]  [ 820/1404]  eta: 0:06:05  lr: 0.000092  min_lr: 0.000001  loss: 4.2888 (4.2647)  class_acc: 0.1667 (0.2031)  loss_scale: 32768.0000 (43225.0231)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1456  max mem: 15572
Epoch: [7]  [ 830/1404]  eta: 0:05:59  lr: 0.000092  min_lr: 0.000001  loss: 4.0969 (4.2623)  class_acc: 0.2500 (0.2040)  loss_scale: 32768.0000 (43099.1865)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1130  max mem: 15572
Epoch: [7]  [ 840/1404]  eta: 0:05:53  lr: 0.000092  min_lr: 0.000001  loss: 4.1445 (4.2607)  class_acc: 0.2083 (0.2043)  loss_scale: 32768.0000 (42976.3424)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1088  max mem: 15572
Epoch: [7]  [ 850/1404]  eta: 0:05:46  lr: 0.000092  min_lr: 0.000001  loss: 4.1945 (4.2604)  class_acc: 0.1667 (0.2040)  loss_scale: 32768.0000 (42856.3854)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0836  max mem: 15572
Epoch: [7]  [ 860/1404]  eta: 0:05:40  lr: 0.000092  min_lr: 0.000001  loss: 4.3433 (4.2620)  class_acc: 0.1667 (0.2039)  loss_scale: 32768.0000 (42739.2149)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0758  max mem: 15572
Epoch: [7]  [ 870/1404]  eta: 0:05:34  lr: 0.000092  min_lr: 0.000001  loss: 4.3304 (4.2624)  class_acc: 0.2083 (0.2041)  loss_scale: 32768.0000 (42624.7348)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0868  max mem: 15572
[2025-01-17 09:50:25,316] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:50:25,316] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:50:25,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:50:25,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 880/1404]  eta: 0:05:28  lr: 0.000092  min_lr: 0.000001  loss: 4.1876 (4.2620)  class_acc: 0.2083 (0.2042)  loss_scale: 32768.0000 (42550.0477)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.1295  max mem: 15572
Epoch: [7]  [ 890/1404]  eta: 0:05:21  lr: 0.000092  min_lr: 0.000001  loss: 4.2105 (4.2618)  class_acc: 0.2083 (0.2045)  loss_scale: 65536.0000 (42808.0269)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0657  max mem: 15572
[2025-01-17 09:50:32,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10721
[2025-01-17 09:50:32,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:50:32,475] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10721
[2025-01-17 09:50:32,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:50:32,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 900/1404]  eta: 0:05:14  lr: 0.000092  min_lr: 0.000001  loss: 4.2398 (4.2625)  class_acc: 0.1667 (0.2040)  loss_scale: 65536.0000 (42769.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0066  max mem: 15572
Epoch: [7]  [ 910/1404]  eta: 0:05:08  lr: 0.000092  min_lr: 0.000001  loss: 4.2398 (4.2612)  class_acc: 0.1667 (0.2050)  loss_scale: 32768.0000 (42659.5477)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0283  max mem: 15572
Epoch: [7]  [ 920/1404]  eta: 0:05:01  lr: 0.000092  min_lr: 0.000001  loss: 4.3338 (4.2620)  class_acc: 0.2500 (0.2049)  loss_scale: 32768.0000 (42552.1477)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0615  max mem: 15572
Epoch: [7]  [ 930/1404]  eta: 0:04:56  lr: 0.000092  min_lr: 0.000001  loss: 4.3735 (4.2618)  class_acc: 0.2083 (0.2050)  loss_scale: 32768.0000 (42447.0548)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1193  max mem: 15572
Epoch: [7]  [ 940/1404]  eta: 0:04:49  lr: 0.000092  min_lr: 0.000001  loss: 4.2019 (4.2601)  class_acc: 0.2083 (0.2052)  loss_scale: 32768.0000 (42344.1955)  weight_decay: 0.0500 (0.0500)  time: 0.6680  data: 0.1349  max mem: 15572
Epoch: [7]  [ 950/1404]  eta: 0:04:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1877 (4.2600)  class_acc: 0.2500 (0.2053)  loss_scale: 32768.0000 (42243.4995)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0498  max mem: 15572
Epoch: [7]  [ 960/1404]  eta: 0:04:37  lr: 0.000092  min_lr: 0.000001  loss: 4.1884 (4.2593)  class_acc: 0.2500 (0.2056)  loss_scale: 32768.0000 (42144.8991)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1026  max mem: 15572
Epoch: [7]  [ 970/1404]  eta: 0:04:31  lr: 0.000092  min_lr: 0.000001  loss: 4.1137 (4.2583)  class_acc: 0.2500 (0.2058)  loss_scale: 32768.0000 (42048.3296)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.1714  max mem: 15572
Epoch: [7]  [ 980/1404]  eta: 0:04:25  lr: 0.000092  min_lr: 0.000001  loss: 4.1945 (4.2589)  class_acc: 0.1667 (0.2057)  loss_scale: 32768.0000 (41953.7288)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1401  max mem: 15572
Epoch: [7]  [ 990/1404]  eta: 0:04:18  lr: 0.000092  min_lr: 0.000001  loss: 4.3183 (4.2585)  class_acc: 0.1667 (0.2055)  loss_scale: 32768.0000 (41861.0373)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1107  max mem: 15572
Epoch: [7]  [1000/1404]  eta: 0:04:12  lr: 0.000092  min_lr: 0.000001  loss: 4.0127 (4.2558)  class_acc: 0.2083 (0.2061)  loss_scale: 32768.0000 (41770.1978)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0568  max mem: 15572
Epoch: [7]  [1010/1404]  eta: 0:04:05  lr: 0.000092  min_lr: 0.000001  loss: 4.2903 (4.2577)  class_acc: 0.2083 (0.2060)  loss_scale: 32768.0000 (41681.1553)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0454  max mem: 15572
Epoch: [7]  [1020/1404]  eta: 0:03:59  lr: 0.000092  min_lr: 0.000001  loss: 4.3766 (4.2573)  class_acc: 0.2083 (0.2059)  loss_scale: 32768.0000 (41593.8570)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0699  max mem: 15572
[2025-01-17 09:51:51,926] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:51:51,926] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:51:51,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:51:51,926] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1030/1404]  eta: 0:03:53  lr: 0.000092  min_lr: 0.000001  loss: 4.2995 (4.2574)  class_acc: 0.1667 (0.2053)  loss_scale: 32768.0000 (41794.2968)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0729  max mem: 15572
Epoch: [7]  [1040/1404]  eta: 0:03:47  lr: 0.000092  min_lr: 0.000001  loss: 4.3946 (4.2585)  class_acc: 0.1667 (0.2051)  loss_scale: 65536.0000 (42022.3631)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0829  max mem: 15572
Epoch: [7]  [1050/1404]  eta: 0:03:41  lr: 0.000092  min_lr: 0.000001  loss: 4.3109 (4.2594)  class_acc: 0.1667 (0.2052)  loss_scale: 65536.0000 (42246.0894)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.1439  max mem: 15572
Epoch: [7]  [1060/1404]  eta: 0:03:34  lr: 0.000092  min_lr: 0.000001  loss: 4.2661 (4.2586)  class_acc: 0.1667 (0.2053)  loss_scale: 65536.0000 (42465.5985)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1049  max mem: 15572
[2025-01-17 09:52:16,890] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10890
[2025-01-17 09:52:16,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:52:16,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:52:16,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10890
[2025-01-17 09:52:16,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [1070/1404]  eta: 0:03:28  lr: 0.000092  min_lr: 0.000001  loss: 4.0903 (4.2584)  class_acc: 0.2083 (0.2053)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.0775  max mem: 15572
Epoch: [7]  [1080/1404]  eta: 0:03:22  lr: 0.000092  min_lr: 0.000001  loss: 4.3192 (4.2590)  class_acc: 0.1667 (0.2050)  loss_scale: 32768.0000 (42316.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.1515  max mem: 15572
Epoch: [7]  [1090/1404]  eta: 0:03:16  lr: 0.000092  min_lr: 0.000001  loss: 4.2130 (4.2573)  class_acc: 0.1667 (0.2053)  loss_scale: 32768.0000 (42228.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1370  max mem: 15572
Epoch: [7]  [1100/1404]  eta: 0:03:09  lr: 0.000092  min_lr: 0.000001  loss: 4.1084 (4.2576)  class_acc: 0.1667 (0.2047)  loss_scale: 32768.0000 (42143.0409)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1048  max mem: 15572
Epoch: [7]  [1110/1404]  eta: 0:03:03  lr: 0.000092  min_lr: 0.000001  loss: 4.1760 (4.2564)  class_acc: 0.1667 (0.2048)  loss_scale: 32768.0000 (42058.6571)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1084  max mem: 15572
Epoch: [7]  [1120/1404]  eta: 0:02:57  lr: 0.000092  min_lr: 0.000001  loss: 4.1485 (4.2555)  class_acc: 0.2083 (0.2052)  loss_scale: 32768.0000 (41975.7788)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1061  max mem: 15572
Epoch: [7]  [1130/1404]  eta: 0:02:51  lr: 0.000092  min_lr: 0.000001  loss: 4.1409 (4.2541)  class_acc: 0.2500 (0.2058)  loss_scale: 32768.0000 (41894.3660)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1116  max mem: 15572
Epoch: [7]  [1140/1404]  eta: 0:02:44  lr: 0.000092  min_lr: 0.000001  loss: 4.1778 (4.2547)  class_acc: 0.2083 (0.2057)  loss_scale: 32768.0000 (41814.3804)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0601  max mem: 15572
Epoch: [7]  [1150/1404]  eta: 0:02:38  lr: 0.000092  min_lr: 0.000001  loss: 4.3435 (4.2545)  class_acc: 0.1667 (0.2055)  loss_scale: 32768.0000 (41735.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0011  max mem: 15572
Epoch: [7]  [1160/1404]  eta: 0:02:32  lr: 0.000092  min_lr: 0.000001  loss: 4.2405 (4.2534)  class_acc: 0.1667 (0.2064)  loss_scale: 32768.0000 (41658.5426)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0010  max mem: 15572
Epoch: [7]  [1170/1404]  eta: 0:02:26  lr: 0.000092  min_lr: 0.000001  loss: 4.2405 (4.2533)  class_acc: 0.2500 (0.2067)  loss_scale: 32768.0000 (41582.6200)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0099  max mem: 15572
[2025-01-17 09:53:24,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=57, lr=[8.93733379008233e-07, 8.93733379008233e-07, 1.2767619700117616e-06, 1.2767619700117616e-06, 1.823945671445374e-06, 1.823945671445374e-06, 2.6056366734933912e-06, 2.6056366734933912e-06, 3.7223381049905594e-06, 3.7223381049905594e-06, 5.317625864272228e-06, 5.317625864272228e-06, 7.596608377531754e-06, 7.596608377531754e-06, 1.0852297682188223e-05, 1.0852297682188223e-05, 1.550328240312603e-05, 1.550328240312603e-05, 2.2147546290180047e-05, 2.2147546290180047e-05, 3.163935184311435e-05, 3.163935184311435e-05, 4.5199074061591934e-05, 4.5199074061591934e-05, 6.45701058022742e-05, 6.45701058022742e-05, 9.224300828896315e-05, 9.224300828896315e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 09:53:24,829] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=44.96400542375562, CurrSamplesPerSec=42.38547370300318, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1180/1404]  eta: 0:02:19  lr: 0.000092  min_lr: 0.000001  loss: 4.3361 (4.2555)  class_acc: 0.1667 (0.2062)  loss_scale: 32768.0000 (41507.9831)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.0102  max mem: 15572
Epoch: [7]  [1190/1404]  eta: 0:02:13  lr: 0.000092  min_lr: 0.000001  loss: 4.4319 (4.2564)  class_acc: 0.1250 (0.2058)  loss_scale: 32768.0000 (41434.5995)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0264  max mem: 15572
[2025-01-17 09:53:37,559] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:53:37,559] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:53:37,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:53:37,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:53:40,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11025
[2025-01-17 09:53:40,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:53:40,960] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11025
[2025-01-17 09:53:40,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:53:40,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [1200/1404]  eta: 0:02:07  lr: 0.000092  min_lr: 0.000001  loss: 4.3533 (4.2566)  class_acc: 0.1667 (0.2061)  loss_scale: 32768.0000 (41526.1415)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0261  max mem: 15572
Epoch: [7]  [1210/1404]  eta: 0:02:01  lr: 0.000092  min_lr: 0.000001  loss: 4.2531 (4.2552)  class_acc: 0.2500 (0.2068)  loss_scale: 32768.0000 (41453.8200)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0677  max mem: 15572
Epoch: [7]  [1220/1404]  eta: 0:01:54  lr: 0.000092  min_lr: 0.000001  loss: 4.1777 (4.2564)  class_acc: 0.2500 (0.2065)  loss_scale: 32768.0000 (41382.6830)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0991  max mem: 15572
Epoch: [7]  [1230/1404]  eta: 0:01:48  lr: 0.000092  min_lr: 0.000001  loss: 4.2132 (4.2560)  class_acc: 0.2083 (0.2066)  loss_scale: 32768.0000 (41312.7019)  weight_decay: 0.0500 (0.0500)  time: 0.6837  data: 0.0322  max mem: 15572
Epoch: [7]  [1240/1404]  eta: 0:01:42  lr: 0.000092  min_lr: 0.000001  loss: 4.1887 (4.2567)  class_acc: 0.2083 (0.2066)  loss_scale: 32768.0000 (41243.8485)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.0006  max mem: 15572
Epoch: [7]  [1250/1404]  eta: 0:01:36  lr: 0.000092  min_lr: 0.000001  loss: 4.1589 (4.2555)  class_acc: 0.2083 (0.2072)  loss_scale: 32768.0000 (41176.0959)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.0007  max mem: 15572
Epoch: [7]  [1260/1404]  eta: 0:01:30  lr: 0.000092  min_lr: 0.000001  loss: 4.0993 (4.2552)  class_acc: 0.2083 (0.2069)  loss_scale: 32768.0000 (41109.4179)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0012  max mem: 15572
Epoch: [7]  [1270/1404]  eta: 0:01:23  lr: 0.000092  min_lr: 0.000001  loss: 4.4343 (4.2568)  class_acc: 0.1667 (0.2065)  loss_scale: 32768.0000 (41043.7891)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0013  max mem: 15572
Epoch: [7]  [1280/1404]  eta: 0:01:17  lr: 0.000092  min_lr: 0.000001  loss: 4.2093 (4.2555)  class_acc: 0.2083 (0.2068)  loss_scale: 32768.0000 (40979.1850)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0008  max mem: 15572
Epoch: [7]  [1290/1404]  eta: 0:01:11  lr: 0.000092  min_lr: 0.000001  loss: 4.1435 (4.2552)  class_acc: 0.2500 (0.2074)  loss_scale: 32768.0000 (40915.5817)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0421  max mem: 15572
Epoch: [7]  [1300/1404]  eta: 0:01:05  lr: 0.000092  min_lr: 0.000001  loss: 4.2209 (4.2553)  class_acc: 0.2500 (0.2073)  loss_scale: 32768.0000 (40852.9562)  weight_decay: 0.0500 (0.0500)  time: 0.7026  data: 0.1229  max mem: 15572
Epoch: [7]  [1310/1404]  eta: 0:00:58  lr: 0.000092  min_lr: 0.000001  loss: 4.1949 (4.2558)  class_acc: 0.2083 (0.2072)  loss_scale: 32768.0000 (40791.2860)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0816  max mem: 15572
Epoch: [7]  [1320/1404]  eta: 0:00:52  lr: 0.000092  min_lr: 0.000001  loss: 4.2354 (4.2564)  class_acc: 0.2083 (0.2071)  loss_scale: 32768.0000 (40730.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0114  max mem: 15572
[2025-01-17 09:55:03,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:55:03,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:55:03,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:55:03,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [1330/1404]  eta: 0:00:46  lr: 0.000092  min_lr: 0.000001  loss: 4.2542 (4.2561)  class_acc: 0.2083 (0.2074)  loss_scale: 32768.0000 (40793.8212)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0480  max mem: 15572
[2025-01-17 09:55:06,777] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11159
[2025-01-17 09:55:06,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:55:06,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:55:06,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11159
[2025-01-17 09:55:06,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [7]  [1340/1404]  eta: 0:00:40  lr: 0.000092  min_lr: 0.000001  loss: 4.2652 (4.2559)  class_acc: 0.2500 (0.2078)  loss_scale: 32768.0000 (40733.9717)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.0373  max mem: 15572
Epoch: [7]  [1350/1404]  eta: 0:00:33  lr: 0.000092  min_lr: 0.000001  loss: 4.2258 (4.2552)  class_acc: 0.2500 (0.2081)  loss_scale: 32768.0000 (40675.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0007  max mem: 15572
Epoch: [7]  [1360/1404]  eta: 0:00:27  lr: 0.000092  min_lr: 0.000001  loss: 4.1400 (4.2543)  class_acc: 0.2500 (0.2082)  loss_scale: 32768.0000 (40616.9111)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0009  max mem: 15572
Epoch: [7]  [1370/1404]  eta: 0:00:21  lr: 0.000092  min_lr: 0.000001  loss: 4.3127 (4.2544)  class_acc: 0.1667 (0.2084)  loss_scale: 32768.0000 (40559.6616)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0009  max mem: 15572
Epoch: [7]  [1380/1404]  eta: 0:00:14  lr: 0.000092  min_lr: 0.000001  loss: 4.3604 (4.2553)  class_acc: 0.1667 (0.2082)  loss_scale: 32768.0000 (40503.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0165  max mem: 15572
Epoch: [7]  [1390/1404]  eta: 0:00:08  lr: 0.000092  min_lr: 0.000001  loss: 4.3459 (4.2555)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (40447.6319)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0167  max mem: 15572
Epoch: [7]  [1400/1404]  eta: 0:00:02  lr: 0.000092  min_lr: 0.000001  loss: 4.2960 (4.2551)  class_acc: 0.2083 (0.2086)  loss_scale: 32768.0000 (40392.8166)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0007  max mem: 15572
Epoch: [7]  [1403/1404]  eta: 0:00:00  lr: 0.000092  min_lr: 0.000001  loss: 4.2629 (4.2556)  class_acc: 0.2083 (0.2085)  loss_scale: 32768.0000 (40376.5242)  weight_decay: 0.0500 (0.0500)  time: 0.4222  data: 0.0006  max mem: 15572
Epoch: [7] Total time: 0:14:34 (0.6225 s / it)
Averaged stats: lr: 0.000092  min_lr: 0.000001  loss: 4.2629 (4.2613)  class_acc: 0.2083 (0.2074)  loss_scale: 32768.0000 (40376.5242)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:13:06  loss: 2.0936 (2.0936)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 5.7859  data: 5.5371  max mem: 15572
Val:  [ 10/136]  eta: 0:01:40  loss: 3.1152 (3.1697)  acc1: 16.6667 (25.2525)  acc5: 61.1111 (52.0202)  time: 0.8003  data: 0.5851  max mem: 15572
Val:  [ 20/136]  eta: 0:01:09  loss: 3.1155 (3.2169)  acc1: 16.6667 (21.4286)  acc5: 61.1111 (54.2328)  time: 0.3394  data: 0.1286  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 3.0388 (2.9816)  acc1: 22.2222 (30.1075)  acc5: 61.1111 (59.4982)  time: 0.3100  data: 0.0936  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.2791 (2.8813)  acc1: 50.0000 (32.3848)  acc5: 77.7778 (63.1436)  time: 0.3083  data: 0.0963  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.8754 (2.9771)  acc1: 27.7778 (30.7190)  acc5: 72.2222 (61.6558)  time: 0.3998  data: 0.1949  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 3.4210 (3.0759)  acc1: 16.6667 (27.6867)  acc5: 50.0000 (59.0164)  time: 0.3938  data: 0.1837  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 3.2147 (3.0339)  acc1: 22.2222 (30.1252)  acc5: 50.0000 (59.5462)  time: 0.3560  data: 0.1563  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.6901 (3.0006)  acc1: 38.8889 (30.7270)  acc5: 66.6667 (61.1111)  time: 0.3537  data: 0.1657  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.8598 (3.0113)  acc1: 22.2222 (29.4872)  acc5: 66.6667 (61.2943)  time: 0.3489  data: 0.1501  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.1951 (3.0725)  acc1: 5.5556 (27.7778)  acc5: 61.1111 (59.9010)  time: 0.3425  data: 0.1505  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.1214 (3.0673)  acc1: 16.6667 (28.5786)  acc5: 61.1111 (60.2603)  time: 0.3721  data: 0.1951  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.7458 (3.0002)  acc1: 44.4444 (30.8081)  acc5: 72.2222 (61.9835)  time: 0.3949  data: 0.1973  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4622 (2.9526)  acc1: 55.5556 (32.5700)  acc5: 72.2222 (62.5530)  time: 0.3088  data: 0.1235  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.5786 (2.9650)  acc1: 38.8889 (32.2277)  acc5: 61.1111 (62.2441)  time: 0.2064  data: 0.0424  max mem: 15572
Val: Total time: 0:00:51 (0.3812 s / it)
* Acc@1 32.248 Acc@5 61.384 loss 2.999
Accuracy of the network on the 4883 val videos: 32.2%
[2025-01-17 09:56:39,047] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 09:56:39,049] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 09:56:39,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 09:56:39,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 09:56:40,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 09:56:40,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 32.25%
Epoch: [8]  [   0/1404]  eta: 3:02:03  lr: 0.000092  min_lr: 0.000001  loss: 4.9971 (4.9971)  class_acc: 0.0000 (0.0000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.7803  data: 5.7355  max mem: 15572
Epoch: [8]  [  10/1404]  eta: 0:29:09  lr: 0.000092  min_lr: 0.000001  loss: 4.2759 (4.3064)  class_acc: 0.2083 (0.2121)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2553  data: 0.5226  max mem: 15572
Epoch: [8]  [  20/1404]  eta: 0:21:49  lr: 0.000092  min_lr: 0.000001  loss: 4.2844 (4.2682)  class_acc: 0.2083 (0.2163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0012  max mem: 15572
Epoch: [8]  [  30/1404]  eta: 0:19:39  lr: 0.000092  min_lr: 0.000001  loss: 4.3224 (4.2583)  class_acc: 0.2083 (0.2164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.0008  max mem: 15572
Epoch: [8]  [  40/1404]  eta: 0:17:41  lr: 0.000092  min_lr: 0.000001  loss: 4.3019 (4.2626)  class_acc: 0.1667 (0.2093)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0006  max mem: 15572
Epoch: [8]  [  50/1404]  eta: 0:17:07  lr: 0.000092  min_lr: 0.000001  loss: 4.2465 (4.2416)  class_acc: 0.1667 (0.2059)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0007  max mem: 15572
[2025-01-17 09:57:22,962] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:57:22,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:57:23,001] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:57:23,003] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:57:23,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11290
[2025-01-17 09:57:23,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:57:23,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11290
[2025-01-17 09:57:23,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:57:23,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [  60/1404]  eta: 0:16:20  lr: 0.000092  min_lr: 0.000001  loss: 4.0936 (4.2198)  class_acc: 0.1667 (0.2090)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.0006  max mem: 15572
Epoch: [8]  [  70/1404]  eta: 0:15:50  lr: 0.000092  min_lr: 0.000001  loss: 4.2105 (4.2300)  class_acc: 0.1667 (0.2089)  loss_scale: 32768.0000 (33691.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0006  max mem: 15572
Epoch: [8]  [  80/1404]  eta: 0:15:22  lr: 0.000092  min_lr: 0.000001  loss: 4.2532 (4.2529)  class_acc: 0.1667 (0.2047)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0008  max mem: 15572
Epoch: [8]  [  90/1404]  eta: 0:15:11  lr: 0.000092  min_lr: 0.000001  loss: 4.2249 (4.2389)  class_acc: 0.2083 (0.2115)  loss_scale: 32768.0000 (33488.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0007  max mem: 15572
Epoch: [8]  [ 100/1404]  eta: 0:15:01  lr: 0.000092  min_lr: 0.000001  loss: 4.2692 (4.2447)  class_acc: 0.2083 (0.2104)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.0010  max mem: 15572
Epoch: [8]  [ 110/1404]  eta: 0:14:42  lr: 0.000092  min_lr: 0.000001  loss: 4.2804 (4.2432)  class_acc: 0.2083 (0.2087)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0009  max mem: 15572
Epoch: [8]  [ 120/1404]  eta: 0:14:27  lr: 0.000092  min_lr: 0.000001  loss: 4.2306 (4.2441)  class_acc: 0.1667 (0.2073)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0006  max mem: 15572
Epoch: [8]  [ 130/1404]  eta: 0:14:16  lr: 0.000092  min_lr: 0.000001  loss: 4.1368 (4.2314)  class_acc: 0.2083 (0.2096)  loss_scale: 32768.0000 (33268.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.0006  max mem: 15572
Epoch: [8]  [ 140/1404]  eta: 0:14:00  lr: 0.000092  min_lr: 0.000001  loss: 4.1277 (4.2329)  class_acc: 0.2083 (0.2072)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0006  max mem: 15572
Epoch: [8]  [ 150/1404]  eta: 0:13:49  lr: 0.000092  min_lr: 0.000001  loss: 4.2689 (4.2297)  class_acc: 0.2083 (0.2114)  loss_scale: 32768.0000 (33202.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0007  max mem: 15572
Epoch: [8]  [ 160/1404]  eta: 0:13:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1213 (4.2263)  class_acc: 0.2917 (0.2148)  loss_scale: 32768.0000 (33175.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0007  max mem: 15572
Epoch: [8]  [ 170/1404]  eta: 0:13:39  lr: 0.000092  min_lr: 0.000001  loss: 4.1213 (4.2230)  class_acc: 0.2500 (0.2166)  loss_scale: 32768.0000 (33151.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6875  data: 0.0009  max mem: 15572
Epoch: [8]  [ 180/1404]  eta: 0:13:27  lr: 0.000092  min_lr: 0.000001  loss: 4.2476 (4.2231)  class_acc: 0.2083 (0.2148)  loss_scale: 32768.0000 (33130.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.0009  max mem: 15572
[2025-01-17 09:58:44,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:58:44,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:58:44,250] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 09:58:44,250] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 09:58:44,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11420
[2025-01-17 09:58:44,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 09:58:44,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 09:58:44,790] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11420
[2025-01-17 09:58:44,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 190/1404]  eta: 0:13:13  lr: 0.000092  min_lr: 0.000001  loss: 4.3409 (4.2324)  class_acc: 0.1667 (0.2138)  loss_scale: 32768.0000 (33282.6806)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0007  max mem: 15572
Epoch: [8]  [ 200/1404]  eta: 0:13:04  lr: 0.000092  min_lr: 0.000001  loss: 4.3288 (4.2338)  class_acc: 0.2083 (0.2143)  loss_scale: 32768.0000 (33257.0746)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.0008  max mem: 15572
Epoch: [8]  [ 210/1404]  eta: 0:12:58  lr: 0.000092  min_lr: 0.000001  loss: 4.2146 (4.2355)  class_acc: 0.2083 (0.2125)  loss_scale: 32768.0000 (33233.8957)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0008  max mem: 15572
Epoch: [8]  [ 220/1404]  eta: 0:12:47  lr: 0.000092  min_lr: 0.000001  loss: 4.2146 (4.2356)  class_acc: 0.1667 (0.2098)  loss_scale: 32768.0000 (33212.8145)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0010  max mem: 15572
Epoch: [8]  [ 230/1404]  eta: 0:12:40  lr: 0.000092  min_lr: 0.000001  loss: 4.2418 (4.2356)  class_acc: 0.2083 (0.2110)  loss_scale: 32768.0000 (33193.5584)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0012  max mem: 15572
Epoch: [8]  [ 240/1404]  eta: 0:12:35  lr: 0.000092  min_lr: 0.000001  loss: 4.2505 (4.2385)  class_acc: 0.2083 (0.2080)  loss_scale: 32768.0000 (33175.9004)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.0009  max mem: 15572
Epoch: [8]  [ 250/1404]  eta: 0:12:27  lr: 0.000092  min_lr: 0.000001  loss: 4.3125 (4.2462)  class_acc: 0.0833 (0.2062)  loss_scale: 32768.0000 (33159.6494)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0007  max mem: 15572
Epoch: [8]  [ 260/1404]  eta: 0:12:18  lr: 0.000092  min_lr: 0.000001  loss: 4.3006 (4.2509)  class_acc: 0.0833 (0.2048)  loss_scale: 32768.0000 (33144.6437)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.0007  max mem: 15572
Epoch: [8]  [ 270/1404]  eta: 0:12:11  lr: 0.000092  min_lr: 0.000001  loss: 4.4049 (4.2640)  class_acc: 0.1250 (0.2030)  loss_scale: 32768.0000 (33130.7454)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0008  max mem: 15572
Epoch: [8]  [ 280/1404]  eta: 0:12:01  lr: 0.000092  min_lr: 0.000001  loss: 4.4031 (4.2637)  class_acc: 0.1667 (0.2026)  loss_scale: 32768.0000 (33117.8363)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0008  max mem: 15572
Epoch: [8]  [ 290/1404]  eta: 0:11:53  lr: 0.000092  min_lr: 0.000001  loss: 4.1734 (4.2598)  class_acc: 0.2083 (0.2032)  loss_scale: 32768.0000 (33105.8144)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0008  max mem: 15572
Epoch: [8]  [ 300/1404]  eta: 0:11:45  lr: 0.000092  min_lr: 0.000001  loss: 4.1849 (4.2575)  class_acc: 0.2083 (0.2045)  loss_scale: 32768.0000 (33094.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0007  max mem: 15572
Epoch: [8]  [ 310/1404]  eta: 0:11:38  lr: 0.000092  min_lr: 0.000001  loss: 4.1966 (4.2581)  class_acc: 0.2083 (0.2046)  loss_scale: 32768.0000 (33084.0900)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.0007  max mem: 15572
[2025-01-17 10:00:05,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:00:05,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:00:05,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:00:05,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 320/1404]  eta: 0:11:33  lr: 0.000092  min_lr: 0.000001  loss: 4.1966 (4.2584)  class_acc: 0.1667 (0.2043)  loss_scale: 32768.0000 (33482.5670)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.0005  max mem: 15572
Epoch: [8]  [ 330/1404]  eta: 0:11:22  lr: 0.000092  min_lr: 0.000001  loss: 4.1140 (4.2559)  class_acc: 0.2083 (0.2051)  loss_scale: 65536.0000 (34450.9486)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0005  max mem: 15572
Epoch: [8]  [ 340/1404]  eta: 0:11:15  lr: 0.000092  min_lr: 0.000001  loss: 4.3145 (4.2596)  class_acc: 0.2083 (0.2047)  loss_scale: 65536.0000 (35362.5337)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0006  max mem: 15572
Epoch: [8]  [ 350/1404]  eta: 0:11:10  lr: 0.000092  min_lr: 0.000001  loss: 4.3145 (4.2541)  class_acc: 0.2083 (0.2069)  loss_scale: 65536.0000 (36222.1766)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.0008  max mem: 15572
Epoch: [8]  [ 360/1404]  eta: 0:11:04  lr: 0.000092  min_lr: 0.000001  loss: 4.1378 (4.2559)  class_acc: 0.1667 (0.2059)  loss_scale: 65536.0000 (37034.1939)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.0009  max mem: 15572
[2025-01-17 10:00:36,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11601
[2025-01-17 10:00:36,068] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:00:36,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:00:36,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11601
[2025-01-17 10:00:36,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 370/1404]  eta: 0:10:56  lr: 0.000092  min_lr: 0.000001  loss: 4.2463 (4.2533)  class_acc: 0.1667 (0.2053)  loss_scale: 65536.0000 (37625.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0009  max mem: 15572
Epoch: [8]  [ 380/1404]  eta: 0:10:49  lr: 0.000092  min_lr: 0.000001  loss: 4.1977 (4.2519)  class_acc: 0.2083 (0.2059)  loss_scale: 32768.0000 (37498.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0008  max mem: 15572
Epoch: [8]  [ 390/1404]  eta: 0:10:43  lr: 0.000092  min_lr: 0.000001  loss: 4.2992 (4.2527)  class_acc: 0.2500 (0.2058)  loss_scale: 32768.0000 (37377.3095)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0007  max mem: 15572
Epoch: [8]  [ 400/1404]  eta: 0:10:34  lr: 0.000092  min_lr: 0.000001  loss: 4.3236 (4.2502)  class_acc: 0.2500 (0.2074)  loss_scale: 32768.0000 (37262.3641)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0007  max mem: 15572
Epoch: [8]  [ 410/1404]  eta: 0:10:29  lr: 0.000092  min_lr: 0.000001  loss: 4.0964 (4.2502)  class_acc: 0.2500 (0.2079)  loss_scale: 32768.0000 (37153.0122)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0006  max mem: 15572
Epoch: [8]  [ 420/1404]  eta: 0:10:23  lr: 0.000092  min_lr: 0.000001  loss: 4.3083 (4.2502)  class_acc: 0.1667 (0.2078)  loss_scale: 32768.0000 (37048.8551)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.0006  max mem: 15572
Epoch: [8]  [ 430/1404]  eta: 0:10:15  lr: 0.000092  min_lr: 0.000001  loss: 4.1676 (4.2453)  class_acc: 0.2500 (0.2100)  loss_scale: 32768.0000 (36949.5313)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0008  max mem: 15572
Epoch: [8]  [ 440/1404]  eta: 0:10:08  lr: 0.000092  min_lr: 0.000001  loss: 4.0890 (4.2432)  class_acc: 0.2500 (0.2106)  loss_scale: 32768.0000 (36854.7120)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0010  max mem: 15572
Epoch: [8]  [ 450/1404]  eta: 0:10:02  lr: 0.000092  min_lr: 0.000001  loss: 4.2549 (4.2430)  class_acc: 0.1667 (0.2105)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0009  max mem: 15572
Epoch: [8]  [ 460/1404]  eta: 0:09:56  lr: 0.000092  min_lr: 0.000001  loss: 4.2624 (4.2414)  class_acc: 0.1667 (0.2109)  loss_scale: 32768.0000 (36677.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0006  max mem: 15572
Epoch: [8]  [ 470/1404]  eta: 0:09:49  lr: 0.000092  min_lr: 0.000001  loss: 4.3105 (4.2425)  class_acc: 0.2083 (0.2108)  loss_scale: 32768.0000 (36594.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.0007  max mem: 15572
Epoch: [8]  [ 480/1404]  eta: 0:09:43  lr: 0.000092  min_lr: 0.000001  loss: 4.1900 (4.2418)  class_acc: 0.1667 (0.2097)  loss_scale: 32768.0000 (36514.8607)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0008  max mem: 15572
Epoch: [8]  [ 490/1404]  eta: 0:09:36  lr: 0.000092  min_lr: 0.000001  loss: 3.9961 (4.2364)  class_acc: 0.2500 (0.2110)  loss_scale: 32768.0000 (36438.5499)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0007  max mem: 15572
[2025-01-17 10:01:56,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:01:56,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:01:56,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:01:56,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 500/1404]  eta: 0:09:30  lr: 0.000092  min_lr: 0.000001  loss: 4.1345 (4.2372)  class_acc: 0.2500 (0.2106)  loss_scale: 32768.0000 (36561.5010)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.0006  max mem: 15572
[2025-01-17 10:01:58,958] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11735
[2025-01-17 10:01:58,958] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:01:58,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:01:58,960] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11735
[2025-01-17 10:01:58,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 510/1404]  eta: 0:09:21  lr: 0.000092  min_lr: 0.000001  loss: 4.2197 (4.2379)  class_acc: 0.2500 (0.2125)  loss_scale: 32768.0000 (36615.5147)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.0006  max mem: 15572
Epoch: [8]  [ 520/1404]  eta: 0:09:16  lr: 0.000092  min_lr: 0.000001  loss: 4.2496 (4.2394)  class_acc: 0.2083 (0.2118)  loss_scale: 32768.0000 (36541.6660)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0007  max mem: 15572
Epoch: [8]  [ 530/1404]  eta: 0:09:12  lr: 0.000092  min_lr: 0.000001  loss: 4.3217 (4.2401)  class_acc: 0.2083 (0.2119)  loss_scale: 32768.0000 (36470.5989)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.0011  max mem: 15572
Epoch: [8]  [ 540/1404]  eta: 0:09:05  lr: 0.000092  min_lr: 0.000001  loss: 4.3217 (4.2409)  class_acc: 0.2083 (0.2120)  loss_scale: 32768.0000 (36402.1590)  weight_decay: 0.0500 (0.0500)  time: 0.6897  data: 0.0010  max mem: 15572
Epoch: [8]  [ 550/1404]  eta: 0:09:00  lr: 0.000092  min_lr: 0.000001  loss: 4.2884 (4.2400)  class_acc: 0.2083 (0.2127)  loss_scale: 32768.0000 (36336.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.0007  max mem: 15572
Epoch: [8]  [ 560/1404]  eta: 0:08:53  lr: 0.000092  min_lr: 0.000001  loss: 4.0967 (4.2370)  class_acc: 0.2083 (0.2137)  loss_scale: 32768.0000 (36272.5989)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.0009  max mem: 15572
Epoch: [8]  [ 570/1404]  eta: 0:08:48  lr: 0.000092  min_lr: 0.000001  loss: 4.2352 (4.2380)  class_acc: 0.2083 (0.2137)  loss_scale: 32768.0000 (36211.2224)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.0010  max mem: 15572
Epoch: [8]  [ 580/1404]  eta: 0:08:40  lr: 0.000092  min_lr: 0.000001  loss: 4.1932 (4.2369)  class_acc: 0.2083 (0.2139)  loss_scale: 32768.0000 (36151.9587)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0009  max mem: 15572
Epoch: [8]  [ 590/1404]  eta: 0:08:34  lr: 0.000092  min_lr: 0.000001  loss: 4.1973 (4.2371)  class_acc: 0.2083 (0.2136)  loss_scale: 32768.0000 (36094.7005)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0008  max mem: 15572
Epoch: [8]  [ 600/1404]  eta: 0:08:26  lr: 0.000092  min_lr: 0.000001  loss: 4.1973 (4.2336)  class_acc: 0.1667 (0.2142)  loss_scale: 32768.0000 (36039.3478)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0200  max mem: 15572
Epoch: [8]  [ 610/1404]  eta: 0:08:20  lr: 0.000092  min_lr: 0.000001  loss: 4.1012 (4.2320)  class_acc: 0.2500 (0.2145)  loss_scale: 32768.0000 (35985.8069)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.0777  max mem: 15572
Epoch: [8]  [ 620/1404]  eta: 0:08:15  lr: 0.000092  min_lr: 0.000001  loss: 4.2764 (4.2343)  class_acc: 0.1667 (0.2138)  loss_scale: 32768.0000 (35933.9903)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0708  max mem: 15572
Epoch: [8]  [ 630/1404]  eta: 0:08:08  lr: 0.000092  min_lr: 0.000001  loss: 4.2511 (4.2332)  class_acc: 0.1667 (0.2142)  loss_scale: 32768.0000 (35883.8162)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.0133  max mem: 15572
[2025-01-17 10:03:20,390] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:03:20,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:03:20,393] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:03:20,393] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 640/1404]  eta: 0:08:02  lr: 0.000092  min_lr: 0.000001  loss: 4.1410 (4.2330)  class_acc: 0.2500 (0.2145)  loss_scale: 32768.0000 (36295.2886)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0663  max mem: 15572
[2025-01-17 10:03:26,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11873
[2025-01-17 10:03:26,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:03:26,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:03:26,355] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11873
[2025-01-17 10:03:26,356] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [ 650/1404]  eta: 0:07:55  lr: 0.000092  min_lr: 0.000001  loss: 4.1971 (4.2335)  class_acc: 0.2083 (0.2143)  loss_scale: 32768.0000 (36241.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0812  max mem: 15572
Epoch: [8]  [ 660/1404]  eta: 0:07:48  lr: 0.000091  min_lr: 0.000001  loss: 4.1971 (4.2330)  class_acc: 0.2500 (0.2149)  loss_scale: 32768.0000 (36188.5628)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.0424  max mem: 15572
Epoch: [8]  [ 670/1404]  eta: 0:07:42  lr: 0.000091  min_lr: 0.000001  loss: 4.2849 (4.2351)  class_acc: 0.2083 (0.2149)  loss_scale: 32768.0000 (36137.5857)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0807  max mem: 15572
Epoch: [8]  [ 680/1404]  eta: 0:07:35  lr: 0.000091  min_lr: 0.000001  loss: 4.1322 (4.2323)  class_acc: 0.2083 (0.2157)  loss_scale: 32768.0000 (36088.1057)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0541  max mem: 15572
Epoch: [8]  [ 690/1404]  eta: 0:07:29  lr: 0.000091  min_lr: 0.000001  loss: 4.1322 (4.2327)  class_acc: 0.2083 (0.2154)  loss_scale: 32768.0000 (36040.0579)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0009  max mem: 15572
Epoch: [8]  [ 700/1404]  eta: 0:07:22  lr: 0.000091  min_lr: 0.000001  loss: 4.4344 (4.2349)  class_acc: 0.1667 (0.2158)  loss_scale: 32768.0000 (35993.3809)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0011  max mem: 15572
Epoch: [8]  [ 710/1404]  eta: 0:07:16  lr: 0.000091  min_lr: 0.000001  loss: 4.4423 (4.2365)  class_acc: 0.1667 (0.2152)  loss_scale: 32768.0000 (35948.0169)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0009  max mem: 15572
Epoch: [8]  [ 720/1404]  eta: 0:07:09  lr: 0.000091  min_lr: 0.000001  loss: 4.4423 (4.2380)  class_acc: 0.1667 (0.2150)  loss_scale: 32768.0000 (35903.9112)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0008  max mem: 15572
Epoch: [8]  [ 730/1404]  eta: 0:07:03  lr: 0.000091  min_lr: 0.000001  loss: 4.2467 (4.2365)  class_acc: 0.2083 (0.2153)  loss_scale: 32768.0000 (35861.0123)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.0008  max mem: 15572
Epoch: [8]  [ 740/1404]  eta: 0:06:56  lr: 0.000091  min_lr: 0.000001  loss: 4.1884 (4.2361)  class_acc: 0.2083 (0.2154)  loss_scale: 32768.0000 (35819.2713)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0007  max mem: 15572
Epoch: [8]  [ 750/1404]  eta: 0:06:50  lr: 0.000091  min_lr: 0.000001  loss: 4.1378 (4.2355)  class_acc: 0.2083 (0.2153)  loss_scale: 32768.0000 (35778.6418)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.0006  max mem: 15572
Epoch: [8]  [ 760/1404]  eta: 0:06:44  lr: 0.000091  min_lr: 0.000001  loss: 4.0870 (4.2320)  class_acc: 0.2083 (0.2156)  loss_scale: 32768.0000 (35739.0802)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0005  max mem: 15572
[2025-01-17 10:04:42,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=64, lr=[8.855418217882891e-07, 8.855418217882891e-07, 1.2650597454118418e-06, 1.2650597454118418e-06, 1.8072282077312026e-06, 1.8072282077312026e-06, 2.581754582473147e-06, 2.581754582473147e-06, 3.6882208321044958e-06, 3.6882208321044958e-06, 5.268886903006422e-06, 5.268886903006422e-06, 7.526981290009176e-06, 7.526981290009176e-06, 1.0752830414298824e-05, 1.0752830414298824e-05, 1.5361186306141176e-05, 1.5361186306141176e-05, 2.194455186591597e-05, 2.194455186591597e-05, 3.1349359808451386e-05, 3.1349359808451386e-05, 4.478479972635913e-05, 4.478479972635913e-05, 6.397828532337019e-05, 6.397828532337019e-05, 9.139755046195741e-05, 9.139755046195741e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:04:42,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=44.6701171434341, CurrSamplesPerSec=53.51475940411624, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-17 10:04:44,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:04:44,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:04:44,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:04:44,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 770/1404]  eta: 0:06:37  lr: 0.000091  min_lr: 0.000001  loss: 4.0870 (4.2321)  class_acc: 0.2083 (0.2161)  loss_scale: 32768.0000 (35743.0454)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0005  max mem: 15572
[2025-01-17 10:04:48,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12006
[2025-01-17 10:04:48,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:04:48,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12006
[2025-01-17 10:04:48,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:04:48,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 780/1404]  eta: 0:06:31  lr: 0.000091  min_lr: 0.000001  loss: 4.3088 (4.2334)  class_acc: 0.1667 (0.2160)  loss_scale: 32768.0000 (35830.8220)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0007  max mem: 15572
Epoch: [8]  [ 790/1404]  eta: 0:06:25  lr: 0.000091  min_lr: 0.000001  loss: 4.3088 (4.2340)  class_acc: 0.2083 (0.2161)  loss_scale: 32768.0000 (35792.1011)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.0007  max mem: 15572
Epoch: [8]  [ 800/1404]  eta: 0:06:18  lr: 0.000091  min_lr: 0.000001  loss: 4.2305 (4.2342)  class_acc: 0.2500 (0.2161)  loss_scale: 32768.0000 (35754.3471)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0009  max mem: 15572
Epoch: [8]  [ 810/1404]  eta: 0:06:12  lr: 0.000091  min_lr: 0.000001  loss: 4.2912 (4.2370)  class_acc: 0.1667 (0.2152)  loss_scale: 32768.0000 (35717.5240)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0010  max mem: 15572
Epoch: [8]  [ 820/1404]  eta: 0:06:05  lr: 0.000091  min_lr: 0.000001  loss: 4.2227 (4.2362)  class_acc: 0.2083 (0.2161)  loss_scale: 32768.0000 (35681.5981)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0008  max mem: 15572
Epoch: [8]  [ 830/1404]  eta: 0:05:58  lr: 0.000091  min_lr: 0.000001  loss: 4.2239 (4.2362)  class_acc: 0.2083 (0.2164)  loss_scale: 32768.0000 (35646.5367)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0009  max mem: 15572
Epoch: [8]  [ 840/1404]  eta: 0:05:52  lr: 0.000091  min_lr: 0.000001  loss: 4.2887 (4.2365)  class_acc: 0.1667 (0.2161)  loss_scale: 32768.0000 (35612.3092)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.0010  max mem: 15572
Epoch: [8]  [ 850/1404]  eta: 0:05:46  lr: 0.000091  min_lr: 0.000001  loss: 4.1965 (4.2357)  class_acc: 0.1667 (0.2163)  loss_scale: 32768.0000 (35578.8860)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.0009  max mem: 15572
Epoch: [8]  [ 860/1404]  eta: 0:05:40  lr: 0.000091  min_lr: 0.000001  loss: 4.1829 (4.2356)  class_acc: 0.1667 (0.2160)  loss_scale: 32768.0000 (35546.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.0010  max mem: 15572
Epoch: [8]  [ 870/1404]  eta: 0:05:34  lr: 0.000091  min_lr: 0.000001  loss: 4.1351 (4.2339)  class_acc: 0.1667 (0.2159)  loss_scale: 32768.0000 (35514.3421)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0009  max mem: 15572
Epoch: [8]  [ 880/1404]  eta: 0:05:27  lr: 0.000091  min_lr: 0.000001  loss: 4.2114 (4.2328)  class_acc: 0.2083 (0.2161)  loss_scale: 32768.0000 (35483.1691)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.0008  max mem: 15572
Epoch: [8]  [ 890/1404]  eta: 0:05:20  lr: 0.000091  min_lr: 0.000001  loss: 4.2060 (4.2321)  class_acc: 0.2083 (0.2164)  loss_scale: 32768.0000 (35452.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0008  max mem: 15572
Epoch: [8]  [ 900/1404]  eta: 0:05:14  lr: 0.000091  min_lr: 0.000001  loss: 4.1913 (4.2314)  class_acc: 0.2083 (0.2166)  loss_scale: 32768.0000 (35422.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0008  max mem: 15572
[2025-01-17 10:06:05,293] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:06:05,293] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:06:05,335] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:06:05,337] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 910/1404]  eta: 0:05:07  lr: 0.000091  min_lr: 0.000001  loss: 4.1913 (4.2294)  class_acc: 0.2083 (0.2170)  loss_scale: 32768.0000 (35681.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0007  max mem: 15572
Epoch: [8]  [ 920/1404]  eta: 0:05:01  lr: 0.000091  min_lr: 0.000001  loss: 4.1387 (4.2300)  class_acc: 0.2083 (0.2165)  loss_scale: 65536.0000 (36005.6634)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0007  max mem: 15572
[2025-01-17 10:06:17,685] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12154
[2025-01-17 10:06:17,685] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:06:17,725] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12154
[2025-01-17 10:06:17,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:06:17,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 930/1404]  eta: 0:04:55  lr: 0.000091  min_lr: 0.000001  loss: 4.2145 (4.2296)  class_acc: 0.1667 (0.2164)  loss_scale: 65536.0000 (36006.0838)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.0008  max mem: 15572
Epoch: [8]  [ 940/1404]  eta: 0:04:49  lr: 0.000091  min_lr: 0.000001  loss: 4.1563 (4.2274)  class_acc: 0.2083 (0.2170)  loss_scale: 32768.0000 (35971.6727)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0011  max mem: 15572
Epoch: [8]  [ 950/1404]  eta: 0:04:42  lr: 0.000091  min_lr: 0.000001  loss: 4.0271 (4.2264)  class_acc: 0.2083 (0.2172)  loss_scale: 32768.0000 (35937.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0009  max mem: 15572
Epoch: [8]  [ 960/1404]  eta: 0:04:35  lr: 0.000091  min_lr: 0.000001  loss: 4.1515 (4.2259)  class_acc: 0.2083 (0.2170)  loss_scale: 32768.0000 (35904.9990)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0101  max mem: 15572
Epoch: [8]  [ 970/1404]  eta: 0:04:29  lr: 0.000091  min_lr: 0.000001  loss: 4.1859 (4.2256)  class_acc: 0.2083 (0.2172)  loss_scale: 32768.0000 (35872.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0325  max mem: 15572
Epoch: [8]  [ 980/1404]  eta: 0:04:23  lr: 0.000091  min_lr: 0.000001  loss: 4.2260 (4.2260)  class_acc: 0.2083 (0.2170)  loss_scale: 32768.0000 (35841.0438)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0445  max mem: 15572
Epoch: [8]  [ 990/1404]  eta: 0:04:17  lr: 0.000091  min_lr: 0.000001  loss: 4.3056 (4.2269)  class_acc: 0.2083 (0.2171)  loss_scale: 32768.0000 (35810.0343)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.0221  max mem: 15572
Epoch: [8]  [1000/1404]  eta: 0:04:10  lr: 0.000091  min_lr: 0.000001  loss: 4.3269 (4.2273)  class_acc: 0.2083 (0.2173)  loss_scale: 32768.0000 (35779.6444)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.0061  max mem: 15572
Epoch: [8]  [1010/1404]  eta: 0:04:04  lr: 0.000091  min_lr: 0.000001  loss: 4.3269 (4.2287)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (35749.8556)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0212  max mem: 15572
Epoch: [8]  [1020/1404]  eta: 0:03:58  lr: 0.000091  min_lr: 0.000001  loss: 4.2639 (4.2291)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (35720.6503)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.0160  max mem: 15572
Epoch: [8]  [1030/1404]  eta: 0:03:52  lr: 0.000091  min_lr: 0.000001  loss: 4.2639 (4.2291)  class_acc: 0.2083 (0.2176)  loss_scale: 32768.0000 (35692.0116)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0202  max mem: 15572
Epoch: [8]  [1040/1404]  eta: 0:03:46  lr: 0.000091  min_lr: 0.000001  loss: 4.1615 (4.2280)  class_acc: 0.1667 (0.2179)  loss_scale: 32768.0000 (35663.9232)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0338  max mem: 15572
Epoch: [8]  [1050/1404]  eta: 0:03:39  lr: 0.000091  min_lr: 0.000001  loss: 4.1276 (4.2263)  class_acc: 0.1667 (0.2175)  loss_scale: 32768.0000 (35636.3692)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0146  max mem: 15572
[2025-01-17 10:07:36,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:07:36,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:07:36,705] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:07:36,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1060/1404]  eta: 0:03:33  lr: 0.000091  min_lr: 0.000001  loss: 4.1646 (4.2254)  class_acc: 0.2083 (0.2182)  loss_scale: 32768.0000 (35918.1753)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.0008  max mem: 15572
Epoch: [8]  [1070/1404]  eta: 0:03:27  lr: 0.000091  min_lr: 0.000001  loss: 4.1835 (4.2265)  class_acc: 0.2500 (0.2180)  loss_scale: 65536.0000 (36194.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.0007  max mem: 15572
[2025-01-17 10:07:54,005] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12312
[2025-01-17 10:07:54,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:07:54,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1080/1404]  eta: 0:03:21  lr: 0.000091  min_lr: 0.000001  loss: 4.3560 (4.2275)  class_acc: 0.1667 (0.2175)  loss_scale: 65536.0000 (36435.8335)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0008  max mem: 15572
[2025-01-17 10:07:54,012] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12312
[2025-01-17 10:07:54,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1090/1404]  eta: 0:03:15  lr: 0.000091  min_lr: 0.000001  loss: 4.3476 (4.2282)  class_acc: 0.1667 (0.2174)  loss_scale: 32768.0000 (36402.2145)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.0007  max mem: 15572
Epoch: [8]  [1100/1404]  eta: 0:03:09  lr: 0.000091  min_lr: 0.000001  loss: 4.1595 (4.2262)  class_acc: 0.2083 (0.2177)  loss_scale: 32768.0000 (36369.2062)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0749  max mem: 15572
Epoch: [8]  [1110/1404]  eta: 0:03:02  lr: 0.000091  min_lr: 0.000001  loss: 4.2458 (4.2276)  class_acc: 0.2083 (0.2178)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0895  max mem: 15572
Epoch: [8]  [1120/1404]  eta: 0:02:56  lr: 0.000091  min_lr: 0.000001  loss: 4.1974 (4.2258)  class_acc: 0.2083 (0.2178)  loss_scale: 32768.0000 (36304.9563)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0157  max mem: 15572
Epoch: [8]  [1130/1404]  eta: 0:02:50  lr: 0.000091  min_lr: 0.000001  loss: 4.0428 (4.2251)  class_acc: 0.2083 (0.2178)  loss_scale: 32768.0000 (36273.6835)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.0011  max mem: 15572
Epoch: [8]  [1140/1404]  eta: 0:02:44  lr: 0.000091  min_lr: 0.000001  loss: 4.0216 (4.2229)  class_acc: 0.2917 (0.2186)  loss_scale: 32768.0000 (36242.9588)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0005  max mem: 15572
Epoch: [8]  [1150/1404]  eta: 0:02:37  lr: 0.000091  min_lr: 0.000001  loss: 4.0032 (4.2214)  class_acc: 0.2917 (0.2192)  loss_scale: 32768.0000 (36212.7680)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.0008  max mem: 15572
Epoch: [8]  [1160/1404]  eta: 0:02:31  lr: 0.000091  min_lr: 0.000001  loss: 4.1031 (4.2204)  class_acc: 0.2917 (0.2198)  loss_scale: 32768.0000 (36183.0973)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0010  max mem: 15572
Epoch: [8]  [1170/1404]  eta: 0:02:25  lr: 0.000091  min_lr: 0.000001  loss: 4.1031 (4.2200)  class_acc: 0.2917 (0.2202)  loss_scale: 32768.0000 (36153.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0007  max mem: 15572
Epoch: [8]  [1180/1404]  eta: 0:02:19  lr: 0.000091  min_lr: 0.000001  loss: 4.1688 (4.2205)  class_acc: 0.2500 (0.2204)  loss_scale: 32768.0000 (36125.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0006  max mem: 15572
Epoch: [8]  [1190/1404]  eta: 0:02:12  lr: 0.000091  min_lr: 0.000001  loss: 4.2405 (4.2205)  class_acc: 0.2083 (0.2204)  loss_scale: 32768.0000 (36097.0747)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0039  max mem: 15572
Epoch: [8]  [1200/1404]  eta: 0:02:06  lr: 0.000091  min_lr: 0.000001  loss: 4.0705 (4.2199)  class_acc: 0.2083 (0.2208)  loss_scale: 32768.0000 (36069.3555)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0039  max mem: 15572
[2025-01-17 10:09:12,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:09:12,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:09:12,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:09:12,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1210/1404]  eta: 0:02:00  lr: 0.000091  min_lr: 0.000001  loss: 3.9910 (4.2180)  class_acc: 0.2917 (0.2214)  loss_scale: 32768.0000 (36096.2114)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0092  max mem: 15572
[2025-01-17 10:09:15,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12444
[2025-01-17 10:09:15,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:09:15,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:09:15,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12444
[2025-01-17 10:09:15,374] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [8]  [1220/1404]  eta: 0:01:54  lr: 0.000091  min_lr: 0.000001  loss: 4.0082 (4.2167)  class_acc: 0.2917 (0.2220)  loss_scale: 32768.0000 (36095.7903)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0092  max mem: 15572
Epoch: [8]  [1230/1404]  eta: 0:01:48  lr: 0.000091  min_lr: 0.000001  loss: 4.2238 (4.2168)  class_acc: 0.2083 (0.2217)  loss_scale: 32768.0000 (36068.7571)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0485  max mem: 15572
Epoch: [8]  [1240/1404]  eta: 0:01:41  lr: 0.000091  min_lr: 0.000001  loss: 4.2347 (4.2177)  class_acc: 0.2083 (0.2217)  loss_scale: 32768.0000 (36042.1595)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.0654  max mem: 15572
Epoch: [8]  [1250/1404]  eta: 0:01:35  lr: 0.000091  min_lr: 0.000001  loss: 4.1592 (4.2171)  class_acc: 0.2083 (0.2222)  loss_scale: 32768.0000 (36015.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0176  max mem: 15572
Epoch: [8]  [1260/1404]  eta: 0:01:29  lr: 0.000091  min_lr: 0.000001  loss: 4.1477 (4.2164)  class_acc: 0.2500 (0.2222)  loss_scale: 32768.0000 (35990.2300)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0717  max mem: 15572
Epoch: [8]  [1270/1404]  eta: 0:01:23  lr: 0.000091  min_lr: 0.000001  loss: 4.1964 (4.2169)  class_acc: 0.2083 (0.2219)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.1380  max mem: 15572
Epoch: [8]  [1280/1404]  eta: 0:01:16  lr: 0.000091  min_lr: 0.000001  loss: 4.3410 (4.2177)  class_acc: 0.2083 (0.2222)  loss_scale: 32768.0000 (35939.9219)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1146  max mem: 15572
Epoch: [8]  [1290/1404]  eta: 0:01:10  lr: 0.000091  min_lr: 0.000001  loss: 4.3097 (4.2182)  class_acc: 0.2083 (0.2222)  loss_scale: 32768.0000 (35915.3524)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.0750  max mem: 15572
Epoch: [8]  [1300/1404]  eta: 0:01:04  lr: 0.000091  min_lr: 0.000001  loss: 4.3317 (4.2187)  class_acc: 0.2083 (0.2219)  loss_scale: 32768.0000 (35891.1606)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0769  max mem: 15572
Epoch: [8]  [1310/1404]  eta: 0:00:58  lr: 0.000091  min_lr: 0.000001  loss: 4.2245 (4.2177)  class_acc: 0.2083 (0.2219)  loss_scale: 32768.0000 (35867.3379)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1202  max mem: 15572
Epoch: [8]  [1320/1404]  eta: 0:00:51  lr: 0.000091  min_lr: 0.000001  loss: 4.2355 (4.2180)  class_acc: 0.2083 (0.2217)  loss_scale: 32768.0000 (35843.8759)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0892  max mem: 15572
Epoch: [8]  [1330/1404]  eta: 0:00:45  lr: 0.000091  min_lr: 0.000001  loss: 4.2960 (4.2179)  class_acc: 0.2083 (0.2222)  loss_scale: 32768.0000 (35820.7663)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0986  max mem: 15572
Epoch: [8]  [1340/1404]  eta: 0:00:39  lr: 0.000091  min_lr: 0.000001  loss: 4.2545 (4.2179)  class_acc: 0.2083 (0.2222)  loss_scale: 32768.0000 (35798.0015)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1313  max mem: 15572
[2025-01-17 10:10:32,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:10:32,569] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:10:32,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:10:32,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:10:37,925] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12581
[2025-01-17 10:10:37,925] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:10:37,995] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12581
[2025-01-17 10:10:37,995] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:10:37,995] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1350/1404]  eta: 0:00:33  lr: 0.000091  min_lr: 0.000001  loss: 4.2974 (4.2184)  class_acc: 0.1667 (0.2221)  loss_scale: 32768.0000 (35969.6107)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1203  max mem: 15572
Epoch: [8]  [1360/1404]  eta: 0:00:27  lr: 0.000091  min_lr: 0.000001  loss: 4.2672 (4.2187)  class_acc: 0.2500 (0.2225)  loss_scale: 32768.0000 (35946.0867)  weight_decay: 0.0500 (0.0500)  time: 0.6707  data: 0.1428  max mem: 15572
Epoch: [8]  [1370/1404]  eta: 0:00:21  lr: 0.000091  min_lr: 0.000001  loss: 4.1207 (4.2175)  class_acc: 0.2917 (0.2227)  loss_scale: 32768.0000 (35922.9059)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0745  max mem: 15572
Epoch: [8]  [1380/1404]  eta: 0:00:14  lr: 0.000091  min_lr: 0.000001  loss: 4.0361 (4.2171)  class_acc: 0.1667 (0.2226)  loss_scale: 32768.0000 (35900.0608)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.0291  max mem: 15572
Epoch: [8]  [1390/1404]  eta: 0:00:08  lr: 0.000091  min_lr: 0.000001  loss: 4.0512 (4.2162)  class_acc: 0.1667 (0.2227)  loss_scale: 32768.0000 (35877.5442)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0635  max mem: 15572
Epoch: [8]  [1400/1404]  eta: 0:00:02  lr: 0.000091  min_lr: 0.000001  loss: 4.1538 (4.2165)  class_acc: 0.2500 (0.2229)  loss_scale: 32768.0000 (35855.3490)  weight_decay: 0.0500 (0.0500)  time: 0.4784  data: 0.0350  max mem: 15572
Epoch: [8]  [1403/1404]  eta: 0:00:00  lr: 0.000091  min_lr: 0.000001  loss: 4.2202 (4.2169)  class_acc: 0.2500 (0.2228)  loss_scale: 32768.0000 (35848.7521)  weight_decay: 0.0500 (0.0500)  time: 0.4549  data: 0.0349  max mem: 15572
Epoch: [8] Total time: 0:14:27 (0.6175 s / it)
Averaged stats: lr: 0.000091  min_lr: 0.000001  loss: 4.2202 (4.2254)  class_acc: 0.2500 (0.2241)  loss_scale: 32768.0000 (35848.7521)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:09:21  loss: 1.8601 (1.8601)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.1293  data: 3.7879  max mem: 15572
Val:  [ 10/136]  eta: 0:01:27  loss: 3.0684 (3.0106)  acc1: 33.3333 (29.7980)  acc5: 66.6667 (57.0707)  time: 0.6916  data: 0.4734  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 3.1873 (3.1234)  acc1: 27.7778 (27.5132)  acc5: 61.1111 (57.4074)  time: 0.3969  data: 0.1946  max mem: 15572
Val:  [ 30/136]  eta: 0:00:52  loss: 3.1220 (2.8677)  acc1: 27.7778 (34.9462)  acc5: 61.1111 (62.3656)  time: 0.3828  data: 0.1717  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 1.9812 (2.7932)  acc1: 55.5556 (36.0434)  acc5: 83.3333 (65.7182)  time: 0.3445  data: 0.1319  max mem: 15572
Val:  [ 50/136]  eta: 0:00:38  loss: 2.9124 (2.8731)  acc1: 22.2222 (33.9869)  acc5: 72.2222 (64.7059)  time: 0.3796  data: 0.1766  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 3.0737 (2.9432)  acc1: 16.6667 (31.7851)  acc5: 61.1111 (63.1148)  time: 0.3910  data: 0.1903  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.8907 (2.8966)  acc1: 27.7778 (34.1158)  acc5: 61.1111 (63.7715)  time: 0.3858  data: 0.1852  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.6149 (2.8791)  acc1: 33.3333 (33.6077)  acc5: 66.6667 (65.2263)  time: 0.3491  data: 0.1544  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.8378 (2.8798)  acc1: 22.2222 (32.9670)  acc5: 72.2222 (65.5067)  time: 0.3757  data: 0.1797  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 3.1116 (2.9404)  acc1: 22.2222 (31.7382)  acc5: 61.1111 (63.8064)  time: 0.4036  data: 0.1828  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.9597 (2.9268)  acc1: 33.3333 (32.5826)  acc5: 61.1111 (63.9640)  time: 0.3523  data: 0.1303  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.5015 (2.8613)  acc1: 50.0000 (34.7107)  acc5: 77.7778 (65.5647)  time: 0.3738  data: 0.1663  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.4227 (2.8249)  acc1: 55.5556 (36.1747)  acc5: 77.7778 (65.6913)  time: 0.3273  data: 0.1463  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.4978 (2.8300)  acc1: 50.0000 (35.9541)  acc5: 72.2222 (65.5201)  time: 0.2161  data: 0.0546  max mem: 15572
Val: Total time: 0:00:52 (0.3859 s / it)
* Acc@1 35.463 Acc@5 64.517 loss 2.867
Accuracy of the network on the 4883 val videos: 35.5%
[2025-01-17 10:12:00,630] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 10:12:00,631] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 10:12:00,631] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 10:12:00,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 10:12:02,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 10:12:02,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 35.46%
Epoch: [9]  [   0/1404]  eta: 3:21:07  lr: 0.000091  min_lr: 0.000001  loss: 3.6209 (3.6209)  class_acc: 0.4583 (0.4583)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5953  data: 6.1546  max mem: 15572
Epoch: [9]  [  10/1404]  eta: 0:31:54  lr: 0.000091  min_lr: 0.000001  loss: 4.1204 (4.0585)  class_acc: 0.2500 (0.2538)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3735  data: 0.5601  max mem: 15572
Epoch: [9]  [  20/1404]  eta: 0:23:24  lr: 0.000091  min_lr: 0.000001  loss: 4.1813 (4.2040)  class_acc: 0.2083 (0.2163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0007  max mem: 15572
Epoch: [9]  [  30/1404]  eta: 0:20:06  lr: 0.000091  min_lr: 0.000001  loss: 4.2716 (4.1970)  class_acc: 0.2083 (0.2325)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0094  max mem: 15572
Epoch: [9]  [  40/1404]  eta: 0:18:15  lr: 0.000091  min_lr: 0.000001  loss: 4.2716 (4.2030)  class_acc: 0.1667 (0.2195)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.0097  max mem: 15572
Epoch: [9]  [  50/1404]  eta: 0:17:24  lr: 0.000091  min_lr: 0.000001  loss: 4.2817 (4.1989)  class_acc: 0.1667 (0.2230)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0009  max mem: 15572
Epoch: [9]  [  60/1404]  eta: 0:16:23  lr: 0.000091  min_lr: 0.000001  loss: 4.2837 (4.2065)  class_acc: 0.1667 (0.2165)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.0007  max mem: 15572
Epoch: [9]  [  70/1404]  eta: 0:15:55  lr: 0.000091  min_lr: 0.000001  loss: 4.2419 (4.2113)  class_acc: 0.1250 (0.2148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.0011  max mem: 15572
[2025-01-17 10:12:55,332] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:12:55,332] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:12:55,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:12:55,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [  80/1404]  eta: 0:15:18  lr: 0.000091  min_lr: 0.000001  loss: 4.2419 (4.2265)  class_acc: 0.2083 (0.2135)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.0011  max mem: 15572
[2025-01-17 10:13:01,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12720
[2025-01-17 10:13:01,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:13:01,233] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12720
[2025-01-17 10:13:01,233] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:13:01,234] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [  90/1404]  eta: 0:14:55  lr: 0.000091  min_lr: 0.000001  loss: 4.2985 (4.2284)  class_acc: 0.2083 (0.2125)  loss_scale: 32768.0000 (36368.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0009  max mem: 15572
Epoch: [9]  [ 100/1404]  eta: 0:14:53  lr: 0.000091  min_lr: 0.000001  loss: 4.2206 (4.2167)  class_acc: 0.2083 (0.2186)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.0476  max mem: 15572
Epoch: [9]  [ 110/1404]  eta: 0:14:33  lr: 0.000091  min_lr: 0.000001  loss: 4.1202 (4.2087)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (35720.0721)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0473  max mem: 15572
Epoch: [9]  [ 120/1404]  eta: 0:14:12  lr: 0.000091  min_lr: 0.000001  loss: 4.1396 (4.1984)  class_acc: 0.2500 (0.2238)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0005  max mem: 15572
Epoch: [9]  [ 130/1404]  eta: 0:14:06  lr: 0.000091  min_lr: 0.000001  loss: 4.1894 (4.1980)  class_acc: 0.2500 (0.2239)  loss_scale: 32768.0000 (35269.3740)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.0474  max mem: 15572
Epoch: [9]  [ 140/1404]  eta: 0:13:49  lr: 0.000091  min_lr: 0.000001  loss: 4.1359 (4.1901)  class_acc: 0.2500 (0.2240)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0593  max mem: 15572
Epoch: [9]  [ 150/1404]  eta: 0:13:42  lr: 0.000091  min_lr: 0.000001  loss: 4.1271 (4.1903)  class_acc: 0.2083 (0.2238)  loss_scale: 32768.0000 (34938.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0732  max mem: 15572
Epoch: [9]  [ 160/1404]  eta: 0:13:39  lr: 0.000091  min_lr: 0.000001  loss: 4.1264 (4.1843)  class_acc: 0.2083 (0.2246)  loss_scale: 32768.0000 (34803.2795)  weight_decay: 0.0500 (0.0500)  time: 0.6727  data: 0.0614  max mem: 15572
Epoch: [9]  [ 170/1404]  eta: 0:13:23  lr: 0.000091  min_lr: 0.000001  loss: 3.9450 (4.1675)  class_acc: 0.2083 (0.2298)  loss_scale: 32768.0000 (34684.2573)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.0008  max mem: 15572
Epoch: [9]  [ 180/1404]  eta: 0:13:16  lr: 0.000091  min_lr: 0.000001  loss: 4.1313 (4.1754)  class_acc: 0.2083 (0.2279)  loss_scale: 32768.0000 (34578.3867)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0009  max mem: 15572
Epoch: [9]  [ 190/1404]  eta: 0:13:03  lr: 0.000091  min_lr: 0.000001  loss: 4.2314 (4.1728)  class_acc: 0.2083 (0.2284)  loss_scale: 32768.0000 (34483.6021)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0008  max mem: 15572
Epoch: [9]  [ 200/1404]  eta: 0:12:50  lr: 0.000091  min_lr: 0.000001  loss: 4.2036 (4.1721)  class_acc: 0.2500 (0.2307)  loss_scale: 32768.0000 (34398.2488)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0007  max mem: 15572
Epoch: [9]  [ 210/1404]  eta: 0:12:42  lr: 0.000091  min_lr: 0.000001  loss: 4.2519 (4.1763)  class_acc: 0.2500 (0.2318)  loss_scale: 32768.0000 (34320.9858)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0008  max mem: 15572
[2025-01-17 10:14:20,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:14:20,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:14:20,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:14:21,000] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 220/1404]  eta: 0:12:41  lr: 0.000091  min_lr: 0.000001  loss: 4.3522 (4.1821)  class_acc: 0.2083 (0.2319)  loss_scale: 32768.0000 (35436.8869)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0007  max mem: 15572
[2025-01-17 10:14:27,940] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12862
[2025-01-17 10:14:27,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:14:27,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12862
[2025-01-17 10:14:27,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:14:27,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 230/1404]  eta: 0:12:31  lr: 0.000091  min_lr: 0.000001  loss: 4.0957 (4.1735)  class_acc: 0.2500 (0.2343)  loss_scale: 65536.0000 (36030.6147)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0007  max mem: 15572
Epoch: [9]  [ 240/1404]  eta: 0:12:23  lr: 0.000091  min_lr: 0.000001  loss: 4.0761 (4.1684)  class_acc: 0.2500 (0.2339)  loss_scale: 32768.0000 (35895.2365)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0009  max mem: 15572
Epoch: [9]  [ 250/1404]  eta: 0:12:15  lr: 0.000090  min_lr: 0.000001  loss: 4.0044 (4.1656)  class_acc: 0.2083 (0.2324)  loss_scale: 32768.0000 (35770.6454)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0009  max mem: 15572
Epoch: [9]  [ 260/1404]  eta: 0:12:09  lr: 0.000090  min_lr: 0.000001  loss: 4.0872 (4.1634)  class_acc: 0.1667 (0.2320)  loss_scale: 32768.0000 (35655.6015)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0008  max mem: 15572
Epoch: [9]  [ 270/1404]  eta: 0:12:01  lr: 0.000090  min_lr: 0.000001  loss: 4.1967 (4.1641)  class_acc: 0.2500 (0.2326)  loss_scale: 32768.0000 (35549.0480)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0007  max mem: 15572
Epoch: [9]  [ 280/1404]  eta: 0:11:52  lr: 0.000090  min_lr: 0.000001  loss: 4.2037 (4.1686)  class_acc: 0.2500 (0.2341)  loss_scale: 32768.0000 (35450.0783)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.0006  max mem: 15572
Epoch: [9]  [ 290/1404]  eta: 0:11:48  lr: 0.000090  min_lr: 0.000001  loss: 4.4050 (4.1775)  class_acc: 0.1667 (0.2320)  loss_scale: 32768.0000 (35357.9107)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0008  max mem: 15572
Epoch: [9]  [ 300/1404]  eta: 0:11:44  lr: 0.000090  min_lr: 0.000001  loss: 4.3295 (4.1759)  class_acc: 0.1667 (0.2315)  loss_scale: 32768.0000 (35271.8671)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.0008  max mem: 15572
Epoch: [9]  [ 310/1404]  eta: 0:11:37  lr: 0.000090  min_lr: 0.000001  loss: 4.1617 (4.1741)  class_acc: 0.2083 (0.2318)  loss_scale: 32768.0000 (35191.3569)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0007  max mem: 15572
Epoch: [9]  [ 320/1404]  eta: 0:11:31  lr: 0.000090  min_lr: 0.000001  loss: 4.1184 (4.1751)  class_acc: 0.2500 (0.2322)  loss_scale: 32768.0000 (35115.8629)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0007  max mem: 15572
Epoch: [9]  [ 330/1404]  eta: 0:11:26  lr: 0.000090  min_lr: 0.000001  loss: 4.2412 (4.1799)  class_acc: 0.2500 (0.2333)  loss_scale: 32768.0000 (35044.9305)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0006  max mem: 15572
Epoch: [9]  [ 340/1404]  eta: 0:11:15  lr: 0.000090  min_lr: 0.000001  loss: 4.3397 (4.1809)  class_acc: 0.2917 (0.2333)  loss_scale: 32768.0000 (34978.1584)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0007  max mem: 15572
Epoch: [9]  [ 350/1404]  eta: 0:11:06  lr: 0.000090  min_lr: 0.000001  loss: 4.2359 (4.1810)  class_acc: 0.2500 (0.2342)  loss_scale: 32768.0000 (34915.1909)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0006  max mem: 15572
[2025-01-17 10:15:47,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:15:47,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:15:47,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:15:47,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 360/1404]  eta: 0:10:58  lr: 0.000090  min_lr: 0.000001  loss: 4.1725 (4.1745)  class_acc: 0.2500 (0.2348)  loss_scale: 32768.0000 (35400.3324)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0007  max mem: 15572
[2025-01-17 10:15:53,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=71, lr=[8.755895920446586e-07, 8.755895920446586e-07, 1.2508422743495125e-06, 1.2508422743495125e-06, 1.786917534785018e-06, 1.786917534785018e-06, 2.5527393354071685e-06, 2.5527393354071685e-06, 3.6467704791530984e-06, 3.6467704791530984e-06, 5.209672113075855e-06, 5.209672113075855e-06, 7.442388732965508e-06, 7.442388732965508e-06, 1.0631983904236441e-05, 1.0631983904236441e-05, 1.5188548434623486e-05, 1.5188548434623486e-05, 2.169792633517641e-05, 2.169792633517641e-05, 3.0997037621680587e-05, 3.0997037621680587e-05, 4.428148231668656e-05, 4.428148231668656e-05, 6.325926045240937e-05, 6.325926045240937e-05, 9.037037207487054e-05, 9.037037207487054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:15:53,054] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=44.54985071381179, CurrSamplesPerSec=51.657886755620424, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 370/1404]  eta: 0:10:53  lr: 0.000090  min_lr: 0.000001  loss: 4.0730 (4.1752)  class_acc: 0.2500 (0.2355)  loss_scale: 65536.0000 (36212.6146)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0007  max mem: 15572
Epoch: [9]  [ 380/1404]  eta: 0:10:48  lr: 0.000090  min_lr: 0.000001  loss: 4.2532 (4.1791)  class_acc: 0.2083 (0.2341)  loss_scale: 65536.0000 (36982.2572)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.0007  max mem: 15572
[2025-01-17 10:16:06,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13022
[2025-01-17 10:16:06,410] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:16:06,509] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13022
[2025-01-17 10:16:06,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:16:06,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 390/1404]  eta: 0:10:38  lr: 0.000090  min_lr: 0.000001  loss: 4.3246 (4.1805)  class_acc: 0.2083 (0.2331)  loss_scale: 65536.0000 (37293.5038)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.0009  max mem: 15572
Epoch: [9]  [ 400/1404]  eta: 0:10:32  lr: 0.000090  min_lr: 0.000001  loss: 4.2743 (4.1816)  class_acc: 0.1667 (0.2325)  loss_scale: 32768.0000 (37180.6484)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0009  max mem: 15572
Epoch: [9]  [ 410/1404]  eta: 0:10:25  lr: 0.000090  min_lr: 0.000001  loss: 4.2810 (4.1845)  class_acc: 0.1667 (0.2314)  loss_scale: 32768.0000 (37073.2847)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0009  max mem: 15572
Epoch: [9]  [ 420/1404]  eta: 0:10:19  lr: 0.000090  min_lr: 0.000001  loss: 4.3471 (4.1876)  class_acc: 0.1667 (0.2307)  loss_scale: 32768.0000 (36971.0214)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0010  max mem: 15572
Epoch: [9]  [ 430/1404]  eta: 0:10:11  lr: 0.000090  min_lr: 0.000001  loss: 4.0261 (4.1854)  class_acc: 0.2083 (0.2309)  loss_scale: 32768.0000 (36873.5035)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0008  max mem: 15572
Epoch: [9]  [ 440/1404]  eta: 0:10:04  lr: 0.000090  min_lr: 0.000001  loss: 4.0136 (4.1836)  class_acc: 0.2083 (0.2312)  loss_scale: 32768.0000 (36780.4082)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0008  max mem: 15572
Epoch: [9]  [ 450/1404]  eta: 0:09:59  lr: 0.000090  min_lr: 0.000001  loss: 4.1078 (4.1779)  class_acc: 0.2500 (0.2328)  loss_scale: 32768.0000 (36691.4412)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0008  max mem: 15572
Epoch: [9]  [ 460/1404]  eta: 0:09:52  lr: 0.000090  min_lr: 0.000001  loss: 4.1078 (4.1794)  class_acc: 0.2083 (0.2320)  loss_scale: 32768.0000 (36606.3341)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.0010  max mem: 15572
Epoch: [9]  [ 470/1404]  eta: 0:09:44  lr: 0.000090  min_lr: 0.000001  loss: 4.2228 (4.1809)  class_acc: 0.2083 (0.2322)  loss_scale: 32768.0000 (36524.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0009  max mem: 15572
Epoch: [9]  [ 480/1404]  eta: 0:09:37  lr: 0.000090  min_lr: 0.000001  loss: 4.0580 (4.1778)  class_acc: 0.2500 (0.2329)  loss_scale: 32768.0000 (36446.7360)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0008  max mem: 15572
Epoch: [9]  [ 490/1404]  eta: 0:09:31  lr: 0.000090  min_lr: 0.000001  loss: 4.2716 (4.1827)  class_acc: 0.2500 (0.2324)  loss_scale: 32768.0000 (36371.8126)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.0010  max mem: 15572
Epoch: [9]  [ 500/1404]  eta: 0:09:24  lr: 0.000090  min_lr: 0.000001  loss: 4.2925 (4.1828)  class_acc: 0.1667 (0.2311)  loss_scale: 32768.0000 (36299.8802)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0008  max mem: 15572
Epoch: [9]  [ 510/1404]  eta: 0:09:17  lr: 0.000090  min_lr: 0.000001  loss: 4.2288 (4.1840)  class_acc: 0.1667 (0.2308)  loss_scale: 32768.0000 (36230.7632)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0007  max mem: 15572
[2025-01-17 10:17:24,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:17:24,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:17:24,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:17:24,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:17:24,737] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13152
[2025-01-17 10:17:24,738] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:17:24,738] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:17:24,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13152
[2025-01-17 10:17:24,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [ 520/1404]  eta: 0:09:10  lr: 0.000090  min_lr: 0.000001  loss: 4.2288 (4.1861)  class_acc: 0.1667 (0.2298)  loss_scale: 32768.0000 (36227.1939)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0007  max mem: 15572
Epoch: [9]  [ 530/1404]  eta: 0:09:03  lr: 0.000090  min_lr: 0.000001  loss: 4.1232 (4.1831)  class_acc: 0.1667 (0.2303)  loss_scale: 32768.0000 (36162.0490)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.0497  max mem: 15572
Epoch: [9]  [ 540/1404]  eta: 0:08:58  lr: 0.000090  min_lr: 0.000001  loss: 4.1114 (4.1832)  class_acc: 0.2500 (0.2304)  loss_scale: 32768.0000 (36099.3124)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1391  max mem: 15572
Epoch: [9]  [ 550/1404]  eta: 0:08:52  lr: 0.000090  min_lr: 0.000001  loss: 4.3060 (4.1832)  class_acc: 0.2083 (0.2305)  loss_scale: 32768.0000 (36038.8530)  weight_decay: 0.0500 (0.0500)  time: 0.6697  data: 0.1640  max mem: 15572
Epoch: [9]  [ 560/1404]  eta: 0:08:45  lr: 0.000090  min_lr: 0.000001  loss: 4.1455 (4.1817)  class_acc: 0.2083 (0.2312)  loss_scale: 32768.0000 (35980.5490)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0777  max mem: 15572
Epoch: [9]  [ 570/1404]  eta: 0:08:39  lr: 0.000090  min_lr: 0.000001  loss: 4.1599 (4.1835)  class_acc: 0.2500 (0.2312)  loss_scale: 32768.0000 (35924.2872)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0080  max mem: 15572
Epoch: [9]  [ 580/1404]  eta: 0:08:33  lr: 0.000090  min_lr: 0.000001  loss: 4.2436 (4.1853)  class_acc: 0.2083 (0.2301)  loss_scale: 32768.0000 (35869.9621)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0048  max mem: 15572
Epoch: [9]  [ 590/1404]  eta: 0:08:26  lr: 0.000090  min_lr: 0.000001  loss: 4.2705 (4.1865)  class_acc: 0.1667 (0.2300)  loss_scale: 32768.0000 (35817.4755)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0010  max mem: 15572
Epoch: [9]  [ 600/1404]  eta: 0:08:20  lr: 0.000090  min_lr: 0.000001  loss: 4.1135 (4.1854)  class_acc: 0.2500 (0.2307)  loss_scale: 32768.0000 (35766.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0009  max mem: 15572
Epoch: [9]  [ 610/1404]  eta: 0:08:14  lr: 0.000090  min_lr: 0.000001  loss: 4.0270 (4.1830)  class_acc: 0.2500 (0.2312)  loss_scale: 32768.0000 (35717.6563)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0007  max mem: 15572
Epoch: [9]  [ 620/1404]  eta: 0:08:07  lr: 0.000090  min_lr: 0.000001  loss: 4.2530 (4.1862)  class_acc: 0.2083 (0.2302)  loss_scale: 32768.0000 (35670.1578)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0009  max mem: 15572
Epoch: [9]  [ 630/1404]  eta: 0:08:01  lr: 0.000090  min_lr: 0.000001  loss: 4.2075 (4.1861)  class_acc: 0.1667 (0.2299)  loss_scale: 32768.0000 (35624.1648)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0007  max mem: 15572
Epoch: [9]  [ 640/1404]  eta: 0:07:56  lr: 0.000090  min_lr: 0.000001  loss: 4.0749 (4.1846)  class_acc: 0.2500 (0.2310)  loss_scale: 32768.0000 (35579.6069)  weight_decay: 0.0500 (0.0500)  time: 0.6905  data: 0.0009  max mem: 15572
[2025-01-17 10:18:44,744] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:18:44,745] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:18:44,746] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:18:44,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:18:47,670] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13285
[2025-01-17 10:18:47,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:18:47,674] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13285
[2025-01-17 10:18:47,675] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:18:47,675] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 650/1404]  eta: 0:07:49  lr: 0.000090  min_lr: 0.000001  loss: 4.1059 (4.1834)  class_acc: 0.2500 (0.2307)  loss_scale: 32768.0000 (35737.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0011  max mem: 15572
Epoch: [9]  [ 660/1404]  eta: 0:07:43  lr: 0.000090  min_lr: 0.000001  loss: 4.1970 (4.1851)  class_acc: 0.2083 (0.2303)  loss_scale: 32768.0000 (35692.8290)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0007  max mem: 15572
Epoch: [9]  [ 670/1404]  eta: 0:07:37  lr: 0.000090  min_lr: 0.000001  loss: 4.1970 (4.1833)  class_acc: 0.2083 (0.2309)  loss_scale: 32768.0000 (35649.2399)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0007  max mem: 15572
Epoch: [9]  [ 680/1404]  eta: 0:07:30  lr: 0.000090  min_lr: 0.000001  loss: 3.9716 (4.1824)  class_acc: 0.2917 (0.2315)  loss_scale: 32768.0000 (35606.9310)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0009  max mem: 15572
Epoch: [9]  [ 690/1404]  eta: 0:07:24  lr: 0.000090  min_lr: 0.000001  loss: 4.2555 (4.1852)  class_acc: 0.2083 (0.2309)  loss_scale: 32768.0000 (35565.8466)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0010  max mem: 15572
Epoch: [9]  [ 700/1404]  eta: 0:07:17  lr: 0.000090  min_lr: 0.000001  loss: 4.2708 (4.1852)  class_acc: 0.2083 (0.2310)  loss_scale: 32768.0000 (35525.9344)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0009  max mem: 15572
Epoch: [9]  [ 710/1404]  eta: 0:07:11  lr: 0.000090  min_lr: 0.000001  loss: 4.2333 (4.1881)  class_acc: 0.2083 (0.2305)  loss_scale: 32768.0000 (35487.1449)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.0008  max mem: 15572
Epoch: [9]  [ 720/1404]  eta: 0:07:04  lr: 0.000090  min_lr: 0.000001  loss: 4.2333 (4.1883)  class_acc: 0.2083 (0.2300)  loss_scale: 32768.0000 (35449.4313)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0011  max mem: 15572
Epoch: [9]  [ 730/1404]  eta: 0:06:58  lr: 0.000090  min_lr: 0.000001  loss: 4.1929 (4.1880)  class_acc: 0.2083 (0.2297)  loss_scale: 32768.0000 (35412.7497)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0012  max mem: 15572
Epoch: [9]  [ 740/1404]  eta: 0:06:52  lr: 0.000090  min_lr: 0.000001  loss: 4.2165 (4.1896)  class_acc: 0.2500 (0.2299)  loss_scale: 32768.0000 (35377.0580)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0011  max mem: 15572
Epoch: [9]  [ 750/1404]  eta: 0:06:46  lr: 0.000090  min_lr: 0.000001  loss: 4.2165 (4.1886)  class_acc: 0.2500 (0.2303)  loss_scale: 32768.0000 (35342.3169)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0009  max mem: 15572
Epoch: [9]  [ 760/1404]  eta: 0:06:40  lr: 0.000090  min_lr: 0.000001  loss: 4.0749 (4.1856)  class_acc: 0.2500 (0.2307)  loss_scale: 32768.0000 (35308.4888)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.0008  max mem: 15572
Epoch: [9]  [ 770/1404]  eta: 0:06:35  lr: 0.000090  min_lr: 0.000001  loss: 4.0924 (4.1843)  class_acc: 0.2500 (0.2308)  loss_scale: 32768.0000 (35275.5383)  weight_decay: 0.0500 (0.0500)  time: 0.6844  data: 0.0009  max mem: 15572
[2025-01-17 10:20:07,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:20:07,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:20:07,895] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:20:07,895] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 780/1404]  eta: 0:06:28  lr: 0.000090  min_lr: 0.000001  loss: 4.1652 (4.1833)  class_acc: 0.1667 (0.2302)  loss_scale: 32768.0000 (35369.3009)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.0009  max mem: 15572
[2025-01-17 10:20:10,474] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13419
[2025-01-17 10:20:10,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:20:10,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13419
[2025-01-17 10:20:10,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:20:10,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 790/1404]  eta: 0:06:22  lr: 0.000090  min_lr: 0.000001  loss: 4.0696 (4.1824)  class_acc: 0.2083 (0.2302)  loss_scale: 32768.0000 (35419.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0008  max mem: 15572
Epoch: [9]  [ 800/1404]  eta: 0:06:15  lr: 0.000090  min_lr: 0.000001  loss: 4.2326 (4.1852)  class_acc: 0.2083 (0.2292)  loss_scale: 32768.0000 (35386.1673)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0007  max mem: 15572
Epoch: [9]  [ 810/1404]  eta: 0:06:09  lr: 0.000090  min_lr: 0.000001  loss: 4.3603 (4.1877)  class_acc: 0.1667 (0.2288)  loss_scale: 32768.0000 (35353.8841)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.0008  max mem: 15572
Epoch: [9]  [ 820/1404]  eta: 0:06:03  lr: 0.000090  min_lr: 0.000001  loss: 4.2414 (4.1858)  class_acc: 0.2500 (0.2301)  loss_scale: 32768.0000 (35322.3873)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.0008  max mem: 15572
Epoch: [9]  [ 830/1404]  eta: 0:05:57  lr: 0.000090  min_lr: 0.000001  loss: 3.9092 (4.1849)  class_acc: 0.2917 (0.2307)  loss_scale: 32768.0000 (35291.6486)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0008  max mem: 15572
Epoch: [9]  [ 840/1404]  eta: 0:05:51  lr: 0.000090  min_lr: 0.000001  loss: 3.9492 (4.1838)  class_acc: 0.2917 (0.2319)  loss_scale: 32768.0000 (35261.6409)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0009  max mem: 15572
Epoch: [9]  [ 850/1404]  eta: 0:05:44  lr: 0.000090  min_lr: 0.000001  loss: 3.9872 (4.1819)  class_acc: 0.2917 (0.2324)  loss_scale: 32768.0000 (35232.3384)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0007  max mem: 15572
Epoch: [9]  [ 860/1404]  eta: 0:05:38  lr: 0.000090  min_lr: 0.000001  loss: 4.1058 (4.1834)  class_acc: 0.2083 (0.2323)  loss_scale: 32768.0000 (35203.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0007  max mem: 15572
Epoch: [9]  [ 870/1404]  eta: 0:05:31  lr: 0.000090  min_lr: 0.000001  loss: 4.3669 (4.1849)  class_acc: 0.1667 (0.2316)  loss_scale: 32768.0000 (35175.7520)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0008  max mem: 15572
Epoch: [9]  [ 880/1404]  eta: 0:05:25  lr: 0.000090  min_lr: 0.000001  loss: 4.3161 (4.1860)  class_acc: 0.1667 (0.2319)  loss_scale: 32768.0000 (35148.4222)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.0008  max mem: 15572
Epoch: [9]  [ 890/1404]  eta: 0:05:18  lr: 0.000090  min_lr: 0.000001  loss: 4.2311 (4.1848)  class_acc: 0.1667 (0.2314)  loss_scale: 32768.0000 (35121.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0010  max mem: 15572
Epoch: [9]  [ 900/1404]  eta: 0:05:12  lr: 0.000090  min_lr: 0.000001  loss: 4.2270 (4.1856)  class_acc: 0.2083 (0.2314)  loss_scale: 32768.0000 (35095.5827)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0010  max mem: 15572
Epoch: [9]  [ 910/1404]  eta: 0:05:06  lr: 0.000090  min_lr: 0.000001  loss: 4.1984 (4.1838)  class_acc: 0.2500 (0.2320)  loss_scale: 32768.0000 (35070.0329)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.0007  max mem: 15572
[2025-01-17 10:21:29,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:21:29,048] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:21:29,066] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:21:29,067] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:21:30,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13550
[2025-01-17 10:21:30,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:21:30,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13550
[2025-01-17 10:21:30,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:21:30,134] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 920/1404]  eta: 0:05:00  lr: 0.000090  min_lr: 0.000001  loss: 4.1662 (4.1835)  class_acc: 0.2500 (0.2322)  loss_scale: 32768.0000 (35116.1954)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0005  max mem: 15572
Epoch: [9]  [ 930/1404]  eta: 0:04:54  lr: 0.000090  min_lr: 0.000001  loss: 4.1408 (4.1821)  class_acc: 0.2083 (0.2322)  loss_scale: 32768.0000 (35090.9731)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0008  max mem: 15572
Epoch: [9]  [ 940/1404]  eta: 0:04:48  lr: 0.000090  min_lr: 0.000001  loss: 4.0550 (4.1811)  class_acc: 0.1667 (0.2320)  loss_scale: 32768.0000 (35066.2869)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0008  max mem: 15572
Epoch: [9]  [ 950/1404]  eta: 0:04:41  lr: 0.000090  min_lr: 0.000001  loss: 4.1135 (4.1813)  class_acc: 0.1667 (0.2316)  loss_scale: 32768.0000 (35042.1199)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0006  max mem: 15572
Epoch: [9]  [ 960/1404]  eta: 0:04:35  lr: 0.000090  min_lr: 0.000001  loss: 4.1301 (4.1807)  class_acc: 0.2083 (0.2315)  loss_scale: 32768.0000 (35018.4558)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0006  max mem: 15572
Epoch: [9]  [ 970/1404]  eta: 0:04:28  lr: 0.000090  min_lr: 0.000001  loss: 4.2269 (4.1818)  class_acc: 0.2083 (0.2315)  loss_scale: 32768.0000 (34995.2791)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0011  max mem: 15572
Epoch: [9]  [ 980/1404]  eta: 0:04:22  lr: 0.000090  min_lr: 0.000001  loss: 4.3038 (4.1819)  class_acc: 0.2083 (0.2317)  loss_scale: 32768.0000 (34972.5749)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0011  max mem: 15572
Epoch: [9]  [ 990/1404]  eta: 0:04:16  lr: 0.000090  min_lr: 0.000001  loss: 4.2763 (4.1835)  class_acc: 0.2083 (0.2313)  loss_scale: 32768.0000 (34950.3290)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.0009  max mem: 15572
Epoch: [9]  [1000/1404]  eta: 0:04:09  lr: 0.000090  min_lr: 0.000001  loss: 4.3189 (4.1847)  class_acc: 0.1667 (0.2311)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.0011  max mem: 15572
Epoch: [9]  [1010/1404]  eta: 0:04:03  lr: 0.000090  min_lr: 0.000001  loss: 4.3044 (4.1830)  class_acc: 0.2083 (0.2318)  loss_scale: 32768.0000 (34907.1573)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.0010  max mem: 15572
Epoch: [9]  [1020/1404]  eta: 0:03:57  lr: 0.000090  min_lr: 0.000001  loss: 3.8971 (4.1810)  class_acc: 0.2500 (0.2321)  loss_scale: 32768.0000 (34886.2057)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0726  max mem: 15572
Epoch: [9]  [1030/1404]  eta: 0:03:51  lr: 0.000090  min_lr: 0.000001  loss: 4.2597 (4.1821)  class_acc: 0.2083 (0.2319)  loss_scale: 32768.0000 (34865.6605)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1024  max mem: 15572
Epoch: [9]  [1040/1404]  eta: 0:03:44  lr: 0.000090  min_lr: 0.000001  loss: 4.4354 (4.1831)  class_acc: 0.1667 (0.2315)  loss_scale: 32768.0000 (34845.5101)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0637  max mem: 15572
[2025-01-17 10:22:47,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:22:47,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:22:47,467] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:22:47,467] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1050/1404]  eta: 0:03:38  lr: 0.000090  min_lr: 0.000001  loss: 4.1978 (4.1832)  class_acc: 0.1667 (0.2316)  loss_scale: 32768.0000 (35075.1665)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0880  max mem: 15572
[2025-01-17 10:22:53,214] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13688
[2025-01-17 10:22:53,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:22:53,230] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13688
[2025-01-17 10:22:53,231] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:22:53,231] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1060/1404]  eta: 0:03:32  lr: 0.000090  min_lr: 0.000001  loss: 4.1668 (4.1828)  class_acc: 0.2083 (0.2316)  loss_scale: 32768.0000 (35084.3054)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0550  max mem: 15572
Epoch: [9]  [1070/1404]  eta: 0:03:26  lr: 0.000090  min_lr: 0.000001  loss: 4.0744 (4.1819)  class_acc: 0.2500 (0.2321)  loss_scale: 32768.0000 (35062.6779)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0190  max mem: 15572
Epoch: [9]  [1080/1404]  eta: 0:03:20  lr: 0.000090  min_lr: 0.000001  loss: 4.1365 (4.1829)  class_acc: 0.2500 (0.2320)  loss_scale: 32768.0000 (35041.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.0190  max mem: 15572
Epoch: [9]  [1090/1404]  eta: 0:03:14  lr: 0.000090  min_lr: 0.000001  loss: 4.2126 (4.1831)  class_acc: 0.2083 (0.2323)  loss_scale: 32768.0000 (35020.6123)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0008  max mem: 15572
Epoch: [9]  [1100/1404]  eta: 0:03:08  lr: 0.000089  min_lr: 0.000001  loss: 4.2183 (4.1841)  class_acc: 0.2500 (0.2325)  loss_scale: 32768.0000 (35000.1526)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0016  max mem: 15572
Epoch: [9]  [1110/1404]  eta: 0:03:02  lr: 0.000089  min_lr: 0.000001  loss: 4.2777 (4.1851)  class_acc: 0.2500 (0.2321)  loss_scale: 32768.0000 (34980.0612)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.0016  max mem: 15572
Epoch: [9]  [1120/1404]  eta: 0:02:55  lr: 0.000089  min_lr: 0.000001  loss: 4.2936 (4.1869)  class_acc: 0.2083 (0.2318)  loss_scale: 32768.0000 (34960.3283)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0009  max mem: 15572
Epoch: [9]  [1130/1404]  eta: 0:02:49  lr: 0.000089  min_lr: 0.000001  loss: 4.2002 (4.1871)  class_acc: 0.2083 (0.2318)  loss_scale: 32768.0000 (34940.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.0009  max mem: 15572
Epoch: [9]  [1140/1404]  eta: 0:02:43  lr: 0.000089  min_lr: 0.000001  loss: 4.1883 (4.1884)  class_acc: 0.2083 (0.2313)  loss_scale: 32768.0000 (34921.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0007  max mem: 15572
Epoch: [9]  [1150/1404]  eta: 0:02:37  lr: 0.000089  min_lr: 0.000001  loss: 4.1883 (4.1880)  class_acc: 0.1667 (0.2311)  loss_scale: 32768.0000 (34903.1868)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0008  max mem: 15572
Epoch: [9]  [1160/1404]  eta: 0:02:31  lr: 0.000089  min_lr: 0.000001  loss: 4.1928 (4.1888)  class_acc: 0.2500 (0.2314)  loss_scale: 32768.0000 (34884.7959)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0008  max mem: 15572
Epoch: [9]  [1170/1404]  eta: 0:02:24  lr: 0.000089  min_lr: 0.000001  loss: 4.2180 (4.1885)  class_acc: 0.2917 (0.2317)  loss_scale: 32768.0000 (34866.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.0009  max mem: 15572
Epoch: [9]  [1180/1404]  eta: 0:02:18  lr: 0.000089  min_lr: 0.000001  loss: 4.1254 (4.1878)  class_acc: 0.2500 (0.2322)  loss_scale: 32768.0000 (34848.9483)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0009  max mem: 15572
[2025-01-17 10:24:15,409] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:24:15,410] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:24:15,410] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:24:15,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:24:20,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13825
[2025-01-17 10:24:20,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:24:20,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:24:20,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13825
[2025-01-17 10:24:20,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [1190/1404]  eta: 0:02:12  lr: 0.000089  min_lr: 0.000001  loss: 4.1413 (4.1872)  class_acc: 0.2500 (0.2323)  loss_scale: 32768.0000 (35051.5802)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.0007  max mem: 15572
Epoch: [9]  [1200/1404]  eta: 0:02:06  lr: 0.000089  min_lr: 0.000001  loss: 4.1361 (4.1869)  class_acc: 0.2500 (0.2324)  loss_scale: 32768.0000 (35032.5662)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0010  max mem: 15572
Epoch: [9]  [1210/1404]  eta: 0:02:00  lr: 0.000089  min_lr: 0.000001  loss: 4.2821 (4.1878)  class_acc: 0.2083 (0.2319)  loss_scale: 32768.0000 (35013.8662)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0010  max mem: 15572
Epoch: [9]  [1220/1404]  eta: 0:01:53  lr: 0.000089  min_lr: 0.000001  loss: 4.3918 (4.1887)  class_acc: 0.1667 (0.2319)  loss_scale: 32768.0000 (34995.4726)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0008  max mem: 15572
Epoch: [9]  [1230/1404]  eta: 0:01:47  lr: 0.000089  min_lr: 0.000001  loss: 4.2292 (4.1886)  class_acc: 0.2083 (0.2320)  loss_scale: 32768.0000 (34977.3777)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0369  max mem: 15572
Epoch: [9]  [1240/1404]  eta: 0:01:41  lr: 0.000089  min_lr: 0.000001  loss: 4.1415 (4.1886)  class_acc: 0.2083 (0.2315)  loss_scale: 32768.0000 (34959.5745)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0747  max mem: 15572
Epoch: [9]  [1250/1404]  eta: 0:01:35  lr: 0.000089  min_lr: 0.000001  loss: 4.1415 (4.1861)  class_acc: 0.2917 (0.2323)  loss_scale: 32768.0000 (34942.0560)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0818  max mem: 15572
Epoch: [9]  [1260/1404]  eta: 0:01:29  lr: 0.000089  min_lr: 0.000001  loss: 4.1967 (4.1864)  class_acc: 0.2917 (0.2323)  loss_scale: 32768.0000 (34924.8152)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.0440  max mem: 15572
Epoch: [9]  [1270/1404]  eta: 0:01:22  lr: 0.000089  min_lr: 0.000001  loss: 4.2565 (4.1872)  class_acc: 0.2083 (0.2317)  loss_scale: 32768.0000 (34907.8458)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0008  max mem: 15572
Epoch: [9]  [1280/1404]  eta: 0:01:16  lr: 0.000089  min_lr: 0.000001  loss: 4.3056 (4.1875)  class_acc: 0.1667 (0.2316)  loss_scale: 32768.0000 (34891.1413)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0526  max mem: 15572
Epoch: [9]  [1290/1404]  eta: 0:01:10  lr: 0.000089  min_lr: 0.000001  loss: 4.1546 (4.1863)  class_acc: 0.2083 (0.2320)  loss_scale: 32768.0000 (34874.6956)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0524  max mem: 15572
Epoch: [9]  [1300/1404]  eta: 0:01:04  lr: 0.000089  min_lr: 0.000001  loss: 4.0951 (4.1866)  class_acc: 0.2083 (0.2317)  loss_scale: 32768.0000 (34858.5027)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0007  max mem: 15572
Epoch: [9]  [1310/1404]  eta: 0:00:58  lr: 0.000089  min_lr: 0.000001  loss: 4.1726 (4.1862)  class_acc: 0.2083 (0.2321)  loss_scale: 32768.0000 (34842.5568)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.0010  max mem: 15572
[2025-01-17 10:25:39,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:25:39,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:25:39,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:25:39,487] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1320/1404]  eta: 0:00:52  lr: 0.000089  min_lr: 0.000001  loss: 4.1381 (4.1848)  class_acc: 0.2500 (0.2323)  loss_scale: 32768.0000 (34901.2687)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0010  max mem: 15572
[2025-01-17 10:25:42,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13959
[2025-01-17 10:25:42,168] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:25:42,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:25:42,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13959
[2025-01-17 10:25:42,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [9]  [1330/1404]  eta: 0:00:45  lr: 0.000089  min_lr: 0.000001  loss: 4.0298 (4.1836)  class_acc: 0.2500 (0.2323)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0008  max mem: 15572
Epoch: [9]  [1340/1404]  eta: 0:00:39  lr: 0.000089  min_lr: 0.000001  loss: 4.1245 (4.1829)  class_acc: 0.2083 (0.2323)  loss_scale: 32768.0000 (34918.3236)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0008  max mem: 15572
Epoch: [9]  [1350/1404]  eta: 0:00:33  lr: 0.000089  min_lr: 0.000001  loss: 4.2150 (4.1833)  class_acc: 0.2083 (0.2321)  loss_scale: 32768.0000 (34902.4071)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0007  max mem: 15572
Epoch: [9]  [1360/1404]  eta: 0:00:27  lr: 0.000089  min_lr: 0.000001  loss: 4.2150 (4.1833)  class_acc: 0.2083 (0.2324)  loss_scale: 32768.0000 (34886.7245)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0006  max mem: 15572
[2025-01-17 10:26:06,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=79, lr=[8.63917353010542e-07, 8.63917353010542e-07, 1.2341676471579173e-06, 1.2341676471579173e-06, 1.763096638797025e-06, 1.763096638797025e-06, 2.51870948399575e-06, 2.51870948399575e-06, 3.5981564057082143e-06, 3.5981564057082143e-06, 5.140223436726021e-06, 5.140223436726021e-06, 7.34317633818003e-06, 7.34317633818003e-06, 1.0490251911685759e-05, 1.0490251911685759e-05, 1.4986074159551083e-05, 1.4986074159551083e-05, 2.1408677370787264e-05, 2.1408677370787264e-05, 3.058382481541038e-05, 3.058382481541038e-05, 4.3691178307729115e-05, 4.3691178307729115e-05, 6.241596901104159e-05, 6.241596901104159e-05, 8.916567001577372e-05, 8.916567001577372e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:26:06,467] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=44.36449703150124, CurrSamplesPerSec=49.04129789055272, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1370/1404]  eta: 0:00:21  lr: 0.000089  min_lr: 0.000001  loss: 4.1655 (4.1828)  class_acc: 0.2500 (0.2325)  loss_scale: 32768.0000 (34871.2706)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0006  max mem: 15572
Epoch: [9]  [1380/1404]  eta: 0:00:14  lr: 0.000089  min_lr: 0.000001  loss: 4.2091 (4.1829)  class_acc: 0.2917 (0.2329)  loss_scale: 32768.0000 (34856.0406)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0007  max mem: 15572
Epoch: [9]  [1390/1404]  eta: 0:00:08  lr: 0.000089  min_lr: 0.000001  loss: 4.1664 (4.1826)  class_acc: 0.2083 (0.2329)  loss_scale: 32768.0000 (34841.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0007  max mem: 15572
Epoch: [9]  [1400/1404]  eta: 0:00:02  lr: 0.000089  min_lr: 0.000001  loss: 4.1645 (4.1825)  class_acc: 0.2083 (0.2331)  loss_scale: 32768.0000 (34826.2327)  weight_decay: 0.0500 (0.0500)  time: 0.4495  data: 0.0005  max mem: 15572
Epoch: [9]  [1403/1404]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000001  loss: 4.1645 (4.1826)  class_acc: 0.2083 (0.2331)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)  time: 0.4278  data: 0.0005  max mem: 15572
Epoch: [9] Total time: 0:14:25 (0.6162 s / it)
Averaged stats: lr: 0.000089  min_lr: 0.000001  loss: 4.1645 (4.1828)  class_acc: 0.2083 (0.2342)  loss_scale: 32768.0000 (34821.8348)  weight_decay: 0.0500 (0.0500)
[2025-01-17 10:26:27,291] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-17 10:26:27,293] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-17 10:26:27,293] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-17 10:26:27,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
[2025-01-17 10:26:27,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-17 10:26:27,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/136]  eta: 0:10:52  loss: 1.9412 (1.9412)  acc1: 66.6667 (66.6667)  acc5: 66.6667 (66.6667)  time: 4.7941  data: 4.6034  max mem: 15572
Val:  [ 10/136]  eta: 0:01:30  loss: 2.8045 (2.8761)  acc1: 33.3333 (31.8182)  acc5: 66.6667 (61.1111)  time: 0.7218  data: 0.5085  max mem: 15572
Val:  [ 20/136]  eta: 0:01:05  loss: 2.9779 (2.9658)  acc1: 33.3333 (30.6878)  acc5: 61.1111 (61.3757)  time: 0.3551  data: 0.1486  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.9083 (2.7259)  acc1: 38.8889 (37.8136)  acc5: 66.6667 (66.6667)  time: 0.3827  data: 0.1779  max mem: 15572
Val:  [ 40/136]  eta: 0:00:44  loss: 2.2152 (2.6786)  acc1: 38.8889 (38.2114)  acc5: 72.2222 (67.8862)  time: 0.3484  data: 0.1352  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.5828 (2.7219)  acc1: 38.8889 (38.4532)  acc5: 72.2222 (68.7364)  time: 0.3368  data: 0.1257  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.9433 (2.8117)  acc1: 22.2222 (34.6995)  acc5: 72.2222 (66.3934)  time: 0.3527  data: 0.1410  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.6423 (2.7769)  acc1: 27.7778 (36.6980)  acc5: 61.1111 (66.6667)  time: 0.3809  data: 0.1712  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.5991 (2.7714)  acc1: 44.4444 (36.6941)  acc5: 72.2222 (67.2154)  time: 0.3706  data: 0.1655  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.6642 (2.7738)  acc1: 33.3333 (36.4469)  acc5: 72.2222 (67.2161)  time: 0.3476  data: 0.1401  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.8891 (2.8514)  acc1: 27.7778 (34.3234)  acc5: 61.1111 (65.2365)  time: 0.3972  data: 0.1810  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 3.0357 (2.8533)  acc1: 22.2222 (34.3343)  acc5: 55.5556 (64.9650)  time: 0.4117  data: 0.1803  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.6034 (2.7846)  acc1: 38.8889 (36.1800)  acc5: 72.2222 (66.8044)  time: 0.3673  data: 0.1425  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.8768 (2.7299)  acc1: 61.1111 (37.8287)  acc5: 83.3333 (67.4300)  time: 0.2828  data: 0.0924  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.3608 (2.7393)  acc1: 38.8889 (37.5512)  acc5: 77.7778 (67.4447)  time: 0.1845  data: 0.0236  max mem: 15572
Val: Total time: 0:00:51 (0.3799 s / it)
* Acc@1 36.753 Acc@5 66.892 loss 2.775
Accuracy of the network on the 4883 val videos: 36.8%
[2025-01-17 10:27:19,189] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 10:27:19,191] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 10:27:19,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 10:27:19,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 10:27:21,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 10:27:21,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 36.75%
Epoch: [10]  [   0/1404]  eta: 3:05:30  lr: 0.000089  min_lr: 0.000001  loss: 3.9836 (3.9836)  class_acc: 0.1250 (0.1250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9276  data: 7.0785  max mem: 15572
Epoch: [10]  [  10/1404]  eta: 0:30:12  lr: 0.000089  min_lr: 0.000001  loss: 4.0886 (4.1144)  class_acc: 0.2500 (0.2803)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3001  data: 0.6439  max mem: 15572
Epoch: [10]  [  20/1404]  eta: 0:21:01  lr: 0.000089  min_lr: 0.000001  loss: 4.0040 (4.0504)  class_acc: 0.2500 (0.2738)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 15572
Epoch: [10]  [  30/1404]  eta: 0:18:49  lr: 0.000089  min_lr: 0.000001  loss: 4.1707 (4.1375)  class_acc: 0.2500 (0.2648)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0624  max mem: 15572
Epoch: [10]  [  40/1404]  eta: 0:18:06  lr: 0.000089  min_lr: 0.000001  loss: 4.1707 (4.1294)  class_acc: 0.2500 (0.2632)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.1476  max mem: 15572
[2025-01-17 10:27:58,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:27:58,899] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:27:58,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:27:58,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [  50/1404]  eta: 0:17:10  lr: 0.000089  min_lr: 0.000001  loss: 4.0743 (4.1420)  class_acc: 0.2083 (0.2541)  loss_scale: 32768.0000 (34695.5294)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.1279  max mem: 15572
[2025-01-17 10:28:04,830] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14098
[2025-01-17 10:28:04,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:28:04,841] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14098
[2025-01-17 10:28:04,841] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:28:04,842] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [  60/1404]  eta: 0:16:05  lr: 0.000089  min_lr: 0.000001  loss: 4.1344 (4.1324)  class_acc: 0.2083 (0.2500)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0429  max mem: 15572
Epoch: [10]  [  70/1404]  eta: 0:15:42  lr: 0.000089  min_lr: 0.000001  loss: 4.1234 (4.1103)  class_acc: 0.2917 (0.2623)  loss_scale: 32768.0000 (37383.2113)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.0516  max mem: 15572
Epoch: [10]  [  80/1404]  eta: 0:15:12  lr: 0.000089  min_lr: 0.000001  loss: 4.0378 (4.1113)  class_acc: 0.2917 (0.2685)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0517  max mem: 15572
Epoch: [10]  [  90/1404]  eta: 0:14:44  lr: 0.000089  min_lr: 0.000001  loss: 4.1527 (4.1267)  class_acc: 0.2500 (0.2637)  loss_scale: 32768.0000 (36368.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0116  max mem: 15572
Epoch: [10]  [ 100/1404]  eta: 0:14:37  lr: 0.000089  min_lr: 0.000001  loss: 4.1846 (4.1347)  class_acc: 0.2083 (0.2607)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0870  max mem: 15572
Epoch: [10]  [ 110/1404]  eta: 0:14:38  lr: 0.000089  min_lr: 0.000001  loss: 4.1557 (4.1286)  class_acc: 0.2083 (0.2583)  loss_scale: 32768.0000 (35720.0721)  weight_decay: 0.0500 (0.0500)  time: 0.7051  data: 0.0856  max mem: 15572
Epoch: [10]  [ 120/1404]  eta: 0:14:20  lr: 0.000089  min_lr: 0.000001  loss: 4.1557 (4.1323)  class_acc: 0.2917 (0.2593)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.0101  max mem: 15572
Epoch: [10]  [ 130/1404]  eta: 0:14:04  lr: 0.000089  min_lr: 0.000001  loss: 4.1558 (4.1352)  class_acc: 0.2500 (0.2583)  loss_scale: 32768.0000 (35269.3740)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.0125  max mem: 15572
Epoch: [10]  [ 140/1404]  eta: 0:13:54  lr: 0.000089  min_lr: 0.000001  loss: 4.1571 (4.1411)  class_acc: 0.2083 (0.2547)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0125  max mem: 15572
Epoch: [10]  [ 150/1404]  eta: 0:13:47  lr: 0.000089  min_lr: 0.000001  loss: 4.2454 (4.1549)  class_acc: 0.1667 (0.2489)  loss_scale: 32768.0000 (34938.0662)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0009  max mem: 15572
Epoch: [10]  [ 160/1404]  eta: 0:13:31  lr: 0.000089  min_lr: 0.000001  loss: 4.2802 (4.1649)  class_acc: 0.1667 (0.2451)  loss_scale: 32768.0000 (34803.2795)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0008  max mem: 15572
Epoch: [10]  [ 170/1404]  eta: 0:13:25  lr: 0.000089  min_lr: 0.000001  loss: 4.2319 (4.1518)  class_acc: 0.2500 (0.2498)  loss_scale: 32768.0000 (34684.2573)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.0009  max mem: 15572
Epoch: [10]  [ 180/1404]  eta: 0:13:19  lr: 0.000089  min_lr: 0.000001  loss: 3.9536 (4.1482)  class_acc: 0.3333 (0.2525)  loss_scale: 32768.0000 (34578.3867)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.0009  max mem: 15572
[2025-01-17 10:29:24,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:29:24,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:29:24,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:29:24,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 190/1404]  eta: 0:13:16  lr: 0.000089  min_lr: 0.000001  loss: 4.0184 (4.1420)  class_acc: 0.3333 (0.2528)  loss_scale: 32768.0000 (35169.8429)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.0742  max mem: 15572
[2025-01-17 10:29:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14235
[2025-01-17 10:29:30,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:29:30,048] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14235
[2025-01-17 10:29:30,048] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:29:30,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 200/1404]  eta: 0:13:07  lr: 0.000089  min_lr: 0.000001  loss: 4.0887 (4.1503)  class_acc: 0.2083 (0.2502)  loss_scale: 32768.0000 (35702.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.1213  max mem: 15572
Epoch: [10]  [ 210/1404]  eta: 0:12:58  lr: 0.000089  min_lr: 0.000001  loss: 4.1385 (4.1531)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (35563.3744)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1001  max mem: 15572
Epoch: [10]  [ 220/1404]  eta: 0:12:48  lr: 0.000089  min_lr: 0.000001  loss: 4.1301 (4.1538)  class_acc: 0.2500 (0.2530)  loss_scale: 32768.0000 (35436.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0888  max mem: 15572
Epoch: [10]  [ 230/1404]  eta: 0:12:45  lr: 0.000089  min_lr: 0.000001  loss: 4.2132 (4.1570)  class_acc: 0.2500 (0.2540)  loss_scale: 32768.0000 (35321.3506)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.1209  max mem: 15572
Epoch: [10]  [ 240/1404]  eta: 0:12:31  lr: 0.000089  min_lr: 0.000001  loss: 4.1151 (4.1555)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (35215.4025)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0849  max mem: 15572
Epoch: [10]  [ 250/1404]  eta: 0:12:21  lr: 0.000089  min_lr: 0.000001  loss: 4.0635 (4.1562)  class_acc: 0.2500 (0.2538)  loss_scale: 32768.0000 (35117.8964)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.0297  max mem: 15572
Epoch: [10]  [ 260/1404]  eta: 0:12:11  lr: 0.000089  min_lr: 0.000001  loss: 4.2219 (4.1571)  class_acc: 0.2083 (0.2535)  loss_scale: 32768.0000 (35027.8621)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0573  max mem: 15572
Epoch: [10]  [ 270/1404]  eta: 0:12:03  lr: 0.000089  min_lr: 0.000001  loss: 4.1243 (4.1540)  class_acc: 0.2500 (0.2537)  loss_scale: 32768.0000 (34944.4723)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.0575  max mem: 15572
Epoch: [10]  [ 280/1404]  eta: 0:11:55  lr: 0.000089  min_lr: 0.000001  loss: 3.9974 (4.1507)  class_acc: 0.2917 (0.2540)  loss_scale: 32768.0000 (34867.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0632  max mem: 15572
Epoch: [10]  [ 290/1404]  eta: 0:11:47  lr: 0.000089  min_lr: 0.000001  loss: 4.0721 (4.1517)  class_acc: 0.2500 (0.2546)  loss_scale: 32768.0000 (34794.8866)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0710  max mem: 15572
Epoch: [10]  [ 300/1404]  eta: 0:11:43  lr: 0.000089  min_lr: 0.000001  loss: 4.2660 (4.1568)  class_acc: 0.2500 (0.2532)  loss_scale: 32768.0000 (34727.5482)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1261  max mem: 15572
Epoch: [10]  [ 310/1404]  eta: 0:11:33  lr: 0.000089  min_lr: 0.000001  loss: 4.2737 (4.1593)  class_acc: 0.2083 (0.2516)  loss_scale: 32768.0000 (34664.5402)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.0994  max mem: 15572
Epoch: [10]  [ 320/1404]  eta: 0:11:24  lr: 0.000089  min_lr: 0.000001  loss: 4.2020 (4.1603)  class_acc: 0.2083 (0.2525)  loss_scale: 32768.0000 (34605.4579)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0505  max mem: 15572
[2025-01-17 10:30:46,946] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:30:46,946] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:30:46,992] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:30:46,993] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 330/1404]  eta: 0:11:15  lr: 0.000089  min_lr: 0.000001  loss: 4.1009 (4.1599)  class_acc: 0.2500 (0.2528)  loss_scale: 32768.0000 (35242.9245)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0499  max mem: 15572
[2025-01-17 10:30:55,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14378
[2025-01-17 10:30:55,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:30:55,048] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14378
[2025-01-17 10:30:55,048] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:30:55,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 340/1404]  eta: 0:11:09  lr: 0.000089  min_lr: 0.000001  loss: 4.1163 (4.1580)  class_acc: 0.2500 (0.2542)  loss_scale: 65536.0000 (35843.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0891  max mem: 15572
Epoch: [10]  [ 350/1404]  eta: 0:11:03  lr: 0.000089  min_lr: 0.000001  loss: 4.1510 (4.1580)  class_acc: 0.2917 (0.2549)  loss_scale: 32768.0000 (35755.3960)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1280  max mem: 15572
Epoch: [10]  [ 360/1404]  eta: 0:10:55  lr: 0.000089  min_lr: 0.000001  loss: 4.2051 (4.1592)  class_acc: 0.2500 (0.2550)  loss_scale: 32768.0000 (35672.6427)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0699  max mem: 15572
Epoch: [10]  [ 370/1404]  eta: 0:10:50  lr: 0.000089  min_lr: 0.000001  loss: 4.2211 (4.1621)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (35594.3504)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0887  max mem: 15572
Epoch: [10]  [ 380/1404]  eta: 0:10:42  lr: 0.000089  min_lr: 0.000001  loss: 4.1299 (4.1589)  class_acc: 0.2500 (0.2546)  loss_scale: 32768.0000 (35520.1680)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0684  max mem: 15572
Epoch: [10]  [ 390/1404]  eta: 0:10:38  lr: 0.000089  min_lr: 0.000001  loss: 4.2355 (4.1631)  class_acc: 0.2500 (0.2532)  loss_scale: 32768.0000 (35449.7801)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.0550  max mem: 15572
Epoch: [10]  [ 400/1404]  eta: 0:10:32  lr: 0.000089  min_lr: 0.000001  loss: 4.2355 (4.1580)  class_acc: 0.1667 (0.2538)  loss_scale: 32768.0000 (35382.9027)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.1356  max mem: 15572
Epoch: [10]  [ 410/1404]  eta: 0:10:23  lr: 0.000089  min_lr: 0.000001  loss: 4.1039 (4.1590)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (35319.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.0817  max mem: 15572
Epoch: [10]  [ 420/1404]  eta: 0:10:16  lr: 0.000089  min_lr: 0.000001  loss: 4.1126 (4.1591)  class_acc: 0.2083 (0.2528)  loss_scale: 32768.0000 (35258.6793)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0273  max mem: 15572
Epoch: [10]  [ 430/1404]  eta: 0:10:11  lr: 0.000089  min_lr: 0.000001  loss: 4.2110 (4.1619)  class_acc: 0.2083 (0.2521)  loss_scale: 32768.0000 (35200.8910)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.0766  max mem: 15572
Epoch: [10]  [ 440/1404]  eta: 0:10:06  lr: 0.000089  min_lr: 0.000001  loss: 4.1113 (4.1594)  class_acc: 0.1667 (0.2521)  loss_scale: 32768.0000 (35145.7234)  weight_decay: 0.0500 (0.0500)  time: 0.6911  data: 0.0689  max mem: 15572
Epoch: [10]  [ 450/1404]  eta: 0:10:00  lr: 0.000089  min_lr: 0.000001  loss: 4.0539 (4.1611)  class_acc: 0.2083 (0.2527)  loss_scale: 32768.0000 (35093.0022)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.0194  max mem: 15572
Epoch: [10]  [ 460/1404]  eta: 0:09:53  lr: 0.000088  min_lr: 0.000001  loss: 4.2760 (4.1627)  class_acc: 0.2083 (0.2519)  loss_scale: 32768.0000 (35042.5683)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.0006  max mem: 15572
[2025-01-17 10:32:15,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:32:15,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:32:15,890] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:32:15,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:32:16,849] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14509
[2025-01-17 10:32:16,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:32:16,868] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14509
[2025-01-17 10:32:16,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:32:16,869] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 470/1404]  eta: 0:09:46  lr: 0.000088  min_lr: 0.000001  loss: 4.1765 (4.1615)  class_acc: 0.2083 (0.2519)  loss_scale: 32768.0000 (35133.4183)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0006  max mem: 15572
Epoch: [10]  [ 480/1404]  eta: 0:09:40  lr: 0.000088  min_lr: 0.000001  loss: 4.1016 (4.1624)  class_acc: 0.2500 (0.2514)  loss_scale: 32768.0000 (35084.2412)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.0006  max mem: 15572
Epoch: [10]  [ 490/1404]  eta: 0:09:34  lr: 0.000088  min_lr: 0.000001  loss: 4.1967 (4.1606)  class_acc: 0.2083 (0.2502)  loss_scale: 32768.0000 (35037.0672)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0006  max mem: 15572
Epoch: [10]  [ 500/1404]  eta: 0:09:29  lr: 0.000088  min_lr: 0.000001  loss: 4.1967 (4.1607)  class_acc: 0.1667 (0.2507)  loss_scale: 32768.0000 (34991.7764)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.0007  max mem: 15572
Epoch: [10]  [ 510/1404]  eta: 0:09:21  lr: 0.000088  min_lr: 0.000001  loss: 4.2568 (4.1600)  class_acc: 0.2500 (0.2505)  loss_scale: 32768.0000 (34948.2583)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0007  max mem: 15572
Epoch: [10]  [ 520/1404]  eta: 0:09:15  lr: 0.000088  min_lr: 0.000001  loss: 4.2568 (4.1620)  class_acc: 0.2500 (0.2508)  loss_scale: 32768.0000 (34906.4107)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0006  max mem: 15572
Epoch: [10]  [ 530/1404]  eta: 0:09:08  lr: 0.000088  min_lr: 0.000001  loss: 4.2042 (4.1614)  class_acc: 0.2500 (0.2508)  loss_scale: 32768.0000 (34866.1394)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0005  max mem: 15572
Epoch: [10]  [ 540/1404]  eta: 0:09:01  lr: 0.000088  min_lr: 0.000001  loss: 4.1150 (4.1614)  class_acc: 0.2083 (0.2505)  loss_scale: 32768.0000 (34827.3567)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0011  max mem: 15572
Epoch: [10]  [ 550/1404]  eta: 0:08:54  lr: 0.000088  min_lr: 0.000001  loss: 4.0953 (4.1612)  class_acc: 0.2083 (0.2503)  loss_scale: 32768.0000 (34789.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.0074  max mem: 15572
Epoch: [10]  [ 560/1404]  eta: 0:08:48  lr: 0.000088  min_lr: 0.000001  loss: 4.0270 (4.1611)  class_acc: 0.2083 (0.2499)  loss_scale: 32768.0000 (34753.9394)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0345  max mem: 15572
Epoch: [10]  [ 570/1404]  eta: 0:08:42  lr: 0.000088  min_lr: 0.000001  loss: 4.2986 (4.1669)  class_acc: 0.2083 (0.2493)  loss_scale: 32768.0000 (34719.1594)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.0428  max mem: 15572
Epoch: [10]  [ 580/1404]  eta: 0:08:36  lr: 0.000088  min_lr: 0.000001  loss: 4.2602 (4.1667)  class_acc: 0.2083 (0.2488)  loss_scale: 32768.0000 (34685.5766)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.0154  max mem: 15572
Epoch: [10]  [ 590/1404]  eta: 0:08:29  lr: 0.000088  min_lr: 0.000001  loss: 4.1216 (4.1661)  class_acc: 0.2500 (0.2488)  loss_scale: 32768.0000 (34653.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0007  max mem: 15572
[2025-01-17 10:33:36,917] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:33:36,917] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:33:37,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:33:37,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 600/1404]  eta: 0:08:22  lr: 0.000088  min_lr: 0.000001  loss: 4.1627 (4.1631)  class_acc: 0.2500 (0.2495)  loss_scale: 32768.0000 (34785.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.0006  max mem: 15572
[2025-01-17 10:33:40,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14645
[2025-01-17 10:33:40,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:33:40,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:33:40,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14645
[2025-01-17 10:33:40,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [10]  [ 610/1404]  eta: 0:08:16  lr: 0.000088  min_lr: 0.000001  loss: 4.2506 (4.1656)  class_acc: 0.2500 (0.2495)  loss_scale: 32768.0000 (34966.8347)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.0009  max mem: 15572
Epoch: [10]  [ 620/1404]  eta: 0:08:10  lr: 0.000088  min_lr: 0.000001  loss: 4.3214 (4.1675)  class_acc: 0.2083 (0.2487)  loss_scale: 32768.0000 (34931.4267)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.0010  max mem: 15572
Epoch: [10]  [ 630/1404]  eta: 0:08:03  lr: 0.000088  min_lr: 0.000001  loss: 4.1000 (4.1635)  class_acc: 0.2083 (0.2496)  loss_scale: 32768.0000 (34897.1410)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.0009  max mem: 15572
Epoch: [10]  [ 640/1404]  eta: 0:07:56  lr: 0.000088  min_lr: 0.000001  loss: 3.8683 (4.1604)  class_acc: 0.2500 (0.2498)  loss_scale: 32768.0000 (34863.9251)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0141  max mem: 15572
Epoch: [10]  [ 650/1404]  eta: 0:07:50  lr: 0.000088  min_lr: 0.000001  loss: 4.0279 (4.1609)  class_acc: 0.2500 (0.2497)  loss_scale: 32768.0000 (34831.7296)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.0505  max mem: 15572
Epoch: [10]  [ 660/1404]  eta: 0:07:44  lr: 0.000088  min_lr: 0.000001  loss: 3.9679 (4.1563)  class_acc: 0.2500 (0.2503)  loss_scale: 32768.0000 (34800.5083)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0579  max mem: 15572
Epoch: [10]  [ 670/1404]  eta: 0:07:37  lr: 0.000088  min_lr: 0.000001  loss: 3.9096 (4.1549)  class_acc: 0.2500 (0.2511)  loss_scale: 32768.0000 (34770.2176)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0216  max mem: 15572
Epoch: [10]  [ 680/1404]  eta: 0:07:31  lr: 0.000088  min_lr: 0.000001  loss: 3.9720 (4.1542)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (34740.8164)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.0010  max mem: 15572
Epoch: [10]  [ 690/1404]  eta: 0:07:24  lr: 0.000088  min_lr: 0.000001  loss: 4.0694 (4.1540)  class_acc: 0.1667 (0.2514)  loss_scale: 32768.0000 (34712.2663)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0008  max mem: 15572
Epoch: [10]  [ 700/1404]  eta: 0:07:18  lr: 0.000088  min_lr: 0.000001  loss: 4.0860 (4.1538)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (34684.5307)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0310  max mem: 15572
Epoch: [10]  [ 710/1404]  eta: 0:07:11  lr: 0.000088  min_lr: 0.000001  loss: 4.0860 (4.1527)  class_acc: 0.2500 (0.2512)  loss_scale: 32768.0000 (34657.5752)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0706  max mem: 15572
Epoch: [10]  [ 720/1404]  eta: 0:07:05  lr: 0.000088  min_lr: 0.000001  loss: 3.9975 (4.1539)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (34631.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.0764  max mem: 15572
Epoch: [10]  [ 730/1404]  eta: 0:06:58  lr: 0.000088  min_lr: 0.000001  loss: 4.0464 (4.1540)  class_acc: 0.2500 (0.2514)  loss_scale: 32768.0000 (34605.8769)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0574  max mem: 15572
[2025-01-17 10:34:58,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:34:58,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:34:58,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:34:58,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:34:59,430] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14775
[2025-01-17 10:34:59,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:34:59,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14775
[2025-01-17 10:34:59,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:34:59,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 740/1404]  eta: 0:06:53  lr: 0.000088  min_lr: 0.000001  loss: 4.0901 (4.1522)  class_acc: 0.2083 (0.2510)  loss_scale: 32768.0000 (34625.2955)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.1146  max mem: 15572
Epoch: [10]  [ 750/1404]  eta: 0:06:47  lr: 0.000088  min_lr: 0.000001  loss: 4.1212 (4.1546)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (34600.5646)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.1537  max mem: 15572
Epoch: [10]  [ 760/1404]  eta: 0:06:40  lr: 0.000088  min_lr: 0.000001  loss: 4.3359 (4.1577)  class_acc: 0.2500 (0.2511)  loss_scale: 32768.0000 (34576.4836)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0934  max mem: 15572
Epoch: [10]  [ 770/1404]  eta: 0:06:33  lr: 0.000088  min_lr: 0.000001  loss: 4.2072 (4.1556)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (34553.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0396  max mem: 15572
Epoch: [10]  [ 780/1404]  eta: 0:06:27  lr: 0.000088  min_lr: 0.000001  loss: 4.0455 (4.1553)  class_acc: 0.2917 (0.2517)  loss_scale: 32768.0000 (34530.1716)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0373  max mem: 15572
Epoch: [10]  [ 790/1404]  eta: 0:06:20  lr: 0.000088  min_lr: 0.000001  loss: 4.1246 (4.1543)  class_acc: 0.2083 (0.2509)  loss_scale: 32768.0000 (34507.8938)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0716  max mem: 15572
Epoch: [10]  [ 800/1404]  eta: 0:06:14  lr: 0.000088  min_lr: 0.000001  loss: 4.2104 (4.1558)  class_acc: 0.1667 (0.2507)  loss_scale: 32768.0000 (34486.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.0548  max mem: 15572
Epoch: [10]  [ 810/1404]  eta: 0:06:08  lr: 0.000088  min_lr: 0.000001  loss: 4.1687 (4.1565)  class_acc: 0.2083 (0.2506)  loss_scale: 32768.0000 (34464.9864)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0360  max mem: 15572
Epoch: [10]  [ 820/1404]  eta: 0:06:01  lr: 0.000088  min_lr: 0.000001  loss: 4.0582 (4.1542)  class_acc: 0.2500 (0.2512)  loss_scale: 32768.0000 (34444.3167)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0738  max mem: 15572
Epoch: [10]  [ 830/1404]  eta: 0:05:55  lr: 0.000088  min_lr: 0.000001  loss: 3.9305 (4.1527)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (34424.1444)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0524  max mem: 15572
Epoch: [10]  [ 840/1404]  eta: 0:05:49  lr: 0.000088  min_lr: 0.000001  loss: 4.1293 (4.1535)  class_acc: 0.2083 (0.2508)  loss_scale: 32768.0000 (34404.4518)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0474  max mem: 15572
Epoch: [10]  [ 850/1404]  eta: 0:05:43  lr: 0.000088  min_lr: 0.000001  loss: 4.1293 (4.1525)  class_acc: 0.2083 (0.2509)  loss_scale: 32768.0000 (34385.2221)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.0844  max mem: 15572
Epoch: [10]  [ 860/1404]  eta: 0:05:37  lr: 0.000088  min_lr: 0.000001  loss: 4.0876 (4.1518)  class_acc: 0.2500 (0.2512)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0376  max mem: 15572
[2025-01-17 10:36:18,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:36:18,531] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:36:18,617] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:36:18,617] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 870/1404]  eta: 0:05:30  lr: 0.000088  min_lr: 0.000001  loss: 3.8985 (4.1487)  class_acc: 0.2500 (0.2514)  loss_scale: 32768.0000 (34611.4351)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.0008  max mem: 15572
[2025-01-17 10:36:23,318] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14913
[2025-01-17 10:36:23,319] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:36:23,319] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14913
[2025-01-17 10:36:23,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:36:23,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 880/1404]  eta: 0:05:24  lr: 0.000088  min_lr: 0.000001  loss: 4.0350 (4.1493)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (34664.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0215  max mem: 15572
Epoch: [10]  [ 890/1404]  eta: 0:05:18  lr: 0.000088  min_lr: 0.000001  loss: 4.0747 (4.1503)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (34643.6094)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.1492  max mem: 15572
Epoch: [10]  [ 900/1404]  eta: 0:05:12  lr: 0.000088  min_lr: 0.000001  loss: 4.3755 (4.1529)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (34622.7925)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.1412  max mem: 15572
Epoch: [10]  [ 910/1404]  eta: 0:05:06  lr: 0.000088  min_lr: 0.000001  loss: 4.1959 (4.1523)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (34602.4325)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0993  max mem: 15572
Epoch: [10]  [ 920/1404]  eta: 0:05:00  lr: 0.000088  min_lr: 0.000001  loss: 4.1590 (4.1531)  class_acc: 0.2083 (0.2511)  loss_scale: 32768.0000 (34582.5147)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1293  max mem: 15572
Epoch: [10]  [ 930/1404]  eta: 0:04:54  lr: 0.000088  min_lr: 0.000001  loss: 4.1555 (4.1520)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (34563.0247)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1036  max mem: 15572
Epoch: [10]  [ 940/1404]  eta: 0:04:47  lr: 0.000088  min_lr: 0.000001  loss: 4.1172 (4.1510)  class_acc: 0.2917 (0.2519)  loss_scale: 32768.0000 (34543.9490)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1131  max mem: 15572
Epoch: [10]  [ 950/1404]  eta: 0:04:41  lr: 0.000088  min_lr: 0.000001  loss: 4.1584 (4.1507)  class_acc: 0.2083 (0.2519)  loss_scale: 32768.0000 (34525.2744)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1272  max mem: 15572
[2025-01-17 10:37:17,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=86, lr=[8.505727956044617e-07, 8.505727956044617e-07, 1.2151039937206597e-06, 1.2151039937206597e-06, 1.7358628481723713e-06, 1.7358628481723713e-06, 2.4798040688176734e-06, 2.4798040688176734e-06, 3.542577241168105e-06, 3.542577241168105e-06, 5.0608246302401504e-06, 5.0608246302401504e-06, 7.229749471771644e-06, 7.229749471771644e-06, 1.032821353110235e-05, 1.032821353110235e-05, 1.4754590758717642e-05, 1.4754590758717642e-05, 2.1077986798168063e-05, 2.1077986798168063e-05, 3.011140971166866e-05, 3.011140971166866e-05, 4.3016299588098086e-05, 4.3016299588098086e-05, 6.145185655442585e-05, 6.145185655442585e-05, 8.778836650632264e-05, 8.778836650632264e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:37:17,931] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=44.44045356709413, CurrSamplesPerSec=44.40333636080125, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 960/1404]  eta: 0:04:35  lr: 0.000088  min_lr: 0.000001  loss: 4.1807 (4.1509)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34506.9886)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1010  max mem: 15572
Epoch: [10]  [ 970/1404]  eta: 0:04:29  lr: 0.000088  min_lr: 0.000001  loss: 4.1807 (4.1518)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (34489.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0963  max mem: 15572
Epoch: [10]  [ 980/1404]  eta: 0:04:23  lr: 0.000088  min_lr: 0.000001  loss: 4.1523 (4.1507)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (34471.5352)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1297  max mem: 15572
Epoch: [10]  [ 990/1404]  eta: 0:04:17  lr: 0.000088  min_lr: 0.000001  loss: 3.9814 (4.1509)  class_acc: 0.2083 (0.2510)  loss_scale: 32768.0000 (34454.3451)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1252  max mem: 15572
Epoch: [10]  [1000/1404]  eta: 0:04:11  lr: 0.000088  min_lr: 0.000001  loss: 4.1683 (4.1501)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (34437.4985)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1503  max mem: 15572
[2025-01-17 10:37:45,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:37:45,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:37:45,313] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:37:45,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:37:48,898] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15046
[2025-01-17 10:37:48,898] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:37:48,916] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15046
[2025-01-17 10:37:48,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:37:48,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1010/1404]  eta: 0:04:04  lr: 0.000088  min_lr: 0.000001  loss: 4.1683 (4.1498)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34550.6311)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.1682  max mem: 15572
Epoch: [10]  [1020/1404]  eta: 0:03:58  lr: 0.000088  min_lr: 0.000001  loss: 4.1853 (4.1511)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (34533.1714)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1049  max mem: 15572
Epoch: [10]  [1030/1404]  eta: 0:03:52  lr: 0.000088  min_lr: 0.000001  loss: 4.0721 (4.1495)  class_acc: 0.2083 (0.2509)  loss_scale: 32768.0000 (34516.0504)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0337  max mem: 15572
Epoch: [10]  [1040/1404]  eta: 0:03:46  lr: 0.000088  min_lr: 0.000001  loss: 4.0421 (4.1490)  class_acc: 0.2500 (0.2507)  loss_scale: 32768.0000 (34499.2584)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0837  max mem: 15572
Epoch: [10]  [1050/1404]  eta: 0:03:39  lr: 0.000088  min_lr: 0.000001  loss: 4.1261 (4.1497)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (34482.7859)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0720  max mem: 15572
Epoch: [10]  [1060/1404]  eta: 0:03:33  lr: 0.000088  min_lr: 0.000001  loss: 4.0569 (4.1477)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (34466.6239)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0383  max mem: 15572
Epoch: [10]  [1070/1404]  eta: 0:03:27  lr: 0.000088  min_lr: 0.000001  loss: 4.0153 (4.1475)  class_acc: 0.2917 (0.2515)  loss_scale: 32768.0000 (34450.7638)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1044  max mem: 15572
Epoch: [10]  [1080/1404]  eta: 0:03:20  lr: 0.000088  min_lr: 0.000001  loss: 4.1174 (4.1459)  class_acc: 0.2917 (0.2518)  loss_scale: 32768.0000 (34435.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.0667  max mem: 15572
Epoch: [10]  [1090/1404]  eta: 0:03:14  lr: 0.000088  min_lr: 0.000001  loss: 3.9353 (4.1450)  class_acc: 0.2917 (0.2521)  loss_scale: 32768.0000 (34419.9157)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0842  max mem: 15572
Epoch: [10]  [1100/1404]  eta: 0:03:08  lr: 0.000088  min_lr: 0.000001  loss: 3.9665 (4.1446)  class_acc: 0.2917 (0.2519)  loss_scale: 32768.0000 (34404.9119)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.1257  max mem: 15572
Epoch: [10]  [1110/1404]  eta: 0:03:02  lr: 0.000088  min_lr: 0.000001  loss: 4.1350 (4.1466)  class_acc: 0.2083 (0.2513)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0456  max mem: 15572
Epoch: [10]  [1120/1404]  eta: 0:02:55  lr: 0.000088  min_lr: 0.000001  loss: 4.3115 (4.1455)  class_acc: 0.2500 (0.2518)  loss_scale: 32768.0000 (34375.7074)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0295  max mem: 15572
Epoch: [10]  [1130/1404]  eta: 0:02:49  lr: 0.000088  min_lr: 0.000001  loss: 4.2385 (4.1465)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34361.4925)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0952  max mem: 15572
[2025-01-17 10:39:06,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:39:06,321] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:39:06,322] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:39:06,322] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:39:06,821] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15176
[2025-01-17 10:39:06,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:39:06,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15176
[2025-01-17 10:39:06,822] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:39:06,822] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1140/1404]  eta: 0:02:43  lr: 0.000088  min_lr: 0.000001  loss: 4.3139 (4.1476)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (34376.2454)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.0955  max mem: 15572
Epoch: [10]  [1150/1404]  eta: 0:02:37  lr: 0.000088  min_lr: 0.000001  loss: 4.2014 (4.1480)  class_acc: 0.2083 (0.2514)  loss_scale: 32768.0000 (34362.2728)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0518  max mem: 15572
Epoch: [10]  [1160/1404]  eta: 0:02:31  lr: 0.000087  min_lr: 0.000001  loss: 4.1836 (4.1485)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (34348.5409)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0764  max mem: 15572
Epoch: [10]  [1170/1404]  eta: 0:02:24  lr: 0.000087  min_lr: 0.000001  loss: 4.1968 (4.1486)  class_acc: 0.2917 (0.2521)  loss_scale: 32768.0000 (34335.0436)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1207  max mem: 15572
Epoch: [10]  [1180/1404]  eta: 0:02:18  lr: 0.000087  min_lr: 0.000001  loss: 4.2094 (4.1498)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (34321.7748)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1405  max mem: 15572
Epoch: [10]  [1190/1404]  eta: 0:02:12  lr: 0.000087  min_lr: 0.000001  loss: 4.2024 (4.1495)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (34308.7288)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.1164  max mem: 15572
Epoch: [10]  [1200/1404]  eta: 0:02:06  lr: 0.000087  min_lr: 0.000001  loss: 4.0776 (4.1493)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (34295.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.0669  max mem: 15572
Epoch: [10]  [1210/1404]  eta: 0:02:00  lr: 0.000087  min_lr: 0.000001  loss: 4.1598 (4.1499)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (34283.2832)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0729  max mem: 15572
Epoch: [10]  [1220/1404]  eta: 0:01:53  lr: 0.000087  min_lr: 0.000001  loss: 4.1415 (4.1489)  class_acc: 0.2083 (0.2521)  loss_scale: 32768.0000 (34270.8731)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1078  max mem: 15572
Epoch: [10]  [1230/1404]  eta: 0:01:47  lr: 0.000087  min_lr: 0.000001  loss: 4.0745 (4.1478)  class_acc: 0.2500 (0.2522)  loss_scale: 32768.0000 (34258.6645)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0863  max mem: 15572
Epoch: [10]  [1240/1404]  eta: 0:01:41  lr: 0.000087  min_lr: 0.000001  loss: 4.1802 (4.1488)  class_acc: 0.2083 (0.2517)  loss_scale: 32768.0000 (34246.6527)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0671  max mem: 15572
Epoch: [10]  [1250/1404]  eta: 0:01:35  lr: 0.000087  min_lr: 0.000001  loss: 4.2921 (4.1492)  class_acc: 0.1667 (0.2514)  loss_scale: 32768.0000 (34234.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0739  max mem: 15572
Epoch: [10]  [1260/1404]  eta: 0:01:29  lr: 0.000087  min_lr: 0.000001  loss: 4.1377 (4.1496)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (34223.2006)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.0830  max mem: 15572
[2025-01-17 10:40:25,863] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:40:25,864] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:40:25,864] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:40:25,864] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1270/1404]  eta: 0:01:22  lr: 0.000087  min_lr: 0.000001  loss: 4.1771 (4.1496)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.0568  max mem: 15572
[2025-01-17 10:40:28,823] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15311
[2025-01-17 10:40:28,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:40:28,871] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15311
[2025-01-17 10:40:28,871] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:40:28,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1280/1404]  eta: 0:01:16  lr: 0.000087  min_lr: 0.000001  loss: 4.2153 (4.1496)  class_acc: 0.2083 (0.2510)  loss_scale: 32768.0000 (34353.9610)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0789  max mem: 15572
Epoch: [10]  [1290/1404]  eta: 0:01:10  lr: 0.000087  min_lr: 0.000001  loss: 4.1792 (4.1480)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34341.6762)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1033  max mem: 15572
Epoch: [10]  [1300/1404]  eta: 0:01:04  lr: 0.000087  min_lr: 0.000001  loss: 4.0082 (4.1478)  class_acc: 0.2917 (0.2517)  loss_scale: 32768.0000 (34329.5803)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1054  max mem: 15572
Epoch: [10]  [1310/1404]  eta: 0:00:58  lr: 0.000087  min_lr: 0.000001  loss: 4.0360 (4.1472)  class_acc: 0.2083 (0.2516)  loss_scale: 32768.0000 (34317.6690)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1184  max mem: 15572
Epoch: [10]  [1320/1404]  eta: 0:00:51  lr: 0.000087  min_lr: 0.000001  loss: 4.1154 (4.1472)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34305.9379)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.0642  max mem: 15572
Epoch: [10]  [1330/1404]  eta: 0:00:45  lr: 0.000087  min_lr: 0.000001  loss: 4.1132 (4.1469)  class_acc: 0.2917 (0.2516)  loss_scale: 32768.0000 (34294.3832)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0437  max mem: 15572
Epoch: [10]  [1340/1404]  eta: 0:00:39  lr: 0.000087  min_lr: 0.000001  loss: 4.1460 (4.1473)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (34283.0007)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0682  max mem: 15572
Epoch: [10]  [1350/1404]  eta: 0:00:33  lr: 0.000087  min_lr: 0.000001  loss: 4.1455 (4.1463)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (34271.7868)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.1282  max mem: 15572
Epoch: [10]  [1360/1404]  eta: 0:00:27  lr: 0.000087  min_lr: 0.000001  loss: 3.9901 (4.1454)  class_acc: 0.2500 (0.2515)  loss_scale: 32768.0000 (34260.7377)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.0879  max mem: 15572
Epoch: [10]  [1370/1404]  eta: 0:00:20  lr: 0.000087  min_lr: 0.000001  loss: 4.0919 (4.1459)  class_acc: 0.2500 (0.2519)  loss_scale: 32768.0000 (34249.8497)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0036  max mem: 15572
Epoch: [10]  [1380/1404]  eta: 0:00:14  lr: 0.000087  min_lr: 0.000001  loss: 4.2908 (4.1469)  class_acc: 0.2917 (0.2515)  loss_scale: 32768.0000 (34239.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0422  max mem: 15572
Epoch: [10]  [1390/1404]  eta: 0:00:08  lr: 0.000087  min_lr: 0.000001  loss: 4.1683 (4.1460)  class_acc: 0.2083 (0.2516)  loss_scale: 32768.0000 (34228.5435)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0847  max mem: 15572
[2025-01-17 10:41:45,331] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:41:45,331] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:41:45,331] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:41:45,331] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [1400/1404]  eta: 0:00:02  lr: 0.000087  min_lr: 0.000001  loss: 4.0802 (4.1464)  class_acc: 0.2083 (0.2515)  loss_scale: 32768.0000 (34241.5075)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0495  max mem: 15572
[2025-01-17 10:41:46,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15442
[2025-01-17 10:41:46,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:41:46,086] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15442
[2025-01-17 10:41:46,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:41:46,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [1403/1404]  eta: 0:00:00  lr: 0.000087  min_lr: 0.000001  loss: 4.0887 (4.1470)  class_acc: 0.2083 (0.2514)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)  time: 0.4468  data: 0.0049  max mem: 15572
Epoch: [10] Total time: 0:14:24 (0.6159 s / it)
Averaged stats: lr: 0.000087  min_lr: 0.000001  loss: 4.0887 (4.1464)  class_acc: 0.2083 (0.2520)  loss_scale: 32768.0000 (34261.6980)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:12:46  loss: 1.7315 (1.7315)  acc1: 66.6667 (66.6667)  acc5: 77.7778 (77.7778)  time: 5.6359  data: 5.4559  max mem: 15572
Val:  [ 10/136]  eta: 0:01:42  loss: 2.9996 (2.8246)  acc1: 27.7778 (30.3030)  acc5: 61.1111 (62.1212)  time: 0.8145  data: 0.6272  max mem: 15572
Val:  [ 20/136]  eta: 0:01:06  loss: 2.9594 (2.8874)  acc1: 27.7778 (30.6878)  acc5: 61.1111 (64.5503)  time: 0.3244  data: 0.1267  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.7163 (2.6415)  acc1: 33.3333 (37.8136)  acc5: 72.2222 (69.8925)  time: 0.3075  data: 0.1057  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.0731 (2.5815)  acc1: 44.4444 (39.1599)  acc5: 77.7778 (71.4092)  time: 0.3244  data: 0.1313  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.4370 (2.6134)  acc1: 33.3333 (38.8889)  acc5: 77.7778 (71.2418)  time: 0.3501  data: 0.1520  max mem: 15572
Val:  [ 60/136]  eta: 0:00:33  loss: 2.7753 (2.6916)  acc1: 27.7778 (35.9745)  acc5: 72.2222 (69.3078)  time: 0.3963  data: 0.1896  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.7080 (2.6823)  acc1: 33.3333 (37.0110)  acc5: 66.6667 (69.3271)  time: 0.4306  data: 0.2104  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.6381 (2.6834)  acc1: 38.8889 (36.7627)  acc5: 72.2222 (70.0960)  time: 0.3885  data: 0.1727  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.6627 (2.6907)  acc1: 27.7778 (35.7753)  acc5: 72.2222 (69.9634)  time: 0.3889  data: 0.1861  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9191 (2.7567)  acc1: 22.2222 (33.9384)  acc5: 61.1111 (67.8768)  time: 0.3771  data: 0.1683  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.8714 (2.7492)  acc1: 33.3333 (34.6847)  acc5: 66.6667 (68.2683)  time: 0.3663  data: 0.1482  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.3378 (2.6761)  acc1: 50.0000 (36.5932)  acc5: 77.7778 (69.9725)  time: 0.3941  data: 0.1914  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0389 (2.6329)  acc1: 55.5556 (38.0831)  acc5: 83.3333 (70.3986)  time: 0.2846  data: 0.1225  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1117 (2.6360)  acc1: 44.4444 (37.9197)  acc5: 77.7778 (70.4750)  time: 0.1663  data: 0.0172  max mem: 15572
Val: Total time: 0:00:52 (0.3849 s / it)
* Acc@1 37.817 Acc@5 69.451 loss 2.672
Accuracy of the network on the 4883 val videos: 37.8%
[2025-01-17 10:42:39,033] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 10:42:39,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 10:42:39,035] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 10:42:39,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 10:42:41,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 10:42:41,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.82%
Epoch: [11]  [   0/1404]  eta: 2:37:50  lr: 0.000087  min_lr: 0.000001  loss: 4.1355 (4.1355)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7455  data: 5.8926  max mem: 15572
Epoch: [11]  [  10/1404]  eta: 0:30:34  lr: 0.000087  min_lr: 0.000001  loss: 4.0951 (4.1157)  class_acc: 0.2083 (0.2045)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3160  data: 0.7372  max mem: 15572
Epoch: [11]  [  20/1404]  eta: 0:21:57  lr: 0.000087  min_lr: 0.000001  loss: 4.0037 (4.0228)  class_acc: 0.2500 (0.2460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.1113  max mem: 15572
Epoch: [11]  [  30/1404]  eta: 0:19:08  lr: 0.000087  min_lr: 0.000001  loss: 3.8867 (3.9954)  class_acc: 0.2917 (0.2460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.0116  max mem: 15572
Epoch: [11]  [  40/1404]  eta: 0:17:17  lr: 0.000087  min_lr: 0.000001  loss: 3.9893 (4.0280)  class_acc: 0.2500 (0.2419)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0116  max mem: 15572
Epoch: [11]  [  50/1404]  eta: 0:16:51  lr: 0.000087  min_lr: 0.000001  loss: 4.1829 (4.0471)  class_acc: 0.2083 (0.2426)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0009  max mem: 15572
Epoch: [11]  [  60/1404]  eta: 0:16:03  lr: 0.000087  min_lr: 0.000001  loss: 3.9722 (4.0624)  class_acc: 0.2500 (0.2520)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0008  max mem: 15572
[2025-01-17 10:43:25,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15505
[2025-01-17 10:43:25,656] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 10:43:25,657] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15505
[2025-01-17 10:43:25,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 10:43:25,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [  70/1404]  eta: 0:15:49  lr: 0.000087  min_lr: 0.000001  loss: 4.0676 (4.0773)  class_acc: 0.2083 (0.2471)  loss_scale: 16384.0000 (30460.3944)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.0005  max mem: 15572
Epoch: [11]  [  80/1404]  eta: 0:15:24  lr: 0.000087  min_lr: 0.000001  loss: 4.0166 (4.0820)  class_acc: 0.2083 (0.2443)  loss_scale: 16384.0000 (28722.5679)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0006  max mem: 15572
Epoch: [11]  [  90/1404]  eta: 0:15:07  lr: 0.000087  min_lr: 0.000001  loss: 4.0166 (4.0755)  class_acc: 0.2500 (0.2514)  loss_scale: 16384.0000 (27366.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0008  max mem: 15572
Epoch: [11]  [ 100/1404]  eta: 0:14:54  lr: 0.000087  min_lr: 0.000001  loss: 4.0676 (4.0786)  class_acc: 0.2917 (0.2537)  loss_scale: 16384.0000 (26279.2871)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.0015  max mem: 15572
Epoch: [11]  [ 110/1404]  eta: 0:14:33  lr: 0.000087  min_lr: 0.000001  loss: 4.3483 (4.1021)  class_acc: 0.2083 (0.2523)  loss_scale: 16384.0000 (25387.8198)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0015  max mem: 15572
Epoch: [11]  [ 120/1404]  eta: 0:14:18  lr: 0.000087  min_lr: 0.000001  loss: 4.3593 (4.1047)  class_acc: 0.2083 (0.2476)  loss_scale: 16384.0000 (24643.7025)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.0009  max mem: 15572
Epoch: [11]  [ 130/1404]  eta: 0:14:05  lr: 0.000087  min_lr: 0.000001  loss: 4.1047 (4.1058)  class_acc: 0.2083 (0.2465)  loss_scale: 16384.0000 (24013.1908)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0009  max mem: 15572
Epoch: [11]  [ 140/1404]  eta: 0:13:49  lr: 0.000087  min_lr: 0.000001  loss: 4.0383 (4.1011)  class_acc: 0.2083 (0.2476)  loss_scale: 16384.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.0007  max mem: 15572
Epoch: [11]  [ 150/1404]  eta: 0:13:34  lr: 0.000087  min_lr: 0.000001  loss: 4.0385 (4.0926)  class_acc: 0.2917 (0.2517)  loss_scale: 16384.0000 (23002.7020)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0009  max mem: 15572
Epoch: [11]  [ 160/1404]  eta: 0:13:31  lr: 0.000087  min_lr: 0.000001  loss: 4.0068 (4.0886)  class_acc: 0.2917 (0.2544)  loss_scale: 16384.0000 (22591.6025)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0043  max mem: 15572
Epoch: [11]  [ 170/1404]  eta: 0:13:22  lr: 0.000087  min_lr: 0.000001  loss: 4.1792 (4.0900)  class_acc: 0.2500 (0.2546)  loss_scale: 16384.0000 (22228.5848)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.0041  max mem: 15572
Epoch: [11]  [ 180/1404]  eta: 0:13:19  lr: 0.000087  min_lr: 0.000001  loss: 4.1792 (4.0989)  class_acc: 0.2083 (0.2509)  loss_scale: 16384.0000 (21905.6796)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.0008  max mem: 15572
[2025-01-17 10:44:44,919] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:44:44,919] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 10:44:44,970] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:44:44,970] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 190/1404]  eta: 0:13:04  lr: 0.000087  min_lr: 0.000001  loss: 4.0929 (4.0937)  class_acc: 0.2083 (0.2524)  loss_scale: 16384.0000 (21702.3665)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0008  max mem: 15572
Epoch: [11]  [ 200/1404]  eta: 0:12:58  lr: 0.000087  min_lr: 0.000001  loss: 4.0390 (4.0926)  class_acc: 0.2500 (0.2533)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.0006  max mem: 15572
Epoch: [11]  [ 210/1404]  eta: 0:12:47  lr: 0.000087  min_lr: 0.000001  loss: 4.1571 (4.0967)  class_acc: 0.2083 (0.2512)  loss_scale: 32768.0000 (22751.2417)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0008  max mem: 15572
Epoch: [11]  [ 220/1404]  eta: 0:12:37  lr: 0.000087  min_lr: 0.000001  loss: 4.1571 (4.1004)  class_acc: 0.2083 (0.2519)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0008  max mem: 15572
Epoch: [11]  [ 230/1404]  eta: 0:12:31  lr: 0.000087  min_lr: 0.000001  loss: 4.2137 (4.1071)  class_acc: 0.2500 (0.2505)  loss_scale: 32768.0000 (23618.4935)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0008  max mem: 15572
Epoch: [11]  [ 240/1404]  eta: 0:12:23  lr: 0.000087  min_lr: 0.000001  loss: 4.1950 (4.1001)  class_acc: 0.2500 (0.2522)  loss_scale: 32768.0000 (23998.1411)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0009  max mem: 15572
Epoch: [11]  [ 250/1404]  eta: 0:12:14  lr: 0.000087  min_lr: 0.000001  loss: 3.9642 (4.1059)  class_acc: 0.2500 (0.2513)  loss_scale: 32768.0000 (24347.5378)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0006  max mem: 15572
Epoch: [11]  [ 260/1404]  eta: 0:12:05  lr: 0.000087  min_lr: 0.000001  loss: 4.0873 (4.1068)  class_acc: 0.2083 (0.2502)  loss_scale: 32768.0000 (24670.1609)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.0007  max mem: 15572
Epoch: [11]  [ 270/1404]  eta: 0:11:57  lr: 0.000087  min_lr: 0.000001  loss: 4.2352 (4.1117)  class_acc: 0.2083 (0.2494)  loss_scale: 32768.0000 (24968.9742)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0208  max mem: 15572
Epoch: [11]  [ 280/1404]  eta: 0:11:53  lr: 0.000087  min_lr: 0.000001  loss: 4.1397 (4.1094)  class_acc: 0.2500 (0.2504)  loss_scale: 32768.0000 (25246.5196)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1004  max mem: 15572
Epoch: [11]  [ 290/1404]  eta: 0:11:45  lr: 0.000087  min_lr: 0.000001  loss: 4.1397 (4.1111)  class_acc: 0.2083 (0.2500)  loss_scale: 32768.0000 (25504.9897)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1278  max mem: 15572
Epoch: [11]  [ 300/1404]  eta: 0:11:38  lr: 0.000087  min_lr: 0.000001  loss: 4.2075 (4.1100)  class_acc: 0.2083 (0.2503)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0986  max mem: 15572
Epoch: [11]  [ 310/1404]  eta: 0:11:29  lr: 0.000087  min_lr: 0.000001  loss: 4.0615 (4.1110)  class_acc: 0.2500 (0.2496)  loss_scale: 32768.0000 (25972.0643)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0574  max mem: 15572
[2025-01-17 10:46:02,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:46:02,388] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:46:02,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:46:02,432] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:46:03,529] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15764
[2025-01-17 10:46:03,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:46:03,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15764
[2025-01-17 10:46:03,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:46:03,589] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 320/1404]  eta: 0:11:21  lr: 0.000087  min_lr: 0.000001  loss: 4.0352 (4.1081)  class_acc: 0.2500 (0.2508)  loss_scale: 32768.0000 (26387.9377)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0068  max mem: 15572
Epoch: [11]  [ 330/1404]  eta: 0:11:15  lr: 0.000087  min_lr: 0.000001  loss: 4.1031 (4.1090)  class_acc: 0.2500 (0.2510)  loss_scale: 32768.0000 (26580.6888)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0339  max mem: 15572
Epoch: [11]  [ 340/1404]  eta: 0:11:10  lr: 0.000087  min_lr: 0.000001  loss: 4.1031 (4.1092)  class_acc: 0.2500 (0.2501)  loss_scale: 32768.0000 (26762.1349)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0443  max mem: 15572
Epoch: [11]  [ 350/1404]  eta: 0:11:04  lr: 0.000087  min_lr: 0.000001  loss: 4.1857 (4.1125)  class_acc: 0.2083 (0.2506)  loss_scale: 32768.0000 (26933.2422)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.0410  max mem: 15572
Epoch: [11]  [ 360/1404]  eta: 0:10:58  lr: 0.000087  min_lr: 0.000001  loss: 4.2055 (4.1089)  class_acc: 0.2500 (0.2529)  loss_scale: 32768.0000 (27094.8698)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.0849  max mem: 15572
Epoch: [11]  [ 370/1404]  eta: 0:10:50  lr: 0.000087  min_lr: 0.000001  loss: 4.0377 (4.1105)  class_acc: 0.2500 (0.2517)  loss_scale: 32768.0000 (27247.7844)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0550  max mem: 15572
Epoch: [11]  [ 380/1404]  eta: 0:10:43  lr: 0.000087  min_lr: 0.000001  loss: 4.3072 (4.1172)  class_acc: 0.2083 (0.2499)  loss_scale: 32768.0000 (27392.6719)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0110  max mem: 15572
Epoch: [11]  [ 390/1404]  eta: 0:10:37  lr: 0.000087  min_lr: 0.000001  loss: 4.2868 (4.1191)  class_acc: 0.2083 (0.2500)  loss_scale: 32768.0000 (27530.1483)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0482  max mem: 15572
Epoch: [11]  [ 400/1404]  eta: 0:10:32  lr: 0.000086  min_lr: 0.000001  loss: 4.1193 (4.1221)  class_acc: 0.2500 (0.2497)  loss_scale: 32768.0000 (27660.7681)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.1276  max mem: 15572
Epoch: [11]  [ 410/1404]  eta: 0:10:24  lr: 0.000086  min_lr: 0.000001  loss: 4.1193 (4.1205)  class_acc: 0.2083 (0.2496)  loss_scale: 32768.0000 (27785.0316)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0948  max mem: 15572
Epoch: [11]  [ 420/1404]  eta: 0:10:16  lr: 0.000086  min_lr: 0.000001  loss: 4.1087 (4.1214)  class_acc: 0.2500 (0.2503)  loss_scale: 32768.0000 (27903.3919)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0139  max mem: 15572
Epoch: [11]  [ 430/1404]  eta: 0:10:12  lr: 0.000086  min_lr: 0.000001  loss: 4.1087 (4.1211)  class_acc: 0.2917 (0.2513)  loss_scale: 32768.0000 (28016.2599)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0894  max mem: 15572
[2025-01-17 10:47:16,617] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15879
[2025-01-17 10:47:16,617] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 10:47:16,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15879
[2025-01-17 10:47:16,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 10:47:16,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 440/1404]  eta: 0:10:06  lr: 0.000086  min_lr: 0.000001  loss: 4.2459 (4.1260)  class_acc: 0.2083 (0.2497)  loss_scale: 32768.0000 (27901.0975)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.1501  max mem: 15572
Epoch: [11]  [ 450/1404]  eta: 0:10:00  lr: 0.000086  min_lr: 0.000001  loss: 4.2537 (4.1260)  class_acc: 0.2083 (0.2497)  loss_scale: 16384.0000 (27645.7295)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1329  max mem: 15572
Epoch: [11]  [ 460/1404]  eta: 0:09:53  lr: 0.000086  min_lr: 0.000001  loss: 4.0428 (4.1255)  class_acc: 0.2500 (0.2500)  loss_scale: 16384.0000 (27401.4403)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0844  max mem: 15572
Epoch: [11]  [ 470/1404]  eta: 0:09:45  lr: 0.000086  min_lr: 0.000001  loss: 4.0197 (4.1217)  class_acc: 0.2917 (0.2509)  loss_scale: 16384.0000 (27167.5244)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0685  max mem: 15572
Epoch: [11]  [ 480/1404]  eta: 0:09:40  lr: 0.000086  min_lr: 0.000001  loss: 3.9810 (4.1178)  class_acc: 0.2917 (0.2516)  loss_scale: 16384.0000 (26943.3347)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1250  max mem: 15572
Epoch: [11]  [ 490/1404]  eta: 0:09:32  lr: 0.000086  min_lr: 0.000001  loss: 3.9192 (4.1136)  class_acc: 0.2500 (0.2520)  loss_scale: 16384.0000 (26728.2770)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0784  max mem: 15572
Epoch: [11]  [ 500/1404]  eta: 0:09:26  lr: 0.000086  min_lr: 0.000001  loss: 3.9395 (4.1125)  class_acc: 0.2917 (0.2529)  loss_scale: 16384.0000 (26521.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.0352  max mem: 15572
Epoch: [11]  [ 510/1404]  eta: 0:09:19  lr: 0.000086  min_lr: 0.000001  loss: 4.0633 (4.1126)  class_acc: 0.2917 (0.2539)  loss_scale: 16384.0000 (26323.4129)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0615  max mem: 15572
Epoch: [11]  [ 520/1404]  eta: 0:09:13  lr: 0.000086  min_lr: 0.000001  loss: 4.1987 (4.1113)  class_acc: 0.2917 (0.2551)  loss_scale: 16384.0000 (26132.6372)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.0848  max mem: 15572
Epoch: [11]  [ 530/1404]  eta: 0:09:05  lr: 0.000086  min_lr: 0.000001  loss: 4.1865 (4.1124)  class_acc: 0.2500 (0.2547)  loss_scale: 16384.0000 (25949.0471)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0585  max mem: 15572
Epoch: [11]  [ 540/1404]  eta: 0:08:58  lr: 0.000086  min_lr: 0.000001  loss: 4.0240 (4.1089)  class_acc: 0.2500 (0.2552)  loss_scale: 16384.0000 (25772.2440)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0463  max mem: 15572
Epoch: [11]  [ 550/1404]  eta: 0:08:51  lr: 0.000086  min_lr: 0.000001  loss: 4.0240 (4.1092)  class_acc: 0.2500 (0.2550)  loss_scale: 16384.0000 (25601.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.0766  max mem: 15572
[2025-01-17 10:48:29,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=93, lr=[8.35610443572728e-07, 8.35610443572728e-07, 1.1937292051038973e-06, 1.1937292051038973e-06, 1.7053274358627106e-06, 1.7053274358627106e-06, 2.436182051232444e-06, 2.436182051232444e-06, 3.480260073189206e-06, 3.480260073189206e-06, 4.9718001045560085e-06, 4.9718001045560085e-06, 7.102571577937155e-06, 7.102571577937155e-06, 1.0146530825624509e-05, 1.0146530825624509e-05, 1.4495044036606441e-05, 1.4495044036606441e-05, 2.070720576658063e-05, 2.070720576658063e-05, 2.9581722523686616e-05, 2.9581722523686616e-05, 4.2259603605266596e-05, 4.2259603605266596e-05, 6.0370862293238e-05, 6.0370862293238e-05, 8.624408899034001e-05, 8.624408899034001e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:48:29,932] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=44.556021521667226, CurrSamplesPerSec=50.0334735145898, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 560/1404]  eta: 0:08:47  lr: 0.000086  min_lr: 0.000001  loss: 4.1634 (4.1132)  class_acc: 0.2083 (0.2536)  loss_scale: 16384.0000 (25437.5472)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.1401  max mem: 15572
[2025-01-17 10:48:34,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:48:34,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:48:34,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 10:48:34,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 570/1404]  eta: 0:08:40  lr: 0.000086  min_lr: 0.000001  loss: 4.0994 (4.1102)  class_acc: 0.2083 (0.2550)  loss_scale: 16384.0000 (25479.8459)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1243  max mem: 15572
Epoch: [11]  [ 580/1404]  eta: 0:08:33  lr: 0.000086  min_lr: 0.000001  loss: 4.0676 (4.1108)  class_acc: 0.2917 (0.2552)  loss_scale: 32768.0000 (25605.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0793  max mem: 15572
Epoch: [11]  [ 590/1404]  eta: 0:08:26  lr: 0.000086  min_lr: 0.000001  loss: 4.0971 (4.1116)  class_acc: 0.2083 (0.2541)  loss_scale: 32768.0000 (25726.4839)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0645  max mem: 15572
Epoch: [11]  [ 600/1404]  eta: 0:08:20  lr: 0.000086  min_lr: 0.000001  loss: 4.1174 (4.1107)  class_acc: 0.2083 (0.2546)  loss_scale: 32768.0000 (25843.6473)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0646  max mem: 15572
Epoch: [11]  [ 610/1404]  eta: 0:08:14  lr: 0.000086  min_lr: 0.000001  loss: 4.1174 (4.1097)  class_acc: 0.2500 (0.2543)  loss_scale: 32768.0000 (25956.9755)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.1072  max mem: 15572
Epoch: [11]  [ 620/1404]  eta: 0:08:08  lr: 0.000086  min_lr: 0.000001  loss: 4.1481 (4.1107)  class_acc: 0.2083 (0.2537)  loss_scale: 32768.0000 (26066.6538)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.0704  max mem: 15572
Epoch: [11]  [ 630/1404]  eta: 0:08:01  lr: 0.000086  min_lr: 0.000001  loss: 4.1564 (4.1124)  class_acc: 0.2083 (0.2540)  loss_scale: 32768.0000 (26172.8558)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0719  max mem: 15572
Epoch: [11]  [ 640/1404]  eta: 0:07:55  lr: 0.000086  min_lr: 0.000001  loss: 4.3020 (4.1152)  class_acc: 0.2083 (0.2536)  loss_scale: 32768.0000 (26275.7441)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0446  max mem: 15572
Epoch: [11]  [ 650/1404]  eta: 0:07:49  lr: 0.000086  min_lr: 0.000001  loss: 4.0740 (4.1130)  class_acc: 0.2500 (0.2547)  loss_scale: 32768.0000 (26375.4716)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0521  max mem: 15572
Epoch: [11]  [ 660/1404]  eta: 0:07:42  lr: 0.000086  min_lr: 0.000001  loss: 4.0104 (4.1113)  class_acc: 0.2917 (0.2549)  loss_scale: 32768.0000 (26472.1815)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0761  max mem: 15572
Epoch: [11]  [ 670/1404]  eta: 0:07:36  lr: 0.000086  min_lr: 0.000001  loss: 3.9778 (4.1100)  class_acc: 0.2917 (0.2554)  loss_scale: 32768.0000 (26566.0089)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.0498  max mem: 15572
Epoch: [11]  [ 680/1404]  eta: 0:07:29  lr: 0.000086  min_lr: 0.000001  loss: 3.9554 (4.1085)  class_acc: 0.2917 (0.2558)  loss_scale: 32768.0000 (26657.0808)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0258  max mem: 15572
Epoch: [11]  [ 690/1404]  eta: 0:07:24  lr: 0.000086  min_lr: 0.000001  loss: 4.0140 (4.1086)  class_acc: 0.2500 (0.2557)  loss_scale: 32768.0000 (26745.5166)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.0684  max mem: 15572
[2025-01-17 10:49:53,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:49:53,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:49:53,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:49:53,175] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:49:55,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16140
[2025-01-17 10:49:55,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:49:55,261] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16140
[2025-01-17 10:49:55,261] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:49:55,262] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 700/1404]  eta: 0:07:17  lr: 0.000086  min_lr: 0.000001  loss: 4.1562 (4.1101)  class_acc: 0.2083 (0.2550)  loss_scale: 32768.0000 (27018.4080)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0686  max mem: 15572
Epoch: [11]  [ 710/1404]  eta: 0:07:10  lr: 0.000086  min_lr: 0.000001  loss: 4.1644 (4.1091)  class_acc: 0.2083 (0.2549)  loss_scale: 32768.0000 (27099.2743)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0405  max mem: 15572
Epoch: [11]  [ 720/1404]  eta: 0:07:05  lr: 0.000086  min_lr: 0.000001  loss: 4.0713 (4.1104)  class_acc: 0.2083 (0.2543)  loss_scale: 32768.0000 (27177.8974)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.1494  max mem: 15572
Epoch: [11]  [ 730/1404]  eta: 0:06:59  lr: 0.000086  min_lr: 0.000001  loss: 4.0713 (4.1074)  class_acc: 0.2500 (0.2548)  loss_scale: 32768.0000 (27254.3694)  weight_decay: 0.0500 (0.0500)  time: 0.6835  data: 0.1564  max mem: 15572
Epoch: [11]  [ 740/1404]  eta: 0:06:52  lr: 0.000086  min_lr: 0.000001  loss: 3.8984 (4.1070)  class_acc: 0.3333 (0.2558)  loss_scale: 32768.0000 (27328.7773)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.0661  max mem: 15572
Epoch: [11]  [ 750/1404]  eta: 0:06:46  lr: 0.000086  min_lr: 0.000001  loss: 4.1441 (4.1079)  class_acc: 0.2500 (0.2554)  loss_scale: 32768.0000 (27401.2037)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.0632  max mem: 15572
Epoch: [11]  [ 760/1404]  eta: 0:06:40  lr: 0.000086  min_lr: 0.000001  loss: 4.1662 (4.1079)  class_acc: 0.2500 (0.2557)  loss_scale: 32768.0000 (27471.7267)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.1412  max mem: 15572
Epoch: [11]  [ 770/1404]  eta: 0:06:34  lr: 0.000086  min_lr: 0.000001  loss: 4.0904 (4.1053)  class_acc: 0.2917 (0.2566)  loss_scale: 32768.0000 (27540.4202)  weight_decay: 0.0500 (0.0500)  time: 0.6852  data: 0.1712  max mem: 15572
Epoch: [11]  [ 780/1404]  eta: 0:06:28  lr: 0.000086  min_lr: 0.000001  loss: 4.1047 (4.1053)  class_acc: 0.2500 (0.2563)  loss_scale: 32768.0000 (27607.3547)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0742  max mem: 15572
Epoch: [11]  [ 790/1404]  eta: 0:06:21  lr: 0.000086  min_lr: 0.000001  loss: 4.1287 (4.1058)  class_acc: 0.2083 (0.2559)  loss_scale: 32768.0000 (27672.5967)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0008  max mem: 15572
Epoch: [11]  [ 800/1404]  eta: 0:06:14  lr: 0.000086  min_lr: 0.000001  loss: 4.1012 (4.1064)  class_acc: 0.2083 (0.2556)  loss_scale: 32768.0000 (27736.2097)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0106  max mem: 15572
Epoch: [11]  [ 810/1404]  eta: 0:06:08  lr: 0.000086  min_lr: 0.000001  loss: 3.9515 (4.1049)  class_acc: 0.2500 (0.2565)  loss_scale: 32768.0000 (27798.2540)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0770  max mem: 15572
Epoch: [11]  [ 820/1404]  eta: 0:06:01  lr: 0.000086  min_lr: 0.000001  loss: 3.9430 (4.1047)  class_acc: 0.3333 (0.2569)  loss_scale: 32768.0000 (27858.7868)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0755  max mem: 15572
[2025-01-17 10:51:14,593] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:51:14,593] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:51:14,602] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:51:14,603] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:51:16,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16273
[2025-01-17 10:51:16,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16273
[2025-01-17 10:51:16,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:51:16,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:51:16,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 830/1404]  eta: 0:05:55  lr: 0.000086  min_lr: 0.000001  loss: 3.9482 (4.1033)  class_acc: 0.2917 (0.2573)  loss_scale: 32768.0000 (28075.5909)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.0627  max mem: 15572
Epoch: [11]  [ 840/1404]  eta: 0:05:49  lr: 0.000086  min_lr: 0.000001  loss: 4.1735 (4.1054)  class_acc: 0.2917 (0.2569)  loss_scale: 32768.0000 (28131.3864)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.1086  max mem: 15572
Epoch: [11]  [ 850/1404]  eta: 0:05:43  lr: 0.000086  min_lr: 0.000001  loss: 4.1994 (4.1054)  class_acc: 0.2917 (0.2571)  loss_scale: 32768.0000 (28185.8707)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.0814  max mem: 15572
Epoch: [11]  [ 860/1404]  eta: 0:05:37  lr: 0.000086  min_lr: 0.000001  loss: 4.1521 (4.1060)  class_acc: 0.2500 (0.2571)  loss_scale: 32768.0000 (28239.0894)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0694  max mem: 15572
Epoch: [11]  [ 870/1404]  eta: 0:05:30  lr: 0.000086  min_lr: 0.000001  loss: 4.1642 (4.1065)  class_acc: 0.2500 (0.2571)  loss_scale: 32768.0000 (28291.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0429  max mem: 15572
Epoch: [11]  [ 880/1404]  eta: 0:05:23  lr: 0.000086  min_lr: 0.000001  loss: 4.0699 (4.1045)  class_acc: 0.2500 (0.2578)  loss_scale: 32768.0000 (28341.9024)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0188  max mem: 15572
Epoch: [11]  [ 890/1404]  eta: 0:05:17  lr: 0.000086  min_lr: 0.000001  loss: 4.0112 (4.1049)  class_acc: 0.2083 (0.2575)  loss_scale: 32768.0000 (28391.5780)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0620  max mem: 15572
Epoch: [11]  [ 900/1404]  eta: 0:05:11  lr: 0.000086  min_lr: 0.000001  loss: 4.0360 (4.1036)  class_acc: 0.2500 (0.2577)  loss_scale: 32768.0000 (28440.1509)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1379  max mem: 15572
Epoch: [11]  [ 910/1404]  eta: 0:05:05  lr: 0.000086  min_lr: 0.000001  loss: 3.9411 (4.1032)  class_acc: 0.2500 (0.2576)  loss_scale: 32768.0000 (28487.6575)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.1718  max mem: 15572
Epoch: [11]  [ 920/1404]  eta: 0:04:59  lr: 0.000086  min_lr: 0.000001  loss: 4.0525 (4.1047)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (28534.1325)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.1198  max mem: 15572
Epoch: [11]  [ 930/1404]  eta: 0:04:53  lr: 0.000086  min_lr: 0.000001  loss: 4.2078 (4.1057)  class_acc: 0.2083 (0.2572)  loss_scale: 32768.0000 (28579.6090)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0749  max mem: 15572
Epoch: [11]  [ 940/1404]  eta: 0:04:46  lr: 0.000086  min_lr: 0.000001  loss: 4.1924 (4.1048)  class_acc: 0.2083 (0.2573)  loss_scale: 32768.0000 (28624.1190)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.0611  max mem: 15572
Epoch: [11]  [ 950/1404]  eta: 0:04:41  lr: 0.000086  min_lr: 0.000001  loss: 4.0465 (4.1041)  class_acc: 0.2500 (0.2576)  loss_scale: 32768.0000 (28667.6930)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.1296  max mem: 15572
[2025-01-17 10:52:35,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:52:35,697] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:52:35,747] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:52:35,748] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 960/1404]  eta: 0:04:34  lr: 0.000086  min_lr: 0.000001  loss: 3.9857 (4.1028)  class_acc: 0.2500 (0.2578)  loss_scale: 32768.0000 (28812.6535)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1015  max mem: 15572
Epoch: [11]  [ 970/1404]  eta: 0:04:28  lr: 0.000086  min_lr: 0.000001  loss: 4.0838 (4.1041)  class_acc: 0.2083 (0.2576)  loss_scale: 65536.0000 (29190.8548)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0008  max mem: 15572
[2025-01-17 10:52:43,232] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16416
[2025-01-17 10:52:43,232] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:52:43,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:52:43,252] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16416
[2025-01-17 10:52:43,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [ 980/1404]  eta: 0:04:21  lr: 0.000086  min_lr: 0.000001  loss: 4.1784 (4.1028)  class_acc: 0.2083 (0.2580)  loss_scale: 65536.0000 (29260.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0152  max mem: 15572
Epoch: [11]  [ 990/1404]  eta: 0:04:15  lr: 0.000086  min_lr: 0.000001  loss: 4.1784 (4.1040)  class_acc: 0.2500 (0.2578)  loss_scale: 32768.0000 (29296.1130)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0308  max mem: 15572
Epoch: [11]  [1000/1404]  eta: 0:04:09  lr: 0.000086  min_lr: 0.000001  loss: 4.2709 (4.1059)  class_acc: 0.2083 (0.2571)  loss_scale: 32768.0000 (29330.7972)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.0737  max mem: 15572
Epoch: [11]  [1010/1404]  eta: 0:04:03  lr: 0.000085  min_lr: 0.000001  loss: 4.1137 (4.1017)  class_acc: 0.2500 (0.2578)  loss_scale: 32768.0000 (29364.7953)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.0914  max mem: 15572
Epoch: [11]  [1020/1404]  eta: 0:03:56  lr: 0.000085  min_lr: 0.000001  loss: 3.7423 (4.1011)  class_acc: 0.2917 (0.2580)  loss_scale: 32768.0000 (29398.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.0705  max mem: 15572
Epoch: [11]  [1030/1404]  eta: 0:03:50  lr: 0.000085  min_lr: 0.000001  loss: 4.0768 (4.1018)  class_acc: 0.2083 (0.2575)  loss_scale: 32768.0000 (29430.8128)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.0848  max mem: 15572
Epoch: [11]  [1040/1404]  eta: 0:03:44  lr: 0.000085  min_lr: 0.000001  loss: 4.2319 (4.1035)  class_acc: 0.2083 (0.2571)  loss_scale: 32768.0000 (29462.8703)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1657  max mem: 15572
Epoch: [11]  [1050/1404]  eta: 0:03:38  lr: 0.000085  min_lr: 0.000001  loss: 4.2319 (4.1037)  class_acc: 0.2083 (0.2567)  loss_scale: 32768.0000 (29494.3178)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1914  max mem: 15572
Epoch: [11]  [1060/1404]  eta: 0:03:32  lr: 0.000085  min_lr: 0.000001  loss: 4.2020 (4.1042)  class_acc: 0.2083 (0.2564)  loss_scale: 32768.0000 (29525.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1267  max mem: 15572
Epoch: [11]  [1070/1404]  eta: 0:03:26  lr: 0.000085  min_lr: 0.000001  loss: 4.2265 (4.1059)  class_acc: 0.2083 (0.2560)  loss_scale: 32768.0000 (29555.4510)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.0791  max mem: 15572
Epoch: [11]  [1080/1404]  eta: 0:03:20  lr: 0.000085  min_lr: 0.000001  loss: 4.1988 (4.1059)  class_acc: 0.1667 (0.2557)  loss_scale: 32768.0000 (29585.1693)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.0666  max mem: 15572
Epoch: [11]  [1090/1404]  eta: 0:03:13  lr: 0.000085  min_lr: 0.000001  loss: 4.0693 (4.1045)  class_acc: 0.2083 (0.2560)  loss_scale: 32768.0000 (29614.3428)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.0728  max mem: 15572
Epoch: [11]  [1100/1404]  eta: 0:03:07  lr: 0.000085  min_lr: 0.000001  loss: 4.0072 (4.1038)  class_acc: 0.2917 (0.2561)  loss_scale: 32768.0000 (29642.9864)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0427  max mem: 15572
[2025-01-17 10:54:01,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:54:01,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:54:01,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:54:01,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:54:02,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16546
[2025-01-17 10:54:02,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:54:02,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:54:02,120] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16546
[2025-01-17 10:54:02,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [1110/1404]  eta: 0:03:01  lr: 0.000085  min_lr: 0.000001  loss: 4.2110 (4.1052)  class_acc: 0.2500 (0.2558)  loss_scale: 32768.0000 (29700.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.0760  max mem: 15572
Epoch: [11]  [1120/1404]  eta: 0:02:54  lr: 0.000085  min_lr: 0.000001  loss: 4.3239 (4.1052)  class_acc: 0.2500 (0.2560)  loss_scale: 32768.0000 (29727.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0664  max mem: 15572
Epoch: [11]  [1130/1404]  eta: 0:02:48  lr: 0.000085  min_lr: 0.000001  loss: 4.3040 (4.1059)  class_acc: 0.2917 (0.2563)  loss_scale: 32768.0000 (29754.8506)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.0449  max mem: 15572
Epoch: [11]  [1140/1404]  eta: 0:02:42  lr: 0.000085  min_lr: 0.000001  loss: 4.2343 (4.1067)  class_acc: 0.2917 (0.2565)  loss_scale: 32768.0000 (29781.2585)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0448  max mem: 15572
Epoch: [11]  [1150/1404]  eta: 0:02:36  lr: 0.000085  min_lr: 0.000001  loss: 4.2710 (4.1080)  class_acc: 0.2500 (0.2564)  loss_scale: 32768.0000 (29807.2076)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0172  max mem: 15572
Epoch: [11]  [1160/1404]  eta: 0:02:30  lr: 0.000085  min_lr: 0.000001  loss: 4.1716 (4.1061)  class_acc: 0.2500 (0.2564)  loss_scale: 32768.0000 (29832.7097)  weight_decay: 0.0500 (0.0500)  time: 0.6560  data: 0.0299  max mem: 15572
Epoch: [11]  [1170/1404]  eta: 0:02:24  lr: 0.000085  min_lr: 0.000001  loss: 3.9609 (4.1057)  class_acc: 0.2917 (0.2568)  loss_scale: 32768.0000 (29857.7763)  weight_decay: 0.0500 (0.0500)  time: 0.7048  data: 0.1004  max mem: 15572
Epoch: [11]  [1180/1404]  eta: 0:02:18  lr: 0.000085  min_lr: 0.000001  loss: 3.9832 (4.1051)  class_acc: 0.2917 (0.2569)  loss_scale: 32768.0000 (29882.4183)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0973  max mem: 15572
Epoch: [11]  [1190/1404]  eta: 0:02:11  lr: 0.000085  min_lr: 0.000001  loss: 4.1802 (4.1065)  class_acc: 0.2083 (0.2565)  loss_scale: 32768.0000 (29906.6465)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0329  max mem: 15572
Epoch: [11]  [1200/1404]  eta: 0:02:05  lr: 0.000085  min_lr: 0.000001  loss: 4.3893 (4.1075)  class_acc: 0.2083 (0.2566)  loss_scale: 32768.0000 (29930.4713)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.0649  max mem: 15572
Epoch: [11]  [1210/1404]  eta: 0:01:59  lr: 0.000085  min_lr: 0.000001  loss: 4.3643 (4.1077)  class_acc: 0.2083 (0.2568)  loss_scale: 32768.0000 (29953.9026)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.0869  max mem: 15572
Epoch: [11]  [1220/1404]  eta: 0:01:53  lr: 0.000085  min_lr: 0.000001  loss: 4.1010 (4.1068)  class_acc: 0.2083 (0.2568)  loss_scale: 32768.0000 (29976.9500)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.0801  max mem: 15572
Epoch: [11]  [1230/1404]  eta: 0:01:47  lr: 0.000085  min_lr: 0.000001  loss: 4.0677 (4.1068)  class_acc: 0.2083 (0.2567)  loss_scale: 32768.0000 (29999.6231)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.0514  max mem: 15572
[2025-01-17 10:55:21,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:55:21,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:55:21,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:55:21,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [1240/1404]  eta: 0:01:41  lr: 0.000085  min_lr: 0.000001  loss: 4.0677 (4.1068)  class_acc: 0.2500 (0.2570)  loss_scale: 32768.0000 (30285.9758)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0990  max mem: 15572
[2025-01-17 10:55:30,372] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16687
[2025-01-17 10:55:30,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:55:30,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 10:55:30,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16687
[2025-01-17 10:55:30,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [1250/1404]  eta: 0:01:35  lr: 0.000085  min_lr: 0.000001  loss: 4.2594 (4.1090)  class_acc: 0.2917 (0.2571)  loss_scale: 65536.0000 (30358.2030)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.1409  max mem: 15572
Epoch: [11]  [1260/1404]  eta: 0:01:28  lr: 0.000085  min_lr: 0.000001  loss: 4.3639 (4.1113)  class_acc: 0.1667 (0.2562)  loss_scale: 32768.0000 (30377.3132)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.0585  max mem: 15572
Epoch: [11]  [1270/1404]  eta: 0:01:22  lr: 0.000085  min_lr: 0.000001  loss: 4.3378 (4.1123)  class_acc: 0.1250 (0.2555)  loss_scale: 32768.0000 (30396.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.0465  max mem: 15572
Epoch: [11]  [1280/1404]  eta: 0:01:16  lr: 0.000085  min_lr: 0.000001  loss: 4.2006 (4.1117)  class_acc: 0.2083 (0.2558)  loss_scale: 32768.0000 (30414.6386)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0651  max mem: 15572
Epoch: [11]  [1290/1404]  eta: 0:01:10  lr: 0.000085  min_lr: 0.000001  loss: 4.0158 (4.1110)  class_acc: 0.2917 (0.2562)  loss_scale: 32768.0000 (30432.8675)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0794  max mem: 15572
Epoch: [11]  [1300/1404]  eta: 0:01:04  lr: 0.000085  min_lr: 0.000001  loss: 4.1841 (4.1135)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (30450.8163)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.0608  max mem: 15572
Epoch: [11]  [1310/1404]  eta: 0:00:57  lr: 0.000085  min_lr: 0.000001  loss: 4.3417 (4.1136)  class_acc: 0.2083 (0.2558)  loss_scale: 32768.0000 (30468.4912)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0008  max mem: 15572
Epoch: [11]  [1320/1404]  eta: 0:00:51  lr: 0.000085  min_lr: 0.000001  loss: 4.1609 (4.1149)  class_acc: 0.2083 (0.2553)  loss_scale: 32768.0000 (30485.8986)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0011  max mem: 15572
Epoch: [11]  [1330/1404]  eta: 0:00:45  lr: 0.000085  min_lr: 0.000001  loss: 4.1666 (4.1150)  class_acc: 0.2083 (0.2551)  loss_scale: 32768.0000 (30503.0443)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0010  max mem: 15572
Epoch: [11]  [1340/1404]  eta: 0:00:39  lr: 0.000085  min_lr: 0.000001  loss: 4.1436 (4.1152)  class_acc: 0.2083 (0.2550)  loss_scale: 32768.0000 (30519.9344)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0009  max mem: 15572
Epoch: [11]  [1350/1404]  eta: 0:00:33  lr: 0.000085  min_lr: 0.000001  loss: 4.0536 (4.1141)  class_acc: 0.2500 (0.2552)  loss_scale: 32768.0000 (30536.5744)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0009  max mem: 15572
Epoch: [11]  [1360/1404]  eta: 0:00:27  lr: 0.000085  min_lr: 0.000001  loss: 3.9483 (4.1145)  class_acc: 0.2500 (0.2554)  loss_scale: 32768.0000 (30552.9699)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.0006  max mem: 15572
Epoch: [11]  [1370/1404]  eta: 0:00:20  lr: 0.000085  min_lr: 0.000001  loss: 4.2044 (4.1149)  class_acc: 0.2500 (0.2553)  loss_scale: 32768.0000 (30569.1262)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0006  max mem: 15572
[2025-01-17 10:56:47,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:56:47,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:56:47,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:56:47,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:56:49,796] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16820
[2025-01-17 10:56:49,796] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16820
[2025-01-17 10:56:49,796] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:56:49,796] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:56:49,797] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [1380/1404]  eta: 0:00:14  lr: 0.000085  min_lr: 0.000001  loss: 4.0592 (4.1140)  class_acc: 0.2500 (0.2559)  loss_scale: 32768.0000 (30679.9594)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.0006  max mem: 15572
Epoch: [11]  [1390/1404]  eta: 0:00:08  lr: 0.000085  min_lr: 0.000001  loss: 4.0592 (4.1150)  class_acc: 0.2500 (0.2559)  loss_scale: 32768.0000 (30694.9705)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0008  max mem: 15572
Epoch: [11]  [1400/1404]  eta: 0:00:02  lr: 0.000085  min_lr: 0.000001  loss: 4.3117 (4.1158)  class_acc: 0.2500 (0.2558)  loss_scale: 32768.0000 (30709.7673)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0007  max mem: 15572
Epoch: [11]  [1403/1404]  eta: 0:00:00  lr: 0.000085  min_lr: 0.000001  loss: 4.1486 (4.1159)  class_acc: 0.2500 (0.2557)  loss_scale: 32768.0000 (30714.1652)  weight_decay: 0.0500 (0.0500)  time: 0.4064  data: 0.0007  max mem: 15572
Epoch: [11] Total time: 0:14:22 (0.6140 s / it)
Averaged stats: lr: 0.000085  min_lr: 0.000001  loss: 4.1486 (4.1245)  class_acc: 0.2500 (0.2562)  loss_scale: 32768.0000 (30714.1652)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:01  loss: 1.8822 (1.8822)  acc1: 61.1111 (61.1111)  acc5: 66.6667 (66.6667)  time: 6.1900  data: 5.9975  max mem: 15572
Val:  [ 10/136]  eta: 0:01:51  loss: 2.6692 (2.6534)  acc1: 33.3333 (31.8182)  acc5: 72.2222 (69.6970)  time: 0.8853  data: 0.6930  max mem: 15572
Val:  [ 20/136]  eta: 0:01:14  loss: 2.7602 (2.7626)  acc1: 27.7778 (30.4233)  acc5: 72.2222 (68.7831)  time: 0.3677  data: 0.1590  max mem: 15572
Val:  [ 30/136]  eta: 0:00:53  loss: 2.4855 (2.5862)  acc1: 38.8889 (35.8423)  acc5: 77.7778 (72.2222)  time: 0.3024  data: 0.0782  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.1293 (2.5545)  acc1: 50.0000 (38.7534)  acc5: 77.7778 (71.9512)  time: 0.2575  data: 0.0473  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.3908 (2.5695)  acc1: 44.4444 (39.2157)  acc5: 72.2222 (72.3312)  time: 0.3223  data: 0.1219  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.5964 (2.6476)  acc1: 33.3333 (36.8852)  acc5: 72.2222 (70.5829)  time: 0.3588  data: 0.1506  max mem: 15572
Val:  [ 70/136]  eta: 0:00:27  loss: 2.5952 (2.6307)  acc1: 33.3333 (38.3412)  acc5: 66.6667 (70.5790)  time: 0.3697  data: 0.1563  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.4890 (2.6214)  acc1: 44.4444 (38.4088)  acc5: 72.2222 (71.0562)  time: 0.3753  data: 0.1545  max mem: 15572
Val:  [ 90/136]  eta: 0:00:18  loss: 2.6706 (2.6351)  acc1: 27.7778 (36.9963)  acc5: 72.2222 (70.8181)  time: 0.3706  data: 0.1544  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9117 (2.7034)  acc1: 22.2222 (35.0385)  acc5: 61.1111 (68.9769)  time: 0.3929  data: 0.1883  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.9117 (2.7031)  acc1: 27.7778 (35.4855)  acc5: 61.1111 (68.9690)  time: 0.3912  data: 0.1907  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.3575 (2.6460)  acc1: 44.4444 (36.9146)  acc5: 77.7778 (70.4775)  time: 0.3747  data: 0.1805  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 2.0627 (2.5989)  acc1: 61.1111 (38.5496)  acc5: 83.3333 (71.2892)  time: 0.2999  data: 0.1296  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.2632 (2.6102)  acc1: 44.4444 (38.4111)  acc5: 77.7778 (71.1302)  time: 0.2295  data: 0.0770  max mem: 15572
Val: Total time: 0:00:51 (0.3800 s / it)
* Acc@1 38.391 Acc@5 69.799 loss 2.654
Accuracy of the network on the 4883 val videos: 38.4%
[2025-01-17 10:57:55,616] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 10:57:55,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 10:57:55,618] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 10:57:55,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 10:57:57,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 10:57:57,996] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.39%
Epoch: [12]  [   0/1404]  eta: 3:19:31  lr: 0.000085  min_lr: 0.000001  loss: 4.8998 (4.8998)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.5269  data: 8.0607  max mem: 15572
Epoch: [12]  [  10/1404]  eta: 0:31:27  lr: 0.000085  min_lr: 0.000001  loss: 4.1096 (4.0952)  class_acc: 0.2083 (0.2803)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3537  data: 0.8616  max mem: 15572
Epoch: [12]  [  20/1404]  eta: 0:22:01  lr: 0.000085  min_lr: 0.000001  loss: 4.1096 (4.1673)  class_acc: 0.2500 (0.2857)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.0712  max mem: 15572
Epoch: [12]  [  30/1404]  eta: 0:19:48  lr: 0.000085  min_lr: 0.000001  loss: 4.2000 (4.1596)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0895  max mem: 15572
Epoch: [12]  [  40/1404]  eta: 0:18:26  lr: 0.000085  min_lr: 0.000001  loss: 4.0744 (4.1339)  class_acc: 0.2917 (0.2886)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.1744  max mem: 15572
Epoch: [12]  [  50/1404]  eta: 0:17:27  lr: 0.000085  min_lr: 0.000001  loss: 4.0002 (4.1058)  class_acc: 0.2917 (0.2810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1499  max mem: 15572
Epoch: [12]  [  60/1404]  eta: 0:16:57  lr: 0.000085  min_lr: 0.000001  loss: 3.9526 (4.1155)  class_acc: 0.2500 (0.2705)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1498  max mem: 15572
Epoch: [12]  [  70/1404]  eta: 0:16:08  lr: 0.000085  min_lr: 0.000001  loss: 4.1396 (4.1277)  class_acc: 0.1667 (0.2647)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1181  max mem: 15572
Epoch: [12]  [  80/1404]  eta: 0:15:52  lr: 0.000085  min_lr: 0.000001  loss: 4.1213 (4.0965)  class_acc: 0.2500 (0.2706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1012  max mem: 15572
Epoch: [12]  [  90/1404]  eta: 0:15:22  lr: 0.000085  min_lr: 0.000001  loss: 3.9431 (4.0979)  class_acc: 0.2500 (0.2720)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0825  max mem: 15572
Epoch: [12]  [ 100/1404]  eta: 0:14:54  lr: 0.000085  min_lr: 0.000001  loss: 4.1549 (4.1025)  class_acc: 0.2500 (0.2715)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0281  max mem: 15572
[2025-01-17 10:59:07,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:59:07,877] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:59:07,885] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 10:59:07,885] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 10:59:10,662] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16954
[2025-01-17 10:59:10,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:59:10,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16954
[2025-01-17 10:59:10,712] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 10:59:10,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 110/1404]  eta: 0:14:37  lr: 0.000085  min_lr: 0.000001  loss: 4.1549 (4.0954)  class_acc: 0.2500 (0.2729)  loss_scale: 32768.0000 (34244.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0361  max mem: 15572
Epoch: [12]  [ 120/1404]  eta: 0:14:24  lr: 0.000085  min_lr: 0.000001  loss: 3.9823 (4.0925)  class_acc: 0.2500 (0.2710)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0438  max mem: 15572
Epoch: [12]  [ 130/1404]  eta: 0:14:13  lr: 0.000085  min_lr: 0.000001  loss: 4.1131 (4.0900)  class_acc: 0.2500 (0.2710)  loss_scale: 32768.0000 (34018.6870)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.0912  max mem: 15572
Epoch: [12]  [ 140/1404]  eta: 0:14:01  lr: 0.000085  min_lr: 0.000001  loss: 3.9573 (4.0804)  class_acc: 0.3333 (0.2763)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1291  max mem: 15572
Epoch: [12]  [ 150/1404]  eta: 0:13:47  lr: 0.000085  min_lr: 0.000001  loss: 3.9573 (4.0793)  class_acc: 0.3333 (0.2762)  loss_scale: 32768.0000 (33853.0331)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0888  max mem: 15572
[2025-01-17 10:59:38,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=100, lr=[8.190914307140542e-07, 8.190914307140542e-07, 1.1701306153057918e-06, 1.1701306153057918e-06, 1.6716151647225601e-06, 1.6716151647225601e-06, 2.3880216638893716e-06, 2.3880216638893716e-06, 3.4114595198419597e-06, 3.4114595198419597e-06, 4.873513599774229e-06, 4.873513599774229e-06, 6.962162285391755e-06, 6.962162285391755e-06, 9.945946121988223e-06, 9.945946121988223e-06, 1.4208494459983175e-05, 1.4208494459983175e-05, 2.0297849228547397e-05, 2.0297849228547397e-05, 2.899692746935342e-05, 2.899692746935342e-05, 4.142418209907632e-05, 4.142418209907632e-05, 5.917740299868046e-05, 5.917740299868046e-05, 8.45391471409721e-05, 8.45391471409721e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 10:59:38,237] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=44.72013595569328, CurrSamplesPerSec=47.34525156691378, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 160/1404]  eta: 0:13:36  lr: 0.000085  min_lr: 0.000001  loss: 4.0812 (4.0750)  class_acc: 0.2917 (0.2790)  loss_scale: 32768.0000 (33785.6398)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.0700  max mem: 15572
Epoch: [12]  [ 170/1404]  eta: 0:13:33  lr: 0.000085  min_lr: 0.000001  loss: 3.9948 (4.0756)  class_acc: 0.2917 (0.2785)  loss_scale: 32768.0000 (33726.1287)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.1278  max mem: 15572
Epoch: [12]  [ 180/1404]  eta: 0:13:22  lr: 0.000084  min_lr: 0.000001  loss: 3.9493 (4.0682)  class_acc: 0.2500 (0.2769)  loss_scale: 32768.0000 (33673.1934)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.1358  max mem: 15572
Epoch: [12]  [ 190/1404]  eta: 0:13:13  lr: 0.000084  min_lr: 0.000001  loss: 3.9493 (4.0612)  class_acc: 0.2500 (0.2777)  loss_scale: 32768.0000 (33625.8010)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0764  max mem: 15572
Epoch: [12]  [ 200/1404]  eta: 0:13:01  lr: 0.000084  min_lr: 0.000001  loss: 4.0563 (4.0637)  class_acc: 0.3333 (0.2792)  loss_scale: 32768.0000 (33583.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.0322  max mem: 15572
Epoch: [12]  [ 210/1404]  eta: 0:12:54  lr: 0.000084  min_lr: 0.000001  loss: 4.1384 (4.0678)  class_acc: 0.2917 (0.2776)  loss_scale: 32768.0000 (33544.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0414  max mem: 15572
Epoch: [12]  [ 220/1404]  eta: 0:12:48  lr: 0.000084  min_lr: 0.000001  loss: 4.0992 (4.0661)  class_acc: 0.2083 (0.2785)  loss_scale: 32768.0000 (33509.3575)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.0559  max mem: 15572
Epoch: [12]  [ 230/1404]  eta: 0:12:41  lr: 0.000084  min_lr: 0.000001  loss: 4.0699 (4.0625)  class_acc: 0.2917 (0.2778)  loss_scale: 32768.0000 (33477.2641)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0194  max mem: 15572
[2025-01-17 11:00:30,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:00:30,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:00:30,390] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:00:30,390] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:00:30,925] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17084
[2025-01-17 11:00:30,926] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:00:30,927] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17084
[2025-01-17 11:00:30,927] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:00:30,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 240/1404]  eta: 0:12:34  lr: 0.000084  min_lr: 0.000001  loss: 4.1880 (4.0769)  class_acc: 0.2500 (0.2749)  loss_scale: 32768.0000 (33583.8008)  weight_decay: 0.0500 (0.0500)  time: 0.6365  data: 0.0008  max mem: 15572
Epoch: [12]  [ 250/1404]  eta: 0:12:23  lr: 0.000084  min_lr: 0.000001  loss: 4.2523 (4.0741)  class_acc: 0.2500 (0.2776)  loss_scale: 32768.0000 (33551.2988)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.0007  max mem: 15572
Epoch: [12]  [ 260/1404]  eta: 0:12:17  lr: 0.000084  min_lr: 0.000001  loss: 4.0496 (4.0771)  class_acc: 0.2917 (0.2768)  loss_scale: 32768.0000 (33521.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.0075  max mem: 15572
Epoch: [12]  [ 270/1404]  eta: 0:12:06  lr: 0.000084  min_lr: 0.000001  loss: 4.2254 (4.0832)  class_acc: 0.2500 (0.2771)  loss_scale: 32768.0000 (33493.4908)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0077  max mem: 15572
Epoch: [12]  [ 280/1404]  eta: 0:11:56  lr: 0.000084  min_lr: 0.000001  loss: 4.1480 (4.0836)  class_acc: 0.2500 (0.2759)  loss_scale: 32768.0000 (33467.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0008  max mem: 15572
Epoch: [12]  [ 290/1404]  eta: 0:11:47  lr: 0.000084  min_lr: 0.000001  loss: 4.1741 (4.0862)  class_acc: 0.2500 (0.2743)  loss_scale: 32768.0000 (33443.6289)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0007  max mem: 15572
Epoch: [12]  [ 300/1404]  eta: 0:11:39  lr: 0.000084  min_lr: 0.000001  loss: 4.2154 (4.0897)  class_acc: 0.1667 (0.2727)  loss_scale: 32768.0000 (33421.1827)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0284  max mem: 15572
Epoch: [12]  [ 310/1404]  eta: 0:11:36  lr: 0.000084  min_lr: 0.000001  loss: 4.1151 (4.0939)  class_acc: 0.2500 (0.2730)  loss_scale: 32768.0000 (33400.1801)  weight_decay: 0.0500 (0.0500)  time: 0.6592  data: 0.1164  max mem: 15572
Epoch: [12]  [ 320/1404]  eta: 0:11:27  lr: 0.000084  min_lr: 0.000001  loss: 4.1048 (4.0962)  class_acc: 0.2500 (0.2736)  loss_scale: 32768.0000 (33380.4860)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1086  max mem: 15572
Epoch: [12]  [ 330/1404]  eta: 0:11:22  lr: 0.000084  min_lr: 0.000001  loss: 4.1419 (4.0977)  class_acc: 0.2917 (0.2733)  loss_scale: 32768.0000 (33361.9819)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0325  max mem: 15572
Epoch: [12]  [ 340/1404]  eta: 0:11:13  lr: 0.000084  min_lr: 0.000001  loss: 4.0115 (4.0938)  class_acc: 0.2917 (0.2741)  loss_scale: 32768.0000 (33344.5630)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0126  max mem: 15572
Epoch: [12]  [ 350/1404]  eta: 0:11:06  lr: 0.000084  min_lr: 0.000001  loss: 3.8784 (4.0884)  class_acc: 0.3333 (0.2775)  loss_scale: 32768.0000 (33328.1368)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.0135  max mem: 15572
Epoch: [12]  [ 360/1404]  eta: 0:11:00  lr: 0.000084  min_lr: 0.000001  loss: 4.0163 (4.0907)  class_acc: 0.2917 (0.2774)  loss_scale: 32768.0000 (33312.6205)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0264  max mem: 15572
[2025-01-17 11:01:49,361] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:01:49,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:01:49,363] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:01:49,363] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 370/1404]  eta: 0:10:52  lr: 0.000084  min_lr: 0.000001  loss: 4.1662 (4.0925)  class_acc: 0.2500 (0.2775)  loss_scale: 32768.0000 (33827.8814)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0137  max mem: 15572
[2025-01-17 11:01:54,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17222
[2025-01-17 11:01:54,255] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17222
[2025-01-17 11:01:54,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:01:54,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:01:54,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 380/1404]  eta: 0:10:43  lr: 0.000084  min_lr: 0.000001  loss: 4.1219 (4.0917)  class_acc: 0.2500 (0.2777)  loss_scale: 32768.0000 (34058.0787)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0214  max mem: 15572
Epoch: [12]  [ 390/1404]  eta: 0:10:40  lr: 0.000084  min_lr: 0.000001  loss: 4.0281 (4.0903)  class_acc: 0.2500 (0.2771)  loss_scale: 32768.0000 (34025.0844)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1216  max mem: 15572
Epoch: [12]  [ 400/1404]  eta: 0:10:31  lr: 0.000084  min_lr: 0.000001  loss: 4.0281 (4.0926)  class_acc: 0.2083 (0.2751)  loss_scale: 32768.0000 (33993.7357)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1011  max mem: 15572
Epoch: [12]  [ 410/1404]  eta: 0:10:23  lr: 0.000084  min_lr: 0.000001  loss: 4.1072 (4.0941)  class_acc: 0.2083 (0.2748)  loss_scale: 32768.0000 (33963.9124)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0177  max mem: 15572
Epoch: [12]  [ 420/1404]  eta: 0:10:16  lr: 0.000084  min_lr: 0.000001  loss: 4.0733 (4.0928)  class_acc: 0.2500 (0.2755)  loss_scale: 32768.0000 (33935.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.0632  max mem: 15572
Epoch: [12]  [ 430/1404]  eta: 0:10:12  lr: 0.000084  min_lr: 0.000001  loss: 4.0733 (4.0920)  class_acc: 0.2917 (0.2760)  loss_scale: 32768.0000 (33908.4176)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.1496  max mem: 15572
Epoch: [12]  [ 440/1404]  eta: 0:10:08  lr: 0.000084  min_lr: 0.000001  loss: 4.0985 (4.0915)  class_acc: 0.2917 (0.2758)  loss_scale: 32768.0000 (33882.5578)  weight_decay: 0.0500 (0.0500)  time: 0.7183  data: 0.2057  max mem: 15572
Epoch: [12]  [ 450/1404]  eta: 0:09:59  lr: 0.000084  min_lr: 0.000001  loss: 3.9950 (4.0877)  class_acc: 0.2917 (0.2774)  loss_scale: 32768.0000 (33857.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1317  max mem: 15572
Epoch: [12]  [ 460/1404]  eta: 0:09:52  lr: 0.000084  min_lr: 0.000001  loss: 3.9053 (4.0871)  class_acc: 0.2917 (0.2778)  loss_scale: 32768.0000 (33834.2039)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0652  max mem: 15572
Epoch: [12]  [ 470/1404]  eta: 0:09:45  lr: 0.000084  min_lr: 0.000001  loss: 4.0797 (4.0882)  class_acc: 0.2917 (0.2773)  loss_scale: 32768.0000 (33811.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0674  max mem: 15572
Epoch: [12]  [ 480/1404]  eta: 0:09:39  lr: 0.000084  min_lr: 0.000001  loss: 4.0797 (4.0896)  class_acc: 0.2500 (0.2765)  loss_scale: 32768.0000 (33789.8711)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0959  max mem: 15572
Epoch: [12]  [ 490/1404]  eta: 0:09:33  lr: 0.000084  min_lr: 0.000001  loss: 4.0734 (4.0863)  class_acc: 0.2500 (0.2766)  loss_scale: 32768.0000 (33769.0591)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.1384  max mem: 15572
Epoch: [12]  [ 500/1404]  eta: 0:09:26  lr: 0.000084  min_lr: 0.000001  loss: 4.1109 (4.0863)  class_acc: 0.2500 (0.2768)  loss_scale: 32768.0000 (33749.0778)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1211  max mem: 15572
[2025-01-17 11:03:13,754] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:03:13,754] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:03:13,796] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:03:13,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:03:15,485] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17354
[2025-01-17 11:03:15,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:03:15,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-01-17 11:03:15,556] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17354
[2025-01-17 11:03:15,556] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [ 510/1404]  eta: 0:09:20  lr: 0.000084  min_lr: 0.000001  loss: 4.1940 (4.0863)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (33922.2544)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.0969  max mem: 15572
Epoch: [12]  [ 520/1404]  eta: 0:09:14  lr: 0.000084  min_lr: 0.000001  loss: 4.0254 (4.0863)  class_acc: 0.2500 (0.2763)  loss_scale: 32768.0000 (33900.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6388  data: 0.1362  max mem: 15572
Epoch: [12]  [ 530/1404]  eta: 0:09:07  lr: 0.000084  min_lr: 0.000001  loss: 4.0111 (4.0866)  class_acc: 0.2917 (0.2772)  loss_scale: 32768.0000 (33878.7797)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1391  max mem: 15572
Epoch: [12]  [ 540/1404]  eta: 0:09:01  lr: 0.000084  min_lr: 0.000001  loss: 4.0111 (4.0852)  class_acc: 0.2917 (0.2775)  loss_scale: 32768.0000 (33858.2477)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1187  max mem: 15572
Epoch: [12]  [ 550/1404]  eta: 0:08:55  lr: 0.000084  min_lr: 0.000001  loss: 4.0437 (4.0841)  class_acc: 0.2917 (0.2781)  loss_scale: 32768.0000 (33838.4610)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1100  max mem: 15572
Epoch: [12]  [ 560/1404]  eta: 0:08:48  lr: 0.000084  min_lr: 0.000001  loss: 4.0589 (4.0839)  class_acc: 0.2917 (0.2783)  loss_scale: 32768.0000 (33819.3797)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0988  max mem: 15572
Epoch: [12]  [ 570/1404]  eta: 0:08:41  lr: 0.000084  min_lr: 0.000001  loss: 4.2025 (4.0880)  class_acc: 0.2500 (0.2770)  loss_scale: 32768.0000 (33800.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0833  max mem: 15572
Epoch: [12]  [ 580/1404]  eta: 0:08:35  lr: 0.000084  min_lr: 0.000001  loss: 4.2272 (4.0901)  class_acc: 0.2500 (0.2765)  loss_scale: 32768.0000 (33783.1876)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0852  max mem: 15572
Epoch: [12]  [ 590/1404]  eta: 0:08:29  lr: 0.000084  min_lr: 0.000001  loss: 4.1317 (4.0915)  class_acc: 0.2500 (0.2755)  loss_scale: 32768.0000 (33766.0102)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1233  max mem: 15572
Epoch: [12]  [ 600/1404]  eta: 0:08:24  lr: 0.000084  min_lr: 0.000001  loss: 3.9296 (4.0886)  class_acc: 0.2500 (0.2762)  loss_scale: 32768.0000 (33749.4043)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.1341  max mem: 15572
Epoch: [12]  [ 610/1404]  eta: 0:08:15  lr: 0.000084  min_lr: 0.000001  loss: 4.0899 (4.0861)  class_acc: 0.2917 (0.2763)  loss_scale: 32768.0000 (33733.3421)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.0674  max mem: 15572
Epoch: [12]  [ 620/1404]  eta: 0:08:09  lr: 0.000084  min_lr: 0.000001  loss: 4.1714 (4.0865)  class_acc: 0.2500 (0.2758)  loss_scale: 32768.0000 (33717.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0277  max mem: 15572
Epoch: [12]  [ 630/1404]  eta: 0:08:02  lr: 0.000084  min_lr: 0.000001  loss: 4.2377 (4.0880)  class_acc: 0.2500 (0.2753)  loss_scale: 32768.0000 (33702.7448)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0617  max mem: 15572
[2025-01-17 11:04:34,701] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:04:34,701] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:04:34,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:04:34,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 640/1404]  eta: 0:07:56  lr: 0.000084  min_lr: 0.000001  loss: 4.2864 (4.0902)  class_acc: 0.2083 (0.2742)  loss_scale: 32768.0000 (33994.8830)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0673  max mem: 15572
[2025-01-17 11:04:38,369] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17489
[2025-01-17 11:04:38,369] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:04:38,403] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17489
[2025-01-17 11:04:38,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:04:38,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 650/1404]  eta: 0:07:49  lr: 0.000084  min_lr: 0.000001  loss: 4.0978 (4.0892)  class_acc: 0.2500 (0.2745)  loss_scale: 32768.0000 (33976.0369)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0759  max mem: 15572
Epoch: [12]  [ 660/1404]  eta: 0:07:43  lr: 0.000084  min_lr: 0.000001  loss: 4.0978 (4.0916)  class_acc: 0.2917 (0.2744)  loss_scale: 32768.0000 (33957.7610)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0611  max mem: 15572
Epoch: [12]  [ 670/1404]  eta: 0:07:36  lr: 0.000084  min_lr: 0.000001  loss: 4.1773 (4.0909)  class_acc: 0.2500 (0.2742)  loss_scale: 32768.0000 (33940.0298)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0187  max mem: 15572
Epoch: [12]  [ 680/1404]  eta: 0:07:29  lr: 0.000084  min_lr: 0.000001  loss: 4.0682 (4.0928)  class_acc: 0.2083 (0.2737)  loss_scale: 32768.0000 (33922.8194)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0152  max mem: 15572
Epoch: [12]  [ 690/1404]  eta: 0:07:23  lr: 0.000084  min_lr: 0.000001  loss: 4.2154 (4.0952)  class_acc: 0.2083 (0.2727)  loss_scale: 32768.0000 (33906.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.0675  max mem: 15572
Epoch: [12]  [ 700/1404]  eta: 0:07:17  lr: 0.000084  min_lr: 0.000001  loss: 4.0900 (4.0936)  class_acc: 0.2500 (0.2731)  loss_scale: 32768.0000 (33889.8716)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1164  max mem: 15572
Epoch: [12]  [ 710/1404]  eta: 0:07:11  lr: 0.000084  min_lr: 0.000001  loss: 4.0489 (4.0926)  class_acc: 0.2917 (0.2736)  loss_scale: 32768.0000 (33874.0928)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.1257  max mem: 15572
Epoch: [12]  [ 720/1404]  eta: 0:07:05  lr: 0.000083  min_lr: 0.000001  loss: 4.0891 (4.0921)  class_acc: 0.2500 (0.2732)  loss_scale: 32768.0000 (33858.7517)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.1277  max mem: 15572
Epoch: [12]  [ 730/1404]  eta: 0:07:00  lr: 0.000083  min_lr: 0.000001  loss: 4.1504 (4.0925)  class_acc: 0.2500 (0.2733)  loss_scale: 32768.0000 (33843.8304)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.1273  max mem: 15572
Epoch: [12]  [ 740/1404]  eta: 0:06:53  lr: 0.000083  min_lr: 0.000001  loss: 4.1504 (4.0944)  class_acc: 0.2500 (0.2722)  loss_scale: 32768.0000 (33829.3117)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0789  max mem: 15572
Epoch: [12]  [ 750/1404]  eta: 0:06:47  lr: 0.000083  min_lr: 0.000001  loss: 4.2186 (4.0962)  class_acc: 0.2083 (0.2717)  loss_scale: 32768.0000 (33815.1798)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0417  max mem: 15572
Epoch: [12]  [ 760/1404]  eta: 0:06:41  lr: 0.000083  min_lr: 0.000001  loss: 4.2231 (4.0973)  class_acc: 0.2083 (0.2712)  loss_scale: 32768.0000 (33801.4192)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1075  max mem: 15572
[2025-01-17 11:05:59,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:05:59,422] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:05:59,422] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:05:59,422] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 770/1404]  eta: 0:06:35  lr: 0.000083  min_lr: 0.000001  loss: 4.0908 (4.0958)  class_acc: 0.2500 (0.2712)  loss_scale: 32768.0000 (33830.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6795  data: 0.1627  max mem: 15572
[2025-01-17 11:06:00,970] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17621
[2025-01-17 11:06:00,970] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:06:00,971] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17621
[2025-01-17 11:06:00,972] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:06:00,972] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 780/1404]  eta: 0:06:29  lr: 0.000083  min_lr: 0.000001  loss: 3.9321 (4.0942)  class_acc: 0.2500 (0.2716)  loss_scale: 32768.0000 (33900.8246)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1275  max mem: 15572
Epoch: [12]  [ 790/1404]  eta: 0:06:24  lr: 0.000083  min_lr: 0.000001  loss: 4.1883 (4.0952)  class_acc: 0.2500 (0.2709)  loss_scale: 32768.0000 (33886.5032)  weight_decay: 0.0500 (0.0500)  time: 0.6837  data: 0.1601  max mem: 15572
Epoch: [12]  [ 800/1404]  eta: 0:06:17  lr: 0.000083  min_lr: 0.000001  loss: 4.0719 (4.0950)  class_acc: 0.2917 (0.2713)  loss_scale: 32768.0000 (33872.5393)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.1626  max mem: 15572
Epoch: [12]  [ 810/1404]  eta: 0:06:11  lr: 0.000083  min_lr: 0.000001  loss: 4.0474 (4.0960)  class_acc: 0.2917 (0.2710)  loss_scale: 32768.0000 (33858.9199)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0954  max mem: 15572
Epoch: [12]  [ 820/1404]  eta: 0:06:05  lr: 0.000083  min_lr: 0.000001  loss: 3.9941 (4.0951)  class_acc: 0.2083 (0.2719)  loss_scale: 32768.0000 (33845.6322)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.1422  max mem: 15572
Epoch: [12]  [ 830/1404]  eta: 0:05:59  lr: 0.000083  min_lr: 0.000001  loss: 4.1392 (4.0958)  class_acc: 0.2500 (0.2713)  loss_scale: 32768.0000 (33832.6643)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.1613  max mem: 15572
Epoch: [12]  [ 840/1404]  eta: 0:05:52  lr: 0.000083  min_lr: 0.000001  loss: 4.0743 (4.0930)  class_acc: 0.2500 (0.2719)  loss_scale: 32768.0000 (33820.0048)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1174  max mem: 15572
Epoch: [12]  [ 850/1404]  eta: 0:05:47  lr: 0.000083  min_lr: 0.000001  loss: 3.8243 (4.0908)  class_acc: 0.2917 (0.2718)  loss_scale: 32768.0000 (33807.6428)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.1754  max mem: 15572
Epoch: [12]  [ 860/1404]  eta: 0:05:41  lr: 0.000083  min_lr: 0.000001  loss: 3.9521 (4.0908)  class_acc: 0.2500 (0.2719)  loss_scale: 32768.0000 (33795.5679)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.1891  max mem: 15572
Epoch: [12]  [ 870/1404]  eta: 0:05:35  lr: 0.000083  min_lr: 0.000001  loss: 4.2729 (4.0919)  class_acc: 0.2917 (0.2722)  loss_scale: 32768.0000 (33783.7704)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1097  max mem: 15572
Epoch: [12]  [ 880/1404]  eta: 0:05:29  lr: 0.000083  min_lr: 0.000001  loss: 3.8353 (4.0882)  class_acc: 0.3750 (0.2739)  loss_scale: 32768.0000 (33772.2406)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.0800  max mem: 15572
Epoch: [12]  [ 890/1404]  eta: 0:05:23  lr: 0.000083  min_lr: 0.000001  loss: 3.9788 (4.0915)  class_acc: 0.3333 (0.2732)  loss_scale: 32768.0000 (33760.9697)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.0491  max mem: 15572
Epoch: [12]  [ 900/1404]  eta: 0:05:17  lr: 0.000083  min_lr: 0.000001  loss: 4.1876 (4.0915)  class_acc: 0.2500 (0.2732)  loss_scale: 32768.0000 (33749.9489)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.0681  max mem: 15572
[2025-01-17 11:07:27,234] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:07:27,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:07:27,266] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:07:27,266] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-17 11:07:28,248] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17752
[2025-01-17 11:07:28,248] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:07:28,274] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17752
[2025-01-17 11:07:28,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-17 11:07:28,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 910/1404]  eta: 0:05:12  lr: 0.000083  min_lr: 0.000001  loss: 4.0966 (4.0923)  class_acc: 0.2083 (0.2725)  loss_scale: 32768.0000 (33811.1087)  weight_decay: 0.0500 (0.0500)  time: 0.7112  data: 0.1428  max mem: 15572
Epoch: [12]  [ 920/1404]  eta: 0:05:06  lr: 0.000083  min_lr: 0.000001  loss: 4.0966 (4.0919)  class_acc: 0.2083 (0.2726)  loss_scale: 32768.0000 (33799.7828)  weight_decay: 0.0500 (0.0500)  time: 0.7422  data: 0.1909  max mem: 15572
[2025-01-17 11:07:43,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17772
[2025-01-17 11:07:43,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 11:07:43,382] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17772
[2025-01-17 11:07:43,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 11:07:43,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [ 930/1404]  eta: 0:05:00  lr: 0.000083  min_lr: 0.000001  loss: 4.0448 (4.0915)  class_acc: 0.2500 (0.2729)  loss_scale: 32768.0000 (33665.5124)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.1904  max mem: 15572
Epoch: [12]  [ 940/1404]  eta: 0:04:53  lr: 0.000083  min_lr: 0.000001  loss: 4.0613 (4.0910)  class_acc: 0.2500 (0.2726)  loss_scale: 16384.0000 (33481.8618)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.1335  max mem: 15572
Epoch: [12]  [ 950/1404]  eta: 0:04:47  lr: 0.000083  min_lr: 0.000001  loss: 4.0618 (4.0912)  class_acc: 0.2500 (0.2725)  loss_scale: 16384.0000 (33302.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6482  data: 0.0993  max mem: 15572
Epoch: [12]  [ 960/1404]  eta: 0:04:42  lr: 0.000083  min_lr: 0.000001  loss: 4.0320 (4.0917)  class_acc: 0.2500 (0.2722)  loss_scale: 16384.0000 (33126.0271)  weight_decay: 0.0500 (0.0500)  time: 0.7230  data: 0.1760  max mem: 15572
Epoch: [12]  [ 970/1404]  eta: 0:04:35  lr: 0.000083  min_lr: 0.000001  loss: 3.9983 (4.0897)  class_acc: 0.2917 (0.2727)  loss_scale: 16384.0000 (32953.6066)  weight_decay: 0.0500 (0.0500)  time: 0.7317  data: 0.1938  max mem: 15572
Epoch: [12]  [ 980/1404]  eta: 0:04:29  lr: 0.000083  min_lr: 0.000001  loss: 3.9834 (4.0895)  class_acc: 0.2917 (0.2731)  loss_scale: 16384.0000 (32784.7013)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0842  max mem: 15572
Epoch: [12]  [ 990/1404]  eta: 0:04:22  lr: 0.000083  min_lr: 0.000001  loss: 4.1061 (4.0902)  class_acc: 0.2500 (0.2730)  loss_scale: 16384.0000 (32619.2048)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0434  max mem: 15572
Epoch: [12]  [1000/1404]  eta: 0:04:16  lr: 0.000083  min_lr: 0.000001  loss: 4.0037 (4.0893)  class_acc: 0.2500 (0.2731)  loss_scale: 16384.0000 (32457.0150)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.0975  max mem: 15572
Epoch: [12]  [1010/1404]  eta: 0:04:10  lr: 0.000083  min_lr: 0.000001  loss: 3.9777 (4.0899)  class_acc: 0.2917 (0.2730)  loss_scale: 16384.0000 (32298.0336)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.1411  max mem: 15572
Epoch: [12]  [1020/1404]  eta: 0:04:04  lr: 0.000083  min_lr: 0.000001  loss: 4.0537 (4.0895)  class_acc: 0.2500 (0.2733)  loss_scale: 16384.0000 (32142.1665)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.1199  max mem: 15572
Epoch: [12]  [1030/1404]  eta: 0:03:57  lr: 0.000083  min_lr: 0.000001  loss: 4.0193 (4.0901)  class_acc: 0.2500 (0.2733)  loss_scale: 16384.0000 (31989.3230)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1142  max mem: 15572
Epoch: [12]  [1040/1404]  eta: 0:03:51  lr: 0.000083  min_lr: 0.000001  loss: 4.0131 (4.0885)  class_acc: 0.2917 (0.2735)  loss_scale: 16384.0000 (31839.4159)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1156  max mem: 15572
Epoch: [12]  [1050/1404]  eta: 0:03:45  lr: 0.000083  min_lr: 0.000001  loss: 3.9930 (4.0886)  class_acc: 0.2500 (0.2735)  loss_scale: 16384.0000 (31692.3616)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1100  max mem: 15572
[2025-01-17 11:09:08,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:09:08,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 11:09:08,198] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:09:08,198] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [1060/1404]  eta: 0:03:38  lr: 0.000083  min_lr: 0.000001  loss: 4.1797 (4.0897)  class_acc: 0.2500 (0.2732)  loss_scale: 16384.0000 (31671.6155)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0931  max mem: 15572
Epoch: [12]  [1070/1404]  eta: 0:03:31  lr: 0.000083  min_lr: 0.000001  loss: 4.2819 (4.0907)  class_acc: 0.2083 (0.2732)  loss_scale: 32768.0000 (31681.8525)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0513  max mem: 15572
Epoch: [12]  [1080/1404]  eta: 0:03:25  lr: 0.000083  min_lr: 0.000001  loss: 4.2980 (4.0904)  class_acc: 0.2083 (0.2732)  loss_scale: 32768.0000 (31691.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1200  max mem: 15572
Epoch: [12]  [1090/1404]  eta: 0:03:19  lr: 0.000083  min_lr: 0.000001  loss: 4.1402 (4.0916)  class_acc: 0.2083 (0.2729)  loss_scale: 32768.0000 (31701.7635)  weight_decay: 0.0500 (0.0500)  time: 0.6925  data: 0.1352  max mem: 15572
Epoch: [12]  [1100/1404]  eta: 0:03:13  lr: 0.000083  min_lr: 0.000001  loss: 4.1402 (4.0921)  class_acc: 0.2083 (0.2726)  loss_scale: 32768.0000 (31711.4478)  weight_decay: 0.0500 (0.0500)  time: 0.7257  data: 0.0347  max mem: 15572
Epoch: [12]  [1110/1404]  eta: 0:03:06  lr: 0.000083  min_lr: 0.000001  loss: 4.1247 (4.0908)  class_acc: 0.2500 (0.2731)  loss_scale: 32768.0000 (31720.9577)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.0008  max mem: 15572
Epoch: [12]  [1120/1404]  eta: 0:03:00  lr: 0.000083  min_lr: 0.000001  loss: 3.9833 (4.0908)  class_acc: 0.2917 (0.2731)  loss_scale: 32768.0000 (31730.2979)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.0008  max mem: 15572
Epoch: [12]  [1130/1404]  eta: 0:02:54  lr: 0.000083  min_lr: 0.000001  loss: 4.0793 (4.0898)  class_acc: 0.2083 (0.2727)  loss_scale: 32768.0000 (31739.4730)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0008  max mem: 15572
Epoch: [12]  [1140/1404]  eta: 0:02:47  lr: 0.000083  min_lr: 0.000001  loss: 4.0793 (4.0894)  class_acc: 0.2083 (0.2726)  loss_scale: 32768.0000 (31748.4873)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.0008  max mem: 15572
[2025-01-17 11:10:07,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17997
[2025-01-17 11:10:07,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-17 11:10:07,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-01-17 11:10:07,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17997
[2025-01-17 11:10:07,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [12]  [1150/1404]  eta: 0:02:41  lr: 0.000083  min_lr: 0.000001  loss: 4.0661 (4.0884)  class_acc: 0.2083 (0.2730)  loss_scale: 32768.0000 (31728.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0251  max mem: 15572
[2025-01-17 11:10:10,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=108, lr=[8.01083251096546e-07, 8.01083251096546e-07, 1.1444046444236373e-06, 1.1444046444236373e-06, 1.6348637777480533e-06, 1.6348637777480533e-06, 2.3355196824972193e-06, 2.3355196824972193e-06, 3.336456689281742e-06, 3.336456689281742e-06, 4.7663666989739175e-06, 4.7663666989739175e-06, 6.809095284248453e-06, 6.809095284248453e-06, 9.727278977497792e-06, 9.727278977497792e-06, 1.3896112824996845e-05, 1.3896112824996845e-05, 1.9851589749995496e-05, 1.9851589749995496e-05, 2.8359413928564995e-05, 2.8359413928564995e-05, 4.0513448469378566e-05, 4.0513448469378566e-05, 5.78763549562551e-05, 5.78763549562551e-05, 8.268050708036443e-05, 8.268050708036443e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-17 11:10:10,569] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=44.82564713422687, CurrSamplesPerSec=47.90061969932948, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [1160/1404]  eta: 0:02:34  lr: 0.000083  min_lr: 0.000001  loss: 4.0729 (4.0883)  class_acc: 0.2500 (0.2730)  loss_scale: 16384.0000 (31596.7063)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0250  max mem: 15572
Epoch: [12]  [1170/1404]  eta: 0:02:28  lr: 0.000083  min_lr: 0.000001  loss: 4.1203 (4.0893)  class_acc: 0.2500 (0.2729)  loss_scale: 16384.0000 (31466.7942)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0257  max mem: 15572
Epoch: [12]  [1180/1404]  eta: 0:02:22  lr: 0.000083  min_lr: 0.000001  loss: 4.0958 (4.0893)  class_acc: 0.2500 (0.2729)  loss_scale: 16384.0000 (31339.0821)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0315  max mem: 15572
Epoch: [12]  [1190/1404]  eta: 0:02:15  lr: 0.000083  min_lr: 0.000001  loss: 4.2359 (4.0913)  class_acc: 0.2500 (0.2724)  loss_scale: 16384.0000 (31213.5147)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0195  max mem: 15572
Epoch: [12]  [1200/1404]  eta: 0:02:09  lr: 0.000083  min_lr: 0.000001  loss: 4.2802 (4.0918)  class_acc: 0.2500 (0.2720)  loss_scale: 16384.0000 (31090.0383)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0234  max mem: 15572
Epoch: [12]  [1210/1404]  eta: 0:02:03  lr: 0.000083  min_lr: 0.000001  loss: 4.0563 (4.0917)  class_acc: 0.2917 (0.2725)  loss_scale: 16384.0000 (30968.6012)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.0104  max mem: 15572
Epoch: [12]  [1220/1404]  eta: 0:01:56  lr: 0.000083  min_lr: 0.000001  loss: 4.0413 (4.0916)  class_acc: 0.2917 (0.2727)  loss_scale: 16384.0000 (30849.1532)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.0007  max mem: 15572
Epoch: [12]  [1230/1404]  eta: 0:01:50  lr: 0.000083  min_lr: 0.000001  loss: 4.1773 (4.0931)  class_acc: 0.2500 (0.2722)  loss_scale: 16384.0000 (30731.6458)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.0008  max mem: 15572
Epoch: [12]  [1240/1404]  eta: 0:01:43  lr: 0.000083  min_lr: 0.000001  loss: 4.1624 (4.0919)  class_acc: 0.2500 (0.2722)  loss_scale: 16384.0000 (30616.0322)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.0011  max mem: 15572
Epoch: [12]  [1250/1404]  eta: 0:01:37  lr: 0.000082  min_lr: 0.000001  loss: 4.1624 (4.0934)  class_acc: 0.2500 (0.2718)  loss_scale: 16384.0000 (30502.2670)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0010  max mem: 15572
Epoch: [12]  [1260/1404]  eta: 0:01:31  lr: 0.000082  min_lr: 0.000001  loss: 4.2959 (4.0944)  class_acc: 0.2083 (0.2715)  loss_scale: 16384.0000 (30390.3061)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0009  max mem: 15572
Epoch: [12]  [1270/1404]  eta: 0:01:24  lr: 0.000082  min_lr: 0.000001  loss: 4.2245 (4.0948)  class_acc: 0.2083 (0.2712)  loss_scale: 16384.0000 (30280.1070)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.0038  max mem: 15572
[2025-01-17 11:11:27,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:11:27,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-01-17 11:11:27,598] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-17 11:11:27,598] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [1280/1404]  eta: 0:01:18  lr: 0.000082  min_lr: 0.000001  loss: 4.1101 (4.0947)  class_acc: 0.2917 (0.2714)  loss_scale: 16384.0000 (30209.9984)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.0037  max mem: 15572
Epoch: [12]  [1290/1404]  eta: 0:01:12  lr: 0.000082  min_lr: 0.000001  loss: 3.9640 (4.0932)  class_acc: 0.3333 (0.2724)  loss_scale: 32768.0000 (30229.8125)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.0007  max mem: 15572
Epoch: [12]  [1300/1404]  eta: 0:01:05  lr: 0.000082  min_lr: 0.000001  loss: 3.9852 (4.0942)  class_acc: 0.2917 (0.2719)  loss_scale: 32768.0000 (30249.3221)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0006  max mem: 15572
Epoch: [12]  [1310/1404]  eta: 0:00:59  lr: 0.000082  min_lr: 0.000001  loss: 4.2871 (4.0952)  class_acc: 0.2500 (0.2723)  loss_scale: 32768.0000 (30268.5339)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0006  max mem: 15572
Epoch: [12]  [1320/1404]  eta: 0:00:53  lr: 0.000082  min_lr: 0.000001  loss: 4.1935 (4.0954)  class_acc: 0.2083 (0.2722)  loss_scale: 32768.0000 (30287.4550)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0009  max mem: 15572
Epoch: [12]  [1330/1404]  eta: 0:00:46  lr: 0.000082  min_lr: 0.000001  loss: 4.1813 (4.0953)  class_acc: 0.2917 (0.2726)  loss_scale: 32768.0000 (30306.0917)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0546  max mem: 15572
Epoch: [12]  [1340/1404]  eta: 0:00:40  lr: 0.000082  min_lr: 0.000001  loss: 4.1280 (4.0963)  class_acc: 0.2917 (0.2727)  loss_scale: 32768.0000 (30324.4504)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0545  max mem: 15572
Epoch: [12]  [1350/1404]  eta: 0:00:34  lr: 0.000082  min_lr: 0.000001  loss: 4.1975 (4.0981)  class_acc: 0.2083 (0.2723)  loss_scale: 32768.0000 (30342.5374)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0011  max mem: 15572
Epoch: [12]  [1360/1404]  eta: 0:00:27  lr: 0.000082  min_lr: 0.000001  loss: 4.2147 (4.0980)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (30360.3586)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0013  max mem: 15572
Epoch: [12]  [1370/1404]  eta: 0:00:21  lr: 0.000082  min_lr: 0.000001  loss: 4.0878 (4.0980)  class_acc: 0.2917 (0.2728)  loss_scale: 32768.0000 (30377.9198)  weight_decay: 0.0500 (0.0500)  time: 0.6591  data: 0.0010  max mem: 15572
Epoch: [12]  [1380/1404]  eta: 0:00:15  lr: 0.000082  min_lr: 0.000001  loss: 4.0741 (4.0977)  class_acc: 0.2917 (0.2730)  loss_scale: 32768.0000 (30395.2266)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.0006  max mem: 15572
Epoch: [12]  [1390/1404]  eta: 0:00:08  lr: 0.000082  min_lr: 0.000001  loss: 4.0944 (4.0981)  class_acc: 0.2500 (0.2729)  loss_scale: 32768.0000 (30412.2847)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0006  max mem: 15572
Epoch: [12]  [1400/1404]  eta: 0:00:02  lr: 0.000082  min_lr: 0.000001  loss: 4.1670 (4.0975)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (30429.0992)  weight_decay: 0.0500 (0.0500)  time: 0.4628  data: 0.0005  max mem: 15572
Epoch: [12]  [1403/1404]  eta: 0:00:00  lr: 0.000082  min_lr: 0.000001  loss: 4.1054 (4.0974)  class_acc: 0.2500 (0.2730)  loss_scale: 32768.0000 (30434.0969)  weight_decay: 0.0500 (0.0500)  time: 0.4460  data: 0.0005  max mem: 15572
Epoch: [12] Total time: 0:14:43 (0.6291 s / it)
Averaged stats: lr: 0.000082  min_lr: 0.000001  loss: 4.1054 (4.0911)  class_acc: 0.2500 (0.2752)  loss_scale: 32768.0000 (30434.0969)  weight_decay: 0.0500 (0.0500)
Val:  [  0/136]  eta: 0:14:21  loss: 2.0058 (2.0058)  acc1: 66.6667 (66.6667)  acc5: 72.2222 (72.2222)  time: 6.3343  data: 6.0616  max mem: 15572
Val:  [ 10/136]  eta: 0:01:45  loss: 2.6315 (2.6035)  acc1: 50.0000 (40.4040)  acc5: 72.2222 (70.2020)  time: 0.8389  data: 0.6314  max mem: 15572
Val:  [ 20/136]  eta: 0:01:07  loss: 2.7120 (2.6727)  acc1: 38.8889 (38.3598)  acc5: 72.2222 (70.6349)  time: 0.2910  data: 0.0817  max mem: 15572
Val:  [ 30/136]  eta: 0:00:51  loss: 2.5905 (2.4945)  acc1: 38.8889 (43.1900)  acc5: 77.7778 (73.1183)  time: 0.2951  data: 0.0657  max mem: 15572
Val:  [ 40/136]  eta: 0:00:43  loss: 2.0143 (2.4620)  acc1: 50.0000 (43.2249)  acc5: 77.7778 (74.1192)  time: 0.3252  data: 0.1015  max mem: 15572
Val:  [ 50/136]  eta: 0:00:37  loss: 2.3880 (2.4928)  acc1: 38.8889 (42.8105)  acc5: 77.7778 (74.4009)  time: 0.3660  data: 0.1548  max mem: 15572
Val:  [ 60/136]  eta: 0:00:32  loss: 2.6158 (2.5791)  acc1: 27.7778 (39.5264)  acc5: 72.2222 (72.3133)  time: 0.3907  data: 0.1778  max mem: 15572
Val:  [ 70/136]  eta: 0:00:28  loss: 2.6422 (2.5563)  acc1: 33.3333 (40.8451)  acc5: 72.2222 (72.4570)  time: 0.3875  data: 0.1849  max mem: 15572
Val:  [ 80/136]  eta: 0:00:23  loss: 2.3261 (2.5280)  acc1: 50.0000 (41.4952)  acc5: 77.7778 (73.5254)  time: 0.3775  data: 0.1787  max mem: 15572
Val:  [ 90/136]  eta: 0:00:19  loss: 2.3795 (2.5466)  acc1: 33.3333 (40.2930)  acc5: 77.7778 (73.0769)  time: 0.4056  data: 0.2085  max mem: 15572
Val:  [100/136]  eta: 0:00:14  loss: 2.9408 (2.6228)  acc1: 22.2222 (38.2288)  acc5: 66.6667 (71.1221)  time: 0.4009  data: 0.2047  max mem: 15572
Val:  [110/136]  eta: 0:00:10  loss: 2.7718 (2.6139)  acc1: 33.3333 (38.9890)  acc5: 66.6667 (71.2713)  time: 0.3695  data: 0.1718  max mem: 15572
Val:  [120/136]  eta: 0:00:06  loss: 2.1688 (2.5628)  acc1: 55.5556 (40.5418)  acc5: 77.7778 (72.4977)  time: 0.3475  data: 0.1368  max mem: 15572
Val:  [130/136]  eta: 0:00:02  loss: 1.9249 (2.5273)  acc1: 55.5556 (41.7727)  acc5: 83.3333 (72.9432)  time: 0.2650  data: 0.0721  max mem: 15572
Val:  [135/136]  eta: 0:00:00  loss: 2.1750 (2.5313)  acc1: 50.0000 (41.8509)  acc5: 77.7778 (72.8501)  time: 0.2097  data: 0.0437  max mem: 15572
Val: Total time: 0:00:51 (0.3815 s / it)
* Acc@1 40.274 Acc@5 71.437 loss 2.581
Accuracy of the network on the 4883 val videos: 40.3%
[2025-01-17 11:13:33,460] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-17 11:13:33,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-01-17 11:13:33,463] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-17 11:13:33,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-17 11:13:35,190] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_30/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-17 11:13:35,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.27%
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 4051257) of binary: /home/maggie/miniconda3/envs/timesformer/bin/python
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
run_class_finetuning.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-17_11:13:40
  host      : maggie-Tornado-R5Q-iCUE-Certified
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 4051258)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 4051258
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-17_11:13:40
  host      : maggie-Tornado-R5Q-iCUE-Certified
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 4051257)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 4051257
========================================================
