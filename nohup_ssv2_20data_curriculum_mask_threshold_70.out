/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-15 14:21:57,296] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=70, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f03a42e43a0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-15 14:22:00,009] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-15 14:22:00,010] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-15 14:22:00,032] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.03861069679260254 seconds
[2025-01-15 14:22:00,232] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-15 14:22:00,233] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-15 14:22:00,235] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-15 14:22:00,235] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-15 14:22:00,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-15 14:22:00,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-15 14:22:00,241] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-15 14:22:00,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:22:00,241] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-15 14:22:00,241] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-15 14:22:00,241] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-15 14:22:00,241] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-15 14:22:00,241] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f03a0d5ee30>
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-15 14:22:00,242] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-15 14:22:00,243] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-15 14:22:00,243] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 70
Epoch: [0]  [   0/2809]  eta: 6:10:13  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 7.9078  data: 4.0445  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 0:49:09  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0152)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0536  data: 0.3679  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:34:15  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0119)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3785  data: 0.0002  max mem: 15572
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3021908 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3021902 got signal: 1
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:289: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:300: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:311: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:320: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/maggie/VideoMAE_curriculum/modeling_finetune.py:329: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
[2025-01-15 14:22:22,402] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
Namespace(batch_size=12, epochs=40, update_freq=1, save_ckpt_freq=10, model='vit_small_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, layer_decay=0.7, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=2, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=2, test_num_crop=3, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/home/maggie/VideoMAE_curriculum/labels/ssv2', eval_data_path=None, nb_classes=174, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='SSV2', output_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/', log_dir='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, mask_curriculum_threshold=70, deepspeed=False, deepspeed_config='/home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Number of the class = 174
Number of the class = 174
Number of the class = 174
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7b13ca0400>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/maggie/VideoMAE_checkpoints/pretrain_checkpoint/pretrain_checkpoint_small_ssv2.pth
Load state_dict by model_key = model
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['mask_token', 'decoder.blocks.0.norm1.weight', 'decoder.blocks.0.norm1.bias', 'decoder.blocks.0.attn.q_bias', 'decoder.blocks.0.attn.v_bias', 'decoder.blocks.0.attn.qkv.weight', 'decoder.blocks.0.attn.proj.weight', 'decoder.blocks.0.attn.proj.bias', 'decoder.blocks.0.norm2.weight', 'decoder.blocks.0.norm2.bias', 'decoder.blocks.0.mlp.fc1.weight', 'decoder.blocks.0.mlp.fc1.bias', 'decoder.blocks.0.mlp.fc2.weight', 'decoder.blocks.0.mlp.fc2.bias', 'decoder.blocks.1.norm1.weight', 'decoder.blocks.1.norm1.bias', 'decoder.blocks.1.attn.q_bias', 'decoder.blocks.1.attn.v_bias', 'decoder.blocks.1.attn.qkv.weight', 'decoder.blocks.1.attn.proj.weight', 'decoder.blocks.1.attn.proj.bias', 'decoder.blocks.1.norm2.weight', 'decoder.blocks.1.norm2.bias', 'decoder.blocks.1.mlp.fc1.weight', 'decoder.blocks.1.mlp.fc1.bias', 'decoder.blocks.1.mlp.fc2.weight', 'decoder.blocks.1.mlp.fc2.bias', 'decoder.blocks.2.norm1.weight', 'decoder.blocks.2.norm1.bias', 'decoder.blocks.2.attn.q_bias', 'decoder.blocks.2.attn.v_bias', 'decoder.blocks.2.attn.qkv.weight', 'decoder.blocks.2.attn.proj.weight', 'decoder.blocks.2.attn.proj.bias', 'decoder.blocks.2.norm2.weight', 'decoder.blocks.2.norm2.bias', 'decoder.blocks.2.mlp.fc1.weight', 'decoder.blocks.2.mlp.fc1.bias', 'decoder.blocks.2.mlp.fc2.weight', 'decoder.blocks.2.mlp.fc2.bias', 'decoder.blocks.3.norm1.weight', 'decoder.blocks.3.norm1.bias', 'decoder.blocks.3.attn.q_bias', 'decoder.blocks.3.attn.v_bias', 'decoder.blocks.3.attn.qkv.weight', 'decoder.blocks.3.attn.proj.weight', 'decoder.blocks.3.attn.proj.bias', 'decoder.blocks.3.norm2.weight', 'decoder.blocks.3.norm2.bias', 'decoder.blocks.3.mlp.fc1.weight', 'decoder.blocks.3.mlp.fc1.bias', 'decoder.blocks.3.mlp.fc2.weight', 'decoder.blocks.3.mlp.fc2.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'decoder.head.weight', 'decoder.head.bias', 'encoder_to_decoder.weight', 'norm.weight', 'norm.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=384, out_features=174, bias=True)
)
number of params: 21946926
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 33709
Number of training training per epoch = 2809
Assigned values = [0.009688901040699992, 0.01384128720099999, 0.019773267429999988, 0.028247524899999984, 0.04035360699999998, 0.05764800999999997, 0.08235429999999996, 0.11764899999999996, 0.16806999999999994, 0.24009999999999995, 0.3429999999999999, 0.48999999999999994, 0.7, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.009688901040699992
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.01384128720099999
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.019773267429999988
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.028247524899999984
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.04035360699999998
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.05764800999999997
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.08235429999999996
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.11764899999999996
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16806999999999994
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.24009999999999995
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.3429999999999999
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.48999999999999994
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.7
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.7
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-01-15 14:22:25,183] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2025-01-15 14:22:25,183] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-15 14:22:25,205] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/maggie/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/maggie/.cache/torch_extensions/py310_cu116/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0399777889251709 seconds
[2025-01-15 14:22:25,451] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-01-15 14:22:25,451] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-15 14:22:25,453] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-15 14:22:25,453] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-01-15 14:22:25,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-01-15 14:22:25,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-15 14:22:25,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-15 14:22:25,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:22:25,460] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2025-01-15 14:22:25,460] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-15 14:22:25,460] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-15 14:22:25,460] [INFO] [config.py:988:print]   amp_enabled .................. False
[2025-01-15 14:22:25,460] [INFO] [config.py:988:print]   amp_params ................... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   bfloat16_enabled ............. False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7b12496e90>
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   communication_data_type ...... None
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   disable_allgather ............ False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   dump_state ................... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   fp16_auto_cast ............... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   fp16_enabled ................. True
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   global_rank .................. 0
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 128
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   loss_scale ................... 0
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   optimizer_name ............... adam
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   pld_enabled .................. False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   pld_params ................... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   scheduler_name ............... None
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   scheduler_params ............. None
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   sparse_attention ............. None
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   steps_per_print .............. 1000
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   train_batch_size ............. 12
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2025-01-15 14:22:25,461] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   world_size ................... 1
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   zero_enabled ................. False
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-15 14:22:25,462] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2025-01-15 14:22:25,462] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 12, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 14045
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 40 epochs
train curriculum learning mask.
Mask curriculum threshold = 70
Epoch: [0]  [   0/2809]  eta: 10:02:15  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1602)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 12.8643  data: 6.7247  max mem: 15572
Epoch: [0]  [  10/2809]  eta: 1:11:32  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0152)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5337  data: 0.6117  max mem: 15572
Epoch: [0]  [  20/2809]  eta: 0:47:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0119)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4331  data: 0.0007  max mem: 15572
Epoch: [0]  [  30/2809]  eta: 0:38:38  lr: 0.000000  min_lr: 0.000000  loss: 5.1602 (5.1601)  class_acc: 0.0000 (0.0094)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4498  data: 0.0011  max mem: 15572
Epoch: [0]  [  40/2809]  eta: 0:33:31  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0081)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4130  data: 0.0009  max mem: 15572
Epoch: [0]  [  50/2809]  eta: 0:30:27  lr: 0.000000  min_lr: 0.000000  loss: 5.1601 (5.1601)  class_acc: 0.0000 (0.0074)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3957  data: 0.0004  max mem: 15572
Epoch: [0]  [  60/2809]  eta: 0:28:41  lr: 0.000000  min_lr: 0.000000  loss: 5.1600 (5.1601)  class_acc: 0.0000 (0.0061)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4212  data: 0.0006  max mem: 15572
Epoch: [0]  [  70/2809]  eta: 0:27:36  lr: 0.000000  min_lr: 0.000000  loss: 5.1599 (5.1600)  class_acc: 0.0000 (0.0065)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4583  data: 0.0196  max mem: 15572
Epoch: [0]  [  80/2809]  eta: 0:27:19  lr: 0.000000  min_lr: 0.000000  loss: 5.1598 (5.1600)  class_acc: 0.0000 (0.0067)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0757  max mem: 15572
Epoch: [0]  [  90/2809]  eta: 0:27:04  lr: 0.000000  min_lr: 0.000000  loss: 5.1597 (5.1599)  class_acc: 0.0000 (0.0060)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1240  max mem: 15572
Epoch: [0]  [ 100/2809]  eta: 0:27:19  lr: 0.000000  min_lr: 0.000000  loss: 5.1594 (5.1599)  class_acc: 0.0000 (0.0066)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1890  max mem: 15572
Epoch: [0]  [ 110/2809]  eta: 0:27:14  lr: 0.000000  min_lr: 0.000000  loss: 5.1590 (5.1598)  class_acc: 0.0000 (0.0068)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.1939  max mem: 15572
Epoch: [0]  [ 120/2809]  eta: 0:27:02  lr: 0.000000  min_lr: 0.000000  loss: 5.1588 (5.1597)  class_acc: 0.0000 (0.0072)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1230  max mem: 15572
[2025-01-15 14:23:43,912] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:23:43,913] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/2809]  eta: 0:26:55  lr: 0.000000  min_lr: 0.000000  loss: 5.1586 (5.1596)  class_acc: 0.0000 (0.0083)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1316  max mem: 15572
Epoch: [0]  [ 140/2809]  eta: 0:26:42  lr: 0.000000  min_lr: 0.000000  loss: 5.1583 (5.1595)  class_acc: 0.0000 (0.0083)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1512  max mem: 15572
Epoch: [0]  [ 150/2809]  eta: 0:26:35  lr: 0.000001  min_lr: 0.000000  loss: 5.1583 (5.1595)  class_acc: 0.0000 (0.0083)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1398  max mem: 15572
Epoch: [0]  [ 160/2809]  eta: 0:26:31  lr: 0.000001  min_lr: 0.000000  loss: 5.1584 (5.1594)  class_acc: 0.0000 (0.0080)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1509  max mem: 15572
Epoch: [0]  [ 170/2809]  eta: 0:26:33  lr: 0.000001  min_lr: 0.000000  loss: 5.1582 (5.1593)  class_acc: 0.0000 (0.0083)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1921  max mem: 15572
Epoch: [0]  [ 180/2809]  eta: 0:26:38  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  class_acc: 0.0000 (0.0085)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.2523  max mem: 15572
Epoch: [0]  [ 190/2809]  eta: 0:26:28  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1592)  class_acc: 0.0000 (0.0096)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.2095  max mem: 15572
Epoch: [0]  [ 200/2809]  eta: 0:26:19  lr: 0.000001  min_lr: 0.000000  loss: 5.1581 (5.1591)  class_acc: 0.0000 (0.0120)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1381  max mem: 15572
Epoch: [0]  [ 210/2809]  eta: 0:26:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1591)  class_acc: 0.0000 (0.0115)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1235  max mem: 15572
Epoch: [0]  [ 220/2809]  eta: 0:26:00  lr: 0.000001  min_lr: 0.000000  loss: 5.1580 (5.1590)  class_acc: 0.0000 (0.0117)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1329  max mem: 15572
Epoch: [0]  [ 230/2809]  eta: 0:25:55  lr: 0.000001  min_lr: 0.000000  loss: 5.1576 (5.1590)  class_acc: 0.0000 (0.0123)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1586  max mem: 15572
Epoch: [0]  [ 240/2809]  eta: 0:25:50  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1589)  class_acc: 0.0000 (0.0121)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1670  max mem: 15572
Epoch: [0]  [ 250/2809]  eta: 0:25:47  lr: 0.000001  min_lr: 0.000000  loss: 5.1574 (5.1588)  class_acc: 0.0000 (0.0120)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1856  max mem: 15572
[2025-01-15 14:25:02,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:25:02,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/2809]  eta: 0:25:45  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1588)  class_acc: 0.0000 (0.0121)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1975  max mem: 15572
Epoch: [0]  [ 270/2809]  eta: 0:25:34  lr: 0.000001  min_lr: 0.000000  loss: 5.1575 (5.1587)  class_acc: 0.0000 (0.0121)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1620  max mem: 15572
Epoch: [0]  [ 280/2809]  eta: 0:25:14  lr: 0.000001  min_lr: 0.000000  loss: 5.1571 (5.1587)  class_acc: 0.0000 (0.0123)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0791  max mem: 15572
Epoch: [0]  [ 290/2809]  eta: 0:25:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1567 (5.1586)  class_acc: 0.0000 (0.0126)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0686  max mem: 15572
Epoch: [0]  [ 300/2809]  eta: 0:24:57  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1585)  class_acc: 0.0000 (0.0125)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1073  max mem: 15572
Epoch: [0]  [ 310/2809]  eta: 0:24:53  lr: 0.000001  min_lr: 0.000000  loss: 5.1563 (5.1584)  class_acc: 0.0000 (0.0123)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1434  max mem: 15572
Epoch: [0]  [ 320/2809]  eta: 0:24:40  lr: 0.000001  min_lr: 0.000000  loss: 5.1562 (5.1584)  class_acc: 0.0000 (0.0123)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1339  max mem: 15572
Epoch: [0]  [ 330/2809]  eta: 0:24:40  lr: 0.000001  min_lr: 0.000000  loss: 5.1555 (5.1583)  class_acc: 0.0000 (0.0123)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1489  max mem: 15572
Epoch: [0]  [ 340/2809]  eta: 0:24:38  lr: 0.000001  min_lr: 0.000000  loss: 5.1554 (5.1582)  class_acc: 0.0000 (0.0126)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.1951  max mem: 15572
Epoch: [0]  [ 350/2809]  eta: 0:24:32  lr: 0.000001  min_lr: 0.000000  loss: 5.1553 (5.1581)  class_acc: 0.0000 (0.0127)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1854  max mem: 15572
Epoch: [0]  [ 360/2809]  eta: 0:24:35  lr: 0.000001  min_lr: 0.000000  loss: 5.1548 (5.1580)  class_acc: 0.0000 (0.0128)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2414  max mem: 15572
Epoch: [0]  [ 370/2809]  eta: 0:24:26  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1579)  class_acc: 0.0000 (0.0128)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2024  max mem: 15572
Epoch: [0]  [ 380/2809]  eta: 0:24:17  lr: 0.000001  min_lr: 0.000000  loss: 5.1541 (5.1578)  class_acc: 0.0000 (0.0128)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1029  max mem: 15572
[2025-01-15 14:26:17,249] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:26:17,249] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/2809]  eta: 0:24:07  lr: 0.000001  min_lr: 0.000000  loss: 5.1535 (5.1577)  class_acc: 0.0000 (0.0131)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0893  max mem: 15572
Epoch: [0]  [ 400/2809]  eta: 0:23:55  lr: 0.000001  min_lr: 0.000000  loss: 5.1520 (5.1576)  class_acc: 0.0000 (0.0128)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0754  max mem: 15572
Epoch: [0]  [ 410/2809]  eta: 0:23:50  lr: 0.000001  min_lr: 0.000000  loss: 5.1539 (5.1575)  class_acc: 0.0000 (0.0129)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1310  max mem: 15572
Epoch: [0]  [ 420/2809]  eta: 0:23:46  lr: 0.000001  min_lr: 0.000000  loss: 5.1532 (5.1573)  class_acc: 0.0000 (0.0128)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1909  max mem: 15572
Epoch: [0]  [ 430/2809]  eta: 0:23:41  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1572)  class_acc: 0.0000 (0.0130)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1819  max mem: 15572
Epoch: [0]  [ 440/2809]  eta: 0:23:31  lr: 0.000001  min_lr: 0.000000  loss: 5.1515 (5.1571)  class_acc: 0.0000 (0.0129)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1292  max mem: 15572
Epoch: [0]  [ 450/2809]  eta: 0:23:29  lr: 0.000002  min_lr: 0.000000  loss: 5.1515 (5.1569)  class_acc: 0.0000 (0.0131)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1541  max mem: 15572
Epoch: [0]  [ 460/2809]  eta: 0:23:25  lr: 0.000002  min_lr: 0.000000  loss: 5.1496 (5.1568)  class_acc: 0.0000 (0.0134)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.2086  max mem: 15572
Epoch: [0]  [ 470/2809]  eta: 0:23:13  lr: 0.000002  min_lr: 0.000000  loss: 5.1490 (5.1566)  class_acc: 0.0000 (0.0134)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1295  max mem: 15572
Epoch: [0]  [ 480/2809]  eta: 0:23:10  lr: 0.000002  min_lr: 0.000000  loss: 5.1484 (5.1564)  class_acc: 0.0000 (0.0135)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1349  max mem: 15572
Epoch: [0]  [ 490/2809]  eta: 0:23:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1474 (5.1562)  class_acc: 0.0000 (0.0134)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.2026  max mem: 15572
Epoch: [0]  [ 500/2809]  eta: 0:23:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1458 (5.1560)  class_acc: 0.0000 (0.0135)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.2087  max mem: 15572
Epoch: [0]  [ 510/2809]  eta: 0:22:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1456 (5.1558)  class_acc: 0.0000 (0.0135)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1398  max mem: 15572
[2025-01-15 14:27:31,852] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:27:31,853] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/2809]  eta: 0:22:46  lr: 0.000002  min_lr: 0.000000  loss: 5.1441 (5.1556)  class_acc: 0.0000 (0.0138)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1081  max mem: 15572
Epoch: [0]  [ 530/2809]  eta: 0:22:39  lr: 0.000002  min_lr: 0.000000  loss: 5.1440 (5.1553)  class_acc: 0.0000 (0.0137)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1563  max mem: 15572
Epoch: [0]  [ 540/2809]  eta: 0:22:30  lr: 0.000002  min_lr: 0.000000  loss: 5.1419 (5.1551)  class_acc: 0.0000 (0.0136)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1178  max mem: 15572
Epoch: [0]  [ 550/2809]  eta: 0:22:23  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1549)  class_acc: 0.0000 (0.0138)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1054  max mem: 15572
Epoch: [0]  [ 560/2809]  eta: 0:22:20  lr: 0.000002  min_lr: 0.000000  loss: 5.1431 (5.1547)  class_acc: 0.0000 (0.0140)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1891  max mem: 15572
Epoch: [0]  [ 570/2809]  eta: 0:22:16  lr: 0.000002  min_lr: 0.000000  loss: 5.1395 (5.1545)  class_acc: 0.0000 (0.0139)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.2339  max mem: 15572
Epoch: [0]  [ 580/2809]  eta: 0:22:08  lr: 0.000002  min_lr: 0.000000  loss: 5.1396 (5.1543)  class_acc: 0.0000 (0.0141)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1624  max mem: 15572
Epoch: [0]  [ 590/2809]  eta: 0:22:02  lr: 0.000002  min_lr: 0.000000  loss: 5.1376 (5.1540)  class_acc: 0.0000 (0.0138)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1183  max mem: 15572
Epoch: [0]  [ 600/2809]  eta: 0:21:56  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1537)  class_acc: 0.0000 (0.0141)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1444  max mem: 15572
Epoch: [0]  [ 610/2809]  eta: 0:21:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1369 (5.1535)  class_acc: 0.0000 (0.0140)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1591  max mem: 15572
Epoch: [0]  [ 620/2809]  eta: 0:21:41  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1531)  class_acc: 0.0000 (0.0141)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1049  max mem: 15572
Epoch: [0]  [ 630/2809]  eta: 0:21:37  lr: 0.000002  min_lr: 0.000000  loss: 5.1308 (5.1529)  class_acc: 0.0000 (0.0141)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1222  max mem: 15572
[2025-01-15 14:28:47,586] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:28:47,586] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/2809]  eta: 0:21:31  lr: 0.000002  min_lr: 0.000000  loss: 5.1382 (5.1527)  class_acc: 0.0000 (0.0144)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1580  max mem: 15572
Epoch: [0]  [ 650/2809]  eta: 0:21:26  lr: 0.000002  min_lr: 0.000000  loss: 5.1335 (5.1523)  class_acc: 0.0000 (0.0148)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1437  max mem: 15572
Epoch: [0]  [ 660/2809]  eta: 0:21:19  lr: 0.000002  min_lr: 0.000000  loss: 5.1286 (5.1520)  class_acc: 0.0000 (0.0148)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1193  max mem: 15572
Epoch: [0]  [ 670/2809]  eta: 0:21:12  lr: 0.000002  min_lr: 0.000000  loss: 5.1264 (5.1516)  class_acc: 0.0000 (0.0149)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0870  max mem: 15572
Epoch: [0]  [ 680/2809]  eta: 0:21:05  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1512)  class_acc: 0.0000 (0.0149)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1199  max mem: 15572
Epoch: [0]  [ 690/2809]  eta: 0:20:58  lr: 0.000002  min_lr: 0.000000  loss: 5.1207 (5.1509)  class_acc: 0.0000 (0.0147)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1201  max mem: 15572
Epoch: [0]  [ 700/2809]  eta: 0:20:51  lr: 0.000002  min_lr: 0.000000  loss: 5.1347 (5.1506)  class_acc: 0.0000 (0.0145)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1099  max mem: 15572
Epoch: [0]  [ 710/2809]  eta: 0:20:48  lr: 0.000002  min_lr: 0.000000  loss: 5.1334 (5.1503)  class_acc: 0.0000 (0.0145)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1437  max mem: 15572
Epoch: [0]  [ 720/2809]  eta: 0:20:44  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1499)  class_acc: 0.0000 (0.0144)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.1948  max mem: 15572
Epoch: [0]  [ 730/2809]  eta: 0:20:36  lr: 0.000002  min_lr: 0.000000  loss: 5.1228 (5.1495)  class_acc: 0.0000 (0.0144)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1371  max mem: 15572
Epoch: [0]  [ 740/2809]  eta: 0:20:30  lr: 0.000002  min_lr: 0.000000  loss: 5.1246 (5.1492)  class_acc: 0.0000 (0.0143)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1146  max mem: 15572
Epoch: [0]  [ 750/2809]  eta: 0:20:23  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1487)  class_acc: 0.0000 (0.0143)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1457  max mem: 15572
Epoch: [0]  [ 760/2809]  eta: 0:20:18  lr: 0.000003  min_lr: 0.000000  loss: 5.1155 (5.1483)  class_acc: 0.0000 (0.0142)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1480  max mem: 15572
[2025-01-15 14:30:03,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:30:03,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/2809]  eta: 0:20:11  lr: 0.000003  min_lr: 0.000000  loss: 5.1169 (5.1479)  class_acc: 0.0000 (0.0144)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1329  max mem: 15572
Epoch: [0]  [ 780/2809]  eta: 0:20:04  lr: 0.000003  min_lr: 0.000000  loss: 5.1170 (5.1475)  class_acc: 0.0000 (0.0145)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0969  max mem: 15572
Epoch: [0]  [ 790/2809]  eta: 0:19:57  lr: 0.000003  min_lr: 0.000000  loss: 5.1164 (5.1472)  class_acc: 0.0000 (0.0145)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1073  max mem: 15572
Epoch: [0]  [ 800/2809]  eta: 0:19:51  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1467)  class_acc: 0.0000 (0.0144)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1340  max mem: 15572
Epoch: [0]  [ 810/2809]  eta: 0:19:46  lr: 0.000003  min_lr: 0.000000  loss: 5.1091 (5.1463)  class_acc: 0.0000 (0.0144)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1598  max mem: 15572
Epoch: [0]  [ 820/2809]  eta: 0:19:39  lr: 0.000003  min_lr: 0.000000  loss: 5.1146 (5.1460)  class_acc: 0.0000 (0.0145)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1335  max mem: 15572
Epoch: [0]  [ 830/2809]  eta: 0:19:30  lr: 0.000003  min_lr: 0.000000  loss: 5.1068 (5.1455)  class_acc: 0.0000 (0.0144)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.5055  data: 0.0650  max mem: 15572
Epoch: [0]  [ 840/2809]  eta: 0:19:25  lr: 0.000003  min_lr: 0.000000  loss: 5.1024 (5.1450)  class_acc: 0.0000 (0.0145)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0945  max mem: 15572
Epoch: [0]  [ 850/2809]  eta: 0:19:19  lr: 0.000003  min_lr: 0.000000  loss: 5.1057 (5.1446)  class_acc: 0.0000 (0.0147)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1651  max mem: 15572
Epoch: [0]  [ 860/2809]  eta: 0:19:11  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1442)  class_acc: 0.0000 (0.0148)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1264  max mem: 15572
Epoch: [0]  [ 870/2809]  eta: 0:19:05  lr: 0.000003  min_lr: 0.000000  loss: 5.1044 (5.1439)  class_acc: 0.0000 (0.0149)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1152  max mem: 15572
Epoch: [0]  [ 880/2809]  eta: 0:19:00  lr: 0.000003  min_lr: 0.000000  loss: 5.1025 (5.1433)  class_acc: 0.0000 (0.0155)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1649  max mem: 15572
Epoch: [0]  [ 890/2809]  eta: 0:18:54  lr: 0.000003  min_lr: 0.000000  loss: 5.0929 (5.1429)  class_acc: 0.0000 (0.0157)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1582  max mem: 15572
[2025-01-15 14:31:16,260] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:31:16,261] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/2809]  eta: 0:18:46  lr: 0.000003  min_lr: 0.000000  loss: 5.1061 (5.1425)  class_acc: 0.0000 (0.0159)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1070  max mem: 15572
Epoch: [0]  [ 910/2809]  eta: 0:18:41  lr: 0.000003  min_lr: 0.000000  loss: 5.1046 (5.1421)  class_acc: 0.0000 (0.0158)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1269  max mem: 15572
Epoch: [0]  [ 920/2809]  eta: 0:18:35  lr: 0.000003  min_lr: 0.000000  loss: 5.1040 (5.1418)  class_acc: 0.0000 (0.0158)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1820  max mem: 15572
Epoch: [0]  [ 930/2809]  eta: 0:18:29  lr: 0.000003  min_lr: 0.000000  loss: 5.1048 (5.1414)  class_acc: 0.0000 (0.0161)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1408  max mem: 15572
Epoch: [0]  [ 940/2809]  eta: 0:18:23  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1410)  class_acc: 0.0000 (0.0161)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1311  max mem: 15572
Epoch: [0]  [ 950/2809]  eta: 0:18:17  lr: 0.000003  min_lr: 0.000000  loss: 5.1055 (5.1407)  class_acc: 0.0000 (0.0160)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1524  max mem: 15572
Epoch: [0]  [ 960/2809]  eta: 0:18:09  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1402)  class_acc: 0.0000 (0.0161)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0921  max mem: 15572
Epoch: [0]  [ 970/2809]  eta: 0:18:06  lr: 0.000003  min_lr: 0.000000  loss: 5.1052 (5.1399)  class_acc: 0.0000 (0.0159)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1864  max mem: 15572
Epoch: [0]  [ 980/2809]  eta: 0:18:01  lr: 0.000003  min_lr: 0.000000  loss: 5.1015 (5.1394)  class_acc: 0.0000 (0.0159)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2351  max mem: 15572
Epoch: [0]  [ 990/2809]  eta: 0:17:53  lr: 0.000003  min_lr: 0.000000  loss: 5.0866 (5.1387)  class_acc: 0.0000 (0.0161)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0973  max mem: 15572
[2025-01-15 14:32:16,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[3.2306541515702744e-08, 3.2306541515702744e-08, 4.615220216528964e-08, 4.615220216528964e-08, 6.59317173789852e-08, 6.59317173789852e-08, 9.418816768426457e-08, 9.418816768426457e-08, 1.3455452526323513e-07, 1.3455452526323513e-07, 1.9222075037605018e-07, 1.9222075037605018e-07, 2.7460107196578597e-07, 2.7460107196578597e-07, 3.922872456654086e-07, 3.922872456654086e-07, 5.604103509505837e-07, 5.604103509505837e-07, 8.005862156436911e-07, 8.005862156436911e-07, 1.1436945937767016e-06, 1.1436945937767016e-06, 1.6338494196810024e-06, 1.6338494196810024e-06, 2.3340705995442894e-06, 2.3340705995442894e-06, 3.3343865707775563e-06, 3.3343865707775563e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:32:16,267] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=28.76744274455894, CurrSamplesPerSec=27.949837432008817, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [1000/2809]  eta: 0:17:46  lr: 0.000003  min_lr: 0.000000  loss: 5.0870 (5.1383)  class_acc: 0.0000 (0.0162)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0849  max mem: 15572
Epoch: [0]  [1010/2809]  eta: 0:17:40  lr: 0.000003  min_lr: 0.000000  loss: 5.0940 (5.1380)  class_acc: 0.0000 (0.0162)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1200  max mem: 15572
Epoch: [0]  [1020/2809]  eta: 0:17:35  lr: 0.000003  min_lr: 0.000000  loss: 5.0888 (5.1375)  class_acc: 0.0000 (0.0162)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1667  max mem: 15572
[2025-01-15 14:32:31,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:32:31,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/2809]  eta: 0:17:29  lr: 0.000003  min_lr: 0.000000  loss: 5.0857 (5.1370)  class_acc: 0.0000 (0.0160)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1796  max mem: 15572
Epoch: [0]  [1040/2809]  eta: 0:17:24  lr: 0.000003  min_lr: 0.000000  loss: 5.0919 (5.1368)  class_acc: 0.0000 (0.0160)  loss_scale: 32768.0000 (4548.4880)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1454  max mem: 15572
Epoch: [0]  [1050/2809]  eta: 0:17:17  lr: 0.000004  min_lr: 0.000000  loss: 5.1028 (5.1364)  class_acc: 0.0000 (0.0162)  loss_scale: 32768.0000 (4816.9895)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1053  max mem: 15572
Epoch: [0]  [1060/2809]  eta: 0:17:11  lr: 0.000004  min_lr: 0.000000  loss: 5.0846 (5.1359)  class_acc: 0.0000 (0.0161)  loss_scale: 32768.0000 (5080.4298)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1039  max mem: 15572
Epoch: [0]  [1070/2809]  eta: 0:17:06  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1354)  class_acc: 0.0000 (0.0161)  loss_scale: 32768.0000 (5338.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1694  max mem: 15572
Epoch: [0]  [1080/2809]  eta: 0:17:00  lr: 0.000004  min_lr: 0.000000  loss: 5.0781 (5.1348)  class_acc: 0.0000 (0.0160)  loss_scale: 32768.0000 (5592.6883)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1595  max mem: 15572
Epoch: [0]  [1090/2809]  eta: 0:16:53  lr: 0.000004  min_lr: 0.000000  loss: 5.0771 (5.1344)  class_acc: 0.0000 (0.0161)  loss_scale: 32768.0000 (5841.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.0983  max mem: 15572
Epoch: [0]  [1100/2809]  eta: 0:16:47  lr: 0.000004  min_lr: 0.000000  loss: 5.0976 (5.1341)  class_acc: 0.0000 (0.0162)  loss_scale: 32768.0000 (6086.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1038  max mem: 15572
Epoch: [0]  [1110/2809]  eta: 0:16:42  lr: 0.000004  min_lr: 0.000000  loss: 5.0843 (5.1335)  class_acc: 0.0000 (0.0162)  loss_scale: 32768.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1531  max mem: 15572
Epoch: [0]  [1120/2809]  eta: 0:16:37  lr: 0.000004  min_lr: 0.000000  loss: 5.0730 (5.1331)  class_acc: 0.0000 (0.0162)  loss_scale: 32768.0000 (6562.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1922  max mem: 15572
Epoch: [0]  [1130/2809]  eta: 0:16:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0699 (5.1326)  class_acc: 0.0000 (0.0162)  loss_scale: 32768.0000 (6794.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1577  max mem: 15572
Epoch: [0]  [1140/2809]  eta: 0:16:23  lr: 0.000004  min_lr: 0.000000  loss: 5.0726 (5.1321)  class_acc: 0.0000 (0.0163)  loss_scale: 32768.0000 (7021.7143)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.1089  max mem: 15572
Epoch: [0]  [1150/2809]  eta: 0:16:17  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1317)  class_acc: 0.0000 (0.0163)  loss_scale: 32768.0000 (7245.4005)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1117  max mem: 15572
[2025-01-15 14:33:45,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:33:45,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [0]  [1160/2809]  eta: 0:16:10  lr: 0.000004  min_lr: 0.000000  loss: 5.0859 (5.1313)  class_acc: 0.0000 (0.0163)  loss_scale: 32768.0000 (7719.2489)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0900  max mem: 15572
Epoch: [0]  [1170/2809]  eta: 0:16:03  lr: 0.000004  min_lr: 0.000000  loss: 5.0784 (5.1307)  class_acc: 0.0000 (0.0166)  loss_scale: 65536.0000 (8212.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0980  max mem: 15572
Epoch: [0]  [1180/2809]  eta: 0:15:56  lr: 0.000004  min_lr: 0.000000  loss: 5.0701 (5.1303)  class_acc: 0.0000 (0.0167)  loss_scale: 65536.0000 (8698.3641)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0890  max mem: 15572
Epoch: [0]  [1190/2809]  eta: 0:15:51  lr: 0.000004  min_lr: 0.000000  loss: 5.0956 (5.1299)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (9175.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1176  max mem: 15572
Epoch: [0]  [1200/2809]  eta: 0:15:45  lr: 0.000004  min_lr: 0.000000  loss: 5.0828 (5.1295)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (9644.8693)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1690  max mem: 15572
Epoch: [0]  [1210/2809]  eta: 0:15:39  lr: 0.000004  min_lr: 0.000000  loss: 5.0916 (5.1291)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (10106.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1414  max mem: 15572
Epoch: [0]  [1220/2809]  eta: 0:15:33  lr: 0.000004  min_lr: 0.000000  loss: 5.0774 (5.1286)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (10560.3669)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1263  max mem: 15572
Epoch: [0]  [1230/2809]  eta: 0:15:27  lr: 0.000004  min_lr: 0.000000  loss: 5.0682 (5.1283)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (11006.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1450  max mem: 15572
Epoch: [0]  [1240/2809]  eta: 0:15:21  lr: 0.000004  min_lr: 0.000000  loss: 5.0855 (5.1280)  class_acc: 0.0000 (0.0166)  loss_scale: 65536.0000 (11446.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1477  max mem: 15572
Epoch: [0]  [1250/2809]  eta: 0:15:16  lr: 0.000004  min_lr: 0.000000  loss: 5.0822 (5.1276)  class_acc: 0.0000 (0.0165)  loss_scale: 65536.0000 (11878.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1468  max mem: 15572
Epoch: [0]  [1260/2809]  eta: 0:15:09  lr: 0.000004  min_lr: 0.000000  loss: 5.0856 (5.1273)  class_acc: 0.0000 (0.0164)  loss_scale: 65536.0000 (12304.2411)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1226  max mem: 15572
Epoch: [0]  [1270/2809]  eta: 0:15:04  lr: 0.000004  min_lr: 0.000000  loss: 5.0768 (5.1269)  class_acc: 0.0000 (0.0163)  loss_scale: 65536.0000 (12723.0590)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1681  max mem: 15572
[2025-01-15 14:34:59,569] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:34:59,569] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [0]  [1280/2809]  eta: 0:14:59  lr: 0.000004  min_lr: 0.000000  loss: 5.0672 (5.1266)  class_acc: 0.0000 (0.0164)  loss_scale: 65536.0000 (13186.4980)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.2174  max mem: 15572
Epoch: [0]  [1290/2809]  eta: 0:14:53  lr: 0.000004  min_lr: 0.000000  loss: 5.0922 (5.1264)  class_acc: 0.0000 (0.0163)  loss_scale: 131072.0000 (14099.6313)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1698  max mem: 15572
Epoch: [0]  [1300/2809]  eta: 0:14:47  lr: 0.000004  min_lr: 0.000000  loss: 5.0757 (5.1259)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (14998.7271)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1670  max mem: 15572
Epoch: [0]  [1310/2809]  eta: 0:14:41  lr: 0.000004  min_lr: 0.000000  loss: 5.0489 (5.1253)  class_acc: 0.0000 (0.0164)  loss_scale: 131072.0000 (15884.1068)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1839  max mem: 15572
Epoch: [0]  [1320/2809]  eta: 0:14:36  lr: 0.000004  min_lr: 0.000000  loss: 5.0517 (5.1248)  class_acc: 0.0000 (0.0165)  loss_scale: 131072.0000 (16756.0818)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1777  max mem: 15572
Epoch: [0]  [1330/2809]  eta: 0:14:30  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1244)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (17614.9542)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1658  max mem: 15572
Epoch: [0]  [1340/2809]  eta: 0:14:23  lr: 0.000004  min_lr: 0.000000  loss: 5.0613 (5.1239)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (18461.0172)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1264  max mem: 15572
Epoch: [0]  [1350/2809]  eta: 0:14:18  lr: 0.000005  min_lr: 0.000000  loss: 5.0655 (5.1235)  class_acc: 0.0000 (0.0166)  loss_scale: 131072.0000 (19294.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1078  max mem: 15572
Epoch: [0]  [1360/2809]  eta: 0:14:11  lr: 0.000005  min_lr: 0.000000  loss: 5.0695 (5.1231)  class_acc: 0.0000 (0.0167)  loss_scale: 131072.0000 (20115.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1176  max mem: 15572
Epoch: [0]  [1370/2809]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 5.0580 (5.1226)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (20925.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1023  max mem: 15572
Epoch: [0]  [1380/2809]  eta: 0:14:00  lr: 0.000005  min_lr: 0.000000  loss: 5.0384 (5.1220)  class_acc: 0.0000 (0.0168)  loss_scale: 131072.0000 (21722.7400)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1503  max mem: 15572
Epoch: [0]  [1390/2809]  eta: 0:13:54  lr: 0.000005  min_lr: 0.000000  loss: 5.0521 (5.1216)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (22508.8598)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1622  max mem: 15572
Epoch: [0]  [1400/2809]  eta: 0:13:47  lr: 0.000005  min_lr: 0.000000  loss: 5.0647 (5.1213)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (23283.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1121  max mem: 15572
[2025-01-15 14:36:14,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:36:14,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
Epoch: [0]  [1410/2809]  eta: 0:13:41  lr: 0.000005  min_lr: 0.000000  loss: 5.0692 (5.1209)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (24326.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1467  max mem: 15572
Epoch: [0]  [1420/2809]  eta: 0:13:37  lr: 0.000005  min_lr: 0.000000  loss: 5.0708 (5.1204)  class_acc: 0.0000 (0.0170)  loss_scale: 262144.0000 (25999.9437)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.2423  max mem: 15572
[2025-01-15 14:36:28,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1430
[2025-01-15 14:36:28,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144 to 131072.0
[2025-01-15 14:36:28,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144, reducing to 131072.0
Epoch: [0]  [1430/2809]  eta: 0:13:31  lr: 0.000005  min_lr: 0.000000  loss: 5.0685 (5.1199)  class_acc: 0.0000 (0.0170)  loss_scale: 262144.0000 (27558.5521)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.2329  max mem: 15572
Epoch: [0]  [1440/2809]  eta: 0:13:25  lr: 0.000005  min_lr: 0.000000  loss: 5.0702 (5.1197)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (28276.8966)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1171  max mem: 15572
Epoch: [0]  [1450/2809]  eta: 0:13:19  lr: 0.000005  min_lr: 0.000000  loss: 5.0621 (5.1192)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (28985.3398)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1240  max mem: 15572
Epoch: [0]  [1460/2809]  eta: 0:13:14  lr: 0.000005  min_lr: 0.000000  loss: 5.0597 (5.1188)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (29684.0849)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.2125  max mem: 15572
Epoch: [0]  [1470/2809]  eta: 0:13:08  lr: 0.000005  min_lr: 0.000000  loss: 5.0780 (5.1186)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (30373.3297)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1618  max mem: 15572
Epoch: [0]  [1480/2809]  eta: 0:13:02  lr: 0.000005  min_lr: 0.000000  loss: 5.0553 (5.1182)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (31053.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1149  max mem: 15572
Epoch: [0]  [1490/2809]  eta: 0:12:56  lr: 0.000005  min_lr: 0.000000  loss: 5.0651 (5.1178)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (31724.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.0936  max mem: 15572
Epoch: [0]  [1500/2809]  eta: 0:12:50  lr: 0.000005  min_lr: 0.000000  loss: 5.0768 (5.1175)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (32385.9614)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.0909  max mem: 15572
Epoch: [0]  [1510/2809]  eta: 0:12:44  lr: 0.000005  min_lr: 0.000000  loss: 5.0635 (5.1170)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (33039.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1254  max mem: 15572
Epoch: [0]  [1520/2809]  eta: 0:12:38  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1166)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (33683.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1137  max mem: 15572
Epoch: [0]  [1530/2809]  eta: 0:12:32  lr: 0.000005  min_lr: 0.000000  loss: 5.0512 (5.1163)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (34319.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1122  max mem: 15572
Epoch: [0]  [1540/2809]  eta: 0:12:26  lr: 0.000005  min_lr: 0.000000  loss: 5.0472 (5.1159)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (34947.5717)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1143  max mem: 15572
Epoch: [0]  [1550/2809]  eta: 0:12:20  lr: 0.000005  min_lr: 0.000000  loss: 5.0354 (5.1154)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (35567.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1581  max mem: 15572
[2025-01-15 14:37:43,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:37:43,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1560/2809]  eta: 0:12:14  lr: 0.000005  min_lr: 0.000000  loss: 5.0520 (5.1152)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (36347.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1458  max mem: 15572
Epoch: [0]  [1570/2809]  eta: 0:12:09  lr: 0.000005  min_lr: 0.000000  loss: 5.0554 (5.1148)  class_acc: 0.0000 (0.0170)  loss_scale: 262144.0000 (37784.3616)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1387  max mem: 15572
Epoch: [0]  [1580/2809]  eta: 0:12:03  lr: 0.000005  min_lr: 0.000000  loss: 5.0238 (5.1142)  class_acc: 0.0000 (0.0169)  loss_scale: 262144.0000 (39203.4611)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1664  max mem: 15572
Epoch: [0]  [1590/2809]  eta: 0:11:57  lr: 0.000005  min_lr: 0.000000  loss: 5.0186 (5.1136)  class_acc: 0.0000 (0.0171)  loss_scale: 262144.0000 (40604.7216)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1645  max mem: 15572
[2025-01-15 14:38:07,468] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1599
[2025-01-15 14:38:07,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:38:07,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1600/2809]  eta: 0:11:50  lr: 0.000005  min_lr: 0.000000  loss: 5.0195 (5.1131)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (41824.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1318  max mem: 15572
Epoch: [0]  [1610/2809]  eta: 0:11:45  lr: 0.000005  min_lr: 0.000000  loss: 5.0479 (5.1127)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (42378.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1450  max mem: 15572
Epoch: [0]  [1620/2809]  eta: 0:11:39  lr: 0.000005  min_lr: 0.000000  loss: 5.0526 (5.1123)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (42925.8779)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1694  max mem: 15572
Epoch: [0]  [1630/2809]  eta: 0:11:34  lr: 0.000005  min_lr: 0.000000  loss: 5.0335 (5.1119)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (43466.3200)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.1742  max mem: 15572
Epoch: [0]  [1640/2809]  eta: 0:11:27  lr: 0.000005  min_lr: 0.000000  loss: 5.0410 (5.1115)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (44000.1755)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1188  max mem: 15572
Epoch: [0]  [1650/2809]  eta: 0:11:21  lr: 0.000006  min_lr: 0.000000  loss: 5.0617 (5.1111)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (44527.5639)  weight_decay: 0.0500 (0.0500)  time: 0.4831  data: 0.0472  max mem: 15572
Epoch: [0]  [1660/2809]  eta: 0:11:15  lr: 0.000006  min_lr: 0.000000  loss: 5.0134 (5.1104)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (45048.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1224  max mem: 15572
Epoch: [0]  [1670/2809]  eta: 0:11:09  lr: 0.000006  min_lr: 0.000000  loss: 5.0045 (5.1100)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (45563.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1668  max mem: 15572
Epoch: [0]  [1680/2809]  eta: 0:11:03  lr: 0.000006  min_lr: 0.000000  loss: 5.0359 (5.1098)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (46072.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1550  max mem: 15572
Epoch: [0]  [1690/2809]  eta: 0:10:58  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1095)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (46574.7416)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.2159  max mem: 15572
Epoch: [0]  [1700/2809]  eta: 0:10:52  lr: 0.000006  min_lr: 0.000000  loss: 5.0668 (5.1094)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (47071.4921)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.2060  max mem: 15572
Epoch: [0]  [1710/2809]  eta: 0:10:46  lr: 0.000006  min_lr: 0.000000  loss: 5.0553 (5.1089)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (47562.4360)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0723  max mem: 15572
Epoch: [0]  [1720/2809]  eta: 0:10:40  lr: 0.000006  min_lr: 0.000000  loss: 5.0238 (5.1084)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (48047.6746)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0659  max mem: 15572
[2025-01-15 14:39:24,904] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:39:24,905] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [1730/2809]  eta: 0:10:35  lr: 0.000006  min_lr: 0.000000  loss: 5.0297 (5.1080)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (48754.4679)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.2007  max mem: 15572
Epoch: [0]  [1740/2809]  eta: 0:10:29  lr: 0.000006  min_lr: 0.000000  loss: 5.0292 (5.1075)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (49980.1401)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.2119  max mem: 15572
Epoch: [0]  [1750/2809]  eta: 0:10:22  lr: 0.000006  min_lr: 0.000000  loss: 5.0127 (5.1070)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (51191.8127)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.1024  max mem: 15572
Epoch: [0]  [1760/2809]  eta: 0:10:16  lr: 0.000006  min_lr: 0.000000  loss: 5.0273 (5.1067)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (52389.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0604  max mem: 15572
Epoch: [0]  [1770/2809]  eta: 0:10:10  lr: 0.000006  min_lr: 0.000000  loss: 5.0398 (5.1062)  class_acc: 0.0000 (0.0171)  loss_scale: 262144.0000 (53574.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0631  max mem: 15572
[2025-01-15 14:39:51,000] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1776
[2025-01-15 14:39:51,003] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:39:51,005] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1780/2809]  eta: 0:10:04  lr: 0.000006  min_lr: 0.000000  loss: 5.0520 (5.1059)  class_acc: 0.0000 (0.0171)  loss_scale: 262144.0000 (54377.2173)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1131  max mem: 15572
Epoch: [0]  [1790/2809]  eta: 0:09:59  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1056)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (54805.4405)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.2044  max mem: 15572
Epoch: [0]  [1800/2809]  eta: 0:09:53  lr: 0.000006  min_lr: 0.000000  loss: 5.0756 (5.1054)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (55228.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1841  max mem: 15572
Epoch: [0]  [1810/2809]  eta: 0:09:46  lr: 0.000006  min_lr: 0.000000  loss: 5.0073 (5.1048)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (55647.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0654  max mem: 15572
Epoch: [0]  [1820/2809]  eta: 0:09:41  lr: 0.000006  min_lr: 0.000000  loss: 5.0470 (5.1046)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (56061.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1078  max mem: 15572
Epoch: [0]  [1830/2809]  eta: 0:09:35  lr: 0.000006  min_lr: 0.000000  loss: 5.0751 (5.1045)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (56471.5587)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1716  max mem: 15572
Epoch: [0]  [1840/2809]  eta: 0:09:29  lr: 0.000006  min_lr: 0.000000  loss: 5.0669 (5.1042)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (56876.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1273  max mem: 15572
Epoch: [0]  [1850/2809]  eta: 0:09:23  lr: 0.000006  min_lr: 0.000000  loss: 5.0295 (5.1039)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (57277.6143)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1463  max mem: 15572
Epoch: [0]  [1860/2809]  eta: 0:09:17  lr: 0.000006  min_lr: 0.000000  loss: 5.0140 (5.1034)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (57674.1451)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1834  max mem: 15572
Epoch: [0]  [1870/2809]  eta: 0:09:12  lr: 0.000006  min_lr: 0.000000  loss: 5.0332 (5.1030)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (58066.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.1999  max mem: 15572
Epoch: [0]  [1880/2809]  eta: 0:09:05  lr: 0.000006  min_lr: 0.000000  loss: 5.0503 (5.1027)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (58454.5582)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1469  max mem: 15572
Epoch: [0]  [1890/2809]  eta: 0:09:00  lr: 0.000006  min_lr: 0.000000  loss: 5.0270 (5.1022)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (58838.5743)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1038  max mem: 15572
Epoch: [0]  [1900/2809]  eta: 0:08:54  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1021)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (59218.5502)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1694  max mem: 15572
[2025-01-15 14:41:07,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:41:07,814] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-15 14:41:09,530] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1909
[2025-01-15 14:41:09,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:41:09,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [1910/2809]  eta: 0:08:48  lr: 0.000006  min_lr: 0.000000  loss: 5.0328 (5.1017)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (59868.9021)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1700  max mem: 15572
Epoch: [0]  [1920/2809]  eta: 0:08:42  lr: 0.000006  min_lr: 0.000000  loss: 5.0367 (5.1014)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (60239.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1249  max mem: 15572
Epoch: [0]  [1930/2809]  eta: 0:08:36  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1008)  class_acc: 0.0000 (0.0170)  loss_scale: 131072.0000 (60606.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1295  max mem: 15572
Epoch: [0]  [1940/2809]  eta: 0:08:31  lr: 0.000006  min_lr: 0.000000  loss: 4.9997 (5.1004)  class_acc: 0.0000 (0.0169)  loss_scale: 131072.0000 (60969.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1561  max mem: 15572
Epoch: [0]  [1950/2809]  eta: 0:08:24  lr: 0.000007  min_lr: 0.000000  loss: 5.0086 (5.1000)  class_acc: 0.0000 (0.0171)  loss_scale: 131072.0000 (61328.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1326  max mem: 15572
Epoch: [0]  [1960/2809]  eta: 0:08:19  lr: 0.000007  min_lr: 0.000000  loss: 4.9853 (5.0996)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (61684.3814)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1279  max mem: 15572
Epoch: [0]  [1970/2809]  eta: 0:08:13  lr: 0.000007  min_lr: 0.000000  loss: 4.9785 (5.0991)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (62036.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1473  max mem: 15572
Epoch: [0]  [1980/2809]  eta: 0:08:07  lr: 0.000007  min_lr: 0.000000  loss: 5.0137 (5.0989)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (62384.9127)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1989  max mem: 15572
Epoch: [0]  [1990/2809]  eta: 0:08:02  lr: 0.000007  min_lr: 0.000000  loss: 5.0183 (5.0985)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (62729.9006)  weight_decay: 0.0500 (0.0500)  time: 0.6554  data: 0.2101  max mem: 15572
[2025-01-15 14:42:03,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[6.464542191180159e-08, 6.464542191180159e-08, 9.235060273114513e-08, 9.235060273114513e-08, 1.3192943247306448e-07, 1.3192943247306448e-07, 1.8847061781866357e-07, 1.8847061781866357e-07, 2.6924373974094795e-07, 2.6924373974094795e-07, 3.8463391391563995e-07, 3.8463391391563995e-07, 5.494770198794857e-07, 5.494770198794857e-07, 7.849671712564082e-07, 7.849671712564082e-07, 1.1213816732234404e-06, 1.1213816732234404e-06, 1.6019738188906293e-06, 1.6019738188906293e-06, 2.288534026986613e-06, 2.288534026986613e-06, 3.2693343242665907e-06, 3.2693343242665907e-06, 4.670477606095129e-06, 4.670477606095129e-06, 6.6721108658501856e-06, 6.6721108658501856e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:42:03,548] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=28.704507265596508, CurrSamplesPerSec=24.29662441004292, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [0]  [2000/2809]  eta: 0:07:55  lr: 0.000007  min_lr: 0.000000  loss: 5.0114 (5.0981)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (63071.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1160  max mem: 15572
Epoch: [0]  [2010/2809]  eta: 0:07:49  lr: 0.000007  min_lr: 0.000000  loss: 4.9854 (5.0975)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (63409.5833)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0568  max mem: 15572
Epoch: [0]  [2020/2809]  eta: 0:07:44  lr: 0.000007  min_lr: 0.000000  loss: 4.9888 (5.0973)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (63744.3800)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1244  max mem: 15572
Epoch: [0]  [2030/2809]  eta: 0:07:37  lr: 0.000007  min_lr: 0.000000  loss: 5.0313 (5.0971)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (64075.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1192  max mem: 15572
[2025-01-15 14:42:25,727] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:42:25,727] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2040/2809]  eta: 0:07:32  lr: 0.000007  min_lr: 0.000000  loss: 4.9939 (5.0964)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (64596.7898)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0721  max mem: 15572
Epoch: [0]  [2050/2809]  eta: 0:07:26  lr: 0.000007  min_lr: 0.000000  loss: 4.9703 (5.0960)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (65559.9649)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.0927  max mem: 15572
Epoch: [0]  [2060/2809]  eta: 0:07:19  lr: 0.000007  min_lr: 0.000000  loss: 5.0084 (5.0957)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (66513.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0822  max mem: 15572
Epoch: [0]  [2070/2809]  eta: 0:07:14  lr: 0.000007  min_lr: 0.000000  loss: 5.0252 (5.0954)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (67458.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1655  max mem: 15572
Epoch: [0]  [2080/2809]  eta: 0:07:08  lr: 0.000007  min_lr: 0.000000  loss: 5.0044 (5.0949)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (68393.9491)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1607  max mem: 15572
[2025-01-15 14:42:52,999] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2086
[2025-01-15 14:42:52,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:42:52,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2090/2809]  eta: 0:07:02  lr: 0.000007  min_lr: 0.000000  loss: 4.9915 (5.0944)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (69007.1200)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0791  max mem: 15572
Epoch: [0]  [2100/2809]  eta: 0:06:56  lr: 0.000007  min_lr: 0.000000  loss: 5.0105 (5.0942)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (69302.5264)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1320  max mem: 15572
Epoch: [0]  [2110/2809]  eta: 0:06:50  lr: 0.000007  min_lr: 0.000000  loss: 5.0079 (5.0937)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (69595.1341)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1838  max mem: 15572
Epoch: [0]  [2120/2809]  eta: 0:06:44  lr: 0.000007  min_lr: 0.000000  loss: 4.9933 (5.0933)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (69884.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1371  max mem: 15572
Epoch: [0]  [2130/2809]  eta: 0:06:38  lr: 0.000007  min_lr: 0.000000  loss: 4.9997 (5.0930)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (70172.1107)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1291  max mem: 15572
Epoch: [0]  [2140/2809]  eta: 0:06:33  lr: 0.000007  min_lr: 0.000000  loss: 5.0188 (5.0929)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (70456.5567)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1497  max mem: 15572
Epoch: [0]  [2150/2809]  eta: 0:06:27  lr: 0.000007  min_lr: 0.000000  loss: 5.0121 (5.0926)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (70738.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1530  max mem: 15572
Epoch: [0]  [2160/2809]  eta: 0:06:21  lr: 0.000007  min_lr: 0.000000  loss: 5.0001 (5.0922)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (71017.5511)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1977  max mem: 15572
Epoch: [0]  [2170/2809]  eta: 0:06:15  lr: 0.000007  min_lr: 0.000000  loss: 5.0220 (5.0919)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (71294.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1546  max mem: 15572
Epoch: [0]  [2180/2809]  eta: 0:06:09  lr: 0.000007  min_lr: 0.000000  loss: 5.0319 (5.0918)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (71568.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.1033  max mem: 15572
Epoch: [0]  [2190/2809]  eta: 0:06:03  lr: 0.000007  min_lr: 0.000000  loss: 5.0153 (5.0913)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (71839.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1497  max mem: 15572
Epoch: [0]  [2200/2809]  eta: 0:05:57  lr: 0.000007  min_lr: 0.000000  loss: 5.0022 (5.0910)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (72108.9541)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.2122  max mem: 15572
Epoch: [0]  [2210/2809]  eta: 0:05:52  lr: 0.000007  min_lr: 0.000000  loss: 5.0012 (5.0904)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (72375.6346)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1826  max mem: 15572
[2025-01-15 14:44:09,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:44:09,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2220/2809]  eta: 0:05:46  lr: 0.000007  min_lr: 0.000000  loss: 4.9886 (5.0900)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (72994.0027)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1329  max mem: 15572
Epoch: [0]  [2230/2809]  eta: 0:05:40  lr: 0.000007  min_lr: 0.000000  loss: 4.9899 (5.0894)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (73841.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1777  max mem: 15572
Epoch: [0]  [2240/2809]  eta: 0:05:34  lr: 0.000007  min_lr: 0.000000  loss: 5.0007 (5.0892)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (74682.0884)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1571  max mem: 15572
Epoch: [0]  [2250/2809]  eta: 0:05:28  lr: 0.000008  min_lr: 0.000000  loss: 5.0239 (5.0888)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (75514.8823)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1405  max mem: 15572
Epoch: [0]  [2260/2809]  eta: 0:05:22  lr: 0.000008  min_lr: 0.000000  loss: 5.0211 (5.0886)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (76340.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1197  max mem: 15572
[2025-01-15 14:44:37,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2263
[2025-01-15 14:44:37,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:44:37,647] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2270/2809]  eta: 0:05:16  lr: 0.000008  min_lr: 0.000000  loss: 4.9858 (5.0880)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (76696.7433)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0733  max mem: 15572
Epoch: [0]  [2280/2809]  eta: 0:05:10  lr: 0.000008  min_lr: 0.000000  loss: 4.9672 (5.0876)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (76935.1267)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1468  max mem: 15572
Epoch: [0]  [2290/2809]  eta: 0:05:04  lr: 0.000008  min_lr: 0.000000  loss: 5.0192 (5.0872)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (77171.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1644  max mem: 15572
Epoch: [0]  [2300/2809]  eta: 0:04:58  lr: 0.000008  min_lr: 0.000000  loss: 4.9886 (5.0867)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (77405.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1215  max mem: 15572
Epoch: [0]  [2310/2809]  eta: 0:04:52  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0864)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (77637.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0688  max mem: 15572
Epoch: [0]  [2320/2809]  eta: 0:04:47  lr: 0.000008  min_lr: 0.000000  loss: 4.9738 (5.0861)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (77868.1189)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1080  max mem: 15572
Epoch: [0]  [2330/2809]  eta: 0:04:41  lr: 0.000008  min_lr: 0.000000  loss: 4.9869 (5.0857)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (78096.3638)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.1902  max mem: 15572
Epoch: [0]  [2340/2809]  eta: 0:04:35  lr: 0.000008  min_lr: 0.000000  loss: 5.0125 (5.0856)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (78322.6587)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1534  max mem: 15572
Epoch: [0]  [2350/2809]  eta: 0:04:29  lr: 0.000008  min_lr: 0.000000  loss: 5.0449 (5.0854)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (78547.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.1957  max mem: 15572
Epoch: [0]  [2360/2809]  eta: 0:04:24  lr: 0.000008  min_lr: 0.000000  loss: 5.0186 (5.0851)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (78769.4977)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.2215  max mem: 15572
Epoch: [0]  [2370/2809]  eta: 0:04:18  lr: 0.000008  min_lr: 0.000000  loss: 5.0500 (5.0852)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (78990.0903)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.1908  max mem: 15572
Epoch: [0]  [2380/2809]  eta: 0:04:12  lr: 0.000008  min_lr: 0.000000  loss: 5.0313 (5.0848)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (79208.8299)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1399  max mem: 15572
Epoch: [0]  [2390/2809]  eta: 0:04:06  lr: 0.000008  min_lr: 0.000000  loss: 5.0198 (5.0846)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (79425.7399)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1365  max mem: 15572
[2025-01-15 14:45:55,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:45:55,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-01-15 14:45:58,963] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2399
[2025-01-15 14:45:58,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:45:58,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2400/2809]  eta: 0:04:00  lr: 0.000008  min_lr: 0.000000  loss: 5.0043 (5.0841)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (80022.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1289  max mem: 15572
Epoch: [0]  [2410/2809]  eta: 0:03:54  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0835)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (80234.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1121  max mem: 15572
Epoch: [0]  [2420/2809]  eta: 0:03:48  lr: 0.000008  min_lr: 0.000000  loss: 4.9712 (5.0834)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (80444.6956)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0975  max mem: 15572
Epoch: [0]  [2430/2809]  eta: 0:03:42  lr: 0.000008  min_lr: 0.000000  loss: 5.0326 (5.0833)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (80652.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1543  max mem: 15572
Epoch: [0]  [2440/2809]  eta: 0:03:36  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0831)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (80859.5035)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1312  max mem: 15572
Epoch: [0]  [2450/2809]  eta: 0:03:31  lr: 0.000008  min_lr: 0.000000  loss: 5.0418 (5.0828)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (81064.3688)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.0664  max mem: 15572
Epoch: [0]  [2460/2809]  eta: 0:03:25  lr: 0.000008  min_lr: 0.000000  loss: 5.0174 (5.0826)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (81267.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0966  max mem: 15572
Epoch: [0]  [2470/2809]  eta: 0:03:19  lr: 0.000008  min_lr: 0.000000  loss: 5.0133 (5.0823)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (81469.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0880  max mem: 15572
Epoch: [0]  [2480/2809]  eta: 0:03:13  lr: 0.000008  min_lr: 0.000000  loss: 4.9949 (5.0819)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (81669.0560)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1327  max mem: 15572
Epoch: [0]  [2490/2809]  eta: 0:03:07  lr: 0.000008  min_lr: 0.000000  loss: 4.9702 (5.0815)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (81867.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1061  max mem: 15572
Epoch: [0]  [2500/2809]  eta: 0:03:01  lr: 0.000008  min_lr: 0.000000  loss: 5.0015 (5.0812)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (82064.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1194  max mem: 15572
Epoch: [0]  [2510/2809]  eta: 0:02:55  lr: 0.000008  min_lr: 0.000000  loss: 4.9884 (5.0809)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (82259.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1578  max mem: 15572
Epoch: [0]  [2520/2809]  eta: 0:02:49  lr: 0.000008  min_lr: 0.000000  loss: 4.9615 (5.0804)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (82452.9187)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.2000  max mem: 15572
[2025-01-15 14:47:12,823] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:47:12,823] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2530/2809]  eta: 0:02:43  lr: 0.000008  min_lr: 0.000000  loss: 4.9685 (5.0801)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (82800.3730)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1706  max mem: 15572
Epoch: [0]  [2540/2809]  eta: 0:02:38  lr: 0.000008  min_lr: 0.000000  loss: 5.0214 (5.0798)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (83506.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1351  max mem: 15572
Epoch: [0]  [2550/2809]  eta: 0:02:32  lr: 0.000009  min_lr: 0.000000  loss: 5.0318 (5.0798)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (84206.4383)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1337  max mem: 15572
Epoch: [0]  [2560/2809]  eta: 0:02:26  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0794)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (84901.2355)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0712  max mem: 15572
[2025-01-15 14:47:33,682] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2565
[2025-01-15 14:47:33,682] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:47:33,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2570/2809]  eta: 0:02:20  lr: 0.000009  min_lr: 0.000000  loss: 4.9903 (5.0792)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (85284.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0582  max mem: 15572
Epoch: [0]  [2580/2809]  eta: 0:02:14  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0788)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (85462.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1232  max mem: 15572
Epoch: [0]  [2590/2809]  eta: 0:02:08  lr: 0.000009  min_lr: 0.000000  loss: 4.9850 (5.0786)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (85638.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1990  max mem: 15572
Epoch: [0]  [2600/2809]  eta: 0:02:02  lr: 0.000009  min_lr: 0.000000  loss: 4.9907 (5.0780)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (85812.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1648  max mem: 15572
Epoch: [0]  [2610/2809]  eta: 0:01:56  lr: 0.000009  min_lr: 0.000000  loss: 4.9891 (5.0776)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (85986.1938)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1406  max mem: 15572
Epoch: [0]  [2620/2809]  eta: 0:01:51  lr: 0.000009  min_lr: 0.000000  loss: 5.0390 (5.0776)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (86158.2114)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1582  max mem: 15572
Epoch: [0]  [2630/2809]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000000  loss: 5.0639 (5.0774)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (86328.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1531  max mem: 15572
Epoch: [0]  [2640/2809]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 4.9870 (5.0772)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (86498.3385)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1163  max mem: 15572
Epoch: [0]  [2650/2809]  eta: 0:01:33  lr: 0.000009  min_lr: 0.000000  loss: 5.0347 (5.0771)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (86666.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0938  max mem: 15572
Epoch: [0]  [2660/2809]  eta: 0:01:27  lr: 0.000009  min_lr: 0.000000  loss: 5.0035 (5.0768)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (86833.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1099  max mem: 15572
Epoch: [0]  [2670/2809]  eta: 0:01:21  lr: 0.000009  min_lr: 0.000000  loss: 4.9875 (5.0765)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (86998.9787)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0714  max mem: 15572
Epoch: [0]  [2680/2809]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 4.9701 (5.0761)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (87163.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1116  max mem: 15572
Epoch: [0]  [2690/2809]  eta: 0:01:09  lr: 0.000009  min_lr: 0.000000  loss: 4.9704 (5.0759)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (87326.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1822  max mem: 15572
[2025-01-15 14:48:47,952] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:48:47,952] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [0]  [2700/2809]  eta: 0:01:03  lr: 0.000009  min_lr: 0.000000  loss: 5.0239 (5.0758)  class_acc: 0.0000 (0.0176)  loss_scale: 131072.0000 (87828.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1606  max mem: 15572
Epoch: [0]  [2710/2809]  eta: 0:00:58  lr: 0.000009  min_lr: 0.000000  loss: 5.0170 (5.0756)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (88471.1826)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1396  max mem: 15572
Epoch: [0]  [2720/2809]  eta: 0:00:52  lr: 0.000009  min_lr: 0.000000  loss: 5.0047 (5.0755)  class_acc: 0.0000 (0.0175)  loss_scale: 262144.0000 (89109.4509)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1347  max mem: 15572
Epoch: [0]  [2730/2809]  eta: 0:00:46  lr: 0.000009  min_lr: 0.000000  loss: 4.9830 (5.0752)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (89743.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1464  max mem: 15572
Epoch: [0]  [2740/2809]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 4.9566 (5.0748)  class_acc: 0.0000 (0.0176)  loss_scale: 262144.0000 (90372.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1357  max mem: 15572
[2025-01-15 14:49:20,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2750
[2025-01-15 14:49:20,717] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:49:20,717] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [0]  [2750/2809]  eta: 0:00:34  lr: 0.000009  min_lr: 0.000000  loss: 4.9599 (5.0745)  class_acc: 0.0000 (0.0177)  loss_scale: 262144.0000 (90948.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1313  max mem: 15572
Epoch: [0]  [2760/2809]  eta: 0:00:28  lr: 0.000009  min_lr: 0.000000  loss: 5.0024 (5.0742)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91094.0905)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1649  max mem: 15572
Epoch: [0]  [2770/2809]  eta: 0:00:22  lr: 0.000009  min_lr: 0.000000  loss: 4.9534 (5.0738)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91238.3630)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1711  max mem: 15572
Epoch: [0]  [2780/2809]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 5.0012 (5.0736)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91381.5980)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1839  max mem: 15572
Epoch: [0]  [2790/2809]  eta: 0:00:11  lr: 0.000009  min_lr: 0.000000  loss: 4.9633 (5.0731)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91523.8065)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1402  max mem: 15572
Epoch: [0]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 4.9705 (5.0731)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91664.9996)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0782  max mem: 15572
Epoch: [0]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 5.0421 (5.0730)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)  time: 0.4559  data: 0.0483  max mem: 15572
Epoch: [0] Total time: 0:27:27 (0.5864 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 5.0421 (5.0730)  class_acc: 0.0000 (0.0177)  loss_scale: 131072.0000 (91777.2303)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:21  loss: 4.9922 (4.9922)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 4.0483  data: 3.7990  max mem: 15572
Val:  [ 10/272]  eta: 0:03:29  loss: 5.1506 (5.0917)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8007  data: 0.6196  max mem: 15572
Val:  [ 20/272]  eta: 0:02:12  loss: 5.1836 (5.0610)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3492  data: 0.1621  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 5.1020 (5.0353)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2287  data: 0.0283  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 4.8739 (4.9999)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2692  data: 0.0562  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 4.8283 (5.0215)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.3160  data: 0.1216  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 4.8307 (5.0150)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.2794  data: 0.1153  max mem: 15572
Val:  [ 70/272]  eta: 0:01:07  loss: 4.8320 (4.9990)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (4.8513)  time: 0.2041  data: 0.0330  max mem: 15572
Val:  [ 80/272]  eta: 0:01:01  loss: 4.9648 (4.9579)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.1221)  time: 0.1972  data: 0.0191  max mem: 15572
Val:  [ 90/272]  eta: 0:00:58  loss: 5.0204 (4.9828)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.1197)  time: 0.2839  data: 0.0997  max mem: 15572
Val:  [100/272]  eta: 0:00:56  loss: 5.1758 (5.0141)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.3157)  time: 0.3514  data: 0.1593  max mem: 15572
Val:  [110/272]  eta: 0:00:53  loss: 5.2500 (5.0352)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.6567)  time: 0.3635  data: 0.1647  max mem: 15572
Val:  [120/272]  eta: 0:00:50  loss: 5.1777 (5.0578)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (6.1065)  time: 0.3482  data: 0.1463  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 5.1120 (5.0564)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (7.5912)  time: 0.3713  data: 0.1689  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 4.8459 (5.0422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (9.0623)  time: 0.4035  data: 0.1968  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 4.8459 (5.0327)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (8.4621)  time: 0.3536  data: 0.1435  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 4.6328 (5.0026)  acc1: 0.0000 (1.8634)  acc5: 0.0000 (9.7999)  time: 0.3196  data: 0.1007  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 4.6250 (5.0105)  acc1: 0.0000 (1.9493)  acc5: 0.0000 (9.4217)  time: 0.3500  data: 0.1377  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 5.1155 (5.0132)  acc1: 0.0000 (1.8416)  acc5: 0.0000 (8.9012)  time: 0.4215  data: 0.2243  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 5.1745 (5.0205)  acc1: 0.0000 (1.7452)  acc5: 0.0000 (8.4351)  time: 0.3795  data: 0.1642  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 5.0762 (5.0293)  acc1: 0.0000 (1.6584)  acc5: 0.0000 (8.0155)  time: 0.2993  data: 0.0808  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 5.0757 (5.0345)  acc1: 0.0000 (1.5798)  acc5: 0.0000 (7.6356)  time: 0.3228  data: 0.1211  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.8477 (5.0207)  acc1: 0.0000 (1.5083)  acc5: 0.0000 (7.2901)  time: 0.3763  data: 0.1773  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.7385 (5.0101)  acc1: 0.0000 (1.4430)  acc5: 0.0000 (6.9745)  time: 0.4047  data: 0.2033  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.7630 (5.0087)  acc1: 0.0000 (1.3831)  acc5: 0.0000 (6.6851)  time: 0.3894  data: 0.1754  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.2500 (5.0253)  acc1: 0.0000 (1.3280)  acc5: 0.0000 (6.4188)  time: 0.3558  data: 0.1291  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 5.1847 (5.0217)  acc1: 0.0000 (1.2771)  acc5: 0.0000 (6.1728)  time: 0.3263  data: 0.1205  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 5.0894 (5.0235)  acc1: 0.0000 (1.2300)  acc5: 0.0000 (5.9451)  time: 0.3098  data: 0.1359  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 5.0948 (5.0255)  acc1: 0.0000 (1.2288)  acc5: 0.0000 (5.9390)  time: 0.3053  data: 0.1358  max mem: 15572
Val: Total time: 0:01:34 (0.3466 s / it)
* Acc@1 1.229 Acc@5 5.939 loss 5.025
Accuracy of the network on the 4883 val videos: 1.2%
[2025-01-15 14:51:27,162] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/maggie/miniconda3/envs/timesformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-01-15 14:51:27,167] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 14:51:27,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 14:51:30,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 14:51:30,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 1.23%
Epoch: [1]  [   0/2809]  eta: 9:40:49  lr: 0.000009  min_lr: 0.000000  loss: 4.8862 (4.8862)  class_acc: 0.0833 (0.0833)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 12.4064  data: 11.9196  max mem: 15572
Epoch: [1]  [  10/2809]  eta: 1:15:13  lr: 0.000009  min_lr: 0.000000  loss: 4.9537 (4.9949)  class_acc: 0.0000 (0.0303)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6125  data: 1.1658  max mem: 15572
Epoch: [1]  [  20/2809]  eta: 0:48:01  lr: 0.000009  min_lr: 0.000000  loss: 5.0245 (5.0164)  class_acc: 0.0000 (0.0238)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4647  data: 0.0454  max mem: 15572
Epoch: [1]  [  30/2809]  eta: 0:38:42  lr: 0.000009  min_lr: 0.000000  loss: 5.0256 (5.0102)  class_acc: 0.0000 (0.0215)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4086  data: 0.0004  max mem: 15572
Epoch: [1]  [  40/2809]  eta: 0:34:39  lr: 0.000010  min_lr: 0.000000  loss: 4.9364 (4.9983)  class_acc: 0.0000 (0.0264)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4546  data: 0.0006  max mem: 15572
Epoch: [1]  [  50/2809]  eta: 0:32:38  lr: 0.000010  min_lr: 0.000000  loss: 4.9784 (5.0038)  class_acc: 0.0000 (0.0229)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0488  max mem: 15572
Epoch: [1]  [  60/2809]  eta: 0:31:55  lr: 0.000010  min_lr: 0.000000  loss: 4.9952 (5.0033)  class_acc: 0.0000 (0.0219)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1420  max mem: 15572
[2025-01-15 14:52:17,242] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:52:17,243] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [  70/2809]  eta: 0:30:08  lr: 0.000010  min_lr: 0.000000  loss: 4.9961 (5.0075)  class_acc: 0.0000 (0.0211)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0939  max mem: 15572
Epoch: [1]  [  80/2809]  eta: 0:29:33  lr: 0.000010  min_lr: 0.000000  loss: 5.0365 (5.0107)  class_acc: 0.0000 (0.0226)  loss_scale: 262144.0000 (148871.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0539  max mem: 15572
Epoch: [1]  [  90/2809]  eta: 0:28:48  lr: 0.000010  min_lr: 0.000000  loss: 4.9996 (5.0053)  class_acc: 0.0000 (0.0256)  loss_scale: 262144.0000 (161319.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0891  max mem: 15572
Epoch: [1]  [ 100/2809]  eta: 0:28:49  lr: 0.000010  min_lr: 0.000000  loss: 4.9581 (5.0023)  class_acc: 0.0000 (0.0248)  loss_scale: 262144.0000 (171302.0198)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1457  max mem: 15572
[2025-01-15 14:52:38,732] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2916
[2025-01-15 14:52:38,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:52:38,735] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 110/2809]  eta: 0:28:23  lr: 0.000010  min_lr: 0.000000  loss: 4.9357 (4.9972)  class_acc: 0.0000 (0.0240)  loss_scale: 262144.0000 (174762.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1734  max mem: 15572
Epoch: [1]  [ 120/2809]  eta: 0:28:25  lr: 0.000010  min_lr: 0.000000  loss: 4.9595 (4.9947)  class_acc: 0.0000 (0.0227)  loss_scale: 131072.0000 (171151.8678)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1693  max mem: 15572
Epoch: [1]  [ 130/2809]  eta: 0:28:12  lr: 0.000010  min_lr: 0.000000  loss: 4.9894 (4.9951)  class_acc: 0.0000 (0.0223)  loss_scale: 131072.0000 (168092.3359)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1833  max mem: 15572
Epoch: [1]  [ 140/2809]  eta: 0:27:46  lr: 0.000010  min_lr: 0.000000  loss: 5.0020 (5.0010)  class_acc: 0.0000 (0.0219)  loss_scale: 131072.0000 (165466.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1171  max mem: 15572
Epoch: [1]  [ 150/2809]  eta: 0:27:32  lr: 0.000010  min_lr: 0.000000  loss: 5.0018 (4.9999)  class_acc: 0.0000 (0.0215)  loss_scale: 131072.0000 (163188.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1161  max mem: 15572
Epoch: [1]  [ 160/2809]  eta: 0:27:17  lr: 0.000010  min_lr: 0.000000  loss: 4.9573 (4.9968)  class_acc: 0.0000 (0.0223)  loss_scale: 131072.0000 (161194.1366)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1408  max mem: 15572
Epoch: [1]  [ 170/2809]  eta: 0:26:51  lr: 0.000010  min_lr: 0.000000  loss: 4.9466 (4.9949)  class_acc: 0.0000 (0.0219)  loss_scale: 131072.0000 (159432.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0925  max mem: 15572
Epoch: [1]  [ 180/2809]  eta: 0:26:48  lr: 0.000010  min_lr: 0.000000  loss: 5.0301 (4.9993)  class_acc: 0.0000 (0.0212)  loss_scale: 131072.0000 (157865.7238)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1158  max mem: 15572
[2025-01-15 14:53:27,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=10, lr=[9.698430230790043e-08, 9.698430230790043e-08, 1.3854900329700063e-07, 1.3854900329700063e-07, 1.9792714756714377e-07, 1.9792714756714377e-07, 2.8275306795306254e-07, 2.8275306795306254e-07, 4.0393295421866083e-07, 4.0393295421866083e-07, 5.770470774552297e-07, 5.770470774552297e-07, 8.243529677931854e-07, 8.243529677931854e-07, 1.1776470968474079e-06, 1.1776470968474079e-06, 1.682352995496297e-06, 1.682352995496297e-06, 2.4033614221375676e-06, 2.4033614221375676e-06, 3.4333734601965247e-06, 3.4333734601965247e-06, 4.9048192288521786e-06, 4.9048192288521786e-06, 7.0068846126459705e-06, 7.0068846126459705e-06, 1.0009835160922815e-05, 1.0009835160922815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 14:53:27,050] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=28.654797228758227, CurrSamplesPerSec=28.43117260483828, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [ 190/2809]  eta: 0:26:38  lr: 0.000010  min_lr: 0.000000  loss: 5.0354 (4.9997)  class_acc: 0.0000 (0.0209)  loss_scale: 131072.0000 (156462.9110)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1505  max mem: 15572
Epoch: [1]  [ 200/2809]  eta: 0:26:25  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9998)  class_acc: 0.0000 (0.0211)  loss_scale: 131072.0000 (155199.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1239  max mem: 15572
Epoch: [1]  [ 210/2809]  eta: 0:26:22  lr: 0.000010  min_lr: 0.000000  loss: 4.9873 (4.9965)  class_acc: 0.0000 (0.0213)  loss_scale: 131072.0000 (154056.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1421  max mem: 15572
Epoch: [1]  [ 220/2809]  eta: 0:26:24  lr: 0.000010  min_lr: 0.000000  loss: 4.9731 (4.9941)  class_acc: 0.0000 (0.0219)  loss_scale: 131072.0000 (153016.1810)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1842  max mem: 15572
Epoch: [1]  [ 230/2809]  eta: 0:26:05  lr: 0.000010  min_lr: 0.000000  loss: 4.9888 (4.9956)  class_acc: 0.0000 (0.0216)  loss_scale: 131072.0000 (152066.2165)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1369  max mem: 15572
[2025-01-15 14:53:55,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:53:55,183] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 240/2809]  eta: 0:26:00  lr: 0.000010  min_lr: 0.000000  loss: 4.9924 (4.9966)  class_acc: 0.0000 (0.0211)  loss_scale: 131072.0000 (153914.4232)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1173  max mem: 15572
Epoch: [1]  [ 250/2809]  eta: 0:25:54  lr: 0.000010  min_lr: 0.000000  loss: 5.0413 (4.9993)  class_acc: 0.0000 (0.0206)  loss_scale: 262144.0000 (158226.3586)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1665  max mem: 15572
Epoch: [1]  [ 260/2809]  eta: 0:25:48  lr: 0.000010  min_lr: 0.000000  loss: 4.9883 (4.9990)  class_acc: 0.0000 (0.0201)  loss_scale: 262144.0000 (162207.8774)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1618  max mem: 15572
Epoch: [1]  [ 270/2809]  eta: 0:25:50  lr: 0.000010  min_lr: 0.000000  loss: 4.9802 (5.0000)  class_acc: 0.0000 (0.0203)  loss_scale: 262144.0000 (165895.5572)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2073  max mem: 15572
[2025-01-15 14:54:18,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3084
[2025-01-15 14:54:18,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:54:18,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 280/2809]  eta: 0:25:32  lr: 0.000010  min_lr: 0.000000  loss: 5.0546 (5.0022)  class_acc: 0.0000 (0.0196)  loss_scale: 262144.0000 (166522.0783)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1463  max mem: 15572
Epoch: [1]  [ 290/2809]  eta: 0:25:26  lr: 0.000010  min_lr: 0.000000  loss: 5.0647 (5.0041)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (165303.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1092  max mem: 15572
Epoch: [1]  [ 300/2809]  eta: 0:25:14  lr: 0.000010  min_lr: 0.000000  loss: 5.0290 (5.0049)  class_acc: 0.0000 (0.0194)  loss_scale: 131072.0000 (164166.5914)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1374  max mem: 15572
Epoch: [1]  [ 310/2809]  eta: 0:25:06  lr: 0.000010  min_lr: 0.000000  loss: 4.9723 (5.0040)  class_acc: 0.0000 (0.0190)  loss_scale: 131072.0000 (163102.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0964  max mem: 15572
Epoch: [1]  [ 320/2809]  eta: 0:24:56  lr: 0.000010  min_lr: 0.000000  loss: 4.9245 (5.0017)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (162104.6231)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.0986  max mem: 15572
Epoch: [1]  [ 330/2809]  eta: 0:24:50  lr: 0.000010  min_lr: 0.000000  loss: 4.8942 (5.0002)  class_acc: 0.0000 (0.0206)  loss_scale: 131072.0000 (161167.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1351  max mem: 15572
Epoch: [1]  [ 340/2809]  eta: 0:24:47  lr: 0.000011  min_lr: 0.000000  loss: 4.9083 (4.9975)  class_acc: 0.0000 (0.0208)  loss_scale: 131072.0000 (160284.5279)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1803  max mem: 15572
Epoch: [1]  [ 350/2809]  eta: 0:24:43  lr: 0.000011  min_lr: 0.000000  loss: 4.9450 (4.9963)  class_acc: 0.0000 (0.0207)  loss_scale: 131072.0000 (159452.2621)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1762  max mem: 15572
Epoch: [1]  [ 360/2809]  eta: 0:24:31  lr: 0.000011  min_lr: 0.000000  loss: 4.9546 (4.9968)  class_acc: 0.0000 (0.0203)  loss_scale: 131072.0000 (158666.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1090  max mem: 15572
Epoch: [1]  [ 370/2809]  eta: 0:24:25  lr: 0.000011  min_lr: 0.000000  loss: 4.9727 (4.9959)  class_acc: 0.0000 (0.0200)  loss_scale: 131072.0000 (157922.3288)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1205  max mem: 15572
Epoch: [1]  [ 380/2809]  eta: 0:24:21  lr: 0.000011  min_lr: 0.000000  loss: 4.9719 (4.9965)  class_acc: 0.0000 (0.0199)  loss_scale: 131072.0000 (157217.5958)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1878  max mem: 15572
Epoch: [1]  [ 390/2809]  eta: 0:24:13  lr: 0.000011  min_lr: 0.000000  loss: 5.0120 (4.9967)  class_acc: 0.0000 (0.0207)  loss_scale: 131072.0000 (156548.9105)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1644  max mem: 15572
Epoch: [1]  [ 400/2809]  eta: 0:24:05  lr: 0.000011  min_lr: 0.000000  loss: 5.0018 (4.9959)  class_acc: 0.0000 (0.0204)  loss_scale: 131072.0000 (155913.5761)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1425  max mem: 15572
[2025-01-15 14:55:35,740] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:55:35,740] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 410/2809]  eta: 0:24:05  lr: 0.000011  min_lr: 0.000000  loss: 4.9906 (4.9958)  class_acc: 0.0000 (0.0205)  loss_scale: 131072.0000 (157541.5280)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.2094  max mem: 15572
[2025-01-15 14:55:40,344] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3220
[2025-01-15 14:55:40,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:55:40,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 420/2809]  eta: 0:24:01  lr: 0.000011  min_lr: 0.000000  loss: 5.0235 (4.9959)  class_acc: 0.0000 (0.0204)  loss_scale: 131072.0000 (156912.7981)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.2205  max mem: 15572
Epoch: [1]  [ 430/2809]  eta: 0:23:52  lr: 0.000011  min_lr: 0.000000  loss: 5.0095 (4.9954)  class_acc: 0.0000 (0.0201)  loss_scale: 131072.0000 (156313.2436)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1349  max mem: 15572
Epoch: [1]  [ 440/2809]  eta: 0:23:42  lr: 0.000011  min_lr: 0.000000  loss: 4.9743 (4.9946)  class_acc: 0.0000 (0.0200)  loss_scale: 131072.0000 (155740.8798)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1007  max mem: 15572
Epoch: [1]  [ 450/2809]  eta: 0:23:41  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9936)  class_acc: 0.0000 (0.0198)  loss_scale: 131072.0000 (155193.8980)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1699  max mem: 15572
Epoch: [1]  [ 460/2809]  eta: 0:23:29  lr: 0.000011  min_lr: 0.000000  loss: 4.9470 (4.9938)  class_acc: 0.0000 (0.0197)  loss_scale: 131072.0000 (154670.6464)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1481  max mem: 15572
Epoch: [1]  [ 470/2809]  eta: 0:23:26  lr: 0.000011  min_lr: 0.000000  loss: 4.9925 (4.9946)  class_acc: 0.0000 (0.0195)  loss_scale: 131072.0000 (154169.6136)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1431  max mem: 15572
Epoch: [1]  [ 480/2809]  eta: 0:23:19  lr: 0.000011  min_lr: 0.000000  loss: 5.0116 (4.9946)  class_acc: 0.0000 (0.0192)  loss_scale: 131072.0000 (153689.4137)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1841  max mem: 15572
Epoch: [1]  [ 490/2809]  eta: 0:23:15  lr: 0.000011  min_lr: 0.000000  loss: 4.9737 (4.9938)  class_acc: 0.0000 (0.0192)  loss_scale: 131072.0000 (153228.7739)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1638  max mem: 15572
Epoch: [1]  [ 500/2809]  eta: 0:23:03  lr: 0.000011  min_lr: 0.000000  loss: 4.9396 (4.9938)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (152786.5230)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1242  max mem: 15572
Epoch: [1]  [ 510/2809]  eta: 0:22:59  lr: 0.000011  min_lr: 0.000000  loss: 4.9434 (4.9925)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (152361.5812)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1327  max mem: 15572
Epoch: [1]  [ 520/2809]  eta: 0:22:54  lr: 0.000011  min_lr: 0.000000  loss: 4.9867 (4.9935)  class_acc: 0.0000 (0.0189)  loss_scale: 131072.0000 (151952.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1871  max mem: 15572
Epoch: [1]  [ 530/2809]  eta: 0:22:46  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9937)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (151559.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1302  max mem: 15572
[2025-01-15 14:56:55,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:56:55,182] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 540/2809]  eta: 0:22:41  lr: 0.000011  min_lr: 0.000000  loss: 5.0357 (4.9952)  class_acc: 0.0000 (0.0191)  loss_scale: 131072.0000 (151423.2902)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1358  max mem: 15572
Epoch: [1]  [ 550/2809]  eta: 0:22:32  lr: 0.000011  min_lr: 0.000000  loss: 5.0675 (4.9961)  class_acc: 0.0000 (0.0189)  loss_scale: 262144.0000 (153432.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1292  max mem: 15572
[2025-01-15 14:57:06,478] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3369
[2025-01-15 14:57:06,478] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:57:06,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [ 560/2809]  eta: 0:22:26  lr: 0.000011  min_lr: 0.000000  loss: 5.0528 (4.9968)  class_acc: 0.0000 (0.0186)  loss_scale: 262144.0000 (155136.9127)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0995  max mem: 15572
Epoch: [1]  [ 570/2809]  eta: 0:22:21  lr: 0.000011  min_lr: 0.000000  loss: 5.0044 (4.9961)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (154715.4606)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1529  max mem: 15572
Epoch: [1]  [ 580/2809]  eta: 0:22:13  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9969)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (154308.5164)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1587  max mem: 15572
Epoch: [1]  [ 590/2809]  eta: 0:22:08  lr: 0.000011  min_lr: 0.000000  loss: 5.0087 (4.9968)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (153915.3435)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1596  max mem: 15572
Epoch: [1]  [ 600/2809]  eta: 0:22:01  lr: 0.000011  min_lr: 0.000000  loss: 4.9798 (4.9968)  class_acc: 0.0000 (0.0186)  loss_scale: 131072.0000 (153535.2546)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1572  max mem: 15572
Epoch: [1]  [ 610/2809]  eta: 0:21:51  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9972)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (153167.6072)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0868  max mem: 15572
Epoch: [1]  [ 620/2809]  eta: 0:21:47  lr: 0.000011  min_lr: 0.000000  loss: 5.0114 (4.9967)  class_acc: 0.0000 (0.0188)  loss_scale: 131072.0000 (152811.8003)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1046  max mem: 15572
Epoch: [1]  [ 630/2809]  eta: 0:21:40  lr: 0.000011  min_lr: 0.000000  loss: 5.0298 (4.9976)  class_acc: 0.0000 (0.0185)  loss_scale: 131072.0000 (152467.2710)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1644  max mem: 15572
Epoch: [1]  [ 640/2809]  eta: 0:21:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0298 (4.9985)  class_acc: 0.0000 (0.0182)  loss_scale: 131072.0000 (152133.4914)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1812  max mem: 15572
Epoch: [1]  [ 650/2809]  eta: 0:21:29  lr: 0.000012  min_lr: 0.000000  loss: 5.0424 (4.9994)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (151809.9662)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1544  max mem: 15572
Epoch: [1]  [ 660/2809]  eta: 0:21:21  lr: 0.000012  min_lr: 0.000000  loss: 5.0207 (4.9996)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (151496.2300)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0967  max mem: 15572
Epoch: [1]  [ 670/2809]  eta: 0:21:15  lr: 0.000012  min_lr: 0.000000  loss: 4.9962 (5.0007)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (151191.8450)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1150  max mem: 15572
Epoch: [1]  [ 680/2809]  eta: 0:21:08  lr: 0.000012  min_lr: 0.000000  loss: 5.0603 (5.0016)  class_acc: 0.0000 (0.0181)  loss_scale: 131072.0000 (150896.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1300  max mem: 15572
[2025-01-15 14:58:21,506] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:58:21,507] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 690/2809]  eta: 0:21:01  lr: 0.000012  min_lr: 0.000000  loss: 5.0687 (5.0027)  class_acc: 0.0000 (0.0178)  loss_scale: 131072.0000 (150988.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1185  max mem: 15572
[2025-01-15 14:58:23,114] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3500
[2025-01-15 14:58:23,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 14:58:23,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-01-15 14:58:24,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3503
[2025-01-15 14:58:24,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 14:58:24,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [ 700/2809]  eta: 0:20:52  lr: 0.000012  min_lr: 0.000000  loss: 4.9769 (5.0019)  class_acc: 0.0000 (0.0181)  loss_scale: 131072.0000 (150050.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0936  max mem: 15572
Epoch: [1]  [ 710/2809]  eta: 0:20:48  lr: 0.000012  min_lr: 0.000000  loss: 4.9769 (5.0022)  class_acc: 0.0000 (0.0179)  loss_scale: 65536.0000 (148861.6596)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1275  max mem: 15572
Epoch: [1]  [ 720/2809]  eta: 0:20:39  lr: 0.000012  min_lr: 0.000000  loss: 4.9902 (5.0015)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (147705.9639)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1244  max mem: 15572
Epoch: [1]  [ 730/2809]  eta: 0:20:32  lr: 0.000012  min_lr: 0.000000  loss: 4.9634 (5.0018)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (146581.8878)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0740  max mem: 15572
Epoch: [1]  [ 740/2809]  eta: 0:20:25  lr: 0.000012  min_lr: 0.000000  loss: 4.9552 (5.0011)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (145488.1511)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1242  max mem: 15572
Epoch: [1]  [ 750/2809]  eta: 0:20:21  lr: 0.000012  min_lr: 0.000000  loss: 4.9283 (5.0004)  class_acc: 0.0000 (0.0179)  loss_scale: 65536.0000 (144423.5419)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1802  max mem: 15572
Epoch: [1]  [ 760/2809]  eta: 0:20:13  lr: 0.000012  min_lr: 0.000000  loss: 4.9603 (5.0008)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (143386.9120)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1480  max mem: 15572
Epoch: [1]  [ 770/2809]  eta: 0:20:11  lr: 0.000012  min_lr: 0.000000  loss: 4.9921 (5.0001)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (142377.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1826  max mem: 15572
Epoch: [1]  [ 780/2809]  eta: 0:20:04  lr: 0.000012  min_lr: 0.000000  loss: 4.9785 (5.0000)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (141393.2907)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1771  max mem: 15572
Epoch: [1]  [ 790/2809]  eta: 0:19:58  lr: 0.000012  min_lr: 0.000000  loss: 4.9425 (4.9993)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (140434.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0941  max mem: 15572
Epoch: [1]  [ 800/2809]  eta: 0:19:51  lr: 0.000012  min_lr: 0.000000  loss: 4.9774 (4.9993)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (139499.2260)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0955  max mem: 15572
Epoch: [1]  [ 810/2809]  eta: 0:19:46  lr: 0.000012  min_lr: 0.000000  loss: 5.0090 (4.9998)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (138587.2256)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1222  max mem: 15572
Epoch: [1]  [ 820/2809]  eta: 0:19:38  lr: 0.000012  min_lr: 0.000000  loss: 5.0157 (5.0001)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (137697.4421)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1146  max mem: 15572
[2025-01-15 14:59:40,158] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 14:59:40,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [ 830/2809]  eta: 0:19:33  lr: 0.000012  min_lr: 0.000000  loss: 5.0141 (5.0004)  class_acc: 0.0000 (0.0173)  loss_scale: 65536.0000 (137459.9856)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1179  max mem: 15572
Epoch: [1]  [ 840/2809]  eta: 0:19:29  lr: 0.000012  min_lr: 0.000000  loss: 5.0072 (5.0007)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (137384.0285)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.2068  max mem: 15572
Epoch: [1]  [ 850/2809]  eta: 0:19:23  lr: 0.000012  min_lr: 0.000000  loss: 5.0431 (5.0012)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (137309.8566)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.2038  max mem: 15572
Epoch: [1]  [ 860/2809]  eta: 0:19:17  lr: 0.000012  min_lr: 0.000000  loss: 5.0151 (5.0006)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (137237.4077)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1592  max mem: 15572
Epoch: [1]  [ 870/2809]  eta: 0:19:12  lr: 0.000012  min_lr: 0.000000  loss: 4.9353 (5.0006)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (137166.6223)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1644  max mem: 15572
Epoch: [1]  [ 880/2809]  eta: 0:19:07  lr: 0.000012  min_lr: 0.000000  loss: 4.9321 (5.0004)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (137097.4438)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1657  max mem: 15572
Epoch: [1]  [ 890/2809]  eta: 0:18:58  lr: 0.000012  min_lr: 0.000000  loss: 4.9883 (5.0007)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (137029.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0782  max mem: 15572
Epoch: [1]  [ 900/2809]  eta: 0:18:52  lr: 0.000012  min_lr: 0.000000  loss: 5.0071 (5.0008)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (136963.6937)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0695  max mem: 15572
Epoch: [1]  [ 910/2809]  eta: 0:18:46  lr: 0.000012  min_lr: 0.000000  loss: 5.0133 (5.0014)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (136899.0209)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1447  max mem: 15572
Epoch: [1]  [ 920/2809]  eta: 0:18:42  lr: 0.000012  min_lr: 0.000000  loss: 4.9953 (5.0014)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (136835.7524)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1764  max mem: 15572
Epoch: [1]  [ 930/2809]  eta: 0:18:36  lr: 0.000012  min_lr: 0.000000  loss: 5.0012 (5.0017)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (136773.8432)  weight_decay: 0.0500 (0.0500)  time: 0.6240  data: 0.1855  max mem: 15572
Epoch: [1]  [ 940/2809]  eta: 0:18:29  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (5.0015)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (136713.2497)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1343  max mem: 15572
Epoch: [1]  [ 950/2809]  eta: 0:18:24  lr: 0.000013  min_lr: 0.000000  loss: 4.9236 (5.0005)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (136653.9306)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1588  max mem: 15572
[2025-01-15 15:00:56,097] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:00:56,098] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [1]  [ 960/2809]  eta: 0:18:19  lr: 0.000013  min_lr: 0.000000  loss: 4.9377 (5.0008)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (137959.7586)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.2107  max mem: 15572
Epoch: [1]  [ 970/2809]  eta: 0:18:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9631 (5.0011)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (139238.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1982  max mem: 15572
Epoch: [1]  [ 980/2809]  eta: 0:18:08  lr: 0.000013  min_lr: 0.000000  loss: 4.9497 (5.0006)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (140491.5474)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1824  max mem: 15572
Epoch: [1]  [ 990/2809]  eta: 0:18:00  lr: 0.000013  min_lr: 0.000000  loss: 4.9086 (5.0002)  class_acc: 0.0000 (0.0174)  loss_scale: 262144.0000 (141719.1201)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0861  max mem: 15572
Epoch: [1]  [1000/2809]  eta: 0:17:54  lr: 0.000013  min_lr: 0.000000  loss: 4.9223 (5.0001)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (142922.1658)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0794  max mem: 15572
Epoch: [1]  [1010/2809]  eta: 0:17:48  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0009)  class_acc: 0.0000 (0.0173)  loss_scale: 262144.0000 (144101.4125)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1451  max mem: 15572
[2025-01-15 15:01:37,253] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3828
[2025-01-15 15:01:37,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-01-15 15:01:37,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [1]  [1020/2809]  eta: 0:17:43  lr: 0.000013  min_lr: 0.000000  loss: 5.0424 (5.0011)  class_acc: 0.0000 (0.0172)  loss_scale: 262144.0000 (145000.8071)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.1662  max mem: 15572
Epoch: [1]  [1030/2809]  eta: 0:17:37  lr: 0.000013  min_lr: 0.000000  loss: 4.9454 (5.0007)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (144865.7071)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1753  max mem: 15572
Epoch: [1]  [1040/2809]  eta: 0:17:31  lr: 0.000013  min_lr: 0.000000  loss: 4.9710 (5.0003)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (144733.2027)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1715  max mem: 15572
Epoch: [1]  [1050/2809]  eta: 0:17:24  lr: 0.000013  min_lr: 0.000000  loss: 4.9333 (4.9996)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (144603.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1208  max mem: 15572
Epoch: [1]  [1060/2809]  eta: 0:17:21  lr: 0.000013  min_lr: 0.000000  loss: 4.9157 (4.9990)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (144475.6871)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1947  max mem: 15572
Epoch: [1]  [1070/2809]  eta: 0:17:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9246 (4.9985)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (144350.5359)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1706  max mem: 15572
Epoch: [1]  [1080/2809]  eta: 0:17:05  lr: 0.000013  min_lr: 0.000000  loss: 4.9418 (4.9980)  class_acc: 0.0000 (0.0173)  loss_scale: 131072.0000 (144227.7003)  weight_decay: 0.0500 (0.0500)  time: 0.4673  data: 0.0341  max mem: 15572
Epoch: [1]  [1090/2809]  eta: 0:16:58  lr: 0.000013  min_lr: 0.000000  loss: 4.9421 (4.9979)  class_acc: 0.0000 (0.0172)  loss_scale: 131072.0000 (144107.1164)  weight_decay: 0.0500 (0.0500)  time: 0.5158  data: 0.0676  max mem: 15572
Epoch: [1]  [1100/2809]  eta: 0:16:53  lr: 0.000013  min_lr: 0.000000  loss: 4.9770 (4.9979)  class_acc: 0.0000 (0.0174)  loss_scale: 131072.0000 (143988.7230)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1274  max mem: 15572
[2025-01-15 15:02:29,977] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3917
[2025-01-15 15:02:29,978] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:02:29,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1110/2809]  eta: 0:16:49  lr: 0.000013  min_lr: 0.000000  loss: 4.9963 (4.9978)  class_acc: 0.0000 (0.0175)  loss_scale: 131072.0000 (143695.4959)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.2322  max mem: 15572
Epoch: [1]  [1120/2809]  eta: 0:16:42  lr: 0.000013  min_lr: 0.000000  loss: 5.0379 (4.9986)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (142998.2658)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1929  max mem: 15572
Epoch: [1]  [1130/2809]  eta: 0:16:37  lr: 0.000013  min_lr: 0.000000  loss: 5.0168 (4.9986)  class_acc: 0.0000 (0.0174)  loss_scale: 65536.0000 (142313.3652)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1581  max mem: 15572
Epoch: [1]  [1140/2809]  eta: 0:16:31  lr: 0.000013  min_lr: 0.000000  loss: 4.9625 (4.9984)  class_acc: 0.0000 (0.0175)  loss_scale: 65536.0000 (141640.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1390  max mem: 15572
Epoch: [1]  [1150/2809]  eta: 0:16:26  lr: 0.000013  min_lr: 0.000000  loss: 5.0072 (4.9988)  class_acc: 0.0000 (0.0176)  loss_scale: 65536.0000 (140979.2667)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1156  max mem: 15572
Epoch: [1]  [1160/2809]  eta: 0:16:20  lr: 0.000013  min_lr: 0.000000  loss: 4.9998 (4.9985)  class_acc: 0.0000 (0.0178)  loss_scale: 65536.0000 (140329.4539)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1555  max mem: 15572
Epoch: [1]  [1170/2809]  eta: 0:16:13  lr: 0.000013  min_lr: 0.000000  loss: 4.9958 (4.9987)  class_acc: 0.0000 (0.0178)  loss_scale: 65536.0000 (139690.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1222  max mem: 15572
Epoch: [1]  [1180/2809]  eta: 0:16:06  lr: 0.000013  min_lr: 0.000000  loss: 5.0764 (4.9994)  class_acc: 0.0000 (0.0177)  loss_scale: 65536.0000 (139062.8417)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1058  max mem: 15572
[2025-01-15 15:03:17,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=17, lr=[1.2932318270399928e-07, 1.2932318270399928e-07, 1.8474740386285612e-07, 1.8474740386285612e-07, 2.6392486266122304e-07, 2.6392486266122304e-07, 3.7703551808746154e-07, 3.7703551808746154e-07, 5.386221686963737e-07, 5.386221686963737e-07, 7.694602409948195e-07, 7.694602409948195e-07, 1.0992289157068852e-06, 1.0992289157068852e-06, 1.5703270224384075e-06, 1.5703270224384075e-06, 2.2433243177691536e-06, 2.2433243177691536e-06, 3.2047490253845054e-06, 3.2047490253845054e-06, 4.578212893406436e-06, 4.578212893406436e-06, 6.5403041334377665e-06, 6.5403041334377665e-06, 9.34329161919681e-06, 9.34329161919681e-06, 1.3347559455995444e-05, 1.3347559455995444e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:03:17,955] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=28.61436974717198, CurrSamplesPerSec=26.902084535950227, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [1190/2809]  eta: 0:16:01  lr: 0.000013  min_lr: 0.000000  loss: 4.9669 (4.9988)  class_acc: 0.0000 (0.0179)  loss_scale: 65536.0000 (138445.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1321  max mem: 15572
Epoch: [1]  [1200/2809]  eta: 0:15:55  lr: 0.000013  min_lr: 0.000000  loss: 4.9559 (4.9986)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (137838.4147)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1716  max mem: 15572
Epoch: [1]  [1210/2809]  eta: 0:15:48  lr: 0.000013  min_lr: 0.000000  loss: 4.9985 (4.9988)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (137241.3675)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1261  max mem: 15572
Epoch: [1]  [1220/2809]  eta: 0:15:41  lr: 0.000013  min_lr: 0.000000  loss: 4.9965 (4.9988)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (136654.0999)  weight_decay: 0.0500 (0.0500)  time: 0.4904  data: 0.0691  max mem: 15572
Epoch: [1]  [1230/2809]  eta: 0:15:35  lr: 0.000013  min_lr: 0.000000  loss: 5.0077 (4.9988)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (136076.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1445  max mem: 15572
[2025-01-15 15:03:45,953] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:03:45,953] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1240/2809]  eta: 0:15:31  lr: 0.000014  min_lr: 0.000000  loss: 5.0098 (4.9993)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (135719.1942)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.2352  max mem: 15572
Epoch: [1]  [1250/2809]  eta: 0:15:25  lr: 0.000014  min_lr: 0.000000  loss: 5.0595 (4.9997)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (135682.0464)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1966  max mem: 15572
Epoch: [1]  [1260/2809]  eta: 0:15:18  lr: 0.000014  min_lr: 0.000000  loss: 5.0337 (4.9994)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (135645.4877)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1063  max mem: 15572
[2025-01-15 15:03:59,264] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4070
[2025-01-15 15:03:59,264] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:03:59,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1270/2809]  eta: 0:15:12  lr: 0.000014  min_lr: 0.000000  loss: 4.9074 (4.9990)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (135093.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.0994  max mem: 15572
Epoch: [1]  [1280/2809]  eta: 0:15:06  lr: 0.000014  min_lr: 0.000000  loss: 4.9494 (4.9987)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (134550.8821)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0990  max mem: 15572
Epoch: [1]  [1290/2809]  eta: 0:15:00  lr: 0.000014  min_lr: 0.000000  loss: 4.9967 (4.9991)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (134016.2974)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0979  max mem: 15572
Epoch: [1]  [1300/2809]  eta: 0:14:53  lr: 0.000014  min_lr: 0.000000  loss: 5.0099 (4.9993)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (133489.9308)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1055  max mem: 15572
Epoch: [1]  [1310/2809]  eta: 0:14:48  lr: 0.000014  min_lr: 0.000000  loss: 5.0345 (4.9997)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (132971.5942)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1241  max mem: 15572
Epoch: [1]  [1320/2809]  eta: 0:14:42  lr: 0.000014  min_lr: 0.000000  loss: 5.0169 (4.9994)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (132461.1052)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1551  max mem: 15572
Epoch: [1]  [1330/2809]  eta: 0:14:36  lr: 0.000014  min_lr: 0.000000  loss: 4.9587 (4.9990)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (131958.2870)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1468  max mem: 15572
Epoch: [1]  [1340/2809]  eta: 0:14:30  lr: 0.000014  min_lr: 0.000000  loss: 4.9883 (4.9997)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (131462.9679)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1035  max mem: 15572
Epoch: [1]  [1350/2809]  eta: 0:14:25  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9997)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (130974.9815)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1269  max mem: 15572
Epoch: [1]  [1360/2809]  eta: 0:14:19  lr: 0.000014  min_lr: 0.000000  loss: 4.9995 (4.9996)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (130494.1661)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1618  max mem: 15572
Epoch: [1]  [1370/2809]  eta: 0:14:15  lr: 0.000014  min_lr: 0.000000  loss: 4.9777 (4.9994)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (130020.3647)  weight_decay: 0.0500 (0.0500)  time: 0.6930  data: 0.2340  max mem: 15572
Epoch: [1]  [1380/2809]  eta: 0:14:09  lr: 0.000014  min_lr: 0.000000  loss: 4.9560 (4.9990)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (129553.4251)  weight_decay: 0.0500 (0.0500)  time: 0.6950  data: 0.2361  max mem: 15572
[2025-01-15 15:05:17,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:05:17,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1390/2809]  eta: 0:14:03  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9988)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (129140.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1116  max mem: 15572
Epoch: [1]  [1400/2809]  eta: 0:13:57  lr: 0.000014  min_lr: 0.000000  loss: 4.9686 (4.9985)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (129154.1014)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1097  max mem: 15572
Epoch: [1]  [1410/2809]  eta: 0:13:50  lr: 0.000014  min_lr: 0.000000  loss: 4.9262 (4.9981)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (129167.6938)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1191  max mem: 15572
Epoch: [1]  [1420/2809]  eta: 0:13:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9153 (4.9979)  class_acc: 0.0000 (0.0184)  loss_scale: 131072.0000 (129181.0950)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1328  max mem: 15572
[2025-01-15 15:05:38,288] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4234
[2025-01-15 15:05:38,289] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:05:38,289] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1430/2809]  eta: 0:13:38  lr: 0.000014  min_lr: 0.000000  loss: 4.9406 (4.9980)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (128919.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1374  max mem: 15572
Epoch: [1]  [1440/2809]  eta: 0:13:32  lr: 0.000014  min_lr: 0.000000  loss: 5.0458 (4.9987)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (128479.6669)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1052  max mem: 15572
Epoch: [1]  [1450/2809]  eta: 0:13:26  lr: 0.000014  min_lr: 0.000000  loss: 5.0130 (4.9986)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (128045.8718)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0932  max mem: 15572
Epoch: [1]  [1460/2809]  eta: 0:13:19  lr: 0.000014  min_lr: 0.000000  loss: 4.9675 (4.9987)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (127618.0151)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0996  max mem: 15572
Epoch: [1]  [1470/2809]  eta: 0:13:14  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9988)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (127195.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1643  max mem: 15572
Epoch: [1]  [1480/2809]  eta: 0:13:07  lr: 0.000014  min_lr: 0.000000  loss: 4.9800 (4.9987)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (126779.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1379  max mem: 15572
Epoch: [1]  [1490/2809]  eta: 0:13:01  lr: 0.000014  min_lr: 0.000000  loss: 4.9972 (4.9988)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (126368.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1230  max mem: 15572
Epoch: [1]  [1500/2809]  eta: 0:12:56  lr: 0.000014  min_lr: 0.000000  loss: 5.0328 (4.9990)  class_acc: 0.0000 (0.0180)  loss_scale: 65536.0000 (125963.5976)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1693  max mem: 15572
Epoch: [1]  [1510/2809]  eta: 0:12:49  lr: 0.000014  min_lr: 0.000000  loss: 4.9441 (4.9987)  class_acc: 0.0000 (0.0180)  loss_scale: 65536.0000 (125563.6797)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1169  max mem: 15572
Epoch: [1]  [1520/2809]  eta: 0:12:44  lr: 0.000014  min_lr: 0.000000  loss: 4.9295 (4.9989)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (125169.0204)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1406  max mem: 15572
Epoch: [1]  [1530/2809]  eta: 0:12:38  lr: 0.000014  min_lr: 0.000000  loss: 5.0137 (4.9988)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (124779.5167)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1773  max mem: 15572
Epoch: [1]  [1540/2809]  eta: 0:12:32  lr: 0.000015  min_lr: 0.000000  loss: 4.9685 (4.9987)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (124395.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1262  max mem: 15572
Epoch: [1]  [1550/2809]  eta: 0:12:26  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9987)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (124015.5770)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1263  max mem: 15572
[2025-01-15 15:06:54,787] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:06:54,787] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1560/2809]  eta: 0:12:21  lr: 0.000015  min_lr: 0.000000  loss: 5.0386 (4.9990)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (123934.8315)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.2070  max mem: 15572
Epoch: [1]  [1570/2809]  eta: 0:12:15  lr: 0.000015  min_lr: 0.000000  loss: 5.0313 (4.9989)  class_acc: 0.0000 (0.0181)  loss_scale: 131072.0000 (123980.2623)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.2052  max mem: 15572
Epoch: [1]  [1580/2809]  eta: 0:12:10  lr: 0.000015  min_lr: 0.000000  loss: 4.9970 (4.9992)  class_acc: 0.0000 (0.0181)  loss_scale: 131072.0000 (124025.1183)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1752  max mem: 15572
Epoch: [1]  [1590/2809]  eta: 0:12:04  lr: 0.000015  min_lr: 0.000000  loss: 4.9959 (4.9992)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (124069.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1788  max mem: 15572
Epoch: [1]  [1600/2809]  eta: 0:11:57  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9994)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (124113.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1191  max mem: 15572
Epoch: [1]  [1610/2809]  eta: 0:11:51  lr: 0.000015  min_lr: 0.000000  loss: 5.0116 (4.9991)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (124156.3451)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0767  max mem: 15572
Epoch: [1]  [1620/2809]  eta: 0:11:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9515 (4.9988)  class_acc: 0.0000 (0.0180)  loss_scale: 131072.0000 (124199.0080)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0929  max mem: 15572
[2025-01-15 15:07:35,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4433
[2025-01-15 15:07:35,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:07:35,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1630/2809]  eta: 0:11:39  lr: 0.000015  min_lr: 0.000000  loss: 4.9242 (4.9983)  class_acc: 0.0000 (0.0179)  loss_scale: 131072.0000 (123959.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1427  max mem: 15572
Epoch: [1]  [1640/2809]  eta: 0:11:33  lr: 0.000015  min_lr: 0.000000  loss: 4.9647 (4.9984)  class_acc: 0.0000 (0.0180)  loss_scale: 65536.0000 (123603.8513)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1758  max mem: 15572
Epoch: [1]  [1650/2809]  eta: 0:11:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9772 (4.9982)  class_acc: 0.0000 (0.0180)  loss_scale: 65536.0000 (123252.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1193  max mem: 15572
Epoch: [1]  [1660/2809]  eta: 0:11:21  lr: 0.000015  min_lr: 0.000000  loss: 4.9385 (4.9980)  class_acc: 0.0000 (0.0180)  loss_scale: 65536.0000 (122904.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1512  max mem: 15572
Epoch: [1]  [1670/2809]  eta: 0:11:16  lr: 0.000015  min_lr: 0.000000  loss: 4.9437 (4.9976)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (122561.3405)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2234  max mem: 15572
Epoch: [1]  [1680/2809]  eta: 0:11:10  lr: 0.000015  min_lr: 0.000000  loss: 4.9766 (4.9976)  class_acc: 0.0000 (0.0181)  loss_scale: 65536.0000 (122222.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1485  max mem: 15572
Epoch: [1]  [1690/2809]  eta: 0:11:04  lr: 0.000015  min_lr: 0.000000  loss: 4.9701 (4.9973)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (121886.8835)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1577  max mem: 15572
Epoch: [1]  [1700/2809]  eta: 0:10:58  lr: 0.000015  min_lr: 0.000000  loss: 4.9874 (4.9973)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (121555.6026)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1416  max mem: 15572
Epoch: [1]  [1710/2809]  eta: 0:10:51  lr: 0.000015  min_lr: 0.000000  loss: 4.9949 (4.9970)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (121228.1940)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0632  max mem: 15572
Epoch: [1]  [1720/2809]  eta: 0:10:45  lr: 0.000015  min_lr: 0.000000  loss: 4.9816 (4.9967)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (120904.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1109  max mem: 15572
Epoch: [1]  [1730/2809]  eta: 0:10:40  lr: 0.000015  min_lr: 0.000000  loss: 4.9203 (4.9964)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (120584.7256)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1694  max mem: 15572
Epoch: [1]  [1740/2809]  eta: 0:10:34  lr: 0.000015  min_lr: 0.000000  loss: 4.9584 (4.9962)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (120268.5353)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1614  max mem: 15572
Epoch: [1]  [1750/2809]  eta: 0:10:27  lr: 0.000015  min_lr: 0.000000  loss: 4.9867 (4.9959)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (119955.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1167  max mem: 15572
[2025-01-15 15:08:51,447] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:08:51,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [1760/2809]  eta: 0:10:21  lr: 0.000015  min_lr: 0.000000  loss: 4.9875 (4.9961)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (119944.6496)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0861  max mem: 15572
Epoch: [1]  [1770/2809]  eta: 0:10:15  lr: 0.000015  min_lr: 0.000000  loss: 5.0192 (4.9961)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (120007.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5132  data: 0.0603  max mem: 15572
[2025-01-15 15:09:02,336] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4584
[2025-01-15 15:09:02,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:09:02,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1780/2809]  eta: 0:10:08  lr: 0.000015  min_lr: 0.000000  loss: 4.9692 (4.9960)  class_acc: 0.0000 (0.0183)  loss_scale: 131072.0000 (119848.8220)  weight_decay: 0.0500 (0.0500)  time: 0.4929  data: 0.0427  max mem: 15572
Epoch: [1]  [1790/2809]  eta: 0:10:02  lr: 0.000015  min_lr: 0.000000  loss: 4.9770 (4.9960)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (119545.5678)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0972  max mem: 15572
Epoch: [1]  [1800/2809]  eta: 0:09:57  lr: 0.000015  min_lr: 0.000000  loss: 4.9557 (4.9958)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (119245.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1334  max mem: 15572
Epoch: [1]  [1810/2809]  eta: 0:09:50  lr: 0.000015  min_lr: 0.000000  loss: 4.9514 (4.9957)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (118949.1066)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.0981  max mem: 15572
Epoch: [1]  [1820/2809]  eta: 0:09:44  lr: 0.000015  min_lr: 0.000000  loss: 4.9958 (4.9961)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (118655.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0969  max mem: 15572
Epoch: [1]  [1830/2809]  eta: 0:09:38  lr: 0.000015  min_lr: 0.000000  loss: 5.0412 (4.9962)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (118365.6756)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0994  max mem: 15572
Epoch: [1]  [1840/2809]  eta: 0:09:33  lr: 0.000016  min_lr: 0.000000  loss: 5.0040 (4.9961)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (118078.7137)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1627  max mem: 15572
Epoch: [1]  [1850/2809]  eta: 0:09:27  lr: 0.000016  min_lr: 0.000000  loss: 4.9427 (4.9959)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (117794.8525)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.2043  max mem: 15572
Epoch: [1]  [1860/2809]  eta: 0:09:20  lr: 0.000016  min_lr: 0.000000  loss: 4.9491 (4.9961)  class_acc: 0.0000 (0.0182)  loss_scale: 65536.0000 (117514.0419)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1048  max mem: 15572
Epoch: [1]  [1870/2809]  eta: 0:09:15  lr: 0.000016  min_lr: 0.000000  loss: 5.0059 (4.9959)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (117236.2330)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1202  max mem: 15572
Epoch: [1]  [1880/2809]  eta: 0:09:09  lr: 0.000016  min_lr: 0.000000  loss: 4.9893 (4.9960)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (116961.3780)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1995  max mem: 15572
Epoch: [1]  [1890/2809]  eta: 0:09:03  lr: 0.000016  min_lr: 0.000000  loss: 4.9609 (4.9959)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (116689.4299)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1661  max mem: 15572
Epoch: [1]  [1900/2809]  eta: 0:08:57  lr: 0.000016  min_lr: 0.000000  loss: 4.9565 (4.9959)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (116420.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0946  max mem: 15572
[2025-01-15 15:10:17,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:10:17,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:10:18,643] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4715
[2025-01-15 15:10:18,644] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:10:18,644] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [1910/2809]  eta: 0:08:51  lr: 0.000016  min_lr: 0.000000  loss: 5.0075 (4.9960)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (116222.6604)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1186  max mem: 15572
Epoch: [1]  [1920/2809]  eta: 0:08:45  lr: 0.000016  min_lr: 0.000000  loss: 4.9994 (4.9959)  class_acc: 0.0000 (0.0183)  loss_scale: 65536.0000 (115958.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1429  max mem: 15572
Epoch: [1]  [1930/2809]  eta: 0:08:40  lr: 0.000016  min_lr: 0.000000  loss: 5.0252 (4.9960)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (115697.6820)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1778  max mem: 15572
Epoch: [1]  [1940/2809]  eta: 0:08:34  lr: 0.000016  min_lr: 0.000000  loss: 5.0145 (4.9959)  class_acc: 0.0000 (0.0184)  loss_scale: 65536.0000 (115439.2499)  weight_decay: 0.0500 (0.0500)  time: 0.7052  data: 0.2506  max mem: 15572
Epoch: [1]  [1950/2809]  eta: 0:08:28  lr: 0.000016  min_lr: 0.000000  loss: 4.9637 (4.9954)  class_acc: 0.0000 (0.0186)  loss_scale: 65536.0000 (115183.4669)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.1886  max mem: 15572
Epoch: [1]  [1960/2809]  eta: 0:08:22  lr: 0.000016  min_lr: 0.000000  loss: 4.9788 (4.9954)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (114930.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1427  max mem: 15572
Epoch: [1]  [1970/2809]  eta: 0:08:17  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9951)  class_acc: 0.0000 (0.0185)  loss_scale: 65536.0000 (114679.6875)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.2044  max mem: 15572
Epoch: [1]  [1980/2809]  eta: 0:08:11  lr: 0.000016  min_lr: 0.000000  loss: 4.9705 (4.9952)  class_acc: 0.0000 (0.0187)  loss_scale: 65536.0000 (114431.6123)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1426  max mem: 15572
Epoch: [1]  [1990/2809]  eta: 0:08:05  lr: 0.000016  min_lr: 0.000000  loss: 5.0016 (4.9951)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (114186.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1075  max mem: 15572
Epoch: [1]  [2000/2809]  eta: 0:07:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9506 (4.9949)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (113942.9005)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1631  max mem: 15572
Epoch: [1]  [2010/2809]  eta: 0:07:53  lr: 0.000016  min_lr: 0.000000  loss: 4.9546 (4.9950)  class_acc: 0.0000 (0.0188)  loss_scale: 65536.0000 (113702.1900)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1286  max mem: 15572
Epoch: [1]  [2020/2809]  eta: 0:07:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  class_acc: 0.0000 (0.0189)  loss_scale: 65536.0000 (113463.8615)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1589  max mem: 15572
Epoch: [1]  [2030/2809]  eta: 0:07:41  lr: 0.000016  min_lr: 0.000000  loss: 4.9621 (4.9949)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (113227.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.2289  max mem: 15572
[2025-01-15 15:11:38,399] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:11:38,400] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:11:40,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4848
[2025-01-15 15:11:40,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:11:40,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2040/2809]  eta: 0:07:35  lr: 0.000016  min_lr: 0.000000  loss: 4.9642 (4.9946)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (113122.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1889  max mem: 15572
Epoch: [1]  [2050/2809]  eta: 0:07:29  lr: 0.000016  min_lr: 0.000000  loss: 4.9642 (4.9948)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (112890.6329)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1126  max mem: 15572
Epoch: [1]  [2060/2809]  eta: 0:07:23  lr: 0.000016  min_lr: 0.000000  loss: 4.9646 (4.9947)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (112660.8675)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1314  max mem: 15572
Epoch: [1]  [2070/2809]  eta: 0:07:17  lr: 0.000016  min_lr: 0.000000  loss: 4.9925 (4.9947)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (112433.3211)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1121  max mem: 15572
[2025-01-15 15:12:00,479] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4882
[2025-01-15 15:12:00,479] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:12:00,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2080/2809]  eta: 0:07:11  lr: 0.000016  min_lr: 0.000000  loss: 5.0045 (4.9946)  class_acc: 0.0000 (0.0190)  loss_scale: 65536.0000 (112081.9914)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.0830  max mem: 15572
Epoch: [1]  [2090/2809]  eta: 0:07:05  lr: 0.000016  min_lr: 0.000000  loss: 4.9630 (4.9945)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (111702.6801)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1330  max mem: 15572
Epoch: [1]  [2100/2809]  eta: 0:06:59  lr: 0.000016  min_lr: 0.000000  loss: 4.9747 (4.9946)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (111326.9795)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0997  max mem: 15572
Epoch: [1]  [2110/2809]  eta: 0:06:53  lr: 0.000016  min_lr: 0.000000  loss: 5.0337 (4.9947)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (110954.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0874  max mem: 15572
Epoch: [1]  [2120/2809]  eta: 0:06:47  lr: 0.000016  min_lr: 0.000000  loss: 4.9544 (4.9945)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (110586.2065)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1822  max mem: 15572
Epoch: [1]  [2130/2809]  eta: 0:06:42  lr: 0.000016  min_lr: 0.000000  loss: 4.9653 (4.9948)  class_acc: 0.0000 (0.0190)  loss_scale: 32768.0000 (110221.0343)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.2216  max mem: 15572
Epoch: [1]  [2140/2809]  eta: 0:06:36  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9946)  class_acc: 0.0000 (0.0191)  loss_scale: 32768.0000 (109859.2732)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1473  max mem: 15572
Epoch: [1]  [2150/2809]  eta: 0:06:30  lr: 0.000017  min_lr: 0.000000  loss: 4.9736 (4.9947)  class_acc: 0.0000 (0.0191)  loss_scale: 32768.0000 (109500.8759)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1621  max mem: 15572
Epoch: [1]  [2160/2809]  eta: 0:06:24  lr: 0.000017  min_lr: 0.000000  loss: 5.0416 (4.9950)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (109145.7955)  weight_decay: 0.0500 (0.0500)  time: 0.6693  data: 0.2428  max mem: 15572
Epoch: [1]  [2170/2809]  eta: 0:06:18  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (108793.9862)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.2051  max mem: 15572
Epoch: [1]  [2180/2809]  eta: 0:06:12  lr: 0.000017  min_lr: 0.000000  loss: 4.9807 (4.9949)  class_acc: 0.0000 (0.0191)  loss_scale: 32768.0000 (108445.4030)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1521  max mem: 15572
[2025-01-15 15:13:10,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.6166206310009812e-07, 1.6166206310009812e-07, 2.3094580442871163e-07, 2.3094580442871163e-07, 3.2992257775530236e-07, 3.2992257775530236e-07, 4.7131796822186054e-07, 4.7131796822186054e-07, 6.733113831740865e-07, 6.733113831740865e-07, 9.618734045344093e-07, 9.618734045344093e-07, 1.3741048636205847e-06, 1.3741048636205847e-06, 1.963006948029407e-06, 1.963006948029407e-06, 2.80429564004201e-06, 2.80429564004201e-06, 4.0061366286314435e-06, 4.0061366286314435e-06, 5.723052326616347e-06, 5.723052326616347e-06, 8.175789038023355e-06, 8.175789038023355e-06, 1.167969862574765e-05, 1.167969862574765e-05, 1.6685283751068073e-05, 1.6685283751068073e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:13:10,635] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=28.546547511097764, CurrSamplesPerSec=19.701596156729227, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [1]  [2190/2809]  eta: 0:06:07  lr: 0.000017  min_lr: 0.000000  loss: 5.0170 (4.9950)  class_acc: 0.0000 (0.0192)  loss_scale: 32768.0000 (108100.0018)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1589  max mem: 15572
Epoch: [1]  [2200/2809]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 4.9662 (4.9948)  class_acc: 0.0000 (0.0193)  loss_scale: 32768.0000 (107757.7392)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1151  max mem: 15572
[2025-01-15 15:13:16,844] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:13:16,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2210/2809]  eta: 0:05:55  lr: 0.000017  min_lr: 0.000000  loss: 4.9382 (4.9944)  class_acc: 0.0000 (0.0194)  loss_scale: 32768.0000 (107551.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1072  max mem: 15572
Epoch: [1]  [2220/2809]  eta: 0:05:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9111 (4.9942)  class_acc: 0.0000 (0.0195)  loss_scale: 65536.0000 (107362.7807)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1171  max mem: 15572
Epoch: [1]  [2230/2809]  eta: 0:05:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9940)  class_acc: 0.0000 (0.0196)  loss_scale: 65536.0000 (107175.3008)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1208  max mem: 15572
[2025-01-15 15:13:40,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5049
[2025-01-15 15:13:40,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:13:40,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [1]  [2240/2809]  eta: 0:05:37  lr: 0.000017  min_lr: 0.000000  loss: 4.9521 (4.9937)  class_acc: 0.0417 (0.0197)  loss_scale: 65536.0000 (106974.8719)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1563  max mem: 15572
Epoch: [1]  [2250/2809]  eta: 0:05:31  lr: 0.000017  min_lr: 0.000000  loss: 4.9245 (4.9934)  class_acc: 0.0000 (0.0197)  loss_scale: 32768.0000 (106645.2101)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1783  max mem: 15572
Epoch: [1]  [2260/2809]  eta: 0:05:25  lr: 0.000017  min_lr: 0.000000  loss: 4.9285 (4.9934)  class_acc: 0.0000 (0.0197)  loss_scale: 32768.0000 (106318.4644)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1139  max mem: 15572
Epoch: [1]  [2270/2809]  eta: 0:05:19  lr: 0.000017  min_lr: 0.000000  loss: 4.9067 (4.9930)  class_acc: 0.0000 (0.0198)  loss_scale: 32768.0000 (105994.5962)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0949  max mem: 15572
Epoch: [1]  [2280/2809]  eta: 0:05:13  lr: 0.000017  min_lr: 0.000000  loss: 4.9214 (4.9929)  class_acc: 0.0000 (0.0199)  loss_scale: 32768.0000 (105673.5677)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1590  max mem: 15572
Epoch: [1]  [2290/2809]  eta: 0:05:07  lr: 0.000017  min_lr: 0.000000  loss: 4.9410 (4.9926)  class_acc: 0.0417 (0.0201)  loss_scale: 32768.0000 (105355.3418)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1243  max mem: 15572
Epoch: [1]  [2300/2809]  eta: 0:05:01  lr: 0.000017  min_lr: 0.000000  loss: 4.9860 (4.9926)  class_acc: 0.0000 (0.0200)  loss_scale: 32768.0000 (105039.8818)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1307  max mem: 15572
Epoch: [1]  [2310/2809]  eta: 0:04:55  lr: 0.000017  min_lr: 0.000000  loss: 5.0122 (4.9924)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (104727.1519)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1209  max mem: 15572
Epoch: [1]  [2320/2809]  eta: 0:04:49  lr: 0.000017  min_lr: 0.000000  loss: 4.9262 (4.9921)  class_acc: 0.0417 (0.0203)  loss_scale: 32768.0000 (104417.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0978  max mem: 15572
Epoch: [1]  [2330/2809]  eta: 0:04:43  lr: 0.000017  min_lr: 0.000000  loss: 4.9459 (4.9923)  class_acc: 0.0000 (0.0203)  loss_scale: 32768.0000 (104109.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1842  max mem: 15572
Epoch: [1]  [2340/2809]  eta: 0:04:38  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9920)  class_acc: 0.0000 (0.0203)  loss_scale: 32768.0000 (103804.9927)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.2359  max mem: 15572
Epoch: [1]  [2350/2809]  eta: 0:04:32  lr: 0.000017  min_lr: 0.000000  loss: 4.9495 (4.9922)  class_acc: 0.0000 (0.0202)  loss_scale: 32768.0000 (103502.8362)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1592  max mem: 15572
Epoch: [1]  [2360/2809]  eta: 0:04:26  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9921)  class_acc: 0.0000 (0.0203)  loss_scale: 32768.0000 (103203.2393)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1761  max mem: 15572
[2025-01-15 15:14:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:14:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [1]  [2370/2809]  eta: 0:04:20  lr: 0.000017  min_lr: 0.000000  loss: 4.9719 (4.9920)  class_acc: 0.0000 (0.0204)  loss_scale: 32768.0000 (102933.8102)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1630  max mem: 15572
Epoch: [1]  [2380/2809]  eta: 0:04:14  lr: 0.000017  min_lr: 0.000000  loss: 4.8816 (4.9915)  class_acc: 0.0417 (0.0205)  loss_scale: 65536.0000 (102776.7425)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1171  max mem: 15572
Epoch: [1]  [2390/2809]  eta: 0:04:08  lr: 0.000017  min_lr: 0.000000  loss: 4.8637 (4.9910)  class_acc: 0.0000 (0.0207)  loss_scale: 65536.0000 (102620.9887)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1189  max mem: 15572
Epoch: [1]  [2400/2809]  eta: 0:04:02  lr: 0.000017  min_lr: 0.000000  loss: 4.8924 (4.9906)  class_acc: 0.0000 (0.0208)  loss_scale: 65536.0000 (102466.5323)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0950  max mem: 15572
Epoch: [1]  [2410/2809]  eta: 0:03:56  lr: 0.000017  min_lr: 0.000000  loss: 4.9439 (4.9907)  class_acc: 0.0000 (0.0207)  loss_scale: 65536.0000 (102313.3571)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0675  max mem: 15572
Epoch: [1]  [2420/2809]  eta: 0:03:50  lr: 0.000017  min_lr: 0.000000  loss: 4.9617 (4.9906)  class_acc: 0.0000 (0.0207)  loss_scale: 65536.0000 (102161.4473)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0911  max mem: 15572
Epoch: [1]  [2430/2809]  eta: 0:03:44  lr: 0.000017  min_lr: 0.000000  loss: 4.9517 (4.9906)  class_acc: 0.0000 (0.0208)  loss_scale: 65536.0000 (102010.7873)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.2084  max mem: 15572
Epoch: [1]  [2440/2809]  eta: 0:03:38  lr: 0.000018  min_lr: 0.000000  loss: 4.9347 (4.9903)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (101861.3617)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.2484  max mem: 15572
Epoch: [1]  [2450/2809]  eta: 0:03:32  lr: 0.000018  min_lr: 0.000000  loss: 4.9592 (4.9904)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (101713.1554)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1964  max mem: 15572
Epoch: [1]  [2460/2809]  eta: 0:03:27  lr: 0.000018  min_lr: 0.000000  loss: 5.0032 (4.9903)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (101566.1536)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1444  max mem: 15572
Epoch: [1]  [2470/2809]  eta: 0:03:20  lr: 0.000018  min_lr: 0.000000  loss: 4.9282 (4.9901)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (101420.3416)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0692  max mem: 15572
Epoch: [1]  [2480/2809]  eta: 0:03:14  lr: 0.000018  min_lr: 0.000000  loss: 4.9123 (4.9899)  class_acc: 0.0000 (0.0209)  loss_scale: 65536.0000 (101275.7050)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0603  max mem: 15572
Epoch: [1]  [2490/2809]  eta: 0:03:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9231 (4.9896)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (101132.2296)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1659  max mem: 15572
[2025-01-15 15:16:13,204] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:16:13,205] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2500/2809]  eta: 0:03:03  lr: 0.000018  min_lr: 0.000000  loss: 4.9236 (4.9896)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (101094.7173)  weight_decay: 0.0500 (0.0500)  time: 0.6688  data: 0.2204  max mem: 15572
[2025-01-15 15:16:17,731] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5313
[2025-01-15 15:16:17,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:16:17,732] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2510/2809]  eta: 0:02:57  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9895)  class_acc: 0.0000 (0.0210)  loss_scale: 65536.0000 (101031.4042)  weight_decay: 0.0500 (0.0500)  time: 0.6467  data: 0.1857  max mem: 15572
Epoch: [1]  [2520/2809]  eta: 0:02:51  lr: 0.000018  min_lr: 0.000000  loss: 4.9559 (4.9893)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (100890.6053)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1429  max mem: 15572
Epoch: [1]  [2530/2809]  eta: 0:02:45  lr: 0.000018  min_lr: 0.000000  loss: 4.9441 (4.9892)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (100750.9190)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1073  max mem: 15572
Epoch: [1]  [2540/2809]  eta: 0:02:39  lr: 0.000018  min_lr: 0.000000  loss: 5.0081 (4.9893)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (100612.3322)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1001  max mem: 15572
Epoch: [1]  [2550/2809]  eta: 0:02:33  lr: 0.000018  min_lr: 0.000000  loss: 4.9991 (4.9892)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (100474.8318)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1601  max mem: 15572
Epoch: [1]  [2560/2809]  eta: 0:02:27  lr: 0.000018  min_lr: 0.000000  loss: 4.9627 (4.9891)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (100338.4053)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1143  max mem: 15572
Epoch: [1]  [2570/2809]  eta: 0:02:21  lr: 0.000018  min_lr: 0.000000  loss: 4.9747 (4.9889)  class_acc: 0.0000 (0.0212)  loss_scale: 65536.0000 (100203.0401)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1342  max mem: 15572
Epoch: [1]  [2580/2809]  eta: 0:02:15  lr: 0.000018  min_lr: 0.000000  loss: 4.9201 (4.9886)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (100068.7238)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.2290  max mem: 15572
Epoch: [1]  [2590/2809]  eta: 0:02:09  lr: 0.000018  min_lr: 0.000000  loss: 4.9467 (4.9890)  class_acc: 0.0000 (0.0213)  loss_scale: 65536.0000 (99935.4442)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1928  max mem: 15572
Epoch: [1]  [2600/2809]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 4.9588 (4.9888)  class_acc: 0.0000 (0.0214)  loss_scale: 65536.0000 (99803.1895)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1439  max mem: 15572
Epoch: [1]  [2610/2809]  eta: 0:01:58  lr: 0.000018  min_lr: 0.000000  loss: 4.9403 (4.9887)  class_acc: 0.0000 (0.0215)  loss_scale: 65536.0000 (99671.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0871  max mem: 15572
Epoch: [1]  [2620/2809]  eta: 0:01:52  lr: 0.000018  min_lr: 0.000000  loss: 4.9255 (4.9884)  class_acc: 0.0417 (0.0216)  loss_scale: 65536.0000 (99541.7077)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1038  max mem: 15572
Epoch: [1]  [2630/2809]  eta: 0:01:46  lr: 0.000018  min_lr: 0.000000  loss: 4.8699 (4.9881)  class_acc: 0.0000 (0.0216)  loss_scale: 65536.0000 (99412.4576)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1676  max mem: 15572
[2025-01-15 15:17:33,699] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:17:33,699] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2640/2809]  eta: 0:01:40  lr: 0.000018  min_lr: 0.000000  loss: 4.9296 (4.9879)  class_acc: 0.0000 (0.0216)  loss_scale: 65536.0000 (99482.7050)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1218  max mem: 15572
[2025-01-15 15:17:40,737] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5456
[2025-01-15 15:17:40,737] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:17:40,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2650/2809]  eta: 0:01:34  lr: 0.000018  min_lr: 0.000000  loss: 4.9296 (4.9876)  class_acc: 0.0000 (0.0217)  loss_scale: 131072.0000 (99502.9800)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0623  max mem: 15572
Epoch: [1]  [2660/2809]  eta: 0:01:28  lr: 0.000018  min_lr: 0.000000  loss: 4.9022 (4.9874)  class_acc: 0.0000 (0.0217)  loss_scale: 65536.0000 (99375.3326)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1176  max mem: 15572
Epoch: [1]  [2670/2809]  eta: 0:01:22  lr: 0.000018  min_lr: 0.000000  loss: 4.9022 (4.9871)  class_acc: 0.0417 (0.0218)  loss_scale: 65536.0000 (99248.6410)  weight_decay: 0.0500 (0.0500)  time: 0.6683  data: 0.2191  max mem: 15572
Epoch: [1]  [2680/2809]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 4.9349 (4.9870)  class_acc: 0.0417 (0.0219)  loss_scale: 65536.0000 (99122.8944)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1805  max mem: 15572
Epoch: [1]  [2690/2809]  eta: 0:01:10  lr: 0.000018  min_lr: 0.000000  loss: 4.9522 (4.9869)  class_acc: 0.0000 (0.0219)  loss_scale: 65536.0000 (98998.0825)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1617  max mem: 15572
Epoch: [1]  [2700/2809]  eta: 0:01:04  lr: 0.000018  min_lr: 0.000000  loss: 4.9121 (4.9864)  class_acc: 0.0000 (0.0220)  loss_scale: 65536.0000 (98874.1947)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.2091  max mem: 15572
Epoch: [1]  [2710/2809]  eta: 0:00:58  lr: 0.000018  min_lr: 0.000000  loss: 4.9121 (4.9862)  class_acc: 0.0417 (0.0221)  loss_scale: 65536.0000 (98751.2210)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1218  max mem: 15572
Epoch: [1]  [2720/2809]  eta: 0:00:52  lr: 0.000018  min_lr: 0.000000  loss: 4.8782 (4.9858)  class_acc: 0.0417 (0.0221)  loss_scale: 65536.0000 (98629.1510)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0936  max mem: 15572
Epoch: [1]  [2730/2809]  eta: 0:00:46  lr: 0.000018  min_lr: 0.000000  loss: 4.8772 (4.9857)  class_acc: 0.0000 (0.0222)  loss_scale: 65536.0000 (98507.9751)  weight_decay: 0.0500 (0.0500)  time: 0.6554  data: 0.2258  max mem: 15572
Epoch: [1]  [2740/2809]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 4.9236 (4.9855)  class_acc: 0.0417 (0.0223)  loss_scale: 65536.0000 (98387.6833)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1905  max mem: 15572
Epoch: [1]  [2750/2809]  eta: 0:00:34  lr: 0.000019  min_lr: 0.000000  loss: 4.9309 (4.9854)  class_acc: 0.0417 (0.0224)  loss_scale: 65536.0000 (98268.2661)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1074  max mem: 15572
Epoch: [1]  [2760/2809]  eta: 0:00:29  lr: 0.000019  min_lr: 0.000000  loss: 4.8917 (4.9851)  class_acc: 0.0417 (0.0225)  loss_scale: 65536.0000 (98149.7139)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1698  max mem: 15572
Epoch: [1]  [2770/2809]  eta: 0:00:23  lr: 0.000019  min_lr: 0.000000  loss: 4.8864 (4.9849)  class_acc: 0.0000 (0.0226)  loss_scale: 65536.0000 (98032.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1298  max mem: 15572
[2025-01-15 15:18:58,406] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:18:58,406] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [1]  [2780/2809]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 4.9066 (4.9845)  class_acc: 0.0417 (0.0227)  loss_scale: 65536.0000 (98032.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0797  max mem: 15572
Epoch: [1]  [2790/2809]  eta: 0:00:11  lr: 0.000019  min_lr: 0.000000  loss: 4.9092 (4.9845)  class_acc: 0.0000 (0.0227)  loss_scale: 131072.0000 (98151.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1428  max mem: 15572
[2025-01-15 15:19:10,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5606
[2025-01-15 15:19:10,582] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:19:10,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [1]  [2800/2809]  eta: 0:00:05  lr: 0.000019  min_lr: 0.000000  loss: 4.9303 (4.9843)  class_acc: 0.0000 (0.0227)  loss_scale: 131072.0000 (98175.3145)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1547  max mem: 15572
Epoch: [1]  [2808/2809]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  class_acc: 0.0000 (0.0228)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)  time: 0.4725  data: 0.0748  max mem: 15572
Epoch: [1] Total time: 0:27:44 (0.5927 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 4.9376 (4.9843)  class_acc: 0.0000 (0.0228)  loss_scale: 65536.0000 (98082.3581)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:11:12  loss: 5.0870 (5.0870)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 2.4728  data: 2.3100  max mem: 15572
Val:  [ 10/272]  eta: 0:02:05  loss: 5.2079 (5.0101)  acc1: 0.0000 (2.5253)  acc5: 0.0000 (18.1818)  time: 0.4772  data: 0.3142  max mem: 15572
Val:  [ 20/272]  eta: 0:01:22  loss: 5.0015 (4.9184)  acc1: 0.0000 (3.4392)  acc5: 0.0000 (18.2540)  time: 0.2210  data: 0.0575  max mem: 15572
Val:  [ 30/272]  eta: 0:01:08  loss: 4.9735 (4.8957)  acc1: 0.0000 (2.3297)  acc5: 0.0000 (17.9211)  time: 0.1752  data: 0.0005  max mem: 15572
Val:  [ 40/272]  eta: 0:01:05  loss: 4.5089 (4.7925)  acc1: 0.0000 (6.7751)  acc5: 27.7778 (28.7263)  time: 0.2333  data: 0.0364  max mem: 15572
Val:  [ 50/272]  eta: 0:01:03  loss: 4.6250 (4.8251)  acc1: 0.0000 (5.4466)  acc5: 0.0000 (23.0937)  time: 0.2970  data: 0.0964  max mem: 15572
Val:  [ 60/272]  eta: 0:01:03  loss: 4.7122 (4.8287)  acc1: 0.0000 (4.5537)  acc5: 0.0000 (19.3989)  time: 0.3427  data: 0.1432  max mem: 15572
Val:  [ 70/272]  eta: 0:01:02  loss: 4.7806 (4.8047)  acc1: 0.0000 (6.5728)  acc5: 0.0000 (20.7355)  time: 0.3677  data: 0.1640  max mem: 15572
Val:  [ 80/272]  eta: 0:01:01  loss: 4.9269 (4.7836)  acc1: 0.0000 (6.3100)  acc5: 0.0000 (22.2222)  time: 0.3721  data: 0.1562  max mem: 15572
Val:  [ 90/272]  eta: 0:00:59  loss: 4.9622 (4.8199)  acc1: 0.0000 (5.6166)  acc5: 0.0000 (19.7802)  time: 0.3844  data: 0.1617  max mem: 15572
Val:  [100/272]  eta: 0:00:57  loss: 4.9603 (4.8372)  acc1: 0.0000 (5.0605)  acc5: 0.0000 (19.4169)  time: 0.4048  data: 0.1915  max mem: 15572
Val:  [110/272]  eta: 0:00:54  loss: 4.8600 (4.8606)  acc1: 0.0000 (4.6046)  acc5: 0.0000 (17.6677)  time: 0.3886  data: 0.1791  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 5.0471 (4.8823)  acc1: 0.0000 (4.2241)  acc5: 0.0000 (16.2075)  time: 0.3568  data: 0.1483  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 4.9312 (4.8661)  acc1: 0.0000 (5.5980)  acc5: 0.0000 (18.3206)  time: 0.3896  data: 0.1800  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 4.6675 (4.8551)  acc1: 0.0000 (5.3586)  acc5: 0.0000 (18.9125)  time: 0.3969  data: 0.1809  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.7878 (4.8553)  acc1: 0.0000 (5.0037)  acc5: 0.0000 (17.6600)  time: 0.3823  data: 0.1642  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 4.5775 (4.8342)  acc1: 0.0000 (5.4865)  acc5: 0.0000 (18.9096)  time: 0.3805  data: 0.1722  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.6402 (4.8488)  acc1: 0.0000 (5.2632)  acc5: 0.0000 (17.9987)  time: 0.3645  data: 0.1547  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 4.9416 (4.8517)  acc1: 0.0000 (4.9724)  acc5: 0.0000 (17.0043)  time: 0.3592  data: 0.1527  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.9375 (4.8615)  acc1: 0.0000 (4.7120)  acc5: 0.0000 (16.1140)  time: 0.3530  data: 0.1543  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.9900 (4.8737)  acc1: 0.0000 (4.4776)  acc5: 0.0000 (15.3123)  time: 0.3645  data: 0.1732  max mem: 15572
Val:  [210/272]  eta: 0:00:22  loss: 5.0091 (4.8826)  acc1: 0.0000 (4.2654)  acc5: 0.0000 (14.6130)  time: 0.3925  data: 0.2052  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.9151 (4.8769)  acc1: 0.0000 (4.0724)  acc5: 0.0000 (14.0774)  time: 0.3877  data: 0.1878  max mem: 15572
Val:  [230/272]  eta: 0:00:15  loss: 4.7648 (4.8730)  acc1: 0.0000 (3.8961)  acc5: 0.0000 (13.4680)  time: 0.3909  data: 0.1968  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.7689 (4.8746)  acc1: 0.0000 (3.7344)  acc5: 0.0000 (13.2319)  time: 0.3863  data: 0.1902  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 5.1456 (4.8933)  acc1: 0.0000 (3.5857)  acc5: 0.0000 (12.7047)  time: 0.3626  data: 0.1430  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.8585 (4.8860)  acc1: 0.0000 (3.4483)  acc5: 0.0000 (12.3883)  time: 0.3795  data: 0.1684  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 4.7574 (4.8861)  acc1: 0.0000 (3.3210)  acc5: 0.0000 (12.6486)  time: 0.2800  data: 0.1040  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 4.7574 (4.8885)  acc1: 0.0000 (3.3176)  acc5: 0.0000 (12.6357)  time: 0.2728  data: 0.1040  max mem: 15572
Val: Total time: 0:01:36 (0.3541 s / it)
* Acc@1 3.318 Acc@5 12.636 loss 4.889
Accuracy of the network on the 4883 val videos: 3.3%
[2025-01-15 15:20:51,557] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 15:20:51,562] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 15:20:51,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 15:20:54,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 15:20:54,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 3.32%
Epoch: [2]  [   0/2809]  eta: 2:36:23  lr: 0.000019  min_lr: 0.000000  loss: 4.9338 (4.9338)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.3405  data: 2.9411  max mem: 15572
Epoch: [2]  [  10/2809]  eta: 0:39:53  lr: 0.000019  min_lr: 0.000000  loss: 4.9616 (4.9631)  class_acc: 0.0417 (0.0341)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8549  data: 0.4438  max mem: 15572
Epoch: [2]  [  20/2809]  eta: 0:30:34  lr: 0.000019  min_lr: 0.000000  loss: 4.9593 (4.9462)  class_acc: 0.0000 (0.0357)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0973  max mem: 15572
Epoch: [2]  [  30/2809]  eta: 0:26:45  lr: 0.000019  min_lr: 0.000000  loss: 4.8483 (4.9134)  class_acc: 0.0000 (0.0390)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4252  data: 0.0005  max mem: 15572
Epoch: [2]  [  40/2809]  eta: 0:26:41  lr: 0.000019  min_lr: 0.000000  loss: 4.8770 (4.9294)  class_acc: 0.0000 (0.0376)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0826  max mem: 15572
Epoch: [2]  [  50/2809]  eta: 0:28:01  lr: 0.000019  min_lr: 0.000000  loss: 4.9402 (4.9242)  class_acc: 0.0000 (0.0368)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.2213  max mem: 15572
Epoch: [2]  [  60/2809]  eta: 0:27:44  lr: 0.000019  min_lr: 0.000000  loss: 4.9353 (4.9343)  class_acc: 0.0000 (0.0348)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.2114  max mem: 15572
Epoch: [2]  [  70/2809]  eta: 0:27:57  lr: 0.000019  min_lr: 0.000000  loss: 4.9544 (4.9414)  class_acc: 0.0000 (0.0352)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1741  max mem: 15572
Epoch: [2]  [  80/2809]  eta: 0:28:14  lr: 0.000019  min_lr: 0.000000  loss: 4.9544 (4.9471)  class_acc: 0.0417 (0.0360)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.2200  max mem: 15572
Epoch: [2]  [  90/2809]  eta: 0:28:08  lr: 0.000019  min_lr: 0.000000  loss: 4.9106 (4.9414)  class_acc: 0.0417 (0.0357)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1998  max mem: 15572
Epoch: [2]  [ 100/2809]  eta: 0:27:33  lr: 0.000019  min_lr: 0.000000  loss: 4.8750 (4.9395)  class_acc: 0.0417 (0.0371)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1147  max mem: 15572
Epoch: [2]  [ 110/2809]  eta: 0:27:19  lr: 0.000019  min_lr: 0.000000  loss: 4.9562 (4.9419)  class_acc: 0.0000 (0.0353)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0860  max mem: 15572
[2025-01-15 15:22:06,873] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:22:06,874] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 120/2809]  eta: 0:27:16  lr: 0.000019  min_lr: 0.000000  loss: 4.9713 (4.9399)  class_acc: 0.0000 (0.0355)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1225  max mem: 15572
Epoch: [2]  [ 130/2809]  eta: 0:26:54  lr: 0.000019  min_lr: 0.000000  loss: 4.9299 (4.9352)  class_acc: 0.0417 (0.0359)  loss_scale: 131072.0000 (72539.8473)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1160  max mem: 15572
[2025-01-15 15:22:16,352] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5750
[2025-01-15 15:22:16,352] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:22:16,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 140/2809]  eta: 0:26:45  lr: 0.000019  min_lr: 0.000000  loss: 4.9299 (4.9407)  class_acc: 0.0000 (0.0372)  loss_scale: 131072.0000 (72507.9149)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1315  max mem: 15572
Epoch: [2]  [ 150/2809]  eta: 0:26:26  lr: 0.000019  min_lr: 0.000000  loss: 4.9178 (4.9358)  class_acc: 0.0000 (0.0367)  loss_scale: 65536.0000 (72046.1987)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1422  max mem: 15572
Epoch: [2]  [ 160/2809]  eta: 0:26:19  lr: 0.000019  min_lr: 0.000000  loss: 4.9212 (4.9362)  class_acc: 0.0000 (0.0378)  loss_scale: 65536.0000 (71641.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1414  max mem: 15572
Epoch: [2]  [ 170/2809]  eta: 0:26:12  lr: 0.000019  min_lr: 0.000000  loss: 4.9570 (4.9349)  class_acc: 0.0000 (0.0373)  loss_scale: 65536.0000 (71284.7719)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1694  max mem: 15572
Epoch: [2]  [ 180/2809]  eta: 0:25:52  lr: 0.000019  min_lr: 0.000000  loss: 4.8715 (4.9342)  class_acc: 0.0000 (0.0373)  loss_scale: 65536.0000 (70967.1602)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1170  max mem: 15572
Epoch: [2]  [ 190/2809]  eta: 0:25:32  lr: 0.000019  min_lr: 0.000000  loss: 4.8898 (4.9311)  class_acc: 0.0000 (0.0369)  loss_scale: 65536.0000 (70682.8063)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0578  max mem: 15572
Epoch: [2]  [ 200/2809]  eta: 0:25:28  lr: 0.000019  min_lr: 0.000000  loss: 4.8781 (4.9291)  class_acc: 0.0000 (0.0375)  loss_scale: 65536.0000 (70426.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0936  max mem: 15572
Epoch: [2]  [ 210/2809]  eta: 0:25:22  lr: 0.000019  min_lr: 0.000000  loss: 4.8733 (4.9277)  class_acc: 0.0417 (0.0381)  loss_scale: 65536.0000 (70194.9573)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1196  max mem: 15572
Epoch: [2]  [ 220/2809]  eta: 0:25:20  lr: 0.000019  min_lr: 0.000000  loss: 4.8677 (4.9235)  class_acc: 0.0417 (0.0379)  loss_scale: 65536.0000 (69984.1448)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1331  max mem: 15572
Epoch: [2]  [ 230/2809]  eta: 0:25:21  lr: 0.000020  min_lr: 0.000000  loss: 4.8242 (4.9224)  class_acc: 0.0000 (0.0370)  loss_scale: 65536.0000 (69791.5844)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1896  max mem: 15572
Epoch: [2]  [ 240/2809]  eta: 0:25:07  lr: 0.000020  min_lr: 0.000000  loss: 4.9405 (4.9235)  class_acc: 0.0000 (0.0386)  loss_scale: 65536.0000 (69615.0041)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1432  max mem: 15572
Epoch: [2]  [ 250/2809]  eta: 0:25:02  lr: 0.000020  min_lr: 0.000000  loss: 4.9063 (4.9230)  class_acc: 0.0417 (0.0383)  loss_scale: 65536.0000 (69452.4940)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1053  max mem: 15572
Epoch: [2]  [ 260/2809]  eta: 0:24:57  lr: 0.000020  min_lr: 0.000000  loss: 4.8917 (4.9249)  class_acc: 0.0000 (0.0378)  loss_scale: 65536.0000 (69302.4368)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1536  max mem: 15572
[2025-01-15 15:23:28,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:23:28,900] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 270/2809]  eta: 0:24:48  lr: 0.000020  min_lr: 0.000000  loss: 4.8917 (4.9244)  class_acc: 0.0000 (0.0372)  loss_scale: 65536.0000 (71581.7565)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1408  max mem: 15572
Epoch: [2]  [ 280/2809]  eta: 0:24:43  lr: 0.000020  min_lr: 0.000000  loss: 4.8560 (4.9220)  class_acc: 0.0000 (0.0378)  loss_scale: 131072.0000 (73698.8470)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1395  max mem: 15572
Epoch: [2]  [ 290/2809]  eta: 0:24:33  lr: 0.000020  min_lr: 0.000000  loss: 4.8550 (4.9221)  class_acc: 0.0000 (0.0372)  loss_scale: 131072.0000 (75670.4330)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1266  max mem: 15572
Epoch: [2]  [ 300/2809]  eta: 0:24:33  lr: 0.000020  min_lr: 0.000000  loss: 4.9174 (4.9221)  class_acc: 0.0000 (0.0377)  loss_scale: 131072.0000 (77511.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1510  max mem: 15572
Epoch: [2]  [ 310/2809]  eta: 0:24:31  lr: 0.000020  min_lr: 0.000000  loss: 4.9816 (4.9231)  class_acc: 0.0000 (0.0368)  loss_scale: 131072.0000 (79233.2347)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1891  max mem: 15572
[2025-01-15 15:23:58,668] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5929
[2025-01-15 15:23:58,668] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:23:58,669] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 320/2809]  eta: 0:24:20  lr: 0.000020  min_lr: 0.000000  loss: 4.9057 (4.9222)  class_acc: 0.0000 (0.0362)  loss_scale: 65536.0000 (78806.5296)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1161  max mem: 15572
Epoch: [2]  [ 330/2809]  eta: 0:24:13  lr: 0.000020  min_lr: 0.000000  loss: 4.8717 (4.9210)  class_acc: 0.0000 (0.0355)  loss_scale: 65536.0000 (78405.6073)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0909  max mem: 15572
Epoch: [2]  [ 340/2809]  eta: 0:24:13  lr: 0.000020  min_lr: 0.000000  loss: 4.8488 (4.9198)  class_acc: 0.0000 (0.0349)  loss_scale: 65536.0000 (78028.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1527  max mem: 15572
Epoch: [2]  [ 350/2809]  eta: 0:24:05  lr: 0.000020  min_lr: 0.000000  loss: 4.8466 (4.9197)  class_acc: 0.0000 (0.0345)  loss_scale: 65536.0000 (77672.2963)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1529  max mem: 15572
Epoch: [2]  [ 360/2809]  eta: 0:24:03  lr: 0.000020  min_lr: 0.000000  loss: 4.9179 (4.9196)  class_acc: 0.0000 (0.0347)  loss_scale: 65536.0000 (77336.1108)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1481  max mem: 15572
Epoch: [2]  [ 370/2809]  eta: 0:23:56  lr: 0.000020  min_lr: 0.000000  loss: 4.8992 (4.9177)  class_acc: 0.0417 (0.0359)  loss_scale: 65536.0000 (77018.0485)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1557  max mem: 15572
Epoch: [2]  [ 380/2809]  eta: 0:23:48  lr: 0.000020  min_lr: 0.000000  loss: 4.8927 (4.9172)  class_acc: 0.0417 (0.0359)  loss_scale: 65536.0000 (76716.6824)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1179  max mem: 15572
[2025-01-15 15:24:39,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[1.9400094349619698e-07, 1.9400094349619698e-07, 2.7714420499456714e-07, 2.7714420499456714e-07, 3.959202928493817e-07, 3.959202928493817e-07, 5.656004183562596e-07, 5.656004183562596e-07, 8.080005976517993e-07, 8.080005976517993e-07, 1.1542865680739992e-06, 1.1542865680739992e-06, 1.6489808115342846e-06, 1.6489808115342846e-06, 2.355686873620407e-06, 2.355686873620407e-06, 3.3652669623148667e-06, 3.3652669623148667e-06, 4.807524231878382e-06, 4.807524231878382e-06, 6.86789175982626e-06, 6.86789175982626e-06, 9.811273942608943e-06, 9.811273942608943e-06, 1.401610563229849e-05, 1.401610563229849e-05, 2.0023008046140703e-05, 2.0023008046140703e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:24:39,630] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=28.533842054679045, CurrSamplesPerSec=31.339736401160398, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [ 390/2809]  eta: 0:23:41  lr: 0.000020  min_lr: 0.000000  loss: 4.9028 (4.9168)  class_acc: 0.0000 (0.0362)  loss_scale: 65536.0000 (76430.7315)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1150  max mem: 15572
Epoch: [2]  [ 400/2809]  eta: 0:23:27  lr: 0.000020  min_lr: 0.000000  loss: 4.9028 (4.9176)  class_acc: 0.0000 (0.0355)  loss_scale: 65536.0000 (76159.0424)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0753  max mem: 15572
Epoch: [2]  [ 410/2809]  eta: 0:23:13  lr: 0.000020  min_lr: 0.000000  loss: 4.9014 (4.9162)  class_acc: 0.0000 (0.0358)  loss_scale: 65536.0000 (75900.5742)  weight_decay: 0.0500 (0.0500)  time: 0.4448  data: 0.0131  max mem: 15572
[2025-01-15 15:24:54,312] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6029
[2025-01-15 15:24:54,312] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:24:54,312] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [ 420/2809]  eta: 0:23:07  lr: 0.000020  min_lr: 0.000000  loss: 4.8742 (4.9166)  class_acc: 0.0000 (0.0354)  loss_scale: 32768.0000 (74876.0475)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0745  max mem: 15572
Epoch: [2]  [ 430/2809]  eta: 0:23:07  lr: 0.000020  min_lr: 0.000000  loss: 4.9590 (4.9162)  class_acc: 0.0000 (0.0353)  loss_scale: 32768.0000 (73899.0626)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1808  max mem: 15572
Epoch: [2]  [ 440/2809]  eta: 0:23:02  lr: 0.000020  min_lr: 0.000000  loss: 4.8702 (4.9145)  class_acc: 0.0000 (0.0354)  loss_scale: 32768.0000 (72966.3855)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1780  max mem: 15572
Epoch: [2]  [ 450/2809]  eta: 0:22:57  lr: 0.000020  min_lr: 0.000000  loss: 4.8924 (4.9149)  class_acc: 0.0000 (0.0352)  loss_scale: 32768.0000 (72075.0687)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1356  max mem: 15572
Epoch: [2]  [ 460/2809]  eta: 0:22:55  lr: 0.000020  min_lr: 0.000000  loss: 4.9140 (4.9141)  class_acc: 0.0000 (0.0353)  loss_scale: 32768.0000 (71222.4208)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.1657  max mem: 15572
Epoch: [2]  [ 470/2809]  eta: 0:22:48  lr: 0.000020  min_lr: 0.000000  loss: 4.8842 (4.9135)  class_acc: 0.0000 (0.0349)  loss_scale: 32768.0000 (70405.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1537  max mem: 15572
Epoch: [2]  [ 480/2809]  eta: 0:22:42  lr: 0.000020  min_lr: 0.000000  loss: 4.8816 (4.9139)  class_acc: 0.0000 (0.0357)  loss_scale: 32768.0000 (69623.4844)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1254  max mem: 15572
Epoch: [2]  [ 490/2809]  eta: 0:22:39  lr: 0.000020  min_lr: 0.000000  loss: 4.9982 (4.9156)  class_acc: 0.0417 (0.0357)  loss_scale: 32768.0000 (68872.8635)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1759  max mem: 15572
Epoch: [2]  [ 500/2809]  eta: 0:22:30  lr: 0.000020  min_lr: 0.000000  loss: 4.9328 (4.9151)  class_acc: 0.0417 (0.0361)  loss_scale: 32768.0000 (68152.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1405  max mem: 15572
Epoch: [2]  [ 510/2809]  eta: 0:22:22  lr: 0.000020  min_lr: 0.000000  loss: 4.8679 (4.9133)  class_acc: 0.0417 (0.0366)  loss_scale: 32768.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0962  max mem: 15572
Epoch: [2]  [ 520/2809]  eta: 0:22:16  lr: 0.000020  min_lr: 0.000000  loss: 4.8442 (4.9121)  class_acc: 0.0417 (0.0369)  loss_scale: 32768.0000 (66793.8887)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1222  max mem: 15572
Epoch: [2]  [ 530/2809]  eta: 0:22:13  lr: 0.000021  min_lr: 0.000000  loss: 4.8396 (4.9109)  class_acc: 0.0417 (0.0370)  loss_scale: 32768.0000 (66153.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1657  max mem: 15572
[2025-01-15 15:26:11,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:26:11,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [ 540/2809]  eta: 0:22:07  lr: 0.000021  min_lr: 0.000000  loss: 4.8469 (4.9116)  class_acc: 0.0000 (0.0371)  loss_scale: 32768.0000 (65596.5693)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1727  max mem: 15572
Epoch: [2]  [ 550/2809]  eta: 0:22:00  lr: 0.000021  min_lr: 0.000000  loss: 4.9558 (4.9128)  class_acc: 0.0000 (0.0371)  loss_scale: 65536.0000 (65595.4701)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1114  max mem: 15572
Epoch: [2]  [ 560/2809]  eta: 0:21:53  lr: 0.000021  min_lr: 0.000000  loss: 4.9283 (4.9115)  class_acc: 0.0417 (0.0378)  loss_scale: 65536.0000 (65594.4100)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0947  max mem: 15572
Epoch: [2]  [ 570/2809]  eta: 0:21:50  lr: 0.000021  min_lr: 0.000000  loss: 4.8688 (4.9107)  class_acc: 0.0417 (0.0378)  loss_scale: 65536.0000 (65593.3870)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1613  max mem: 15572
Epoch: [2]  [ 580/2809]  eta: 0:21:48  lr: 0.000021  min_lr: 0.000000  loss: 4.8886 (4.9109)  class_acc: 0.0000 (0.0383)  loss_scale: 65536.0000 (65592.3993)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.2289  max mem: 15572
Epoch: [2]  [ 590/2809]  eta: 0:21:42  lr: 0.000021  min_lr: 0.000000  loss: 4.9193 (4.9122)  class_acc: 0.0000 (0.0379)  loss_scale: 65536.0000 (65591.4450)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1990  max mem: 15572
Epoch: [2]  [ 600/2809]  eta: 0:21:33  lr: 0.000021  min_lr: 0.000000  loss: 4.9484 (4.9122)  class_acc: 0.0000 (0.0378)  loss_scale: 65536.0000 (65590.5225)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1178  max mem: 15572
Epoch: [2]  [ 610/2809]  eta: 0:21:26  lr: 0.000021  min_lr: 0.000000  loss: 4.8911 (4.9109)  class_acc: 0.0000 (0.0378)  loss_scale: 65536.0000 (65589.6301)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.1008  max mem: 15572
Epoch: [2]  [ 620/2809]  eta: 0:21:21  lr: 0.000021  min_lr: 0.000000  loss: 4.8659 (4.9115)  class_acc: 0.0000 (0.0379)  loss_scale: 65536.0000 (65588.7665)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1454  max mem: 15572
Epoch: [2]  [ 630/2809]  eta: 0:21:16  lr: 0.000021  min_lr: 0.000000  loss: 4.8822 (4.9110)  class_acc: 0.0000 (0.0379)  loss_scale: 65536.0000 (65587.9303)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1667  max mem: 15572
Epoch: [2]  [ 640/2809]  eta: 0:21:11  lr: 0.000021  min_lr: 0.000000  loss: 4.8974 (4.9103)  class_acc: 0.0000 (0.0380)  loss_scale: 65536.0000 (65587.1201)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1698  max mem: 15572
Epoch: [2]  [ 650/2809]  eta: 0:21:03  lr: 0.000021  min_lr: 0.000000  loss: 4.9071 (4.9102)  class_acc: 0.0417 (0.0382)  loss_scale: 65536.0000 (65586.3349)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1312  max mem: 15572
Epoch: [2]  [ 660/2809]  eta: 0:20:57  lr: 0.000021  min_lr: 0.000000  loss: 4.8877 (4.9092)  class_acc: 0.0417 (0.0382)  loss_scale: 65536.0000 (65585.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1124  max mem: 15572
[2025-01-15 15:27:26,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:27:26,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 670/2809]  eta: 0:20:51  lr: 0.000021  min_lr: 0.000000  loss: 4.8270 (4.9085)  class_acc: 0.0000 (0.0383)  loss_scale: 65536.0000 (65877.8420)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1346  max mem: 15572
[2025-01-15 15:27:32,533] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6295
[2025-01-15 15:27:32,534] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:27:32,534] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 680/2809]  eta: 0:20:46  lr: 0.000021  min_lr: 0.000000  loss: 4.9009 (4.9101)  class_acc: 0.0000 (0.0384)  loss_scale: 65536.0000 (66450.2320)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1526  max mem: 15572
Epoch: [2]  [ 690/2809]  eta: 0:20:42  lr: 0.000021  min_lr: 0.000000  loss: 4.9043 (4.9102)  class_acc: 0.0000 (0.0380)  loss_scale: 65536.0000 (66437.0014)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1773  max mem: 15572
Epoch: [2]  [ 700/2809]  eta: 0:20:36  lr: 0.000021  min_lr: 0.000000  loss: 4.9030 (4.9099)  class_acc: 0.0000 (0.0377)  loss_scale: 65536.0000 (66424.1484)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1467  max mem: 15572
Epoch: [2]  [ 710/2809]  eta: 0:20:28  lr: 0.000021  min_lr: 0.000000  loss: 4.8793 (4.9086)  class_acc: 0.0000 (0.0384)  loss_scale: 65536.0000 (66411.6568)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1046  max mem: 15572
Epoch: [2]  [ 720/2809]  eta: 0:20:23  lr: 0.000021  min_lr: 0.000000  loss: 4.8384 (4.9079)  class_acc: 0.0417 (0.0384)  loss_scale: 65536.0000 (66399.5118)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1294  max mem: 15572
Epoch: [2]  [ 730/2809]  eta: 0:20:16  lr: 0.000021  min_lr: 0.000000  loss: 4.8847 (4.9072)  class_acc: 0.0417 (0.0388)  loss_scale: 65536.0000 (66387.6990)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1349  max mem: 15572
Epoch: [2]  [ 740/2809]  eta: 0:20:09  lr: 0.000021  min_lr: 0.000000  loss: 4.9265 (4.9082)  class_acc: 0.0417 (0.0388)  loss_scale: 65536.0000 (66376.2051)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1148  max mem: 15572
Epoch: [2]  [ 750/2809]  eta: 0:20:04  lr: 0.000021  min_lr: 0.000000  loss: 4.9347 (4.9083)  class_acc: 0.0000 (0.0389)  loss_scale: 65536.0000 (66365.0173)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1467  max mem: 15572
Epoch: [2]  [ 760/2809]  eta: 0:19:58  lr: 0.000021  min_lr: 0.000000  loss: 4.8749 (4.9077)  class_acc: 0.0000 (0.0390)  loss_scale: 65536.0000 (66354.1235)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1359  max mem: 15572
Epoch: [2]  [ 770/2809]  eta: 0:19:51  lr: 0.000021  min_lr: 0.000000  loss: 4.8749 (4.9067)  class_acc: 0.0000 (0.0391)  loss_scale: 65536.0000 (66343.5123)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0915  max mem: 15572
Epoch: [2]  [ 780/2809]  eta: 0:19:46  lr: 0.000021  min_lr: 0.000000  loss: 4.7918 (4.9048)  class_acc: 0.0000 (0.0392)  loss_scale: 65536.0000 (66333.1729)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.0971  max mem: 15572
Epoch: [2]  [ 790/2809]  eta: 0:19:37  lr: 0.000021  min_lr: 0.000000  loss: 4.7851 (4.9041)  class_acc: 0.0000 (0.0391)  loss_scale: 65536.0000 (66323.0948)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0712  max mem: 15572
Epoch: [2]  [ 800/2809]  eta: 0:19:31  lr: 0.000021  min_lr: 0.000000  loss: 4.7867 (4.9030)  class_acc: 0.0417 (0.0393)  loss_scale: 65536.0000 (66313.2684)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0784  max mem: 15572
[2025-01-15 15:28:46,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:28:46,297] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 810/2809]  eta: 0:19:28  lr: 0.000021  min_lr: 0.000000  loss: 4.7774 (4.9020)  class_acc: 0.0000 (0.0391)  loss_scale: 65536.0000 (66707.7287)  weight_decay: 0.0500 (0.0500)  time: 0.6353  data: 0.1895  max mem: 15572
[2025-01-15 15:28:51,119] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6432
[2025-01-15 15:28:51,120] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:28:51,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 820/2809]  eta: 0:19:23  lr: 0.000021  min_lr: 0.000000  loss: 4.8446 (4.9020)  class_acc: 0.0000 (0.0386)  loss_scale: 65536.0000 (66932.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2095  max mem: 15572
Epoch: [2]  [ 830/2809]  eta: 0:19:17  lr: 0.000022  min_lr: 0.000000  loss: 4.8446 (4.9009)  class_acc: 0.0000 (0.0390)  loss_scale: 65536.0000 (66916.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1497  max mem: 15572
Epoch: [2]  [ 840/2809]  eta: 0:19:12  lr: 0.000022  min_lr: 0.000000  loss: 4.7387 (4.8997)  class_acc: 0.0417 (0.0392)  loss_scale: 65536.0000 (66899.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1564  max mem: 15572
Epoch: [2]  [ 850/2809]  eta: 0:19:05  lr: 0.000022  min_lr: 0.000000  loss: 4.8405 (4.8997)  class_acc: 0.0417 (0.0390)  loss_scale: 65536.0000 (66883.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1597  max mem: 15572
Epoch: [2]  [ 860/2809]  eta: 0:18:59  lr: 0.000022  min_lr: 0.000000  loss: 4.8822 (4.8996)  class_acc: 0.0000 (0.0387)  loss_scale: 65536.0000 (66868.0325)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1444  max mem: 15572
Epoch: [2]  [ 870/2809]  eta: 0:18:54  lr: 0.000022  min_lr: 0.000000  loss: 4.8690 (4.8990)  class_acc: 0.0000 (0.0385)  loss_scale: 65536.0000 (66852.7394)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1530  max mem: 15572
Epoch: [2]  [ 880/2809]  eta: 0:18:48  lr: 0.000022  min_lr: 0.000000  loss: 4.9291 (4.8990)  class_acc: 0.0000 (0.0385)  loss_scale: 65536.0000 (66837.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1545  max mem: 15572
Epoch: [2]  [ 890/2809]  eta: 0:18:44  lr: 0.000022  min_lr: 0.000000  loss: 4.9295 (4.8989)  class_acc: 0.0417 (0.0386)  loss_scale: 65536.0000 (66823.1829)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1847  max mem: 15572
Epoch: [2]  [ 900/2809]  eta: 0:18:37  lr: 0.000022  min_lr: 0.000000  loss: 4.8914 (4.8991)  class_acc: 0.0417 (0.0387)  loss_scale: 65536.0000 (66808.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1662  max mem: 15572
Epoch: [2]  [ 910/2809]  eta: 0:18:33  lr: 0.000022  min_lr: 0.000000  loss: 4.9031 (4.8990)  class_acc: 0.0000 (0.0385)  loss_scale: 65536.0000 (66794.9243)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1557  max mem: 15572
Epoch: [2]  [ 920/2809]  eta: 0:18:27  lr: 0.000022  min_lr: 0.000000  loss: 4.9031 (4.8990)  class_acc: 0.0000 (0.0386)  loss_scale: 65536.0000 (66781.2552)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1819  max mem: 15572
Epoch: [2]  [ 930/2809]  eta: 0:18:19  lr: 0.000022  min_lr: 0.000000  loss: 4.9418 (4.8988)  class_acc: 0.0000 (0.0385)  loss_scale: 65536.0000 (66767.8797)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1069  max mem: 15572
Epoch: [2]  [ 940/2809]  eta: 0:18:13  lr: 0.000022  min_lr: 0.000000  loss: 4.8764 (4.8981)  class_acc: 0.0000 (0.0385)  loss_scale: 65536.0000 (66754.7885)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.1064  max mem: 15572
[2025-01-15 15:30:07,389] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:30:07,390] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [ 950/2809]  eta: 0:18:06  lr: 0.000022  min_lr: 0.000000  loss: 4.8335 (4.8977)  class_acc: 0.0417 (0.0386)  loss_scale: 65536.0000 (67293.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1203  max mem: 15572
[2025-01-15 15:30:11,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6569
[2025-01-15 15:30:11,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:30:11,937] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [ 960/2809]  eta: 0:18:00  lr: 0.000022  min_lr: 0.000000  loss: 4.7518 (4.8963)  class_acc: 0.0417 (0.0386)  loss_scale: 65536.0000 (67274.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0809  max mem: 15572
Epoch: [2]  [ 970/2809]  eta: 0:17:56  lr: 0.000022  min_lr: 0.000000  loss: 4.7672 (4.8957)  class_acc: 0.0417 (0.0387)  loss_scale: 65536.0000 (67257.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1704  max mem: 15572
Epoch: [2]  [ 980/2809]  eta: 0:17:49  lr: 0.000022  min_lr: 0.000000  loss: 4.8582 (4.8954)  class_acc: 0.0833 (0.0390)  loss_scale: 65536.0000 (67239.5352)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1567  max mem: 15572
Epoch: [2]  [ 990/2809]  eta: 0:17:43  lr: 0.000022  min_lr: 0.000000  loss: 4.9024 (4.8958)  class_acc: 0.0833 (0.0391)  loss_scale: 65536.0000 (67222.3451)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0770  max mem: 15572
Epoch: [2]  [1000/2809]  eta: 0:17:36  lr: 0.000022  min_lr: 0.000000  loss: 4.9419 (4.8958)  class_acc: 0.0000 (0.0392)  loss_scale: 65536.0000 (67205.4985)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0696  max mem: 15572
Epoch: [2]  [1010/2809]  eta: 0:17:30  lr: 0.000022  min_lr: 0.000000  loss: 4.9778 (4.8963)  class_acc: 0.0000 (0.0391)  loss_scale: 65536.0000 (67188.9852)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0816  max mem: 15572
Epoch: [2]  [1020/2809]  eta: 0:17:25  lr: 0.000022  min_lr: 0.000000  loss: 4.8278 (4.8953)  class_acc: 0.0417 (0.0393)  loss_scale: 65536.0000 (67172.7953)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1446  max mem: 15572
Epoch: [2]  [1030/2809]  eta: 0:17:20  lr: 0.000022  min_lr: 0.000000  loss: 4.7568 (4.8946)  class_acc: 0.0417 (0.0396)  loss_scale: 65536.0000 (67156.9195)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1855  max mem: 15572
Epoch: [2]  [1040/2809]  eta: 0:17:14  lr: 0.000022  min_lr: 0.000000  loss: 4.8269 (4.8946)  class_acc: 0.0000 (0.0396)  loss_scale: 65536.0000 (67141.3487)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1522  max mem: 15572
Epoch: [2]  [1050/2809]  eta: 0:17:09  lr: 0.000022  min_lr: 0.000000  loss: 4.8620 (4.8941)  class_acc: 0.0417 (0.0398)  loss_scale: 65536.0000 (67126.0742)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1539  max mem: 15572
Epoch: [2]  [1060/2809]  eta: 0:17:02  lr: 0.000022  min_lr: 0.000000  loss: 4.8520 (4.8937)  class_acc: 0.0000 (0.0397)  loss_scale: 65536.0000 (67111.0877)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1287  max mem: 15572
Epoch: [2]  [1070/2809]  eta: 0:16:57  lr: 0.000022  min_lr: 0.000000  loss: 4.8280 (4.8926)  class_acc: 0.0417 (0.0401)  loss_scale: 65536.0000 (67096.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1049  max mem: 15572
[2025-01-15 15:31:28,700] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:31:28,701] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1080/2809]  eta: 0:16:52  lr: 0.000022  min_lr: 0.000000  loss: 4.8372 (4.8922)  class_acc: 0.0417 (0.0401)  loss_scale: 65536.0000 (67142.5717)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1953  max mem: 15572
Epoch: [2]  [1090/2809]  eta: 0:16:45  lr: 0.000022  min_lr: 0.000000  loss: 4.8518 (4.8907)  class_acc: 0.0417 (0.0403)  loss_scale: 131072.0000 (67728.5426)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1454  max mem: 15572
Epoch: [2]  [1100/2809]  eta: 0:16:40  lr: 0.000022  min_lr: 0.000000  loss: 4.8650 (4.8909)  class_acc: 0.0417 (0.0404)  loss_scale: 131072.0000 (68303.8692)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1253  max mem: 15572
Epoch: [2]  [1110/2809]  eta: 0:16:34  lr: 0.000022  min_lr: 0.000000  loss: 4.9125 (4.8902)  class_acc: 0.0417 (0.0405)  loss_scale: 131072.0000 (68868.8389)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1566  max mem: 15572
Epoch: [2]  [1120/2809]  eta: 0:16:28  lr: 0.000022  min_lr: 0.000000  loss: 4.8213 (4.8895)  class_acc: 0.0417 (0.0404)  loss_scale: 131072.0000 (69423.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1391  max mem: 15572
[2025-01-15 15:31:52,485] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6740
[2025-01-15 15:31:52,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:31:52,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1130/2809]  eta: 0:16:22  lr: 0.000023  min_lr: 0.000000  loss: 4.8198 (4.8894)  class_acc: 0.0000 (0.0405)  loss_scale: 131072.0000 (69447.2997)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1481  max mem: 15572
Epoch: [2]  [1140/2809]  eta: 0:16:15  lr: 0.000023  min_lr: 0.000000  loss: 4.9092 (4.8895)  class_acc: 0.0000 (0.0403)  loss_scale: 65536.0000 (69413.0202)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1027  max mem: 15572
Epoch: [2]  [1150/2809]  eta: 0:16:10  lr: 0.000023  min_lr: 0.000000  loss: 4.8588 (4.8889)  class_acc: 0.0000 (0.0404)  loss_scale: 65536.0000 (69379.3362)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1227  max mem: 15572
Epoch: [2]  [1160/2809]  eta: 0:16:04  lr: 0.000023  min_lr: 0.000000  loss: 4.8183 (4.8888)  class_acc: 0.0417 (0.0404)  loss_scale: 65536.0000 (69346.2326)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1389  max mem: 15572
Epoch: [2]  [1170/2809]  eta: 0:15:57  lr: 0.000023  min_lr: 0.000000  loss: 4.8621 (4.8884)  class_acc: 0.0417 (0.0407)  loss_scale: 65536.0000 (69313.6943)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0846  max mem: 15572
Epoch: [2]  [1180/2809]  eta: 0:15:52  lr: 0.000023  min_lr: 0.000000  loss: 4.8734 (4.8884)  class_acc: 0.0417 (0.0407)  loss_scale: 65536.0000 (69281.7070)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1359  max mem: 15572
Epoch: [2]  [1190/2809]  eta: 0:15:47  lr: 0.000023  min_lr: 0.000000  loss: 4.8734 (4.8878)  class_acc: 0.0000 (0.0408)  loss_scale: 65536.0000 (69250.2569)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1819  max mem: 15572
Epoch: [2]  [1200/2809]  eta: 0:15:41  lr: 0.000023  min_lr: 0.000000  loss: 4.8292 (4.8875)  class_acc: 0.0000 (0.0406)  loss_scale: 65536.0000 (69219.3306)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1818  max mem: 15572
Epoch: [2]  [1210/2809]  eta: 0:15:36  lr: 0.000023  min_lr: 0.000000  loss: 4.8670 (4.8873)  class_acc: 0.0000 (0.0407)  loss_scale: 65536.0000 (69188.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1953  max mem: 15572
Epoch: [2]  [1220/2809]  eta: 0:15:31  lr: 0.000023  min_lr: 0.000000  loss: 4.8670 (4.8870)  class_acc: 0.0417 (0.0407)  loss_scale: 65536.0000 (69158.9975)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1874  max mem: 15572
Epoch: [2]  [1230/2809]  eta: 0:15:26  lr: 0.000023  min_lr: 0.000000  loss: 4.9200 (4.8869)  class_acc: 0.0417 (0.0407)  loss_scale: 65536.0000 (69129.5662)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1708  max mem: 15572
Epoch: [2]  [1240/2809]  eta: 0:15:19  lr: 0.000023  min_lr: 0.000000  loss: 4.9200 (4.8866)  class_acc: 0.0000 (0.0406)  loss_scale: 65536.0000 (69100.6092)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.0954  max mem: 15572
Epoch: [2]  [1250/2809]  eta: 0:15:13  lr: 0.000023  min_lr: 0.000000  loss: 4.8233 (4.8863)  class_acc: 0.0417 (0.0406)  loss_scale: 65536.0000 (69072.1151)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1075  max mem: 15572
[2025-01-15 15:33:09,278] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:33:09,279] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1260/2809]  eta: 0:15:07  lr: 0.000023  min_lr: 0.000000  loss: 4.8454 (4.8864)  class_acc: 0.0417 (0.0405)  loss_scale: 65536.0000 (69563.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1249  max mem: 15572
[2025-01-15 15:33:14,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6879
[2025-01-15 15:33:14,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:33:14,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1270/2809]  eta: 0:15:01  lr: 0.000023  min_lr: 0.000000  loss: 4.8454 (4.8862)  class_acc: 0.0417 (0.0408)  loss_scale: 65536.0000 (69532.0976)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1289  max mem: 15572
Epoch: [2]  [1280/2809]  eta: 0:14:55  lr: 0.000023  min_lr: 0.000000  loss: 4.8563 (4.8860)  class_acc: 0.0417 (0.0408)  loss_scale: 65536.0000 (69500.9024)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1489  max mem: 15572
Epoch: [2]  [1290/2809]  eta: 0:14:49  lr: 0.000023  min_lr: 0.000000  loss: 4.8423 (4.8853)  class_acc: 0.0417 (0.0410)  loss_scale: 65536.0000 (69470.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.0969  max mem: 15572
Epoch: [2]  [1300/2809]  eta: 0:14:43  lr: 0.000023  min_lr: 0.000000  loss: 4.7574 (4.8844)  class_acc: 0.0833 (0.0413)  loss_scale: 65536.0000 (69439.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0897  max mem: 15572
Epoch: [2]  [1310/2809]  eta: 0:14:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8285 (4.8840)  class_acc: 0.0833 (0.0414)  loss_scale: 65536.0000 (69410.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1142  max mem: 15572
Epoch: [2]  [1320/2809]  eta: 0:14:32  lr: 0.000023  min_lr: 0.000000  loss: 4.8869 (4.8841)  class_acc: 0.0417 (0.0413)  loss_scale: 65536.0000 (69380.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.2021  max mem: 15572
Epoch: [2]  [1330/2809]  eta: 0:14:25  lr: 0.000023  min_lr: 0.000000  loss: 5.0063 (4.8849)  class_acc: 0.0000 (0.0410)  loss_scale: 65536.0000 (69351.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1668  max mem: 15572
Epoch: [2]  [1340/2809]  eta: 0:14:18  lr: 0.000023  min_lr: 0.000000  loss: 4.8612 (4.8845)  class_acc: 0.0000 (0.0409)  loss_scale: 65536.0000 (69323.5019)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0686  max mem: 15572
Epoch: [2]  [1350/2809]  eta: 0:14:13  lr: 0.000023  min_lr: 0.000000  loss: 4.8264 (4.8843)  class_acc: 0.0000 (0.0407)  loss_scale: 65536.0000 (69295.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1221  max mem: 15572
Epoch: [2]  [1360/2809]  eta: 0:14:06  lr: 0.000023  min_lr: 0.000000  loss: 4.8365 (4.8836)  class_acc: 0.0417 (0.0409)  loss_scale: 65536.0000 (69267.8442)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0997  max mem: 15572
Epoch: [2]  [1370/2809]  eta: 0:14:00  lr: 0.000023  min_lr: 0.000000  loss: 4.8193 (4.8831)  class_acc: 0.0417 (0.0410)  loss_scale: 65536.0000 (69240.6244)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0890  max mem: 15572
Epoch: [2]  [1380/2809]  eta: 0:13:55  lr: 0.000023  min_lr: 0.000000  loss: 4.8275 (4.8827)  class_acc: 0.0000 (0.0408)  loss_scale: 65536.0000 (69213.7987)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1584  max mem: 15572
[2025-01-15 15:34:23,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[2.263398238922958e-07, 2.263398238922958e-07, 3.233426055604226e-07, 3.233426055604226e-07, 4.619180079434609e-07, 4.619180079434609e-07, 6.598828684906585e-07, 6.598828684906585e-07, 9.426898121295122e-07, 9.426898121295122e-07, 1.346699731613589e-06, 1.346699731613589e-06, 1.923856759447984e-06, 1.923856759447984e-06, 2.748366799211406e-06, 2.748366799211406e-06, 3.926238284587723e-06, 3.926238284587723e-06, 5.60891183512532e-06, 5.60891183512532e-06, 8.012731193036171e-06, 8.012731193036171e-06, 1.1446758847194531e-05, 1.1446758847194531e-05, 1.635251263884933e-05, 1.635251263884933e-05, 2.336073234121333e-05, 2.336073234121333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:34:23,171] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=28.529482395185244, CurrSamplesPerSec=31.849929917261235, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-15 15:34:28,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:34:28,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1390/2809]  eta: 0:13:49  lr: 0.000023  min_lr: 0.000000  loss: 4.8280 (4.8825)  class_acc: 0.0000 (0.0410)  loss_scale: 65536.0000 (69234.4730)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1513  max mem: 15572
[2025-01-15 15:34:29,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7010
[2025-01-15 15:34:29,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:34:29,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1400/2809]  eta: 0:13:44  lr: 0.000023  min_lr: 0.000000  loss: 4.8279 (4.8824)  class_acc: 0.0417 (0.0410)  loss_scale: 65536.0000 (69254.8522)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1236  max mem: 15572
Epoch: [2]  [1410/2809]  eta: 0:13:37  lr: 0.000023  min_lr: 0.000000  loss: 4.8223 (4.8822)  class_acc: 0.0417 (0.0412)  loss_scale: 65536.0000 (69228.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.0906  max mem: 15572
Epoch: [2]  [1420/2809]  eta: 0:13:32  lr: 0.000023  min_lr: 0.000000  loss: 4.8086 (4.8813)  class_acc: 0.0833 (0.0413)  loss_scale: 65536.0000 (69202.5109)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1191  max mem: 15572
Epoch: [2]  [1430/2809]  eta: 0:13:26  lr: 0.000024  min_lr: 0.000000  loss: 4.7491 (4.8802)  class_acc: 0.0417 (0.0415)  loss_scale: 65536.0000 (69176.8889)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1568  max mem: 15572
Epoch: [2]  [1440/2809]  eta: 0:13:22  lr: 0.000024  min_lr: 0.000000  loss: 4.7345 (4.8798)  class_acc: 0.0417 (0.0416)  loss_scale: 65536.0000 (69151.6225)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.2351  max mem: 15572
Epoch: [2]  [1450/2809]  eta: 0:13:16  lr: 0.000024  min_lr: 0.000000  loss: 4.8764 (4.8797)  class_acc: 0.0417 (0.0417)  loss_scale: 65536.0000 (69126.7043)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.2404  max mem: 15572
Epoch: [2]  [1460/2809]  eta: 0:13:10  lr: 0.000024  min_lr: 0.000000  loss: 4.8070 (4.8785)  class_acc: 0.0417 (0.0419)  loss_scale: 65536.0000 (69102.1273)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1734  max mem: 15572
Epoch: [2]  [1470/2809]  eta: 0:13:04  lr: 0.000024  min_lr: 0.000000  loss: 4.8391 (4.8789)  class_acc: 0.0000 (0.0417)  loss_scale: 65536.0000 (69077.8844)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1362  max mem: 15572
Epoch: [2]  [1480/2809]  eta: 0:12:59  lr: 0.000024  min_lr: 0.000000  loss: 4.8862 (4.8787)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (69053.9689)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1420  max mem: 15572
Epoch: [2]  [1490/2809]  eta: 0:12:52  lr: 0.000024  min_lr: 0.000000  loss: 4.7952 (4.8776)  class_acc: 0.0000 (0.0417)  loss_scale: 65536.0000 (69030.3742)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1042  max mem: 15572
Epoch: [2]  [1500/2809]  eta: 0:12:46  lr: 0.000024  min_lr: 0.000000  loss: 4.7952 (4.8771)  class_acc: 0.0000 (0.0417)  loss_scale: 65536.0000 (69007.0939)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1078  max mem: 15572
Epoch: [2]  [1510/2809]  eta: 0:12:41  lr: 0.000024  min_lr: 0.000000  loss: 4.8425 (4.8773)  class_acc: 0.0417 (0.0417)  loss_scale: 65536.0000 (68984.1218)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1708  max mem: 15572
Epoch: [2]  [1520/2809]  eta: 0:12:35  lr: 0.000024  min_lr: 0.000000  loss: 4.8907 (4.8774)  class_acc: 0.0417 (0.0417)  loss_scale: 65536.0000 (68961.4517)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1545  max mem: 15572
[2025-01-15 15:35:47,634] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:35:47,634] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:35:48,539] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7141
[2025-01-15 15:35:48,539] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:35:48,539] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1530/2809]  eta: 0:12:29  lr: 0.000024  min_lr: 0.000000  loss: 4.8437 (4.8770)  class_acc: 0.0417 (0.0418)  loss_scale: 65536.0000 (69024.6897)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1665  max mem: 15572
Epoch: [2]  [1540/2809]  eta: 0:12:23  lr: 0.000024  min_lr: 0.000000  loss: 4.8437 (4.8772)  class_acc: 0.0000 (0.0417)  loss_scale: 65536.0000 (69002.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1373  max mem: 15572
Epoch: [2]  [1550/2809]  eta: 0:12:17  lr: 0.000024  min_lr: 0.000000  loss: 4.8527 (4.8771)  class_acc: 0.0000 (0.0418)  loss_scale: 65536.0000 (68979.7034)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0951  max mem: 15572
Epoch: [2]  [1560/2809]  eta: 0:12:11  lr: 0.000024  min_lr: 0.000000  loss: 4.8184 (4.8765)  class_acc: 0.0417 (0.0420)  loss_scale: 65536.0000 (68957.6425)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0807  max mem: 15572
Epoch: [2]  [1570/2809]  eta: 0:12:05  lr: 0.000024  min_lr: 0.000000  loss: 4.7791 (4.8763)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (68935.8625)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1152  max mem: 15572
Epoch: [2]  [1580/2809]  eta: 0:11:58  lr: 0.000024  min_lr: 0.000000  loss: 4.7997 (4.8758)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (68914.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0900  max mem: 15572
Epoch: [2]  [1590/2809]  eta: 0:11:52  lr: 0.000024  min_lr: 0.000000  loss: 4.8651 (4.8764)  class_acc: 0.0000 (0.0420)  loss_scale: 65536.0000 (68893.1238)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0968  max mem: 15572
Epoch: [2]  [1600/2809]  eta: 0:11:47  lr: 0.000024  min_lr: 0.000000  loss: 4.9436 (4.8760)  class_acc: 0.0000 (0.0421)  loss_scale: 65536.0000 (68872.1549)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.2024  max mem: 15572
Epoch: [2]  [1610/2809]  eta: 0:11:41  lr: 0.000024  min_lr: 0.000000  loss: 4.8231 (4.8756)  class_acc: 0.0417 (0.0421)  loss_scale: 65536.0000 (68851.4463)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1642  max mem: 15572
Epoch: [2]  [1620/2809]  eta: 0:11:36  lr: 0.000024  min_lr: 0.000000  loss: 4.8150 (4.8750)  class_acc: 0.0000 (0.0422)  loss_scale: 65536.0000 (68830.9932)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1622  max mem: 15572
Epoch: [2]  [1630/2809]  eta: 0:11:29  lr: 0.000024  min_lr: 0.000000  loss: 4.7991 (4.8745)  class_acc: 0.0000 (0.0422)  loss_scale: 65536.0000 (68810.7909)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1263  max mem: 15572
Epoch: [2]  [1640/2809]  eta: 0:11:23  lr: 0.000024  min_lr: 0.000000  loss: 4.7396 (4.8740)  class_acc: 0.0417 (0.0423)  loss_scale: 65536.0000 (68790.8349)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0365  max mem: 15572
Epoch: [2]  [1650/2809]  eta: 0:11:18  lr: 0.000024  min_lr: 0.000000  loss: 4.7755 (4.8741)  class_acc: 0.0417 (0.0423)  loss_scale: 65536.0000 (68771.1205)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1114  max mem: 15572
[2025-01-15 15:37:02,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:37:02,758] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:37:08,332] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7276
[2025-01-15 15:37:08,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:37:08,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1660/2809]  eta: 0:11:13  lr: 0.000024  min_lr: 0.000000  loss: 4.7878 (4.8733)  class_acc: 0.0417 (0.0425)  loss_scale: 65536.0000 (68988.3781)  weight_decay: 0.0500 (0.0500)  time: 0.6934  data: 0.2199  max mem: 15572
Epoch: [2]  [1670/2809]  eta: 0:11:06  lr: 0.000024  min_lr: 0.000000  loss: 4.7937 (4.8729)  class_acc: 0.0417 (0.0425)  loss_scale: 65536.0000 (68967.7175)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1611  max mem: 15572
Epoch: [2]  [1680/2809]  eta: 0:11:00  lr: 0.000024  min_lr: 0.000000  loss: 4.7937 (4.8724)  class_acc: 0.0417 (0.0428)  loss_scale: 65536.0000 (68947.3028)  weight_decay: 0.0500 (0.0500)  time: 0.5032  data: 0.0481  max mem: 15572
Epoch: [2]  [1690/2809]  eta: 0:10:55  lr: 0.000024  min_lr: 0.000000  loss: 4.8806 (4.8728)  class_acc: 0.0417 (0.0427)  loss_scale: 65536.0000 (68927.1295)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1441  max mem: 15572
Epoch: [2]  [1700/2809]  eta: 0:10:49  lr: 0.000024  min_lr: 0.000000  loss: 4.8864 (4.8727)  class_acc: 0.0000 (0.0425)  loss_scale: 65536.0000 (68907.1934)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.2149  max mem: 15572
Epoch: [2]  [1710/2809]  eta: 0:10:43  lr: 0.000024  min_lr: 0.000000  loss: 4.8509 (4.8727)  class_acc: 0.0000 (0.0426)  loss_scale: 65536.0000 (68887.4904)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1559  max mem: 15572
Epoch: [2]  [1720/2809]  eta: 0:10:38  lr: 0.000024  min_lr: 0.000000  loss: 4.8650 (4.8727)  class_acc: 0.0417 (0.0427)  loss_scale: 65536.0000 (68868.0163)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1810  max mem: 15572
Epoch: [2]  [1730/2809]  eta: 0:10:32  lr: 0.000025  min_lr: 0.000000  loss: 4.8650 (4.8723)  class_acc: 0.0833 (0.0429)  loss_scale: 65536.0000 (68848.7672)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1644  max mem: 15572
Epoch: [2]  [1740/2809]  eta: 0:10:26  lr: 0.000025  min_lr: 0.000000  loss: 4.8011 (4.8716)  class_acc: 0.0417 (0.0429)  loss_scale: 65536.0000 (68829.7392)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0828  max mem: 15572
Epoch: [2]  [1750/2809]  eta: 0:10:20  lr: 0.000025  min_lr: 0.000000  loss: 4.7846 (4.8712)  class_acc: 0.0417 (0.0429)  loss_scale: 65536.0000 (68810.9286)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1230  max mem: 15572
Epoch: [2]  [1760/2809]  eta: 0:10:14  lr: 0.000025  min_lr: 0.000000  loss: 4.8558 (4.8713)  class_acc: 0.0000 (0.0428)  loss_scale: 65536.0000 (68792.3316)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1167  max mem: 15572
Epoch: [2]  [1770/2809]  eta: 0:10:08  lr: 0.000025  min_lr: 0.000000  loss: 4.8558 (4.8714)  class_acc: 0.0000 (0.0428)  loss_scale: 65536.0000 (68773.9447)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0737  max mem: 15572
Epoch: [2]  [1780/2809]  eta: 0:10:01  lr: 0.000025  min_lr: 0.000000  loss: 4.7713 (4.8713)  class_acc: 0.0417 (0.0429)  loss_scale: 65536.0000 (68755.7642)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0745  max mem: 15572
[2025-01-15 15:38:20,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:38:20,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [1790/2809]  eta: 0:09:56  lr: 0.000025  min_lr: 0.000000  loss: 4.7037 (4.8703)  class_acc: 0.0000 (0.0431)  loss_scale: 65536.0000 (68884.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1367  max mem: 15572
[2025-01-15 15:38:25,918] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7413
[2025-01-15 15:38:25,918] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:38:25,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1800/2809]  eta: 0:09:50  lr: 0.000025  min_lr: 0.000000  loss: 4.7689 (4.8700)  class_acc: 0.0000 (0.0431)  loss_scale: 65536.0000 (69011.1183)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1709  max mem: 15572
Epoch: [2]  [1810/2809]  eta: 0:09:44  lr: 0.000025  min_lr: 0.000000  loss: 4.8488 (4.8701)  class_acc: 0.0000 (0.0430)  loss_scale: 65536.0000 (68991.9293)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1014  max mem: 15572
Epoch: [2]  [1820/2809]  eta: 0:09:38  lr: 0.000025  min_lr: 0.000000  loss: 4.8390 (4.8694)  class_acc: 0.0417 (0.0431)  loss_scale: 65536.0000 (68972.9511)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1023  max mem: 15572
Epoch: [2]  [1830/2809]  eta: 0:09:32  lr: 0.000025  min_lr: 0.000000  loss: 4.7187 (4.8685)  class_acc: 0.0417 (0.0432)  loss_scale: 65536.0000 (68954.1802)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1040  max mem: 15572
Epoch: [2]  [1840/2809]  eta: 0:09:26  lr: 0.000025  min_lr: 0.000000  loss: 4.7477 (4.8681)  class_acc: 0.0417 (0.0432)  loss_scale: 65536.0000 (68935.6133)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1071  max mem: 15572
Epoch: [2]  [1850/2809]  eta: 0:09:20  lr: 0.000025  min_lr: 0.000000  loss: 4.7890 (4.8677)  class_acc: 0.0000 (0.0432)  loss_scale: 65536.0000 (68917.2469)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1323  max mem: 15572
Epoch: [2]  [1860/2809]  eta: 0:09:14  lr: 0.000025  min_lr: 0.000000  loss: 4.7572 (4.8675)  class_acc: 0.0000 (0.0432)  loss_scale: 65536.0000 (68899.0779)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1321  max mem: 15572
Epoch: [2]  [1870/2809]  eta: 0:09:09  lr: 0.000025  min_lr: 0.000000  loss: 4.7572 (4.8668)  class_acc: 0.0000 (0.0433)  loss_scale: 65536.0000 (68881.1032)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1504  max mem: 15572
Epoch: [2]  [1880/2809]  eta: 0:09:03  lr: 0.000025  min_lr: 0.000000  loss: 4.7677 (4.8666)  class_acc: 0.0417 (0.0434)  loss_scale: 65536.0000 (68863.3195)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0904  max mem: 15572
Epoch: [2]  [1890/2809]  eta: 0:08:56  lr: 0.000025  min_lr: 0.000000  loss: 4.8321 (4.8666)  class_acc: 0.0417 (0.0435)  loss_scale: 65536.0000 (68845.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0665  max mem: 15572
Epoch: [2]  [1900/2809]  eta: 0:08:51  lr: 0.000025  min_lr: 0.000000  loss: 4.8684 (4.8659)  class_acc: 0.0833 (0.0438)  loss_scale: 65536.0000 (68828.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1236  max mem: 15572
Epoch: [2]  [1910/2809]  eta: 0:08:45  lr: 0.000025  min_lr: 0.000000  loss: 4.8245 (4.8655)  class_acc: 0.0833 (0.0440)  loss_scale: 65536.0000 (68811.0853)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1619  max mem: 15572
Epoch: [2]  [1920/2809]  eta: 0:08:39  lr: 0.000025  min_lr: 0.000000  loss: 4.8074 (4.8654)  class_acc: 0.0000 (0.0438)  loss_scale: 65536.0000 (68794.0364)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1815  max mem: 15572
[2025-01-15 15:39:41,235] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:39:41,236] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:39:45,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7547
[2025-01-15 15:39:45,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:39:45,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [1930/2809]  eta: 0:08:34  lr: 0.000025  min_lr: 0.000000  loss: 4.8126 (4.8651)  class_acc: 0.0000 (0.0438)  loss_scale: 65536.0000 (68946.8586)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1972  max mem: 15572
Epoch: [2]  [1940/2809]  eta: 0:08:28  lr: 0.000025  min_lr: 0.000000  loss: 4.8407 (4.8651)  class_acc: 0.0417 (0.0439)  loss_scale: 65536.0000 (68929.2859)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1953  max mem: 15572
Epoch: [2]  [1950/2809]  eta: 0:08:22  lr: 0.000025  min_lr: 0.000000  loss: 4.8089 (4.8643)  class_acc: 0.0000 (0.0440)  loss_scale: 65536.0000 (68911.8934)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0973  max mem: 15572
Epoch: [2]  [1960/2809]  eta: 0:08:16  lr: 0.000025  min_lr: 0.000000  loss: 4.7976 (4.8643)  class_acc: 0.0833 (0.0443)  loss_scale: 65536.0000 (68894.6782)  weight_decay: 0.0500 (0.0500)  time: 0.5099  data: 0.0559  max mem: 15572
Epoch: [2]  [1970/2809]  eta: 0:08:10  lr: 0.000025  min_lr: 0.000000  loss: 4.8803 (4.8642)  class_acc: 0.0417 (0.0442)  loss_scale: 65536.0000 (68877.6377)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0933  max mem: 15572
Epoch: [2]  [1980/2809]  eta: 0:08:04  lr: 0.000025  min_lr: 0.000000  loss: 4.7697 (4.8637)  class_acc: 0.0000 (0.0441)  loss_scale: 65536.0000 (68860.7693)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1230  max mem: 15572
Epoch: [2]  [1990/2809]  eta: 0:07:58  lr: 0.000025  min_lr: 0.000000  loss: 4.8324 (4.8635)  class_acc: 0.0000 (0.0441)  loss_scale: 65536.0000 (68844.0703)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1635  max mem: 15572
Epoch: [2]  [2000/2809]  eta: 0:07:53  lr: 0.000025  min_lr: 0.000000  loss: 4.8524 (4.8631)  class_acc: 0.0417 (0.0442)  loss_scale: 65536.0000 (68827.5382)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1556  max mem: 15572
[2025-01-15 15:40:31,646] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7627
[2025-01-15 15:40:31,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:40:31,647] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2010/2809]  eta: 0:07:47  lr: 0.000025  min_lr: 0.000000  loss: 4.8237 (4.8628)  class_acc: 0.0417 (0.0442)  loss_scale: 65536.0000 (68778.5818)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1411  max mem: 15572
Epoch: [2]  [2020/2809]  eta: 0:07:41  lr: 0.000025  min_lr: 0.000000  loss: 4.8237 (4.8625)  class_acc: 0.0417 (0.0444)  loss_scale: 32768.0000 (68600.3998)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1117  max mem: 15572
Epoch: [2]  [2030/2809]  eta: 0:07:35  lr: 0.000026  min_lr: 0.000000  loss: 4.7360 (4.8622)  class_acc: 0.0417 (0.0444)  loss_scale: 32768.0000 (68423.9724)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0800  max mem: 15572
Epoch: [2]  [2040/2809]  eta: 0:07:29  lr: 0.000026  min_lr: 0.000000  loss: 4.8378 (4.8623)  class_acc: 0.0417 (0.0444)  loss_scale: 32768.0000 (68249.2739)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0959  max mem: 15572
Epoch: [2]  [2050/2809]  eta: 0:07:23  lr: 0.000026  min_lr: 0.000000  loss: 4.8406 (4.8622)  class_acc: 0.0417 (0.0443)  loss_scale: 32768.0000 (68076.2789)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1522  max mem: 15572
Epoch: [2]  [2060/2809]  eta: 0:07:18  lr: 0.000026  min_lr: 0.000000  loss: 4.8331 (4.8622)  class_acc: 0.0000 (0.0443)  loss_scale: 32768.0000 (67904.9626)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.1710  max mem: 15572
Epoch: [2]  [2070/2809]  eta: 0:07:12  lr: 0.000026  min_lr: 0.000000  loss: 4.7659 (4.8616)  class_acc: 0.0000 (0.0444)  loss_scale: 32768.0000 (67735.3008)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1460  max mem: 15572
Epoch: [2]  [2080/2809]  eta: 0:07:06  lr: 0.000026  min_lr: 0.000000  loss: 4.7526 (4.8611)  class_acc: 0.0417 (0.0447)  loss_scale: 32768.0000 (67567.2696)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0840  max mem: 15572
Epoch: [2]  [2090/2809]  eta: 0:07:00  lr: 0.000026  min_lr: 0.000000  loss: 4.8679 (4.8613)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (67400.8455)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1314  max mem: 15572
Epoch: [2]  [2100/2809]  eta: 0:06:54  lr: 0.000026  min_lr: 0.000000  loss: 4.8266 (4.8607)  class_acc: 0.0000 (0.0447)  loss_scale: 32768.0000 (67236.0057)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1644  max mem: 15572
Epoch: [2]  [2110/2809]  eta: 0:06:49  lr: 0.000026  min_lr: 0.000000  loss: 4.8060 (4.8608)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (67072.7276)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1855  max mem: 15572
Epoch: [2]  [2120/2809]  eta: 0:06:43  lr: 0.000026  min_lr: 0.000000  loss: 4.8469 (4.8604)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (66910.9892)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.2212  max mem: 15572
Epoch: [2]  [2130/2809]  eta: 0:06:37  lr: 0.000026  min_lr: 0.000000  loss: 4.7227 (4.8596)  class_acc: 0.0417 (0.0448)  loss_scale: 32768.0000 (66750.7687)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1584  max mem: 15572
[2025-01-15 15:41:47,365] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:41:47,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2140/2809]  eta: 0:06:31  lr: 0.000026  min_lr: 0.000000  loss: 4.7177 (4.8594)  class_acc: 0.0833 (0.0450)  loss_scale: 32768.0000 (66637.9598)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1766  max mem: 15572
[2025-01-15 15:41:52,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7763
[2025-01-15 15:41:52,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:41:52,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2150/2809]  eta: 0:06:26  lr: 0.000026  min_lr: 0.000000  loss: 4.8507 (4.8594)  class_acc: 0.0833 (0.0450)  loss_scale: 32768.0000 (66541.4338)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.2028  max mem: 15572
Epoch: [2]  [2160/2809]  eta: 0:06:20  lr: 0.000026  min_lr: 0.000000  loss: 4.7955 (4.8589)  class_acc: 0.0000 (0.0449)  loss_scale: 32768.0000 (66385.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1328  max mem: 15572
Epoch: [2]  [2170/2809]  eta: 0:06:14  lr: 0.000026  min_lr: 0.000000  loss: 4.7482 (4.8585)  class_acc: 0.0000 (0.0449)  loss_scale: 32768.0000 (66230.3012)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1371  max mem: 15572
Epoch: [2]  [2180/2809]  eta: 0:06:08  lr: 0.000026  min_lr: 0.000000  loss: 4.7630 (4.8585)  class_acc: 0.0417 (0.0448)  loss_scale: 32768.0000 (66076.8748)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1662  max mem: 15572
Epoch: [2]  [2190/2809]  eta: 0:06:03  lr: 0.000026  min_lr: 0.000000  loss: 4.8727 (4.8586)  class_acc: 0.0000 (0.0446)  loss_scale: 32768.0000 (65924.8489)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1984  max mem: 15572
Epoch: [2]  [2200/2809]  eta: 0:05:56  lr: 0.000026  min_lr: 0.000000  loss: 4.7837 (4.8584)  class_acc: 0.0000 (0.0446)  loss_scale: 32768.0000 (65774.2045)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1515  max mem: 15572
Epoch: [2]  [2210/2809]  eta: 0:05:51  lr: 0.000026  min_lr: 0.000000  loss: 4.7644 (4.8582)  class_acc: 0.0000 (0.0445)  loss_scale: 32768.0000 (65624.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1214  max mem: 15572
Epoch: [2]  [2220/2809]  eta: 0:05:45  lr: 0.000026  min_lr: 0.000000  loss: 4.7747 (4.8582)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (65476.9851)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1624  max mem: 15572
Epoch: [2]  [2230/2809]  eta: 0:05:39  lr: 0.000026  min_lr: 0.000000  loss: 4.7299 (4.8575)  class_acc: 0.0417 (0.0447)  loss_scale: 32768.0000 (65330.3738)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1191  max mem: 15572
Epoch: [2]  [2240/2809]  eta: 0:05:33  lr: 0.000026  min_lr: 0.000000  loss: 4.7299 (4.8572)  class_acc: 0.0000 (0.0446)  loss_scale: 32768.0000 (65185.0710)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0884  max mem: 15572
Epoch: [2]  [2250/2809]  eta: 0:05:27  lr: 0.000026  min_lr: 0.000000  loss: 4.8214 (4.8570)  class_acc: 0.0000 (0.0445)  loss_scale: 32768.0000 (65041.0591)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0624  max mem: 15572
Epoch: [2]  [2260/2809]  eta: 0:05:21  lr: 0.000026  min_lr: 0.000000  loss: 4.8026 (4.8568)  class_acc: 0.0000 (0.0445)  loss_scale: 32768.0000 (64898.3211)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0408  max mem: 15572
Epoch: [2]  [2270/2809]  eta: 0:05:15  lr: 0.000026  min_lr: 0.000000  loss: 4.7329 (4.8561)  class_acc: 0.0417 (0.0446)  loss_scale: 32768.0000 (64756.8402)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0717  max mem: 15572
[2025-01-15 15:43:06,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:43:06,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2280/2809]  eta: 0:05:09  lr: 0.000026  min_lr: 0.000000  loss: 4.7162 (4.8557)  class_acc: 0.0417 (0.0447)  loss_scale: 32768.0000 (64717.1591)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1434  max mem: 15572
Epoch: [2]  [2290/2809]  eta: 0:05:03  lr: 0.000026  min_lr: 0.000000  loss: 4.8132 (4.8557)  class_acc: 0.0417 (0.0447)  loss_scale: 65536.0000 (64720.7333)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1221  max mem: 15572
Epoch: [2]  [2300/2809]  eta: 0:04:57  lr: 0.000026  min_lr: 0.000000  loss: 4.8132 (4.8554)  class_acc: 0.0000 (0.0447)  loss_scale: 65536.0000 (64724.2764)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.0955  max mem: 15572
Epoch: [2]  [2310/2809]  eta: 0:04:51  lr: 0.000026  min_lr: 0.000000  loss: 4.8172 (4.8552)  class_acc: 0.0000 (0.0448)  loss_scale: 65536.0000 (64727.7888)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1057  max mem: 15572
Epoch: [2]  [2320/2809]  eta: 0:04:46  lr: 0.000026  min_lr: 0.000000  loss: 4.8300 (4.8553)  class_acc: 0.0000 (0.0448)  loss_scale: 65536.0000 (64731.2710)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1303  max mem: 15572
Epoch: [2]  [2330/2809]  eta: 0:04:40  lr: 0.000027  min_lr: 0.000000  loss: 4.8892 (4.8555)  class_acc: 0.0000 (0.0447)  loss_scale: 65536.0000 (64734.7233)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1637  max mem: 15572
Epoch: [2]  [2340/2809]  eta: 0:04:34  lr: 0.000027  min_lr: 0.000000  loss: 4.8892 (4.8555)  class_acc: 0.0417 (0.0447)  loss_scale: 65536.0000 (64738.1461)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1772  max mem: 15572
Epoch: [2]  [2350/2809]  eta: 0:04:28  lr: 0.000027  min_lr: 0.000000  loss: 4.8687 (4.8556)  class_acc: 0.0000 (0.0446)  loss_scale: 65536.0000 (64741.5398)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1750  max mem: 15572
Epoch: [2]  [2360/2809]  eta: 0:04:22  lr: 0.000027  min_lr: 0.000000  loss: 4.8416 (4.8555)  class_acc: 0.0000 (0.0447)  loss_scale: 65536.0000 (64744.9047)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1208  max mem: 15572
Epoch: [2]  [2370/2809]  eta: 0:04:16  lr: 0.000027  min_lr: 0.000000  loss: 4.7484 (4.8549)  class_acc: 0.0417 (0.0448)  loss_scale: 65536.0000 (64748.2412)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1152  max mem: 15572
Epoch: [2]  [2380/2809]  eta: 0:04:10  lr: 0.000027  min_lr: 0.000000  loss: 4.7326 (4.8546)  class_acc: 0.0417 (0.0449)  loss_scale: 65536.0000 (64751.5498)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1343  max mem: 15572
[2025-01-15 15:44:09,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[2.5867870428839465e-07, 2.5867870428839465e-07, 3.695410061262781e-07, 3.695410061262781e-07, 5.279157230375402e-07, 5.279157230375402e-07, 7.541653186250575e-07, 7.541653186250575e-07, 1.077379026607225e-06, 1.077379026607225e-06, 1.5391128951531786e-06, 1.5391128951531786e-06, 2.198732707361684e-06, 2.198732707361684e-06, 3.141046724802406e-06, 3.141046724802406e-06, 4.48720960686058e-06, 4.48720960686058e-06, 6.4102994383722576e-06, 6.4102994383722576e-06, 9.157570626246083e-06, 9.157570626246083e-06, 1.3082243751780119e-05, 1.3082243751780119e-05, 1.868891964540017e-05, 1.868891964540017e-05, 2.669845663628596e-05, 2.669845663628596e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:44:09,577] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=28.48318606888611, CurrSamplesPerSec=25.95523963590627, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [2]  [2390/2809]  eta: 0:04:04  lr: 0.000027  min_lr: 0.000000  loss: 4.7195 (4.8540)  class_acc: 0.0417 (0.0451)  loss_scale: 65536.0000 (64754.8306)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0879  max mem: 15572
Epoch: [2]  [2400/2809]  eta: 0:03:59  lr: 0.000027  min_lr: 0.000000  loss: 4.7195 (4.8536)  class_acc: 0.0417 (0.0451)  loss_scale: 65536.0000 (64758.0841)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1133  max mem: 15572
[2025-01-15 15:44:21,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:44:21,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:44:25,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8025
[2025-01-15 15:44:25,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:44:25,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2410/2809]  eta: 0:03:53  lr: 0.000027  min_lr: 0.000000  loss: 4.7344 (4.8531)  class_acc: 0.0417 (0.0452)  loss_scale: 65536.0000 (64897.2211)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1672  max mem: 15572
Epoch: [2]  [2420/2809]  eta: 0:03:47  lr: 0.000027  min_lr: 0.000000  loss: 4.8207 (4.8528)  class_acc: 0.0417 (0.0453)  loss_scale: 65536.0000 (64899.8596)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1780  max mem: 15572
Epoch: [2]  [2430/2809]  eta: 0:03:41  lr: 0.000027  min_lr: 0.000000  loss: 4.8341 (4.8525)  class_acc: 0.0000 (0.0452)  loss_scale: 65536.0000 (64902.4763)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1266  max mem: 15572
Epoch: [2]  [2440/2809]  eta: 0:03:35  lr: 0.000027  min_lr: 0.000000  loss: 4.7207 (4.8522)  class_acc: 0.0000 (0.0452)  loss_scale: 65536.0000 (64905.0717)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1554  max mem: 15572
[2025-01-15 15:44:50,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8068
[2025-01-15 15:44:50,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 15:44:50,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [2]  [2450/2809]  eta: 0:03:30  lr: 0.000027  min_lr: 0.000000  loss: 4.8032 (4.8522)  class_acc: 0.0000 (0.0451)  loss_scale: 65536.0000 (64894.2766)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1874  max mem: 15572
Epoch: [2]  [2460/2809]  eta: 0:03:24  lr: 0.000027  min_lr: 0.000000  loss: 4.8110 (4.8517)  class_acc: 0.0417 (0.0452)  loss_scale: 32768.0000 (64763.7351)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1419  max mem: 15572
Epoch: [2]  [2470/2809]  eta: 0:03:18  lr: 0.000027  min_lr: 0.000000  loss: 4.7844 (4.8516)  class_acc: 0.0417 (0.0452)  loss_scale: 32768.0000 (64634.2501)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1758  max mem: 15572
Epoch: [2]  [2480/2809]  eta: 0:03:12  lr: 0.000027  min_lr: 0.000000  loss: 4.7796 (4.8515)  class_acc: 0.0417 (0.0453)  loss_scale: 32768.0000 (64505.8089)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1750  max mem: 15572
Epoch: [2]  [2490/2809]  eta: 0:03:06  lr: 0.000027  min_lr: 0.000000  loss: 4.7796 (4.8512)  class_acc: 0.0417 (0.0453)  loss_scale: 32768.0000 (64378.3990)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1547  max mem: 15572
Epoch: [2]  [2500/2809]  eta: 0:03:00  lr: 0.000027  min_lr: 0.000000  loss: 4.7519 (4.8510)  class_acc: 0.0417 (0.0454)  loss_scale: 32768.0000 (64252.0080)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1401  max mem: 15572
Epoch: [2]  [2510/2809]  eta: 0:02:54  lr: 0.000027  min_lr: 0.000000  loss: 4.8409 (4.8510)  class_acc: 0.0417 (0.0454)  loss_scale: 32768.0000 (64126.6237)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.0809  max mem: 15572
Epoch: [2]  [2520/2809]  eta: 0:02:49  lr: 0.000027  min_lr: 0.000000  loss: 4.7975 (4.8504)  class_acc: 0.0417 (0.0456)  loss_scale: 32768.0000 (64002.2340)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0678  max mem: 15572
Epoch: [2]  [2530/2809]  eta: 0:02:43  lr: 0.000027  min_lr: 0.000000  loss: 4.7852 (4.8502)  class_acc: 0.0833 (0.0456)  loss_scale: 32768.0000 (63878.8273)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0960  max mem: 15572
Epoch: [2]  [2540/2809]  eta: 0:02:37  lr: 0.000027  min_lr: 0.000000  loss: 4.8209 (4.8500)  class_acc: 0.0417 (0.0458)  loss_scale: 32768.0000 (63756.3920)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1049  max mem: 15572
Epoch: [2]  [2550/2809]  eta: 0:02:31  lr: 0.000027  min_lr: 0.000000  loss: 4.8409 (4.8498)  class_acc: 0.0417 (0.0457)  loss_scale: 32768.0000 (63634.9165)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1288  max mem: 15572
Epoch: [2]  [2560/2809]  eta: 0:02:25  lr: 0.000027  min_lr: 0.000000  loss: 4.7572 (4.8493)  class_acc: 0.0417 (0.0459)  loss_scale: 32768.0000 (63514.3897)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1805  max mem: 15572
Epoch: [2]  [2570/2809]  eta: 0:02:19  lr: 0.000027  min_lr: 0.000000  loss: 4.6738 (4.8489)  class_acc: 0.0833 (0.0459)  loss_scale: 32768.0000 (63394.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1691  max mem: 15572
[2025-01-15 15:46:05,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:46:05,084] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [2]  [2580/2809]  eta: 0:02:13  lr: 0.000027  min_lr: 0.000000  loss: 4.7575 (4.8488)  class_acc: 0.0417 (0.0459)  loss_scale: 32768.0000 (63301.5296)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1106  max mem: 15572
Epoch: [2]  [2590/2809]  eta: 0:02:08  lr: 0.000027  min_lr: 0.000000  loss: 4.7546 (4.8482)  class_acc: 0.0417 (0.0460)  loss_scale: 65536.0000 (63310.1536)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1153  max mem: 15572
Epoch: [2]  [2600/2809]  eta: 0:02:02  lr: 0.000027  min_lr: 0.000000  loss: 4.6861 (4.8479)  class_acc: 0.0417 (0.0460)  loss_scale: 65536.0000 (63318.7113)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1503  max mem: 15572
Epoch: [2]  [2610/2809]  eta: 0:01:56  lr: 0.000027  min_lr: 0.000000  loss: 4.7713 (4.8479)  class_acc: 0.0000 (0.0459)  loss_scale: 65536.0000 (63327.2034)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.1922  max mem: 15572
Epoch: [2]  [2620/2809]  eta: 0:01:50  lr: 0.000027  min_lr: 0.000000  loss: 4.7713 (4.8477)  class_acc: 0.0417 (0.0459)  loss_scale: 65536.0000 (63335.6307)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1790  max mem: 15572
Epoch: [2]  [2630/2809]  eta: 0:01:44  lr: 0.000028  min_lr: 0.000000  loss: 4.8171 (4.8476)  class_acc: 0.0417 (0.0459)  loss_scale: 65536.0000 (63343.9939)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1533  max mem: 15572
Epoch: [2]  [2640/2809]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000000  loss: 4.7919 (4.8472)  class_acc: 0.0833 (0.0460)  loss_scale: 65536.0000 (63352.2938)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.2017  max mem: 15572
Epoch: [2]  [2650/2809]  eta: 0:01:33  lr: 0.000028  min_lr: 0.000000  loss: 4.7460 (4.8471)  class_acc: 0.0833 (0.0460)  loss_scale: 65536.0000 (63360.5311)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1891  max mem: 15572
Epoch: [2]  [2660/2809]  eta: 0:01:27  lr: 0.000028  min_lr: 0.000000  loss: 4.7420 (4.8468)  class_acc: 0.0417 (0.0461)  loss_scale: 65536.0000 (63368.7065)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.2017  max mem: 15572
Epoch: [2]  [2670/2809]  eta: 0:01:21  lr: 0.000028  min_lr: 0.000000  loss: 4.7264 (4.8468)  class_acc: 0.0417 (0.0461)  loss_scale: 65536.0000 (63376.8207)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.2065  max mem: 15572
Epoch: [2]  [2680/2809]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000000  loss: 4.7485 (4.8465)  class_acc: 0.0417 (0.0461)  loss_scale: 65536.0000 (63384.8743)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1239  max mem: 15572
Epoch: [2]  [2690/2809]  eta: 0:01:09  lr: 0.000028  min_lr: 0.000000  loss: 4.7119 (4.8459)  class_acc: 0.0833 (0.0462)  loss_scale: 65536.0000 (63392.8681)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1565  max mem: 15572
Epoch: [2]  [2700/2809]  eta: 0:01:03  lr: 0.000028  min_lr: 0.000000  loss: 4.6394 (4.8454)  class_acc: 0.0417 (0.0463)  loss_scale: 65536.0000 (63400.8027)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1684  max mem: 15572
[2025-01-15 15:47:22,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:47:22,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [2]  [2710/2809]  eta: 0:00:58  lr: 0.000028  min_lr: 0.000000  loss: 4.7002 (4.8453)  class_acc: 0.0417 (0.0463)  loss_scale: 65536.0000 (63505.3751)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1537  max mem: 15572
[2025-01-15 15:47:28,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8334
[2025-01-15 15:47:28,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:47:28,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [2]  [2720/2809]  eta: 0:00:52  lr: 0.000028  min_lr: 0.000000  loss: 4.7179 (4.8448)  class_acc: 0.0417 (0.0464)  loss_scale: 65536.0000 (63633.2642)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1497  max mem: 15572
Epoch: [2]  [2730/2809]  eta: 0:00:46  lr: 0.000028  min_lr: 0.000000  loss: 4.6610 (4.8441)  class_acc: 0.0417 (0.0464)  loss_scale: 65536.0000 (63640.2314)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1873  max mem: 15572
Epoch: [2]  [2740/2809]  eta: 0:00:40  lr: 0.000028  min_lr: 0.000000  loss: 4.6925 (4.8438)  class_acc: 0.0417 (0.0465)  loss_scale: 65536.0000 (63647.1478)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1713  max mem: 15572
Epoch: [2]  [2750/2809]  eta: 0:00:34  lr: 0.000028  min_lr: 0.000000  loss: 4.6930 (4.8435)  class_acc: 0.0417 (0.0465)  loss_scale: 65536.0000 (63654.0138)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1226  max mem: 15572
Epoch: [2]  [2760/2809]  eta: 0:00:28  lr: 0.000028  min_lr: 0.000000  loss: 4.8138 (4.8435)  class_acc: 0.0000 (0.0465)  loss_scale: 65536.0000 (63660.8301)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1523  max mem: 15572
Epoch: [2]  [2770/2809]  eta: 0:00:22  lr: 0.000028  min_lr: 0.000000  loss: 4.7892 (4.8433)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (63667.5973)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.0966  max mem: 15572
Epoch: [2]  [2780/2809]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000000  loss: 4.7239 (4.8429)  class_acc: 0.0833 (0.0466)  loss_scale: 65536.0000 (63674.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0779  max mem: 15572
Epoch: [2]  [2790/2809]  eta: 0:00:11  lr: 0.000028  min_lr: 0.000000  loss: 4.7584 (4.8426)  class_acc: 0.0417 (0.0466)  loss_scale: 65536.0000 (63680.9860)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0495  max mem: 15572
Epoch: [2]  [2800/2809]  eta: 0:00:05  lr: 0.000028  min_lr: 0.000000  loss: 4.7190 (4.8422)  class_acc: 0.0833 (0.0469)  loss_scale: 65536.0000 (63687.6087)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0192  max mem: 15572
Epoch: [2]  [2808/2809]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000000  loss: 4.7299 (4.8420)  class_acc: 0.0833 (0.0469)  loss_scale: 65536.0000 (63692.8729)  weight_decay: 0.0500 (0.0500)  time: 0.4258  data: 0.0191  max mem: 15572
Epoch: [2] Total time: 0:27:25 (0.5858 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000000  loss: 4.7299 (4.8420)  class_acc: 0.0833 (0.0469)  loss_scale: 65536.0000 (63692.8729)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:15  loss: 3.9638 (3.9638)  acc1: 0.0000 (0.0000)  acc5: 38.8889 (38.8889)  time: 4.4678  data: 4.2593  max mem: 15572
Val:  [ 10/272]  eta: 0:03:08  loss: 5.1801 (4.7568)  acc1: 0.0000 (13.6364)  acc5: 0.0000 (20.7071)  time: 0.7203  data: 0.5279  max mem: 15572
Val:  [ 20/272]  eta: 0:02:12  loss: 4.8177 (4.7032)  acc1: 0.0000 (11.3757)  acc5: 0.0000 (17.9894)  time: 0.3300  data: 0.1357  max mem: 15572
Val:  [ 30/272]  eta: 0:01:43  loss: 4.6361 (4.6469)  acc1: 0.0000 (8.0645)  acc5: 0.0000 (18.6380)  time: 0.2659  data: 0.0754  max mem: 15572
Val:  [ 40/272]  eta: 0:01:32  loss: 4.1762 (4.4920)  acc1: 0.0000 (10.1626)  acc5: 27.7778 (28.1843)  time: 0.2612  data: 0.0815  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 3.9987 (4.4781)  acc1: 0.0000 (9.2593)  acc5: 66.6667 (30.3922)  time: 0.3147  data: 0.1346  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 4.0395 (4.4284)  acc1: 0.0000 (8.2878)  acc5: 27.7778 (31.4208)  time: 0.3178  data: 0.1439  max mem: 15572
Val:  [ 70/272]  eta: 0:01:09  loss: 4.1553 (4.3770)  acc1: 0.0000 (7.6682)  acc5: 27.7778 (31.6119)  time: 0.2385  data: 0.0738  max mem: 15572
Val:  [ 80/272]  eta: 0:01:01  loss: 4.1892 (4.3726)  acc1: 0.0000 (10.1509)  acc5: 0.0000 (31.6187)  time: 0.1689  data: 0.0004  max mem: 15572
Val:  [ 90/272]  eta: 0:00:56  loss: 4.8856 (4.4370)  acc1: 0.0000 (9.0354)  acc5: 0.0000 (28.1441)  time: 0.1850  data: 0.0006  max mem: 15572
Val:  [100/272]  eta: 0:00:54  loss: 4.8878 (4.4876)  acc1: 0.0000 (8.7459)  acc5: 0.0000 (26.7327)  time: 0.2912  data: 0.0853  max mem: 15572
Val:  [110/272]  eta: 0:00:52  loss: 4.8641 (4.5254)  acc1: 0.0000 (7.9580)  acc5: 0.0000 (24.4745)  time: 0.3844  data: 0.1783  max mem: 15572
Val:  [120/272]  eta: 0:00:49  loss: 4.8307 (4.5625)  acc1: 0.0000 (7.3003)  acc5: 0.0000 (22.5895)  time: 0.3704  data: 0.1798  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 4.8305 (4.5219)  acc1: 0.0000 (8.5666)  acc5: 0.0000 (25.2332)  time: 0.3897  data: 0.1950  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 4.2852 (4.5036)  acc1: 0.0000 (9.7715)  acc5: 5.5556 (25.3743)  time: 0.3964  data: 0.1955  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 4.4082 (4.5098)  acc1: 0.0000 (9.1244)  acc5: 0.0000 (24.0618)  time: 0.4619  data: 0.2655  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 4.3186 (4.4976)  acc1: 0.0000 (9.2823)  acc5: 11.1111 (25.0863)  time: 0.4290  data: 0.2211  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 4.4173 (4.5247)  acc1: 0.0000 (8.8694)  acc5: 0.0000 (23.8142)  time: 0.3055  data: 0.1024  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 4.8203 (4.5279)  acc1: 0.0000 (8.3794)  acc5: 0.0000 (22.6519)  time: 0.3213  data: 0.1330  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 4.8203 (4.5437)  acc1: 0.0000 (7.9407)  acc5: 0.0000 (21.6405)  time: 0.4064  data: 0.1988  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 4.8394 (4.5683)  acc1: 0.0000 (7.5456)  acc5: 0.0000 (20.5638)  time: 0.4089  data: 0.1882  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 4.8240 (4.5758)  acc1: 0.0000 (7.6619)  acc5: 0.0000 (20.3528)  time: 0.3118  data: 0.1023  max mem: 15572
Val:  [220/272]  eta: 0:00:18  loss: 4.5260 (4.5684)  acc1: 0.0000 (7.3906)  acc5: 0.0000 (20.4123)  time: 0.3493  data: 0.1435  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 4.3688 (4.5606)  acc1: 0.0000 (7.3112)  acc5: 27.7778 (20.9716)  time: 0.3989  data: 0.1946  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 4.3688 (4.5601)  acc1: 0.0000 (7.0770)  acc5: 22.2222 (20.8391)  time: 0.3781  data: 0.1797  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.7643 (4.5830)  acc1: 0.0000 (6.8172)  acc5: 0.0000 (20.1859)  time: 0.3481  data: 0.1467  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 4.3776 (4.5535)  acc1: 0.0000 (7.2158)  acc5: 22.2222 (21.7752)  time: 0.3450  data: 0.1402  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.9535 (4.5508)  acc1: 0.0000 (6.9906)  acc5: 55.5556 (22.0377)  time: 0.2876  data: 0.1021  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.9535 (4.5525)  acc1: 0.0000 (6.9834)  acc5: 38.8889 (22.0152)  time: 0.2807  data: 0.1020  max mem: 15572
Val: Total time: 0:01:34 (0.3472 s / it)
* Acc@1 6.983 Acc@5 22.015 loss 4.552
Accuracy of the network on the 4883 val videos: 7.0%
[2025-01-15 15:49:54,783] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 15:49:54,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 15:49:54,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 15:49:57,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 15:49:57,751] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 6.98%
Epoch: [3]  [   0/2809]  eta: 7:55:52  lr: 0.000028  min_lr: 0.000000  loss: 4.5312 (4.5312)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.1646  data: 9.5966  max mem: 15572
Epoch: [3]  [  10/2809]  eta: 1:12:00  lr: 0.000028  min_lr: 0.000000  loss: 4.6671 (4.6944)  class_acc: 0.0417 (0.0417)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5435  data: 1.0772  max mem: 15572
Epoch: [3]  [  20/2809]  eta: 0:47:13  lr: 0.000028  min_lr: 0.000000  loss: 4.7195 (4.6936)  class_acc: 0.0417 (0.0694)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1284  max mem: 15572
Epoch: [3]  [  30/2809]  eta: 0:37:59  lr: 0.000028  min_lr: 0.000000  loss: 4.7190 (4.6921)  class_acc: 0.0417 (0.0672)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4226  data: 0.0160  max mem: 15572
[2025-01-15 15:50:25,689] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:50:25,690] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [  40/2809]  eta: 0:33:24  lr: 0.000028  min_lr: 0.000000  loss: 4.7272 (4.7071)  class_acc: 0.0417 (0.0600)  loss_scale: 65536.0000 (73528.1951)  weight_decay: 0.0500 (0.0500)  time: 0.4176  data: 0.0005  max mem: 15572
[2025-01-15 15:50:31,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8473
[2025-01-15 15:50:31,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:50:31,141] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [  50/2809]  eta: 0:31:43  lr: 0.000028  min_lr: 0.000000  loss: 4.7560 (4.7144)  class_acc: 0.0417 (0.0564)  loss_scale: 65536.0000 (78386.1961)  weight_decay: 0.0500 (0.0500)  time: 0.4877  data: 0.0526  max mem: 15572
Epoch: [3]  [  60/2809]  eta: 0:30:47  lr: 0.000028  min_lr: 0.000000  loss: 4.7818 (4.7190)  class_acc: 0.0417 (0.0526)  loss_scale: 65536.0000 (76279.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1054  max mem: 15572
Epoch: [3]  [  70/2809]  eta: 0:30:06  lr: 0.000028  min_lr: 0.000000  loss: 4.8010 (4.7387)  class_acc: 0.0417 (0.0569)  loss_scale: 65536.0000 (74766.4225)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1280  max mem: 15572
Epoch: [3]  [  80/2809]  eta: 0:30:00  lr: 0.000028  min_lr: 0.000000  loss: 4.7851 (4.7327)  class_acc: 0.0833 (0.0633)  loss_scale: 65536.0000 (73626.8642)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1774  max mem: 15572
Epoch: [3]  [  90/2809]  eta: 0:29:31  lr: 0.000028  min_lr: 0.000000  loss: 4.7272 (4.7364)  class_acc: 0.0417 (0.0627)  loss_scale: 65536.0000 (72737.7582)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1561  max mem: 15572
Epoch: [3]  [ 100/2809]  eta: 0:28:38  lr: 0.000028  min_lr: 0.000000  loss: 4.7744 (4.7418)  class_acc: 0.0417 (0.0644)  loss_scale: 65536.0000 (72024.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0816  max mem: 15572
Epoch: [3]  [ 110/2809]  eta: 0:28:42  lr: 0.000028  min_lr: 0.000000  loss: 4.7405 (4.7463)  class_acc: 0.0417 (0.0627)  loss_scale: 65536.0000 (71440.1441)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1311  max mem: 15572
Epoch: [3]  [ 120/2809]  eta: 0:28:41  lr: 0.000029  min_lr: 0.000000  loss: 4.7645 (4.7516)  class_acc: 0.0417 (0.0610)  loss_scale: 65536.0000 (70952.1983)  weight_decay: 0.0500 (0.0500)  time: 0.6696  data: 0.1998  max mem: 15572
Epoch: [3]  [ 130/2809]  eta: 0:28:23  lr: 0.000029  min_lr: 0.000000  loss: 4.8560 (4.7551)  class_acc: 0.0417 (0.0614)  loss_scale: 65536.0000 (70538.7481)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1670  max mem: 15572
Epoch: [3]  [ 140/2809]  eta: 0:28:04  lr: 0.000029  min_lr: 0.000000  loss: 4.8302 (4.7609)  class_acc: 0.0417 (0.0612)  loss_scale: 65536.0000 (70183.9433)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1350  max mem: 15572
Epoch: [3]  [ 150/2809]  eta: 0:27:46  lr: 0.000029  min_lr: 0.000000  loss: 4.7611 (4.7552)  class_acc: 0.0833 (0.0624)  loss_scale: 65536.0000 (69876.1325)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1267  max mem: 15572
Epoch: [3]  [ 160/2809]  eta: 0:27:39  lr: 0.000029  min_lr: 0.000000  loss: 4.6861 (4.7528)  class_acc: 0.0833 (0.0616)  loss_scale: 65536.0000 (69606.5590)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1355  max mem: 15572
Epoch: [3]  [ 170/2809]  eta: 0:27:27  lr: 0.000029  min_lr: 0.000000  loss: 4.8235 (4.7617)  class_acc: 0.0417 (0.0612)  loss_scale: 65536.0000 (69368.5146)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1498  max mem: 15572
[2025-01-15 15:51:47,984] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:51:47,985] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 180/2809]  eta: 0:27:16  lr: 0.000029  min_lr: 0.000000  loss: 4.8828 (4.7647)  class_acc: 0.0000 (0.0596)  loss_scale: 65536.0000 (71329.2376)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1372  max mem: 15572
[2025-01-15 15:51:53,110] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8610
[2025-01-15 15:51:53,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:51:53,110] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 190/2809]  eta: 0:27:16  lr: 0.000029  min_lr: 0.000000  loss: 4.8319 (4.7661)  class_acc: 0.0417 (0.0604)  loss_scale: 65536.0000 (71712.1675)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1783  max mem: 15572
Epoch: [3]  [ 200/2809]  eta: 0:26:57  lr: 0.000029  min_lr: 0.000000  loss: 4.7086 (4.7647)  class_acc: 0.0833 (0.0620)  loss_scale: 65536.0000 (71404.8955)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1656  max mem: 15572
Epoch: [3]  [ 210/2809]  eta: 0:26:44  lr: 0.000029  min_lr: 0.000000  loss: 4.7348 (4.7646)  class_acc: 0.0833 (0.0622)  loss_scale: 65536.0000 (71126.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1050  max mem: 15572
Epoch: [3]  [ 220/2809]  eta: 0:26:35  lr: 0.000029  min_lr: 0.000000  loss: 4.8110 (4.7693)  class_acc: 0.0417 (0.0611)  loss_scale: 65536.0000 (70873.7738)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1253  max mem: 15572
Epoch: [3]  [ 230/2809]  eta: 0:26:11  lr: 0.000029  min_lr: 0.000000  loss: 4.8775 (4.7692)  class_acc: 0.0417 (0.0611)  loss_scale: 65536.0000 (70642.7013)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0804  max mem: 15572
Epoch: [3]  [ 240/2809]  eta: 0:26:02  lr: 0.000029  min_lr: 0.000000  loss: 4.8742 (4.7740)  class_acc: 0.0417 (0.0602)  loss_scale: 65536.0000 (70430.8050)  weight_decay: 0.0500 (0.0500)  time: 0.5163  data: 0.0767  max mem: 15572
Epoch: [3]  [ 250/2809]  eta: 0:25:52  lr: 0.000029  min_lr: 0.000000  loss: 4.8275 (4.7730)  class_acc: 0.0833 (0.0621)  loss_scale: 65536.0000 (70235.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1230  max mem: 15572
Epoch: [3]  [ 260/2809]  eta: 0:25:49  lr: 0.000029  min_lr: 0.000000  loss: 4.7051 (4.7670)  class_acc: 0.0833 (0.0634)  loss_scale: 65536.0000 (70055.7241)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1657  max mem: 15572
Epoch: [3]  [ 270/2809]  eta: 0:25:39  lr: 0.000029  min_lr: 0.000000  loss: 4.6951 (4.7667)  class_acc: 0.0833 (0.0630)  loss_scale: 65536.0000 (69888.9446)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1651  max mem: 15572
Epoch: [3]  [ 280/2809]  eta: 0:25:37  lr: 0.000029  min_lr: 0.000000  loss: 4.7903 (4.7681)  class_acc: 0.0417 (0.0627)  loss_scale: 65536.0000 (69734.0356)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1625  max mem: 15572
Epoch: [3]  [ 290/2809]  eta: 0:25:28  lr: 0.000029  min_lr: 0.000000  loss: 4.7706 (4.7680)  class_acc: 0.0833 (0.0630)  loss_scale: 65536.0000 (69589.7732)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1539  max mem: 15572
Epoch: [3]  [ 300/2809]  eta: 0:25:19  lr: 0.000029  min_lr: 0.000000  loss: 4.7881 (4.7693)  class_acc: 0.0833 (0.0637)  loss_scale: 65536.0000 (69455.0963)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1023  max mem: 15572
Epoch: [3]  [ 310/2809]  eta: 0:25:16  lr: 0.000029  min_lr: 0.000000  loss: 4.7953 (4.7689)  class_acc: 0.0833 (0.0650)  loss_scale: 65536.0000 (69329.0804)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1441  max mem: 15572
[2025-01-15 15:53:08,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:53:08,005] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:53:10,073] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8744
[2025-01-15 15:53:10,073] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:53:10,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 320/2809]  eta: 0:25:14  lr: 0.000029  min_lr: 0.000000  loss: 4.7853 (4.7695)  class_acc: 0.0833 (0.0652)  loss_scale: 65536.0000 (70231.7259)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.2019  max mem: 15572
Epoch: [3]  [ 330/2809]  eta: 0:25:04  lr: 0.000029  min_lr: 0.000000  loss: 4.7853 (4.7701)  class_acc: 0.0833 (0.0660)  loss_scale: 65536.0000 (70089.8610)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1711  max mem: 15572
Epoch: [3]  [ 340/2809]  eta: 0:24:58  lr: 0.000029  min_lr: 0.000000  loss: 4.7312 (4.7696)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (69956.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1519  max mem: 15572
Epoch: [3]  [ 350/2809]  eta: 0:24:45  lr: 0.000029  min_lr: 0.000000  loss: 4.7064 (4.7677)  class_acc: 0.0417 (0.0652)  loss_scale: 65536.0000 (69830.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1333  max mem: 15572
Epoch: [3]  [ 360/2809]  eta: 0:24:36  lr: 0.000029  min_lr: 0.000000  loss: 4.8269 (4.7690)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (69711.4238)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.1040  max mem: 15572
Epoch: [3]  [ 370/2809]  eta: 0:24:35  lr: 0.000029  min_lr: 0.000000  loss: 4.8573 (4.7700)  class_acc: 0.0833 (0.0651)  loss_scale: 65536.0000 (69598.8787)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1829  max mem: 15572
Epoch: [3]  [ 380/2809]  eta: 0:24:28  lr: 0.000029  min_lr: 0.000000  loss: 4.8098 (4.7679)  class_acc: 0.0417 (0.0647)  loss_scale: 65536.0000 (69492.2415)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.2033  max mem: 15572
Epoch: [3]  [ 390/2809]  eta: 0:24:15  lr: 0.000029  min_lr: 0.000000  loss: 4.7012 (4.7666)  class_acc: 0.0417 (0.0649)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1112  max mem: 15572
Epoch: [3]  [ 400/2809]  eta: 0:24:06  lr: 0.000029  min_lr: 0.000000  loss: 4.7025 (4.7683)  class_acc: 0.0417 (0.0645)  loss_scale: 65536.0000 (69294.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0927  max mem: 15572
Epoch: [3]  [ 410/2809]  eta: 0:23:55  lr: 0.000029  min_lr: 0.000000  loss: 4.8165 (4.7692)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (69203.4647)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0925  max mem: 15572
Epoch: [3]  [ 420/2809]  eta: 0:23:43  lr: 0.000030  min_lr: 0.000000  loss: 4.7846 (4.7709)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (69116.3515)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0534  max mem: 15572
Epoch: [3]  [ 430/2809]  eta: 0:23:34  lr: 0.000030  min_lr: 0.000000  loss: 4.7453 (4.7700)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (69033.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0755  max mem: 15572
Epoch: [3]  [ 440/2809]  eta: 0:23:29  lr: 0.000030  min_lr: 0.000000  loss: 4.7303 (4.7684)  class_acc: 0.0417 (0.0641)  loss_scale: 65536.0000 (68953.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1129  max mem: 15572
[2025-01-15 15:54:24,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:54:24,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 450/2809]  eta: 0:23:26  lr: 0.000030  min_lr: 0.000000  loss: 4.7820 (4.7694)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (69604.7539)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1639  max mem: 15572
[2025-01-15 15:54:29,273] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8881
[2025-01-15 15:54:29,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:54:29,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 460/2809]  eta: 0:23:16  lr: 0.000030  min_lr: 0.000000  loss: 4.7381 (4.7695)  class_acc: 0.0000 (0.0631)  loss_scale: 65536.0000 (69942.9761)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1290  max mem: 15572
Epoch: [3]  [ 470/2809]  eta: 0:23:07  lr: 0.000030  min_lr: 0.000000  loss: 4.7231 (4.7676)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (69849.4098)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0892  max mem: 15572
Epoch: [3]  [ 480/2809]  eta: 0:22:57  lr: 0.000030  min_lr: 0.000000  loss: 4.7928 (4.7665)  class_acc: 0.0833 (0.0637)  loss_scale: 65536.0000 (69759.7339)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0894  max mem: 15572
Epoch: [3]  [ 490/2809]  eta: 0:22:50  lr: 0.000030  min_lr: 0.000000  loss: 4.7162 (4.7653)  class_acc: 0.0833 (0.0639)  loss_scale: 65536.0000 (69673.7108)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0957  max mem: 15572
Epoch: [3]  [ 500/2809]  eta: 0:22:48  lr: 0.000030  min_lr: 0.000000  loss: 4.7162 (4.7651)  class_acc: 0.0417 (0.0635)  loss_scale: 65536.0000 (69591.1218)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1931  max mem: 15572
Epoch: [3]  [ 510/2809]  eta: 0:22:40  lr: 0.000030  min_lr: 0.000000  loss: 4.8071 (4.7643)  class_acc: 0.0417 (0.0637)  loss_scale: 65536.0000 (69511.7652)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1870  max mem: 15572
Epoch: [3]  [ 520/2809]  eta: 0:22:40  lr: 0.000030  min_lr: 0.000000  loss: 4.7685 (4.7626)  class_acc: 0.0833 (0.0640)  loss_scale: 65536.0000 (69435.4549)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.2025  max mem: 15572
Epoch: [3]  [ 530/2809]  eta: 0:22:35  lr: 0.000030  min_lr: 0.000000  loss: 4.6845 (4.7620)  class_acc: 0.0833 (0.0640)  loss_scale: 65536.0000 (69362.0188)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.2058  max mem: 15572
Epoch: [3]  [ 540/2809]  eta: 0:22:28  lr: 0.000030  min_lr: 0.000000  loss: 4.7246 (4.7633)  class_acc: 0.0417 (0.0641)  loss_scale: 65536.0000 (69291.2976)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1227  max mem: 15572
Epoch: [3]  [ 550/2809]  eta: 0:22:18  lr: 0.000030  min_lr: 0.000000  loss: 4.7605 (4.7619)  class_acc: 0.0417 (0.0639)  loss_scale: 65536.0000 (69223.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0773  max mem: 15572
Epoch: [3]  [ 560/2809]  eta: 0:22:12  lr: 0.000030  min_lr: 0.000000  loss: 4.7426 (4.7621)  class_acc: 0.0417 (0.0642)  loss_scale: 65536.0000 (69157.4189)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1093  max mem: 15572
Epoch: [3]  [ 570/2809]  eta: 0:22:06  lr: 0.000030  min_lr: 0.000000  loss: 4.7426 (4.7603)  class_acc: 0.0833 (0.0651)  loss_scale: 65536.0000 (69093.9965)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1578  max mem: 15572
[2025-01-15 15:55:37,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=50, lr=[2.9101758468449346e-07, 2.9101758468449346e-07, 4.157394066921336e-07, 4.157394066921336e-07, 5.939134381316195e-07, 5.939134381316195e-07, 8.484477687594565e-07, 8.484477687594565e-07, 1.2120682410849378e-06, 1.2120682410849378e-06, 1.7315260586927683e-06, 1.7315260586927683e-06, 2.4736086552753836e-06, 2.4736086552753836e-06, 3.5337266503934053e-06, 3.5337266503934053e-06, 5.0481809291334365e-06, 5.0481809291334365e-06, 7.211687041619195e-06, 7.211687041619195e-06, 1.0302410059455993e-05, 1.0302410059455993e-05, 1.4717728656365707e-05, 1.4717728656365707e-05, 2.102532665195101e-05, 2.102532665195101e-05, 3.0036180931358588e-05, 3.0036180931358588e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 15:55:37,169] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=28.48905914958158, CurrSamplesPerSec=31.380828456672432, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [ 580/2809]  eta: 0:22:02  lr: 0.000030  min_lr: 0.000000  loss: 4.6351 (4.7585)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (69032.7573)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1678  max mem: 15572
[2025-01-15 15:55:44,133] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:55:44,133] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [ 590/2809]  eta: 0:21:59  lr: 0.000030  min_lr: 0.000000  loss: 4.6994 (4.7584)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (69860.7107)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.2153  max mem: 15572
Epoch: [3]  [ 600/2809]  eta: 0:21:52  lr: 0.000030  min_lr: 0.000000  loss: 4.7398 (4.7590)  class_acc: 0.0833 (0.0657)  loss_scale: 131072.0000 (70879.2013)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1638  max mem: 15572
[2025-01-15 15:55:59,669] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9034
[2025-01-15 15:55:59,669] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:55:59,669] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 610/2809]  eta: 0:21:46  lr: 0.000030  min_lr: 0.000000  loss: 4.7244 (4.7577)  class_acc: 0.0417 (0.0653)  loss_scale: 131072.0000 (71435.3126)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1361  max mem: 15572
Epoch: [3]  [ 620/2809]  eta: 0:21:39  lr: 0.000030  min_lr: 0.000000  loss: 4.7601 (4.7580)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (71340.3156)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1614  max mem: 15572
Epoch: [3]  [ 630/2809]  eta: 0:21:34  lr: 0.000030  min_lr: 0.000000  loss: 4.7777 (4.7586)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (71248.3296)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1485  max mem: 15572
Epoch: [3]  [ 640/2809]  eta: 0:21:33  lr: 0.000030  min_lr: 0.000000  loss: 4.7062 (4.7579)  class_acc: 0.0833 (0.0655)  loss_scale: 65536.0000 (71159.2137)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.2154  max mem: 15572
Epoch: [3]  [ 650/2809]  eta: 0:21:23  lr: 0.000030  min_lr: 0.000000  loss: 4.7043 (4.7571)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (71072.8356)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1514  max mem: 15572
Epoch: [3]  [ 660/2809]  eta: 0:21:17  lr: 0.000030  min_lr: 0.000000  loss: 4.7099 (4.7575)  class_acc: 0.0417 (0.0652)  loss_scale: 65536.0000 (70989.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0829  max mem: 15572
Epoch: [3]  [ 670/2809]  eta: 0:21:12  lr: 0.000030  min_lr: 0.000000  loss: 4.7438 (4.7568)  class_acc: 0.0417 (0.0651)  loss_scale: 65536.0000 (70907.8033)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1775  max mem: 15572
Epoch: [3]  [ 680/2809]  eta: 0:21:02  lr: 0.000030  min_lr: 0.000000  loss: 4.7734 (4.7575)  class_acc: 0.0833 (0.0650)  loss_scale: 65536.0000 (70828.9222)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1175  max mem: 15572
Epoch: [3]  [ 690/2809]  eta: 0:20:59  lr: 0.000030  min_lr: 0.000000  loss: 4.7356 (4.7560)  class_acc: 0.0833 (0.0660)  loss_scale: 65536.0000 (70752.3242)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1267  max mem: 15572
Epoch: [3]  [ 700/2809]  eta: 0:20:53  lr: 0.000030  min_lr: 0.000000  loss: 4.6491 (4.7554)  class_acc: 0.0833 (0.0659)  loss_scale: 65536.0000 (70677.9116)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1995  max mem: 15572
Epoch: [3]  [ 710/2809]  eta: 0:20:46  lr: 0.000030  min_lr: 0.000000  loss: 4.6537 (4.7552)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (70605.5921)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1349  max mem: 15572
Epoch: [3]  [ 720/2809]  eta: 0:20:36  lr: 0.000031  min_lr: 0.000000  loss: 4.7598 (4.7557)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (70535.2788)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0640  max mem: 15572
Epoch: [3]  [ 730/2809]  eta: 0:20:30  lr: 0.000031  min_lr: 0.000000  loss: 4.7983 (4.7563)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70466.8892)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0815  max mem: 15572
[2025-01-15 15:57:14,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:57:14,535] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:57:15,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9165
[2025-01-15 15:57:15,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:57:15,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 740/2809]  eta: 0:20:23  lr: 0.000031  min_lr: 0.000000  loss: 4.7983 (4.7563)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70577.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1076  max mem: 15572
Epoch: [3]  [ 750/2809]  eta: 0:20:19  lr: 0.000031  min_lr: 0.000000  loss: 4.7699 (4.7565)  class_acc: 0.0000 (0.0654)  loss_scale: 65536.0000 (70510.1039)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1423  max mem: 15572
Epoch: [3]  [ 760/2809]  eta: 0:20:11  lr: 0.000031  min_lr: 0.000000  loss: 4.8033 (4.7564)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70444.7411)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1262  max mem: 15572
Epoch: [3]  [ 770/2809]  eta: 0:20:06  lr: 0.000031  min_lr: 0.000000  loss: 4.7925 (4.7560)  class_acc: 0.0833 (0.0659)  loss_scale: 65536.0000 (70381.0739)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0961  max mem: 15572
Epoch: [3]  [ 780/2809]  eta: 0:19:58  lr: 0.000031  min_lr: 0.000000  loss: 4.6735 (4.7549)  class_acc: 0.0833 (0.0664)  loss_scale: 65536.0000 (70319.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0877  max mem: 15572
Epoch: [3]  [ 790/2809]  eta: 0:19:51  lr: 0.000031  min_lr: 0.000000  loss: 4.6735 (4.7540)  class_acc: 0.0417 (0.0663)  loss_scale: 65536.0000 (70258.5689)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0736  max mem: 15572
Epoch: [3]  [ 800/2809]  eta: 0:19:48  lr: 0.000031  min_lr: 0.000000  loss: 4.7139 (4.7536)  class_acc: 0.0417 (0.0663)  loss_scale: 65536.0000 (70199.6105)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1604  max mem: 15572
Epoch: [3]  [ 810/2809]  eta: 0:19:40  lr: 0.000031  min_lr: 0.000000  loss: 4.6795 (4.7529)  class_acc: 0.0833 (0.0664)  loss_scale: 65536.0000 (70142.1060)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1527  max mem: 15572
Epoch: [3]  [ 820/2809]  eta: 0:19:34  lr: 0.000031  min_lr: 0.000000  loss: 4.6684 (4.7525)  class_acc: 0.0833 (0.0661)  loss_scale: 65536.0000 (70086.0024)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1027  max mem: 15572
Epoch: [3]  [ 830/2809]  eta: 0:19:29  lr: 0.000031  min_lr: 0.000000  loss: 4.6659 (4.7513)  class_acc: 0.0000 (0.0657)  loss_scale: 65536.0000 (70031.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1485  max mem: 15572
Epoch: [3]  [ 840/2809]  eta: 0:19:23  lr: 0.000031  min_lr: 0.000000  loss: 4.6690 (4.7503)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (69977.7979)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1731  max mem: 15572
Epoch: [3]  [ 850/2809]  eta: 0:19:17  lr: 0.000031  min_lr: 0.000000  loss: 4.7037 (4.7505)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (69925.6028)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1518  max mem: 15572
Epoch: [3]  [ 860/2809]  eta: 0:19:13  lr: 0.000031  min_lr: 0.000000  loss: 4.5973 (4.7494)  class_acc: 0.0417 (0.0658)  loss_scale: 65536.0000 (69874.6202)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1831  max mem: 15572
[2025-01-15 15:58:32,443] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:58:32,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 15:58:33,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9296
[2025-01-15 15:58:33,252] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 15:58:33,252] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [ 870/2809]  eta: 0:19:07  lr: 0.000031  min_lr: 0.000000  loss: 4.6245 (4.7484)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (69975.2928)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1837  max mem: 15572
Epoch: [3]  [ 880/2809]  eta: 0:19:01  lr: 0.000031  min_lr: 0.000000  loss: 4.6917 (4.7482)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (69924.9035)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1415  max mem: 15572
Epoch: [3]  [ 890/2809]  eta: 0:18:57  lr: 0.000031  min_lr: 0.000000  loss: 4.6410 (4.7475)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (69875.6453)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1512  max mem: 15572
Epoch: [3]  [ 900/2809]  eta: 0:18:50  lr: 0.000031  min_lr: 0.000000  loss: 4.6049 (4.7463)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (69827.4806)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1260  max mem: 15572
Epoch: [3]  [ 910/2809]  eta: 0:18:45  lr: 0.000031  min_lr: 0.000000  loss: 4.7045 (4.7465)  class_acc: 0.0833 (0.0652)  loss_scale: 65536.0000 (69780.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1328  max mem: 15572
Epoch: [3]  [ 920/2809]  eta: 0:18:38  lr: 0.000031  min_lr: 0.000000  loss: 4.7074 (4.7461)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (69734.2888)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1496  max mem: 15572
Epoch: [3]  [ 930/2809]  eta: 0:18:32  lr: 0.000031  min_lr: 0.000000  loss: 4.7074 (4.7452)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (69689.1944)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1153  max mem: 15572
Epoch: [3]  [ 940/2809]  eta: 0:18:24  lr: 0.000031  min_lr: 0.000000  loss: 4.7283 (4.7458)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (69645.0584)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0995  max mem: 15572
Epoch: [3]  [ 950/2809]  eta: 0:18:17  lr: 0.000031  min_lr: 0.000000  loss: 4.7557 (4.7462)  class_acc: 0.0000 (0.0652)  loss_scale: 65536.0000 (69601.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0849  max mem: 15572
Epoch: [3]  [ 960/2809]  eta: 0:18:12  lr: 0.000031  min_lr: 0.000000  loss: 4.7169 (4.7459)  class_acc: 0.0417 (0.0653)  loss_scale: 65536.0000 (69559.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1284  max mem: 15572
Epoch: [3]  [ 970/2809]  eta: 0:18:05  lr: 0.000031  min_lr: 0.000000  loss: 4.7271 (4.7459)  class_acc: 0.0833 (0.0655)  loss_scale: 65536.0000 (69518.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1432  max mem: 15572
Epoch: [3]  [ 980/2809]  eta: 0:18:00  lr: 0.000031  min_lr: 0.000000  loss: 4.6833 (4.7450)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (69477.5127)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1196  max mem: 15572
Epoch: [3]  [ 990/2809]  eta: 0:17:56  lr: 0.000031  min_lr: 0.000000  loss: 4.6662 (4.7446)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (69437.7397)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1535  max mem: 15572
[2025-01-15 15:59:50,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 15:59:50,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1000/2809]  eta: 0:17:52  lr: 0.000031  min_lr: 0.000000  loss: 4.6662 (4.7440)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (69595.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6831  data: 0.2204  max mem: 15572
Epoch: [3]  [1010/2809]  eta: 0:17:45  lr: 0.000031  min_lr: 0.000000  loss: 4.7110 (4.7440)  class_acc: 0.0417 (0.0656)  loss_scale: 131072.0000 (70203.2522)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.1764  max mem: 15572
Epoch: [3]  [1020/2809]  eta: 0:17:40  lr: 0.000032  min_lr: 0.000000  loss: 4.6920 (4.7441)  class_acc: 0.0833 (0.0658)  loss_scale: 131072.0000 (70799.4202)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1234  max mem: 15572
Epoch: [3]  [1030/2809]  eta: 0:17:33  lr: 0.000032  min_lr: 0.000000  loss: 4.6703 (4.7434)  class_acc: 0.0833 (0.0658)  loss_scale: 131072.0000 (71384.0233)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1223  max mem: 15572
[2025-01-15 16:00:10,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9460
[2025-01-15 16:00:10,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:00:10,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1040/2809]  eta: 0:17:26  lr: 0.000032  min_lr: 0.000000  loss: 4.6150 (4.7425)  class_acc: 0.0417 (0.0657)  loss_scale: 131072.0000 (71453.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0882  max mem: 15572
Epoch: [3]  [1050/2809]  eta: 0:17:19  lr: 0.000032  min_lr: 0.000000  loss: 4.6411 (4.7429)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (71397.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0969  max mem: 15572
Epoch: [3]  [1060/2809]  eta: 0:17:13  lr: 0.000032  min_lr: 0.000000  loss: 4.7886 (4.7437)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (71342.2055)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1107  max mem: 15572
Epoch: [3]  [1070/2809]  eta: 0:17:07  lr: 0.000032  min_lr: 0.000000  loss: 4.7858 (4.7435)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (71287.9925)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1206  max mem: 15572
Epoch: [3]  [1080/2809]  eta: 0:17:01  lr: 0.000032  min_lr: 0.000000  loss: 4.7067 (4.7434)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (71234.7826)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1255  max mem: 15572
Epoch: [3]  [1090/2809]  eta: 0:16:55  lr: 0.000032  min_lr: 0.000000  loss: 4.7067 (4.7432)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (71182.5481)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1301  max mem: 15572
Epoch: [3]  [1100/2809]  eta: 0:16:49  lr: 0.000032  min_lr: 0.000000  loss: 4.6625 (4.7424)  class_acc: 0.0833 (0.0655)  loss_scale: 65536.0000 (71131.2625)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1149  max mem: 15572
Epoch: [3]  [1110/2809]  eta: 0:16:43  lr: 0.000032  min_lr: 0.000000  loss: 4.6625 (4.7423)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (71080.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1174  max mem: 15572
Epoch: [3]  [1120/2809]  eta: 0:16:37  lr: 0.000032  min_lr: 0.000000  loss: 4.7219 (4.7421)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (71031.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1529  max mem: 15572
Epoch: [3]  [1130/2809]  eta: 0:16:30  lr: 0.000032  min_lr: 0.000000  loss: 4.7297 (4.7415)  class_acc: 0.0833 (0.0655)  loss_scale: 65536.0000 (70982.8470)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1386  max mem: 15572
Epoch: [3]  [1140/2809]  eta: 0:16:24  lr: 0.000032  min_lr: 0.000000  loss: 4.7692 (4.7421)  class_acc: 0.0833 (0.0654)  loss_scale: 65536.0000 (70935.1096)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0803  max mem: 15572
Epoch: [3]  [1150/2809]  eta: 0:16:18  lr: 0.000032  min_lr: 0.000000  loss: 4.7820 (4.7420)  class_acc: 0.0000 (0.0655)  loss_scale: 65536.0000 (70888.2016)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0985  max mem: 15572
Epoch: [3]  [1160/2809]  eta: 0:16:10  lr: 0.000032  min_lr: 0.000000  loss: 4.7042 (4.7412)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (70842.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0896  max mem: 15572
[2025-01-15 16:01:23,295] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:01:23,296] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:01:26,728] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9595
[2025-01-15 16:01:26,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:01:26,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1170/2809]  eta: 0:16:05  lr: 0.000032  min_lr: 0.000000  loss: 4.6789 (4.7406)  class_acc: 0.0833 (0.0656)  loss_scale: 65536.0000 (71132.5841)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0972  max mem: 15572
Epoch: [3]  [1180/2809]  eta: 0:15:59  lr: 0.000032  min_lr: 0.000000  loss: 4.7683 (4.7405)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (71085.1956)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1845  max mem: 15572
Epoch: [3]  [1190/2809]  eta: 0:15:53  lr: 0.000032  min_lr: 0.000000  loss: 4.7131 (4.7398)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (71038.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1382  max mem: 15572
Epoch: [3]  [1200/2809]  eta: 0:15:46  lr: 0.000032  min_lr: 0.000000  loss: 4.7131 (4.7393)  class_acc: 0.0833 (0.0662)  loss_scale: 65536.0000 (70992.7860)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0790  max mem: 15572
Epoch: [3]  [1210/2809]  eta: 0:15:41  lr: 0.000032  min_lr: 0.000000  loss: 4.7500 (4.7397)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (70947.7258)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1664  max mem: 15572
Epoch: [3]  [1220/2809]  eta: 0:15:34  lr: 0.000032  min_lr: 0.000000  loss: 4.7248 (4.7394)  class_acc: 0.0417 (0.0660)  loss_scale: 65536.0000 (70903.4038)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1567  max mem: 15572
Epoch: [3]  [1230/2809]  eta: 0:15:29  lr: 0.000032  min_lr: 0.000000  loss: 4.7524 (4.7397)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (70859.8018)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1478  max mem: 15572
Epoch: [3]  [1240/2809]  eta: 0:15:23  lr: 0.000032  min_lr: 0.000000  loss: 4.7875 (4.7406)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (70816.9025)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1805  max mem: 15572
Epoch: [3]  [1250/2809]  eta: 0:15:17  lr: 0.000032  min_lr: 0.000000  loss: 4.7642 (4.7411)  class_acc: 0.0417 (0.0658)  loss_scale: 65536.0000 (70774.6890)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1439  max mem: 15572
Epoch: [3]  [1260/2809]  eta: 0:15:11  lr: 0.000032  min_lr: 0.000000  loss: 4.7642 (4.7416)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (70733.1451)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1248  max mem: 15572
Epoch: [3]  [1270/2809]  eta: 0:15:05  lr: 0.000032  min_lr: 0.000000  loss: 4.6796 (4.7407)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (70692.2549)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0987  max mem: 15572
Epoch: [3]  [1280/2809]  eta: 0:14:57  lr: 0.000032  min_lr: 0.000000  loss: 4.7832 (4.7420)  class_acc: 0.0417 (0.0655)  loss_scale: 65536.0000 (70652.0031)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0536  max mem: 15572
Epoch: [3]  [1290/2809]  eta: 0:14:52  lr: 0.000032  min_lr: 0.000000  loss: 4.8774 (4.7419)  class_acc: 0.0417 (0.0657)  loss_scale: 65536.0000 (70612.3749)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0995  max mem: 15572
[2025-01-15 16:02:41,915] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:02:41,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:02:43,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9727
[2025-01-15 16:02:43,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:02:43,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1300/2809]  eta: 0:14:47  lr: 0.000032  min_lr: 0.000000  loss: 4.7162 (4.7415)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70724.4766)  weight_decay: 0.0500 (0.0500)  time: 0.6406  data: 0.1952  max mem: 15572
Epoch: [3]  [1310/2809]  eta: 0:14:41  lr: 0.000032  min_lr: 0.000000  loss: 4.7598 (4.7417)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70684.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1629  max mem: 15572
Epoch: [3]  [1320/2809]  eta: 0:14:34  lr: 0.000033  min_lr: 0.000000  loss: 4.7241 (4.7410)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70645.9228)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1204  max mem: 15572
Epoch: [3]  [1330/2809]  eta: 0:14:27  lr: 0.000033  min_lr: 0.000000  loss: 4.7165 (4.7415)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70607.5312)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0629  max mem: 15572
Epoch: [3]  [1340/2809]  eta: 0:14:21  lr: 0.000033  min_lr: 0.000000  loss: 4.7689 (4.7411)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70569.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0764  max mem: 15572
Epoch: [3]  [1350/2809]  eta: 0:14:16  lr: 0.000033  min_lr: 0.000000  loss: 4.7748 (4.7419)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70532.4530)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1513  max mem: 15572
Epoch: [3]  [1360/2809]  eta: 0:14:10  lr: 0.000033  min_lr: 0.000000  loss: 4.7781 (4.7421)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70495.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1318  max mem: 15572
Epoch: [3]  [1370/2809]  eta: 0:14:04  lr: 0.000033  min_lr: 0.000000  loss: 4.7632 (4.7425)  class_acc: 0.0417 (0.0656)  loss_scale: 65536.0000 (70459.5653)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1087  max mem: 15572
Epoch: [3]  [1380/2809]  eta: 0:13:59  lr: 0.000033  min_lr: 0.000000  loss: 4.8336 (4.7427)  class_acc: 0.0417 (0.0654)  loss_scale: 65536.0000 (70423.9131)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1623  max mem: 15572
Epoch: [3]  [1390/2809]  eta: 0:13:53  lr: 0.000033  min_lr: 0.000000  loss: 4.6010 (4.7413)  class_acc: 0.0417 (0.0658)  loss_scale: 65536.0000 (70388.7735)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1483  max mem: 15572
Epoch: [3]  [1400/2809]  eta: 0:13:47  lr: 0.000033  min_lr: 0.000000  loss: 4.5919 (4.7409)  class_acc: 0.0833 (0.0658)  loss_scale: 65536.0000 (70354.1356)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1282  max mem: 15572
Epoch: [3]  [1410/2809]  eta: 0:13:41  lr: 0.000033  min_lr: 0.000000  loss: 4.6710 (4.7407)  class_acc: 0.0833 (0.0660)  loss_scale: 65536.0000 (70319.9887)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1534  max mem: 15572
Epoch: [3]  [1420/2809]  eta: 0:13:35  lr: 0.000033  min_lr: 0.000000  loss: 4.7052 (4.7408)  class_acc: 0.0417 (0.0659)  loss_scale: 65536.0000 (70286.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1286  max mem: 15572
[2025-01-15 16:03:58,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:03:58,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1430/2809]  eta: 0:13:29  lr: 0.000033  min_lr: 0.000000  loss: 4.7189 (4.7406)  class_acc: 0.0417 (0.0661)  loss_scale: 65536.0000 (70344.7212)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1449  max mem: 15572
[2025-01-15 16:04:03,948] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9865
[2025-01-15 16:04:03,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:04:03,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1440/2809]  eta: 0:13:24  lr: 0.000033  min_lr: 0.000000  loss: 4.7159 (4.7402)  class_acc: 0.0833 (0.0662)  loss_scale: 65536.0000 (70629.7071)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1658  max mem: 15572
Epoch: [3]  [1450/2809]  eta: 0:13:18  lr: 0.000033  min_lr: 0.000000  loss: 4.6800 (4.7399)  class_acc: 0.0833 (0.0663)  loss_scale: 65536.0000 (70594.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1522  max mem: 15572
Epoch: [3]  [1460/2809]  eta: 0:13:12  lr: 0.000033  min_lr: 0.000000  loss: 4.6800 (4.7396)  class_acc: 0.0417 (0.0664)  loss_scale: 65536.0000 (70559.9781)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1549  max mem: 15572
Epoch: [3]  [1470/2809]  eta: 0:13:07  lr: 0.000033  min_lr: 0.000000  loss: 4.6786 (4.7393)  class_acc: 0.0417 (0.0664)  loss_scale: 65536.0000 (70525.8246)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1989  max mem: 15572
Epoch: [3]  [1480/2809]  eta: 0:13:01  lr: 0.000033  min_lr: 0.000000  loss: 4.6476 (4.7389)  class_acc: 0.0833 (0.0666)  loss_scale: 65536.0000 (70492.1323)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.2099  max mem: 15572
Epoch: [3]  [1490/2809]  eta: 0:12:56  lr: 0.000033  min_lr: 0.000000  loss: 4.6849 (4.7391)  class_acc: 0.0833 (0.0666)  loss_scale: 65536.0000 (70458.8920)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1963  max mem: 15572
Epoch: [3]  [1500/2809]  eta: 0:12:50  lr: 0.000033  min_lr: 0.000000  loss: 4.6558 (4.7380)  class_acc: 0.0833 (0.0668)  loss_scale: 65536.0000 (70426.0946)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1818  max mem: 15572
Epoch: [3]  [1510/2809]  eta: 0:12:44  lr: 0.000033  min_lr: 0.000000  loss: 4.5943 (4.7375)  class_acc: 0.0833 (0.0670)  loss_scale: 65536.0000 (70393.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1082  max mem: 15572
Epoch: [3]  [1520/2809]  eta: 0:12:39  lr: 0.000033  min_lr: 0.000000  loss: 4.6957 (4.7374)  class_acc: 0.0833 (0.0671)  loss_scale: 65536.0000 (70361.7936)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1253  max mem: 15572
Epoch: [3]  [1530/2809]  eta: 0:12:33  lr: 0.000033  min_lr: 0.000000  loss: 4.6951 (4.7363)  class_acc: 0.0833 (0.0674)  loss_scale: 65536.0000 (70330.2730)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1682  max mem: 15572
Epoch: [3]  [1540/2809]  eta: 0:12:27  lr: 0.000033  min_lr: 0.000000  loss: 4.6176 (4.7359)  class_acc: 0.0833 (0.0674)  loss_scale: 65536.0000 (70299.1616)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1516  max mem: 15572
Epoch: [3]  [1550/2809]  eta: 0:12:20  lr: 0.000033  min_lr: 0.000000  loss: 4.6924 (4.7354)  class_acc: 0.0417 (0.0676)  loss_scale: 65536.0000 (70268.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1235  max mem: 15572
Epoch: [3]  [1560/2809]  eta: 0:12:15  lr: 0.000033  min_lr: 0.000000  loss: 4.6532 (4.7347)  class_acc: 0.0833 (0.0679)  loss_scale: 65536.0000 (70238.1345)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1108  max mem: 15572
[2025-01-15 16:05:22,314] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:05:22,314] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1570/2809]  eta: 0:12:09  lr: 0.000033  min_lr: 0.000000  loss: 4.6146 (4.7338)  class_acc: 0.0833 (0.0681)  loss_scale: 65536.0000 (70375.0681)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1752  max mem: 15572
[2025-01-15 16:05:24,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=57, lr=[3.233564650805923e-07, 3.233564650805923e-07, 4.6193780725798904e-07, 4.6193780725798904e-07, 6.599111532256987e-07, 6.599111532256987e-07, 9.427302188938554e-07, 9.427302188938554e-07, 1.3467574555626506e-06, 1.3467574555626506e-06, 1.923939222232358e-06, 1.923939222232358e-06, 2.748484603189083e-06, 2.748484603189083e-06, 3.926406575984405e-06, 3.926406575984405e-06, 5.609152251406292e-06, 5.609152251406292e-06, 8.013074644866134e-06, 8.013074644866134e-06, 1.1447249492665904e-05, 1.1447249492665904e-05, 1.6353213560951293e-05, 1.6353213560951293e-05, 2.336173365850185e-05, 2.336173365850185e-05, 3.3373905226431215e-05, 3.3373905226431215e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:05:24,717] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=28.465473737941533, CurrSamplesPerSec=20.603076892848268, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-15 16:05:27,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10004
[2025-01-15 16:05:27,246] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:05:27,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1580/2809]  eta: 0:12:03  lr: 0.000033  min_lr: 0.000000  loss: 4.6232 (4.7338)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (70593.1739)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1285  max mem: 15572
Epoch: [3]  [1590/2809]  eta: 0:11:57  lr: 0.000033  min_lr: 0.000000  loss: 4.6833 (4.7336)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (70561.3878)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1015  max mem: 15572
Epoch: [3]  [1600/2809]  eta: 0:11:51  lr: 0.000033  min_lr: 0.000000  loss: 4.7120 (4.7338)  class_acc: 0.0417 (0.0678)  loss_scale: 65536.0000 (70529.9988)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1135  max mem: 15572
Epoch: [3]  [1610/2809]  eta: 0:11:45  lr: 0.000034  min_lr: 0.000000  loss: 4.7218 (4.7337)  class_acc: 0.0417 (0.0678)  loss_scale: 65536.0000 (70498.9994)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1019  max mem: 15572
Epoch: [3]  [1620/2809]  eta: 0:11:39  lr: 0.000034  min_lr: 0.000000  loss: 4.7318 (4.7336)  class_acc: 0.0417 (0.0678)  loss_scale: 65536.0000 (70468.3825)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1685  max mem: 15572
Epoch: [3]  [1630/2809]  eta: 0:11:34  lr: 0.000034  min_lr: 0.000000  loss: 4.7645 (4.7337)  class_acc: 0.0417 (0.0677)  loss_scale: 65536.0000 (70438.1410)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.2076  max mem: 15572
Epoch: [3]  [1640/2809]  eta: 0:11:27  lr: 0.000034  min_lr: 0.000000  loss: 4.6583 (4.7331)  class_acc: 0.0833 (0.0680)  loss_scale: 65536.0000 (70408.2681)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1427  max mem: 15572
Epoch: [3]  [1650/2809]  eta: 0:11:22  lr: 0.000034  min_lr: 0.000000  loss: 4.6583 (4.7330)  class_acc: 0.0417 (0.0679)  loss_scale: 65536.0000 (70378.7571)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0909  max mem: 15572
Epoch: [3]  [1660/2809]  eta: 0:11:16  lr: 0.000034  min_lr: 0.000000  loss: 4.7281 (4.7337)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (70349.6014)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1108  max mem: 15572
Epoch: [3]  [1670/2809]  eta: 0:11:10  lr: 0.000034  min_lr: 0.000000  loss: 4.7243 (4.7330)  class_acc: 0.0833 (0.0681)  loss_scale: 65536.0000 (70320.7947)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1221  max mem: 15572
Epoch: [3]  [1680/2809]  eta: 0:11:03  lr: 0.000034  min_lr: 0.000000  loss: 4.7029 (4.7333)  class_acc: 0.0833 (0.0680)  loss_scale: 65536.0000 (70292.3308)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0993  max mem: 15572
Epoch: [3]  [1690/2809]  eta: 0:10:57  lr: 0.000034  min_lr: 0.000000  loss: 4.7029 (4.7329)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (70264.2034)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0835  max mem: 15572
Epoch: [3]  [1700/2809]  eta: 0:10:51  lr: 0.000034  min_lr: 0.000000  loss: 4.7379 (4.7331)  class_acc: 0.0833 (0.0680)  loss_scale: 65536.0000 (70236.4068)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1178  max mem: 15572
[2025-01-15 16:06:40,828] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:06:40,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1710/2809]  eta: 0:10:45  lr: 0.000034  min_lr: 0.000000  loss: 4.7444 (4.7324)  class_acc: 0.0833 (0.0683)  loss_scale: 65536.0000 (70400.4489)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1587  max mem: 15572
[2025-01-15 16:06:47,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10144
[2025-01-15 16:06:47,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:06:47,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1720/2809]  eta: 0:10:39  lr: 0.000034  min_lr: 0.000000  loss: 4.6949 (4.7326)  class_acc: 0.0417 (0.0682)  loss_scale: 131072.0000 (70600.6647)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0968  max mem: 15572
Epoch: [3]  [1730/2809]  eta: 0:10:33  lr: 0.000034  min_lr: 0.000000  loss: 4.7513 (4.7327)  class_acc: 0.0417 (0.0680)  loss_scale: 65536.0000 (70571.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0525  max mem: 15572
Epoch: [3]  [1740/2809]  eta: 0:10:27  lr: 0.000034  min_lr: 0.000000  loss: 4.6531 (4.7320)  class_acc: 0.0833 (0.0683)  loss_scale: 65536.0000 (70542.4836)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1557  max mem: 15572
Epoch: [3]  [1750/2809]  eta: 0:10:21  lr: 0.000034  min_lr: 0.000000  loss: 4.5826 (4.7316)  class_acc: 0.0833 (0.0685)  loss_scale: 65536.0000 (70513.8915)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1464  max mem: 15572
Epoch: [3]  [1760/2809]  eta: 0:10:15  lr: 0.000034  min_lr: 0.000000  loss: 4.5826 (4.7305)  class_acc: 0.0833 (0.0687)  loss_scale: 65536.0000 (70485.6241)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1128  max mem: 15572
Epoch: [3]  [1770/2809]  eta: 0:10:09  lr: 0.000034  min_lr: 0.000000  loss: 4.6699 (4.7305)  class_acc: 0.0417 (0.0685)  loss_scale: 65536.0000 (70457.6759)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1045  max mem: 15572
Epoch: [3]  [1780/2809]  eta: 0:10:03  lr: 0.000034  min_lr: 0.000000  loss: 4.7482 (4.7310)  class_acc: 0.0417 (0.0686)  loss_scale: 65536.0000 (70430.0415)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1035  max mem: 15572
Epoch: [3]  [1790/2809]  eta: 0:09:57  lr: 0.000034  min_lr: 0.000000  loss: 4.7482 (4.7307)  class_acc: 0.0833 (0.0687)  loss_scale: 65536.0000 (70402.7158)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1459  max mem: 15572
Epoch: [3]  [1800/2809]  eta: 0:09:51  lr: 0.000034  min_lr: 0.000000  loss: 4.6998 (4.7305)  class_acc: 0.0833 (0.0690)  loss_scale: 65536.0000 (70375.6935)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1188  max mem: 15572
Epoch: [3]  [1810/2809]  eta: 0:09:45  lr: 0.000034  min_lr: 0.000000  loss: 4.6983 (4.7304)  class_acc: 0.0833 (0.0691)  loss_scale: 65536.0000 (70348.9696)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1187  max mem: 15572
Epoch: [3]  [1820/2809]  eta: 0:09:39  lr: 0.000034  min_lr: 0.000000  loss: 4.7076 (4.7302)  class_acc: 0.0833 (0.0692)  loss_scale: 65536.0000 (70322.5393)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1236  max mem: 15572
Epoch: [3]  [1830/2809]  eta: 0:09:33  lr: 0.000034  min_lr: 0.000000  loss: 4.7336 (4.7302)  class_acc: 0.0417 (0.0692)  loss_scale: 65536.0000 (70296.3976)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1349  max mem: 15572
Epoch: [3]  [1840/2809]  eta: 0:09:28  lr: 0.000034  min_lr: 0.000000  loss: 4.7428 (4.7304)  class_acc: 0.0000 (0.0690)  loss_scale: 65536.0000 (70270.5399)  weight_decay: 0.0500 (0.0500)  time: 0.6743  data: 0.2260  max mem: 15572
[2025-01-15 16:08:03,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:08:03,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1850/2809]  eta: 0:09:23  lr: 0.000034  min_lr: 0.000000  loss: 4.6974 (4.7299)  class_acc: 0.0000 (0.0690)  loss_scale: 65536.0000 (70421.9903)  weight_decay: 0.0500 (0.0500)  time: 0.6717  data: 0.2151  max mem: 15572
[2025-01-15 16:08:05,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10278
[2025-01-15 16:08:05,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:08:05,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1860/2809]  eta: 0:09:17  lr: 0.000034  min_lr: 0.000000  loss: 4.6310 (4.7295)  class_acc: 0.0833 (0.0691)  loss_scale: 65536.0000 (70395.7356)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1561  max mem: 15572
Epoch: [3]  [1870/2809]  eta: 0:09:10  lr: 0.000034  min_lr: 0.000000  loss: 4.6317 (4.7292)  class_acc: 0.0833 (0.0692)  loss_scale: 65536.0000 (70369.7616)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0796  max mem: 15572
Epoch: [3]  [1880/2809]  eta: 0:09:05  lr: 0.000034  min_lr: 0.000000  loss: 4.7091 (4.7294)  class_acc: 0.0417 (0.0690)  loss_scale: 65536.0000 (70344.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0930  max mem: 15572
Epoch: [3]  [1890/2809]  eta: 0:08:59  lr: 0.000034  min_lr: 0.000000  loss: 4.6542 (4.7284)  class_acc: 0.0417 (0.0692)  loss_scale: 65536.0000 (70318.6378)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1664  max mem: 15572
Epoch: [3]  [1900/2809]  eta: 0:08:54  lr: 0.000034  min_lr: 0.000000  loss: 4.6209 (4.7280)  class_acc: 0.0833 (0.0695)  loss_scale: 65536.0000 (70293.4792)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1913  max mem: 15572
Epoch: [3]  [1910/2809]  eta: 0:08:48  lr: 0.000035  min_lr: 0.000000  loss: 4.5606 (4.7273)  class_acc: 0.0833 (0.0697)  loss_scale: 65536.0000 (70268.5840)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.2063  max mem: 15572
Epoch: [3]  [1920/2809]  eta: 0:08:42  lr: 0.000035  min_lr: 0.000000  loss: 4.6462 (4.7276)  class_acc: 0.0833 (0.0697)  loss_scale: 65536.0000 (70243.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1392  max mem: 15572
Epoch: [3]  [1930/2809]  eta: 0:08:36  lr: 0.000035  min_lr: 0.000000  loss: 4.6940 (4.7277)  class_acc: 0.0833 (0.0697)  loss_scale: 65536.0000 (70219.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1345  max mem: 15572
Epoch: [3]  [1940/2809]  eta: 0:08:30  lr: 0.000035  min_lr: 0.000000  loss: 4.7500 (4.7282)  class_acc: 0.0417 (0.0696)  loss_scale: 65536.0000 (70195.4374)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1431  max mem: 15572
Epoch: [3]  [1950/2809]  eta: 0:08:24  lr: 0.000035  min_lr: 0.000000  loss: 4.8009 (4.7285)  class_acc: 0.0417 (0.0695)  loss_scale: 65536.0000 (70171.5551)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1188  max mem: 15572
Epoch: [3]  [1960/2809]  eta: 0:08:18  lr: 0.000035  min_lr: 0.000000  loss: 4.7968 (4.7285)  class_acc: 0.0417 (0.0694)  loss_scale: 65536.0000 (70147.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0779  max mem: 15572
Epoch: [3]  [1970/2809]  eta: 0:08:12  lr: 0.000035  min_lr: 0.000000  loss: 4.7968 (4.7287)  class_acc: 0.0417 (0.0694)  loss_scale: 65536.0000 (70124.5175)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0686  max mem: 15572
[2025-01-15 16:09:22,077] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:09:22,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [1980/2809]  eta: 0:08:06  lr: 0.000035  min_lr: 0.000000  loss: 4.6552 (4.7280)  class_acc: 0.0833 (0.0696)  loss_scale: 65536.0000 (70134.4372)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0988  max mem: 15572
[2025-01-15 16:09:23,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10410
[2025-01-15 16:09:23,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:09:23,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [1990/2809]  eta: 0:08:01  lr: 0.000035  min_lr: 0.000000  loss: 4.6497 (4.7278)  class_acc: 0.0833 (0.0697)  loss_scale: 65536.0000 (70177.1733)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1106  max mem: 15572
Epoch: [3]  [2000/2809]  eta: 0:07:54  lr: 0.000035  min_lr: 0.000000  loss: 4.6682 (4.7272)  class_acc: 0.0417 (0.0697)  loss_scale: 65536.0000 (70153.9790)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1012  max mem: 15572
Epoch: [3]  [2010/2809]  eta: 0:07:48  lr: 0.000035  min_lr: 0.000000  loss: 4.6000 (4.7266)  class_acc: 0.0833 (0.0698)  loss_scale: 65536.0000 (70131.0154)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0970  max mem: 15572
Epoch: [3]  [2020/2809]  eta: 0:07:43  lr: 0.000035  min_lr: 0.000000  loss: 4.5034 (4.7261)  class_acc: 0.0833 (0.0700)  loss_scale: 65536.0000 (70108.2791)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1322  max mem: 15572
Epoch: [3]  [2030/2809]  eta: 0:07:37  lr: 0.000035  min_lr: 0.000000  loss: 4.5229 (4.7255)  class_acc: 0.0833 (0.0700)  loss_scale: 65536.0000 (70085.7666)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1124  max mem: 15572
Epoch: [3]  [2040/2809]  eta: 0:07:31  lr: 0.000035  min_lr: 0.000000  loss: 4.5973 (4.7249)  class_acc: 0.0833 (0.0701)  loss_scale: 65536.0000 (70063.4748)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1288  max mem: 15572
Epoch: [3]  [2050/2809]  eta: 0:07:25  lr: 0.000035  min_lr: 0.000000  loss: 4.5988 (4.7248)  class_acc: 0.0833 (0.0700)  loss_scale: 65536.0000 (70041.4003)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.2077  max mem: 15572
Epoch: [3]  [2060/2809]  eta: 0:07:19  lr: 0.000035  min_lr: 0.000000  loss: 4.6344 (4.7243)  class_acc: 0.0833 (0.0701)  loss_scale: 65536.0000 (70019.5400)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1310  max mem: 15572
Epoch: [3]  [2070/2809]  eta: 0:07:13  lr: 0.000035  min_lr: 0.000000  loss: 4.6621 (4.7237)  class_acc: 0.0417 (0.0701)  loss_scale: 65536.0000 (69997.8909)  weight_decay: 0.0500 (0.0500)  time: 0.5120  data: 0.0701  max mem: 15572
Epoch: [3]  [2080/2809]  eta: 0:07:07  lr: 0.000035  min_lr: 0.000000  loss: 4.7023 (4.7233)  class_acc: 0.0417 (0.0700)  loss_scale: 65536.0000 (69976.4498)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1473  max mem: 15572
Epoch: [3]  [2090/2809]  eta: 0:07:01  lr: 0.000035  min_lr: 0.000000  loss: 4.6719 (4.7228)  class_acc: 0.0417 (0.0700)  loss_scale: 65536.0000 (69955.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1152  max mem: 15572
Epoch: [3]  [2100/2809]  eta: 0:06:56  lr: 0.000035  min_lr: 0.000000  loss: 4.6719 (4.7226)  class_acc: 0.0833 (0.0701)  loss_scale: 65536.0000 (69934.1799)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1448  max mem: 15572
Epoch: [3]  [2110/2809]  eta: 0:06:49  lr: 0.000035  min_lr: 0.000000  loss: 4.6413 (4.7222)  class_acc: 0.0833 (0.0701)  loss_scale: 65536.0000 (69913.3453)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1383  max mem: 15572
[2025-01-15 16:10:37,785] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:10:37,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2120/2809]  eta: 0:06:44  lr: 0.000035  min_lr: 0.000000  loss: 4.6268 (4.7219)  class_acc: 0.0417 (0.0701)  loss_scale: 65536.0000 (70170.7949)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0863  max mem: 15572
Epoch: [3]  [2130/2809]  eta: 0:06:38  lr: 0.000035  min_lr: 0.000000  loss: 4.6470 (4.7214)  class_acc: 0.0417 (0.0701)  loss_scale: 131072.0000 (70456.5819)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1792  max mem: 15572
[2025-01-15 16:10:52,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10563
[2025-01-15 16:10:52,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:10:52,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2140/2809]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000000  loss: 4.5541 (4.7203)  class_acc: 0.0833 (0.0704)  loss_scale: 131072.0000 (70586.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1495  max mem: 15572
Epoch: [3]  [2150/2809]  eta: 0:06:26  lr: 0.000035  min_lr: 0.000000  loss: 4.5766 (4.7202)  class_acc: 0.0833 (0.0704)  loss_scale: 65536.0000 (70563.1688)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0611  max mem: 15572
Epoch: [3]  [2160/2809]  eta: 0:06:20  lr: 0.000035  min_lr: 0.000000  loss: 4.6989 (4.7206)  class_acc: 0.0833 (0.0704)  loss_scale: 65536.0000 (70539.9056)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0894  max mem: 15572
Epoch: [3]  [2170/2809]  eta: 0:06:14  lr: 0.000035  min_lr: 0.000000  loss: 4.6335 (4.7196)  class_acc: 0.0833 (0.0707)  loss_scale: 65536.0000 (70516.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0864  max mem: 15572
Epoch: [3]  [2180/2809]  eta: 0:06:08  lr: 0.000035  min_lr: 0.000000  loss: 4.4739 (4.7195)  class_acc: 0.0833 (0.0706)  loss_scale: 65536.0000 (70494.0193)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1161  max mem: 15572
Epoch: [3]  [2190/2809]  eta: 0:06:02  lr: 0.000035  min_lr: 0.000000  loss: 4.6850 (4.7192)  class_acc: 0.0833 (0.0707)  loss_scale: 65536.0000 (70471.3902)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1074  max mem: 15572
Epoch: [3]  [2200/2809]  eta: 0:05:56  lr: 0.000035  min_lr: 0.000000  loss: 4.6246 (4.7191)  class_acc: 0.0833 (0.0706)  loss_scale: 65536.0000 (70448.9668)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0876  max mem: 15572
Epoch: [3]  [2210/2809]  eta: 0:05:50  lr: 0.000036  min_lr: 0.000000  loss: 4.6414 (4.7191)  class_acc: 0.0417 (0.0706)  loss_scale: 65536.0000 (70426.7463)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1659  max mem: 15572
Epoch: [3]  [2220/2809]  eta: 0:05:45  lr: 0.000036  min_lr: 0.000000  loss: 4.6780 (4.7185)  class_acc: 0.0833 (0.0706)  loss_scale: 65536.0000 (70404.7258)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.2042  max mem: 15572
Epoch: [3]  [2230/2809]  eta: 0:05:39  lr: 0.000036  min_lr: 0.000000  loss: 4.5663 (4.7176)  class_acc: 0.0833 (0.0708)  loss_scale: 65536.0000 (70382.9027)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.2249  max mem: 15572
Epoch: [3]  [2240/2809]  eta: 0:05:33  lr: 0.000036  min_lr: 0.000000  loss: 4.7012 (4.7177)  class_acc: 0.0833 (0.0709)  loss_scale: 65536.0000 (70361.2744)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1357  max mem: 15572
Epoch: [3]  [2250/2809]  eta: 0:05:27  lr: 0.000036  min_lr: 0.000000  loss: 4.7260 (4.7176)  class_acc: 0.0417 (0.0709)  loss_scale: 65536.0000 (70339.8383)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1069  max mem: 15572
Epoch: [3]  [2260/2809]  eta: 0:05:21  lr: 0.000036  min_lr: 0.000000  loss: 4.6681 (4.7175)  class_acc: 0.0417 (0.0709)  loss_scale: 65536.0000 (70318.5918)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1066  max mem: 15572
[2025-01-15 16:12:06,608] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:12:06,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [3]  [2270/2809]  eta: 0:05:15  lr: 0.000036  min_lr: 0.000000  loss: 4.6817 (4.7178)  class_acc: 0.0833 (0.0708)  loss_scale: 65536.0000 (70470.6790)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0559  max mem: 15572
Epoch: [3]  [2280/2809]  eta: 0:05:09  lr: 0.000036  min_lr: 0.000000  loss: 4.8176 (4.7179)  class_acc: 0.0833 (0.0708)  loss_scale: 131072.0000 (70736.3577)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0736  max mem: 15572
Epoch: [3]  [2290/2809]  eta: 0:05:03  lr: 0.000036  min_lr: 0.000000  loss: 4.7352 (4.7176)  class_acc: 0.0833 (0.0709)  loss_scale: 131072.0000 (70999.7172)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1367  max mem: 15572
Epoch: [3]  [2300/2809]  eta: 0:04:57  lr: 0.000036  min_lr: 0.000000  loss: 4.6386 (4.7172)  class_acc: 0.0417 (0.0710)  loss_scale: 131072.0000 (71260.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1404  max mem: 15572
[2025-01-15 16:12:28,100] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10732
[2025-01-15 16:12:28,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:12:28,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2310/2809]  eta: 0:04:52  lr: 0.000036  min_lr: 0.000000  loss: 4.6246 (4.7166)  class_acc: 0.0417 (0.0709)  loss_scale: 131072.0000 (71349.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1291  max mem: 15572
Epoch: [3]  [2320/2809]  eta: 0:04:46  lr: 0.000036  min_lr: 0.000000  loss: 4.6246 (4.7161)  class_acc: 0.0833 (0.0710)  loss_scale: 65536.0000 (71324.4016)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1653  max mem: 15572
Epoch: [3]  [2330/2809]  eta: 0:04:40  lr: 0.000036  min_lr: 0.000000  loss: 4.6293 (4.7159)  class_acc: 0.0833 (0.0711)  loss_scale: 65536.0000 (71299.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1148  max mem: 15572
Epoch: [3]  [2340/2809]  eta: 0:04:34  lr: 0.000036  min_lr: 0.000000  loss: 4.6337 (4.7158)  class_acc: 0.0417 (0.0710)  loss_scale: 65536.0000 (71274.9492)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1008  max mem: 15572
Epoch: [3]  [2350/2809]  eta: 0:04:28  lr: 0.000036  min_lr: 0.000000  loss: 4.7245 (4.7156)  class_acc: 0.0417 (0.0710)  loss_scale: 65536.0000 (71250.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1382  max mem: 15572
Epoch: [3]  [2360/2809]  eta: 0:04:23  lr: 0.000036  min_lr: 0.000000  loss: 4.6496 (4.7154)  class_acc: 0.0833 (0.0710)  loss_scale: 65536.0000 (71226.3346)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1782  max mem: 15572
Epoch: [3]  [2370/2809]  eta: 0:04:17  lr: 0.000036  min_lr: 0.000000  loss: 4.6444 (4.7151)  class_acc: 0.0833 (0.0710)  loss_scale: 65536.0000 (71202.3349)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.2233  max mem: 15572
[2025-01-15 16:13:12,446] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10805
[2025-01-15 16:13:12,446] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:13:12,447] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [2380/2809]  eta: 0:04:11  lr: 0.000036  min_lr: 0.000000  loss: 4.7045 (4.7149)  class_acc: 0.0833 (0.0711)  loss_scale: 65536.0000 (71137.2499)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1307  max mem: 15572
Epoch: [3]  [2390/2809]  eta: 0:04:05  lr: 0.000036  min_lr: 0.000000  loss: 4.7442 (4.7150)  class_acc: 0.0833 (0.0711)  loss_scale: 32768.0000 (70976.7762)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0919  max mem: 15572
Epoch: [3]  [2400/2809]  eta: 0:03:59  lr: 0.000036  min_lr: 0.000000  loss: 4.7016 (4.7145)  class_acc: 0.0833 (0.0712)  loss_scale: 32768.0000 (70817.6393)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1024  max mem: 15572
Epoch: [3]  [2410/2809]  eta: 0:03:53  lr: 0.000036  min_lr: 0.000000  loss: 4.6263 (4.7141)  class_acc: 0.0417 (0.0713)  loss_scale: 32768.0000 (70659.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1374  max mem: 15572
Epoch: [3]  [2420/2809]  eta: 0:03:47  lr: 0.000036  min_lr: 0.000000  loss: 4.6227 (4.7136)  class_acc: 0.0417 (0.0714)  loss_scale: 32768.0000 (70503.3094)  weight_decay: 0.0500 (0.0500)  time: 0.6499  data: 0.1800  max mem: 15572
Epoch: [3]  [2430/2809]  eta: 0:03:41  lr: 0.000036  min_lr: 0.000000  loss: 4.6903 (4.7136)  class_acc: 0.0833 (0.0714)  loss_scale: 32768.0000 (70348.0839)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1099  max mem: 15572
Epoch: [3]  [2440/2809]  eta: 0:03:36  lr: 0.000036  min_lr: 0.000000  loss: 4.7216 (4.7138)  class_acc: 0.0417 (0.0714)  loss_scale: 32768.0000 (70194.1303)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1209  max mem: 15572
Epoch: [3]  [2450/2809]  eta: 0:03:30  lr: 0.000036  min_lr: 0.000000  loss: 4.6545 (4.7135)  class_acc: 0.0417 (0.0715)  loss_scale: 32768.0000 (70041.4329)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1376  max mem: 15572
Epoch: [3]  [2460/2809]  eta: 0:03:24  lr: 0.000036  min_lr: 0.000000  loss: 4.6123 (4.7134)  class_acc: 0.0833 (0.0715)  loss_scale: 32768.0000 (69889.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1380  max mem: 15572
Epoch: [3]  [2470/2809]  eta: 0:03:18  lr: 0.000036  min_lr: 0.000000  loss: 4.6028 (4.7128)  class_acc: 0.0833 (0.0717)  loss_scale: 32768.0000 (69739.7459)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1200  max mem: 15572
Epoch: [3]  [2480/2809]  eta: 0:03:12  lr: 0.000036  min_lr: 0.000000  loss: 4.6366 (4.7127)  class_acc: 0.0833 (0.0717)  loss_scale: 32768.0000 (69590.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1179  max mem: 15572
Epoch: [3]  [2490/2809]  eta: 0:03:06  lr: 0.000036  min_lr: 0.000000  loss: 4.6996 (4.7127)  class_acc: 0.0833 (0.0717)  loss_scale: 32768.0000 (69442.9033)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1322  max mem: 15572
Epoch: [3]  [2500/2809]  eta: 0:03:01  lr: 0.000036  min_lr: 0.000000  loss: 4.6905 (4.7125)  class_acc: 0.0833 (0.0718)  loss_scale: 32768.0000 (69296.2623)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1715  max mem: 15572
[2025-01-15 16:14:29,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:14:29,534] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [3]  [2510/2809]  eta: 0:02:55  lr: 0.000037  min_lr: 0.000000  loss: 4.6236 (4.7123)  class_acc: 0.0833 (0.0719)  loss_scale: 32768.0000 (69202.9885)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2255  max mem: 15572
Epoch: [3]  [2520/2809]  eta: 0:02:49  lr: 0.000037  min_lr: 0.000000  loss: 4.6852 (4.7122)  class_acc: 0.0417 (0.0718)  loss_scale: 65536.0000 (69188.4427)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1576  max mem: 15572
Epoch: [3]  [2530/2809]  eta: 0:02:43  lr: 0.000037  min_lr: 0.000000  loss: 4.7236 (4.7122)  class_acc: 0.0417 (0.0718)  loss_scale: 65536.0000 (69174.0119)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1143  max mem: 15572
Epoch: [3]  [2540/2809]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000000  loss: 4.7465 (4.7123)  class_acc: 0.0417 (0.0719)  loss_scale: 65536.0000 (69159.6946)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.2303  max mem: 15572
Epoch: [3]  [2550/2809]  eta: 0:02:31  lr: 0.000037  min_lr: 0.000000  loss: 4.7098 (4.7115)  class_acc: 0.0417 (0.0719)  loss_scale: 65536.0000 (69145.4896)  weight_decay: 0.0500 (0.0500)  time: 0.6677  data: 0.2190  max mem: 15572
Epoch: [3]  [2560/2809]  eta: 0:02:26  lr: 0.000037  min_lr: 0.000000  loss: 4.4977 (4.7109)  class_acc: 0.0833 (0.0719)  loss_scale: 65536.0000 (69131.3955)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0932  max mem: 15572
Epoch: [3]  [2570/2809]  eta: 0:02:20  lr: 0.000037  min_lr: 0.000000  loss: 4.4977 (4.7105)  class_acc: 0.0833 (0.0720)  loss_scale: 65536.0000 (69117.4111)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0877  max mem: 15572
[2025-01-15 16:15:07,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[3.556953454766912e-07, 3.556953454766912e-07, 5.081362078238446e-07, 5.081362078238446e-07, 7.259088683197781e-07, 7.259088683197781e-07, 1.0370126690282546e-06, 1.0370126690282546e-06, 1.4814466700403637e-06, 1.4814466700403637e-06, 2.116352385771948e-06, 2.116352385771948e-06, 3.023360551102783e-06, 3.023360551102783e-06, 4.319086501575405e-06, 4.319086501575405e-06, 6.17012357367915e-06, 6.17012357367915e-06, 8.814462248113072e-06, 8.814462248113072e-06, 1.2592088925875817e-05, 1.2592088925875817e-05, 1.7988698465536884e-05, 1.7988698465536884e-05, 2.5698140665052692e-05, 2.5698140665052692e-05, 3.671162952150385e-05, 3.671162952150385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:15:07,095] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=28.43774773448955, CurrSamplesPerSec=32.00907646694454, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [3]  [2580/2809]  eta: 0:02:14  lr: 0.000037  min_lr: 0.000000  loss: 4.5447 (4.7099)  class_acc: 0.0833 (0.0721)  loss_scale: 65536.0000 (69103.5351)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1300  max mem: 15572
Epoch: [3]  [2590/2809]  eta: 0:02:08  lr: 0.000037  min_lr: 0.000000  loss: 4.5634 (4.7099)  class_acc: 0.0833 (0.0721)  loss_scale: 65536.0000 (69089.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1026  max mem: 15572
Epoch: [3]  [2600/2809]  eta: 0:02:02  lr: 0.000037  min_lr: 0.000000  loss: 4.6736 (4.7096)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (69076.1030)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1541  max mem: 15572
Epoch: [3]  [2610/2809]  eta: 0:01:56  lr: 0.000037  min_lr: 0.000000  loss: 4.6145 (4.7091)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (69062.5446)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.2072  max mem: 15572
Epoch: [3]  [2620/2809]  eta: 0:01:50  lr: 0.000037  min_lr: 0.000000  loss: 4.6576 (4.7091)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (69049.0897)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1416  max mem: 15572
Epoch: [3]  [2630/2809]  eta: 0:01:44  lr: 0.000037  min_lr: 0.000000  loss: 4.6576 (4.7086)  class_acc: 0.0833 (0.0722)  loss_scale: 65536.0000 (69035.7370)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0970  max mem: 15572
[2025-01-15 16:15:44,010] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:15:44,011] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:15:46,274] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11067
[2025-01-15 16:15:46,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:15:46,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [3]  [2640/2809]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000000  loss: 4.6490 (4.7087)  class_acc: 0.0417 (0.0722)  loss_scale: 65536.0000 (69146.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5030  data: 0.0543  max mem: 15572
Epoch: [3]  [2650/2809]  eta: 0:01:33  lr: 0.000037  min_lr: 0.000000  loss: 4.6490 (4.7083)  class_acc: 0.0833 (0.0724)  loss_scale: 65536.0000 (69132.9400)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1264  max mem: 15572
Epoch: [3]  [2660/2809]  eta: 0:01:27  lr: 0.000037  min_lr: 0.000000  loss: 4.5470 (4.7081)  class_acc: 0.0833 (0.0724)  loss_scale: 65536.0000 (69119.4228)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.2027  max mem: 15572
Epoch: [3]  [2670/2809]  eta: 0:01:21  lr: 0.000037  min_lr: 0.000000  loss: 4.6905 (4.7081)  class_acc: 0.0417 (0.0724)  loss_scale: 65536.0000 (69106.0067)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1790  max mem: 15572
Epoch: [3]  [2680/2809]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000000  loss: 4.6905 (4.7077)  class_acc: 0.0833 (0.0725)  loss_scale: 65536.0000 (69092.6908)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1092  max mem: 15572
Epoch: [3]  [2690/2809]  eta: 0:01:09  lr: 0.000037  min_lr: 0.000000  loss: 4.5988 (4.7079)  class_acc: 0.0833 (0.0725)  loss_scale: 65536.0000 (69079.4738)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1019  max mem: 15572
Epoch: [3]  [2700/2809]  eta: 0:01:03  lr: 0.000037  min_lr: 0.000000  loss: 4.7536 (4.7081)  class_acc: 0.0417 (0.0724)  loss_scale: 65536.0000 (69066.3547)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.1828  max mem: 15572
Epoch: [3]  [2710/2809]  eta: 0:00:58  lr: 0.000037  min_lr: 0.000000  loss: 4.7246 (4.7081)  class_acc: 0.0417 (0.0723)  loss_scale: 65536.0000 (69053.3323)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1412  max mem: 15572
Epoch: [3]  [2720/2809]  eta: 0:00:52  lr: 0.000037  min_lr: 0.000000  loss: 4.7797 (4.7084)  class_acc: 0.0417 (0.0723)  loss_scale: 65536.0000 (69040.4057)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0963  max mem: 15572
Epoch: [3]  [2730/2809]  eta: 0:00:46  lr: 0.000037  min_lr: 0.000000  loss: 4.7687 (4.7081)  class_acc: 0.0417 (0.0723)  loss_scale: 65536.0000 (69027.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0965  max mem: 15572
Epoch: [3]  [2740/2809]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000000  loss: 4.6773 (4.7076)  class_acc: 0.0833 (0.0724)  loss_scale: 65536.0000 (69014.8355)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1530  max mem: 15572
Epoch: [3]  [2750/2809]  eta: 0:00:34  lr: 0.000037  min_lr: 0.000000  loss: 4.5843 (4.7070)  class_acc: 0.0833 (0.0724)  loss_scale: 65536.0000 (69002.1897)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.2454  max mem: 15572
Epoch: [3]  [2760/2809]  eta: 0:00:28  lr: 0.000037  min_lr: 0.000000  loss: 4.5819 (4.7063)  class_acc: 0.1250 (0.0726)  loss_scale: 65536.0000 (68989.6356)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.2328  max mem: 15572
[2025-01-15 16:17:01,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11192
[2025-01-15 16:17:01,471] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:17:01,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [3]  [2770/2809]  eta: 0:00:22  lr: 0.000037  min_lr: 0.000000  loss: 4.6158 (4.7058)  class_acc: 0.0833 (0.0726)  loss_scale: 65536.0000 (68906.2201)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1719  max mem: 15572
Epoch: [3]  [2780/2809]  eta: 0:00:17  lr: 0.000037  min_lr: 0.000000  loss: 4.5827 (4.7056)  class_acc: 0.0833 (0.0727)  loss_scale: 32768.0000 (68776.2733)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1015  max mem: 15572
Epoch: [3]  [2790/2809]  eta: 0:00:11  lr: 0.000037  min_lr: 0.000000  loss: 4.6823 (4.7055)  class_acc: 0.0833 (0.0727)  loss_scale: 32768.0000 (68647.2576)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1232  max mem: 15572
Epoch: [3]  [2800/2809]  eta: 0:00:05  lr: 0.000037  min_lr: 0.000000  loss: 4.6823 (4.7055)  class_acc: 0.0417 (0.0726)  loss_scale: 32768.0000 (68519.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1286  max mem: 15572
Epoch: [3]  [2808/2809]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000000  loss: 4.6101 (4.7055)  class_acc: 0.0417 (0.0726)  loss_scale: 32768.0000 (68417.3443)  weight_decay: 0.0500 (0.0500)  time: 0.4758  data: 0.0687  max mem: 15572
Epoch: [3] Total time: 0:27:26 (0.5863 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000000  loss: 4.6101 (4.7055)  class_acc: 0.0417 (0.0726)  loss_scale: 32768.0000 (68417.3443)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:13  loss: 2.5101 (2.5101)  acc1: 94.4444 (94.4444)  acc5: 100.0000 (100.0000)  time: 5.1248  data: 4.9298  max mem: 15572
Val:  [ 10/272]  eta: 0:03:25  loss: 4.8099 (4.4049)  acc1: 0.0000 (21.2121)  acc5: 0.0000 (28.2828)  time: 0.7856  data: 0.5954  max mem: 15572
Val:  [ 20/272]  eta: 0:02:11  loss: 4.4890 (4.3695)  acc1: 0.0000 (15.3439)  acc5: 0.0000 (24.0741)  time: 0.2933  data: 0.1076  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 4.4836 (4.3582)  acc1: 0.0000 (12.1864)  acc5: 11.1111 (25.0896)  time: 0.2533  data: 0.0633  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.8483 (4.2205)  acc1: 0.0000 (12.4661)  acc5: 27.7778 (30.6233)  time: 0.3087  data: 0.1099  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 3.7167 (4.2083)  acc1: 0.0000 (12.4183)  acc5: 50.0000 (32.4619)  time: 0.3602  data: 0.1617  max mem: 15572
Val:  [ 60/272]  eta: 0:01:25  loss: 3.5569 (4.1171)  acc1: 5.5556 (14.6630)  acc5: 55.5556 (37.1585)  time: 0.3776  data: 0.1767  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 3.6337 (4.0398)  acc1: 22.2222 (16.1189)  acc5: 55.5556 (37.8717)  time: 0.3577  data: 0.1434  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 3.8067 (4.0685)  acc1: 5.5556 (16.3237)  acc5: 33.3333 (37.3114)  time: 0.3048  data: 0.0974  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 4.8288 (4.1538)  acc1: 0.0000 (14.5299)  acc5: 0.0000 (33.2112)  time: 0.2637  data: 0.0694  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 4.7806 (4.2026)  acc1: 0.0000 (13.7514)  acc5: 0.0000 (31.4631)  time: 0.2699  data: 0.0749  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 4.6525 (4.2445)  acc1: 0.0000 (12.6126)  acc5: 0.0000 (30.0300)  time: 0.3107  data: 0.1070  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 4.6450 (4.2798)  acc1: 0.0000 (11.7539)  acc5: 5.5556 (28.5583)  time: 0.3006  data: 0.0999  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 4.5170 (4.2179)  acc1: 0.0000 (13.2740)  acc5: 22.2222 (31.1281)  time: 0.2303  data: 0.0409  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 4.1676 (4.2013)  acc1: 0.0000 (14.1844)  acc5: 27.7778 (31.2845)  time: 0.1949  data: 0.0114  max mem: 15572
Val:  [150/272]  eta: 0:00:38  loss: 4.3234 (4.2191)  acc1: 0.0000 (13.2818)  acc5: 5.5556 (29.7277)  time: 0.1877  data: 0.0142  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 4.2798 (4.2138)  acc1: 0.0000 (13.6646)  acc5: 11.1111 (30.8489)  time: 0.2656  data: 0.0930  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 4.2894 (4.2402)  acc1: 0.0000 (12.9955)  acc5: 33.3333 (30.6368)  time: 0.3667  data: 0.1713  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 4.2955 (4.2439)  acc1: 0.0000 (12.5230)  acc5: 11.1111 (29.7729)  time: 0.3757  data: 0.1561  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 4.5694 (4.2617)  acc1: 0.0000 (11.8674)  acc5: 5.5556 (28.7958)  time: 0.3173  data: 0.1025  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 4.4733 (4.2682)  acc1: 0.0000 (11.3875)  acc5: 11.1111 (29.1321)  time: 0.2859  data: 0.0746  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 4.3819 (4.2856)  acc1: 0.0000 (10.9531)  acc5: 22.2222 (28.2780)  time: 0.3733  data: 0.1665  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 4.4633 (4.2904)  acc1: 0.0000 (10.5329)  acc5: 5.5556 (28.0292)  time: 0.3295  data: 0.1359  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 4.3407 (4.2858)  acc1: 0.0000 (11.3516)  acc5: 22.2222 (28.8360)  time: 0.3715  data: 0.1634  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.9942 (4.2759)  acc1: 16.6667 (11.3877)  acc5: 55.5556 (29.6911)  time: 0.4208  data: 0.1932  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 4.3624 (4.2998)  acc1: 0.0000 (11.2660)  acc5: 22.2222 (29.0173)  time: 0.3256  data: 0.1112  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 4.0380 (4.2508)  acc1: 11.1111 (12.2180)  acc5: 55.5556 (30.9706)  time: 0.3350  data: 0.1352  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.8376 (4.2467)  acc1: 16.6667 (12.1566)  acc5: 66.6667 (31.4883)  time: 0.2929  data: 0.1152  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.8376 (4.2490)  acc1: 16.6667 (12.1442)  acc5: 66.6667 (31.4766)  time: 0.2860  data: 0.1152  max mem: 15572
Val: Total time: 0:01:29 (0.3283 s / it)
* Acc@1 12.144 Acc@5 31.477 loss 4.249
Accuracy of the network on the 4883 val videos: 12.1%
[2025-01-15 16:18:53,952] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 16:18:53,955] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 16:18:53,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 16:18:56,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 16:18:56,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 12.14%
Epoch: [4]  [   0/2809]  eta: 7:06:32  lr: 0.000038  min_lr: 0.000000  loss: 4.1982 (4.1982)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.1108  data: 8.5900  max mem: 15572
Epoch: [4]  [  10/2809]  eta: 1:13:37  lr: 0.000038  min_lr: 0.000000  loss: 4.5495 (4.5046)  class_acc: 0.1250 (0.1212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5783  data: 1.0846  max mem: 15572
Epoch: [4]  [  20/2809]  eta: 0:49:08  lr: 0.000038  min_lr: 0.000000  loss: 4.5793 (4.5835)  class_acc: 0.0833 (0.0913)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.1674  max mem: 15572
Epoch: [4]  [  30/2809]  eta: 0:42:47  lr: 0.000038  min_lr: 0.000000  loss: 4.6793 (4.6125)  class_acc: 0.0833 (0.0887)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0812  max mem: 15572
Epoch: [4]  [  40/2809]  eta: 0:40:22  lr: 0.000038  min_lr: 0.000000  loss: 4.6996 (4.6207)  class_acc: 0.0833 (0.0854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.2171  max mem: 15572
Epoch: [4]  [  50/2809]  eta: 0:38:47  lr: 0.000038  min_lr: 0.000000  loss: 4.7338 (4.6370)  class_acc: 0.0417 (0.0801)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.2625  max mem: 15572
Epoch: [4]  [  60/2809]  eta: 0:35:22  lr: 0.000038  min_lr: 0.000000  loss: 4.7187 (4.6515)  class_acc: 0.0417 (0.0786)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1264  max mem: 15572
Epoch: [4]  [  70/2809]  eta: 0:33:12  lr: 0.000038  min_lr: 0.000000  loss: 4.6674 (4.6530)  class_acc: 0.0417 (0.0786)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4319  data: 0.0004  max mem: 15572
Epoch: [4]  [  80/2809]  eta: 0:31:26  lr: 0.000038  min_lr: 0.000000  loss: 4.5530 (4.6491)  class_acc: 0.0417 (0.0792)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4444  data: 0.0005  max mem: 15572
[2025-01-15 16:19:55,244] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:19:55,244] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [  90/2809]  eta: 0:30:05  lr: 0.000038  min_lr: 0.000000  loss: 4.5564 (4.6352)  class_acc: 0.0833 (0.0847)  loss_scale: 32768.0000 (34928.5275)  weight_decay: 0.0500 (0.0500)  time: 0.4383  data: 0.0007  max mem: 15572
Epoch: [4]  [ 100/2809]  eta: 0:29:13  lr: 0.000038  min_lr: 0.000000  loss: 4.5564 (4.6323)  class_acc: 0.0833 (0.0891)  loss_scale: 65536.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  time: 0.4694  data: 0.0166  max mem: 15572
Epoch: [4]  [ 110/2809]  eta: 0:28:45  lr: 0.000038  min_lr: 0.000000  loss: 4.5026 (4.6274)  class_acc: 0.0833 (0.0916)  loss_scale: 65536.0000 (40443.3874)  weight_decay: 0.0500 (0.0500)  time: 0.5263  data: 0.0793  max mem: 15572
Epoch: [4]  [ 120/2809]  eta: 0:28:42  lr: 0.000038  min_lr: 0.000000  loss: 4.6232 (4.6252)  class_acc: 0.0833 (0.0916)  loss_scale: 65536.0000 (42517.1570)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1773  max mem: 15572
Epoch: [4]  [ 130/2809]  eta: 0:28:43  lr: 0.000038  min_lr: 0.000000  loss: 4.6545 (4.6276)  class_acc: 0.0833 (0.0906)  loss_scale: 65536.0000 (44274.3206)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.2417  max mem: 15572
Epoch: [4]  [ 140/2809]  eta: 0:28:31  lr: 0.000038  min_lr: 0.000000  loss: 4.5832 (4.6201)  class_acc: 0.0833 (0.0925)  loss_scale: 65536.0000 (45782.2411)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.2187  max mem: 15572
Epoch: [4]  [ 150/2809]  eta: 0:28:37  lr: 0.000038  min_lr: 0.000000  loss: 4.5229 (4.6163)  class_acc: 0.0833 (0.0900)  loss_scale: 65536.0000 (47090.4371)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.2263  max mem: 15572
Epoch: [4]  [ 160/2809]  eta: 0:28:09  lr: 0.000038  min_lr: 0.000000  loss: 4.6143 (4.6166)  class_acc: 0.0417 (0.0885)  loss_scale: 65536.0000 (48236.1242)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1766  max mem: 15572
Epoch: [4]  [ 170/2809]  eta: 0:27:54  lr: 0.000038  min_lr: 0.000000  loss: 4.5089 (4.6059)  class_acc: 0.0417 (0.0906)  loss_scale: 65536.0000 (49247.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1115  max mem: 15572
Epoch: [4]  [ 180/2809]  eta: 0:27:44  lr: 0.000038  min_lr: 0.000000  loss: 4.5649 (4.6130)  class_acc: 0.0417 (0.0902)  loss_scale: 65536.0000 (50147.7127)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1555  max mem: 15572
Epoch: [4]  [ 190/2809]  eta: 0:27:25  lr: 0.000038  min_lr: 0.000000  loss: 4.7332 (4.6223)  class_acc: 0.0417 (0.0888)  loss_scale: 65536.0000 (50953.3822)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1300  max mem: 15572
Epoch: [4]  [ 200/2809]  eta: 0:27:10  lr: 0.000038  min_lr: 0.000000  loss: 4.7009 (4.6230)  class_acc: 0.0833 (0.0881)  loss_scale: 65536.0000 (51678.8856)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1173  max mem: 15572
Epoch: [4]  [ 210/2809]  eta: 0:26:54  lr: 0.000038  min_lr: 0.000000  loss: 4.7009 (4.6256)  class_acc: 0.0833 (0.0871)  loss_scale: 65536.0000 (52335.6209)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1266  max mem: 15572
[2025-01-15 16:21:09,397] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:21:09,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:21:11,734] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11454
[2025-01-15 16:21:11,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:21:11,734] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 220/2809]  eta: 0:26:29  lr: 0.000038  min_lr: 0.000000  loss: 4.6657 (4.6227)  class_acc: 0.0833 (0.0877)  loss_scale: 65536.0000 (54415.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0613  max mem: 15572
Epoch: [4]  [ 230/2809]  eta: 0:26:34  lr: 0.000038  min_lr: 0.000000  loss: 4.6657 (4.6263)  class_acc: 0.0833 (0.0868)  loss_scale: 65536.0000 (54897.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1503  max mem: 15572
Epoch: [4]  [ 240/2809]  eta: 0:26:29  lr: 0.000038  min_lr: 0.000000  loss: 4.6720 (4.6243)  class_acc: 0.0833 (0.0870)  loss_scale: 65536.0000 (55338.4896)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.2295  max mem: 15572
Epoch: [4]  [ 250/2809]  eta: 0:26:21  lr: 0.000038  min_lr: 0.000000  loss: 4.6485 (4.6285)  class_acc: 0.0833 (0.0870)  loss_scale: 65536.0000 (55744.7649)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1759  max mem: 15572
Epoch: [4]  [ 260/2809]  eta: 0:26:15  lr: 0.000038  min_lr: 0.000000  loss: 4.7671 (4.6357)  class_acc: 0.0417 (0.0857)  loss_scale: 65536.0000 (56119.9080)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1679  max mem: 15572
Epoch: [4]  [ 270/2809]  eta: 0:26:05  lr: 0.000038  min_lr: 0.000000  loss: 4.7671 (4.6376)  class_acc: 0.0417 (0.0847)  loss_scale: 65536.0000 (56467.3653)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1414  max mem: 15572
Epoch: [4]  [ 280/2809]  eta: 0:26:05  lr: 0.000038  min_lr: 0.000000  loss: 4.6850 (4.6415)  class_acc: 0.0833 (0.0847)  loss_scale: 65536.0000 (56790.0925)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1694  max mem: 15572
Epoch: [4]  [ 290/2809]  eta: 0:25:45  lr: 0.000038  min_lr: 0.000000  loss: 4.6295 (4.6381)  class_acc: 0.0833 (0.0849)  loss_scale: 65536.0000 (57090.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1073  max mem: 15572
Epoch: [4]  [ 300/2809]  eta: 0:25:38  lr: 0.000039  min_lr: 0.000000  loss: 4.5669 (4.6404)  class_acc: 0.0833 (0.0842)  loss_scale: 65536.0000 (57371.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0849  max mem: 15572
Epoch: [4]  [ 310/2809]  eta: 0:25:33  lr: 0.000039  min_lr: 0.000000  loss: 4.6252 (4.6407)  class_acc: 0.0833 (0.0844)  loss_scale: 65536.0000 (57633.7492)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1564  max mem: 15572
Epoch: [4]  [ 320/2809]  eta: 0:25:37  lr: 0.000039  min_lr: 0.000000  loss: 4.6258 (4.6418)  class_acc: 0.0833 (0.0849)  loss_scale: 65536.0000 (57879.9252)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.2172  max mem: 15572
Epoch: [4]  [ 330/2809]  eta: 0:25:29  lr: 0.000039  min_lr: 0.000000  loss: 4.6437 (4.6408)  class_acc: 0.0833 (0.0850)  loss_scale: 65536.0000 (58111.2266)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.2288  max mem: 15572
Epoch: [4]  [ 340/2809]  eta: 0:25:20  lr: 0.000039  min_lr: 0.000000  loss: 4.5870 (4.6391)  class_acc: 0.0833 (0.0860)  loss_scale: 65536.0000 (58328.9619)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1629  max mem: 15572
[2025-01-15 16:22:30,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:22:30,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 350/2809]  eta: 0:25:07  lr: 0.000039  min_lr: 0.000000  loss: 4.4453 (4.6350)  class_acc: 0.0833 (0.0859)  loss_scale: 65536.0000 (59281.1396)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1159  max mem: 15572
Epoch: [4]  [ 360/2809]  eta: 0:24:56  lr: 0.000039  min_lr: 0.000000  loss: 4.4423 (4.6336)  class_acc: 0.0833 (0.0859)  loss_scale: 131072.0000 (61269.8061)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0827  max mem: 15572
[2025-01-15 16:22:40,371] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11602
[2025-01-15 16:22:40,371] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:22:40,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 370/2809]  eta: 0:24:52  lr: 0.000039  min_lr: 0.000000  loss: 4.6642 (4.6336)  class_acc: 0.0417 (0.0856)  loss_scale: 131072.0000 (62268.0323)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1412  max mem: 15572
Epoch: [4]  [ 380/2809]  eta: 0:24:47  lr: 0.000039  min_lr: 0.000000  loss: 4.7492 (4.6343)  class_acc: 0.0833 (0.0854)  loss_scale: 65536.0000 (62353.8058)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1883  max mem: 15572
Epoch: [4]  [ 390/2809]  eta: 0:24:35  lr: 0.000039  min_lr: 0.000000  loss: 4.7328 (4.6364)  class_acc: 0.0833 (0.0851)  loss_scale: 65536.0000 (62435.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1251  max mem: 15572
Epoch: [4]  [ 400/2809]  eta: 0:24:37  lr: 0.000039  min_lr: 0.000000  loss: 4.6187 (4.6347)  class_acc: 0.0833 (0.0856)  loss_scale: 65536.0000 (62512.5187)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1913  max mem: 15572
Epoch: [4]  [ 410/2809]  eta: 0:24:29  lr: 0.000039  min_lr: 0.000000  loss: 4.6062 (4.6356)  class_acc: 0.0833 (0.0858)  loss_scale: 65536.0000 (62586.0827)  weight_decay: 0.0500 (0.0500)  time: 0.6680  data: 0.2279  max mem: 15572
Epoch: [4]  [ 420/2809]  eta: 0:24:22  lr: 0.000039  min_lr: 0.000000  loss: 4.5939 (4.6334)  class_acc: 0.0833 (0.0859)  loss_scale: 65536.0000 (62656.1520)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1407  max mem: 15572
Epoch: [4]  [ 430/2809]  eta: 0:24:14  lr: 0.000039  min_lr: 0.000000  loss: 4.4608 (4.6292)  class_acc: 0.0833 (0.0863)  loss_scale: 65536.0000 (62722.9698)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1357  max mem: 15572
Epoch: [4]  [ 440/2809]  eta: 0:24:05  lr: 0.000039  min_lr: 0.000000  loss: 4.5425 (4.6280)  class_acc: 0.0833 (0.0865)  loss_scale: 65536.0000 (62786.7574)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1115  max mem: 15572
Epoch: [4]  [ 450/2809]  eta: 0:23:58  lr: 0.000039  min_lr: 0.000000  loss: 4.5996 (4.6270)  class_acc: 0.1250 (0.0874)  loss_scale: 65536.0000 (62847.7162)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1398  max mem: 15572
Epoch: [4]  [ 460/2809]  eta: 0:23:50  lr: 0.000039  min_lr: 0.000000  loss: 4.6116 (4.6275)  class_acc: 0.0833 (0.0875)  loss_scale: 65536.0000 (62906.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1508  max mem: 15572
Epoch: [4]  [ 470/2809]  eta: 0:23:42  lr: 0.000039  min_lr: 0.000000  loss: 4.6116 (4.6258)  class_acc: 0.0417 (0.0875)  loss_scale: 65536.0000 (62961.8684)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1190  max mem: 15572
Epoch: [4]  [ 480/2809]  eta: 0:23:37  lr: 0.000039  min_lr: 0.000000  loss: 4.6258 (4.6257)  class_acc: 0.0417 (0.0868)  loss_scale: 65536.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1516  max mem: 15572
Epoch: [4]  [ 490/2809]  eta: 0:23:26  lr: 0.000039  min_lr: 0.000000  loss: 4.6110 (4.6259)  class_acc: 0.0417 (0.0870)  loss_scale: 65536.0000 (63066.7210)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1266  max mem: 15572
[2025-01-15 16:23:57,331] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:23:57,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:23:58,263] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11733
[2025-01-15 16:23:58,263] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:23:58,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 500/2809]  eta: 0:23:17  lr: 0.000039  min_lr: 0.000000  loss: 4.6364 (4.6265)  class_acc: 0.0833 (0.0866)  loss_scale: 65536.0000 (63377.6287)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0679  max mem: 15572
Epoch: [4]  [ 510/2809]  eta: 0:23:06  lr: 0.000039  min_lr: 0.000000  loss: 4.5425 (4.6226)  class_acc: 0.0833 (0.0868)  loss_scale: 65536.0000 (63419.8669)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0734  max mem: 15572
Epoch: [4]  [ 520/2809]  eta: 0:22:55  lr: 0.000039  min_lr: 0.000000  loss: 4.5288 (4.6256)  class_acc: 0.0833 (0.0860)  loss_scale: 65536.0000 (63460.4837)  weight_decay: 0.0500 (0.0500)  time: 0.4981  data: 0.0558  max mem: 15572
Epoch: [4]  [ 530/2809]  eta: 0:22:47  lr: 0.000039  min_lr: 0.000000  loss: 4.7708 (4.6264)  class_acc: 0.0417 (0.0859)  loss_scale: 65536.0000 (63499.5706)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0693  max mem: 15572
Epoch: [4]  [ 540/2809]  eta: 0:22:43  lr: 0.000039  min_lr: 0.000000  loss: 4.6425 (4.6271)  class_acc: 0.0833 (0.0867)  loss_scale: 65536.0000 (63537.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1586  max mem: 15572
Epoch: [4]  [ 550/2809]  eta: 0:22:35  lr: 0.000039  min_lr: 0.000000  loss: 4.6971 (4.6287)  class_acc: 0.0833 (0.0864)  loss_scale: 65536.0000 (63573.4882)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1439  max mem: 15572
Epoch: [4]  [ 560/2809]  eta: 0:22:25  lr: 0.000039  min_lr: 0.000000  loss: 4.6630 (4.6265)  class_acc: 0.0833 (0.0869)  loss_scale: 65536.0000 (63608.4706)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0709  max mem: 15572
Epoch: [4]  [ 570/2809]  eta: 0:22:22  lr: 0.000039  min_lr: 0.000000  loss: 4.4834 (4.6258)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (63642.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1330  max mem: 15572
Epoch: [4]  [ 580/2809]  eta: 0:22:12  lr: 0.000039  min_lr: 0.000000  loss: 4.5538 (4.6246)  class_acc: 0.0833 (0.0874)  loss_scale: 65536.0000 (63674.8227)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1228  max mem: 15572
Epoch: [4]  [ 590/2809]  eta: 0:22:05  lr: 0.000039  min_lr: 0.000000  loss: 4.6007 (4.6239)  class_acc: 0.0417 (0.0872)  loss_scale: 65536.0000 (63706.3147)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0862  max mem: 15572
Epoch: [4]  [ 600/2809]  eta: 0:21:59  lr: 0.000040  min_lr: 0.000000  loss: 4.5934 (4.6231)  class_acc: 0.0417 (0.0870)  loss_scale: 65536.0000 (63736.7587)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1438  max mem: 15572
Epoch: [4]  [ 610/2809]  eta: 0:21:53  lr: 0.000040  min_lr: 0.000000  loss: 4.5934 (4.6233)  class_acc: 0.0833 (0.0877)  loss_scale: 65536.0000 (63766.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1532  max mem: 15572
Epoch: [4]  [ 620/2809]  eta: 0:21:47  lr: 0.000040  min_lr: 0.000000  loss: 4.6202 (4.6234)  class_acc: 0.0833 (0.0878)  loss_scale: 65536.0000 (63794.7053)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1500  max mem: 15572
[2025-01-15 16:25:11,337] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:25:11,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 630/2809]  eta: 0:21:41  lr: 0.000040  min_lr: 0.000000  loss: 4.6188 (4.6227)  class_acc: 0.0833 (0.0880)  loss_scale: 65536.0000 (64341.6038)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1526  max mem: 15572
[2025-01-15 16:25:17,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11871
[2025-01-15 16:25:17,546] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:25:17,546] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 640/2809]  eta: 0:21:34  lr: 0.000040  min_lr: 0.000000  loss: 4.5231 (4.6213)  class_acc: 0.0833 (0.0882)  loss_scale: 65536.0000 (64769.1981)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1365  max mem: 15572
Epoch: [4]  [ 650/2809]  eta: 0:21:29  lr: 0.000040  min_lr: 0.000000  loss: 4.3854 (4.6180)  class_acc: 0.1250 (0.0892)  loss_scale: 65536.0000 (64780.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1410  max mem: 15572
Epoch: [4]  [ 660/2809]  eta: 0:21:24  lr: 0.000040  min_lr: 0.000000  loss: 4.5182 (4.6176)  class_acc: 0.1250 (0.0894)  loss_scale: 65536.0000 (64792.3994)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1593  max mem: 15572
Epoch: [4]  [ 670/2809]  eta: 0:21:19  lr: 0.000040  min_lr: 0.000000  loss: 4.6206 (4.6183)  class_acc: 0.0833 (0.0893)  loss_scale: 65536.0000 (64803.4814)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1694  max mem: 15572
Epoch: [4]  [ 680/2809]  eta: 0:21:13  lr: 0.000040  min_lr: 0.000000  loss: 4.5874 (4.6173)  class_acc: 0.0833 (0.0897)  loss_scale: 65536.0000 (64814.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1642  max mem: 15572
Epoch: [4]  [ 690/2809]  eta: 0:21:06  lr: 0.000040  min_lr: 0.000000  loss: 4.5638 (4.6177)  class_acc: 0.0833 (0.0895)  loss_scale: 65536.0000 (64824.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1352  max mem: 15572
Epoch: [4]  [ 700/2809]  eta: 0:21:00  lr: 0.000040  min_lr: 0.000000  loss: 4.5525 (4.6171)  class_acc: 0.0833 (0.0902)  loss_scale: 65536.0000 (64834.8302)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1311  max mem: 15572
Epoch: [4]  [ 710/2809]  eta: 0:20:54  lr: 0.000040  min_lr: 0.000000  loss: 4.5525 (4.6169)  class_acc: 0.1250 (0.0910)  loss_scale: 65536.0000 (64844.6920)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1525  max mem: 15572
Epoch: [4]  [ 720/2809]  eta: 0:20:47  lr: 0.000040  min_lr: 0.000000  loss: 4.6396 (4.6164)  class_acc: 0.0833 (0.0910)  loss_scale: 65536.0000 (64854.2802)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1366  max mem: 15572
Epoch: [4]  [ 730/2809]  eta: 0:20:39  lr: 0.000040  min_lr: 0.000000  loss: 4.6396 (4.6164)  class_acc: 0.0833 (0.0910)  loss_scale: 65536.0000 (64863.6060)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1063  max mem: 15572
Epoch: [4]  [ 740/2809]  eta: 0:20:37  lr: 0.000040  min_lr: 0.000000  loss: 4.6423 (4.6169)  class_acc: 0.0833 (0.0910)  loss_scale: 65536.0000 (64872.6802)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1971  max mem: 15572
Epoch: [4]  [ 750/2809]  eta: 0:20:30  lr: 0.000040  min_lr: 0.000000  loss: 4.4860 (4.6149)  class_acc: 0.0833 (0.0915)  loss_scale: 65536.0000 (64881.5126)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1901  max mem: 15572
Epoch: [4]  [ 760/2809]  eta: 0:20:23  lr: 0.000040  min_lr: 0.000000  loss: 4.5064 (4.6157)  class_acc: 0.1250 (0.0916)  loss_scale: 65536.0000 (64890.1130)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1124  max mem: 15572
[2025-01-15 16:26:34,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=70, lr=[3.8803422587279e-07, 3.8803422587279e-07, 5.543346083897001e-07, 5.543346083897001e-07, 7.919065834138574e-07, 7.919065834138574e-07, 1.1312951191626535e-06, 1.1312951191626535e-06, 1.6161358845180763e-06, 1.6161358845180763e-06, 2.3087655493115375e-06, 2.3087655493115375e-06, 3.298236499016483e-06, 3.298236499016483e-06, 4.7117664271664045e-06, 4.7117664271664045e-06, 6.731094895952006e-06, 6.731094895952006e-06, 9.61584985136001e-06, 9.61584985136001e-06, 1.3736928359085727e-05, 1.3736928359085727e-05, 1.9624183370122472e-05, 1.9624183370122472e-05, 2.8034547671603533e-05, 2.8034547671603533e-05, 4.0049353816576476e-05, 4.0049353816576476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:26:34,681] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=28.44502722106787, CurrSamplesPerSec=32.08598481878355, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-15 16:26:35,068] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:26:35,068] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [ 770/2809]  eta: 0:20:17  lr: 0.000040  min_lr: 0.000000  loss: 4.5832 (4.6152)  class_acc: 0.0833 (0.0915)  loss_scale: 65536.0000 (65493.4994)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1422  max mem: 15572
[2025-01-15 16:26:40,336] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12009
[2025-01-15 16:26:40,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:26:40,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 780/2809]  eta: 0:20:12  lr: 0.000040  min_lr: 0.000000  loss: 4.5510 (4.6136)  class_acc: 0.0833 (0.0918)  loss_scale: 65536.0000 (65661.8694)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1703  max mem: 15572
Epoch: [4]  [ 790/2809]  eta: 0:20:06  lr: 0.000040  min_lr: 0.000000  loss: 4.5696 (4.6147)  class_acc: 0.0833 (0.0917)  loss_scale: 65536.0000 (65660.2781)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1838  max mem: 15572
Epoch: [4]  [ 800/2809]  eta: 0:20:01  lr: 0.000040  min_lr: 0.000000  loss: 4.5628 (4.6137)  class_acc: 0.0833 (0.0919)  loss_scale: 65536.0000 (65658.7266)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1653  max mem: 15572
Epoch: [4]  [ 810/2809]  eta: 0:19:53  lr: 0.000040  min_lr: 0.000000  loss: 4.5032 (4.6126)  class_acc: 0.0833 (0.0922)  loss_scale: 65536.0000 (65657.2133)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1097  max mem: 15572
Epoch: [4]  [ 820/2809]  eta: 0:19:46  lr: 0.000040  min_lr: 0.000000  loss: 4.5941 (4.6119)  class_acc: 0.0833 (0.0923)  loss_scale: 65536.0000 (65655.7369)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0907  max mem: 15572
Epoch: [4]  [ 830/2809]  eta: 0:19:40  lr: 0.000040  min_lr: 0.000000  loss: 4.5941 (4.6109)  class_acc: 0.0833 (0.0926)  loss_scale: 65536.0000 (65654.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1443  max mem: 15572
Epoch: [4]  [ 840/2809]  eta: 0:19:32  lr: 0.000040  min_lr: 0.000000  loss: 4.4686 (4.6102)  class_acc: 0.0833 (0.0924)  loss_scale: 65536.0000 (65652.8894)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1210  max mem: 15572
Epoch: [4]  [ 850/2809]  eta: 0:19:26  lr: 0.000040  min_lr: 0.000000  loss: 4.6558 (4.6114)  class_acc: 0.0833 (0.0921)  loss_scale: 65536.0000 (65651.5159)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1175  max mem: 15572
Epoch: [4]  [ 860/2809]  eta: 0:19:20  lr: 0.000040  min_lr: 0.000000  loss: 4.6231 (4.6093)  class_acc: 0.0833 (0.0926)  loss_scale: 65536.0000 (65650.1742)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1529  max mem: 15572
Epoch: [4]  [ 870/2809]  eta: 0:19:13  lr: 0.000040  min_lr: 0.000000  loss: 4.4885 (4.6092)  class_acc: 0.1250 (0.0928)  loss_scale: 65536.0000 (65648.8634)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1253  max mem: 15572
Epoch: [4]  [ 880/2809]  eta: 0:19:07  lr: 0.000040  min_lr: 0.000000  loss: 4.5325 (4.6069)  class_acc: 0.0833 (0.0930)  loss_scale: 65536.0000 (65647.5823)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1103  max mem: 15572
Epoch: [4]  [ 890/2809]  eta: 0:19:00  lr: 0.000040  min_lr: 0.000000  loss: 4.4573 (4.6063)  class_acc: 0.0833 (0.0931)  loss_scale: 65536.0000 (65646.3300)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0916  max mem: 15572
Epoch: [4]  [ 900/2809]  eta: 0:18:54  lr: 0.000041  min_lr: 0.000000  loss: 4.5266 (4.6059)  class_acc: 0.0833 (0.0934)  loss_scale: 65536.0000 (65645.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1034  max mem: 15572
[2025-01-15 16:27:55,710] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:27:55,710] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:27:56,610] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12140
[2025-01-15 16:27:56,611] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:27:56,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [ 910/2809]  eta: 0:18:49  lr: 0.000041  min_lr: 0.000000  loss: 4.5423 (4.6052)  class_acc: 0.1250 (0.0938)  loss_scale: 65536.0000 (65787.7849)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1644  max mem: 15572
Epoch: [4]  [ 920/2809]  eta: 0:18:44  lr: 0.000041  min_lr: 0.000000  loss: 4.5749 (4.6051)  class_acc: 0.0833 (0.0938)  loss_scale: 65536.0000 (65785.0510)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1939  max mem: 15572
Epoch: [4]  [ 930/2809]  eta: 0:18:36  lr: 0.000041  min_lr: 0.000000  loss: 4.6368 (4.6053)  class_acc: 0.0833 (0.0938)  loss_scale: 65536.0000 (65782.3759)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1375  max mem: 15572
Epoch: [4]  [ 940/2809]  eta: 0:18:31  lr: 0.000041  min_lr: 0.000000  loss: 4.5674 (4.6039)  class_acc: 0.1250 (0.0942)  loss_scale: 65536.0000 (65779.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1378  max mem: 15572
Epoch: [4]  [ 950/2809]  eta: 0:18:25  lr: 0.000041  min_lr: 0.000000  loss: 4.5674 (4.6038)  class_acc: 0.0417 (0.0937)  loss_scale: 65536.0000 (65777.1945)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1730  max mem: 15572
Epoch: [4]  [ 960/2809]  eta: 0:18:19  lr: 0.000041  min_lr: 0.000000  loss: 4.6375 (4.6036)  class_acc: 0.0417 (0.0937)  loss_scale: 65536.0000 (65774.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1549  max mem: 15572
Epoch: [4]  [ 970/2809]  eta: 0:18:12  lr: 0.000041  min_lr: 0.000000  loss: 4.6237 (4.6034)  class_acc: 0.0417 (0.0933)  loss_scale: 65536.0000 (65772.2266)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1162  max mem: 15572
Epoch: [4]  [ 980/2809]  eta: 0:18:07  lr: 0.000041  min_lr: 0.000000  loss: 4.5321 (4.6036)  class_acc: 0.0417 (0.0930)  loss_scale: 65536.0000 (65769.8186)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1459  max mem: 15572
Epoch: [4]  [ 990/2809]  eta: 0:18:00  lr: 0.000041  min_lr: 0.000000  loss: 4.4895 (4.6026)  class_acc: 0.0833 (0.0931)  loss_scale: 65536.0000 (65767.4591)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1469  max mem: 15572
[2025-01-15 16:28:48,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12230
[2025-01-15 16:28:48,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:28:48,887] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [1000/2809]  eta: 0:17:53  lr: 0.000041  min_lr: 0.000000  loss: 4.5026 (4.6022)  class_acc: 0.0833 (0.0929)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0871  max mem: 15572
Epoch: [4]  [1010/2809]  eta: 0:17:48  lr: 0.000041  min_lr: 0.000000  loss: 4.5230 (4.6017)  class_acc: 0.0833 (0.0931)  loss_scale: 32768.0000 (65211.8853)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1516  max mem: 15572
Epoch: [4]  [1020/2809]  eta: 0:17:42  lr: 0.000041  min_lr: 0.000000  loss: 4.5542 (4.6010)  class_acc: 0.1250 (0.0935)  loss_scale: 32768.0000 (64894.1195)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1745  max mem: 15572
Epoch: [4]  [1030/2809]  eta: 0:17:36  lr: 0.000041  min_lr: 0.000000  loss: 4.4928 (4.6008)  class_acc: 0.0833 (0.0935)  loss_scale: 32768.0000 (64582.5179)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1178  max mem: 15572
Epoch: [4]  [1040/2809]  eta: 0:17:31  lr: 0.000041  min_lr: 0.000000  loss: 4.4620 (4.6000)  class_acc: 0.1250 (0.0941)  loss_scale: 32768.0000 (64276.9030)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1259  max mem: 15572
Epoch: [4]  [1050/2809]  eta: 0:17:25  lr: 0.000041  min_lr: 0.000000  loss: 4.5365 (4.6002)  class_acc: 0.1250 (0.0942)  loss_scale: 32768.0000 (63977.1037)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1554  max mem: 15572
Epoch: [4]  [1060/2809]  eta: 0:17:19  lr: 0.000041  min_lr: 0.000000  loss: 4.6518 (4.6012)  class_acc: 0.0833 (0.0939)  loss_scale: 32768.0000 (63682.9557)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1588  max mem: 15572
Epoch: [4]  [1070/2809]  eta: 0:17:12  lr: 0.000041  min_lr: 0.000000  loss: 4.6797 (4.6011)  class_acc: 0.0417 (0.0938)  loss_scale: 32768.0000 (63394.3007)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1016  max mem: 15572
Epoch: [4]  [1080/2809]  eta: 0:17:06  lr: 0.000041  min_lr: 0.000000  loss: 4.5896 (4.6013)  class_acc: 0.0833 (0.0937)  loss_scale: 32768.0000 (63110.9861)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1015  max mem: 15572
Epoch: [4]  [1090/2809]  eta: 0:16:59  lr: 0.000041  min_lr: 0.000000  loss: 4.7357 (4.6019)  class_acc: 0.0833 (0.0938)  loss_scale: 32768.0000 (62832.8653)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1495  max mem: 15572
Epoch: [4]  [1100/2809]  eta: 0:16:53  lr: 0.000041  min_lr: 0.000000  loss: 4.6152 (4.6001)  class_acc: 0.1250 (0.0940)  loss_scale: 32768.0000 (62559.7965)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1318  max mem: 15572
Epoch: [4]  [1110/2809]  eta: 0:16:46  lr: 0.000041  min_lr: 0.000000  loss: 4.5371 (4.6007)  class_acc: 0.0833 (0.0939)  loss_scale: 32768.0000 (62291.6436)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1017  max mem: 15572
Epoch: [4]  [1120/2809]  eta: 0:16:41  lr: 0.000041  min_lr: 0.000000  loss: 4.5695 (4.6010)  class_acc: 0.0417 (0.0936)  loss_scale: 32768.0000 (62028.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1276  max mem: 15572
[2025-01-15 16:30:03,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:30:03,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [1130/2809]  eta: 0:16:36  lr: 0.000041  min_lr: 0.000000  loss: 4.5395 (4.6009)  class_acc: 0.0417 (0.0934)  loss_scale: 32768.0000 (62001.3439)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.2010  max mem: 15572
Epoch: [4]  [1140/2809]  eta: 0:16:30  lr: 0.000041  min_lr: 0.000000  loss: 4.5395 (4.6000)  class_acc: 0.0833 (0.0935)  loss_scale: 65536.0000 (62032.3225)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1904  max mem: 15572
Epoch: [4]  [1150/2809]  eta: 0:16:24  lr: 0.000041  min_lr: 0.000000  loss: 4.5649 (4.5999)  class_acc: 0.0833 (0.0933)  loss_scale: 65536.0000 (62062.7628)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1491  max mem: 15572
Epoch: [4]  [1160/2809]  eta: 0:16:19  lr: 0.000041  min_lr: 0.000000  loss: 4.5574 (4.5996)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (62092.6787)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1603  max mem: 15572
Epoch: [4]  [1170/2809]  eta: 0:16:12  lr: 0.000041  min_lr: 0.000000  loss: 4.5574 (4.6004)  class_acc: 0.0417 (0.0929)  loss_scale: 65536.0000 (62122.0837)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1392  max mem: 15572
Epoch: [4]  [1180/2809]  eta: 0:16:05  lr: 0.000041  min_lr: 0.000000  loss: 4.5373 (4.5998)  class_acc: 0.0833 (0.0931)  loss_scale: 65536.0000 (62150.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0942  max mem: 15572
Epoch: [4]  [1190/2809]  eta: 0:16:00  lr: 0.000041  min_lr: 0.000000  loss: 4.5355 (4.5999)  class_acc: 0.0833 (0.0932)  loss_scale: 65536.0000 (62179.4123)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1638  max mem: 15572
Epoch: [4]  [1200/2809]  eta: 0:15:54  lr: 0.000042  min_lr: 0.000000  loss: 4.5987 (4.5992)  class_acc: 0.1250 (0.0936)  loss_scale: 65536.0000 (62207.3605)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1564  max mem: 15572
Epoch: [4]  [1210/2809]  eta: 0:15:49  lr: 0.000042  min_lr: 0.000000  loss: 4.5820 (4.5992)  class_acc: 0.1250 (0.0940)  loss_scale: 65536.0000 (62234.8472)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1546  max mem: 15572
Epoch: [4]  [1220/2809]  eta: 0:15:42  lr: 0.000042  min_lr: 0.000000  loss: 4.5820 (4.5988)  class_acc: 0.1250 (0.0940)  loss_scale: 65536.0000 (62261.8837)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1569  max mem: 15572
Epoch: [4]  [1230/2809]  eta: 0:15:37  lr: 0.000042  min_lr: 0.000000  loss: 4.5001 (4.5971)  class_acc: 0.1250 (0.0944)  loss_scale: 65536.0000 (62288.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1164  max mem: 15572
Epoch: [4]  [1240/2809]  eta: 0:15:30  lr: 0.000042  min_lr: 0.000000  loss: 4.5001 (4.5974)  class_acc: 0.1250 (0.0943)  loss_scale: 65536.0000 (62314.6495)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1408  max mem: 15572
Epoch: [4]  [1250/2809]  eta: 0:15:25  lr: 0.000042  min_lr: 0.000000  loss: 4.6665 (4.5980)  class_acc: 0.0417 (0.0941)  loss_scale: 65536.0000 (62340.3997)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1524  max mem: 15572
[2025-01-15 16:31:20,765] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:31:20,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:31:21,306] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12488
[2025-01-15 16:31:21,306] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:31:21,306] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1260/2809]  eta: 0:15:18  lr: 0.000042  min_lr: 0.000000  loss: 4.7102 (4.5986)  class_acc: 0.0833 (0.0941)  loss_scale: 65536.0000 (62417.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1111  max mem: 15572
Epoch: [4]  [1270/2809]  eta: 0:15:13  lr: 0.000042  min_lr: 0.000000  loss: 4.6250 (4.5980)  class_acc: 0.0833 (0.0942)  loss_scale: 65536.0000 (62442.2470)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1383  max mem: 15572
Epoch: [4]  [1280/2809]  eta: 0:15:06  lr: 0.000042  min_lr: 0.000000  loss: 4.5813 (4.5979)  class_acc: 0.0833 (0.0942)  loss_scale: 65536.0000 (62466.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1399  max mem: 15572
Epoch: [4]  [1290/2809]  eta: 0:15:00  lr: 0.000042  min_lr: 0.000000  loss: 4.6259 (4.5981)  class_acc: 0.0833 (0.0944)  loss_scale: 65536.0000 (62490.1751)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0969  max mem: 15572
Epoch: [4]  [1300/2809]  eta: 0:14:54  lr: 0.000042  min_lr: 0.000000  loss: 4.5280 (4.5984)  class_acc: 0.0833 (0.0943)  loss_scale: 65536.0000 (62513.5865)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1604  max mem: 15572
Epoch: [4]  [1310/2809]  eta: 0:14:48  lr: 0.000042  min_lr: 0.000000  loss: 4.5828 (4.5981)  class_acc: 0.0833 (0.0945)  loss_scale: 65536.0000 (62536.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1377  max mem: 15572
Epoch: [4]  [1320/2809]  eta: 0:14:42  lr: 0.000042  min_lr: 0.000000  loss: 4.5571 (4.5974)  class_acc: 0.0833 (0.0947)  loss_scale: 65536.0000 (62559.3460)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1430  max mem: 15572
Epoch: [4]  [1330/2809]  eta: 0:14:36  lr: 0.000042  min_lr: 0.000000  loss: 4.5322 (4.5967)  class_acc: 0.1250 (0.0950)  loss_scale: 65536.0000 (62581.7100)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1534  max mem: 15572
Epoch: [4]  [1340/2809]  eta: 0:14:29  lr: 0.000042  min_lr: 0.000000  loss: 4.6037 (4.5971)  class_acc: 0.0833 (0.0950)  loss_scale: 65536.0000 (62603.7405)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1071  max mem: 15572
Epoch: [4]  [1350/2809]  eta: 0:14:24  lr: 0.000042  min_lr: 0.000000  loss: 4.6612 (4.5971)  class_acc: 0.0833 (0.0951)  loss_scale: 65536.0000 (62625.4449)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1480  max mem: 15572
Epoch: [4]  [1360/2809]  eta: 0:14:19  lr: 0.000042  min_lr: 0.000000  loss: 4.4765 (4.5963)  class_acc: 0.0417 (0.0950)  loss_scale: 65536.0000 (62646.8303)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1933  max mem: 15572
Epoch: [4]  [1370/2809]  eta: 0:14:14  lr: 0.000042  min_lr: 0.000000  loss: 4.4765 (4.5957)  class_acc: 0.0833 (0.0951)  loss_scale: 65536.0000 (62667.9037)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.2100  max mem: 15572
Epoch: [4]  [1380/2809]  eta: 0:14:06  lr: 0.000042  min_lr: 0.000000  loss: 4.5337 (4.5962)  class_acc: 0.0833 (0.0951)  loss_scale: 65536.0000 (62688.6720)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1179  max mem: 15572
[2025-01-15 16:32:36,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:32:36,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:32:39,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12621
[2025-01-15 16:32:39,707] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:32:39,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1390/2809]  eta: 0:14:00  lr: 0.000042  min_lr: 0.000000  loss: 4.5729 (4.5958)  class_acc: 0.0833 (0.0954)  loss_scale: 65536.0000 (62897.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0634  max mem: 15572
Epoch: [4]  [1400/2809]  eta: 0:13:55  lr: 0.000042  min_lr: 0.000000  loss: 4.5744 (4.5957)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (62916.4311)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1677  max mem: 15572
Epoch: [4]  [1410/2809]  eta: 0:13:50  lr: 0.000042  min_lr: 0.000000  loss: 4.6538 (4.5958)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (62934.9965)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.2174  max mem: 15572
Epoch: [4]  [1420/2809]  eta: 0:13:43  lr: 0.000042  min_lr: 0.000000  loss: 4.5953 (4.5958)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (62953.3005)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1463  max mem: 15572
Epoch: [4]  [1430/2809]  eta: 0:13:37  lr: 0.000042  min_lr: 0.000000  loss: 4.5862 (4.5958)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (62971.3487)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1224  max mem: 15572
Epoch: [4]  [1440/2809]  eta: 0:13:31  lr: 0.000042  min_lr: 0.000000  loss: 4.5820 (4.5957)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (62989.1464)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1443  max mem: 15572
Epoch: [4]  [1450/2809]  eta: 0:13:25  lr: 0.000042  min_lr: 0.000000  loss: 4.6160 (4.5958)  class_acc: 0.0833 (0.0953)  loss_scale: 65536.0000 (63006.6988)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1573  max mem: 15572
Epoch: [4]  [1460/2809]  eta: 0:13:20  lr: 0.000042  min_lr: 0.000000  loss: 4.5939 (4.5957)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (63024.0110)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.2235  max mem: 15572
Epoch: [4]  [1470/2809]  eta: 0:13:14  lr: 0.000042  min_lr: 0.000000  loss: 4.5644 (4.5958)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (63041.0877)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1999  max mem: 15572
Epoch: [4]  [1480/2809]  eta: 0:13:10  lr: 0.000042  min_lr: 0.000000  loss: 4.6193 (4.5961)  class_acc: 0.0417 (0.0955)  loss_scale: 65536.0000 (63057.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.2045  max mem: 15572
Epoch: [4]  [1490/2809]  eta: 0:13:02  lr: 0.000042  min_lr: 0.000000  loss: 4.6115 (4.5956)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (63074.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1264  max mem: 15572
Epoch: [4]  [1500/2809]  eta: 0:12:56  lr: 0.000043  min_lr: 0.000000  loss: 4.5557 (4.5952)  class_acc: 0.0833 (0.0959)  loss_scale: 65536.0000 (63090.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0689  max mem: 15572
Epoch: [4]  [1510/2809]  eta: 0:12:50  lr: 0.000043  min_lr: 0.000000  loss: 4.5613 (4.5950)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (63107.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1471  max mem: 15572
[2025-01-15 16:33:57,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:33:57,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:33:59,492] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12754
[2025-01-15 16:33:59,493] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:33:59,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1520/2809]  eta: 0:12:45  lr: 0.000043  min_lr: 0.000000  loss: 4.5485 (4.5942)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (63295.4530)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1569  max mem: 15572
Epoch: [4]  [1530/2809]  eta: 0:12:39  lr: 0.000043  min_lr: 0.000000  loss: 4.4744 (4.5941)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (63310.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1531  max mem: 15572
Epoch: [4]  [1540/2809]  eta: 0:12:33  lr: 0.000043  min_lr: 0.000000  loss: 4.6205 (4.5948)  class_acc: 0.0833 (0.0957)  loss_scale: 65536.0000 (63324.5321)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1666  max mem: 15572
Epoch: [4]  [1550/2809]  eta: 0:12:28  lr: 0.000043  min_lr: 0.000000  loss: 4.6257 (4.5945)  class_acc: 0.0833 (0.0957)  loss_scale: 65536.0000 (63338.7905)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1785  max mem: 15572
Epoch: [4]  [1560/2809]  eta: 0:12:21  lr: 0.000043  min_lr: 0.000000  loss: 4.6300 (4.5945)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (63352.8661)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1354  max mem: 15572
Epoch: [4]  [1570/2809]  eta: 0:12:15  lr: 0.000043  min_lr: 0.000000  loss: 4.6809 (4.5954)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (63366.7626)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.0937  max mem: 15572
Epoch: [4]  [1580/2809]  eta: 0:12:09  lr: 0.000043  min_lr: 0.000000  loss: 4.7046 (4.5960)  class_acc: 0.0417 (0.0954)  loss_scale: 65536.0000 (63380.4832)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0942  max mem: 15572
Epoch: [4]  [1590/2809]  eta: 0:12:04  lr: 0.000043  min_lr: 0.000000  loss: 4.7040 (4.5961)  class_acc: 0.0417 (0.0953)  loss_scale: 65536.0000 (63394.0314)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.1896  max mem: 15572
Epoch: [4]  [1600/2809]  eta: 0:11:57  lr: 0.000043  min_lr: 0.000000  loss: 4.6055 (4.5962)  class_acc: 0.0833 (0.0953)  loss_scale: 65536.0000 (63407.4104)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1601  max mem: 15572
Epoch: [4]  [1610/2809]  eta: 0:11:52  lr: 0.000043  min_lr: 0.000000  loss: 4.6353 (4.5959)  class_acc: 0.0833 (0.0955)  loss_scale: 65536.0000 (63420.6232)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1043  max mem: 15572
Epoch: [4]  [1620/2809]  eta: 0:11:45  lr: 0.000043  min_lr: 0.000000  loss: 4.5445 (4.5951)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (63433.6730)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1222  max mem: 15572
Epoch: [4]  [1630/2809]  eta: 0:11:38  lr: 0.000043  min_lr: 0.000000  loss: 4.5355 (4.5957)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (63446.5628)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0778  max mem: 15572
Epoch: [4]  [1640/2809]  eta: 0:11:33  lr: 0.000043  min_lr: 0.000000  loss: 4.5649 (4.5956)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (63459.2956)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1635  max mem: 15572
[2025-01-15 16:35:15,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:35:15,811] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1650/2809]  eta: 0:11:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5732 (4.5959)  class_acc: 0.0833 (0.0959)  loss_scale: 65536.0000 (63630.6529)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1770  max mem: 15572
[2025-01-15 16:35:22,232] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12896
[2025-01-15 16:35:22,232] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:35:22,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1660/2809]  eta: 0:11:20  lr: 0.000043  min_lr: 0.000000  loss: 4.5756 (4.5957)  class_acc: 0.0833 (0.0957)  loss_scale: 131072.0000 (63997.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0677  max mem: 15572
Epoch: [4]  [1670/2809]  eta: 0:11:15  lr: 0.000043  min_lr: 0.000000  loss: 4.5786 (4.5963)  class_acc: 0.0833 (0.0956)  loss_scale: 65536.0000 (64006.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0903  max mem: 15572
Epoch: [4]  [1680/2809]  eta: 0:11:09  lr: 0.000043  min_lr: 0.000000  loss: 4.5627 (4.5953)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (64015.5336)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1525  max mem: 15572
Epoch: [4]  [1690/2809]  eta: 0:11:03  lr: 0.000043  min_lr: 0.000000  loss: 4.6311 (4.5960)  class_acc: 0.0833 (0.0958)  loss_scale: 65536.0000 (64024.5251)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1566  max mem: 15572
Epoch: [4]  [1700/2809]  eta: 0:10:56  lr: 0.000043  min_lr: 0.000000  loss: 4.5900 (4.5950)  class_acc: 0.0833 (0.0959)  loss_scale: 65536.0000 (64033.4109)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1550  max mem: 15572
Epoch: [4]  [1710/2809]  eta: 0:10:51  lr: 0.000043  min_lr: 0.000000  loss: 4.4490 (4.5948)  class_acc: 0.1250 (0.0960)  loss_scale: 65536.0000 (64042.1929)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1608  max mem: 15572
Epoch: [4]  [1720/2809]  eta: 0:10:45  lr: 0.000043  min_lr: 0.000000  loss: 4.4490 (4.5944)  class_acc: 0.1250 (0.0961)  loss_scale: 65536.0000 (64050.8727)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1763  max mem: 15572
Epoch: [4]  [1730/2809]  eta: 0:10:39  lr: 0.000043  min_lr: 0.000000  loss: 4.5409 (4.5947)  class_acc: 0.0833 (0.0961)  loss_scale: 65536.0000 (64059.4523)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1568  max mem: 15572
Epoch: [4]  [1740/2809]  eta: 0:10:33  lr: 0.000043  min_lr: 0.000000  loss: 4.6582 (4.5948)  class_acc: 0.0833 (0.0960)  loss_scale: 65536.0000 (64067.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1238  max mem: 15572
Epoch: [4]  [1750/2809]  eta: 0:10:27  lr: 0.000043  min_lr: 0.000000  loss: 4.5541 (4.5944)  class_acc: 0.0417 (0.0960)  loss_scale: 65536.0000 (64076.3175)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1396  max mem: 15572
Epoch: [4]  [1760/2809]  eta: 0:10:21  lr: 0.000043  min_lr: 0.000000  loss: 4.4355 (4.5932)  class_acc: 0.0833 (0.0963)  loss_scale: 65536.0000 (64084.6065)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1719  max mem: 15572
[2025-01-15 16:36:24,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=77, lr=[4.2037310626888885e-07, 4.2037310626888885e-07, 6.005330089555555e-07, 6.005330089555555e-07, 8.579042985079365e-07, 8.579042985079365e-07, 1.2255775692970524e-06, 1.2255775692970524e-06, 1.750825098995789e-06, 1.750825098995789e-06, 2.5011787128511272e-06, 2.5011787128511272e-06, 3.573112446930182e-06, 3.573112446930182e-06, 5.1044463527574035e-06, 5.1044463527574035e-06, 7.292066218224862e-06, 7.292066218224862e-06, 1.0417237454606947e-05, 1.0417237454606947e-05, 1.4881767792295639e-05, 1.4881767792295639e-05, 2.1259668274708057e-05, 2.1259668274708057e-05, 3.037095467815437e-05, 3.037095467815437e-05, 4.33870781116491e-05, 4.33870781116491e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:36:24,277] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=28.448503664329778, CurrSamplesPerSec=30.086231621130327, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [1770/2809]  eta: 0:10:15  lr: 0.000043  min_lr: 0.000000  loss: 4.4513 (4.5931)  class_acc: 0.1250 (0.0965)  loss_scale: 65536.0000 (64092.8018)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1618  max mem: 15572
Epoch: [4]  [1780/2809]  eta: 0:10:09  lr: 0.000043  min_lr: 0.000000  loss: 4.5456 (4.5927)  class_acc: 0.0833 (0.0965)  loss_scale: 65536.0000 (64100.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1162  max mem: 15572
[2025-01-15 16:36:38,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:36:38,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1790/2809]  eta: 0:10:03  lr: 0.000043  min_lr: 0.000000  loss: 4.5446 (4.5927)  class_acc: 0.1250 (0.0965)  loss_scale: 65536.0000 (64182.1016)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1059  max mem: 15572
[2025-01-15 16:36:44,452] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13036
[2025-01-15 16:36:44,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:36:44,452] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1800/2809]  eta: 0:09:57  lr: 0.000044  min_lr: 0.000000  loss: 4.5972 (4.5931)  class_acc: 0.1250 (0.0965)  loss_scale: 131072.0000 (64517.1172)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1284  max mem: 15572
Epoch: [4]  [1810/2809]  eta: 0:09:51  lr: 0.000044  min_lr: 0.000000  loss: 4.6077 (4.5932)  class_acc: 0.1250 (0.0967)  loss_scale: 65536.0000 (64522.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1099  max mem: 15572
Epoch: [4]  [1820/2809]  eta: 0:09:45  lr: 0.000044  min_lr: 0.000000  loss: 4.6077 (4.5932)  class_acc: 0.0833 (0.0966)  loss_scale: 65536.0000 (64528.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1112  max mem: 15572
Epoch: [4]  [1830/2809]  eta: 0:09:39  lr: 0.000044  min_lr: 0.000000  loss: 4.4610 (4.5920)  class_acc: 0.0833 (0.0969)  loss_scale: 65536.0000 (64533.8110)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1638  max mem: 15572
Epoch: [4]  [1840/2809]  eta: 0:09:33  lr: 0.000044  min_lr: 0.000000  loss: 4.4629 (4.5918)  class_acc: 0.1250 (0.0969)  loss_scale: 65536.0000 (64539.2548)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1635  max mem: 15572
Epoch: [4]  [1850/2809]  eta: 0:09:27  lr: 0.000044  min_lr: 0.000000  loss: 4.6135 (4.5916)  class_acc: 0.0417 (0.0969)  loss_scale: 65536.0000 (64544.6397)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1682  max mem: 15572
Epoch: [4]  [1860/2809]  eta: 0:09:22  lr: 0.000044  min_lr: 0.000000  loss: 4.6239 (4.5918)  class_acc: 0.0417 (0.0968)  loss_scale: 65536.0000 (64549.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1892  max mem: 15572
Epoch: [4]  [1870/2809]  eta: 0:09:16  lr: 0.000044  min_lr: 0.000000  loss: 4.5896 (4.5920)  class_acc: 0.0417 (0.0968)  loss_scale: 65536.0000 (64555.2368)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1445  max mem: 15572
Epoch: [4]  [1880/2809]  eta: 0:09:10  lr: 0.000044  min_lr: 0.000000  loss: 4.5242 (4.5913)  class_acc: 0.0833 (0.0971)  loss_scale: 65536.0000 (64560.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1296  max mem: 15572
Epoch: [4]  [1890/2809]  eta: 0:09:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4716 (4.5913)  class_acc: 0.1250 (0.0972)  loss_scale: 65536.0000 (64565.6097)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1366  max mem: 15572
Epoch: [4]  [1900/2809]  eta: 0:08:58  lr: 0.000044  min_lr: 0.000000  loss: 4.5952 (4.5915)  class_acc: 0.0833 (0.0971)  loss_scale: 65536.0000 (64570.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1215  max mem: 15572
Epoch: [4]  [1910/2809]  eta: 0:08:52  lr: 0.000044  min_lr: 0.000000  loss: 4.6305 (4.5914)  class_acc: 0.0833 (0.0970)  loss_scale: 65536.0000 (64575.7656)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1075  max mem: 15572
Epoch: [4]  [1920/2809]  eta: 0:08:46  lr: 0.000044  min_lr: 0.000000  loss: 4.6305 (4.5914)  class_acc: 0.0833 (0.0971)  loss_scale: 65536.0000 (64580.7642)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1152  max mem: 15572
[2025-01-15 16:38:00,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:38:00,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [1930/2809]  eta: 0:08:40  lr: 0.000044  min_lr: 0.000000  loss: 4.4590 (4.5903)  class_acc: 0.1250 (0.0975)  loss_scale: 65536.0000 (64653.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1224  max mem: 15572
[2025-01-15 16:38:05,817] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13174
[2025-01-15 16:38:05,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:38:05,818] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [1940/2809]  eta: 0:08:34  lr: 0.000044  min_lr: 0.000000  loss: 4.3499 (4.5899)  class_acc: 0.1250 (0.0976)  loss_scale: 65536.0000 (64894.4833)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1163  max mem: 15572
Epoch: [4]  [1950/2809]  eta: 0:08:28  lr: 0.000044  min_lr: 0.000000  loss: 4.5617 (4.5895)  class_acc: 0.1250 (0.0977)  loss_scale: 65536.0000 (64897.7714)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1863  max mem: 15572
Epoch: [4]  [1960/2809]  eta: 0:08:22  lr: 0.000044  min_lr: 0.000000  loss: 4.5172 (4.5888)  class_acc: 0.1250 (0.0978)  loss_scale: 65536.0000 (64901.0260)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1265  max mem: 15572
Epoch: [4]  [1970/2809]  eta: 0:08:16  lr: 0.000044  min_lr: 0.000000  loss: 4.4273 (4.5881)  class_acc: 0.1250 (0.0980)  loss_scale: 65536.0000 (64904.2476)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0767  max mem: 15572
Epoch: [4]  [1980/2809]  eta: 0:08:10  lr: 0.000044  min_lr: 0.000000  loss: 4.5596 (4.5883)  class_acc: 0.0833 (0.0980)  loss_scale: 65536.0000 (64907.4366)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1585  max mem: 15572
Epoch: [4]  [1990/2809]  eta: 0:08:04  lr: 0.000044  min_lr: 0.000000  loss: 4.6806 (4.5884)  class_acc: 0.0833 (0.0980)  loss_scale: 65536.0000 (64910.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1888  max mem: 15572
Epoch: [4]  [2000/2809]  eta: 0:07:59  lr: 0.000044  min_lr: 0.000000  loss: 4.4382 (4.5878)  class_acc: 0.1250 (0.0981)  loss_scale: 65536.0000 (64913.7191)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1854  max mem: 15572
Epoch: [4]  [2010/2809]  eta: 0:07:53  lr: 0.000044  min_lr: 0.000000  loss: 4.4239 (4.5865)  class_acc: 0.0833 (0.0981)  loss_scale: 65536.0000 (64916.8135)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1001  max mem: 15572
Epoch: [4]  [2020/2809]  eta: 0:07:47  lr: 0.000044  min_lr: 0.000000  loss: 4.4895 (4.5866)  class_acc: 0.0833 (0.0982)  loss_scale: 65536.0000 (64919.8773)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0974  max mem: 15572
Epoch: [4]  [2030/2809]  eta: 0:07:40  lr: 0.000044  min_lr: 0.000000  loss: 4.5604 (4.5861)  class_acc: 0.0833 (0.0982)  loss_scale: 65536.0000 (64922.9109)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0969  max mem: 15572
Epoch: [4]  [2040/2809]  eta: 0:07:34  lr: 0.000044  min_lr: 0.000000  loss: 4.4405 (4.5855)  class_acc: 0.1250 (0.0984)  loss_scale: 65536.0000 (64925.9147)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0547  max mem: 15572
Epoch: [4]  [2050/2809]  eta: 0:07:28  lr: 0.000044  min_lr: 0.000000  loss: 4.4405 (4.5851)  class_acc: 0.1250 (0.0986)  loss_scale: 65536.0000 (64928.8893)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0897  max mem: 15572
Epoch: [4]  [2060/2809]  eta: 0:07:22  lr: 0.000044  min_lr: 0.000000  loss: 4.6034 (4.5856)  class_acc: 0.0833 (0.0985)  loss_scale: 65536.0000 (64931.8350)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0943  max mem: 15572
[2025-01-15 16:39:20,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:39:20,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2070/2809]  eta: 0:07:16  lr: 0.000044  min_lr: 0.000000  loss: 4.7171 (4.5859)  class_acc: 0.0417 (0.0984)  loss_scale: 65536.0000 (65061.3308)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1057  max mem: 15572
[2025-01-15 16:39:25,729] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13315
[2025-01-15 16:39:25,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:39:25,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2080/2809]  eta: 0:07:10  lr: 0.000044  min_lr: 0.000000  loss: 4.4965 (4.5852)  class_acc: 0.0833 (0.0985)  loss_scale: 131072.0000 (65315.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0880  max mem: 15572
Epoch: [4]  [2090/2809]  eta: 0:07:04  lr: 0.000044  min_lr: 0.000000  loss: 4.4701 (4.5851)  class_acc: 0.1250 (0.0986)  loss_scale: 65536.0000 (65316.6064)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0973  max mem: 15572
Epoch: [4]  [2100/2809]  eta: 0:06:58  lr: 0.000045  min_lr: 0.000000  loss: 4.5032 (4.5847)  class_acc: 0.0833 (0.0987)  loss_scale: 65536.0000 (65317.6506)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1280  max mem: 15572
Epoch: [4]  [2110/2809]  eta: 0:06:52  lr: 0.000045  min_lr: 0.000000  loss: 4.5000 (4.5847)  class_acc: 0.1250 (0.0989)  loss_scale: 65536.0000 (65318.6850)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0923  max mem: 15572
Epoch: [4]  [2120/2809]  eta: 0:06:46  lr: 0.000045  min_lr: 0.000000  loss: 4.5368 (4.5846)  class_acc: 0.1250 (0.0988)  loss_scale: 65536.0000 (65319.7096)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1388  max mem: 15572
Epoch: [4]  [2130/2809]  eta: 0:06:40  lr: 0.000045  min_lr: 0.000000  loss: 4.6158 (4.5848)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (65320.7245)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1563  max mem: 15572
Epoch: [4]  [2140/2809]  eta: 0:06:34  lr: 0.000045  min_lr: 0.000000  loss: 4.6671 (4.5850)  class_acc: 0.0833 (0.0987)  loss_scale: 65536.0000 (65321.7300)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0753  max mem: 15572
Epoch: [4]  [2150/2809]  eta: 0:06:28  lr: 0.000045  min_lr: 0.000000  loss: 4.6177 (4.5849)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (65322.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0783  max mem: 15572
Epoch: [4]  [2160/2809]  eta: 0:06:23  lr: 0.000045  min_lr: 0.000000  loss: 4.5918 (4.5847)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (65323.7131)  weight_decay: 0.0500 (0.0500)  time: 0.7031  data: 0.2440  max mem: 15572
Epoch: [4]  [2170/2809]  eta: 0:06:17  lr: 0.000045  min_lr: 0.000000  loss: 4.4803 (4.5841)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (65324.6909)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.2649  max mem: 15572
Epoch: [4]  [2180/2809]  eta: 0:06:11  lr: 0.000045  min_lr: 0.000000  loss: 4.4534 (4.5838)  class_acc: 0.0833 (0.0988)  loss_scale: 65536.0000 (65325.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0998  max mem: 15572
Epoch: [4]  [2190/2809]  eta: 0:06:05  lr: 0.000045  min_lr: 0.000000  loss: 4.6055 (4.5843)  class_acc: 0.0833 (0.0989)  loss_scale: 65536.0000 (65326.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1012  max mem: 15572
Epoch: [4]  [2200/2809]  eta: 0:05:59  lr: 0.000045  min_lr: 0.000000  loss: 4.6669 (4.5846)  class_acc: 0.1250 (0.0989)  loss_scale: 65536.0000 (65327.5711)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1316  max mem: 15572
[2025-01-15 16:40:41,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:40:41,777] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2210/2809]  eta: 0:05:53  lr: 0.000045  min_lr: 0.000000  loss: 4.6088 (4.5844)  class_acc: 0.1250 (0.0992)  loss_scale: 65536.0000 (65417.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.1044  max mem: 15572
[2025-01-15 16:40:43,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13448
[2025-01-15 16:40:43,471] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:40:43,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2220/2809]  eta: 0:05:47  lr: 0.000045  min_lr: 0.000000  loss: 4.6088 (4.5845)  class_acc: 0.0833 (0.0991)  loss_scale: 65536.0000 (65447.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0921  max mem: 15572
Epoch: [4]  [2230/2809]  eta: 0:05:41  lr: 0.000045  min_lr: 0.000000  loss: 4.6176 (4.5846)  class_acc: 0.0833 (0.0991)  loss_scale: 65536.0000 (65447.8745)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1321  max mem: 15572
Epoch: [4]  [2240/2809]  eta: 0:05:35  lr: 0.000045  min_lr: 0.000000  loss: 4.5419 (4.5841)  class_acc: 0.0833 (0.0990)  loss_scale: 65536.0000 (65448.2677)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1202  max mem: 15572
Epoch: [4]  [2250/2809]  eta: 0:05:29  lr: 0.000045  min_lr: 0.000000  loss: 4.6163 (4.5841)  class_acc: 0.0833 (0.0990)  loss_scale: 65536.0000 (65448.6575)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0971  max mem: 15572
Epoch: [4]  [2260/2809]  eta: 0:05:23  lr: 0.000045  min_lr: 0.000000  loss: 4.6250 (4.5843)  class_acc: 0.0833 (0.0990)  loss_scale: 65536.0000 (65449.0438)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1560  max mem: 15572
Epoch: [4]  [2270/2809]  eta: 0:05:17  lr: 0.000045  min_lr: 0.000000  loss: 4.6167 (4.5846)  class_acc: 0.0833 (0.0989)  loss_scale: 65536.0000 (65449.4267)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1229  max mem: 15572
Epoch: [4]  [2280/2809]  eta: 0:05:11  lr: 0.000045  min_lr: 0.000000  loss: 4.6755 (4.5846)  class_acc: 0.0833 (0.0989)  loss_scale: 65536.0000 (65449.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1289  max mem: 15572
Epoch: [4]  [2290/2809]  eta: 0:05:05  lr: 0.000045  min_lr: 0.000000  loss: 4.5504 (4.5841)  class_acc: 0.0833 (0.0990)  loss_scale: 65536.0000 (65450.1825)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1578  max mem: 15572
Epoch: [4]  [2300/2809]  eta: 0:05:00  lr: 0.000045  min_lr: 0.000000  loss: 4.5393 (4.5841)  class_acc: 0.1250 (0.0990)  loss_scale: 65536.0000 (65450.5554)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1417  max mem: 15572
Epoch: [4]  [2310/2809]  eta: 0:04:54  lr: 0.000045  min_lr: 0.000000  loss: 4.5358 (4.5837)  class_acc: 0.1250 (0.0991)  loss_scale: 65536.0000 (65450.9251)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1893  max mem: 15572
Epoch: [4]  [2320/2809]  eta: 0:04:48  lr: 0.000045  min_lr: 0.000000  loss: 4.3296 (4.5821)  class_acc: 0.1250 (0.0995)  loss_scale: 65536.0000 (65451.2917)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1905  max mem: 15572
Epoch: [4]  [2330/2809]  eta: 0:04:42  lr: 0.000045  min_lr: 0.000000  loss: 4.3173 (4.5814)  class_acc: 0.1250 (0.0996)  loss_scale: 65536.0000 (65451.6551)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1702  max mem: 15572
Epoch: [4]  [2340/2809]  eta: 0:04:36  lr: 0.000045  min_lr: 0.000000  loss: 4.5078 (4.5811)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (65452.0154)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1576  max mem: 15572
[2025-01-15 16:42:01,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:42:01,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:42:04,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13583
[2025-01-15 16:42:04,463] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:42:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2350/2809]  eta: 0:04:30  lr: 0.000045  min_lr: 0.000000  loss: 4.5754 (4.5812)  class_acc: 0.0833 (0.0995)  loss_scale: 65536.0000 (65619.6274)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1894  max mem: 15572
Epoch: [4]  [2360/2809]  eta: 0:04:25  lr: 0.000045  min_lr: 0.000000  loss: 4.5671 (4.5811)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (65619.2732)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.2202  max mem: 15572
Epoch: [4]  [2370/2809]  eta: 0:04:19  lr: 0.000045  min_lr: 0.000000  loss: 4.4594 (4.5809)  class_acc: 0.0833 (0.0997)  loss_scale: 65536.0000 (65618.9220)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1792  max mem: 15572
Epoch: [4]  [2380/2809]  eta: 0:04:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4674 (4.5810)  class_acc: 0.0833 (0.0997)  loss_scale: 65536.0000 (65618.5737)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1828  max mem: 15572
Epoch: [4]  [2390/2809]  eta: 0:04:07  lr: 0.000045  min_lr: 0.000000  loss: 4.4888 (4.5807)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (65618.2284)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1585  max mem: 15572
Epoch: [4]  [2400/2809]  eta: 0:04:01  lr: 0.000046  min_lr: 0.000000  loss: 4.4893 (4.5807)  class_acc: 0.0833 (0.0996)  loss_scale: 65536.0000 (65617.8859)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1496  max mem: 15572
Epoch: [4]  [2410/2809]  eta: 0:03:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4055 (4.5797)  class_acc: 0.0833 (0.0999)  loss_scale: 65536.0000 (65617.5462)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1774  max mem: 15572
[2025-01-15 16:42:48,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13654
[2025-01-15 16:42:48,978] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:42:48,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [4]  [2420/2809]  eta: 0:03:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3565 (4.5789)  class_acc: 0.1667 (0.1001)  loss_scale: 65536.0000 (65576.6047)  weight_decay: 0.0500 (0.0500)  time: 0.6742  data: 0.2190  max mem: 15572
Epoch: [4]  [2430/2809]  eta: 0:03:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4660 (4.5788)  class_acc: 0.1250 (0.1002)  loss_scale: 32768.0000 (65441.6454)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1399  max mem: 15572
Epoch: [4]  [2440/2809]  eta: 0:03:38  lr: 0.000046  min_lr: 0.000000  loss: 4.5742 (4.5789)  class_acc: 0.0833 (0.1002)  loss_scale: 32768.0000 (65307.7919)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0649  max mem: 15572
Epoch: [4]  [2450/2809]  eta: 0:03:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4893 (4.5783)  class_acc: 0.1250 (0.1004)  loss_scale: 32768.0000 (65175.0306)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.0975  max mem: 15572
Epoch: [4]  [2460/2809]  eta: 0:03:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3673 (4.5773)  class_acc: 0.1250 (0.1005)  loss_scale: 32768.0000 (65043.3482)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1277  max mem: 15572
Epoch: [4]  [2470/2809]  eta: 0:03:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3707 (4.5770)  class_acc: 0.1250 (0.1007)  loss_scale: 32768.0000 (64912.7317)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1619  max mem: 15572
Epoch: [4]  [2480/2809]  eta: 0:03:14  lr: 0.000046  min_lr: 0.000000  loss: 4.4732 (4.5770)  class_acc: 0.1667 (0.1008)  loss_scale: 32768.0000 (64783.1681)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1507  max mem: 15572
Epoch: [4]  [2490/2809]  eta: 0:03:08  lr: 0.000046  min_lr: 0.000000  loss: 4.5943 (4.5773)  class_acc: 0.1250 (0.1010)  loss_scale: 32768.0000 (64654.6447)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1523  max mem: 15572
Epoch: [4]  [2500/2809]  eta: 0:03:02  lr: 0.000046  min_lr: 0.000000  loss: 4.5840 (4.5769)  class_acc: 0.1250 (0.1011)  loss_scale: 32768.0000 (64527.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0804  max mem: 15572
Epoch: [4]  [2510/2809]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000000  loss: 4.4788 (4.5769)  class_acc: 0.1250 (0.1012)  loss_scale: 32768.0000 (64400.6691)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0926  max mem: 15572
Epoch: [4]  [2520/2809]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000000  loss: 4.6072 (4.5773)  class_acc: 0.0833 (0.1010)  loss_scale: 32768.0000 (64275.1924)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1511  max mem: 15572
Epoch: [4]  [2530/2809]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000000  loss: 4.6472 (4.5772)  class_acc: 0.0417 (0.1011)  loss_scale: 32768.0000 (64150.7072)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1424  max mem: 15572
Epoch: [4]  [2540/2809]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000000  loss: 4.6039 (4.5772)  class_acc: 0.0833 (0.1011)  loss_scale: 32768.0000 (64027.2019)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1317  max mem: 15572
[2025-01-15 16:44:02,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:44:02,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [4]  [2550/2809]  eta: 0:02:32  lr: 0.000046  min_lr: 0.000000  loss: 4.6208 (4.5772)  class_acc: 0.0833 (0.1010)  loss_scale: 32768.0000 (63956.0455)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0999  max mem: 15572
Epoch: [4]  [2560/2809]  eta: 0:02:26  lr: 0.000046  min_lr: 0.000000  loss: 4.6002 (4.5770)  class_acc: 0.0833 (0.1011)  loss_scale: 65536.0000 (63962.2148)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0831  max mem: 15572
Epoch: [4]  [2570/2809]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4648 (4.5768)  class_acc: 0.1250 (0.1011)  loss_scale: 65536.0000 (63968.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.1021  max mem: 15572
Epoch: [4]  [2580/2809]  eta: 0:02:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4283 (4.5760)  class_acc: 0.1250 (0.1013)  loss_scale: 65536.0000 (63974.4099)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0973  max mem: 15572
Epoch: [4]  [2590/2809]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000000  loss: 4.4046 (4.5760)  class_acc: 0.0833 (0.1013)  loss_scale: 65536.0000 (63980.4369)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1314  max mem: 15572
Epoch: [4]  [2600/2809]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5979 (4.5760)  class_acc: 0.0833 (0.1014)  loss_scale: 65536.0000 (63986.4175)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1721  max mem: 15572
Epoch: [4]  [2610/2809]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000000  loss: 4.5466 (4.5759)  class_acc: 0.1250 (0.1013)  loss_scale: 65536.0000 (63992.3524)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1265  max mem: 15572
Epoch: [4]  [2620/2809]  eta: 0:01:51  lr: 0.000046  min_lr: 0.000000  loss: 4.5466 (4.5759)  class_acc: 0.0833 (0.1013)  loss_scale: 65536.0000 (63998.2419)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0596  max mem: 15572
Epoch: [4]  [2630/2809]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000000  loss: 4.5451 (4.5757)  class_acc: 0.0833 (0.1015)  loss_scale: 65536.0000 (64004.0867)  weight_decay: 0.0500 (0.0500)  time: 0.4840  data: 0.0319  max mem: 15572
Epoch: [4]  [2640/2809]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4254 (4.5749)  class_acc: 0.1250 (0.1017)  loss_scale: 65536.0000 (64009.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1215  max mem: 15572
Epoch: [4]  [2650/2809]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.5396 (4.5747)  class_acc: 0.1250 (0.1018)  loss_scale: 65536.0000 (64015.6439)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1387  max mem: 15572
Epoch: [4]  [2660/2809]  eta: 0:01:27  lr: 0.000046  min_lr: 0.000000  loss: 4.6022 (4.5747)  class_acc: 0.0833 (0.1018)  loss_scale: 65536.0000 (64021.3574)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1255  max mem: 15572
Epoch: [4]  [2670/2809]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.5961 (4.5747)  class_acc: 0.0833 (0.1019)  loss_scale: 65536.0000 (64027.0281)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.1895  max mem: 15572
[2025-01-15 16:45:15,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:45:15,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [4]  [2680/2809]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000000  loss: 4.6989 (4.5753)  class_acc: 0.0833 (0.1018)  loss_scale: 65536.0000 (64179.3241)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.2131  max mem: 15572
[2025-01-15 16:45:22,886] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13922
[2025-01-15 16:45:22,887] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:45:22,887] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [4]  [2690/2809]  eta: 0:01:10  lr: 0.000046  min_lr: 0.000000  loss: 4.5299 (4.5745)  class_acc: 0.1250 (0.1020)  loss_scale: 131072.0000 (64306.1345)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1807  max mem: 15572
Epoch: [4]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4510 (4.5742)  class_acc: 0.1667 (0.1021)  loss_scale: 65536.0000 (64310.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1108  max mem: 15572
Epoch: [4]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4841 (4.5740)  class_acc: 0.1250 (0.1021)  loss_scale: 65536.0000 (64315.2077)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1246  max mem: 15572
Epoch: [4]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4703 (4.5735)  class_acc: 0.0833 (0.1022)  loss_scale: 65536.0000 (64319.6942)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.1943  max mem: 15572
Epoch: [4]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4433 (4.5735)  class_acc: 0.0833 (0.1022)  loss_scale: 65536.0000 (64324.1479)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1975  max mem: 15572
Epoch: [4]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4433 (4.5728)  class_acc: 0.1250 (0.1022)  loss_scale: 65536.0000 (64328.5691)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1900  max mem: 15572
Epoch: [4]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4980 (4.5728)  class_acc: 0.0833 (0.1022)  loss_scale: 65536.0000 (64332.9582)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1438  max mem: 15572
Epoch: [4]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5805 (4.5729)  class_acc: 0.0833 (0.1022)  loss_scale: 65536.0000 (64337.3155)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1016  max mem: 15572
[2025-01-15 16:46:07,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=84, lr=[4.527119866649877e-07, 4.527119866649877e-07, 6.467314095214111e-07, 6.467314095214111e-07, 9.239020136020159e-07, 9.239020136020159e-07, 1.3198600194314515e-06, 1.3198600194314515e-06, 1.8855143134735021e-06, 1.8855143134735021e-06, 2.6935918763907173e-06, 2.6935918763907173e-06, 3.847988394843882e-06, 3.847988394843882e-06, 5.497126278348404e-06, 5.497126278348404e-06, 7.85303754049772e-06, 7.85303754049772e-06, 1.1218625057853886e-05, 1.1218625057853886e-05, 1.6026607225505554e-05, 1.6026607225505554e-05, 2.2895153179293648e-05, 2.2895153179293648e-05, 3.270736168470521e-05, 3.270736168470521e-05, 4.672480240672174e-05, 4.672480240672174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:46:07,429] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=28.44022098408243, CurrSamplesPerSec=31.435825049403782, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [4]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4707 (4.5728)  class_acc: 0.1250 (0.1023)  loss_scale: 65536.0000 (64341.6413)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1733  max mem: 15572
Epoch: [4]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4777 (4.5728)  class_acc: 0.0833 (0.1024)  loss_scale: 65536.0000 (64345.9360)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.2208  max mem: 15572
Epoch: [4]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5180 (4.5727)  class_acc: 0.0833 (0.1024)  loss_scale: 65536.0000 (64350.1999)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.2186  max mem: 15572
Epoch: [4]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5262 (4.5727)  class_acc: 0.0833 (0.1025)  loss_scale: 65536.0000 (64354.4334)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1551  max mem: 15572
Epoch: [4]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4892 (4.5723)  class_acc: 0.0833 (0.1025)  loss_scale: 65536.0000 (64357.7985)  weight_decay: 0.0500 (0.0500)  time: 0.4360  data: 0.0347  max mem: 15572
Epoch: [4] Total time: 0:27:37 (0.5899 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.4892 (4.5723)  class_acc: 0.0833 (0.1025)  loss_scale: 65536.0000 (64357.7985)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:19:26  loss: 1.1021 (1.1021)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.2899  data: 4.0584  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 4.3630 (3.9953)  acc1: 0.0000 (20.2020)  acc5: 5.5556 (27.7778)  time: 0.7683  data: 0.5772  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 4.2294 (4.0508)  acc1: 0.0000 (15.0794)  acc5: 22.2222 (28.5714)  time: 0.3597  data: 0.1589  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 4.2042 (4.1103)  acc1: 0.0000 (10.3943)  acc5: 27.7778 (30.1075)  time: 0.2612  data: 0.0633  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 3.9859 (4.0088)  acc1: 0.0000 (12.1951)  acc5: 38.8889 (34.8238)  time: 0.2757  data: 0.0793  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 3.6534 (3.9780)  acc1: 11.1111 (12.9630)  acc5: 55.5556 (37.0370)  time: 0.3319  data: 0.1203  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 3.0781 (3.8369)  acc1: 11.1111 (17.3042)  acc5: 72.2222 (41.9854)  time: 0.3296  data: 0.1243  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 3.0458 (3.7660)  acc1: 27.7778 (18.5446)  acc5: 66.6667 (43.5055)  time: 0.3377  data: 0.1291  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.6258 (3.7830)  acc1: 11.1111 (18.1756)  acc5: 50.0000 (42.6612)  time: 0.3478  data: 0.1356  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 4.3620 (3.8520)  acc1: 0.0000 (16.2393)  acc5: 5.5556 (39.0720)  time: 0.3247  data: 0.1185  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 4.3620 (3.9093)  acc1: 0.0000 (15.7316)  acc5: 5.5556 (37.1837)  time: 0.3215  data: 0.1241  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 4.4089 (3.9608)  acc1: 0.0000 (14.4144)  acc5: 11.1111 (35.4855)  time: 0.3739  data: 0.1772  max mem: 15572
Val:  [120/272]  eta: 0:00:56  loss: 4.3997 (3.9972)  acc1: 0.0000 (13.4068)  acc5: 11.1111 (34.4353)  time: 0.3917  data: 0.1966  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 4.3121 (3.9251)  acc1: 0.0000 (14.9279)  acc5: 27.7778 (36.4292)  time: 0.3303  data: 0.1420  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 3.8294 (3.9133)  acc1: 11.1111 (15.7210)  acc5: 44.4444 (36.6036)  time: 0.2280  data: 0.0462  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 4.0202 (3.9252)  acc1: 0.0000 (14.7535)  acc5: 27.7778 (36.0927)  time: 0.1782  data: 0.0005  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 3.8941 (3.9135)  acc1: 5.5556 (15.6660)  acc5: 38.8889 (37.6121)  time: 0.1873  data: 0.0006  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 4.0431 (3.9479)  acc1: 0.0000 (14.9123)  acc5: 33.3333 (36.4198)  time: 0.2624  data: 0.0620  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 4.2243 (3.9514)  acc1: 0.0000 (15.1934)  acc5: 11.1111 (36.2492)  time: 0.3572  data: 0.1502  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 4.2872 (3.9764)  acc1: 0.0000 (14.3979)  acc5: 11.1111 (35.3403)  time: 0.3454  data: 0.1441  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 4.2332 (3.9834)  acc1: 0.0000 (14.4002)  acc5: 27.7778 (35.9867)  time: 0.3446  data: 0.1457  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.8850 (3.9946)  acc1: 0.0000 (14.5076)  acc5: 44.4444 (35.9136)  time: 0.3717  data: 0.1699  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 4.1166 (4.0024)  acc1: 0.0000 (14.2534)  acc5: 27.7778 (35.7466)  time: 0.3644  data: 0.1595  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.9521 (3.9811)  acc1: 11.1111 (15.7528)  acc5: 50.0000 (37.2054)  time: 0.3720  data: 0.1702  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.5806 (3.9689)  acc1: 11.1111 (15.7215)  acc5: 77.7778 (38.4740)  time: 0.3830  data: 0.1681  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.9299 (3.9977)  acc1: 0.0000 (15.2722)  acc5: 33.3333 (37.5609)  time: 0.3519  data: 0.1349  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.8133 (3.9341)  acc1: 27.7778 (17.4117)  acc5: 66.6667 (39.5275)  time: 0.3283  data: 0.1101  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 3.3758 (3.9324)  acc1: 33.3333 (17.3432)  acc5: 77.7778 (39.7294)  time: 0.2928  data: 0.0817  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 3.3758 (3.9348)  acc1: 33.3333 (17.3254)  acc5: 77.7778 (39.7092)  time: 0.2776  data: 0.0816  max mem: 15572
Val: Total time: 0:01:31 (0.3369 s / it)
* Acc@1 17.325 Acc@5 39.709 loss 3.935
Accuracy of the network on the 4883 val videos: 17.3%
[2025-01-15 16:48:05,795] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 16:48:05,799] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 16:48:05,799] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 16:48:09,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 16:48:09,109] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 17.33%
Epoch: [5]  [   0/2809]  eta: 8:19:12  lr: 0.000047  min_lr: 0.000000  loss: 4.6069 (4.6069)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.6630  data: 10.1164  max mem: 15572
[2025-01-15 16:48:22,791] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:48:22,792] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [  10/2809]  eta: 1:12:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4322 (4.5252)  class_acc: 0.1250 (0.1174)  loss_scale: 65536.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 1.5453  data: 1.0744  max mem: 15572
[2025-01-15 16:48:27,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14058
[2025-01-15 16:48:27,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:48:27,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [  20/2809]  eta: 0:53:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4078 (4.4868)  class_acc: 0.1250 (0.1270)  loss_scale: 65536.0000 (87381.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.2019  max mem: 15572
Epoch: [5]  [  30/2809]  eta: 0:46:46  lr: 0.000047  min_lr: 0.000000  loss: 4.4372 (4.4884)  class_acc: 0.0833 (0.1237)  loss_scale: 65536.0000 (80334.4516)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.2582  max mem: 15572
Epoch: [5]  [  40/2809]  eta: 0:43:00  lr: 0.000047  min_lr: 0.000000  loss: 4.5583 (4.5051)  class_acc: 0.0833 (0.1159)  loss_scale: 65536.0000 (76725.0732)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.2443  max mem: 15572
Epoch: [5]  [  50/2809]  eta: 0:39:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4891 (4.4817)  class_acc: 0.0833 (0.1201)  loss_scale: 65536.0000 (74531.1373)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1529  max mem: 15572
Epoch: [5]  [  60/2809]  eta: 0:35:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3225 (4.4749)  class_acc: 0.1250 (0.1230)  loss_scale: 65536.0000 (73056.5246)  weight_decay: 0.0500 (0.0500)  time: 0.4622  data: 0.0502  max mem: 15572
Epoch: [5]  [  70/2809]  eta: 0:33:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4213 (4.4884)  class_acc: 0.1250 (0.1215)  loss_scale: 65536.0000 (71997.2958)  weight_decay: 0.0500 (0.0500)  time: 0.4291  data: 0.0006  max mem: 15572
Epoch: [5]  [  80/2809]  eta: 0:31:57  lr: 0.000047  min_lr: 0.000000  loss: 4.5792 (4.4962)  class_acc: 0.0833 (0.1214)  loss_scale: 65536.0000 (71199.6049)  weight_decay: 0.0500 (0.0500)  time: 0.4723  data: 0.0009  max mem: 15572
Epoch: [5]  [  90/2809]  eta: 0:30:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4211 (4.4909)  class_acc: 0.1250 (0.1232)  loss_scale: 65536.0000 (70577.2308)  weight_decay: 0.0500 (0.0500)  time: 0.4731  data: 0.0008  max mem: 15572
Epoch: [5]  [ 100/2809]  eta: 0:29:36  lr: 0.000047  min_lr: 0.000000  loss: 4.6041 (4.5139)  class_acc: 0.0833 (0.1172)  loss_scale: 65536.0000 (70078.0990)  weight_decay: 0.0500 (0.0500)  time: 0.4665  data: 0.0007  max mem: 15572
Epoch: [5]  [ 110/2809]  eta: 0:28:56  lr: 0.000047  min_lr: 0.000000  loss: 4.6077 (4.5142)  class_acc: 0.0833 (0.1197)  loss_scale: 65536.0000 (69668.9009)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0528  max mem: 15572
Epoch: [5]  [ 120/2809]  eta: 0:28:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4838 (4.5080)  class_acc: 0.1667 (0.1233)  loss_scale: 65536.0000 (69327.3388)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1568  max mem: 15572
Epoch: [5]  [ 130/2809]  eta: 0:28:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3840 (4.5034)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (69037.9237)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1878  max mem: 15572
Epoch: [5]  [ 140/2809]  eta: 0:28:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5382 (4.5103)  class_acc: 0.1250 (0.1215)  loss_scale: 65536.0000 (68789.5603)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1570  max mem: 15572
[2025-01-15 16:49:41,370] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:49:41,371] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:49:42,184] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14189
[2025-01-15 16:49:42,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:49:42,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 150/2809]  eta: 0:28:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5830 (4.5171)  class_acc: 0.0833 (0.1187)  loss_scale: 65536.0000 (69442.1192)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1564  max mem: 15572
Epoch: [5]  [ 160/2809]  eta: 0:28:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5213 (4.5074)  class_acc: 0.0833 (0.1211)  loss_scale: 65536.0000 (69199.5031)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1966  max mem: 15572
Epoch: [5]  [ 170/2809]  eta: 0:27:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4523 (4.5079)  class_acc: 0.0833 (0.1196)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1951  max mem: 15572
Epoch: [5]  [ 180/2809]  eta: 0:27:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4523 (4.4996)  class_acc: 0.0833 (0.1199)  loss_scale: 65536.0000 (68794.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.2090  max mem: 15572
Epoch: [5]  [ 190/2809]  eta: 0:27:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4866 (4.5020)  class_acc: 0.0833 (0.1182)  loss_scale: 65536.0000 (68624.0838)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.2146  max mem: 15572
Epoch: [5]  [ 200/2809]  eta: 0:27:26  lr: 0.000047  min_lr: 0.000000  loss: 4.5565 (4.5095)  class_acc: 0.0833 (0.1165)  loss_scale: 65536.0000 (68470.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1443  max mem: 15572
Epoch: [5]  [ 210/2809]  eta: 0:27:20  lr: 0.000047  min_lr: 0.000000  loss: 4.5260 (4.5000)  class_acc: 0.0833 (0.1199)  loss_scale: 65536.0000 (68331.3744)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1737  max mem: 15572
Epoch: [5]  [ 220/2809]  eta: 0:27:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3007 (4.5002)  class_acc: 0.1667 (0.1209)  loss_scale: 65536.0000 (68204.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1500  max mem: 15572
Epoch: [5]  [ 230/2809]  eta: 0:26:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4995 (4.5064)  class_acc: 0.0833 (0.1194)  loss_scale: 65536.0000 (68089.3506)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0875  max mem: 15572
Epoch: [5]  [ 240/2809]  eta: 0:26:35  lr: 0.000047  min_lr: 0.000000  loss: 4.5106 (4.5094)  class_acc: 0.0833 (0.1188)  loss_scale: 65536.0000 (67983.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1404  max mem: 15572
Epoch: [5]  [ 250/2809]  eta: 0:26:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5106 (4.5116)  class_acc: 0.1250 (0.1192)  loss_scale: 65536.0000 (67885.8964)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.2217  max mem: 15572
Epoch: [5]  [ 260/2809]  eta: 0:26:29  lr: 0.000047  min_lr: 0.000000  loss: 4.5081 (4.5085)  class_acc: 0.1250 (0.1194)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.2170  max mem: 15572
Epoch: [5]  [ 270/2809]  eta: 0:26:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5081 (4.5093)  class_acc: 0.0833 (0.1184)  loss_scale: 65536.0000 (67712.4723)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1745  max mem: 15572
[2025-01-15 16:51:01,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:51:01,183] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:51:02,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14322
[2025-01-15 16:51:02,888] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:51:02,888] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 280/2809]  eta: 0:26:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5406 (4.5095)  class_acc: 0.0833 (0.1182)  loss_scale: 65536.0000 (68567.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1760  max mem: 15572
Epoch: [5]  [ 290/2809]  eta: 0:26:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4822 (4.5098)  class_acc: 0.0417 (0.1166)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1488  max mem: 15572
Epoch: [5]  [ 300/2809]  eta: 0:26:04  lr: 0.000047  min_lr: 0.000000  loss: 4.5038 (4.5095)  class_acc: 0.0833 (0.1171)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1838  max mem: 15572
Epoch: [5]  [ 310/2809]  eta: 0:26:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5448 (4.5093)  class_acc: 0.1250 (0.1184)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.2345  max mem: 15572
Epoch: [5]  [ 320/2809]  eta: 0:25:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5192 (4.5091)  class_acc: 0.1250 (0.1185)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1557  max mem: 15572
Epoch: [5]  [ 330/2809]  eta: 0:25:40  lr: 0.000047  min_lr: 0.000000  loss: 4.6048 (4.5121)  class_acc: 0.0833 (0.1177)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0894  max mem: 15572
Epoch: [5]  [ 340/2809]  eta: 0:25:30  lr: 0.000047  min_lr: 0.000000  loss: 4.6017 (4.5134)  class_acc: 0.0833 (0.1173)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0990  max mem: 15572
Epoch: [5]  [ 350/2809]  eta: 0:25:17  lr: 0.000047  min_lr: 0.000000  loss: 4.5063 (4.5129)  class_acc: 0.1250 (0.1179)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0766  max mem: 15572
Epoch: [5]  [ 360/2809]  eta: 0:25:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4993 (4.5134)  class_acc: 0.1250 (0.1185)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0738  max mem: 15572
Epoch: [5]  [ 370/2809]  eta: 0:24:54  lr: 0.000047  min_lr: 0.000000  loss: 4.5024 (4.5104)  class_acc: 0.1250 (0.1192)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0669  max mem: 15572
Epoch: [5]  [ 380/2809]  eta: 0:24:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4701 (4.5114)  class_acc: 0.1250 (0.1188)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0785  max mem: 15572
Epoch: [5]  [ 390/2809]  eta: 0:24:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5944 (4.5147)  class_acc: 0.0833 (0.1182)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1153  max mem: 15572
Epoch: [5]  [ 400/2809]  eta: 0:24:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5740 (4.5117)  class_acc: 0.1250 (0.1191)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1085  max mem: 15572
[2025-01-15 16:52:17,879] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:52:17,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:52:18,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14452
[2025-01-15 16:52:18,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:52:18,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 410/2809]  eta: 0:24:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5104 (4.5141)  class_acc: 0.1250 (0.1192)  loss_scale: 65536.0000 (67768.3698)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1352  max mem: 15572
Epoch: [5]  [ 420/2809]  eta: 0:24:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6595 (4.5164)  class_acc: 0.0417 (0.1187)  loss_scale: 65536.0000 (67715.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1397  max mem: 15572
Epoch: [5]  [ 430/2809]  eta: 0:24:11  lr: 0.000047  min_lr: 0.000000  loss: 4.6120 (4.5193)  class_acc: 0.0417 (0.1182)  loss_scale: 65536.0000 (67664.7796)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1910  max mem: 15572
Epoch: [5]  [ 440/2809]  eta: 0:24:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4937 (4.5171)  class_acc: 0.0833 (0.1183)  loss_scale: 65536.0000 (67616.5079)  weight_decay: 0.0500 (0.0500)  time: 0.7079  data: 0.2694  max mem: 15572
Epoch: [5]  [ 450/2809]  eta: 0:23:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4447 (4.5166)  class_acc: 0.1667 (0.1194)  loss_scale: 65536.0000 (67570.3769)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1346  max mem: 15572
Epoch: [5]  [ 460/2809]  eta: 0:23:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4788 (4.5161)  class_acc: 0.1667 (0.1192)  loss_scale: 65536.0000 (67526.2473)  weight_decay: 0.0500 (0.0500)  time: 0.4867  data: 0.0365  max mem: 15572
Epoch: [5]  [ 470/2809]  eta: 0:23:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5431 (4.5151)  class_acc: 0.0833 (0.1193)  loss_scale: 65536.0000 (67483.9915)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1084  max mem: 15572
Epoch: [5]  [ 480/2809]  eta: 0:23:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4895 (4.5142)  class_acc: 0.0833 (0.1195)  loss_scale: 65536.0000 (67443.4927)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1584  max mem: 15572
Epoch: [5]  [ 490/2809]  eta: 0:23:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4774 (4.5113)  class_acc: 0.1250 (0.1206)  loss_scale: 65536.0000 (67404.6436)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1774  max mem: 15572
Epoch: [5]  [ 500/2809]  eta: 0:23:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4619 (4.5095)  class_acc: 0.1250 (0.1213)  loss_scale: 65536.0000 (67367.3453)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1492  max mem: 15572
Epoch: [5]  [ 510/2809]  eta: 0:23:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4862 (4.5106)  class_acc: 0.1250 (0.1212)  loss_scale: 65536.0000 (67331.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1395  max mem: 15572
Epoch: [5]  [ 520/2809]  eta: 0:23:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3637 (4.5078)  class_acc: 0.1250 (0.1215)  loss_scale: 65536.0000 (67297.0441)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1405  max mem: 15572
Epoch: [5]  [ 530/2809]  eta: 0:23:03  lr: 0.000047  min_lr: 0.000000  loss: 4.5199 (4.5093)  class_acc: 0.1250 (0.1207)  loss_scale: 65536.0000 (67263.8795)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1658  max mem: 15572
[2025-01-15 16:53:34,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:53:34,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 540/2809]  eta: 0:22:53  lr: 0.000047  min_lr: 0.000000  loss: 4.5199 (4.5069)  class_acc: 0.0833 (0.1211)  loss_scale: 65536.0000 (67837.6340)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1437  max mem: 15572
[2025-01-15 16:53:42,623] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14592
[2025-01-15 16:53:42,623] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:53:42,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 550/2809]  eta: 0:22:52  lr: 0.000047  min_lr: 0.000000  loss: 4.6497 (4.5119)  class_acc: 0.0833 (0.1199)  loss_scale: 131072.0000 (68509.5027)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1657  max mem: 15572
Epoch: [5]  [ 560/2809]  eta: 0:22:42  lr: 0.000047  min_lr: 0.000000  loss: 4.7785 (4.5160)  class_acc: 0.0417 (0.1194)  loss_scale: 65536.0000 (68456.4991)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1632  max mem: 15572
Epoch: [5]  [ 570/2809]  eta: 0:22:39  lr: 0.000047  min_lr: 0.000000  loss: 4.6350 (4.5164)  class_acc: 0.0833 (0.1193)  loss_scale: 65536.0000 (68405.3520)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1385  max mem: 15572
Epoch: [5]  [ 580/2809]  eta: 0:22:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5215 (4.5161)  class_acc: 0.1250 (0.1198)  loss_scale: 65536.0000 (68355.9656)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1363  max mem: 15572
Epoch: [5]  [ 590/2809]  eta: 0:22:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4894 (4.5162)  class_acc: 0.0833 (0.1194)  loss_scale: 65536.0000 (68308.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1367  max mem: 15572
Epoch: [5]  [ 600/2809]  eta: 0:22:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4730 (4.5149)  class_acc: 0.0833 (0.1196)  loss_scale: 65536.0000 (68262.1231)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1860  max mem: 15572
Epoch: [5]  [ 610/2809]  eta: 0:22:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4105 (4.5128)  class_acc: 0.1250 (0.1202)  loss_scale: 65536.0000 (68217.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1164  max mem: 15572
Epoch: [5]  [ 620/2809]  eta: 0:22:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4485 (4.5134)  class_acc: 0.1667 (0.1204)  loss_scale: 65536.0000 (68174.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1205  max mem: 15572
Epoch: [5]  [ 630/2809]  eta: 0:22:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4139 (4.5100)  class_acc: 0.1250 (0.1208)  loss_scale: 65536.0000 (68132.5135)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1989  max mem: 15572
Epoch: [5]  [ 640/2809]  eta: 0:21:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3135 (4.5081)  class_acc: 0.1250 (0.1206)  loss_scale: 65536.0000 (68092.0062)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2072  max mem: 15572
Epoch: [5]  [ 650/2809]  eta: 0:21:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3135 (4.5066)  class_acc: 0.0833 (0.1205)  loss_scale: 65536.0000 (68052.7435)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1545  max mem: 15572
Epoch: [5]  [ 660/2809]  eta: 0:21:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4644 (4.5061)  class_acc: 0.0833 (0.1207)  loss_scale: 65536.0000 (68014.6687)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1523  max mem: 15572
Epoch: [5]  [ 670/2809]  eta: 0:21:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4644 (4.5056)  class_acc: 0.1250 (0.1209)  loss_scale: 65536.0000 (67977.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1399  max mem: 15572
[2025-01-15 16:54:59,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:54:59,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 16:55:00,410] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14722
[2025-01-15 16:55:00,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:55:00,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 680/2809]  eta: 0:21:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4452 (4.5040)  class_acc: 0.1250 (0.1211)  loss_scale: 65536.0000 (68038.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1149  max mem: 15572
Epoch: [5]  [ 690/2809]  eta: 0:21:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4666 (4.5037)  class_acc: 0.1250 (0.1217)  loss_scale: 65536.0000 (68001.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1437  max mem: 15572
Epoch: [5]  [ 700/2809]  eta: 0:21:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4666 (4.5036)  class_acc: 0.1667 (0.1221)  loss_scale: 65536.0000 (67966.7218)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1665  max mem: 15572
Epoch: [5]  [ 710/2809]  eta: 0:21:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4877 (4.5036)  class_acc: 0.1250 (0.1222)  loss_scale: 65536.0000 (67932.5345)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1903  max mem: 15572
Epoch: [5]  [ 720/2809]  eta: 0:21:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4764 (4.5030)  class_acc: 0.0833 (0.1220)  loss_scale: 65536.0000 (67899.2954)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1603  max mem: 15572
Epoch: [5]  [ 730/2809]  eta: 0:20:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4892 (4.5031)  class_acc: 0.0833 (0.1219)  loss_scale: 65536.0000 (67866.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1184  max mem: 15572
Epoch: [5]  [ 740/2809]  eta: 0:20:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3882 (4.5011)  class_acc: 0.1667 (0.1230)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0889  max mem: 15572
Epoch: [5]  [ 750/2809]  eta: 0:20:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3882 (4.5015)  class_acc: 0.1667 (0.1231)  loss_scale: 65536.0000 (67804.8895)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1111  max mem: 15572
Epoch: [5]  [ 760/2809]  eta: 0:20:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5203 (4.5023)  class_acc: 0.0833 (0.1230)  loss_scale: 65536.0000 (67775.0749)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1407  max mem: 15572
Epoch: [5]  [ 770/2809]  eta: 0:20:28  lr: 0.000047  min_lr: 0.000000  loss: 4.5106 (4.5008)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (67746.0337)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0845  max mem: 15572
Epoch: [5]  [ 780/2809]  eta: 0:20:24  lr: 0.000047  min_lr: 0.000000  loss: 4.2709 (4.4982)  class_acc: 0.1667 (0.1233)  loss_scale: 65536.0000 (67717.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1485  max mem: 15572
Epoch: [5]  [ 790/2809]  eta: 0:20:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5385 (4.4994)  class_acc: 0.1667 (0.1232)  loss_scale: 65536.0000 (67690.1542)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1484  max mem: 15572
Epoch: [5]  [ 800/2809]  eta: 0:20:11  lr: 0.000047  min_lr: 0.000000  loss: 4.5391 (4.4997)  class_acc: 0.1250 (0.1232)  loss_scale: 65536.0000 (67663.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1209  max mem: 15572
[2025-01-15 16:56:16,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:56:16,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 810/2809]  eta: 0:20:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4733 (4.4985)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (68041.0752)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1293  max mem: 15572
Epoch: [5]  [ 820/2809]  eta: 0:19:58  lr: 0.000047  min_lr: 0.000000  loss: 4.5181 (4.5000)  class_acc: 0.1250 (0.1234)  loss_scale: 131072.0000 (68808.8088)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1093  max mem: 15572
[2025-01-15 16:56:30,016] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14874
[2025-01-15 16:56:30,017] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:56:30,018] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 830/2809]  eta: 0:19:53  lr: 0.000047  min_lr: 0.000000  loss: 4.4597 (4.5002)  class_acc: 0.1250 (0.1234)  loss_scale: 131072.0000 (69400.3369)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1397  max mem: 15572
Epoch: [5]  [ 840/2809]  eta: 0:19:48  lr: 0.000047  min_lr: 0.000000  loss: 4.4455 (4.5000)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (69354.3876)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1633  max mem: 15572
Epoch: [5]  [ 850/2809]  eta: 0:19:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4306 (4.4995)  class_acc: 0.0833 (0.1230)  loss_scale: 65536.0000 (69309.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1552  max mem: 15572
Epoch: [5]  [ 860/2809]  eta: 0:19:35  lr: 0.000047  min_lr: 0.000000  loss: 4.4783 (4.4993)  class_acc: 0.1250 (0.1230)  loss_scale: 65536.0000 (69265.6911)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1732  max mem: 15572
Epoch: [5]  [ 870/2809]  eta: 0:19:32  lr: 0.000047  min_lr: 0.000000  loss: 4.6345 (4.5013)  class_acc: 0.0833 (0.1225)  loss_scale: 65536.0000 (69222.8703)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.2681  max mem: 15572
Epoch: [5]  [ 880/2809]  eta: 0:19:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5156 (4.4994)  class_acc: 0.1250 (0.1229)  loss_scale: 65536.0000 (69181.0216)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1505  max mem: 15572
Epoch: [5]  [ 890/2809]  eta: 0:19:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3830 (4.4988)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (69140.1122)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0932  max mem: 15572
Epoch: [5]  [ 900/2809]  eta: 0:19:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4951 (4.4985)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (69100.1110)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.2034  max mem: 15572
Epoch: [5]  [ 910/2809]  eta: 0:19:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4939 (4.4987)  class_acc: 0.1250 (0.1229)  loss_scale: 65536.0000 (69060.9879)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1526  max mem: 15572
Epoch: [5]  [ 920/2809]  eta: 0:19:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4939 (4.4992)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (69022.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1285  max mem: 15572
Epoch: [5]  [ 930/2809]  eta: 0:18:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4558 (4.4980)  class_acc: 0.0833 (0.1231)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1551  max mem: 15572
Epoch: [5]  [ 940/2809]  eta: 0:18:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4558 (4.4983)  class_acc: 0.0833 (0.1229)  loss_scale: 65536.0000 (68948.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0914  max mem: 15572
Epoch: [5]  [ 950/2809]  eta: 0:18:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5189 (4.4991)  class_acc: 0.0833 (0.1226)  loss_scale: 65536.0000 (68912.7234)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1105  max mem: 15572
[2025-01-15 16:57:45,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=91, lr=[4.540618353164525e-07, 4.540618353164525e-07, 6.486597647377894e-07, 6.486597647377894e-07, 9.266568067682706e-07, 9.266568067682706e-07, 1.3237954382403866e-06, 1.3237954382403866e-06, 1.8911363403434095e-06, 1.8911363403434095e-06, 2.701623343347728e-06, 2.701623343347728e-06, 3.859461919068183e-06, 3.859461919068183e-06, 5.5135170272402624e-06, 5.5135170272402624e-06, 7.876452896057517e-06, 7.876452896057517e-06, 1.1252075565796455e-05, 1.1252075565796455e-05, 1.6074393665423507e-05, 1.6074393665423507e-05, 2.2963419522033585e-05, 2.2963419522033585e-05, 3.280488503147655e-05, 3.280488503147655e-05, 4.686412147353793e-05, 4.686412147353793e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 16:57:45,146] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=28.437292330489406, CurrSamplesPerSec=30.898746532983164, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-15 16:57:47,101] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:57:47,101] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [ 960/2809]  eta: 0:18:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5099 (4.4991)  class_acc: 0.0833 (0.1224)  loss_scale: 65536.0000 (69082.1727)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1328  max mem: 15572
Epoch: [5]  [ 970/2809]  eta: 0:18:27  lr: 0.000047  min_lr: 0.000000  loss: 4.3865 (4.4977)  class_acc: 0.1250 (0.1227)  loss_scale: 131072.0000 (69720.5850)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1517  max mem: 15572
Epoch: [5]  [ 980/2809]  eta: 0:18:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4486 (4.4986)  class_acc: 0.1250 (0.1225)  loss_scale: 131072.0000 (70345.9817)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1760  max mem: 15572
[2025-01-15 16:58:03,432] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15029
[2025-01-15 16:58:03,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 16:58:03,433] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [ 990/2809]  eta: 0:18:15  lr: 0.000047  min_lr: 0.000000  loss: 4.5072 (4.4981)  class_acc: 0.1250 (0.1229)  loss_scale: 131072.0000 (70495.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1200  max mem: 15572
Epoch: [5]  [1000/2809]  eta: 0:18:09  lr: 0.000047  min_lr: 0.000000  loss: 4.4251 (4.4976)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (70446.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1296  max mem: 15572
Epoch: [5]  [1010/2809]  eta: 0:18:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3501 (4.4970)  class_acc: 0.1250 (0.1231)  loss_scale: 65536.0000 (70397.7211)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1707  max mem: 15572
Epoch: [5]  [1020/2809]  eta: 0:17:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3607 (4.4955)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (70350.1038)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1733  max mem: 15572
Epoch: [5]  [1030/2809]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3607 (4.4953)  class_acc: 0.1250 (0.1233)  loss_scale: 65536.0000 (70303.4103)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1011  max mem: 15572
[2025-01-15 16:58:33,716] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15080
[2025-01-15 16:58:33,717] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 16:58:33,717] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1040/2809]  eta: 0:17:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3407 (4.4941)  class_acc: 0.1250 (0.1237)  loss_scale: 65536.0000 (70068.7493)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1142  max mem: 15572
Epoch: [5]  [1050/2809]  eta: 0:17:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5630 (4.4948)  class_acc: 0.1250 (0.1237)  loss_scale: 32768.0000 (69713.8421)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1651  max mem: 15572
Epoch: [5]  [1060/2809]  eta: 0:17:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4833 (4.4935)  class_acc: 0.1250 (0.1237)  loss_scale: 32768.0000 (69365.6249)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0995  max mem: 15572
Epoch: [5]  [1070/2809]  eta: 0:17:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3636 (4.4928)  class_acc: 0.1250 (0.1237)  loss_scale: 32768.0000 (69023.9104)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1292  max mem: 15572
Epoch: [5]  [1080/2809]  eta: 0:17:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5423 (4.4937)  class_acc: 0.0833 (0.1235)  loss_scale: 32768.0000 (68688.5180)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1873  max mem: 15572
Epoch: [5]  [1090/2809]  eta: 0:17:10  lr: 0.000047  min_lr: 0.000000  loss: 4.5816 (4.4942)  class_acc: 0.0417 (0.1230)  loss_scale: 32768.0000 (68359.2741)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.1047  max mem: 15572
Epoch: [5]  [1100/2809]  eta: 0:17:05  lr: 0.000047  min_lr: 0.000000  loss: 4.5426 (4.4941)  class_acc: 0.0417 (0.1231)  loss_scale: 32768.0000 (68036.0109)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1201  max mem: 15572
Epoch: [5]  [1110/2809]  eta: 0:17:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4827 (4.4946)  class_acc: 0.0833 (0.1230)  loss_scale: 32768.0000 (67718.5671)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.2282  max mem: 15572
Epoch: [5]  [1120/2809]  eta: 0:16:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5440 (4.4947)  class_acc: 0.0833 (0.1230)  loss_scale: 32768.0000 (67406.7868)  weight_decay: 0.0500 (0.0500)  time: 0.7059  data: 0.2519  max mem: 15572
Epoch: [5]  [1130/2809]  eta: 0:16:51  lr: 0.000047  min_lr: 0.000000  loss: 4.5160 (4.4935)  class_acc: 0.0833 (0.1234)  loss_scale: 32768.0000 (67100.5199)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.2493  max mem: 15572
Epoch: [5]  [1140/2809]  eta: 0:16:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4807 (4.4944)  class_acc: 0.0833 (0.1229)  loss_scale: 32768.0000 (66799.6214)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1863  max mem: 15572
Epoch: [5]  [1150/2809]  eta: 0:16:38  lr: 0.000047  min_lr: 0.000000  loss: 4.5117 (4.4932)  class_acc: 0.0833 (0.1233)  loss_scale: 32768.0000 (66503.9513)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0934  max mem: 15572
Epoch: [5]  [1160/2809]  eta: 0:16:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4845 (4.4926)  class_acc: 0.1250 (0.1235)  loss_scale: 32768.0000 (66213.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0933  max mem: 15572
[2025-01-15 16:59:51,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 16:59:51,357] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1170/2809]  eta: 0:16:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4650 (4.4924)  class_acc: 0.1250 (0.1233)  loss_scale: 32768.0000 (66123.6413)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1260  max mem: 15572
Epoch: [5]  [1180/2809]  eta: 0:16:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4729 (4.4920)  class_acc: 0.1250 (0.1234)  loss_scale: 65536.0000 (66118.6655)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1171  max mem: 15572
Epoch: [5]  [1190/2809]  eta: 0:16:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4005 (4.4908)  class_acc: 0.1250 (0.1236)  loss_scale: 65536.0000 (66113.7733)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1083  max mem: 15572
Epoch: [5]  [1200/2809]  eta: 0:16:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4161 (4.4915)  class_acc: 0.1250 (0.1238)  loss_scale: 65536.0000 (66108.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1060  max mem: 15572
Epoch: [5]  [1210/2809]  eta: 0:15:58  lr: 0.000047  min_lr: 0.000000  loss: 4.6226 (4.4913)  class_acc: 0.1250 (0.1239)  loss_scale: 65536.0000 (66104.2312)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0727  max mem: 15572
Epoch: [5]  [1220/2809]  eta: 0:15:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5968 (4.4912)  class_acc: 0.1250 (0.1238)  loss_scale: 65536.0000 (66099.5774)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0960  max mem: 15572
Epoch: [5]  [1230/2809]  eta: 0:15:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5452 (4.4912)  class_acc: 0.1250 (0.1238)  loss_scale: 65536.0000 (66094.9992)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1488  max mem: 15572
Epoch: [5]  [1240/2809]  eta: 0:15:40  lr: 0.000047  min_lr: 0.000000  loss: 4.5452 (4.4902)  class_acc: 0.1667 (0.1241)  loss_scale: 65536.0000 (66090.4948)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1705  max mem: 15572
Epoch: [5]  [1250/2809]  eta: 0:15:34  lr: 0.000047  min_lr: 0.000000  loss: 4.5401 (4.4910)  class_acc: 0.1250 (0.1242)  loss_scale: 65536.0000 (66086.0624)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1581  max mem: 15572
Epoch: [5]  [1260/2809]  eta: 0:15:27  lr: 0.000047  min_lr: 0.000000  loss: 4.5123 (4.4909)  class_acc: 0.0833 (0.1240)  loss_scale: 65536.0000 (66081.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0729  max mem: 15572
Epoch: [5]  [1270/2809]  eta: 0:15:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4938 (4.4911)  class_acc: 0.1250 (0.1244)  loss_scale: 65536.0000 (66077.4068)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0714  max mem: 15572
Epoch: [5]  [1280/2809]  eta: 0:15:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4052 (4.4891)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (66073.1803)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1672  max mem: 15572
Epoch: [5]  [1290/2809]  eta: 0:15:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3473 (4.4886)  class_acc: 0.1250 (0.1249)  loss_scale: 65536.0000 (66069.0194)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.1919  max mem: 15572
[2025-01-15 17:01:05,395] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:01:05,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:01:08,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15344
[2025-01-15 17:01:08,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:01:08,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1300/2809]  eta: 0:15:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4134 (4.4887)  class_acc: 0.1250 (0.1246)  loss_scale: 65536.0000 (66417.5373)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1724  max mem: 15572
Epoch: [5]  [1310/2809]  eta: 0:14:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4468 (4.4886)  class_acc: 0.0833 (0.1248)  loss_scale: 65536.0000 (66410.8131)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1739  max mem: 15572
Epoch: [5]  [1320/2809]  eta: 0:14:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4545 (4.4886)  class_acc: 0.1250 (0.1248)  loss_scale: 65536.0000 (66404.1908)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1355  max mem: 15572
Epoch: [5]  [1330/2809]  eta: 0:14:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5464 (4.4895)  class_acc: 0.0833 (0.1244)  loss_scale: 65536.0000 (66397.6679)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1436  max mem: 15572
Epoch: [5]  [1340/2809]  eta: 0:14:39  lr: 0.000047  min_lr: 0.000000  loss: 4.5547 (4.4888)  class_acc: 0.0833 (0.1248)  loss_scale: 65536.0000 (66391.2424)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1486  max mem: 15572
Epoch: [5]  [1350/2809]  eta: 0:14:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4137 (4.4880)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (66384.9119)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1393  max mem: 15572
Epoch: [5]  [1360/2809]  eta: 0:14:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3264 (4.4870)  class_acc: 0.1250 (0.1249)  loss_scale: 65536.0000 (66378.6745)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1652  max mem: 15572
Epoch: [5]  [1370/2809]  eta: 0:14:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4484 (4.4875)  class_acc: 0.1250 (0.1247)  loss_scale: 65536.0000 (66372.5281)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1155  max mem: 15572
Epoch: [5]  [1380/2809]  eta: 0:14:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4065 (4.4863)  class_acc: 0.1250 (0.1249)  loss_scale: 65536.0000 (66366.4707)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1417  max mem: 15572
Epoch: [5]  [1390/2809]  eta: 0:14:10  lr: 0.000047  min_lr: 0.000000  loss: 4.3164 (4.4850)  class_acc: 0.1250 (0.1251)  loss_scale: 65536.0000 (66360.5004)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1671  max mem: 15572
Epoch: [5]  [1400/2809]  eta: 0:14:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4224 (4.4855)  class_acc: 0.0833 (0.1251)  loss_scale: 65536.0000 (66354.6153)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1154  max mem: 15572
Epoch: [5]  [1410/2809]  eta: 0:13:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4211 (4.4850)  class_acc: 0.1250 (0.1253)  loss_scale: 65536.0000 (66348.8136)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1229  max mem: 15572
Epoch: [5]  [1420/2809]  eta: 0:13:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3779 (4.4854)  class_acc: 0.1250 (0.1252)  loss_scale: 65536.0000 (66343.0936)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1273  max mem: 15572
[2025-01-15 17:02:25,703] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:02:25,703] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1430/2809]  eta: 0:13:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4977 (4.4844)  class_acc: 0.1250 (0.1255)  loss_scale: 65536.0000 (66474.8456)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1213  max mem: 15572
[2025-01-15 17:02:31,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15482
[2025-01-15 17:02:31,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:02:31,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1440/2809]  eta: 0:13:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4250 (4.4837)  class_acc: 0.1667 (0.1258)  loss_scale: 65536.0000 (66741.2075)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1491  max mem: 15572
Epoch: [5]  [1450/2809]  eta: 0:13:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4965 (4.4848)  class_acc: 0.1667 (0.1258)  loss_scale: 65536.0000 (66732.9014)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1743  max mem: 15572
Epoch: [5]  [1460/2809]  eta: 0:13:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4481 (4.4840)  class_acc: 0.1250 (0.1260)  loss_scale: 65536.0000 (66724.7091)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1340  max mem: 15572
Epoch: [5]  [1470/2809]  eta: 0:13:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4316 (4.4840)  class_acc: 0.1250 (0.1259)  loss_scale: 65536.0000 (66716.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1490  max mem: 15572
Epoch: [5]  [1480/2809]  eta: 0:13:16  lr: 0.000047  min_lr: 0.000000  loss: 4.5115 (4.4842)  class_acc: 0.0833 (0.1257)  loss_scale: 65536.0000 (66708.6563)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.2229  max mem: 15572
Epoch: [5]  [1490/2809]  eta: 0:13:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4957 (4.4841)  class_acc: 0.0833 (0.1257)  loss_scale: 65536.0000 (66700.7914)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1952  max mem: 15572
Epoch: [5]  [1500/2809]  eta: 0:13:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4774 (4.4838)  class_acc: 0.1250 (0.1258)  loss_scale: 65536.0000 (66693.0313)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1226  max mem: 15572
Epoch: [5]  [1510/2809]  eta: 0:12:59  lr: 0.000047  min_lr: 0.000000  loss: 4.4441 (4.4843)  class_acc: 0.1250 (0.1259)  loss_scale: 65536.0000 (66685.3739)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.1813  max mem: 15572
Epoch: [5]  [1520/2809]  eta: 0:12:53  lr: 0.000047  min_lr: 0.000000  loss: 4.4591 (4.4834)  class_acc: 0.0833 (0.1260)  loss_scale: 65536.0000 (66677.8172)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.1837  max mem: 15572
Epoch: [5]  [1530/2809]  eta: 0:12:47  lr: 0.000047  min_lr: 0.000000  loss: 4.4389 (4.4831)  class_acc: 0.1250 (0.1263)  loss_scale: 65536.0000 (66670.3592)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1096  max mem: 15572
Epoch: [5]  [1540/2809]  eta: 0:12:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3962 (4.4824)  class_acc: 0.1250 (0.1264)  loss_scale: 65536.0000 (66662.9981)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1342  max mem: 15572
Epoch: [5]  [1550/2809]  eta: 0:12:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3314 (4.4810)  class_acc: 0.1250 (0.1266)  loss_scale: 65536.0000 (66655.7318)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1540  max mem: 15572
Epoch: [5]  [1560/2809]  eta: 0:12:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4304 (4.4809)  class_acc: 0.1250 (0.1266)  loss_scale: 65536.0000 (66648.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1447  max mem: 15572
[2025-01-15 17:03:49,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:03:49,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [1570/2809]  eta: 0:12:22  lr: 0.000047  min_lr: 0.000000  loss: 4.5342 (4.4810)  class_acc: 0.1250 (0.1267)  loss_scale: 65536.0000 (66850.0573)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1346  max mem: 15572
Epoch: [5]  [1580/2809]  eta: 0:12:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4881 (4.4813)  class_acc: 0.1250 (0.1266)  loss_scale: 131072.0000 (67256.2682)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1323  max mem: 15572
[2025-01-15 17:04:04,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15635
[2025-01-15 17:04:04,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:04:04,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1590/2809]  eta: 0:12:11  lr: 0.000047  min_lr: 0.000000  loss: 4.3822 (4.4805)  class_acc: 0.1667 (0.1269)  loss_scale: 131072.0000 (67616.1810)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1806  max mem: 15572
Epoch: [5]  [1600/2809]  eta: 0:12:04  lr: 0.000047  min_lr: 0.000000  loss: 4.3253 (4.4796)  class_acc: 0.1667 (0.1270)  loss_scale: 65536.0000 (67603.1880)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1175  max mem: 15572
Epoch: [5]  [1610/2809]  eta: 0:11:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4431 (4.4798)  class_acc: 0.1250 (0.1270)  loss_scale: 65536.0000 (67590.3563)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0874  max mem: 15572
Epoch: [5]  [1620/2809]  eta: 0:11:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3668 (4.4791)  class_acc: 0.1250 (0.1272)  loss_scale: 65536.0000 (67577.6829)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1485  max mem: 15572
Epoch: [5]  [1630/2809]  eta: 0:11:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3720 (4.4794)  class_acc: 0.1250 (0.1272)  loss_scale: 65536.0000 (67565.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0803  max mem: 15572
Epoch: [5]  [1640/2809]  eta: 0:11:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4188 (4.4778)  class_acc: 0.0833 (0.1274)  loss_scale: 65536.0000 (67552.7995)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0835  max mem: 15572
Epoch: [5]  [1650/2809]  eta: 0:11:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3675 (4.4772)  class_acc: 0.1250 (0.1274)  loss_scale: 65536.0000 (67540.5839)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1354  max mem: 15572
Epoch: [5]  [1660/2809]  eta: 0:11:27  lr: 0.000047  min_lr: 0.000000  loss: 4.3720 (4.4775)  class_acc: 0.1250 (0.1275)  loss_scale: 65536.0000 (67528.5154)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1326  max mem: 15572
Epoch: [5]  [1670/2809]  eta: 0:11:21  lr: 0.000047  min_lr: 0.000000  loss: 4.3182 (4.4773)  class_acc: 0.1250 (0.1276)  loss_scale: 65536.0000 (67516.5913)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1644  max mem: 15572
Epoch: [5]  [1680/2809]  eta: 0:11:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3550 (4.4760)  class_acc: 0.1250 (0.1278)  loss_scale: 65536.0000 (67504.8090)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.2018  max mem: 15572
Epoch: [5]  [1690/2809]  eta: 0:11:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3977 (4.4759)  class_acc: 0.1667 (0.1279)  loss_scale: 65536.0000 (67493.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1433  max mem: 15572
Epoch: [5]  [1700/2809]  eta: 0:11:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4521 (4.4757)  class_acc: 0.1250 (0.1279)  loss_scale: 65536.0000 (67481.6602)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0920  max mem: 15572
Epoch: [5]  [1710/2809]  eta: 0:10:56  lr: 0.000047  min_lr: 0.000000  loss: 4.5301 (4.4759)  class_acc: 0.1250 (0.1278)  loss_scale: 65536.0000 (67470.2887)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0590  max mem: 15572
[2025-01-15 17:05:16,803] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15764
[2025-01-15 17:05:16,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:05:16,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [5]  [1720/2809]  eta: 0:10:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4055 (4.4750)  class_acc: 0.1250 (0.1281)  loss_scale: 65536.0000 (67420.9692)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0758  max mem: 15572
Epoch: [5]  [1730/2809]  eta: 0:10:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4402 (4.4750)  class_acc: 0.1250 (0.1281)  loss_scale: 32768.0000 (67220.7787)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1527  max mem: 15572
Epoch: [5]  [1740/2809]  eta: 0:10:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4507 (4.4743)  class_acc: 0.1250 (0.1284)  loss_scale: 32768.0000 (67022.8880)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1329  max mem: 15572
Epoch: [5]  [1750/2809]  eta: 0:10:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4507 (4.4747)  class_acc: 0.1250 (0.1282)  loss_scale: 32768.0000 (66827.2576)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1123  max mem: 15572
Epoch: [5]  [1760/2809]  eta: 0:10:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5549 (4.4750)  class_acc: 0.0833 (0.1280)  loss_scale: 32768.0000 (66633.8489)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1519  max mem: 15572
Epoch: [5]  [1770/2809]  eta: 0:10:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4857 (4.4753)  class_acc: 0.0833 (0.1279)  loss_scale: 32768.0000 (66442.6245)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1493  max mem: 15572
Epoch: [5]  [1780/2809]  eta: 0:10:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3996 (4.4754)  class_acc: 0.1250 (0.1279)  loss_scale: 32768.0000 (66253.5474)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1020  max mem: 15572
Epoch: [5]  [1790/2809]  eta: 0:10:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5699 (4.4764)  class_acc: 0.0417 (0.1277)  loss_scale: 32768.0000 (66066.5818)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1155  max mem: 15572
Epoch: [5]  [1800/2809]  eta: 0:10:01  lr: 0.000047  min_lr: 0.000000  loss: 4.6664 (4.4769)  class_acc: 0.0417 (0.1276)  loss_scale: 32768.0000 (65881.6924)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1506  max mem: 15572
Epoch: [5]  [1810/2809]  eta: 0:09:55  lr: 0.000047  min_lr: 0.000000  loss: 4.4758 (4.4761)  class_acc: 0.1250 (0.1279)  loss_scale: 32768.0000 (65698.8448)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1678  max mem: 15572
Epoch: [5]  [1820/2809]  eta: 0:09:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3054 (4.4760)  class_acc: 0.1250 (0.1277)  loss_scale: 32768.0000 (65518.0055)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1965  max mem: 15572
Epoch: [5]  [1830/2809]  eta: 0:09:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4515 (4.4755)  class_acc: 0.1250 (0.1279)  loss_scale: 32768.0000 (65339.1415)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1477  max mem: 15572
Epoch: [5]  [1840/2809]  eta: 0:09:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4135 (4.4755)  class_acc: 0.1667 (0.1278)  loss_scale: 32768.0000 (65162.2205)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1689  max mem: 15572
[2025-01-15 17:06:34,075] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:06:34,075] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [5]  [1850/2809]  eta: 0:09:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4750 (4.4756)  class_acc: 0.1667 (0.1280)  loss_scale: 32768.0000 (65040.3198)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.2400  max mem: 15572
Epoch: [5]  [1860/2809]  eta: 0:09:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4875 (4.4760)  class_acc: 0.1250 (0.1277)  loss_scale: 65536.0000 (65042.9833)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.2251  max mem: 15572
Epoch: [5]  [1870/2809]  eta: 0:09:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4284 (4.4754)  class_acc: 0.0833 (0.1278)  loss_scale: 65536.0000 (65045.6184)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1475  max mem: 15572
Epoch: [5]  [1880/2809]  eta: 0:09:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3147 (4.4748)  class_acc: 0.1250 (0.1279)  loss_scale: 65536.0000 (65048.2254)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0862  max mem: 15572
Epoch: [5]  [1890/2809]  eta: 0:09:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3486 (4.4746)  class_acc: 0.1250 (0.1280)  loss_scale: 65536.0000 (65050.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1324  max mem: 15572
Epoch: [5]  [1900/2809]  eta: 0:09:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4562 (4.4744)  class_acc: 0.1250 (0.1279)  loss_scale: 65536.0000 (65053.3572)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1498  max mem: 15572
Epoch: [5]  [1910/2809]  eta: 0:08:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4109 (4.4740)  class_acc: 0.1250 (0.1281)  loss_scale: 65536.0000 (65055.8828)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1202  max mem: 15572
Epoch: [5]  [1920/2809]  eta: 0:08:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3318 (4.4740)  class_acc: 0.1250 (0.1279)  loss_scale: 65536.0000 (65058.3821)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1035  max mem: 15572
Epoch: [5]  [1930/2809]  eta: 0:08:44  lr: 0.000047  min_lr: 0.000000  loss: 4.6250 (4.4742)  class_acc: 0.0833 (0.1278)  loss_scale: 65536.0000 (65060.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0917  max mem: 15572
Epoch: [5]  [1940/2809]  eta: 0:08:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4382 (4.4734)  class_acc: 0.1250 (0.1279)  loss_scale: 65536.0000 (65063.3035)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0452  max mem: 15572
Epoch: [5]  [1950/2809]  eta: 0:08:31  lr: 0.000047  min_lr: 0.000000  loss: 4.2893 (4.4731)  class_acc: 0.1250 (0.1280)  loss_scale: 65536.0000 (65065.7263)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0555  max mem: 15572
[2025-01-15 17:07:32,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=97, lr=[4.537251678590611e-07, 4.537251678590611e-07, 6.481788112272302e-07, 6.481788112272302e-07, 9.259697303246147e-07, 9.259697303246147e-07, 1.3228139004637354e-06, 1.3228139004637354e-06, 1.889734143519622e-06, 1.889734143519622e-06, 2.6996202050280316e-06, 2.6996202050280316e-06, 3.856600292897189e-06, 3.856600292897189e-06, 5.509428989853127e-06, 5.509428989853127e-06, 7.870612842647324e-06, 7.870612842647324e-06, 1.124373263235332e-05, 1.124373263235332e-05, 1.606247518907617e-05, 1.606247518907617e-05, 2.2946393127251676e-05, 2.2946393127251676e-05, 3.2780561610359544e-05, 3.2780561610359544e-05, 4.682937372908506e-05, 4.682937372908506e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:07:32,894] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=28.429665462208547, CurrSamplesPerSec=31.60586708262218, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [5]  [1960/2809]  eta: 0:08:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4050 (4.4731)  class_acc: 0.1250 (0.1280)  loss_scale: 65536.0000 (65068.1244)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1373  max mem: 15572
Epoch: [5]  [1970/2809]  eta: 0:08:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4440 (4.4730)  class_acc: 0.1250 (0.1280)  loss_scale: 65536.0000 (65070.4982)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1533  max mem: 15572
[2025-01-15 17:07:45,329] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:07:45,329] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:07:48,259] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16024
[2025-01-15 17:07:48,259] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:07:48,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [1980/2809]  eta: 0:08:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5371 (4.4737)  class_acc: 0.1250 (0.1280)  loss_scale: 65536.0000 (65172.0949)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1662  max mem: 15572
Epoch: [5]  [1990/2809]  eta: 0:08:07  lr: 0.000047  min_lr: 0.000000  loss: 4.5528 (4.4740)  class_acc: 0.1250 (0.1281)  loss_scale: 65536.0000 (65173.9227)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1321  max mem: 15572
Epoch: [5]  [2000/2809]  eta: 0:08:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4901 (4.4738)  class_acc: 0.1250 (0.1282)  loss_scale: 65536.0000 (65175.7321)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0929  max mem: 15572
Epoch: [5]  [2010/2809]  eta: 0:07:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3765 (4.4731)  class_acc: 0.1250 (0.1283)  loss_scale: 65536.0000 (65177.5236)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1020  max mem: 15572
Epoch: [5]  [2020/2809]  eta: 0:07:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3816 (4.4730)  class_acc: 0.1250 (0.1282)  loss_scale: 65536.0000 (65179.2974)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1360  max mem: 15572
Epoch: [5]  [2030/2809]  eta: 0:07:43  lr: 0.000047  min_lr: 0.000000  loss: 4.4974 (4.4725)  class_acc: 0.1250 (0.1283)  loss_scale: 65536.0000 (65181.0537)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.1857  max mem: 15572
Epoch: [5]  [2040/2809]  eta: 0:07:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4974 (4.4726)  class_acc: 0.1250 (0.1283)  loss_scale: 65536.0000 (65182.7927)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1471  max mem: 15572
Epoch: [5]  [2050/2809]  eta: 0:07:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.4721)  class_acc: 0.1250 (0.1285)  loss_scale: 65536.0000 (65184.5149)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.0968  max mem: 15572
Epoch: [5]  [2060/2809]  eta: 0:07:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.4710)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65186.2203)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1552  max mem: 15572
Epoch: [5]  [2070/2809]  eta: 0:07:19  lr: 0.000047  min_lr: 0.000000  loss: 4.2964 (4.4704)  class_acc: 0.1667 (0.1287)  loss_scale: 65536.0000 (65187.9092)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1540  max mem: 15572
Epoch: [5]  [2080/2809]  eta: 0:07:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4107 (4.4708)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65189.5819)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1147  max mem: 15572
Epoch: [5]  [2090/2809]  eta: 0:07:07  lr: 0.000047  min_lr: 0.000000  loss: 4.4551 (4.4708)  class_acc: 0.1250 (0.1287)  loss_scale: 65536.0000 (65191.2386)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1641  max mem: 15572
Epoch: [5]  [2100/2809]  eta: 0:07:01  lr: 0.000047  min_lr: 0.000000  loss: 4.5142 (4.4710)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65192.8796)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1550  max mem: 15572
[2025-01-15 17:09:05,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:09:05,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2110/2809]  eta: 0:06:55  lr: 0.000047  min_lr: 0.000000  loss: 4.5054 (4.4713)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65287.6400)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1278  max mem: 15572
[2025-01-15 17:09:11,477] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16161
[2025-01-15 17:09:11,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:09:11,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2120/2809]  eta: 0:06:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4958 (4.4714)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65443.3041)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.1995  max mem: 15572
Epoch: [5]  [2130/2809]  eta: 0:06:44  lr: 0.000047  min_lr: 0.000000  loss: 4.2584 (4.4705)  class_acc: 0.1250 (0.1286)  loss_scale: 65536.0000 (65443.7391)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1677  max mem: 15572
Epoch: [5]  [2140/2809]  eta: 0:06:38  lr: 0.000047  min_lr: 0.000000  loss: 4.2584 (4.4695)  class_acc: 0.1667 (0.1288)  loss_scale: 65536.0000 (65444.1700)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0522  max mem: 15572
Epoch: [5]  [2150/2809]  eta: 0:06:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4176 (4.4697)  class_acc: 0.1667 (0.1287)  loss_scale: 65536.0000 (65444.5969)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0631  max mem: 15572
Epoch: [5]  [2160/2809]  eta: 0:06:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5452 (4.4700)  class_acc: 0.1250 (0.1288)  loss_scale: 65536.0000 (65445.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1257  max mem: 15572
Epoch: [5]  [2170/2809]  eta: 0:06:19  lr: 0.000047  min_lr: 0.000000  loss: 4.5655 (4.4703)  class_acc: 0.1250 (0.1288)  loss_scale: 65536.0000 (65445.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1405  max mem: 15572
Epoch: [5]  [2180/2809]  eta: 0:06:13  lr: 0.000047  min_lr: 0.000000  loss: 4.5840 (4.4706)  class_acc: 0.1250 (0.1289)  loss_scale: 65536.0000 (65445.8542)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1190  max mem: 15572
Epoch: [5]  [2190/2809]  eta: 0:06:08  lr: 0.000047  min_lr: 0.000000  loss: 4.5365 (4.4711)  class_acc: 0.0833 (0.1287)  loss_scale: 65536.0000 (65446.2656)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1466  max mem: 15572
Epoch: [5]  [2200/2809]  eta: 0:06:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4650 (4.4709)  class_acc: 0.1250 (0.1288)  loss_scale: 65536.0000 (65446.6733)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1583  max mem: 15572
Epoch: [5]  [2210/2809]  eta: 0:05:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3787 (4.4706)  class_acc: 0.1250 (0.1288)  loss_scale: 65536.0000 (65447.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1943  max mem: 15572
Epoch: [5]  [2220/2809]  eta: 0:05:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3787 (4.4701)  class_acc: 0.1667 (0.1291)  loss_scale: 65536.0000 (65447.4777)  weight_decay: 0.0500 (0.0500)  time: 0.6782  data: 0.2310  max mem: 15572
Epoch: [5]  [2230/2809]  eta: 0:05:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3500 (4.4691)  class_acc: 0.1667 (0.1293)  loss_scale: 65536.0000 (65447.8745)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1903  max mem: 15572
Epoch: [5]  [2240/2809]  eta: 0:05:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3500 (4.4694)  class_acc: 0.1250 (0.1294)  loss_scale: 65536.0000 (65448.2677)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1667  max mem: 15572
[2025-01-15 17:10:27,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:10:27,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2250/2809]  eta: 0:05:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4979 (4.4696)  class_acc: 0.1250 (0.1294)  loss_scale: 65536.0000 (65623.3425)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1652  max mem: 15572
[2025-01-15 17:10:35,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16302
[2025-01-15 17:10:35,009] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:10:35,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2260/2809]  eta: 0:05:26  lr: 0.000047  min_lr: 0.000000  loss: 4.2948 (4.4688)  class_acc: 0.1250 (0.1295)  loss_scale: 131072.0000 (65796.8686)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1731  max mem: 15572
Epoch: [5]  [2270/2809]  eta: 0:05:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4571 (4.4691)  class_acc: 0.0833 (0.1294)  loss_scale: 65536.0000 (65795.7199)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.1934  max mem: 15572
Epoch: [5]  [2280/2809]  eta: 0:05:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4452 (4.4684)  class_acc: 0.1250 (0.1296)  loss_scale: 65536.0000 (65794.5813)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1030  max mem: 15572
Epoch: [5]  [2290/2809]  eta: 0:05:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3965 (4.4685)  class_acc: 0.1667 (0.1297)  loss_scale: 65536.0000 (65793.4526)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0577  max mem: 15572
Epoch: [5]  [2300/2809]  eta: 0:05:02  lr: 0.000047  min_lr: 0.000000  loss: 4.4865 (4.4684)  class_acc: 0.0833 (0.1296)  loss_scale: 65536.0000 (65792.3338)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1444  max mem: 15572
Epoch: [5]  [2310/2809]  eta: 0:04:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3474 (4.4680)  class_acc: 0.0833 (0.1297)  loss_scale: 65536.0000 (65791.2246)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0929  max mem: 15572
Epoch: [5]  [2320/2809]  eta: 0:04:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3474 (4.4679)  class_acc: 0.1250 (0.1296)  loss_scale: 65536.0000 (65790.1249)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0822  max mem: 15572
Epoch: [5]  [2330/2809]  eta: 0:04:44  lr: 0.000047  min_lr: 0.000000  loss: 4.3948 (4.4675)  class_acc: 0.0833 (0.1295)  loss_scale: 65536.0000 (65789.0347)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1215  max mem: 15572
Epoch: [5]  [2340/2809]  eta: 0:04:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3491 (4.4668)  class_acc: 0.1250 (0.1299)  loss_scale: 65536.0000 (65787.9539)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1499  max mem: 15572
Epoch: [5]  [2350/2809]  eta: 0:04:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3625 (4.4668)  class_acc: 0.1250 (0.1297)  loss_scale: 65536.0000 (65786.8822)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1525  max mem: 15572
Epoch: [5]  [2360/2809]  eta: 0:04:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3970 (4.4666)  class_acc: 0.0833 (0.1298)  loss_scale: 65536.0000 (65785.8196)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1025  max mem: 15572
Epoch: [5]  [2370/2809]  eta: 0:04:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3930 (4.4661)  class_acc: 0.1667 (0.1299)  loss_scale: 65536.0000 (65784.7659)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1297  max mem: 15572
Epoch: [5]  [2380/2809]  eta: 0:04:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3010 (4.4652)  class_acc: 0.1667 (0.1300)  loss_scale: 65536.0000 (65783.7211)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1551  max mem: 15572
[2025-01-15 17:11:48,742] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:11:48,743] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2390/2809]  eta: 0:04:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3010 (4.4652)  class_acc: 0.1250 (0.1299)  loss_scale: 65536.0000 (65919.7323)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1716  max mem: 15572
[2025-01-15 17:11:53,178] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16437
[2025-01-15 17:11:53,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:11:53,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2400/2809]  eta: 0:04:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4378 (4.4655)  class_acc: 0.1250 (0.1299)  loss_scale: 65536.0000 (65945.4294)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1447  max mem: 15572
Epoch: [5]  [2410/2809]  eta: 0:03:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4278 (4.4653)  class_acc: 0.1250 (0.1300)  loss_scale: 65536.0000 (65943.7312)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1441  max mem: 15572
Epoch: [5]  [2420/2809]  eta: 0:03:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3799 (4.4642)  class_acc: 0.1667 (0.1301)  loss_scale: 65536.0000 (65942.0471)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.2125  max mem: 15572
Epoch: [5]  [2430/2809]  eta: 0:03:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4363 (4.4647)  class_acc: 0.0833 (0.1300)  loss_scale: 65536.0000 (65940.3768)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1376  max mem: 15572
Epoch: [5]  [2440/2809]  eta: 0:03:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4718 (4.4645)  class_acc: 0.1250 (0.1301)  loss_scale: 65536.0000 (65938.7202)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0452  max mem: 15572
Epoch: [5]  [2450/2809]  eta: 0:03:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3591 (4.4639)  class_acc: 0.1667 (0.1302)  loss_scale: 65536.0000 (65937.0771)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0851  max mem: 15572
Epoch: [5]  [2460/2809]  eta: 0:03:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4525 (4.4647)  class_acc: 0.1250 (0.1301)  loss_scale: 65536.0000 (65935.4474)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1635  max mem: 15572
Epoch: [5]  [2470/2809]  eta: 0:03:21  lr: 0.000047  min_lr: 0.000000  loss: 4.5261 (4.4648)  class_acc: 0.1250 (0.1300)  loss_scale: 65536.0000 (65933.8308)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1691  max mem: 15572
Epoch: [5]  [2480/2809]  eta: 0:03:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4943 (4.4646)  class_acc: 0.1250 (0.1301)  loss_scale: 65536.0000 (65932.2273)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1282  max mem: 15572
Epoch: [5]  [2490/2809]  eta: 0:03:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2818 (4.4636)  class_acc: 0.1250 (0.1303)  loss_scale: 65536.0000 (65930.6367)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.1897  max mem: 15572
Epoch: [5]  [2500/2809]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000000  loss: 4.2818 (4.4639)  class_acc: 0.1667 (0.1303)  loss_scale: 65536.0000 (65929.0588)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1619  max mem: 15572
Epoch: [5]  [2510/2809]  eta: 0:02:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4740 (4.4633)  class_acc: 0.1250 (0.1304)  loss_scale: 65536.0000 (65927.4934)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.1170  max mem: 15572
Epoch: [5]  [2520/2809]  eta: 0:02:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3910 (4.4629)  class_acc: 0.1667 (0.1305)  loss_scale: 65536.0000 (65925.9405)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1228  max mem: 15572
[2025-01-15 17:13:09,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:13:09,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:13:10,341] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16568
[2025-01-15 17:13:10,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:13:10,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2530/2809]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3355 (4.4621)  class_acc: 0.1667 (0.1307)  loss_scale: 65536.0000 (65976.1865)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1067  max mem: 15572
Epoch: [5]  [2540/2809]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000000  loss: 4.4758 (4.4624)  class_acc: 0.0833 (0.1306)  loss_scale: 65536.0000 (65974.4542)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1065  max mem: 15572
Epoch: [5]  [2550/2809]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4991 (4.4624)  class_acc: 0.0833 (0.1306)  loss_scale: 65536.0000 (65972.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1116  max mem: 15572
Epoch: [5]  [2560/2809]  eta: 0:02:27  lr: 0.000047  min_lr: 0.000000  loss: 4.4214 (4.4623)  class_acc: 0.0833 (0.1305)  loss_scale: 65536.0000 (65971.0301)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1496  max mem: 15572
Epoch: [5]  [2570/2809]  eta: 0:02:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4009 (4.4626)  class_acc: 0.1250 (0.1305)  loss_scale: 65536.0000 (65969.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.1959  max mem: 15572
Epoch: [5]  [2580/2809]  eta: 0:02:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3748 (4.4624)  class_acc: 0.1250 (0.1305)  loss_scale: 65536.0000 (65967.6590)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2054  max mem: 15572
Epoch: [5]  [2590/2809]  eta: 0:02:10  lr: 0.000047  min_lr: 0.000000  loss: 4.4825 (4.4628)  class_acc: 0.1250 (0.1304)  loss_scale: 65536.0000 (65965.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1301  max mem: 15572
Epoch: [5]  [2600/2809]  eta: 0:02:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4996 (4.4631)  class_acc: 0.1250 (0.1304)  loss_scale: 65536.0000 (65964.3399)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0843  max mem: 15572
Epoch: [5]  [2610/2809]  eta: 0:01:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4452 (4.4629)  class_acc: 0.1250 (0.1303)  loss_scale: 65536.0000 (65962.6993)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1360  max mem: 15572
Epoch: [5]  [2620/2809]  eta: 0:01:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4280 (4.4629)  class_acc: 0.1250 (0.1303)  loss_scale: 65536.0000 (65961.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1463  max mem: 15572
Epoch: [5]  [2630/2809]  eta: 0:01:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3840 (4.4625)  class_acc: 0.1667 (0.1304)  loss_scale: 65536.0000 (65959.4557)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1260  max mem: 15572
Epoch: [5]  [2640/2809]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3636 (4.4626)  class_acc: 0.0833 (0.1303)  loss_scale: 65536.0000 (65957.8523)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1574  max mem: 15572
Epoch: [5]  [2650/2809]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3636 (4.4623)  class_acc: 0.0833 (0.1304)  loss_scale: 65536.0000 (65956.2610)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1653  max mem: 15572
[2025-01-15 17:14:26,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:14:26,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:14:27,290] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16698
[2025-01-15 17:14:27,290] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:14:27,290] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2660/2809]  eta: 0:01:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3508 (4.4623)  class_acc: 0.0833 (0.1304)  loss_scale: 65536.0000 (65979.3100)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1853  max mem: 15572
Epoch: [5]  [2670/2809]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000000  loss: 4.4477 (4.4623)  class_acc: 0.1250 (0.1304)  loss_scale: 65536.0000 (65977.6503)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1434  max mem: 15572
Epoch: [5]  [2680/2809]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4287 (4.4617)  class_acc: 0.1250 (0.1305)  loss_scale: 65536.0000 (65976.0030)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0356  max mem: 15572
Epoch: [5]  [2690/2809]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2591 (4.4607)  class_acc: 0.1667 (0.1306)  loss_scale: 65536.0000 (65974.3679)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0592  max mem: 15572
Epoch: [5]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.1955 (4.4598)  class_acc: 0.2083 (0.1309)  loss_scale: 65536.0000 (65972.7449)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0825  max mem: 15572
Epoch: [5]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3982 (4.4604)  class_acc: 0.1667 (0.1309)  loss_scale: 65536.0000 (65971.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0822  max mem: 15572
Epoch: [5]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5444 (4.4606)  class_acc: 0.0833 (0.1308)  loss_scale: 65536.0000 (65969.5347)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1338  max mem: 15572
Epoch: [5]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.5343 (4.4611)  class_acc: 0.0833 (0.1306)  loss_scale: 65536.0000 (65967.9473)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1299  max mem: 15572
Epoch: [5]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3947 (4.4605)  class_acc: 0.1250 (0.1309)  loss_scale: 65536.0000 (65966.3714)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1603  max mem: 15572
Epoch: [5]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2066 (4.4596)  class_acc: 0.1250 (0.1309)  loss_scale: 65536.0000 (65964.8070)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1342  max mem: 15572
Epoch: [5]  [2760/2809]  eta: 0:00:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3063 (4.4592)  class_acc: 0.1250 (0.1311)  loss_scale: 65536.0000 (65963.2539)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0906  max mem: 15572
Epoch: [5]  [2770/2809]  eta: 0:00:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3684 (4.4589)  class_acc: 0.1250 (0.1311)  loss_scale: 65536.0000 (65961.7120)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1486  max mem: 15572
Epoch: [5]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4220 (4.4588)  class_acc: 0.1250 (0.1312)  loss_scale: 65536.0000 (65960.1812)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1929  max mem: 15572
[2025-01-15 17:15:39,819] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:15:39,820] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [5]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4497 (4.4588)  class_acc: 0.1250 (0.1311)  loss_scale: 65536.0000 (66169.9921)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.2149  max mem: 15572
[2025-01-15 17:15:46,466] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16837
[2025-01-15 17:15:46,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:15:46,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [5]  [2800/2809]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000000  loss: 4.4417 (4.4589)  class_acc: 0.1250 (0.1312)  loss_scale: 65536.0000 (66191.1260)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1342  max mem: 15572
Epoch: [5]  [2808/2809]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4417 (4.4590)  class_acc: 0.1250 (0.1312)  loss_scale: 65536.0000 (66189.2602)  weight_decay: 0.0500 (0.0500)  time: 0.4317  data: 0.0259  max mem: 15572
Epoch: [5] Total time: 0:27:44 (0.5925 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000000  loss: 4.4417 (4.4590)  class_acc: 0.1250 (0.1312)  loss_scale: 65536.0000 (66189.2602)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:21:46  loss: 0.8775 (0.8775)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.8045  data: 4.5271  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 4.2032 (3.7985)  acc1: 0.0000 (24.2424)  acc5: 11.1111 (30.8081)  time: 0.7468  data: 0.5188  max mem: 15572
Val:  [ 20/272]  eta: 0:02:07  loss: 3.9888 (3.8339)  acc1: 0.0000 (18.5185)  acc5: 27.7778 (31.7460)  time: 0.2910  data: 0.0815  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 3.9727 (3.9218)  acc1: 0.0000 (12.9032)  acc5: 27.7778 (31.3620)  time: 0.3030  data: 0.1009  max mem: 15572
Val:  [ 40/272]  eta: 0:01:43  loss: 3.6353 (3.7914)  acc1: 0.0000 (13.8211)  acc5: 38.8889 (36.9919)  time: 0.3795  data: 0.1753  max mem: 15572
Val:  [ 50/272]  eta: 0:01:37  loss: 3.2218 (3.7569)  acc1: 11.1111 (13.2898)  acc5: 50.0000 (39.2157)  time: 0.4080  data: 0.2004  max mem: 15572
Val:  [ 60/272]  eta: 0:01:28  loss: 2.8789 (3.6041)  acc1: 16.6667 (19.3078)  acc5: 72.2222 (43.7158)  time: 0.3569  data: 0.1553  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 2.9500 (3.5127)  acc1: 33.3333 (21.2050)  acc5: 72.2222 (47.0266)  time: 0.2561  data: 0.0590  max mem: 15572
Val:  [ 80/272]  eta: 0:01:12  loss: 3.1917 (3.5131)  acc1: 11.1111 (22.0165)  acc5: 61.1111 (46.6392)  time: 0.2555  data: 0.0531  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 4.4147 (3.6139)  acc1: 0.0000 (19.7192)  acc5: 11.1111 (42.4908)  time: 0.2585  data: 0.0511  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 4.3698 (3.6711)  acc1: 0.0000 (18.9769)  acc5: 11.1111 (40.7591)  time: 0.2106  data: 0.0187  max mem: 15572
Val:  [110/272]  eta: 0:00:53  loss: 4.1322 (3.7314)  acc1: 0.0000 (17.4675)  acc5: 16.6667 (39.1391)  time: 0.1951  data: 0.0139  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 4.2059 (3.7796)  acc1: 0.0000 (16.0239)  acc5: 16.6667 (38.1084)  time: 0.1876  data: 0.0011  max mem: 15572
Val:  [130/272]  eta: 0:00:46  loss: 4.1712 (3.7173)  acc1: 0.0000 (18.1934)  acc5: 33.3333 (39.8643)  time: 0.2959  data: 0.1107  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 3.4243 (3.7012)  acc1: 22.2222 (19.0701)  acc5: 55.5556 (40.2679)  time: 0.3924  data: 0.1893  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 3.9413 (3.7175)  acc1: 5.5556 (18.1015)  acc5: 27.7778 (39.5511)  time: 0.3910  data: 0.1896  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 3.7546 (3.7031)  acc1: 11.1111 (19.2892)  acc5: 38.8889 (41.0973)  time: 0.3797  data: 0.1892  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 3.6567 (3.7338)  acc1: 11.1111 (18.4860)  acc5: 50.0000 (40.5133)  time: 0.3539  data: 0.1645  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.8985 (3.7325)  acc1: 0.0000 (18.0172)  acc5: 33.3333 (40.7919)  time: 0.3963  data: 0.1931  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.9173 (3.7518)  acc1: 0.0000 (17.4229)  acc5: 33.3333 (40.0524)  time: 0.4104  data: 0.1849  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.7038 (3.7499)  acc1: 0.0000 (17.5235)  acc5: 44.4444 (40.9895)  time: 0.3157  data: 0.0850  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 3.5535 (3.7643)  acc1: 5.5556 (17.5355)  acc5: 55.5556 (41.0742)  time: 0.2846  data: 0.0668  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.8640 (3.7696)  acc1: 11.1111 (17.5968)  acc5: 38.8889 (40.9754)  time: 0.3415  data: 0.1320  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 3.4422 (3.7388)  acc1: 27.7778 (19.5286)  acc5: 61.1111 (42.4723)  time: 0.3598  data: 0.1459  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 3.0221 (3.7184)  acc1: 44.4444 (19.9862)  acc5: 83.3333 (43.8220)  time: 0.3504  data: 0.1482  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.5728 (3.7457)  acc1: 11.1111 (19.4112)  acc5: 38.8889 (43.2050)  time: 0.3406  data: 0.1514  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 3.4621 (3.6773)  acc1: 33.3333 (21.5837)  acc5: 72.2222 (44.9553)  time: 0.3453  data: 0.1444  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.8905 (3.6746)  acc1: 55.5556 (21.5662)  acc5: 77.7778 (44.9980)  time: 0.2867  data: 0.1002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.8905 (3.6777)  acc1: 55.5556 (21.5646)  acc5: 77.7778 (44.9724)  time: 0.2795  data: 0.1002  max mem: 15572
Val: Total time: 0:01:31 (0.3373 s / it)
* Acc@1 21.565 Acc@5 44.972 loss 3.678
Accuracy of the network on the 4883 val videos: 21.6%
[2025-01-15 17:17:25,135] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 17:17:25,139] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 17:17:25,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 17:17:28,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 17:17:28,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.56%
Epoch: [6]  [   0/2809]  eta: 6:49:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3507 (4.3507)  class_acc: 0.0833 (0.0833)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7442  data: 8.1513  max mem: 15572
Epoch: [6]  [  10/2809]  eta: 1:08:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3507 (4.4300)  class_acc: 0.1667 (0.1629)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4657  data: 0.9459  max mem: 15572
Epoch: [6]  [  20/2809]  eta: 0:52:12  lr: 0.000047  min_lr: 0.000000  loss: 4.3904 (4.4289)  class_acc: 0.1250 (0.1429)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7422  data: 0.2627  max mem: 15572
Epoch: [6]  [  30/2809]  eta: 0:44:53  lr: 0.000047  min_lr: 0.000000  loss: 4.4350 (4.4452)  class_acc: 0.0833 (0.1452)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6964  data: 0.2417  max mem: 15572
Epoch: [6]  [  40/2809]  eta: 0:38:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4107 (4.4164)  class_acc: 0.0833 (0.1463)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0918  max mem: 15572
Epoch: [6]  [  50/2809]  eta: 0:34:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4619 (4.4302)  class_acc: 0.0833 (0.1438)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4238  data: 0.0006  max mem: 15572
Epoch: [6]  [  60/2809]  eta: 0:32:13  lr: 0.000047  min_lr: 0.000000  loss: 4.4604 (4.4289)  class_acc: 0.1250 (0.1448)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4472  data: 0.0009  max mem: 15572
Epoch: [6]  [  70/2809]  eta: 0:30:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3393 (4.4209)  class_acc: 0.1667 (0.1479)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4518  data: 0.0008  max mem: 15572
Epoch: [6]  [  80/2809]  eta: 0:29:33  lr: 0.000047  min_lr: 0.000000  loss: 4.3829 (4.4248)  class_acc: 0.1250 (0.1471)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4866  data: 0.0237  max mem: 15572
Epoch: [6]  [  90/2809]  eta: 0:28:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4421 (4.4288)  class_acc: 0.1250 (0.1433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0820  max mem: 15572
Epoch: [6]  [ 100/2809]  eta: 0:28:42  lr: 0.000047  min_lr: 0.000000  loss: 4.5367 (4.4304)  class_acc: 0.1250 (0.1419)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1484  max mem: 15572
Epoch: [6]  [ 110/2809]  eta: 0:28:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4411 (4.4114)  class_acc: 0.1250 (0.1441)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1568  max mem: 15572
[2025-01-15 17:18:39,800] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:18:39,800] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 120/2809]  eta: 0:28:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4388 (4.4121)  class_acc: 0.1250 (0.1439)  loss_scale: 65536.0000 (70410.5785)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1369  max mem: 15572
Epoch: [6]  [ 130/2809]  eta: 0:27:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3963 (4.4017)  class_acc: 0.1250 (0.1425)  loss_scale: 131072.0000 (75041.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1303  max mem: 15572
[2025-01-15 17:18:51,526] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16988
[2025-01-15 17:18:51,526] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:18:51,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 140/2809]  eta: 0:27:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3108 (4.4007)  class_acc: 0.1250 (0.1439)  loss_scale: 131072.0000 (75761.4752)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1770  max mem: 15572
[2025-01-15 17:18:59,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=105, lr=[4.531573325722194e-07, 4.531573325722194e-07, 6.473676179603135e-07, 6.473676179603135e-07, 9.248108828004479e-07, 9.248108828004479e-07, 1.32115840400064e-06, 1.32115840400064e-06, 1.887369148572343e-06, 1.887369148572343e-06, 2.696241640817633e-06, 2.696241640817633e-06, 3.851773772596618e-06, 3.851773772596618e-06, 5.502533960852313e-06, 5.502533960852313e-06, 7.86076280121759e-06, 7.86076280121759e-06, 1.1229661144596558e-05, 1.1229661144596558e-05, 1.6042373063709367e-05, 1.6042373063709367e-05, 2.29176758052991e-05, 2.29176758052991e-05, 3.2739536864713004e-05, 3.2739536864713004e-05, 4.6770766949590006e-05, 4.6770766949590006e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:18:59,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=28.427926228435116, CurrSamplesPerSec=31.422577690762765, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [ 150/2809]  eta: 0:27:20  lr: 0.000047  min_lr: 0.000000  loss: 4.2997 (4.3949)  class_acc: 0.1667 (0.1462)  loss_scale: 65536.0000 (75084.2914)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1606  max mem: 15572
Epoch: [6]  [ 160/2809]  eta: 0:27:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3366 (4.3890)  class_acc: 0.1250 (0.1465)  loss_scale: 65536.0000 (74491.2298)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1370  max mem: 15572
Epoch: [6]  [ 170/2809]  eta: 0:27:00  lr: 0.000047  min_lr: 0.000000  loss: 4.4205 (4.3899)  class_acc: 0.1250 (0.1462)  loss_scale: 65536.0000 (73967.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1467  max mem: 15572
Epoch: [6]  [ 180/2809]  eta: 0:26:45  lr: 0.000047  min_lr: 0.000000  loss: 4.2998 (4.3823)  class_acc: 0.1250 (0.1473)  loss_scale: 65536.0000 (73501.7017)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1248  max mem: 15572
Epoch: [6]  [ 190/2809]  eta: 0:26:37  lr: 0.000047  min_lr: 0.000000  loss: 4.2998 (4.3829)  class_acc: 0.1667 (0.1470)  loss_scale: 65536.0000 (73084.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1435  max mem: 15572
Epoch: [6]  [ 200/2809]  eta: 0:26:23  lr: 0.000047  min_lr: 0.000000  loss: 4.3475 (4.3805)  class_acc: 0.1667 (0.1468)  loss_scale: 65536.0000 (72709.0945)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1215  max mem: 15572
Epoch: [6]  [ 210/2809]  eta: 0:26:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3175 (4.3812)  class_acc: 0.1250 (0.1447)  loss_scale: 65536.0000 (72369.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1521  max mem: 15572
Epoch: [6]  [ 220/2809]  eta: 0:26:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3175 (4.3781)  class_acc: 0.1250 (0.1440)  loss_scale: 65536.0000 (72059.9457)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.2002  max mem: 15572
Epoch: [6]  [ 230/2809]  eta: 0:26:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3748 (4.3824)  class_acc: 0.1250 (0.1421)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1509  max mem: 15572
Epoch: [6]  [ 240/2809]  eta: 0:25:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3356 (4.3798)  class_acc: 0.1250 (0.1413)  loss_scale: 65536.0000 (71518.5394)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1215  max mem: 15572
[2025-01-15 17:19:56,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17097
[2025-01-15 17:19:56,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:19:56,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 250/2809]  eta: 0:25:48  lr: 0.000047  min_lr: 0.000000  loss: 4.2851 (4.3812)  class_acc: 0.1250 (0.1414)  loss_scale: 65536.0000 (70235.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1325  max mem: 15572
Epoch: [6]  [ 260/2809]  eta: 0:25:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3667 (4.3838)  class_acc: 0.0833 (0.1406)  loss_scale: 32768.0000 (68800.2452)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1503  max mem: 15572
Epoch: [6]  [ 270/2809]  eta: 0:25:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4870 (4.3871)  class_acc: 0.0833 (0.1407)  loss_scale: 32768.0000 (67470.6421)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1553  max mem: 15572
Epoch: [6]  [ 280/2809]  eta: 0:25:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4880 (4.3909)  class_acc: 0.0833 (0.1392)  loss_scale: 32768.0000 (66235.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.1087  max mem: 15572
Epoch: [6]  [ 290/2809]  eta: 0:25:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3983 (4.3882)  class_acc: 0.1250 (0.1406)  loss_scale: 32768.0000 (65085.5808)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1764  max mem: 15572
Epoch: [6]  [ 300/2809]  eta: 0:25:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3755 (4.3910)  class_acc: 0.1250 (0.1393)  loss_scale: 32768.0000 (64011.9070)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.2310  max mem: 15572
Epoch: [6]  [ 310/2809]  eta: 0:25:12  lr: 0.000047  min_lr: 0.000000  loss: 4.4038 (4.3877)  class_acc: 0.1250 (0.1405)  loss_scale: 32768.0000 (63007.2797)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.1943  max mem: 15572
Epoch: [6]  [ 320/2809]  eta: 0:25:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3076 (4.3845)  class_acc: 0.2083 (0.1421)  loss_scale: 32768.0000 (62065.2461)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1570  max mem: 15572
Epoch: [6]  [ 330/2809]  eta: 0:24:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3467 (4.3847)  class_acc: 0.1667 (0.1422)  loss_scale: 32768.0000 (61180.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1255  max mem: 15572
Epoch: [6]  [ 340/2809]  eta: 0:24:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3819)  class_acc: 0.1250 (0.1432)  loss_scale: 32768.0000 (60346.9326)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1189  max mem: 15572
Epoch: [6]  [ 350/2809]  eta: 0:24:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3846)  class_acc: 0.1250 (0.1429)  loss_scale: 32768.0000 (59561.2080)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1308  max mem: 15572
Epoch: [6]  [ 360/2809]  eta: 0:24:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4060 (4.3855)  class_acc: 0.1667 (0.1436)  loss_scale: 32768.0000 (58819.0139)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1491  max mem: 15572
Epoch: [6]  [ 370/2809]  eta: 0:24:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3977 (4.3830)  class_acc: 0.1667 (0.1440)  loss_scale: 32768.0000 (58116.8302)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1529  max mem: 15572
[2025-01-15 17:21:14,422] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:21:14,423] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 380/2809]  eta: 0:24:21  lr: 0.000047  min_lr: 0.000000  loss: 4.3832 (4.3825)  class_acc: 0.1250 (0.1440)  loss_scale: 32768.0000 (58225.5538)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1689  max mem: 15572
Epoch: [6]  [ 390/2809]  eta: 0:24:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3150 (4.3811)  class_acc: 0.1250 (0.1443)  loss_scale: 65536.0000 (58412.5217)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1660  max mem: 15572
Epoch: [6]  [ 400/2809]  eta: 0:24:13  lr: 0.000047  min_lr: 0.000000  loss: 4.2864 (4.3788)  class_acc: 0.1250 (0.1447)  loss_scale: 65536.0000 (58590.1646)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1515  max mem: 15572
Epoch: [6]  [ 410/2809]  eta: 0:24:01  lr: 0.000047  min_lr: 0.000000  loss: 4.2708 (4.3780)  class_acc: 0.1667 (0.1455)  loss_scale: 65536.0000 (58759.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1067  max mem: 15572
Epoch: [6]  [ 420/2809]  eta: 0:23:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2344 (4.3751)  class_acc: 0.1667 (0.1460)  loss_scale: 65536.0000 (58920.1330)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0726  max mem: 15572
Epoch: [6]  [ 430/2809]  eta: 0:23:45  lr: 0.000047  min_lr: 0.000000  loss: 4.1454 (4.3742)  class_acc: 0.2083 (0.1467)  loss_scale: 65536.0000 (59073.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1113  max mem: 15572
Epoch: [6]  [ 440/2809]  eta: 0:23:43  lr: 0.000047  min_lr: 0.000000  loss: 4.2595 (4.3729)  class_acc: 0.2083 (0.1472)  loss_scale: 65536.0000 (59220.1723)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1808  max mem: 15572
Epoch: [6]  [ 450/2809]  eta: 0:23:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3438 (4.3731)  class_acc: 0.1667 (0.1475)  loss_scale: 65536.0000 (59360.2129)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1533  max mem: 15572
Epoch: [6]  [ 460/2809]  eta: 0:23:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4735 (4.3757)  class_acc: 0.1250 (0.1479)  loss_scale: 65536.0000 (59494.1779)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1242  max mem: 15572
Epoch: [6]  [ 470/2809]  eta: 0:23:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4901 (4.3741)  class_acc: 0.1667 (0.1489)  loss_scale: 65536.0000 (59622.4544)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1566  max mem: 15572
Epoch: [6]  [ 480/2809]  eta: 0:23:14  lr: 0.000047  min_lr: 0.000000  loss: 4.4013 (4.3760)  class_acc: 0.1250 (0.1480)  loss_scale: 65536.0000 (59745.3971)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1357  max mem: 15572
Epoch: [6]  [ 490/2809]  eta: 0:23:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4013 (4.3746)  class_acc: 0.1250 (0.1489)  loss_scale: 65536.0000 (59863.3320)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0737  max mem: 15572
[2025-01-15 17:22:28,988] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:22:28,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 500/2809]  eta: 0:23:03  lr: 0.000047  min_lr: 0.000000  loss: 4.3968 (4.3744)  class_acc: 0.1667 (0.1496)  loss_scale: 65536.0000 (60107.3693)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1560  max mem: 15572
[2025-01-15 17:22:31,669] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17360
[2025-01-15 17:22:31,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:22:31,670] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 510/2809]  eta: 0:22:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4147 (4.3734)  class_acc: 0.1667 (0.1495)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1870  max mem: 15572
Epoch: [6]  [ 520/2809]  eta: 0:22:50  lr: 0.000047  min_lr: 0.000000  loss: 4.2767 (4.3723)  class_acc: 0.1667 (0.1498)  loss_scale: 65536.0000 (60944.7063)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1481  max mem: 15572
Epoch: [6]  [ 530/2809]  eta: 0:22:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2564 (4.3699)  class_acc: 0.1250 (0.1496)  loss_scale: 65536.0000 (61031.1714)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1267  max mem: 15572
Epoch: [6]  [ 540/2809]  eta: 0:22:32  lr: 0.000047  min_lr: 0.000000  loss: 4.3015 (4.3688)  class_acc: 0.1250 (0.1498)  loss_scale: 65536.0000 (61114.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0713  max mem: 15572
Epoch: [6]  [ 550/2809]  eta: 0:22:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3583 (4.3672)  class_acc: 0.1667 (0.1506)  loss_scale: 65536.0000 (61194.6860)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1290  max mem: 15572
Epoch: [6]  [ 560/2809]  eta: 0:22:18  lr: 0.000047  min_lr: 0.000000  loss: 4.3529 (4.3675)  class_acc: 0.1250 (0.1502)  loss_scale: 65536.0000 (61272.0713)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1332  max mem: 15572
Epoch: [6]  [ 570/2809]  eta: 0:22:16  lr: 0.000047  min_lr: 0.000000  loss: 4.4326 (4.3687)  class_acc: 0.1250 (0.1500)  loss_scale: 65536.0000 (61346.7461)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.1637  max mem: 15572
Epoch: [6]  [ 580/2809]  eta: 0:22:08  lr: 0.000047  min_lr: 0.000000  loss: 4.4511 (4.3692)  class_acc: 0.0833 (0.1492)  loss_scale: 65536.0000 (61418.8503)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1656  max mem: 15572
Epoch: [6]  [ 590/2809]  eta: 0:22:01  lr: 0.000047  min_lr: 0.000000  loss: 4.4160 (4.3689)  class_acc: 0.0833 (0.1490)  loss_scale: 65536.0000 (61488.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1078  max mem: 15572
Epoch: [6]  [ 600/2809]  eta: 0:21:56  lr: 0.000047  min_lr: 0.000000  loss: 4.3863 (4.3681)  class_acc: 0.1250 (0.1492)  loss_scale: 65536.0000 (61555.8602)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1621  max mem: 15572
Epoch: [6]  [ 610/2809]  eta: 0:21:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3863 (4.3682)  class_acc: 0.1250 (0.1491)  loss_scale: 65536.0000 (61621.0016)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1467  max mem: 15572
Epoch: [6]  [ 620/2809]  eta: 0:21:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3900 (4.3690)  class_acc: 0.1250 (0.1494)  loss_scale: 65536.0000 (61684.0451)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0647  max mem: 15572
Epoch: [6]  [ 630/2809]  eta: 0:21:33  lr: 0.000047  min_lr: 0.000000  loss: 4.4506 (4.3695)  class_acc: 0.1667 (0.1498)  loss_scale: 65536.0000 (61745.0903)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1026  max mem: 15572
[2025-01-15 17:23:46,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:23:46,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:23:47,481] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17491
[2025-01-15 17:23:47,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:23:47,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 640/2809]  eta: 0:21:25  lr: 0.000047  min_lr: 0.000000  loss: 4.4179 (4.3677)  class_acc: 0.1667 (0.1503)  loss_scale: 65536.0000 (62008.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1236  max mem: 15572
Epoch: [6]  [ 650/2809]  eta: 0:21:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3335 (4.3665)  class_acc: 0.2083 (0.1506)  loss_scale: 65536.0000 (62062.8940)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1173  max mem: 15572
Epoch: [6]  [ 660/2809]  eta: 0:21:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3499 (4.3694)  class_acc: 0.0833 (0.1496)  loss_scale: 65536.0000 (62115.4372)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1572  max mem: 15572
Epoch: [6]  [ 670/2809]  eta: 0:21:09  lr: 0.000047  min_lr: 0.000000  loss: 4.5955 (4.3705)  class_acc: 0.0833 (0.1495)  loss_scale: 65536.0000 (62166.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1645  max mem: 15572
Epoch: [6]  [ 680/2809]  eta: 0:21:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4791 (4.3694)  class_acc: 0.1250 (0.1497)  loss_scale: 65536.0000 (62215.8943)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1696  max mem: 15572
Epoch: [6]  [ 690/2809]  eta: 0:20:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3633 (4.3676)  class_acc: 0.1667 (0.1503)  loss_scale: 65536.0000 (62263.9421)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1667  max mem: 15572
Epoch: [6]  [ 700/2809]  eta: 0:20:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3098 (4.3646)  class_acc: 0.1667 (0.1510)  loss_scale: 65536.0000 (62310.6191)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1469  max mem: 15572
Epoch: [6]  [ 710/2809]  eta: 0:20:45  lr: 0.000047  min_lr: 0.000000  loss: 4.2121 (4.3627)  class_acc: 0.1667 (0.1518)  loss_scale: 65536.0000 (62355.9831)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1323  max mem: 15572
Epoch: [6]  [ 720/2809]  eta: 0:20:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2236 (4.3631)  class_acc: 0.1667 (0.1517)  loss_scale: 65536.0000 (62400.0888)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1546  max mem: 15572
Epoch: [6]  [ 730/2809]  eta: 0:20:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3422 (4.3630)  class_acc: 0.1250 (0.1521)  loss_scale: 65536.0000 (62442.9877)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1517  max mem: 15572
Epoch: [6]  [ 740/2809]  eta: 0:20:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3422 (4.3632)  class_acc: 0.1667 (0.1520)  loss_scale: 65536.0000 (62484.7287)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1068  max mem: 15572
Epoch: [6]  [ 750/2809]  eta: 0:20:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4185 (4.3640)  class_acc: 0.1667 (0.1520)  loss_scale: 65536.0000 (62525.3582)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1247  max mem: 15572
Epoch: [6]  [ 760/2809]  eta: 0:20:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3329 (4.3634)  class_acc: 0.1667 (0.1523)  loss_scale: 65536.0000 (62564.9198)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1971  max mem: 15572
[2025-01-15 17:25:03,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:25:03,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 770/2809]  eta: 0:20:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4052 (4.3643)  class_acc: 0.1250 (0.1521)  loss_scale: 65536.0000 (63028.4617)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.2110  max mem: 15572
[2025-01-15 17:25:07,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17625
[2025-01-15 17:25:07,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:25:07,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 780/2809]  eta: 0:20:06  lr: 0.000047  min_lr: 0.000000  loss: 4.4052 (4.3630)  class_acc: 0.1667 (0.1525)  loss_scale: 65536.0000 (63060.5685)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1678  max mem: 15572
Epoch: [6]  [ 790/2809]  eta: 0:19:59  lr: 0.000047  min_lr: 0.000000  loss: 4.2894 (4.3630)  class_acc: 0.1667 (0.1521)  loss_scale: 65536.0000 (63091.8635)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1447  max mem: 15572
Epoch: [6]  [ 800/2809]  eta: 0:19:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2077 (4.3609)  class_acc: 0.1667 (0.1525)  loss_scale: 65536.0000 (63122.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1169  max mem: 15572
Epoch: [6]  [ 810/2809]  eta: 0:19:45  lr: 0.000047  min_lr: 0.000000  loss: 4.2128 (4.3611)  class_acc: 0.1250 (0.1523)  loss_scale: 65536.0000 (63152.1381)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1113  max mem: 15572
Epoch: [6]  [ 820/2809]  eta: 0:19:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3832 (4.3630)  class_acc: 0.1250 (0.1519)  loss_scale: 65536.0000 (63181.1742)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.1059  max mem: 15572
Epoch: [6]  [ 830/2809]  eta: 0:19:35  lr: 0.000047  min_lr: 0.000000  loss: 4.3130 (4.3600)  class_acc: 0.1667 (0.1526)  loss_scale: 65536.0000 (63209.5114)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1786  max mem: 15572
Epoch: [6]  [ 840/2809]  eta: 0:19:27  lr: 0.000047  min_lr: 0.000000  loss: 4.1031 (4.3578)  class_acc: 0.1667 (0.1531)  loss_scale: 65536.0000 (63237.1748)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1781  max mem: 15572
Epoch: [6]  [ 850/2809]  eta: 0:19:21  lr: 0.000047  min_lr: 0.000000  loss: 4.3720 (4.3588)  class_acc: 0.1667 (0.1535)  loss_scale: 65536.0000 (63264.1880)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1118  max mem: 15572
Epoch: [6]  [ 860/2809]  eta: 0:19:16  lr: 0.000047  min_lr: 0.000000  loss: 4.2956 (4.3570)  class_acc: 0.2083 (0.1539)  loss_scale: 65536.0000 (63290.5738)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1403  max mem: 15572
Epoch: [6]  [ 870/2809]  eta: 0:19:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2458 (4.3585)  class_acc: 0.1250 (0.1534)  loss_scale: 65536.0000 (63316.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1201  max mem: 15572
Epoch: [6]  [ 880/2809]  eta: 0:19:04  lr: 0.000047  min_lr: 0.000000  loss: 4.3804 (4.3590)  class_acc: 0.0833 (0.1532)  loss_scale: 65536.0000 (63341.5482)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1154  max mem: 15572
Epoch: [6]  [ 890/2809]  eta: 0:18:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4541 (4.3603)  class_acc: 0.1250 (0.1530)  loss_scale: 65536.0000 (63366.1773)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0937  max mem: 15572
[2025-01-15 17:26:22,369] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:26:22,369] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [ 900/2809]  eta: 0:18:50  lr: 0.000047  min_lr: 0.000000  loss: 4.4723 (4.3619)  class_acc: 0.0833 (0.1527)  loss_scale: 65536.0000 (63462.9967)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.1061  max mem: 15572
[2025-01-15 17:26:27,988] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17762
[2025-01-15 17:26:27,989] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:26:27,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [ 910/2809]  eta: 0:18:45  lr: 0.000047  min_lr: 0.000000  loss: 4.4422 (4.3615)  class_acc: 0.1250 (0.1528)  loss_scale: 65536.0000 (63989.3216)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1823  max mem: 15572
Epoch: [6]  [ 920/2809]  eta: 0:18:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4276 (4.3630)  class_acc: 0.1250 (0.1525)  loss_scale: 65536.0000 (64006.1151)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1566  max mem: 15572
Epoch: [6]  [ 930/2809]  eta: 0:18:31  lr: 0.000047  min_lr: 0.000000  loss: 4.4099 (4.3621)  class_acc: 0.1250 (0.1528)  loss_scale: 65536.0000 (64022.5478)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0962  max mem: 15572
[2025-01-15 17:26:41,941] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17787
[2025-01-15 17:26:41,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:26:41,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 940/2809]  eta: 0:18:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3708 (4.3625)  class_acc: 0.1250 (0.1528)  loss_scale: 65536.0000 (63760.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0920  max mem: 15572
Epoch: [6]  [ 950/2809]  eta: 0:18:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3708 (4.3620)  class_acc: 0.1667 (0.1533)  loss_scale: 32768.0000 (63434.1619)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0690  max mem: 15572
Epoch: [6]  [ 960/2809]  eta: 0:18:11  lr: 0.000047  min_lr: 0.000000  loss: 4.3004 (4.3617)  class_acc: 0.1667 (0.1533)  loss_scale: 32768.0000 (63115.0552)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1203  max mem: 15572
Epoch: [6]  [ 970/2809]  eta: 0:18:06  lr: 0.000047  min_lr: 0.000000  loss: 4.2553 (4.3603)  class_acc: 0.1667 (0.1534)  loss_scale: 32768.0000 (62802.5211)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1872  max mem: 15572
Epoch: [6]  [ 980/2809]  eta: 0:18:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3312 (4.3608)  class_acc: 0.1250 (0.1532)  loss_scale: 32768.0000 (62496.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.1829  max mem: 15572
Epoch: [6]  [ 990/2809]  eta: 0:17:54  lr: 0.000047  min_lr: 0.000000  loss: 4.4201 (4.3606)  class_acc: 0.1250 (0.1532)  loss_scale: 32768.0000 (62196.3754)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1305  max mem: 15572
Epoch: [6]  [1000/2809]  eta: 0:17:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3703 (4.3600)  class_acc: 0.1667 (0.1533)  loss_scale: 32768.0000 (61902.3856)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0905  max mem: 15572
Epoch: [6]  [1010/2809]  eta: 0:17:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3855 (4.3607)  class_acc: 0.1250 (0.1529)  loss_scale: 32768.0000 (61614.2117)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1046  max mem: 15572
Epoch: [6]  [1020/2809]  eta: 0:17:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4086 (4.3611)  class_acc: 0.0833 (0.1527)  loss_scale: 32768.0000 (61331.6827)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0733  max mem: 15572
Epoch: [6]  [1030/2809]  eta: 0:17:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3229 (4.3600)  class_acc: 0.1667 (0.1529)  loss_scale: 32768.0000 (61054.6343)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0896  max mem: 15572
Epoch: [6]  [1040/2809]  eta: 0:17:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3437 (4.3596)  class_acc: 0.1667 (0.1529)  loss_scale: 32768.0000 (60782.9087)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1385  max mem: 15572
Epoch: [6]  [1050/2809]  eta: 0:17:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3437 (4.3600)  class_acc: 0.1250 (0.1529)  loss_scale: 32768.0000 (60516.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1515  max mem: 15572
Epoch: [6]  [1060/2809]  eta: 0:17:09  lr: 0.000047  min_lr: 0.000000  loss: 4.3126 (4.3604)  class_acc: 0.1250 (0.1529)  loss_scale: 32768.0000 (60254.8238)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1229  max mem: 15572
[2025-01-15 17:27:56,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:27:56,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1070/2809]  eta: 0:17:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4021 (4.3620)  class_acc: 0.1667 (0.1531)  loss_scale: 32768.0000 (60273.5387)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1259  max mem: 15572
Epoch: [6]  [1080/2809]  eta: 0:16:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4021 (4.3622)  class_acc: 0.1250 (0.1524)  loss_scale: 65536.0000 (60322.2202)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1722  max mem: 15572
Epoch: [6]  [1090/2809]  eta: 0:16:52  lr: 0.000047  min_lr: 0.000000  loss: 4.5338 (4.3643)  class_acc: 0.0833 (0.1519)  loss_scale: 65536.0000 (60370.0092)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1666  max mem: 15572
Epoch: [6]  [1100/2809]  eta: 0:16:47  lr: 0.000047  min_lr: 0.000000  loss: 4.2751 (4.3615)  class_acc: 0.1250 (0.1523)  loss_scale: 65536.0000 (60416.9301)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1908  max mem: 15572
Epoch: [6]  [1110/2809]  eta: 0:16:41  lr: 0.000047  min_lr: 0.000000  loss: 4.2339 (4.3605)  class_acc: 0.1250 (0.1523)  loss_scale: 65536.0000 (60463.0063)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1712  max mem: 15572
Epoch: [6]  [1120/2809]  eta: 0:16:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3692 (4.3617)  class_acc: 0.1250 (0.1518)  loss_scale: 65536.0000 (60508.2605)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1413  max mem: 15572
Epoch: [6]  [1130/2809]  eta: 0:16:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3597 (4.3594)  class_acc: 0.1250 (0.1527)  loss_scale: 65536.0000 (60552.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1118  max mem: 15572
Epoch: [6]  [1140/2809]  eta: 0:16:24  lr: 0.000047  min_lr: 0.000000  loss: 4.2465 (4.3572)  class_acc: 0.2500 (0.1533)  loss_scale: 65536.0000 (60596.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1137  max mem: 15572
[2025-01-15 17:28:44,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=111, lr=[4.523589092123934e-07, 4.523589092123934e-07, 6.462270131605621e-07, 6.462270131605621e-07, 9.231814473722315e-07, 9.231814473722315e-07, 1.318830639103188e-06, 1.318830639103188e-06, 1.8840437701474115e-06, 1.8840437701474115e-06, 2.691491100210588e-06, 2.691491100210588e-06, 3.844987286015126e-06, 3.844987286015126e-06, 5.492838980021609e-06, 5.492838980021609e-06, 7.846912828602299e-06, 7.846912828602299e-06, 1.1209875469431857e-05, 1.1209875469431857e-05, 1.601410781347408e-05, 1.601410781347408e-05, 2.2877296876391547e-05, 2.2877296876391547e-05, 3.268185268055935e-05, 3.268185268055935e-05, 4.668836097222765e-05, 4.668836097222765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:28:44,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=28.433719606289674, CurrSamplesPerSec=24.041304024887882, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [1150/2809]  eta: 0:16:17  lr: 0.000047  min_lr: 0.000000  loss: 4.4133 (4.3579)  class_acc: 0.1250 (0.1527)  loss_scale: 65536.0000 (60639.3050)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1087  max mem: 15572
Epoch: [6]  [1160/2809]  eta: 0:16:11  lr: 0.000047  min_lr: 0.000000  loss: 4.4039 (4.3584)  class_acc: 0.1250 (0.1526)  loss_scale: 65536.0000 (60681.4815)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0752  max mem: 15572
Epoch: [6]  [1170/2809]  eta: 0:16:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4039 (4.3596)  class_acc: 0.1250 (0.1527)  loss_scale: 65536.0000 (60722.9377)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1022  max mem: 15572
Epoch: [6]  [1180/2809]  eta: 0:15:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4267 (4.3589)  class_acc: 0.1667 (0.1532)  loss_scale: 65536.0000 (60763.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1210  max mem: 15572
[2025-01-15 17:29:09,866] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:29:09,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1190/2809]  eta: 0:15:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2995 (4.3586)  class_acc: 0.1667 (0.1532)  loss_scale: 65536.0000 (60858.7876)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1466  max mem: 15572
[2025-01-15 17:29:16,732] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18053
[2025-01-15 17:29:16,732] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:29:16,732] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1200/2809]  eta: 0:15:48  lr: 0.000047  min_lr: 0.000000  loss: 4.2995 (4.3580)  class_acc: 0.1250 (0.1530)  loss_scale: 65536.0000 (61334.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.2180  max mem: 15572
Epoch: [6]  [1210/2809]  eta: 0:15:42  lr: 0.000047  min_lr: 0.000000  loss: 4.3138 (4.3581)  class_acc: 0.0833 (0.1527)  loss_scale: 65536.0000 (61368.9711)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.2066  max mem: 15572
Epoch: [6]  [1220/2809]  eta: 0:15:37  lr: 0.000047  min_lr: 0.000000  loss: 4.3619 (4.3579)  class_acc: 0.1250 (0.1527)  loss_scale: 65536.0000 (61403.0991)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1445  max mem: 15572
Epoch: [6]  [1230/2809]  eta: 0:15:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3809 (4.3588)  class_acc: 0.1250 (0.1523)  loss_scale: 65536.0000 (61436.6726)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1726  max mem: 15572
Epoch: [6]  [1240/2809]  eta: 0:15:26  lr: 0.000047  min_lr: 0.000000  loss: 4.4432 (4.3593)  class_acc: 0.1250 (0.1523)  loss_scale: 65536.0000 (61469.7051)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1748  max mem: 15572
Epoch: [6]  [1250/2809]  eta: 0:15:20  lr: 0.000047  min_lr: 0.000000  loss: 4.4432 (4.3606)  class_acc: 0.1250 (0.1522)  loss_scale: 65536.0000 (61502.2094)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1742  max mem: 15572
Epoch: [6]  [1260/2809]  eta: 0:15:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3360 (4.3598)  class_acc: 0.1250 (0.1521)  loss_scale: 65536.0000 (61534.1983)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1325  max mem: 15572
Epoch: [6]  [1270/2809]  eta: 0:15:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3088 (4.3593)  class_acc: 0.1667 (0.1524)  loss_scale: 65536.0000 (61565.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1181  max mem: 15572
Epoch: [6]  [1280/2809]  eta: 0:15:02  lr: 0.000047  min_lr: 0.000000  loss: 4.3904 (4.3596)  class_acc: 0.1667 (0.1522)  loss_scale: 65536.0000 (61596.6776)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1497  max mem: 15572
Epoch: [6]  [1290/2809]  eta: 0:14:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3068 (4.3585)  class_acc: 0.1667 (0.1524)  loss_scale: 65536.0000 (61627.1913)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1417  max mem: 15572
Epoch: [6]  [1300/2809]  eta: 0:14:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3068 (4.3582)  class_acc: 0.2083 (0.1527)  loss_scale: 65536.0000 (61657.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1207  max mem: 15572
Epoch: [6]  [1310/2809]  eta: 0:14:43  lr: 0.000047  min_lr: 0.000000  loss: 4.2989 (4.3572)  class_acc: 0.1667 (0.1527)  loss_scale: 65536.0000 (61686.8223)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1068  max mem: 15572
Epoch: [6]  [1320/2809]  eta: 0:14:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3661 (4.3579)  class_acc: 0.1667 (0.1529)  loss_scale: 65536.0000 (61715.9606)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0970  max mem: 15572
[2025-01-15 17:30:30,585] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:30:30,585] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1330/2809]  eta: 0:14:30  lr: 0.000047  min_lr: 0.000000  loss: 4.4244 (4.3584)  class_acc: 0.1667 (0.1530)  loss_scale: 65536.0000 (61892.3757)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0803  max mem: 15572
Epoch: [6]  [1340/2809]  eta: 0:14:23  lr: 0.000047  min_lr: 0.000000  loss: 4.2592 (4.3580)  class_acc: 0.1250 (0.1528)  loss_scale: 131072.0000 (62408.2565)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0753  max mem: 15572
Epoch: [6]  [1350/2809]  eta: 0:14:17  lr: 0.000047  min_lr: 0.000000  loss: 4.2592 (4.3574)  class_acc: 0.1250 (0.1529)  loss_scale: 131072.0000 (62916.5004)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0991  max mem: 15572
Epoch: [6]  [1360/2809]  eta: 0:14:11  lr: 0.000047  min_lr: 0.000000  loss: 4.3517 (4.3577)  class_acc: 0.1667 (0.1529)  loss_scale: 131072.0000 (63417.2755)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1270  max mem: 15572
[2025-01-15 17:30:49,583] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18215
[2025-01-15 17:30:49,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:30:49,584] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1370/2809]  eta: 0:14:06  lr: 0.000047  min_lr: 0.000000  loss: 4.3595 (4.3583)  class_acc: 0.1667 (0.1527)  loss_scale: 65536.0000 (63432.7294)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1507  max mem: 15572
Epoch: [6]  [1380/2809]  eta: 0:14:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3235 (4.3582)  class_acc: 0.1667 (0.1531)  loss_scale: 65536.0000 (63447.9594)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1404  max mem: 15572
Epoch: [6]  [1390/2809]  eta: 0:13:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3846 (4.3590)  class_acc: 0.1250 (0.1529)  loss_scale: 65536.0000 (63462.9705)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1710  max mem: 15572
Epoch: [6]  [1400/2809]  eta: 0:13:50  lr: 0.000047  min_lr: 0.000000  loss: 4.5046 (4.3604)  class_acc: 0.1250 (0.1526)  loss_scale: 65536.0000 (63477.7673)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.2275  max mem: 15572
Epoch: [6]  [1410/2809]  eta: 0:13:43  lr: 0.000047  min_lr: 0.000000  loss: 4.5271 (4.3619)  class_acc: 0.0833 (0.1523)  loss_scale: 65536.0000 (63492.3544)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1576  max mem: 15572
Epoch: [6]  [1420/2809]  eta: 0:13:37  lr: 0.000047  min_lr: 0.000000  loss: 4.5080 (4.3633)  class_acc: 0.1250 (0.1524)  loss_scale: 65536.0000 (63506.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1322  max mem: 15572
Epoch: [6]  [1430/2809]  eta: 0:13:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3949 (4.3631)  class_acc: 0.1667 (0.1527)  loss_scale: 65536.0000 (63520.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1275  max mem: 15572
Epoch: [6]  [1440/2809]  eta: 0:13:25  lr: 0.000047  min_lr: 0.000000  loss: 4.1870 (4.3620)  class_acc: 0.2083 (0.1530)  loss_scale: 65536.0000 (63534.9008)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1039  max mem: 15572
Epoch: [6]  [1450/2809]  eta: 0:13:19  lr: 0.000047  min_lr: 0.000000  loss: 4.1870 (4.3622)  class_acc: 0.2083 (0.1531)  loss_scale: 65536.0000 (63548.6919)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1381  max mem: 15572
Epoch: [6]  [1460/2809]  eta: 0:13:14  lr: 0.000047  min_lr: 0.000000  loss: 4.2301 (4.3615)  class_acc: 0.1667 (0.1533)  loss_scale: 65536.0000 (63562.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1865  max mem: 15572
Epoch: [6]  [1470/2809]  eta: 0:13:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3711 (4.3627)  class_acc: 0.1250 (0.1530)  loss_scale: 65536.0000 (63575.7118)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.1928  max mem: 15572
Epoch: [6]  [1480/2809]  eta: 0:13:02  lr: 0.000047  min_lr: 0.000000  loss: 4.5075 (4.3629)  class_acc: 0.0833 (0.1530)  loss_scale: 65536.0000 (63588.9480)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1145  max mem: 15572
[2025-01-15 17:32:05,878] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:32:05,878] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1490/2809]  eta: 0:12:55  lr: 0.000047  min_lr: 0.000000  loss: 4.3597 (4.3625)  class_acc: 0.1250 (0.1530)  loss_scale: 65536.0000 (63645.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0721  max mem: 15572
Epoch: [6]  [1500/2809]  eta: 0:12:49  lr: 0.000047  min_lr: 0.000000  loss: 4.2203 (4.3622)  class_acc: 0.1667 (0.1533)  loss_scale: 131072.0000 (64095.1686)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1155  max mem: 15572
Epoch: [6]  [1510/2809]  eta: 0:12:43  lr: 0.000047  min_lr: 0.000000  loss: 4.1781 (4.3618)  class_acc: 0.2083 (0.1537)  loss_scale: 131072.0000 (64538.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1325  max mem: 15572
[2025-01-15 17:32:20,918] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18368
[2025-01-15 17:32:20,919] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:32:20,919] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1520/2809]  eta: 0:12:38  lr: 0.000047  min_lr: 0.000000  loss: 4.3293 (4.3619)  class_acc: 0.1667 (0.1537)  loss_scale: 131072.0000 (64674.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1453  max mem: 15572
Epoch: [6]  [1530/2809]  eta: 0:12:32  lr: 0.000047  min_lr: 0.000000  loss: 4.4082 (4.3631)  class_acc: 0.1250 (0.1534)  loss_scale: 65536.0000 (64679.8798)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1504  max mem: 15572
Epoch: [6]  [1540/2809]  eta: 0:12:25  lr: 0.000047  min_lr: 0.000000  loss: 4.5356 (4.3634)  class_acc: 0.0833 (0.1533)  loss_scale: 65536.0000 (64685.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1112  max mem: 15572
Epoch: [6]  [1550/2809]  eta: 0:12:19  lr: 0.000047  min_lr: 0.000000  loss: 4.4712 (4.3636)  class_acc: 0.1250 (0.1531)  loss_scale: 65536.0000 (64690.9194)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1039  max mem: 15572
Epoch: [6]  [1560/2809]  eta: 0:12:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3874 (4.3634)  class_acc: 0.1667 (0.1533)  loss_scale: 65536.0000 (64696.3331)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1331  max mem: 15572
Epoch: [6]  [1570/2809]  eta: 0:12:08  lr: 0.000047  min_lr: 0.000000  loss: 4.2767 (4.3631)  class_acc: 0.1667 (0.1534)  loss_scale: 65536.0000 (64701.6779)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1954  max mem: 15572
Epoch: [6]  [1580/2809]  eta: 0:12:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4542 (4.3632)  class_acc: 0.1250 (0.1532)  loss_scale: 65536.0000 (64706.9551)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.2368  max mem: 15572
Epoch: [6]  [1590/2809]  eta: 0:11:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4791 (4.3626)  class_acc: 0.1250 (0.1534)  loss_scale: 65536.0000 (64712.1659)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.2070  max mem: 15572
Epoch: [6]  [1600/2809]  eta: 0:11:51  lr: 0.000047  min_lr: 0.000000  loss: 4.3602 (4.3626)  class_acc: 0.1667 (0.1533)  loss_scale: 65536.0000 (64717.3117)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1094  max mem: 15572
Epoch: [6]  [1610/2809]  eta: 0:11:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3705 (4.3624)  class_acc: 0.1250 (0.1532)  loss_scale: 65536.0000 (64722.3935)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.0982  max mem: 15572
Epoch: [6]  [1620/2809]  eta: 0:11:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4239 (4.3631)  class_acc: 0.1250 (0.1530)  loss_scale: 65536.0000 (64727.4127)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1993  max mem: 15572
Epoch: [6]  [1630/2809]  eta: 0:11:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4239 (4.3628)  class_acc: 0.1250 (0.1531)  loss_scale: 65536.0000 (64732.3703)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1958  max mem: 15572
Epoch: [6]  [1640/2809]  eta: 0:11:28  lr: 0.000047  min_lr: 0.000000  loss: 4.3189 (4.3626)  class_acc: 0.1667 (0.1533)  loss_scale: 65536.0000 (64737.2675)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1148  max mem: 15572
[2025-01-15 17:33:37,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:33:37,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:33:39,701] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18502
[2025-01-15 17:33:39,702] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:33:39,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1650/2809]  eta: 0:11:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3045 (4.3622)  class_acc: 0.1667 (0.1534)  loss_scale: 65536.0000 (64940.5790)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0933  max mem: 15572
Epoch: [6]  [1660/2809]  eta: 0:11:16  lr: 0.000047  min_lr: 0.000000  loss: 4.2818 (4.3619)  class_acc: 0.1667 (0.1535)  loss_scale: 65536.0000 (64944.1638)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1412  max mem: 15572
Epoch: [6]  [1670/2809]  eta: 0:11:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2767 (4.3614)  class_acc: 0.1667 (0.1535)  loss_scale: 65536.0000 (64947.7056)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1631  max mem: 15572
Epoch: [6]  [1680/2809]  eta: 0:11:04  lr: 0.000047  min_lr: 0.000000  loss: 4.4007 (4.3615)  class_acc: 0.1250 (0.1536)  loss_scale: 65536.0000 (64951.2052)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1314  max mem: 15572
Epoch: [6]  [1690/2809]  eta: 0:10:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3495 (4.3604)  class_acc: 0.1667 (0.1537)  loss_scale: 65536.0000 (64954.6635)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.0935  max mem: 15572
Epoch: [6]  [1700/2809]  eta: 0:10:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2750 (4.3603)  class_acc: 0.1667 (0.1538)  loss_scale: 65536.0000 (64958.0811)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1053  max mem: 15572
Epoch: [6]  [1710/2809]  eta: 0:10:46  lr: 0.000047  min_lr: 0.000000  loss: 4.2657 (4.3598)  class_acc: 0.1667 (0.1540)  loss_scale: 65536.0000 (64961.4588)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1365  max mem: 15572
Epoch: [6]  [1720/2809]  eta: 0:10:40  lr: 0.000047  min_lr: 0.000000  loss: 4.3424 (4.3603)  class_acc: 0.1250 (0.1537)  loss_scale: 65536.0000 (64964.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1024  max mem: 15572
Epoch: [6]  [1730/2809]  eta: 0:10:34  lr: 0.000047  min_lr: 0.000000  loss: 4.3731 (4.3600)  class_acc: 0.1250 (0.1536)  loss_scale: 65536.0000 (64968.0971)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1085  max mem: 15572
Epoch: [6]  [1740/2809]  eta: 0:10:29  lr: 0.000047  min_lr: 0.000000  loss: 4.3268 (4.3594)  class_acc: 0.1250 (0.1537)  loss_scale: 65536.0000 (64971.3590)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1258  max mem: 15572
Epoch: [6]  [1750/2809]  eta: 0:10:22  lr: 0.000047  min_lr: 0.000000  loss: 4.3765 (4.3595)  class_acc: 0.1250 (0.1538)  loss_scale: 65536.0000 (64974.5837)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0841  max mem: 15572
Epoch: [6]  [1760/2809]  eta: 0:10:16  lr: 0.000047  min_lr: 0.000000  loss: 4.3765 (4.3596)  class_acc: 0.1250 (0.1540)  loss_scale: 65536.0000 (64977.7717)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0810  max mem: 15572
Epoch: [6]  [1770/2809]  eta: 0:10:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2363 (4.3593)  class_acc: 0.1667 (0.1542)  loss_scale: 65536.0000 (64980.9238)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1313  max mem: 15572
[2025-01-15 17:34:54,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:34:54,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1780/2809]  eta: 0:10:04  lr: 0.000047  min_lr: 0.000000  loss: 4.2323 (4.3586)  class_acc: 0.2083 (0.1545)  loss_scale: 65536.0000 (65131.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1524  max mem: 15572
[2025-01-15 17:34:57,733] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18637
[2025-01-15 17:34:57,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:34:57,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1790/2809]  eta: 0:09:58  lr: 0.000047  min_lr: 0.000000  loss: 4.3481 (4.3590)  class_acc: 0.1667 (0.1544)  loss_scale: 65536.0000 (65206.6734)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1399  max mem: 15572
Epoch: [6]  [1800/2809]  eta: 0:09:52  lr: 0.000047  min_lr: 0.000000  loss: 4.2183 (4.3577)  class_acc: 0.1667 (0.1549)  loss_scale: 65536.0000 (65208.5019)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1462  max mem: 15572
Epoch: [6]  [1810/2809]  eta: 0:09:46  lr: 0.000047  min_lr: 0.000000  loss: 4.2008 (4.3573)  class_acc: 0.2083 (0.1549)  loss_scale: 65536.0000 (65210.3103)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1142  max mem: 15572
Epoch: [6]  [1820/2809]  eta: 0:09:40  lr: 0.000047  min_lr: 0.000000  loss: 4.2008 (4.3557)  class_acc: 0.2083 (0.1551)  loss_scale: 65536.0000 (65212.0988)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1113  max mem: 15572
Epoch: [6]  [1830/2809]  eta: 0:09:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2661 (4.3559)  class_acc: 0.1250 (0.1550)  loss_scale: 65536.0000 (65213.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1385  max mem: 15572
Epoch: [6]  [1840/2809]  eta: 0:09:28  lr: 0.000047  min_lr: 0.000000  loss: 4.4695 (4.3567)  class_acc: 0.0833 (0.1548)  loss_scale: 65536.0000 (65215.6176)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1333  max mem: 15572
Epoch: [6]  [1850/2809]  eta: 0:09:23  lr: 0.000047  min_lr: 0.000000  loss: 4.4936 (4.3572)  class_acc: 0.1250 (0.1548)  loss_scale: 65536.0000 (65217.3485)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1420  max mem: 15572
Epoch: [6]  [1860/2809]  eta: 0:09:18  lr: 0.000047  min_lr: 0.000000  loss: 4.1160 (4.3561)  class_acc: 0.1667 (0.1552)  loss_scale: 65536.0000 (65219.0607)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.2228  max mem: 15572
Epoch: [6]  [1870/2809]  eta: 0:09:11  lr: 0.000047  min_lr: 0.000000  loss: 4.1160 (4.3554)  class_acc: 0.1667 (0.1552)  loss_scale: 65536.0000 (65220.7547)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.1885  max mem: 15572
Epoch: [6]  [1880/2809]  eta: 0:09:06  lr: 0.000047  min_lr: 0.000000  loss: 4.2078 (4.3552)  class_acc: 0.1250 (0.1551)  loss_scale: 65536.0000 (65222.4306)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.0943  max mem: 15572
Epoch: [6]  [1890/2809]  eta: 0:09:00  lr: 0.000047  min_lr: 0.000000  loss: 4.3380 (4.3552)  class_acc: 0.1667 (0.1554)  loss_scale: 65536.0000 (65224.0888)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1155  max mem: 15572
Epoch: [6]  [1900/2809]  eta: 0:08:54  lr: 0.000047  min_lr: 0.000000  loss: 4.3380 (4.3547)  class_acc: 0.2083 (0.1556)  loss_scale: 65536.0000 (65225.7296)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1054  max mem: 15572
Epoch: [6]  [1910/2809]  eta: 0:08:48  lr: 0.000047  min_lr: 0.000000  loss: 4.3523 (4.3551)  class_acc: 0.1667 (0.1556)  loss_scale: 65536.0000 (65227.3532)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1418  max mem: 15572
[2025-01-15 17:36:14,159] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:36:14,159] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [1920/2809]  eta: 0:08:42  lr: 0.000047  min_lr: 0.000000  loss: 4.4165 (4.3554)  class_acc: 0.1250 (0.1552)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1294  max mem: 15572
[2025-01-15 17:36:19,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18776
[2025-01-15 17:36:19,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:36:19,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [1930/2809]  eta: 0:08:36  lr: 0.000047  min_lr: 0.000000  loss: 4.4741 (4.3560)  class_acc: 0.1250 (0.1551)  loss_scale: 65536.0000 (65569.9389)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0445  max mem: 15572
Epoch: [6]  [1940/2809]  eta: 0:08:30  lr: 0.000047  min_lr: 0.000000  loss: 4.3742 (4.3559)  class_acc: 0.1250 (0.1552)  loss_scale: 65536.0000 (65569.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0946  max mem: 15572
Epoch: [6]  [1950/2809]  eta: 0:08:24  lr: 0.000047  min_lr: 0.000000  loss: 4.3433 (4.3565)  class_acc: 0.1250 (0.1550)  loss_scale: 65536.0000 (65569.5910)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1583  max mem: 15572
Epoch: [6]  [1960/2809]  eta: 0:08:18  lr: 0.000047  min_lr: 0.000000  loss: 4.5657 (4.3567)  class_acc: 0.1250 (0.1552)  loss_scale: 65536.0000 (65569.4197)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1781  max mem: 15572
Epoch: [6]  [1970/2809]  eta: 0:08:13  lr: 0.000047  min_lr: 0.000000  loss: 4.3224 (4.3566)  class_acc: 0.1667 (0.1553)  loss_scale: 65536.0000 (65569.2501)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.2378  max mem: 15572
Epoch: [6]  [1980/2809]  eta: 0:08:07  lr: 0.000047  min_lr: 0.000000  loss: 4.3224 (4.3564)  class_acc: 0.1667 (0.1554)  loss_scale: 65536.0000 (65569.0823)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.2056  max mem: 15572
Epoch: [6]  [1990/2809]  eta: 0:08:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3069 (4.3559)  class_acc: 0.1667 (0.1554)  loss_scale: 65536.0000 (65568.9161)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0958  max mem: 15572
Epoch: [6]  [2000/2809]  eta: 0:07:55  lr: 0.000047  min_lr: 0.000000  loss: 4.2454 (4.3554)  class_acc: 0.1250 (0.1553)  loss_scale: 65536.0000 (65568.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0821  max mem: 15572
Epoch: [6]  [2010/2809]  eta: 0:07:49  lr: 0.000047  min_lr: 0.000000  loss: 4.3610 (4.3562)  class_acc: 0.1250 (0.1553)  loss_scale: 65536.0000 (65568.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1295  max mem: 15572
Epoch: [6]  [2020/2809]  eta: 0:07:44  lr: 0.000047  min_lr: 0.000000  loss: 4.4348 (4.3563)  class_acc: 0.1250 (0.1553)  loss_scale: 65536.0000 (65568.4275)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.2074  max mem: 15572
Epoch: [6]  [2030/2809]  eta: 0:07:38  lr: 0.000047  min_lr: 0.000000  loss: 4.4260 (4.3571)  class_acc: 0.1250 (0.1552)  loss_scale: 65536.0000 (65568.2678)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.1799  max mem: 15572
Epoch: [6]  [2040/2809]  eta: 0:07:32  lr: 0.000047  min_lr: 0.000000  loss: 4.5609 (4.3580)  class_acc: 0.0833 (0.1550)  loss_scale: 65536.0000 (65568.1098)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0951  max mem: 15572
Epoch: [6]  [2050/2809]  eta: 0:07:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3052 (4.3571)  class_acc: 0.1667 (0.1553)  loss_scale: 65536.0000 (65567.9532)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1287  max mem: 15572
[2025-01-15 17:37:36,546] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:37:36,546] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:37:38,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18907
[2025-01-15 17:37:38,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:37:38,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2060/2809]  eta: 0:07:20  lr: 0.000047  min_lr: 0.000000  loss: 4.1912 (4.3565)  class_acc: 0.1667 (0.1554)  loss_scale: 65536.0000 (65631.3945)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.2050  max mem: 15572
Epoch: [6]  [2070/2809]  eta: 0:07:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3345 (4.3564)  class_acc: 0.1667 (0.1555)  loss_scale: 65536.0000 (65630.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.2014  max mem: 15572
Epoch: [6]  [2080/2809]  eta: 0:07:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3057 (4.3556)  class_acc: 0.1667 (0.1558)  loss_scale: 65536.0000 (65630.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1143  max mem: 15572
Epoch: [6]  [2090/2809]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000000  loss: 4.2562 (4.3553)  class_acc: 0.1667 (0.1558)  loss_scale: 65536.0000 (65630.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1501  max mem: 15572
Epoch: [6]  [2100/2809]  eta: 0:06:57  lr: 0.000047  min_lr: 0.000000  loss: 4.3749 (4.3551)  class_acc: 0.1250 (0.1557)  loss_scale: 65536.0000 (65629.5783)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1391  max mem: 15572
Epoch: [6]  [2110/2809]  eta: 0:06:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4044 (4.3552)  class_acc: 0.1250 (0.1558)  loss_scale: 65536.0000 (65629.1350)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0846  max mem: 15572
Epoch: [6]  [2120/2809]  eta: 0:06:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3611 (4.3554)  class_acc: 0.1250 (0.1558)  loss_scale: 65536.0000 (65628.6959)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1209  max mem: 15572
Epoch: [6]  [2130/2809]  eta: 0:06:39  lr: 0.000047  min_lr: 0.000000  loss: 4.2466 (4.3549)  class_acc: 0.1667 (0.1559)  loss_scale: 65536.0000 (65628.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1074  max mem: 15572
Epoch: [6]  [2140/2809]  eta: 0:06:33  lr: 0.000047  min_lr: 0.000000  loss: 4.2210 (4.3539)  class_acc: 0.2083 (0.1562)  loss_scale: 65536.0000 (65627.8300)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1325  max mem: 15572
[2025-01-15 17:38:31,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=118, lr=[4.5133071296510653e-07, 4.5133071296510653e-07, 6.447581613787237e-07, 6.447581613787237e-07, 9.21083087683891e-07, 9.21083087683891e-07, 1.3158329824055587e-06, 1.3158329824055587e-06, 1.8797614034365126e-06, 1.8797614034365126e-06, 2.685373433480732e-06, 2.685373433480732e-06, 3.836247762115332e-06, 3.836247762115332e-06, 5.480353945879047e-06, 5.480353945879047e-06, 7.829077065541495e-06, 7.829077065541495e-06, 1.1184395807916422e-05, 1.1184395807916422e-05, 1.597770829702346e-05, 1.597770829702346e-05, 2.2825297567176373e-05, 2.2825297567176373e-05, 3.2607567953109105e-05, 3.2607567953109105e-05, 4.6582239933013015e-05, 4.6582239933013015e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:38:31,896] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=28.42951436337154, CurrSamplesPerSec=24.02568511296428, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [6]  [2150/2809]  eta: 0:06:27  lr: 0.000047  min_lr: 0.000000  loss: 4.1669 (4.3534)  class_acc: 0.2083 (0.1565)  loss_scale: 65536.0000 (65627.4031)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1377  max mem: 15572
Epoch: [6]  [2160/2809]  eta: 0:06:21  lr: 0.000047  min_lr: 0.000000  loss: 4.3595 (4.3539)  class_acc: 0.1667 (0.1564)  loss_scale: 65536.0000 (65626.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1163  max mem: 15572
Epoch: [6]  [2170/2809]  eta: 0:06:15  lr: 0.000047  min_lr: 0.000000  loss: 4.3595 (4.3536)  class_acc: 0.1250 (0.1565)  loss_scale: 65536.0000 (65626.5610)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1359  max mem: 15572
Epoch: [6]  [2180/2809]  eta: 0:06:09  lr: 0.000047  min_lr: 0.000000  loss: 4.2044 (4.3529)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (65626.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1216  max mem: 15572
[2025-01-15 17:38:52,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:38:52,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:38:54,159] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19038
[2025-01-15 17:38:54,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:38:54,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2190/2809]  eta: 0:06:03  lr: 0.000047  min_lr: 0.000000  loss: 4.2044 (4.3527)  class_acc: 0.1667 (0.1569)  loss_scale: 65536.0000 (65685.5573)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1358  max mem: 15572
Epoch: [6]  [2200/2809]  eta: 0:05:58  lr: 0.000047  min_lr: 0.000000  loss: 4.4409 (4.3531)  class_acc: 0.1250 (0.1567)  loss_scale: 65536.0000 (65684.8778)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1903  max mem: 15572
Epoch: [6]  [2210/2809]  eta: 0:05:52  lr: 0.000047  min_lr: 0.000000  loss: 4.3971 (4.3531)  class_acc: 0.1250 (0.1568)  loss_scale: 65536.0000 (65684.2044)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1606  max mem: 15572
Epoch: [6]  [2220/2809]  eta: 0:05:46  lr: 0.000047  min_lr: 0.000000  loss: 4.3790 (4.3532)  class_acc: 0.1667 (0.1570)  loss_scale: 65536.0000 (65683.5371)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1059  max mem: 15572
Epoch: [6]  [2230/2809]  eta: 0:05:40  lr: 0.000047  min_lr: 0.000000  loss: 4.4448 (4.3540)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (65682.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1554  max mem: 15572
Epoch: [6]  [2240/2809]  eta: 0:05:34  lr: 0.000047  min_lr: 0.000000  loss: 4.4448 (4.3532)  class_acc: 0.1250 (0.1568)  loss_scale: 65536.0000 (65682.2204)  weight_decay: 0.0500 (0.0500)  time: 0.6898  data: 0.2519  max mem: 15572
Epoch: [6]  [2250/2809]  eta: 0:05:29  lr: 0.000047  min_lr: 0.000000  loss: 4.4712 (4.3541)  class_acc: 0.1250 (0.1565)  loss_scale: 65536.0000 (65681.5709)  weight_decay: 0.0500 (0.0500)  time: 0.7075  data: 0.2539  max mem: 15572
Epoch: [6]  [2260/2809]  eta: 0:05:23  lr: 0.000047  min_lr: 0.000000  loss: 4.5240 (4.3547)  class_acc: 0.1250 (0.1565)  loss_scale: 65536.0000 (65680.9270)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.2115  max mem: 15572
Epoch: [6]  [2270/2809]  eta: 0:05:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3341 (4.3540)  class_acc: 0.1667 (0.1566)  loss_scale: 65536.0000 (65680.2889)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1622  max mem: 15572
Epoch: [6]  [2280/2809]  eta: 0:05:11  lr: 0.000047  min_lr: 0.000000  loss: 4.1826 (4.3536)  class_acc: 0.1667 (0.1566)  loss_scale: 65536.0000 (65679.6563)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0885  max mem: 15572
Epoch: [6]  [2290/2809]  eta: 0:05:05  lr: 0.000047  min_lr: 0.000000  loss: 4.2194 (4.3533)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (65679.0292)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0819  max mem: 15572
Epoch: [6]  [2300/2809]  eta: 0:04:59  lr: 0.000047  min_lr: 0.000000  loss: 4.3757 (4.3533)  class_acc: 0.1667 (0.1566)  loss_scale: 65536.0000 (65678.4076)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1086  max mem: 15572
Epoch: [6]  [2310/2809]  eta: 0:04:53  lr: 0.000047  min_lr: 0.000000  loss: 4.3455 (4.3528)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (65677.7914)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1084  max mem: 15572
[2025-01-15 17:40:11,178] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:40:11,178] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:40:13,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19173
[2025-01-15 17:40:13,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:40:13,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2320/2809]  eta: 0:04:47  lr: 0.000047  min_lr: 0.000000  loss: 4.3502 (4.3529)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (65846.5972)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1057  max mem: 15572
Epoch: [6]  [2330/2809]  eta: 0:04:41  lr: 0.000047  min_lr: 0.000000  loss: 4.3754 (4.3530)  class_acc: 0.1250 (0.1565)  loss_scale: 65536.0000 (65845.2647)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0945  max mem: 15572
Epoch: [6]  [2340/2809]  eta: 0:04:36  lr: 0.000047  min_lr: 0.000000  loss: 4.3485 (4.3531)  class_acc: 0.1250 (0.1564)  loss_scale: 65536.0000 (65843.9436)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1491  max mem: 15572
Epoch: [6]  [2350/2809]  eta: 0:04:30  lr: 0.000047  min_lr: 0.000000  loss: 4.5359 (4.3536)  class_acc: 0.1250 (0.1562)  loss_scale: 65536.0000 (65842.6338)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1789  max mem: 15572
Epoch: [6]  [2360/2809]  eta: 0:04:24  lr: 0.000047  min_lr: 0.000000  loss: 4.4768 (4.3540)  class_acc: 0.1250 (0.1561)  loss_scale: 65536.0000 (65841.3350)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1285  max mem: 15572
Epoch: [6]  [2370/2809]  eta: 0:04:18  lr: 0.000047  min_lr: 0.000000  loss: 4.4032 (4.3542)  class_acc: 0.1250 (0.1560)  loss_scale: 65536.0000 (65840.0472)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1166  max mem: 15572
Epoch: [6]  [2380/2809]  eta: 0:04:12  lr: 0.000047  min_lr: 0.000000  loss: 4.3868 (4.3540)  class_acc: 0.1250 (0.1561)  loss_scale: 65536.0000 (65838.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1034  max mem: 15572
Epoch: [6]  [2390/2809]  eta: 0:04:06  lr: 0.000047  min_lr: 0.000000  loss: 4.1316 (4.3536)  class_acc: 0.1667 (0.1562)  loss_scale: 65536.0000 (65837.5040)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1341  max mem: 15572
Epoch: [6]  [2400/2809]  eta: 0:04:00  lr: 0.000047  min_lr: 0.000000  loss: 4.2045 (4.3529)  class_acc: 0.1667 (0.1562)  loss_scale: 65536.0000 (65836.2482)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1651  max mem: 15572
Epoch: [6]  [2410/2809]  eta: 0:03:54  lr: 0.000047  min_lr: 0.000000  loss: 4.2203 (4.3528)  class_acc: 0.1667 (0.1562)  loss_scale: 65536.0000 (65835.0029)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1869  max mem: 15572
Epoch: [6]  [2420/2809]  eta: 0:03:48  lr: 0.000047  min_lr: 0.000000  loss: 4.2855 (4.3522)  class_acc: 0.1667 (0.1563)  loss_scale: 65536.0000 (65833.7679)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1868  max mem: 15572
Epoch: [6]  [2430/2809]  eta: 0:03:42  lr: 0.000047  min_lr: 0.000000  loss: 4.3759 (4.3519)  class_acc: 0.1250 (0.1561)  loss_scale: 65536.0000 (65832.5430)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1306  max mem: 15572
Epoch: [6]  [2440/2809]  eta: 0:03:37  lr: 0.000047  min_lr: 0.000000  loss: 4.4009 (4.3520)  class_acc: 0.1667 (0.1564)  loss_scale: 65536.0000 (65831.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1447  max mem: 15572
[2025-01-15 17:41:31,969] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:41:31,970] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2450/2809]  eta: 0:03:31  lr: 0.000047  min_lr: 0.000000  loss: 4.3022 (4.3512)  class_acc: 0.2083 (0.1566)  loss_scale: 65536.0000 (65910.3386)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.1909  max mem: 15572
Epoch: [6]  [2460/2809]  eta: 0:03:25  lr: 0.000047  min_lr: 0.000000  loss: 4.2821 (4.3514)  class_acc: 0.1667 (0.1567)  loss_scale: 131072.0000 (66175.1158)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.2111  max mem: 15572
Epoch: [6]  [2470/2809]  eta: 0:03:19  lr: 0.000047  min_lr: 0.000000  loss: 4.3577 (4.3514)  class_acc: 0.1667 (0.1567)  loss_scale: 131072.0000 (66437.7499)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1292  max mem: 15572
[2025-01-15 17:41:48,594] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19329
[2025-01-15 17:41:48,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:41:48,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2480/2809]  eta: 0:03:13  lr: 0.000047  min_lr: 0.000000  loss: 4.2821 (4.3513)  class_acc: 0.1667 (0.1568)  loss_scale: 131072.0000 (66539.7759)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0886  max mem: 15572
Epoch: [6]  [2490/2809]  eta: 0:03:07  lr: 0.000047  min_lr: 0.000000  loss: 4.2840 (4.3516)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66535.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1085  max mem: 15572
Epoch: [6]  [2500/2809]  eta: 0:03:01  lr: 0.000047  min_lr: 0.000000  loss: 4.3767 (4.3519)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (66531.7489)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1248  max mem: 15572
Epoch: [6]  [2510/2809]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000000  loss: 4.4787 (4.3526)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (66527.7834)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1311  max mem: 15572
Epoch: [6]  [2520/2809]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000000  loss: 4.3901 (4.3522)  class_acc: 0.1667 (0.1566)  loss_scale: 65536.0000 (66523.8493)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1322  max mem: 15572
Epoch: [6]  [2530/2809]  eta: 0:02:44  lr: 0.000047  min_lr: 0.000000  loss: 4.1816 (4.3514)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (66519.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1209  max mem: 15572
Epoch: [6]  [2540/2809]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000000  loss: 4.1816 (4.3505)  class_acc: 0.2083 (0.1569)  loss_scale: 65536.0000 (66516.0740)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1411  max mem: 15572
Epoch: [6]  [2550/2809]  eta: 0:02:32  lr: 0.000047  min_lr: 0.000000  loss: 4.2906 (4.3507)  class_acc: 0.1250 (0.1568)  loss_scale: 65536.0000 (66512.2321)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1705  max mem: 15572
Epoch: [6]  [2560/2809]  eta: 0:02:26  lr: 0.000047  min_lr: 0.000000  loss: 4.3752 (4.3506)  class_acc: 0.1250 (0.1569)  loss_scale: 65536.0000 (66508.4201)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1376  max mem: 15572
Epoch: [6]  [2570/2809]  eta: 0:02:20  lr: 0.000047  min_lr: 0.000000  loss: 4.3752 (4.3512)  class_acc: 0.1250 (0.1568)  loss_scale: 65536.0000 (66504.6379)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0932  max mem: 15572
Epoch: [6]  [2580/2809]  eta: 0:02:14  lr: 0.000047  min_lr: 0.000000  loss: 4.3426 (4.3510)  class_acc: 0.1250 (0.1569)  loss_scale: 65536.0000 (66500.8849)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0623  max mem: 15572
Epoch: [6]  [2590/2809]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000000  loss: 4.3426 (4.3513)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66497.1609)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1351  max mem: 15572
Epoch: [6]  [2600/2809]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000000  loss: 4.4224 (4.3518)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66493.4656)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1289  max mem: 15572
[2025-01-15 17:43:04,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:43:04,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:43:05,989] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19462
[2025-01-15 17:43:05,989] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:43:05,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2610/2809]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000000  loss: 4.4622 (4.3519)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66590.1984)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1288  max mem: 15572
Epoch: [6]  [2620/2809]  eta: 0:01:51  lr: 0.000047  min_lr: 0.000000  loss: 4.4829 (4.3524)  class_acc: 0.1250 (0.1567)  loss_scale: 65536.0000 (66586.1763)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1076  max mem: 15572
Epoch: [6]  [2630/2809]  eta: 0:01:45  lr: 0.000047  min_lr: 0.000000  loss: 4.3401 (4.3524)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66582.1847)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1270  max mem: 15572
Epoch: [6]  [2640/2809]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000000  loss: 4.3401 (4.3521)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66578.2234)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1544  max mem: 15572
Epoch: [6]  [2650/2809]  eta: 0:01:33  lr: 0.000047  min_lr: 0.000000  loss: 4.2747 (4.3514)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66574.2920)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0784  max mem: 15572
Epoch: [6]  [2660/2809]  eta: 0:01:27  lr: 0.000047  min_lr: 0.000000  loss: 4.2787 (4.3514)  class_acc: 0.1250 (0.1567)  loss_scale: 65536.0000 (66570.3901)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0924  max mem: 15572
Epoch: [6]  [2670/2809]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000000  loss: 4.4289 (4.3514)  class_acc: 0.1250 (0.1567)  loss_scale: 65536.0000 (66566.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1381  max mem: 15572
Epoch: [6]  [2680/2809]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000000  loss: 4.4289 (4.3514)  class_acc: 0.1250 (0.1567)  loss_scale: 65536.0000 (66562.6736)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1671  max mem: 15572
Epoch: [6]  [2690/2809]  eta: 0:01:10  lr: 0.000047  min_lr: 0.000000  loss: 4.2442 (4.3511)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (66558.8584)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1736  max mem: 15572
Epoch: [6]  [2700/2809]  eta: 0:01:04  lr: 0.000047  min_lr: 0.000000  loss: 4.2178 (4.3509)  class_acc: 0.1667 (0.1568)  loss_scale: 65536.0000 (66555.0715)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1667  max mem: 15572
Epoch: [6]  [2710/2809]  eta: 0:00:58  lr: 0.000047  min_lr: 0.000000  loss: 4.2876 (4.3510)  class_acc: 0.0833 (0.1567)  loss_scale: 65536.0000 (66551.3124)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1026  max mem: 15572
Epoch: [6]  [2720/2809]  eta: 0:00:52  lr: 0.000047  min_lr: 0.000000  loss: 4.4484 (4.3512)  class_acc: 0.0417 (0.1566)  loss_scale: 65536.0000 (66547.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1016  max mem: 15572
Epoch: [6]  [2730/2809]  eta: 0:00:46  lr: 0.000047  min_lr: 0.000000  loss: 4.2516 (4.3505)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (66543.8770)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1471  max mem: 15572
[2025-01-15 17:44:20,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:44:20,372] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [6]  [2740/2809]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000000  loss: 4.1772 (4.3501)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (66635.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1158  max mem: 15572
Epoch: [6]  [2750/2809]  eta: 0:00:34  lr: 0.000047  min_lr: 0.000000  loss: 4.2524 (4.3504)  class_acc: 0.1250 (0.1567)  loss_scale: 131072.0000 (66870.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1123  max mem: 15572
[2025-01-15 17:44:29,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19606
[2025-01-15 17:44:29,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:44:29,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [6]  [2760/2809]  eta: 0:00:28  lr: 0.000047  min_lr: 0.000000  loss: 4.2935 (4.3501)  class_acc: 0.1667 (0.1568)  loss_scale: 131072.0000 (66888.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1434  max mem: 15572
Epoch: [6]  [2770/2809]  eta: 0:00:22  lr: 0.000047  min_lr: 0.000000  loss: 4.2955 (4.3502)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (66884.0881)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1500  max mem: 15572
Epoch: [6]  [2780/2809]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000000  loss: 4.3303 (4.3503)  class_acc: 0.1250 (0.1566)  loss_scale: 65536.0000 (66879.2406)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1569  max mem: 15572
Epoch: [6]  [2790/2809]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000000  loss: 4.2332 (4.3494)  class_acc: 0.1667 (0.1569)  loss_scale: 65536.0000 (66874.4278)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1703  max mem: 15572
Epoch: [6]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.0912 (4.3488)  class_acc: 0.2083 (0.1570)  loss_scale: 65536.0000 (66869.6494)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1215  max mem: 15572
Epoch: [6]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.1127 (4.3484)  class_acc: 0.2083 (0.1571)  loss_scale: 65536.0000 (66865.8512)  weight_decay: 0.0500 (0.0500)  time: 0.4628  data: 0.0574  max mem: 15572
Epoch: [6] Total time: 0:27:31 (0.5880 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.1127 (4.3484)  class_acc: 0.2083 (0.1571)  loss_scale: 65536.0000 (66865.8512)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:42  loss: 0.6627 (0.6627)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.1280  data: 3.9457  max mem: 15572
Val:  [ 10/272]  eta: 0:03:35  loss: 4.2233 (3.5769)  acc1: 0.0000 (25.7576)  acc5: 16.6667 (31.8182)  time: 0.8236  data: 0.6374  max mem: 15572
Val:  [ 20/272]  eta: 0:02:25  loss: 3.7750 (3.5594)  acc1: 5.5556 (21.9577)  acc5: 33.3333 (38.8889)  time: 0.4008  data: 0.2229  max mem: 15572
Val:  [ 30/272]  eta: 0:01:56  loss: 3.7577 (3.6993)  acc1: 5.5556 (16.3082)  acc5: 38.8889 (37.6344)  time: 0.2943  data: 0.1052  max mem: 15572
Val:  [ 40/272]  eta: 0:01:45  loss: 3.6595 (3.6592)  acc1: 5.5556 (16.1247)  acc5: 38.8889 (40.1084)  time: 0.3258  data: 0.1190  max mem: 15572
Val:  [ 50/272]  eta: 0:01:38  loss: 3.5177 (3.5885)  acc1: 16.6667 (17.7560)  acc5: 44.4444 (42.7015)  time: 0.3825  data: 0.1877  max mem: 15572
Val:  [ 60/272]  eta: 0:01:29  loss: 2.2838 (3.4131)  acc1: 44.4444 (24.1348)  acc5: 72.2222 (46.7213)  time: 0.3583  data: 0.1646  max mem: 15572
Val:  [ 70/272]  eta: 0:01:22  loss: 2.4530 (3.2989)  acc1: 50.0000 (25.6651)  acc5: 77.7778 (51.4085)  time: 0.3204  data: 0.1243  max mem: 15572
Val:  [ 80/272]  eta: 0:01:15  loss: 3.0419 (3.3054)  acc1: 22.2222 (25.8573)  acc5: 72.2222 (51.0288)  time: 0.3091  data: 0.1098  max mem: 15572
Val:  [ 90/272]  eta: 0:01:10  loss: 4.2742 (3.4125)  acc1: 5.5556 (23.3822)  acc5: 16.6667 (47.2527)  time: 0.3002  data: 0.0961  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 4.2212 (3.4852)  acc1: 5.5556 (22.4422)  acc5: 22.2222 (45.5446)  time: 0.3076  data: 0.0994  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 4.1039 (3.5549)  acc1: 0.0000 (20.4204)  acc5: 27.7778 (43.8438)  time: 0.2629  data: 0.0583  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 4.1572 (3.5960)  acc1: 0.0000 (19.0542)  acc5: 22.2222 (43.2507)  time: 0.2036  data: 0.0131  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 3.7398 (3.5373)  acc1: 5.5556 (20.9924)  acc5: 50.0000 (44.7837)  time: 0.2252  data: 0.0573  max mem: 15572
Val:  [140/272]  eta: 0:00:43  loss: 3.4276 (3.5241)  acc1: 16.6667 (21.5918)  acc5: 50.0000 (44.7991)  time: 0.2223  data: 0.0626  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 3.6062 (3.5249)  acc1: 11.1111 (21.0081)  acc5: 44.4444 (45.1435)  time: 0.1921  data: 0.0095  max mem: 15572
Val:  [160/272]  eta: 0:00:35  loss: 3.5291 (3.5001)  acc1: 16.6667 (21.8427)  acc5: 55.5556 (46.6529)  time: 0.1994  data: 0.0007  max mem: 15572
Val:  [170/272]  eta: 0:00:31  loss: 3.5897 (3.5293)  acc1: 11.1111 (20.9877)  acc5: 55.5556 (46.0364)  time: 0.2137  data: 0.0125  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 3.6440 (3.5189)  acc1: 5.5556 (20.9638)  acc5: 44.4444 (46.7158)  time: 0.3105  data: 0.1056  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 3.7248 (3.5460)  acc1: 0.0000 (20.0407)  acc5: 44.4444 (45.8697)  time: 0.4253  data: 0.2245  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 3.4623 (3.5477)  acc1: 0.0000 (19.7623)  acc5: 44.4444 (46.4345)  time: 0.3922  data: 0.1894  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 3.2739 (3.5569)  acc1: 5.5556 (19.9052)  acc5: 72.2222 (46.7351)  time: 0.3314  data: 0.1221  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 3.4924 (3.5514)  acc1: 11.1111 (20.3117)  acc5: 55.5556 (46.8829)  time: 0.3489  data: 0.1332  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.9669 (3.5166)  acc1: 50.0000 (21.9096)  acc5: 72.2222 (48.1722)  time: 0.3632  data: 0.1574  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.7918 (3.4920)  acc1: 44.4444 (22.4297)  acc5: 77.7778 (49.4698)  time: 0.3669  data: 0.1681  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.2316 (3.5228)  acc1: 11.1111 (21.6910)  acc5: 66.6667 (48.7384)  time: 0.3720  data: 0.1725  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.9456 (3.4462)  acc1: 16.6667 (24.1379)  acc5: 77.7778 (50.3831)  time: 0.3680  data: 0.1737  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.2545 (3.4458)  acc1: 61.1111 (24.0262)  acc5: 77.7778 (50.2870)  time: 0.3353  data: 0.1558  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.2545 (3.4487)  acc1: 55.5556 (24.0221)  acc5: 77.7778 (50.2765)  time: 0.2506  data: 0.0774  max mem: 15572
Val: Total time: 0:01:30 (0.3309 s / it)
* Acc@1 24.022 Acc@5 50.276 loss 3.449
Accuracy of the network on the 4883 val videos: 24.0%
[2025-01-15 17:46:30,236] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 17:46:30,240] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 17:46:30,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 17:46:33,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 17:46:33,436] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.02%
Epoch: [7]  [   0/2809]  eta: 8:20:39  lr: 0.000046  min_lr: 0.000000  loss: 4.2312 (4.2312)  class_acc: 0.1250 (0.1250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.6939  data: 10.2302  max mem: 15572
Epoch: [7]  [  10/2809]  eta: 1:15:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2130 (4.1675)  class_acc: 0.1250 (0.1477)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6235  data: 1.1632  max mem: 15572
Epoch: [7]  [  20/2809]  eta: 0:52:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2012 (4.2504)  class_acc: 0.1667 (0.1567)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.2032  max mem: 15572
Epoch: [7]  [  30/2809]  eta: 0:46:00  lr: 0.000046  min_lr: 0.000000  loss: 4.1979 (4.2726)  class_acc: 0.1667 (0.1546)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.1758  max mem: 15572
Epoch: [7]  [  40/2809]  eta: 0:42:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2252 (4.3034)  class_acc: 0.1250 (0.1535)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6761  data: 0.1866  max mem: 15572
Epoch: [7]  [  50/2809]  eta: 0:38:24  lr: 0.000046  min_lr: 0.000000  loss: 4.1879 (4.2772)  class_acc: 0.1250 (0.1618)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1315  max mem: 15572
Epoch: [7]  [  60/2809]  eta: 0:34:59  lr: 0.000046  min_lr: 0.000000  loss: 4.1942 (4.2861)  class_acc: 0.1667 (0.1612)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4575  data: 0.0459  max mem: 15572
Epoch: [7]  [  70/2809]  eta: 0:32:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3811 (4.3103)  class_acc: 0.1667 (0.1590)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4132  data: 0.0004  max mem: 15572
[2025-01-15 17:47:25,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:47:25,222] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:47:25,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19736
[2025-01-15 17:47:25,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:47:25,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [  80/2809]  eta: 0:31:15  lr: 0.000046  min_lr: 0.000000  loss: 4.3932 (4.3026)  class_acc: 0.1667 (0.1636)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4549  data: 0.0008  max mem: 15572
Epoch: [7]  [  90/2809]  eta: 0:29:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3802 (4.3042)  class_acc: 0.1667 (0.1648)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.4573  data: 0.0008  max mem: 15572
Epoch: [7]  [ 100/2809]  eta: 0:28:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2930 (4.2982)  class_acc: 0.1667 (0.1716)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.4372  data: 0.0009  max mem: 15572
Epoch: [7]  [ 110/2809]  eta: 0:28:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2374 (4.2901)  class_acc: 0.1667 (0.1727)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0800  max mem: 15572
Epoch: [7]  [ 120/2809]  eta: 0:28:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1808 (4.2818)  class_acc: 0.1667 (0.1770)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1639  max mem: 15572
Epoch: [7]  [ 130/2809]  eta: 0:27:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2208 (4.2690)  class_acc: 0.2083 (0.1803)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1400  max mem: 15572
Epoch: [7]  [ 140/2809]  eta: 0:27:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2456 (4.2590)  class_acc: 0.2083 (0.1832)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1396  max mem: 15572
Epoch: [7]  [ 150/2809]  eta: 0:27:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2456 (4.2628)  class_acc: 0.1667 (0.1821)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1809  max mem: 15572
Epoch: [7]  [ 160/2809]  eta: 0:27:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2745 (4.2666)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1373  max mem: 15572
Epoch: [7]  [ 170/2809]  eta: 0:27:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2738 (4.2689)  class_acc: 0.1667 (0.1801)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1653  max mem: 15572
Epoch: [7]  [ 180/2809]  eta: 0:27:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2989 (4.2691)  class_acc: 0.2083 (0.1809)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.2141  max mem: 15572
Epoch: [7]  [ 190/2809]  eta: 0:27:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2548 (4.2654)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1384  max mem: 15572
Epoch: [7]  [ 200/2809]  eta: 0:26:37  lr: 0.000046  min_lr: 0.000000  loss: 4.2703 (4.2734)  class_acc: 0.1667 (0.1791)  loss_scale: 65536.0000 (65862.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0696  max mem: 15572
[2025-01-15 17:48:38,836] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:48:38,836] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:48:39,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19867
[2025-01-15 17:48:39,739] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:48:39,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 210/2809]  eta: 0:26:28  lr: 0.000046  min_lr: 0.000000  loss: 4.3201 (4.2724)  class_acc: 0.1667 (0.1787)  loss_scale: 65536.0000 (66467.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0872  max mem: 15572
Epoch: [7]  [ 220/2809]  eta: 0:26:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3721 (4.2762)  class_acc: 0.1667 (0.1795)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1450  max mem: 15572
Epoch: [7]  [ 230/2809]  eta: 0:26:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4214 (4.2825)  class_acc: 0.1667 (0.1782)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1389  max mem: 15572
Epoch: [7]  [ 240/2809]  eta: 0:26:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4712 (4.2836)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (66351.8008)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1199  max mem: 15572
Epoch: [7]  [ 250/2809]  eta: 0:25:58  lr: 0.000046  min_lr: 0.000000  loss: 4.3583 (4.2772)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (66319.2988)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1514  max mem: 15572
Epoch: [7]  [ 260/2809]  eta: 0:25:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2706 (4.2832)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (66289.2874)  weight_decay: 0.0500 (0.0500)  time: 0.6575  data: 0.2082  max mem: 15572
Epoch: [7]  [ 270/2809]  eta: 0:25:42  lr: 0.000046  min_lr: 0.000000  loss: 4.4108 (4.2916)  class_acc: 0.1250 (0.1774)  loss_scale: 65536.0000 (66261.4908)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1539  max mem: 15572
Epoch: [7]  [ 280/2809]  eta: 0:25:37  lr: 0.000046  min_lr: 0.000000  loss: 4.3589 (4.2885)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (66235.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1348  max mem: 15572
Epoch: [7]  [ 290/2809]  eta: 0:25:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2858 (4.2924)  class_acc: 0.1667 (0.1786)  loss_scale: 65536.0000 (66211.6289)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1557  max mem: 15572
Epoch: [7]  [ 300/2809]  eta: 0:25:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2560 (4.2884)  class_acc: 0.2083 (0.1805)  loss_scale: 65536.0000 (66189.1827)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1214  max mem: 15572
Epoch: [7]  [ 310/2809]  eta: 0:25:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2323 (4.2893)  class_acc: 0.1667 (0.1795)  loss_scale: 65536.0000 (66168.1801)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1105  max mem: 15572
Epoch: [7]  [ 320/2809]  eta: 0:24:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4025 (4.2939)  class_acc: 0.1250 (0.1787)  loss_scale: 65536.0000 (66148.4860)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1213  max mem: 15572
Epoch: [7]  [ 330/2809]  eta: 0:24:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2668 (4.2894)  class_acc: 0.1667 (0.1800)  loss_scale: 65536.0000 (66129.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1404  max mem: 15572
[2025-01-15 17:49:55,802] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:49:55,802] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:49:57,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=125, lr=[4.500737936126402e-07, 4.500737936126402e-07, 6.429625623037718e-07, 6.429625623037718e-07, 9.185179461482454e-07, 9.185179461482454e-07, 1.3121684944974935e-06, 1.3121684944974935e-06, 1.8745264207107052e-06, 1.8745264207107052e-06, 2.677894886729579e-06, 2.677894886729579e-06, 3.825564123899399e-06, 3.825564123899399e-06, 5.46509160557057e-06, 5.46509160557057e-06, 7.807273722243671e-06, 7.807273722243671e-06, 1.1153248174633818e-05, 1.1153248174633818e-05, 1.593321167804831e-05, 1.593321167804831e-05, 2.2761730968640446e-05, 2.2761730968640446e-05, 3.2516758526629215e-05, 3.2516758526629215e-05, 4.645251218089888e-05, 4.645251218089888e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:49:57,241] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=28.428105999183067, CurrSamplesPerSec=25.31263729631865, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [ 340/2809]  eta: 0:24:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2258 (4.2905)  class_acc: 0.1667 (0.1805)  loss_scale: 65536.0000 (67650.0645)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1552  max mem: 15572
[2025-01-15 17:50:00,462] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20006
[2025-01-15 17:50:00,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:50:00,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 350/2809]  eta: 0:24:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2258 (4.2892)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1653  max mem: 15572
Epoch: [7]  [ 360/2809]  eta: 0:24:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2771 (4.2887)  class_acc: 0.1667 (0.1801)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1503  max mem: 15572
Epoch: [7]  [ 370/2809]  eta: 0:24:26  lr: 0.000046  min_lr: 0.000000  loss: 4.5831 (4.2967)  class_acc: 0.1250 (0.1790)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1137  max mem: 15572
Epoch: [7]  [ 380/2809]  eta: 0:24:24  lr: 0.000046  min_lr: 0.000000  loss: 4.4237 (4.2975)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1337  max mem: 15572
Epoch: [7]  [ 390/2809]  eta: 0:24:13  lr: 0.000046  min_lr: 0.000000  loss: 4.4004 (4.3001)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1408  max mem: 15572
Epoch: [7]  [ 400/2809]  eta: 0:24:09  lr: 0.000046  min_lr: 0.000000  loss: 4.3554 (4.3008)  class_acc: 0.1667 (0.1787)  loss_scale: 65536.0000 (67660.6085)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1401  max mem: 15572
Epoch: [7]  [ 410/2809]  eta: 0:24:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2888 (4.3033)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (67608.9148)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1447  max mem: 15572
Epoch: [7]  [ 420/2809]  eta: 0:23:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2534 (4.3027)  class_acc: 0.1667 (0.1789)  loss_scale: 65536.0000 (67559.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1245  max mem: 15572
Epoch: [7]  [ 430/2809]  eta: 0:23:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2534 (4.3075)  class_acc: 0.1250 (0.1773)  loss_scale: 65536.0000 (67512.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1320  max mem: 15572
Epoch: [7]  [ 440/2809]  eta: 0:23:37  lr: 0.000046  min_lr: 0.000000  loss: 4.3723 (4.3070)  class_acc: 0.1667 (0.1771)  loss_scale: 65536.0000 (67467.9002)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1434  max mem: 15572
Epoch: [7]  [ 450/2809]  eta: 0:23:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3383 (4.3066)  class_acc: 0.1667 (0.1768)  loss_scale: 65536.0000 (67425.0643)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.1970  max mem: 15572
Epoch: [7]  [ 460/2809]  eta: 0:23:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2093 (4.3045)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (67384.0868)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1853  max mem: 15572
Epoch: [7]  [ 470/2809]  eta: 0:23:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2569 (4.3057)  class_acc: 0.1667 (0.1769)  loss_scale: 65536.0000 (67344.8493)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1536  max mem: 15572
[2025-01-15 17:51:16,794] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:51:16,795] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:51:19,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20138
[2025-01-15 17:51:19,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:51:19,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 480/2809]  eta: 0:23:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2597 (4.3027)  class_acc: 0.2083 (0.1784)  loss_scale: 65536.0000 (67715.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1545  max mem: 15572
Epoch: [7]  [ 490/2809]  eta: 0:23:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2807 (4.3022)  class_acc: 0.1667 (0.1784)  loss_scale: 65536.0000 (67671.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1474  max mem: 15572
Epoch: [7]  [ 500/2809]  eta: 0:22:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3226 (4.3003)  class_acc: 0.1250 (0.1783)  loss_scale: 65536.0000 (67628.9661)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1227  max mem: 15572
Epoch: [7]  [ 510/2809]  eta: 0:22:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3226 (4.2987)  class_acc: 0.2083 (0.1791)  loss_scale: 65536.0000 (67588.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1180  max mem: 15572
Epoch: [7]  [ 520/2809]  eta: 0:22:44  lr: 0.000046  min_lr: 0.000000  loss: 4.1026 (4.2916)  class_acc: 0.2083 (0.1797)  loss_scale: 65536.0000 (67548.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1241  max mem: 15572
Epoch: [7]  [ 530/2809]  eta: 0:22:34  lr: 0.000046  min_lr: 0.000000  loss: 4.1256 (4.2910)  class_acc: 0.1667 (0.1800)  loss_scale: 65536.0000 (67510.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0945  max mem: 15572
Epoch: [7]  [ 540/2809]  eta: 0:22:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2264 (4.2893)  class_acc: 0.1667 (0.1796)  loss_scale: 65536.0000 (67474.2181)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1348  max mem: 15572
Epoch: [7]  [ 550/2809]  eta: 0:22:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2056 (4.2887)  class_acc: 0.1667 (0.1791)  loss_scale: 65536.0000 (67439.0417)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.1890  max mem: 15572
Epoch: [7]  [ 560/2809]  eta: 0:22:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3418 (4.2900)  class_acc: 0.1250 (0.1785)  loss_scale: 65536.0000 (67405.1194)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.1811  max mem: 15572
Epoch: [7]  [ 570/2809]  eta: 0:22:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2885 (4.2894)  class_acc: 0.1667 (0.1786)  loss_scale: 65536.0000 (67372.3853)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1586  max mem: 15572
Epoch: [7]  [ 580/2809]  eta: 0:22:08  lr: 0.000046  min_lr: 0.000000  loss: 4.4035 (4.2906)  class_acc: 0.1250 (0.1776)  loss_scale: 65536.0000 (67340.7780)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1122  max mem: 15572
Epoch: [7]  [ 590/2809]  eta: 0:22:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4035 (4.2891)  class_acc: 0.1250 (0.1779)  loss_scale: 65536.0000 (67310.2403)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0979  max mem: 15572
Epoch: [7]  [ 600/2809]  eta: 0:21:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2280 (4.2903)  class_acc: 0.1667 (0.1774)  loss_scale: 65536.0000 (67280.7188)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0826  max mem: 15572
[2025-01-15 17:52:32,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:52:32,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 610/2809]  eta: 0:21:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3090 (4.2908)  class_acc: 0.1667 (0.1775)  loss_scale: 65536.0000 (68002.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0692  max mem: 15572
[2025-01-15 17:52:37,098] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20276
[2025-01-15 17:52:37,099] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:52:37,099] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 620/2809]  eta: 0:21:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2986 (4.2905)  class_acc: 0.1667 (0.1777)  loss_scale: 65536.0000 (68174.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0988  max mem: 15572
Epoch: [7]  [ 630/2809]  eta: 0:21:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2872 (4.2937)  class_acc: 0.1250 (0.1770)  loss_scale: 65536.0000 (68132.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1319  max mem: 15572
Epoch: [7]  [ 640/2809]  eta: 0:21:23  lr: 0.000046  min_lr: 0.000000  loss: 4.4053 (4.2942)  class_acc: 0.1250 (0.1767)  loss_scale: 65536.0000 (68092.0062)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1477  max mem: 15572
Epoch: [7]  [ 650/2809]  eta: 0:21:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2981 (4.2944)  class_acc: 0.1667 (0.1770)  loss_scale: 65536.0000 (68052.7435)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1552  max mem: 15572
Epoch: [7]  [ 660/2809]  eta: 0:21:11  lr: 0.000046  min_lr: 0.000000  loss: 4.3777 (4.2943)  class_acc: 0.2083 (0.1770)  loss_scale: 65536.0000 (68014.6687)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1478  max mem: 15572
Epoch: [7]  [ 670/2809]  eta: 0:21:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3578 (4.2934)  class_acc: 0.2083 (0.1773)  loss_scale: 65536.0000 (67977.7288)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.2068  max mem: 15572
Epoch: [7]  [ 680/2809]  eta: 0:21:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2937 (4.2942)  class_acc: 0.1667 (0.1770)  loss_scale: 65536.0000 (67941.8737)  weight_decay: 0.0500 (0.0500)  time: 0.7017  data: 0.2299  max mem: 15572
Epoch: [7]  [ 690/2809]  eta: 0:20:58  lr: 0.000046  min_lr: 0.000000  loss: 4.1458 (4.2910)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (67907.0564)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1313  max mem: 15572
Epoch: [7]  [ 700/2809]  eta: 0:20:54  lr: 0.000046  min_lr: 0.000000  loss: 4.1423 (4.2901)  class_acc: 0.1667 (0.1771)  loss_scale: 65536.0000 (67873.2325)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1439  max mem: 15572
Epoch: [7]  [ 710/2809]  eta: 0:20:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1625 (4.2891)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (67840.3601)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1599  max mem: 15572
Epoch: [7]  [ 720/2809]  eta: 0:20:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2302 (4.2898)  class_acc: 0.1667 (0.1768)  loss_scale: 65536.0000 (67808.3994)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0744  max mem: 15572
Epoch: [7]  [ 730/2809]  eta: 0:20:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4216 (4.2908)  class_acc: 0.1250 (0.1765)  loss_scale: 65536.0000 (67777.3133)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0492  max mem: 15572
Epoch: [7]  [ 740/2809]  eta: 0:20:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4540 (4.2921)  class_acc: 0.1250 (0.1761)  loss_scale: 65536.0000 (67747.0661)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1013  max mem: 15572
[2025-01-15 17:53:54,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:53:54,012] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:53:54,861] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20407
[2025-01-15 17:53:54,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:53:54,861] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [ 750/2809]  eta: 0:20:19  lr: 0.000046  min_lr: 0.000000  loss: 4.0513 (4.2878)  class_acc: 0.1667 (0.1766)  loss_scale: 65536.0000 (67892.1545)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1049  max mem: 15572
Epoch: [7]  [ 760/2809]  eta: 0:20:13  lr: 0.000046  min_lr: 0.000000  loss: 4.3328 (4.2900)  class_acc: 0.1667 (0.1759)  loss_scale: 65536.0000 (67861.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1118  max mem: 15572
Epoch: [7]  [ 770/2809]  eta: 0:20:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3716 (4.2910)  class_acc: 0.1250 (0.1761)  loss_scale: 65536.0000 (67831.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1133  max mem: 15572
Epoch: [7]  [ 780/2809]  eta: 0:20:03  lr: 0.000046  min_lr: 0.000000  loss: 4.1503 (4.2890)  class_acc: 0.2083 (0.1766)  loss_scale: 65536.0000 (67801.6492)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1871  max mem: 15572
Epoch: [7]  [ 790/2809]  eta: 0:19:57  lr: 0.000046  min_lr: 0.000000  loss: 4.0631 (4.2879)  class_acc: 0.2083 (0.1770)  loss_scale: 65536.0000 (67773.0063)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.2092  max mem: 15572
Epoch: [7]  [ 800/2809]  eta: 0:19:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2724 (4.2880)  class_acc: 0.2083 (0.1769)  loss_scale: 65536.0000 (67745.0787)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1722  max mem: 15572
[2025-01-15 17:54:35,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20473
[2025-01-15 17:54:35,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 17:54:35,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 810/2809]  eta: 0:19:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3205 (4.2880)  class_acc: 0.1667 (0.1771)  loss_scale: 65536.0000 (67677.4353)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1845  max mem: 15572
Epoch: [7]  [ 820/2809]  eta: 0:19:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3205 (4.2872)  class_acc: 0.1250 (0.1768)  loss_scale: 32768.0000 (67252.2290)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1676  max mem: 15572
Epoch: [7]  [ 830/2809]  eta: 0:19:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2012 (4.2870)  class_acc: 0.1250 (0.1772)  loss_scale: 32768.0000 (66837.2563)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1088  max mem: 15572
Epoch: [7]  [ 840/2809]  eta: 0:19:26  lr: 0.000046  min_lr: 0.000000  loss: 4.1999 (4.2868)  class_acc: 0.1667 (0.1773)  loss_scale: 32768.0000 (66432.1522)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0603  max mem: 15572
Epoch: [7]  [ 850/2809]  eta: 0:19:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2290 (4.2863)  class_acc: 0.1667 (0.1780)  loss_scale: 32768.0000 (66036.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1498  max mem: 15572
Epoch: [7]  [ 860/2809]  eta: 0:19:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2430 (4.2848)  class_acc: 0.1667 (0.1781)  loss_scale: 32768.0000 (65650.1742)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.2072  max mem: 15572
Epoch: [7]  [ 870/2809]  eta: 0:19:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2313 (4.2838)  class_acc: 0.1667 (0.1781)  loss_scale: 32768.0000 (65272.6521)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1701  max mem: 15572
Epoch: [7]  [ 880/2809]  eta: 0:19:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1033 (4.2812)  class_acc: 0.1667 (0.1787)  loss_scale: 32768.0000 (64903.7003)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1062  max mem: 15572
Epoch: [7]  [ 890/2809]  eta: 0:18:59  lr: 0.000046  min_lr: 0.000000  loss: 4.1285 (4.2802)  class_acc: 0.2083 (0.1789)  loss_scale: 32768.0000 (64543.0303)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1298  max mem: 15572
Epoch: [7]  [ 900/2809]  eta: 0:18:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3077 (4.2801)  class_acc: 0.1667 (0.1788)  loss_scale: 32768.0000 (64190.3663)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1134  max mem: 15572
Epoch: [7]  [ 910/2809]  eta: 0:18:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3400 (4.2814)  class_acc: 0.1667 (0.1787)  loss_scale: 32768.0000 (63845.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0462  max mem: 15572
Epoch: [7]  [ 920/2809]  eta: 0:18:36  lr: 0.000046  min_lr: 0.000000  loss: 4.4192 (4.2823)  class_acc: 0.1667 (0.1785)  loss_scale: 32768.0000 (63508.0130)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0622  max mem: 15572
Epoch: [7]  [ 930/2809]  eta: 0:18:30  lr: 0.000046  min_lr: 0.000000  loss: 4.4836 (4.2828)  class_acc: 0.1667 (0.1779)  loss_scale: 32768.0000 (63177.8303)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0973  max mem: 15572
[2025-01-15 17:55:48,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:55:48,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 940/2809]  eta: 0:18:25  lr: 0.000046  min_lr: 0.000000  loss: 4.4452 (4.2844)  class_acc: 0.1250 (0.1776)  loss_scale: 32768.0000 (62924.3103)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1563  max mem: 15572
Epoch: [7]  [ 950/2809]  eta: 0:18:18  lr: 0.000046  min_lr: 0.000000  loss: 4.4452 (4.2853)  class_acc: 0.1250 (0.1776)  loss_scale: 65536.0000 (62951.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1542  max mem: 15572
Epoch: [7]  [ 960/2809]  eta: 0:18:11  lr: 0.000046  min_lr: 0.000000  loss: 4.4290 (4.2862)  class_acc: 0.1250 (0.1770)  loss_scale: 65536.0000 (62978.6639)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1295  max mem: 15572
Epoch: [7]  [ 970/2809]  eta: 0:18:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2605 (4.2863)  class_acc: 0.1250 (0.1770)  loss_scale: 65536.0000 (63005.0010)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0960  max mem: 15572
Epoch: [7]  [ 980/2809]  eta: 0:17:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2345 (4.2868)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (63030.8012)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0992  max mem: 15572
Epoch: [7]  [ 990/2809]  eta: 0:17:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2892 (4.2875)  class_acc: 0.1667 (0.1771)  loss_scale: 65536.0000 (63056.0807)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1684  max mem: 15572
Epoch: [7]  [1000/2809]  eta: 0:17:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2683 (4.2869)  class_acc: 0.1667 (0.1770)  loss_scale: 65536.0000 (63080.8551)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1335  max mem: 15572
Epoch: [7]  [1010/2809]  eta: 0:17:40  lr: 0.000046  min_lr: 0.000000  loss: 4.0261 (4.2844)  class_acc: 0.1250 (0.1771)  loss_scale: 65536.0000 (63105.1395)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.0974  max mem: 15572
Epoch: [7]  [1020/2809]  eta: 0:17:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2108 (4.2847)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (63128.9481)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1246  max mem: 15572
Epoch: [7]  [1030/2809]  eta: 0:17:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3454 (4.2852)  class_acc: 0.1667 (0.1770)  loss_scale: 65536.0000 (63152.2949)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.1088  max mem: 15572
Epoch: [7]  [1040/2809]  eta: 0:17:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3415 (4.2863)  class_acc: 0.1250 (0.1765)  loss_scale: 65536.0000 (63175.1931)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1254  max mem: 15572
Epoch: [7]  [1050/2809]  eta: 0:17:14  lr: 0.000046  min_lr: 0.000000  loss: 4.3368 (4.2862)  class_acc: 0.1250 (0.1763)  loss_scale: 65536.0000 (63197.6556)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1458  max mem: 15572
Epoch: [7]  [1060/2809]  eta: 0:17:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2994 (4.2862)  class_acc: 0.1667 (0.1767)  loss_scale: 65536.0000 (63219.6946)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1359  max mem: 15572
[2025-01-15 17:57:01,403] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:57:01,404] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:57:02,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20732
[2025-01-15 17:57:02,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:57:02,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1070/2809]  eta: 0:17:01  lr: 0.000046  min_lr: 0.000000  loss: 4.1469 (4.2847)  class_acc: 0.2917 (0.1773)  loss_scale: 65536.0000 (63363.7049)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.1132  max mem: 15572
Epoch: [7]  [1080/2809]  eta: 0:16:56  lr: 0.000046  min_lr: 0.000000  loss: 4.1469 (4.2846)  class_acc: 0.1667 (0.1773)  loss_scale: 65536.0000 (63383.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1363  max mem: 15572
Epoch: [7]  [1090/2809]  eta: 0:16:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3002 (4.2840)  class_acc: 0.1667 (0.1772)  loss_scale: 65536.0000 (63403.5270)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1779  max mem: 15572
Epoch: [7]  [1100/2809]  eta: 0:16:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2129 (4.2833)  class_acc: 0.1667 (0.1773)  loss_scale: 65536.0000 (63422.8955)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1671  max mem: 15572
Epoch: [7]  [1110/2809]  eta: 0:16:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2951 (4.2843)  class_acc: 0.1667 (0.1767)  loss_scale: 65536.0000 (63441.9154)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1325  max mem: 15572
Epoch: [7]  [1120/2809]  eta: 0:16:33  lr: 0.000046  min_lr: 0.000000  loss: 4.4666 (4.2854)  class_acc: 0.1250 (0.1762)  loss_scale: 65536.0000 (63460.5959)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1554  max mem: 15572
Epoch: [7]  [1130/2809]  eta: 0:16:28  lr: 0.000046  min_lr: 0.000000  loss: 4.3320 (4.2845)  class_acc: 0.1667 (0.1765)  loss_scale: 65536.0000 (63478.9461)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1896  max mem: 15572
Epoch: [7]  [1140/2809]  eta: 0:16:22  lr: 0.000046  min_lr: 0.000000  loss: 4.1836 (4.2830)  class_acc: 0.2083 (0.1767)  loss_scale: 65536.0000 (63496.9746)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1581  max mem: 15572
Epoch: [7]  [1150/2809]  eta: 0:16:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2195 (4.2833)  class_acc: 0.2083 (0.1768)  loss_scale: 65536.0000 (63514.6898)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1063  max mem: 15572
Epoch: [7]  [1160/2809]  eta: 0:16:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3579 (4.2841)  class_acc: 0.1667 (0.1766)  loss_scale: 65536.0000 (63532.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1883  max mem: 15572
Epoch: [7]  [1170/2809]  eta: 0:16:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4185 (4.2854)  class_acc: 0.1250 (0.1763)  loss_scale: 65536.0000 (63549.2126)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.2399  max mem: 15572
Epoch: [7]  [1180/2809]  eta: 0:16:00  lr: 0.000046  min_lr: 0.000000  loss: 4.4857 (4.2861)  class_acc: 0.1250 (0.1765)  loss_scale: 65536.0000 (63566.0356)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1623  max mem: 15572
Epoch: [7]  [1190/2809]  eta: 0:15:55  lr: 0.000046  min_lr: 0.000000  loss: 4.4125 (4.2855)  class_acc: 0.2083 (0.1767)  loss_scale: 65536.0000 (63582.5760)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1786  max mem: 15572
[2025-01-15 17:58:20,270] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:58:20,270] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1200/2809]  eta: 0:15:48  lr: 0.000046  min_lr: 0.000000  loss: 4.3253 (4.2867)  class_acc: 0.2083 (0.1768)  loss_scale: 65536.0000 (63762.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1183  max mem: 15572
[2025-01-15 17:58:23,812] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20868
[2025-01-15 17:58:23,813] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:58:23,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1210/2809]  eta: 0:15:42  lr: 0.000046  min_lr: 0.000000  loss: 4.3253 (4.2851)  class_acc: 0.2083 (0.1773)  loss_scale: 65536.0000 (63993.6581)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0969  max mem: 15572
Epoch: [7]  [1220/2809]  eta: 0:15:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3001 (4.2846)  class_acc: 0.2083 (0.1775)  loss_scale: 65536.0000 (64006.2899)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1184  max mem: 15572
Epoch: [7]  [1230/2809]  eta: 0:15:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3540 (4.2848)  class_acc: 0.1667 (0.1775)  loss_scale: 65536.0000 (64018.7165)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1464  max mem: 15572
Epoch: [7]  [1240/2809]  eta: 0:15:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2183 (4.2845)  class_acc: 0.1667 (0.1775)  loss_scale: 65536.0000 (64030.9428)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1675  max mem: 15572
Epoch: [7]  [1250/2809]  eta: 0:15:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2183 (4.2843)  class_acc: 0.1250 (0.1773)  loss_scale: 65536.0000 (64042.9736)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0894  max mem: 15572
Epoch: [7]  [1260/2809]  eta: 0:15:12  lr: 0.000046  min_lr: 0.000000  loss: 4.4318 (4.2860)  class_acc: 0.1250 (0.1772)  loss_scale: 65536.0000 (64054.8136)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1519  max mem: 15572
Epoch: [7]  [1270/2809]  eta: 0:15:06  lr: 0.000046  min_lr: 0.000000  loss: 4.4056 (4.2853)  class_acc: 0.2083 (0.1776)  loss_scale: 65536.0000 (64066.4673)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.2042  max mem: 15572
Epoch: [7]  [1280/2809]  eta: 0:15:01  lr: 0.000046  min_lr: 0.000000  loss: 4.1208 (4.2837)  class_acc: 0.2083 (0.1777)  loss_scale: 65536.0000 (64077.9391)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1672  max mem: 15572
Epoch: [7]  [1290/2809]  eta: 0:14:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4341 (4.2847)  class_acc: 0.1667 (0.1776)  loss_scale: 65536.0000 (64089.2332)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1059  max mem: 15572
Epoch: [7]  [1300/2809]  eta: 0:14:48  lr: 0.000046  min_lr: 0.000000  loss: 4.4341 (4.2845)  class_acc: 0.1667 (0.1778)  loss_scale: 65536.0000 (64100.3536)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1038  max mem: 15572
Epoch: [7]  [1310/2809]  eta: 0:14:42  lr: 0.000046  min_lr: 0.000000  loss: 4.1933 (4.2833)  class_acc: 0.2083 (0.1779)  loss_scale: 65536.0000 (64111.3043)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1466  max mem: 15572
Epoch: [7]  [1320/2809]  eta: 0:14:36  lr: 0.000046  min_lr: 0.000000  loss: 4.1702 (4.2823)  class_acc: 0.1667 (0.1779)  loss_scale: 65536.0000 (64122.0893)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1086  max mem: 15572
Epoch: [7]  [1330/2809]  eta: 0:14:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2393 (4.2826)  class_acc: 0.1667 (0.1779)  loss_scale: 65536.0000 (64132.7122)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0697  max mem: 15572
[2025-01-15 17:59:38,693] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 17:59:38,694] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 17:59:39,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=132, lr=[4.485894344622119e-07, 4.485894344622119e-07, 6.408420492317313e-07, 6.408420492317313e-07, 9.154886417596163e-07, 9.154886417596163e-07, 1.307840916799452e-06, 1.307840916799452e-06, 1.86834416685636e-06, 1.86834416685636e-06, 2.669063095509086e-06, 2.669063095509086e-06, 3.812947279298694e-06, 3.812947279298694e-06, 5.447067541855278e-06, 5.447067541855278e-06, 7.781525059793253e-06, 7.781525059793253e-06, 1.1116464371133221e-05, 1.1116464371133221e-05, 1.588066338733317e-05, 1.588066338733317e-05, 2.2686661981904537e-05, 2.2686661981904537e-05, 3.2409517117006484e-05, 3.2409517117006484e-05, 4.629931016715212e-05, 4.629931016715212e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 17:59:39,587] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=28.430622945773482, CurrSamplesPerSec=28.80526890809686, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [1340/2809]  eta: 0:14:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2415 (4.2820)  class_acc: 0.1667 (0.1782)  loss_scale: 65536.0000 (64485.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1377  max mem: 15572
[2025-01-15 17:59:48,508] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21012
[2025-01-15 17:59:48,508] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 17:59:48,508] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1350/2809]  eta: 0:14:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2399 (4.2809)  class_acc: 0.2083 (0.1784)  loss_scale: 131072.0000 (64881.1251)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1661  max mem: 15572
Epoch: [7]  [1360/2809]  eta: 0:14:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3056 (4.2804)  class_acc: 0.2083 (0.1785)  loss_scale: 65536.0000 (64885.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0927  max mem: 15572
Epoch: [7]  [1370/2809]  eta: 0:14:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3749 (4.2813)  class_acc: 0.1250 (0.1783)  loss_scale: 65536.0000 (64890.6783)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1490  max mem: 15572
Epoch: [7]  [1380/2809]  eta: 0:14:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3756 (4.2817)  class_acc: 0.1250 (0.1785)  loss_scale: 65536.0000 (64895.3512)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1344  max mem: 15572
Epoch: [7]  [1390/2809]  eta: 0:13:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3420 (4.2819)  class_acc: 0.1667 (0.1785)  loss_scale: 65536.0000 (64899.9569)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0983  max mem: 15572
Epoch: [7]  [1400/2809]  eta: 0:13:48  lr: 0.000046  min_lr: 0.000000  loss: 4.3420 (4.2812)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (64904.4968)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1423  max mem: 15572
Epoch: [7]  [1410/2809]  eta: 0:13:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2336 (4.2813)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (64908.9724)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1411  max mem: 15572
Epoch: [7]  [1420/2809]  eta: 0:13:36  lr: 0.000046  min_lr: 0.000000  loss: 4.2437 (4.2815)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (64913.3849)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1827  max mem: 15572
Epoch: [7]  [1430/2809]  eta: 0:13:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2437 (4.2806)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (64917.7358)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1350  max mem: 15572
Epoch: [7]  [1440/2809]  eta: 0:13:24  lr: 0.000046  min_lr: 0.000000  loss: 4.1896 (4.2802)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (64922.0264)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1252  max mem: 15572
Epoch: [7]  [1450/2809]  eta: 0:13:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2033 (4.2794)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (64926.2578)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1687  max mem: 15572
Epoch: [7]  [1460/2809]  eta: 0:13:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2424 (4.2784)  class_acc: 0.2083 (0.1797)  loss_scale: 65536.0000 (64930.4312)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1881  max mem: 15572
Epoch: [7]  [1470/2809]  eta: 0:13:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2424 (4.2788)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (64934.5479)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1614  max mem: 15572
[2025-01-15 18:01:02,922] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:01:02,922] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1480/2809]  eta: 0:13:03  lr: 0.000046  min_lr: 0.000000  loss: 4.2978 (4.2785)  class_acc: 0.1250 (0.1792)  loss_scale: 65536.0000 (65071.3626)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1739  max mem: 15572
[2025-01-15 18:01:09,233] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21149
[2025-01-15 18:01:09,234] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:01:09,234] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1490/2809]  eta: 0:12:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3259 (4.2787)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (65294.2508)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1735  max mem: 15572
Epoch: [7]  [1500/2809]  eta: 0:12:49  lr: 0.000046  min_lr: 0.000000  loss: 4.3259 (4.2784)  class_acc: 0.1667 (0.1794)  loss_scale: 65536.0000 (65295.8614)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0441  max mem: 15572
Epoch: [7]  [1510/2809]  eta: 0:12:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3897 (4.2798)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (65297.4507)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1136  max mem: 15572
Epoch: [7]  [1520/2809]  eta: 0:12:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2537 (4.2795)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (65299.0191)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1671  max mem: 15572
Epoch: [7]  [1530/2809]  eta: 0:12:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3727 (4.2807)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (65300.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1244  max mem: 15572
Epoch: [7]  [1540/2809]  eta: 0:12:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4637 (4.2810)  class_acc: 0.1250 (0.1788)  loss_scale: 65536.0000 (65302.0947)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1570  max mem: 15572
Epoch: [7]  [1550/2809]  eta: 0:12:20  lr: 0.000046  min_lr: 0.000000  loss: 4.4204 (4.2818)  class_acc: 0.1250 (0.1784)  loss_scale: 65536.0000 (65303.6028)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1824  max mem: 15572
Epoch: [7]  [1560/2809]  eta: 0:12:15  lr: 0.000046  min_lr: 0.000000  loss: 4.4422 (4.2821)  class_acc: 0.0833 (0.1783)  loss_scale: 65536.0000 (65305.0916)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.1934  max mem: 15572
Epoch: [7]  [1570/2809]  eta: 0:12:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2849 (4.2825)  class_acc: 0.1250 (0.1780)  loss_scale: 65536.0000 (65306.5614)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1363  max mem: 15572
Epoch: [7]  [1580/2809]  eta: 0:12:03  lr: 0.000046  min_lr: 0.000000  loss: 4.5008 (4.2836)  class_acc: 0.1250 (0.1777)  loss_scale: 65536.0000 (65308.0127)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1372  max mem: 15572
Epoch: [7]  [1590/2809]  eta: 0:11:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4512 (4.2836)  class_acc: 0.1250 (0.1777)  loss_scale: 65536.0000 (65309.4456)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1796  max mem: 15572
Epoch: [7]  [1600/2809]  eta: 0:11:51  lr: 0.000046  min_lr: 0.000000  loss: 4.3486 (4.2836)  class_acc: 0.1250 (0.1778)  loss_scale: 65536.0000 (65310.8607)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1286  max mem: 15572
Epoch: [7]  [1610/2809]  eta: 0:11:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2652 (4.2844)  class_acc: 0.1667 (0.1778)  loss_scale: 65536.0000 (65312.2582)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1373  max mem: 15572
[2025-01-15 18:02:24,793] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:02:24,793] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:02:25,205] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21279
[2025-01-15 18:02:25,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:02:25,206] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1620/2809]  eta: 0:11:39  lr: 0.000046  min_lr: 0.000000  loss: 4.4370 (4.2854)  class_acc: 0.1667 (0.1777)  loss_scale: 65536.0000 (65354.0679)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0966  max mem: 15572
Epoch: [7]  [1630/2809]  eta: 0:11:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3900 (4.2848)  class_acc: 0.1667 (0.1780)  loss_scale: 65536.0000 (65355.1833)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0983  max mem: 15572
Epoch: [7]  [1640/2809]  eta: 0:11:28  lr: 0.000046  min_lr: 0.000000  loss: 4.3296 (4.2851)  class_acc: 0.1667 (0.1777)  loss_scale: 65536.0000 (65356.2852)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1738  max mem: 15572
Epoch: [7]  [1650/2809]  eta: 0:11:22  lr: 0.000046  min_lr: 0.000000  loss: 4.0525 (4.2834)  class_acc: 0.1667 (0.1779)  loss_scale: 65536.0000 (65357.3737)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1589  max mem: 15572
Epoch: [7]  [1660/2809]  eta: 0:11:16  lr: 0.000046  min_lr: 0.000000  loss: 4.0504 (4.2823)  class_acc: 0.2083 (0.1782)  loss_scale: 65536.0000 (65358.4491)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1604  max mem: 15572
Epoch: [7]  [1670/2809]  eta: 0:11:09  lr: 0.000046  min_lr: 0.000000  loss: 4.1728 (4.2815)  class_acc: 0.2083 (0.1786)  loss_scale: 65536.0000 (65359.5117)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0905  max mem: 15572
Epoch: [7]  [1680/2809]  eta: 0:11:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2357 (4.2820)  class_acc: 0.1667 (0.1784)  loss_scale: 65536.0000 (65360.5616)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0774  max mem: 15572
Epoch: [7]  [1690/2809]  eta: 0:10:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2989 (4.2817)  class_acc: 0.1667 (0.1784)  loss_scale: 65536.0000 (65361.5991)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1742  max mem: 15572
Epoch: [7]  [1700/2809]  eta: 0:10:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2157 (4.2808)  class_acc: 0.2083 (0.1788)  loss_scale: 65536.0000 (65362.6243)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1959  max mem: 15572
Epoch: [7]  [1710/2809]  eta: 0:10:46  lr: 0.000046  min_lr: 0.000000  loss: 4.1723 (4.2805)  class_acc: 0.2083 (0.1790)  loss_scale: 65536.0000 (65363.6376)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1173  max mem: 15572
Epoch: [7]  [1720/2809]  eta: 0:10:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2229 (4.2806)  class_acc: 0.1667 (0.1789)  loss_scale: 65536.0000 (65364.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0306  max mem: 15572
Epoch: [7]  [1730/2809]  eta: 0:10:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3964 (4.2813)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (65365.6291)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0492  max mem: 15572
Epoch: [7]  [1740/2809]  eta: 0:10:28  lr: 0.000046  min_lr: 0.000000  loss: 4.4796 (4.2816)  class_acc: 0.1667 (0.1789)  loss_scale: 65536.0000 (65366.6077)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1289  max mem: 15572
[2025-01-15 18:03:39,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:03:39,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1750/2809]  eta: 0:10:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3931 (4.2813)  class_acc: 0.1250 (0.1786)  loss_scale: 65536.0000 (65592.1416)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1176  max mem: 15572
Epoch: [7]  [1760/2809]  eta: 0:10:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2018 (4.2816)  class_acc: 0.1667 (0.1787)  loss_scale: 131072.0000 (65963.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0659  max mem: 15572
[2025-01-15 18:03:48,760] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21425
[2025-01-15 18:03:48,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:03:48,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1770/2809]  eta: 0:10:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2557 (4.2816)  class_acc: 0.1667 (0.1786)  loss_scale: 131072.0000 (65998.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.1086  max mem: 15572
Epoch: [7]  [1780/2809]  eta: 0:10:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2876 (4.2813)  class_acc: 0.1667 (0.1788)  loss_scale: 65536.0000 (65995.9663)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1446  max mem: 15572
Epoch: [7]  [1790/2809]  eta: 0:09:58  lr: 0.000046  min_lr: 0.000000  loss: 4.4018 (4.2819)  class_acc: 0.1667 (0.1787)  loss_scale: 65536.0000 (65993.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1443  max mem: 15572
Epoch: [7]  [1800/2809]  eta: 0:09:52  lr: 0.000046  min_lr: 0.000000  loss: 4.3813 (4.2810)  class_acc: 0.1667 (0.1790)  loss_scale: 65536.0000 (65990.8584)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1525  max mem: 15572
Epoch: [7]  [1810/2809]  eta: 0:09:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3668 (4.2816)  class_acc: 0.2083 (0.1790)  loss_scale: 65536.0000 (65988.3468)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1708  max mem: 15572
Epoch: [7]  [1820/2809]  eta: 0:09:40  lr: 0.000046  min_lr: 0.000000  loss: 4.3668 (4.2811)  class_acc: 0.2083 (0.1793)  loss_scale: 65536.0000 (65985.8627)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1639  max mem: 15572
Epoch: [7]  [1830/2809]  eta: 0:09:34  lr: 0.000046  min_lr: 0.000000  loss: 4.0600 (4.2802)  class_acc: 0.2083 (0.1795)  loss_scale: 65536.0000 (65983.4058)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1390  max mem: 15572
Epoch: [7]  [1840/2809]  eta: 0:09:28  lr: 0.000046  min_lr: 0.000000  loss: 4.1601 (4.2799)  class_acc: 0.2083 (0.1796)  loss_scale: 65536.0000 (65980.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1046  max mem: 15572
Epoch: [7]  [1850/2809]  eta: 0:09:22  lr: 0.000046  min_lr: 0.000000  loss: 4.0381 (4.2790)  class_acc: 0.2083 (0.1798)  loss_scale: 65536.0000 (65978.5716)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0951  max mem: 15572
Epoch: [7]  [1860/2809]  eta: 0:09:16  lr: 0.000046  min_lr: 0.000000  loss: 4.1971 (4.2790)  class_acc: 0.2083 (0.1798)  loss_scale: 65536.0000 (65976.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1374  max mem: 15572
Epoch: [7]  [1870/2809]  eta: 0:09:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2974 (4.2792)  class_acc: 0.1667 (0.1796)  loss_scale: 65536.0000 (65973.8407)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1540  max mem: 15572
Epoch: [7]  [1880/2809]  eta: 0:09:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2881 (4.2785)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (65971.5130)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1257  max mem: 15572
Epoch: [7]  [1890/2809]  eta: 0:08:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2564 (4.2789)  class_acc: 0.1250 (0.1796)  loss_scale: 65536.0000 (65969.2099)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1304  max mem: 15572
[2025-01-15 18:05:04,739] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:05:04,739] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:05:08,781] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21561
[2025-01-15 18:05:08,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:05:08,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1900/2809]  eta: 0:08:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2479 (4.2785)  class_acc: 0.1250 (0.1797)  loss_scale: 65536.0000 (66208.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1258  max mem: 15572
Epoch: [7]  [1910/2809]  eta: 0:08:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2183 (4.2784)  class_acc: 0.2083 (0.1799)  loss_scale: 65536.0000 (66204.7347)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1168  max mem: 15572
Epoch: [7]  [1920/2809]  eta: 0:08:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2930 (4.2790)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (66201.2535)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1272  max mem: 15572
Epoch: [7]  [1930/2809]  eta: 0:08:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3176 (4.2784)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (66197.8084)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1083  max mem: 15572
Epoch: [7]  [1940/2809]  eta: 0:08:29  lr: 0.000046  min_lr: 0.000000  loss: 4.1315 (4.2777)  class_acc: 0.2083 (0.1801)  loss_scale: 65536.0000 (66194.3988)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1455  max mem: 15572
Epoch: [7]  [1950/2809]  eta: 0:08:23  lr: 0.000046  min_lr: 0.000000  loss: 3.9352 (4.2769)  class_acc: 0.2083 (0.1801)  loss_scale: 65536.0000 (66191.0241)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1665  max mem: 15572
Epoch: [7]  [1960/2809]  eta: 0:08:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2439 (4.2776)  class_acc: 0.1250 (0.1800)  loss_scale: 65536.0000 (66187.6838)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1693  max mem: 15572
Epoch: [7]  [1970/2809]  eta: 0:08:12  lr: 0.000046  min_lr: 0.000000  loss: 4.3774 (4.2778)  class_acc: 0.1250 (0.1799)  loss_scale: 65536.0000 (66184.3775)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1300  max mem: 15572
Epoch: [7]  [1980/2809]  eta: 0:08:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3129 (4.2780)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (66181.1045)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1460  max mem: 15572
Epoch: [7]  [1990/2809]  eta: 0:08:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2816 (4.2780)  class_acc: 0.1667 (0.1799)  loss_scale: 65536.0000 (66177.8644)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.2138  max mem: 15572
Epoch: [7]  [2000/2809]  eta: 0:07:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2287 (4.2771)  class_acc: 0.1667 (0.1800)  loss_scale: 65536.0000 (66174.6567)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1777  max mem: 15572
Epoch: [7]  [2010/2809]  eta: 0:07:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2731 (4.2774)  class_acc: 0.1250 (0.1799)  loss_scale: 65536.0000 (66171.4809)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1315  max mem: 15572
Epoch: [7]  [2020/2809]  eta: 0:07:43  lr: 0.000046  min_lr: 0.000000  loss: 4.4182 (4.2784)  class_acc: 0.1250 (0.1796)  loss_scale: 65536.0000 (66168.3365)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1227  max mem: 15572
[2025-01-15 18:06:24,771] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:06:24,771] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2030/2809]  eta: 0:07:37  lr: 0.000046  min_lr: 0.000000  loss: 4.5328 (4.2791)  class_acc: 0.0833 (0.1794)  loss_scale: 65536.0000 (66294.2944)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1318  max mem: 15572
[2025-01-15 18:06:31,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21700
[2025-01-15 18:06:31,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:06:31,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2040/2809]  eta: 0:07:31  lr: 0.000046  min_lr: 0.000000  loss: 4.3499 (4.2789)  class_acc: 0.1250 (0.1795)  loss_scale: 65536.0000 (66483.2376)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0997  max mem: 15572
Epoch: [7]  [2050/2809]  eta: 0:07:25  lr: 0.000046  min_lr: 0.000000  loss: 4.3319 (4.2793)  class_acc: 0.1250 (0.1794)  loss_scale: 65536.0000 (66478.6192)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1170  max mem: 15572
Epoch: [7]  [2060/2809]  eta: 0:07:19  lr: 0.000046  min_lr: 0.000000  loss: 4.3182 (4.2793)  class_acc: 0.1250 (0.1794)  loss_scale: 65536.0000 (66474.0456)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1657  max mem: 15572
Epoch: [7]  [2070/2809]  eta: 0:07:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2969 (4.2796)  class_acc: 0.1250 (0.1794)  loss_scale: 65536.0000 (66469.5162)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1710  max mem: 15572
Epoch: [7]  [2080/2809]  eta: 0:07:08  lr: 0.000046  min_lr: 0.000000  loss: 4.3239 (4.2794)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (66465.0303)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1825  max mem: 15572
Epoch: [7]  [2090/2809]  eta: 0:07:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2429 (4.2790)  class_acc: 0.1667 (0.1794)  loss_scale: 65536.0000 (66460.5873)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1217  max mem: 15572
Epoch: [7]  [2100/2809]  eta: 0:06:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3640 (4.2794)  class_acc: 0.1667 (0.1794)  loss_scale: 65536.0000 (66456.1866)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.0768  max mem: 15572
Epoch: [7]  [2110/2809]  eta: 0:06:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4467 (4.2794)  class_acc: 0.1667 (0.1794)  loss_scale: 65536.0000 (66451.8276)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1182  max mem: 15572
Epoch: [7]  [2120/2809]  eta: 0:06:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3292 (4.2794)  class_acc: 0.1667 (0.1794)  loss_scale: 65536.0000 (66447.5097)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1306  max mem: 15572
Epoch: [7]  [2130/2809]  eta: 0:06:38  lr: 0.000046  min_lr: 0.000000  loss: 4.1460 (4.2781)  class_acc: 0.1667 (0.1797)  loss_scale: 65536.0000 (66443.2323)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1057  max mem: 15572
Epoch: [7]  [2140/2809]  eta: 0:06:32  lr: 0.000046  min_lr: 0.000000  loss: 4.2667 (4.2792)  class_acc: 0.1667 (0.1795)  loss_scale: 65536.0000 (66438.9949)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0883  max mem: 15572
Epoch: [7]  [2150/2809]  eta: 0:06:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3694 (4.2795)  class_acc: 0.1250 (0.1794)  loss_scale: 65536.0000 (66434.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1274  max mem: 15572
Epoch: [7]  [2160/2809]  eta: 0:06:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3694 (4.2801)  class_acc: 0.1250 (0.1792)  loss_scale: 65536.0000 (66430.6377)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1105  max mem: 15572
[2025-01-15 18:07:48,037] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:07:48,037] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2170/2809]  eta: 0:06:15  lr: 0.000046  min_lr: 0.000000  loss: 4.3768 (4.2800)  class_acc: 0.1667 (0.1792)  loss_scale: 65536.0000 (66577.4519)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1910  max mem: 15572
[2025-01-15 18:07:50,403] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21834
[2025-01-15 18:07:50,403] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:07:50,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-15 18:07:50,809] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21835
[2025-01-15 18:07:50,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:07:50,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2180/2809]  eta: 0:06:09  lr: 0.000046  min_lr: 0.000000  loss: 4.2635 (4.2793)  class_acc: 0.1667 (0.1793)  loss_scale: 65536.0000 (66437.4580)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.2028  max mem: 15572
Epoch: [7]  [2190/2809]  eta: 0:06:03  lr: 0.000046  min_lr: 0.000000  loss: 4.1596 (4.2790)  class_acc: 0.1667 (0.1792)  loss_scale: 32768.0000 (66283.7864)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1276  max mem: 15572
Epoch: [7]  [2200/2809]  eta: 0:05:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2187 (4.2787)  class_acc: 0.1667 (0.1792)  loss_scale: 32768.0000 (66131.5111)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1157  max mem: 15572
Epoch: [7]  [2210/2809]  eta: 0:05:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2424 (4.2787)  class_acc: 0.2083 (0.1792)  loss_scale: 32768.0000 (65980.6133)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1010  max mem: 15572
Epoch: [7]  [2220/2809]  eta: 0:05:45  lr: 0.000046  min_lr: 0.000000  loss: 4.1716 (4.2778)  class_acc: 0.1667 (0.1791)  loss_scale: 32768.0000 (65831.0743)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1368  max mem: 15572
Epoch: [7]  [2230/2809]  eta: 0:05:40  lr: 0.000046  min_lr: 0.000000  loss: 3.9560 (4.2768)  class_acc: 0.1667 (0.1793)  loss_scale: 32768.0000 (65682.8758)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1562  max mem: 15572
Epoch: [7]  [2240/2809]  eta: 0:05:34  lr: 0.000046  min_lr: 0.000000  loss: 4.1881 (4.2768)  class_acc: 0.2083 (0.1794)  loss_scale: 32768.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1208  max mem: 15572
Epoch: [7]  [2250/2809]  eta: 0:05:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2407 (4.2761)  class_acc: 0.2083 (0.1795)  loss_scale: 32768.0000 (65390.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0358  max mem: 15572
Epoch: [7]  [2260/2809]  eta: 0:05:22  lr: 0.000046  min_lr: 0.000000  loss: 4.0925 (4.2757)  class_acc: 0.2083 (0.1798)  loss_scale: 32768.0000 (65246.1460)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0724  max mem: 15572
Epoch: [7]  [2270/2809]  eta: 0:05:16  lr: 0.000046  min_lr: 0.000000  loss: 4.0925 (4.2754)  class_acc: 0.2500 (0.1800)  loss_scale: 32768.0000 (65103.1334)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1257  max mem: 15572
Epoch: [7]  [2280/2809]  eta: 0:05:10  lr: 0.000046  min_lr: 0.000000  loss: 4.4128 (4.2761)  class_acc: 0.1667 (0.1799)  loss_scale: 32768.0000 (64961.3748)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1467  max mem: 15572
Epoch: [7]  [2290/2809]  eta: 0:05:04  lr: 0.000046  min_lr: 0.000000  loss: 4.3974 (4.2759)  class_acc: 0.2083 (0.1801)  loss_scale: 32768.0000 (64820.8538)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1237  max mem: 15572
Epoch: [7]  [2300/2809]  eta: 0:04:58  lr: 0.000046  min_lr: 0.000000  loss: 4.2320 (4.2755)  class_acc: 0.2083 (0.1801)  loss_scale: 32768.0000 (64681.5541)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1209  max mem: 15572
[2025-01-15 18:09:04,526] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:09:04,526] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [2310/2809]  eta: 0:04:52  lr: 0.000046  min_lr: 0.000000  loss: 4.0946 (4.2757)  class_acc: 0.1667 (0.1798)  loss_scale: 32768.0000 (64685.2514)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1906  max mem: 15572
Epoch: [7]  [2320/2809]  eta: 0:04:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3998 (4.2760)  class_acc: 0.0833 (0.1798)  loss_scale: 65536.0000 (64688.9168)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1472  max mem: 15572
Epoch: [7]  [2330/2809]  eta: 0:04:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2777 (4.2760)  class_acc: 0.1667 (0.1799)  loss_scale: 65536.0000 (64692.5508)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1456  max mem: 15572
[2025-01-15 18:09:24,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=140, lr=[4.4687915103572624e-07, 4.4687915103572624e-07, 6.383987871938948e-07, 6.383987871938948e-07, 9.119982674198497e-07, 9.119982674198497e-07, 1.3028546677426425e-06, 1.3028546677426425e-06, 1.8612209539180608e-06, 1.8612209539180608e-06, 2.658887077025801e-06, 2.658887077025801e-06, 3.798410110036859e-06, 3.798410110036859e-06, 5.4263001571955135e-06, 5.4263001571955135e-06, 7.751857367422162e-06, 7.751857367422162e-06, 1.1074081953460234e-05, 1.1074081953460234e-05, 1.582011707637176e-05, 1.582011707637176e-05, 2.260016725195966e-05, 2.260016725195966e-05, 3.2285953217085234e-05, 3.2285953217085234e-05, 4.6122790310121765e-05, 4.6122790310121765e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:09:24,187] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=28.42588102293684, CurrSamplesPerSec=32.269255132265634, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [7]  [2340/2809]  eta: 0:04:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2415 (4.2759)  class_acc: 0.2083 (0.1799)  loss_scale: 65536.0000 (64696.1538)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1294  max mem: 15572
Epoch: [7]  [2350/2809]  eta: 0:04:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4138 (4.2765)  class_acc: 0.1667 (0.1798)  loss_scale: 65536.0000 (64699.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1483  max mem: 15572
Epoch: [7]  [2360/2809]  eta: 0:04:23  lr: 0.000046  min_lr: 0.000000  loss: 4.1589 (4.2758)  class_acc: 0.2083 (0.1800)  loss_scale: 65536.0000 (64703.2681)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1975  max mem: 15572
Epoch: [7]  [2370/2809]  eta: 0:04:17  lr: 0.000046  min_lr: 0.000000  loss: 4.1109 (4.2747)  class_acc: 0.2083 (0.1802)  loss_scale: 65536.0000 (64706.7803)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1177  max mem: 15572
Epoch: [7]  [2380/2809]  eta: 0:04:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2943 (4.2750)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (64710.2629)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0571  max mem: 15572
Epoch: [7]  [2390/2809]  eta: 0:04:05  lr: 0.000046  min_lr: 0.000000  loss: 4.3675 (4.2750)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (64713.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0950  max mem: 15572
Epoch: [7]  [2400/2809]  eta: 0:03:59  lr: 0.000046  min_lr: 0.000000  loss: 4.3675 (4.2753)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (64717.1412)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.1973  max mem: 15572
Epoch: [7]  [2410/2809]  eta: 0:03:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3758 (4.2757)  class_acc: 0.1250 (0.1801)  loss_scale: 65536.0000 (64720.5375)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.2453  max mem: 15572
Epoch: [7]  [2420/2809]  eta: 0:03:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1934 (4.2748)  class_acc: 0.1667 (0.1803)  loss_scale: 65536.0000 (64723.9058)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1856  max mem: 15572
[2025-01-15 18:10:18,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:10:18,830] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2430/2809]  eta: 0:03:42  lr: 0.000046  min_lr: 0.000000  loss: 4.1359 (4.2748)  class_acc: 0.1667 (0.1801)  loss_scale: 65536.0000 (64781.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1480  max mem: 15572
[2025-01-15 18:10:21,904] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22095
[2025-01-15 18:10:21,905] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:10:21,905] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2440/2809]  eta: 0:03:36  lr: 0.000046  min_lr: 0.000000  loss: 4.1808 (4.2745)  class_acc: 0.1667 (0.1803)  loss_scale: 65536.0000 (64811.1036)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1779  max mem: 15572
Epoch: [7]  [2450/2809]  eta: 0:03:30  lr: 0.000046  min_lr: 0.000000  loss: 4.1469 (4.2737)  class_acc: 0.2083 (0.1803)  loss_scale: 65536.0000 (64814.0612)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1302  max mem: 15572
Epoch: [7]  [2460/2809]  eta: 0:03:24  lr: 0.000046  min_lr: 0.000000  loss: 3.9539 (4.2728)  class_acc: 0.2083 (0.1805)  loss_scale: 65536.0000 (64816.9947)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0816  max mem: 15572
Epoch: [7]  [2470/2809]  eta: 0:03:18  lr: 0.000046  min_lr: 0.000000  loss: 4.1056 (4.2729)  class_acc: 0.1667 (0.1804)  loss_scale: 65536.0000 (64819.9045)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1217  max mem: 15572
Epoch: [7]  [2480/2809]  eta: 0:03:13  lr: 0.000046  min_lr: 0.000000  loss: 4.1890 (4.2723)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (64822.7908)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1418  max mem: 15572
Epoch: [7]  [2490/2809]  eta: 0:03:07  lr: 0.000046  min_lr: 0.000000  loss: 4.2310 (4.2724)  class_acc: 0.1667 (0.1805)  loss_scale: 65536.0000 (64825.6540)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0830  max mem: 15572
Epoch: [7]  [2500/2809]  eta: 0:03:01  lr: 0.000046  min_lr: 0.000000  loss: 4.2512 (4.2715)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (64828.4942)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1016  max mem: 15572
Epoch: [7]  [2510/2809]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2642 (4.2711)  class_acc: 0.1250 (0.1805)  loss_scale: 65536.0000 (64831.3118)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0985  max mem: 15572
Epoch: [7]  [2520/2809]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000000  loss: 4.3685 (4.2716)  class_acc: 0.1250 (0.1803)  loss_scale: 65536.0000 (64834.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0583  max mem: 15572
Epoch: [7]  [2530/2809]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000000  loss: 4.2951 (4.2712)  class_acc: 0.1667 (0.1803)  loss_scale: 65536.0000 (64836.8803)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1480  max mem: 15572
Epoch: [7]  [2540/2809]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000000  loss: 4.2489 (4.2710)  class_acc: 0.1667 (0.1803)  loss_scale: 65536.0000 (64839.6316)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.2184  max mem: 15572
Epoch: [7]  [2550/2809]  eta: 0:02:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2675 (4.2704)  class_acc: 0.2083 (0.1805)  loss_scale: 65536.0000 (64842.3614)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1400  max mem: 15572
Epoch: [7]  [2560/2809]  eta: 0:02:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2860 (4.2707)  class_acc: 0.1667 (0.1803)  loss_scale: 65536.0000 (64845.0699)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1092  max mem: 15572
[2025-01-15 18:11:37,076] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:11:37,076] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:11:39,213] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22228
[2025-01-15 18:11:39,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:11:39,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2570/2809]  eta: 0:02:20  lr: 0.000046  min_lr: 0.000000  loss: 4.2635 (4.2702)  class_acc: 0.1667 (0.1805)  loss_scale: 65536.0000 (64949.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1502  max mem: 15572
Epoch: [7]  [2580/2809]  eta: 0:02:14  lr: 0.000046  min_lr: 0.000000  loss: 4.2384 (4.2696)  class_acc: 0.1667 (0.1807)  loss_scale: 65536.0000 (64951.9907)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0891  max mem: 15572
Epoch: [7]  [2590/2809]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2593 (4.2700)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (64954.2447)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1071  max mem: 15572
Epoch: [7]  [2600/2809]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000000  loss: 4.4008 (4.2708)  class_acc: 0.1667 (0.1804)  loss_scale: 65536.0000 (64956.4814)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1141  max mem: 15572
Epoch: [7]  [2610/2809]  eta: 0:01:56  lr: 0.000046  min_lr: 0.000000  loss: 4.4117 (4.2716)  class_acc: 0.1250 (0.1803)  loss_scale: 65536.0000 (64958.7009)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1125  max mem: 15572
Epoch: [7]  [2620/2809]  eta: 0:01:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3990 (4.2720)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (64960.9035)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1463  max mem: 15572
Epoch: [7]  [2630/2809]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000000  loss: 4.2568 (4.2716)  class_acc: 0.1667 (0.1802)  loss_scale: 65536.0000 (64963.0893)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1808  max mem: 15572
Epoch: [7]  [2640/2809]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000000  loss: 4.1900 (4.2712)  class_acc: 0.2083 (0.1803)  loss_scale: 65536.0000 (64965.2586)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1620  max mem: 15572
Epoch: [7]  [2650/2809]  eta: 0:01:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2283 (4.2712)  class_acc: 0.2083 (0.1803)  loss_scale: 65536.0000 (64967.4115)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0781  max mem: 15572
Epoch: [7]  [2660/2809]  eta: 0:01:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2959 (4.2709)  class_acc: 0.1667 (0.1804)  loss_scale: 65536.0000 (64969.5483)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1105  max mem: 15572
Epoch: [7]  [2670/2809]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2234 (4.2708)  class_acc: 0.1667 (0.1805)  loss_scale: 65536.0000 (64971.6690)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1314  max mem: 15572
Epoch: [7]  [2680/2809]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000000  loss: 4.1610 (4.2706)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (64973.7740)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1190  max mem: 15572
Epoch: [7]  [2690/2809]  eta: 0:01:09  lr: 0.000046  min_lr: 0.000000  loss: 4.1613 (4.2705)  class_acc: 0.2083 (0.1807)  loss_scale: 65536.0000 (64975.8632)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1503  max mem: 15572
[2025-01-15 18:12:52,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:12:52,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [2700/2809]  eta: 0:01:03  lr: 0.000046  min_lr: 0.000000  loss: 4.4147 (4.2713)  class_acc: 0.1667 (0.1805)  loss_scale: 65536.0000 (65147.7823)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1282  max mem: 15572
[2025-01-15 18:12:59,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22369
[2025-01-15 18:12:59,806] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:12:59,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [2710/2809]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3219 (4.2711)  class_acc: 0.1250 (0.1805)  loss_scale: 131072.0000 (65270.0848)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1116  max mem: 15572
Epoch: [7]  [2720/2809]  eta: 0:00:52  lr: 0.000046  min_lr: 0.000000  loss: 4.2893 (4.2714)  class_acc: 0.1250 (0.1804)  loss_scale: 65536.0000 (65271.0621)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1244  max mem: 15572
Epoch: [7]  [2730/2809]  eta: 0:00:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2893 (4.2710)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (65272.0322)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0911  max mem: 15572
Epoch: [7]  [2740/2809]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2649 (4.2713)  class_acc: 0.1667 (0.1806)  loss_scale: 65536.0000 (65272.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.0972  max mem: 15572
[2025-01-15 18:13:21,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22407
[2025-01-15 18:13:21,065] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:13:21,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [2750/2809]  eta: 0:00:34  lr: 0.000046  min_lr: 0.000000  loss: 4.3889 (4.2715)  class_acc: 0.1667 (0.1807)  loss_scale: 65536.0000 (65190.5722)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1662  max mem: 15572
Epoch: [7]  [2760/2809]  eta: 0:00:28  lr: 0.000046  min_lr: 0.000000  loss: 4.2872 (4.2715)  class_acc: 0.2083 (0.1806)  loss_scale: 32768.0000 (65073.1416)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1519  max mem: 15572
Epoch: [7]  [2770/2809]  eta: 0:00:22  lr: 0.000046  min_lr: 0.000000  loss: 4.3969 (4.2720)  class_acc: 0.1667 (0.1806)  loss_scale: 32768.0000 (64956.5586)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1348  max mem: 15572
Epoch: [7]  [2780/2809]  eta: 0:00:16  lr: 0.000046  min_lr: 0.000000  loss: 4.4484 (4.2726)  class_acc: 0.1250 (0.1806)  loss_scale: 32768.0000 (64840.8141)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1647  max mem: 15572
Epoch: [7]  [2790/2809]  eta: 0:00:11  lr: 0.000046  min_lr: 0.000000  loss: 4.3928 (4.2728)  class_acc: 0.1250 (0.1806)  loss_scale: 32768.0000 (64725.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0756  max mem: 15572
Epoch: [7]  [2800/2809]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1959 (4.2726)  class_acc: 0.1667 (0.1808)  loss_scale: 32768.0000 (64611.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0408  max mem: 15572
Epoch: [7]  [2808/2809]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2039 (4.2729)  class_acc: 0.1667 (0.1806)  loss_scale: 32768.0000 (64521.1136)  weight_decay: 0.0500 (0.0500)  time: 0.4710  data: 0.0405  max mem: 15572
Epoch: [7] Total time: 0:27:23 (0.5851 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000000  loss: 4.2039 (4.2729)  class_acc: 0.1667 (0.1806)  loss_scale: 32768.0000 (64521.1136)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:38  loss: 0.5047 (0.5047)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.2165  data: 5.0247  max mem: 15572
Val:  [ 10/272]  eta: 0:03:32  loss: 4.0946 (3.4487)  acc1: 0.0000 (24.7475)  acc5: 11.1111 (31.3131)  time: 0.8121  data: 0.6287  max mem: 15572
Val:  [ 20/272]  eta: 0:02:27  loss: 3.6243 (3.4278)  acc1: 11.1111 (22.7513)  acc5: 38.8889 (41.5344)  time: 0.3525  data: 0.1620  max mem: 15572
Val:  [ 30/272]  eta: 0:01:55  loss: 3.6243 (3.5688)  acc1: 5.5556 (16.6667)  acc5: 44.4444 (41.0394)  time: 0.2921  data: 0.0862  max mem: 15572
Val:  [ 40/272]  eta: 0:01:45  loss: 3.4906 (3.5313)  acc1: 5.5556 (16.3957)  acc5: 50.0000 (43.7669)  time: 0.3210  data: 0.1081  max mem: 15572
Val:  [ 50/272]  eta: 0:01:36  loss: 3.3484 (3.4765)  acc1: 11.1111 (16.9935)  acc5: 55.5556 (46.1874)  time: 0.3680  data: 0.1550  max mem: 15572
Val:  [ 60/272]  eta: 0:01:27  loss: 2.2203 (3.3052)  acc1: 38.8889 (23.6794)  acc5: 72.2222 (49.7268)  time: 0.3253  data: 0.1020  max mem: 15572
Val:  [ 70/272]  eta: 0:01:20  loss: 2.2203 (3.2017)  acc1: 50.0000 (25.3521)  acc5: 72.2222 (54.1471)  time: 0.3020  data: 0.0818  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 2.7994 (3.1855)  acc1: 27.7778 (26.2689)  acc5: 72.2222 (54.1152)  time: 0.3117  data: 0.0988  max mem: 15572
Val:  [ 90/272]  eta: 0:01:10  loss: 3.8839 (3.2662)  acc1: 5.5556 (23.9316)  acc5: 33.3333 (51.5263)  time: 0.3503  data: 0.1410  max mem: 15572
Val:  [100/272]  eta: 0:01:06  loss: 3.8839 (3.3311)  acc1: 5.5556 (23.1023)  acc5: 33.3333 (50.2200)  time: 0.3672  data: 0.1666  max mem: 15572
Val:  [110/272]  eta: 0:01:00  loss: 3.8370 (3.3902)  acc1: 5.5556 (21.3213)  acc5: 38.8889 (48.5986)  time: 0.3200  data: 0.1218  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.8370 (3.4292)  acc1: 5.5556 (20.2020)  acc5: 38.8889 (48.0716)  time: 0.2657  data: 0.0659  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.6720 (3.3740)  acc1: 11.1111 (22.3494)  acc5: 44.4444 (49.0246)  time: 0.2807  data: 0.0761  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 3.0906 (3.3601)  acc1: 27.7778 (23.0496)  acc5: 55.5556 (49.2514)  time: 0.3374  data: 0.1440  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.3187 (3.3612)  acc1: 11.1111 (22.2958)  acc5: 61.1111 (50.0000)  time: 0.3404  data: 0.1542  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.2491 (3.3309)  acc1: 22.2222 (23.4645)  acc5: 66.6667 (51.4493)  time: 0.3089  data: 0.1094  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 3.3735 (3.3626)  acc1: 22.2222 (22.5796)  acc5: 55.5556 (50.8447)  time: 0.3184  data: 0.1093  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.4514 (3.3551)  acc1: 5.5556 (22.0381)  acc5: 55.5556 (51.7495)  time: 0.3496  data: 0.1348  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.4202 (3.3866)  acc1: 11.1111 (21.6987)  acc5: 55.5556 (50.9308)  time: 0.3279  data: 0.1185  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.1305 (3.3817)  acc1: 5.5556 (21.7523)  acc5: 61.1111 (51.6031)  time: 0.3013  data: 0.1084  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.9592 (3.3916)  acc1: 11.1111 (21.9326)  acc5: 72.2222 (51.6588)  time: 0.2477  data: 0.0684  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.4827 (3.3805)  acc1: 22.2222 (22.4233)  acc5: 55.5556 (52.0865)  time: 0.2168  data: 0.0408  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.5633 (3.3465)  acc1: 38.8889 (23.8336)  acc5: 66.6667 (53.0784)  time: 0.2289  data: 0.0487  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.3594 (3.3209)  acc1: 55.5556 (24.7118)  acc5: 77.7778 (53.9650)  time: 0.2653  data: 0.0775  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 3.0080 (3.3467)  acc1: 16.6667 (24.3471)  acc5: 61.1111 (53.3865)  time: 0.3150  data: 0.1208  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.7504 (3.2698)  acc1: 55.5556 (26.7348)  acc5: 72.2222 (54.8106)  time: 0.3472  data: 0.1566  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.1945 (3.2716)  acc1: 55.5556 (26.5273)  acc5: 77.7778 (54.6125)  time: 0.3090  data: 0.1353  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.1945 (3.2762)  acc1: 55.5556 (26.5206)  acc5: 77.7778 (54.5771)  time: 0.3027  data: 0.1353  max mem: 15572
Val: Total time: 0:01:29 (0.3289 s / it)
* Acc@1 26.521 Acc@5 54.577 loss 3.276
Accuracy of the network on the 4883 val videos: 26.5%
[2025-01-15 18:15:26,492] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 18:15:26,496] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 18:15:26,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 18:15:29,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 18:15:29,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 26.52%
Epoch: [8]  [   0/2809]  eta: 8:20:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2531 (4.2531)  class_acc: 0.1250 (0.1250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 10.6912  data: 10.1833  max mem: 15572
Epoch: [8]  [  10/2809]  eta: 1:16:57  lr: 0.000046  min_lr: 0.000000  loss: 4.0881 (4.1865)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6496  data: 1.1655  max mem: 15572
Epoch: [8]  [  20/2809]  eta: 0:51:15  lr: 0.000046  min_lr: 0.000000  loss: 4.0881 (4.1950)  class_acc: 0.1667 (0.1845)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1499  max mem: 15572
Epoch: [8]  [  30/2809]  eta: 0:45:51  lr: 0.000046  min_lr: 0.000000  loss: 4.2957 (4.2850)  class_acc: 0.1667 (0.1653)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1540  max mem: 15572
Epoch: [8]  [  40/2809]  eta: 0:42:48  lr: 0.000046  min_lr: 0.000000  loss: 4.3175 (4.2915)  class_acc: 0.1667 (0.1657)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7437  data: 0.2773  max mem: 15572
Epoch: [8]  [  50/2809]  eta: 0:39:42  lr: 0.000046  min_lr: 0.000000  loss: 4.3288 (4.2970)  class_acc: 0.1250 (0.1642)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.2273  max mem: 15572
Epoch: [8]  [  60/2809]  eta: 0:37:50  lr: 0.000046  min_lr: 0.000000  loss: 4.3288 (4.2937)  class_acc: 0.1250 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1799  max mem: 15572
[2025-01-15 18:16:23,919] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:16:23,920] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [  70/2809]  eta: 0:36:36  lr: 0.000046  min_lr: 0.000000  loss: 4.2652 (4.2588)  class_acc: 0.1667 (0.1743)  loss_scale: 32768.0000 (35998.6479)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1831  max mem: 15572
Epoch: [8]  [  80/2809]  eta: 0:35:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2652 (4.2628)  class_acc: 0.2083 (0.1780)  loss_scale: 65536.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1831  max mem: 15572
Epoch: [8]  [  90/2809]  eta: 0:34:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2788 (4.2735)  class_acc: 0.2083 (0.1790)  loss_scale: 65536.0000 (42490.3736)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1713  max mem: 15572
Epoch: [8]  [ 100/2809]  eta: 0:32:38  lr: 0.000046  min_lr: 0.000000  loss: 4.3045 (4.2750)  class_acc: 0.1667 (0.1811)  loss_scale: 65536.0000 (44772.1188)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0776  max mem: 15572
Epoch: [8]  [ 110/2809]  eta: 0:31:23  lr: 0.000046  min_lr: 0.000000  loss: 4.1680 (4.2553)  class_acc: 0.2083 (0.1862)  loss_scale: 65536.0000 (46642.7387)  weight_decay: 0.0500 (0.0500)  time: 0.4362  data: 0.0005  max mem: 15572
Epoch: [8]  [ 120/2809]  eta: 0:30:22  lr: 0.000046  min_lr: 0.000000  loss: 3.9783 (4.2252)  class_acc: 0.2500 (0.1970)  loss_scale: 65536.0000 (48204.1653)  weight_decay: 0.0500 (0.0500)  time: 0.4503  data: 0.0007  max mem: 15572
Epoch: [8]  [ 130/2809]  eta: 0:29:30  lr: 0.000046  min_lr: 0.000000  loss: 4.0495 (4.2218)  class_acc: 0.2500 (0.1982)  loss_scale: 65536.0000 (49527.2061)  weight_decay: 0.0500 (0.0500)  time: 0.4569  data: 0.0010  max mem: 15572
Epoch: [8]  [ 140/2809]  eta: 0:29:12  lr: 0.000046  min_lr: 0.000000  loss: 4.1275 (4.2224)  class_acc: 0.2083 (0.2007)  loss_scale: 65536.0000 (50662.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0847  max mem: 15572
Epoch: [8]  [ 150/2809]  eta: 0:28:54  lr: 0.000046  min_lr: 0.000000  loss: 4.1524 (4.2273)  class_acc: 0.2083 (0.2001)  loss_scale: 65536.0000 (51647.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1526  max mem: 15572
Epoch: [8]  [ 160/2809]  eta: 0:28:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3173 (4.2357)  class_acc: 0.1667 (0.1972)  loss_scale: 65536.0000 (52510.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1665  max mem: 15572
Epoch: [8]  [ 170/2809]  eta: 0:28:41  lr: 0.000046  min_lr: 0.000000  loss: 4.3173 (4.2379)  class_acc: 0.1667 (0.1971)  loss_scale: 65536.0000 (53271.9532)  weight_decay: 0.0500 (0.0500)  time: 0.6537  data: 0.2144  max mem: 15572
Epoch: [8]  [ 180/2809]  eta: 0:28:27  lr: 0.000046  min_lr: 0.000000  loss: 4.1408 (4.2293)  class_acc: 0.2500 (0.1996)  loss_scale: 65536.0000 (53949.5249)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1937  max mem: 15572
Epoch: [8]  [ 190/2809]  eta: 0:27:58  lr: 0.000046  min_lr: 0.000000  loss: 4.1520 (4.2362)  class_acc: 0.2083 (0.1985)  loss_scale: 65536.0000 (54556.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1014  max mem: 15572
[2025-01-15 18:17:33,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:17:33,351] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 200/2809]  eta: 0:27:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2366 (4.2337)  class_acc: 0.1667 (0.1984)  loss_scale: 65536.0000 (58036.8557)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1085  max mem: 15572
[2025-01-15 18:17:41,138] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22678
[2025-01-15 18:17:41,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:17:41,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 210/2809]  eta: 0:27:29  lr: 0.000046  min_lr: 0.000000  loss: 4.1371 (4.2249)  class_acc: 0.1667 (0.1992)  loss_scale: 131072.0000 (59945.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1369  max mem: 15572
Epoch: [8]  [ 220/2809]  eta: 0:27:24  lr: 0.000046  min_lr: 0.000000  loss: 4.0546 (4.2195)  class_acc: 0.2083 (0.1993)  loss_scale: 65536.0000 (60198.2262)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1485  max mem: 15572
Epoch: [8]  [ 230/2809]  eta: 0:27:15  lr: 0.000046  min_lr: 0.000000  loss: 4.1608 (4.2143)  class_acc: 0.2083 (0.2004)  loss_scale: 65536.0000 (60429.2987)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1893  max mem: 15572
Epoch: [8]  [ 240/2809]  eta: 0:26:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2050 (4.2179)  class_acc: 0.1667 (0.1976)  loss_scale: 65536.0000 (60641.1950)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.1009  max mem: 15572
Epoch: [8]  [ 250/2809]  eta: 0:26:42  lr: 0.000046  min_lr: 0.000000  loss: 4.3006 (4.2181)  class_acc: 0.1250 (0.1962)  loss_scale: 65536.0000 (60836.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0892  max mem: 15572
Epoch: [8]  [ 260/2809]  eta: 0:26:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2317 (4.2196)  class_acc: 0.1667 (0.1968)  loss_scale: 65536.0000 (61016.2759)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1384  max mem: 15572
Epoch: [8]  [ 270/2809]  eta: 0:26:27  lr: 0.000046  min_lr: 0.000000  loss: 4.2420 (4.2195)  class_acc: 0.1667 (0.1957)  loss_scale: 65536.0000 (61183.0554)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1706  max mem: 15572
Epoch: [8]  [ 280/2809]  eta: 0:26:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2661 (4.2239)  class_acc: 0.1667 (0.1954)  loss_scale: 65536.0000 (61337.9644)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1700  max mem: 15572
Epoch: [8]  [ 290/2809]  eta: 0:26:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2651 (4.2268)  class_acc: 0.2083 (0.1940)  loss_scale: 65536.0000 (61482.2268)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1330  max mem: 15572
Epoch: [8]  [ 300/2809]  eta: 0:25:56  lr: 0.000046  min_lr: 0.000000  loss: 4.0970 (4.2180)  class_acc: 0.2083 (0.1963)  loss_scale: 65536.0000 (61616.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1498  max mem: 15572
Epoch: [8]  [ 310/2809]  eta: 0:25:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2145 (4.2201)  class_acc: 0.2083 (0.1969)  loss_scale: 65536.0000 (61742.9196)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1365  max mem: 15572
Epoch: [8]  [ 320/2809]  eta: 0:25:35  lr: 0.000046  min_lr: 0.000000  loss: 4.3011 (4.2196)  class_acc: 0.1667 (0.1970)  loss_scale: 65536.0000 (61861.0841)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1240  max mem: 15572
Epoch: [8]  [ 330/2809]  eta: 0:25:25  lr: 0.000046  min_lr: 0.000000  loss: 4.3310 (4.2247)  class_acc: 0.1250 (0.1954)  loss_scale: 65536.0000 (61972.1088)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1297  max mem: 15572
[2025-01-15 18:18:56,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:18:56,278] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:19:00,078] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22810
[2025-01-15 18:19:00,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:19:00,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 340/2809]  eta: 0:25:26  lr: 0.000046  min_lr: 0.000000  loss: 4.3917 (4.2246)  class_acc: 0.1250 (0.1954)  loss_scale: 65536.0000 (62653.1848)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1977  max mem: 15572
Epoch: [8]  [ 350/2809]  eta: 0:25:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2839 (4.2235)  class_acc: 0.1667 (0.1948)  loss_scale: 65536.0000 (62735.3162)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1750  max mem: 15572
Epoch: [8]  [ 360/2809]  eta: 0:25:12  lr: 0.000046  min_lr: 0.000000  loss: 4.1731 (4.2199)  class_acc: 0.1667 (0.1964)  loss_scale: 65536.0000 (62812.8975)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1504  max mem: 15572
Epoch: [8]  [ 370/2809]  eta: 0:25:04  lr: 0.000046  min_lr: 0.000000  loss: 4.1731 (4.2181)  class_acc: 0.2083 (0.1965)  loss_scale: 65536.0000 (62886.2965)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1738  max mem: 15572
Epoch: [8]  [ 380/2809]  eta: 0:24:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2569 (4.2211)  class_acc: 0.1667 (0.1960)  loss_scale: 65536.0000 (62955.8425)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1205  max mem: 15572
Epoch: [8]  [ 390/2809]  eta: 0:24:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3571 (4.2234)  class_acc: 0.1667 (0.1954)  loss_scale: 65536.0000 (63021.8312)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1283  max mem: 15572
Epoch: [8]  [ 400/2809]  eta: 0:24:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1429 (4.2209)  class_acc: 0.2083 (0.1951)  loss_scale: 65536.0000 (63084.5287)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1843  max mem: 15572
Epoch: [8]  [ 410/2809]  eta: 0:24:36  lr: 0.000046  min_lr: 0.000000  loss: 4.0793 (4.2160)  class_acc: 0.2083 (0.1964)  loss_scale: 65536.0000 (63144.1752)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.1918  max mem: 15572
Epoch: [8]  [ 420/2809]  eta: 0:24:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1408 (4.2183)  class_acc: 0.2500 (0.1966)  loss_scale: 65536.0000 (63200.9881)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0955  max mem: 15572
Epoch: [8]  [ 430/2809]  eta: 0:24:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2476 (4.2176)  class_acc: 0.1667 (0.1962)  loss_scale: 65536.0000 (63255.1647)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1394  max mem: 15572
Epoch: [8]  [ 440/2809]  eta: 0:24:10  lr: 0.000046  min_lr: 0.000000  loss: 4.3288 (4.2220)  class_acc: 0.1667 (0.1948)  loss_scale: 65536.0000 (63306.8844)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1810  max mem: 15572
Epoch: [8]  [ 450/2809]  eta: 0:23:57  lr: 0.000046  min_lr: 0.000000  loss: 4.3408 (4.2225)  class_acc: 0.1250 (0.1948)  loss_scale: 65536.0000 (63356.3104)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0604  max mem: 15572
Epoch: [8]  [ 460/2809]  eta: 0:23:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3056 (4.2232)  class_acc: 0.1667 (0.1945)  loss_scale: 65536.0000 (63403.5922)  weight_decay: 0.0500 (0.0500)  time: 0.4781  data: 0.0196  max mem: 15572
[2025-01-15 18:20:12,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:20:12,890] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:20:15,334] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22940
[2025-01-15 18:20:15,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:20:15,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 470/2809]  eta: 0:23:41  lr: 0.000046  min_lr: 0.000000  loss: 4.2443 (4.2239)  class_acc: 0.1667 (0.1937)  loss_scale: 65536.0000 (63588.0085)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1139  max mem: 15572
Epoch: [8]  [ 480/2809]  eta: 0:23:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2341 (4.2260)  class_acc: 0.1250 (0.1935)  loss_scale: 65536.0000 (63628.5073)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1385  max mem: 15572
Epoch: [8]  [ 490/2809]  eta: 0:23:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2279 (4.2262)  class_acc: 0.1667 (0.1936)  loss_scale: 65536.0000 (63667.3564)  weight_decay: 0.0500 (0.0500)  time: 0.4893  data: 0.0459  max mem: 15572
Epoch: [8]  [ 500/2809]  eta: 0:23:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2139 (4.2258)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (63704.6547)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1087  max mem: 15572
Epoch: [8]  [ 510/2809]  eta: 0:23:04  lr: 0.000046  min_lr: 0.000000  loss: 4.2139 (4.2283)  class_acc: 0.1667 (0.1928)  loss_scale: 65536.0000 (63740.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1510  max mem: 15572
Epoch: [8]  [ 520/2809]  eta: 0:22:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3371 (4.2296)  class_acc: 0.1667 (0.1919)  loss_scale: 65536.0000 (63774.9559)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1029  max mem: 15572
[2025-01-15 18:20:47,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=147, lr=[4.449446895224366e-07, 4.449446895224366e-07, 6.356352707463381e-07, 6.356352707463381e-07, 9.080503867804831e-07, 9.080503867804831e-07, 1.297214838257833e-06, 1.297214838257833e-06, 1.8531640546540472e-06, 1.8531640546540472e-06, 2.6473772209343534e-06, 2.6473772209343534e-06, 3.7819674584776477e-06, 3.7819674584776477e-06, 5.402810654968069e-06, 5.402810654968069e-06, 7.718300935668669e-06, 7.718300935668669e-06, 1.1026144193812387e-05, 1.1026144193812387e-05, 1.5751634562589122e-05, 1.5751634562589122e-05, 2.2502335089413037e-05, 2.2502335089413037e-05, 3.214619298487577e-05, 3.214619298487577e-05, 4.5923132835536815e-05, 4.5923132835536815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:20:47,793] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=28.42710205153245, CurrSamplesPerSec=28.787722521294096, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [ 530/2809]  eta: 0:22:49  lr: 0.000046  min_lr: 0.000000  loss: 4.3371 (4.2303)  class_acc: 0.1667 (0.1922)  loss_scale: 65536.0000 (63808.1205)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1222  max mem: 15572
Epoch: [8]  [ 540/2809]  eta: 0:22:44  lr: 0.000046  min_lr: 0.000000  loss: 4.3820 (4.2333)  class_acc: 0.1667 (0.1917)  loss_scale: 65536.0000 (63840.0591)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1589  max mem: 15572
Epoch: [8]  [ 550/2809]  eta: 0:22:37  lr: 0.000046  min_lr: 0.000000  loss: 4.3414 (4.2349)  class_acc: 0.1667 (0.1915)  loss_scale: 65536.0000 (63870.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1594  max mem: 15572
Epoch: [8]  [ 560/2809]  eta: 0:22:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2048 (4.2329)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (63900.5205)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1375  max mem: 15572
Epoch: [8]  [ 570/2809]  eta: 0:22:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2244 (4.2316)  class_acc: 0.2083 (0.1929)  loss_scale: 65536.0000 (63929.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1056  max mem: 15572
Epoch: [8]  [ 580/2809]  eta: 0:22:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2244 (4.2293)  class_acc: 0.2500 (0.1935)  loss_scale: 65536.0000 (63956.8193)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1154  max mem: 15572
Epoch: [8]  [ 590/2809]  eta: 0:22:17  lr: 0.000046  min_lr: 0.000000  loss: 4.0950 (4.2268)  class_acc: 0.2083 (0.1940)  loss_scale: 65536.0000 (63983.5398)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.2118  max mem: 15572
[2025-01-15 18:21:30,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:21:30,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:21:31,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23071
[2025-01-15 18:21:31,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:21:31,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 600/2809]  eta: 0:22:08  lr: 0.000046  min_lr: 0.000000  loss: 4.1855 (4.2259)  class_acc: 0.2083 (0.1942)  loss_scale: 65536.0000 (64227.4609)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1767  max mem: 15572
Epoch: [8]  [ 610/2809]  eta: 0:22:02  lr: 0.000046  min_lr: 0.000000  loss: 4.1956 (4.2273)  class_acc: 0.1250 (0.1935)  loss_scale: 65536.0000 (64248.8773)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.0983  max mem: 15572
Epoch: [8]  [ 620/2809]  eta: 0:21:56  lr: 0.000046  min_lr: 0.000000  loss: 4.3496 (4.2297)  class_acc: 0.1250 (0.1926)  loss_scale: 65536.0000 (64269.6039)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1569  max mem: 15572
Epoch: [8]  [ 630/2809]  eta: 0:21:46  lr: 0.000046  min_lr: 0.000000  loss: 4.3496 (4.2312)  class_acc: 0.1250 (0.1922)  loss_scale: 65536.0000 (64289.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1257  max mem: 15572
Epoch: [8]  [ 640/2809]  eta: 0:21:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1703 (4.2282)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (64309.1170)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1112  max mem: 15572
Epoch: [8]  [ 650/2809]  eta: 0:21:35  lr: 0.000046  min_lr: 0.000000  loss: 4.0001 (4.2266)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (64327.9631)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1719  max mem: 15572
Epoch: [8]  [ 660/2809]  eta: 0:21:31  lr: 0.000046  min_lr: 0.000000  loss: 4.2117 (4.2266)  class_acc: 0.2083 (0.1930)  loss_scale: 65536.0000 (64346.2390)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1860  max mem: 15572
Epoch: [8]  [ 670/2809]  eta: 0:21:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2450 (4.2272)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (64363.9702)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1785  max mem: 15572
Epoch: [8]  [ 680/2809]  eta: 0:21:21  lr: 0.000046  min_lr: 0.000000  loss: 4.3184 (4.2302)  class_acc: 0.1667 (0.1923)  loss_scale: 65536.0000 (64381.1806)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.2184  max mem: 15572
Epoch: [8]  [ 690/2809]  eta: 0:21:12  lr: 0.000046  min_lr: 0.000000  loss: 4.2282 (4.2291)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (64397.8929)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1725  max mem: 15572
Epoch: [8]  [ 700/2809]  eta: 0:21:07  lr: 0.000046  min_lr: 0.000000  loss: 4.0511 (4.2265)  class_acc: 0.2083 (0.1936)  loss_scale: 65536.0000 (64414.1284)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1116  max mem: 15572
Epoch: [8]  [ 710/2809]  eta: 0:20:59  lr: 0.000046  min_lr: 0.000000  loss: 4.0799 (4.2264)  class_acc: 0.2083 (0.1939)  loss_scale: 65536.0000 (64429.9072)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.0938  max mem: 15572
Epoch: [8]  [ 720/2809]  eta: 0:20:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2694 (4.2296)  class_acc: 0.1667 (0.1932)  loss_scale: 65536.0000 (64445.2483)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0512  max mem: 15572
[2025-01-15 18:22:45,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:22:45,487] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 730/2809]  eta: 0:20:43  lr: 0.000046  min_lr: 0.000000  loss: 4.3565 (4.2303)  class_acc: 0.1250 (0.1927)  loss_scale: 65536.0000 (64729.1272)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0830  max mem: 15572
[2025-01-15 18:22:50,683] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23209
[2025-01-15 18:22:50,684] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:22:50,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 740/2809]  eta: 0:20:37  lr: 0.000046  min_lr: 0.000000  loss: 4.1484 (4.2295)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (65270.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1292  max mem: 15572
Epoch: [8]  [ 750/2809]  eta: 0:20:33  lr: 0.000046  min_lr: 0.000000  loss: 4.1538 (4.2295)  class_acc: 0.1667 (0.1926)  loss_scale: 65536.0000 (65274.2051)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.1810  max mem: 15572
Epoch: [8]  [ 760/2809]  eta: 0:20:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2639 (4.2287)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (65277.6452)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1536  max mem: 15572
Epoch: [8]  [ 770/2809]  eta: 0:20:21  lr: 0.000046  min_lr: 0.000000  loss: 4.2671 (4.2293)  class_acc: 0.2083 (0.1928)  loss_scale: 65536.0000 (65280.9961)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1555  max mem: 15572
Epoch: [8]  [ 780/2809]  eta: 0:20:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2780 (4.2298)  class_acc: 0.1667 (0.1926)  loss_scale: 65536.0000 (65284.2612)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1487  max mem: 15572
Epoch: [8]  [ 790/2809]  eta: 0:20:09  lr: 0.000046  min_lr: 0.000000  loss: 4.3515 (4.2305)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (65287.4437)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1731  max mem: 15572
Epoch: [8]  [ 800/2809]  eta: 0:20:03  lr: 0.000046  min_lr: 0.000000  loss: 4.3515 (4.2319)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (65290.5468)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.2017  max mem: 15572
Epoch: [8]  [ 810/2809]  eta: 0:19:57  lr: 0.000046  min_lr: 0.000000  loss: 4.4045 (4.2331)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (65293.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1268  max mem: 15572
Epoch: [8]  [ 820/2809]  eta: 0:19:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2225 (4.2305)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (65296.5262)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1169  max mem: 15572
Epoch: [8]  [ 830/2809]  eta: 0:19:46  lr: 0.000046  min_lr: 0.000000  loss: 4.1570 (4.2300)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (65299.4079)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1864  max mem: 15572
Epoch: [8]  [ 840/2809]  eta: 0:19:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2989 (4.2331)  class_acc: 0.1667 (0.1919)  loss_scale: 65536.0000 (65302.2212)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1887  max mem: 15572
Epoch: [8]  [ 850/2809]  eta: 0:19:33  lr: 0.000046  min_lr: 0.000000  loss: 4.2989 (4.2321)  class_acc: 0.1667 (0.1923)  loss_scale: 65536.0000 (65304.9683)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1390  max mem: 15572
Epoch: [8]  [ 860/2809]  eta: 0:19:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2214 (4.2320)  class_acc: 0.1667 (0.1922)  loss_scale: 65536.0000 (65307.6516)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1036  max mem: 15572
[2025-01-15 18:24:08,032] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:24:08,032] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:24:10,629] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23341
[2025-01-15 18:24:10,629] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:24:10,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 870/2809]  eta: 0:19:19  lr: 0.000046  min_lr: 0.000000  loss: 4.2929 (4.2319)  class_acc: 0.1667 (0.1923)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1015  max mem: 15572
Epoch: [8]  [ 880/2809]  eta: 0:19:13  lr: 0.000046  min_lr: 0.000000  loss: 4.2637 (4.2319)  class_acc: 0.2083 (0.1921)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1458  max mem: 15572
Epoch: [8]  [ 890/2809]  eta: 0:19:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2637 (4.2327)  class_acc: 0.1667 (0.1919)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1084  max mem: 15572
Epoch: [8]  [ 900/2809]  eta: 0:19:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2661 (4.2316)  class_acc: 0.1667 (0.1922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1319  max mem: 15572
Epoch: [8]  [ 910/2809]  eta: 0:18:54  lr: 0.000046  min_lr: 0.000000  loss: 4.1652 (4.2306)  class_acc: 0.2083 (0.1926)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1770  max mem: 15572
Epoch: [8]  [ 920/2809]  eta: 0:18:47  lr: 0.000046  min_lr: 0.000000  loss: 4.3506 (4.2312)  class_acc: 0.2083 (0.1927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1261  max mem: 15572
Epoch: [8]  [ 930/2809]  eta: 0:18:44  lr: 0.000046  min_lr: 0.000000  loss: 4.0610 (4.2270)  class_acc: 0.2083 (0.1932)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.1848  max mem: 15572
Epoch: [8]  [ 940/2809]  eta: 0:18:35  lr: 0.000046  min_lr: 0.000000  loss: 4.0867 (4.2271)  class_acc: 0.2083 (0.1932)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1359  max mem: 15572
Epoch: [8]  [ 950/2809]  eta: 0:18:28  lr: 0.000046  min_lr: 0.000000  loss: 4.1380 (4.2254)  class_acc: 0.2083 (0.1935)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0612  max mem: 15572
Epoch: [8]  [ 960/2809]  eta: 0:18:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1659 (4.2250)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0980  max mem: 15572
Epoch: [8]  [ 970/2809]  eta: 0:18:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1975 (4.2246)  class_acc: 0.2083 (0.1935)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0857  max mem: 15572
Epoch: [8]  [ 980/2809]  eta: 0:18:06  lr: 0.000046  min_lr: 0.000000  loss: 4.3826 (4.2258)  class_acc: 0.1250 (0.1930)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0881  max mem: 15572
Epoch: [8]  [ 990/2809]  eta: 0:17:59  lr: 0.000046  min_lr: 0.000000  loss: 4.4246 (4.2279)  class_acc: 0.1250 (0.1927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0757  max mem: 15572
[2025-01-15 18:25:23,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:25:23,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:25:24,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23472
[2025-01-15 18:25:24,255] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:25:24,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1000/2809]  eta: 0:17:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2580 (4.2277)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (65666.9411)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0915  max mem: 15572
Epoch: [8]  [1010/2809]  eta: 0:17:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2628 (4.2282)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (65665.6459)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0820  max mem: 15572
Epoch: [8]  [1020/2809]  eta: 0:17:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2628 (4.2271)  class_acc: 0.2083 (0.1934)  loss_scale: 65536.0000 (65664.3761)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1959  max mem: 15572
Epoch: [8]  [1030/2809]  eta: 0:17:38  lr: 0.000046  min_lr: 0.000000  loss: 4.1969 (4.2267)  class_acc: 0.2500 (0.1937)  loss_scale: 65536.0000 (65663.1309)  weight_decay: 0.0500 (0.0500)  time: 0.7207  data: 0.2764  max mem: 15572
Epoch: [8]  [1040/2809]  eta: 0:17:32  lr: 0.000046  min_lr: 0.000000  loss: 4.4410 (4.2280)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (65661.9097)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1888  max mem: 15572
Epoch: [8]  [1050/2809]  eta: 0:17:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2847 (4.2273)  class_acc: 0.1667 (0.1935)  loss_scale: 65536.0000 (65660.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1312  max mem: 15572
Epoch: [8]  [1060/2809]  eta: 0:17:19  lr: 0.000046  min_lr: 0.000000  loss: 4.1134 (4.2266)  class_acc: 0.1667 (0.1937)  loss_scale: 65536.0000 (65659.5363)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1100  max mem: 15572
Epoch: [8]  [1070/2809]  eta: 0:17:12  lr: 0.000046  min_lr: 0.000000  loss: 4.1815 (4.2276)  class_acc: 0.1667 (0.1935)  loss_scale: 65536.0000 (65658.3828)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1155  max mem: 15572
[2025-01-15 18:26:11,038] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23550
[2025-01-15 18:26:11,038] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:26:11,038] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [1080/2809]  eta: 0:17:06  lr: 0.000046  min_lr: 0.000000  loss: 4.1903 (4.2277)  class_acc: 0.1667 (0.1935)  loss_scale: 65536.0000 (65566.3127)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1163  max mem: 15572
Epoch: [8]  [1090/2809]  eta: 0:17:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2658 (4.2307)  class_acc: 0.1250 (0.1929)  loss_scale: 32768.0000 (65265.6865)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1141  max mem: 15572
Epoch: [8]  [1100/2809]  eta: 0:16:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3504 (4.2287)  class_acc: 0.1250 (0.1932)  loss_scale: 32768.0000 (64970.5213)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1350  max mem: 15572
Epoch: [8]  [1110/2809]  eta: 0:16:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2502 (4.2289)  class_acc: 0.1667 (0.1934)  loss_scale: 32768.0000 (64680.6697)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1845  max mem: 15572
Epoch: [8]  [1120/2809]  eta: 0:16:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2223 (4.2272)  class_acc: 0.2083 (0.1939)  loss_scale: 32768.0000 (64395.9893)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1404  max mem: 15572
Epoch: [8]  [1130/2809]  eta: 0:16:36  lr: 0.000046  min_lr: 0.000000  loss: 4.1718 (4.2274)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (64116.3431)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.1003  max mem: 15572
Epoch: [8]  [1140/2809]  eta: 0:16:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3589 (4.2293)  class_acc: 0.1667 (0.1936)  loss_scale: 32768.0000 (63841.5986)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1348  max mem: 15572
Epoch: [8]  [1150/2809]  eta: 0:16:24  lr: 0.000046  min_lr: 0.000000  loss: 4.4030 (4.2306)  class_acc: 0.1250 (0.1932)  loss_scale: 32768.0000 (63571.6281)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1491  max mem: 15572
Epoch: [8]  [1160/2809]  eta: 0:16:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2503 (4.2302)  class_acc: 0.1667 (0.1932)  loss_scale: 32768.0000 (63306.3084)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1149  max mem: 15572
Epoch: [8]  [1170/2809]  eta: 0:16:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2503 (4.2312)  class_acc: 0.1667 (0.1931)  loss_scale: 32768.0000 (63045.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1226  max mem: 15572
Epoch: [8]  [1180/2809]  eta: 0:16:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2996 (4.2311)  class_acc: 0.2083 (0.1933)  loss_scale: 32768.0000 (62789.1482)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1464  max mem: 15572
Epoch: [8]  [1190/2809]  eta: 0:16:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2996 (4.2309)  class_acc: 0.1667 (0.1933)  loss_scale: 32768.0000 (62537.0814)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1709  max mem: 15572
Epoch: [8]  [1200/2809]  eta: 0:15:53  lr: 0.000046  min_lr: 0.000000  loss: 4.3073 (4.2320)  class_acc: 0.1667 (0.1930)  loss_scale: 32768.0000 (62289.2123)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1566  max mem: 15572
[2025-01-15 18:27:25,479] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:27:25,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [1210/2809]  eta: 0:15:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2960 (4.2317)  class_acc: 0.1667 (0.1934)  loss_scale: 32768.0000 (62153.6713)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1033  max mem: 15572
Epoch: [8]  [1220/2809]  eta: 0:15:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2139 (4.2312)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (62181.3726)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1543  max mem: 15572
Epoch: [8]  [1230/2809]  eta: 0:15:36  lr: 0.000046  min_lr: 0.000000  loss: 4.3002 (4.2320)  class_acc: 0.1250 (0.1933)  loss_scale: 65536.0000 (62208.6239)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1803  max mem: 15572
Epoch: [8]  [1240/2809]  eta: 0:15:30  lr: 0.000046  min_lr: 0.000000  loss: 4.3876 (4.2318)  class_acc: 0.2083 (0.1936)  loss_scale: 65536.0000 (62235.4359)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1636  max mem: 15572
Epoch: [8]  [1250/2809]  eta: 0:15:24  lr: 0.000046  min_lr: 0.000000  loss: 4.3475 (4.2324)  class_acc: 0.1667 (0.1935)  loss_scale: 65536.0000 (62261.8193)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1479  max mem: 15572
Epoch: [8]  [1260/2809]  eta: 0:15:18  lr: 0.000046  min_lr: 0.000000  loss: 4.2355 (4.2327)  class_acc: 0.1667 (0.1932)  loss_scale: 65536.0000 (62287.7843)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1181  max mem: 15572
Epoch: [8]  [1270/2809]  eta: 0:15:11  lr: 0.000046  min_lr: 0.000000  loss: 4.1298 (4.2319)  class_acc: 0.2083 (0.1936)  loss_scale: 65536.0000 (62313.3407)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1124  max mem: 15572
Epoch: [8]  [1280/2809]  eta: 0:15:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2238 (4.2323)  class_acc: 0.1667 (0.1931)  loss_scale: 65536.0000 (62338.4980)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1490  max mem: 15572
Epoch: [8]  [1290/2809]  eta: 0:15:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2467 (4.2315)  class_acc: 0.1250 (0.1932)  loss_scale: 65536.0000 (62363.2657)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1700  max mem: 15572
Epoch: [8]  [1300/2809]  eta: 0:14:55  lr: 0.000046  min_lr: 0.000000  loss: 4.2990 (4.2316)  class_acc: 0.2083 (0.1934)  loss_scale: 65536.0000 (62387.6526)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.1864  max mem: 15572
Epoch: [8]  [1310/2809]  eta: 0:14:49  lr: 0.000046  min_lr: 0.000000  loss: 4.2274 (4.2298)  class_acc: 0.2083 (0.1938)  loss_scale: 65536.0000 (62411.6674)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1690  max mem: 15572
Epoch: [8]  [1320/2809]  eta: 0:14:42  lr: 0.000046  min_lr: 0.000000  loss: 4.0907 (4.2295)  class_acc: 0.2083 (0.1937)  loss_scale: 65536.0000 (62435.3187)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1110  max mem: 15572
Epoch: [8]  [1330/2809]  eta: 0:14:37  lr: 0.000046  min_lr: 0.000000  loss: 4.1638 (4.2290)  class_acc: 0.2083 (0.1938)  loss_scale: 65536.0000 (62458.6146)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1351  max mem: 15572
[2025-01-15 18:28:42,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:28:42,189] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:28:43,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23809
[2025-01-15 18:28:43,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:28:43,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1340/2809]  eta: 0:14:32  lr: 0.000046  min_lr: 0.000000  loss: 4.1590 (4.2283)  class_acc: 0.2500 (0.1941)  loss_scale: 65536.0000 (62579.3050)  weight_decay: 0.0500 (0.0500)  time: 0.6697  data: 0.2163  max mem: 15572
Epoch: [8]  [1350/2809]  eta: 0:14:25  lr: 0.000046  min_lr: 0.000000  loss: 4.1529 (4.2276)  class_acc: 0.2083 (0.1941)  loss_scale: 65536.0000 (62601.1902)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1612  max mem: 15572
Epoch: [8]  [1360/2809]  eta: 0:14:19  lr: 0.000046  min_lr: 0.000000  loss: 4.1509 (4.2275)  class_acc: 0.2083 (0.1941)  loss_scale: 65536.0000 (62622.7539)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0982  max mem: 15572
Epoch: [8]  [1370/2809]  eta: 0:14:13  lr: 0.000046  min_lr: 0.000000  loss: 4.1424 (4.2270)  class_acc: 0.1667 (0.1939)  loss_scale: 65536.0000 (62644.0029)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1527  max mem: 15572
Epoch: [8]  [1380/2809]  eta: 0:14:06  lr: 0.000046  min_lr: 0.000000  loss: 4.1549 (4.2266)  class_acc: 0.2083 (0.1941)  loss_scale: 65536.0000 (62664.9442)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1099  max mem: 15572
Epoch: [8]  [1390/2809]  eta: 0:14:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3244 (4.2285)  class_acc: 0.2083 (0.1939)  loss_scale: 65536.0000 (62685.5845)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0772  max mem: 15572
Epoch: [8]  [1400/2809]  eta: 0:13:54  lr: 0.000046  min_lr: 0.000000  loss: 4.4293 (4.2292)  class_acc: 0.2083 (0.1939)  loss_scale: 65536.0000 (62705.9300)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1008  max mem: 15572
Epoch: [8]  [1410/2809]  eta: 0:13:48  lr: 0.000046  min_lr: 0.000000  loss: 4.2392 (4.2291)  class_acc: 0.2083 (0.1940)  loss_scale: 65536.0000 (62725.9872)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1109  max mem: 15572
Epoch: [8]  [1420/2809]  eta: 0:13:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2994 (4.2298)  class_acc: 0.1667 (0.1938)  loss_scale: 65536.0000 (62745.7621)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1206  max mem: 15572
Epoch: [8]  [1430/2809]  eta: 0:13:36  lr: 0.000046  min_lr: 0.000000  loss: 4.3780 (4.2310)  class_acc: 0.1250 (0.1933)  loss_scale: 65536.0000 (62765.2607)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1228  max mem: 15572
Epoch: [8]  [1440/2809]  eta: 0:13:30  lr: 0.000046  min_lr: 0.000000  loss: 4.2472 (4.2308)  class_acc: 0.1250 (0.1932)  loss_scale: 65536.0000 (62784.4885)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1603  max mem: 15572
Epoch: [8]  [1450/2809]  eta: 0:13:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2687 (4.2315)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (62803.4514)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1274  max mem: 15572
Epoch: [8]  [1460/2809]  eta: 0:13:17  lr: 0.000046  min_lr: 0.000000  loss: 4.2687 (4.2311)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (62822.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0786  max mem: 15572
[2025-01-15 18:29:57,811] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:29:57,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:29:58,610] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23940
[2025-01-15 18:29:58,610] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:29:58,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1470/2809]  eta: 0:13:10  lr: 0.000046  min_lr: 0.000000  loss: 4.1631 (4.2310)  class_acc: 0.2083 (0.1931)  loss_scale: 65536.0000 (62929.7077)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.1018  max mem: 15572
Epoch: [8]  [1480/2809]  eta: 0:13:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1595 (4.2315)  class_acc: 0.2083 (0.1929)  loss_scale: 65536.0000 (62947.3059)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1635  max mem: 15572
Epoch: [8]  [1490/2809]  eta: 0:12:59  lr: 0.000046  min_lr: 0.000000  loss: 4.2480 (4.2312)  class_acc: 0.1667 (0.1928)  loss_scale: 65536.0000 (62964.6680)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1959  max mem: 15572
Epoch: [8]  [1500/2809]  eta: 0:12:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2843 (4.2311)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (62981.7988)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1748  max mem: 15572
Epoch: [8]  [1510/2809]  eta: 0:12:48  lr: 0.000046  min_lr: 0.000000  loss: 4.3745 (4.2321)  class_acc: 0.1250 (0.1926)  loss_scale: 65536.0000 (62998.7028)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1968  max mem: 15572
Epoch: [8]  [1520/2809]  eta: 0:12:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2041 (4.2304)  class_acc: 0.1667 (0.1931)  loss_scale: 65536.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1508  max mem: 15572
[2025-01-15 18:30:34,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=154, lr=[4.427880249960953e-07, 4.427880249960953e-07, 6.325543214229933e-07, 6.325543214229933e-07, 9.036490306042763e-07, 9.036490306042763e-07, 1.2909271865775375e-06, 1.2909271865775375e-06, 1.844181695110768e-06, 1.844181695110768e-06, 2.6345452787296686e-06, 2.6345452787296686e-06, 3.7636361124709554e-06, 3.7636361124709554e-06, 5.3766230178156515e-06, 5.3766230178156515e-06, 7.68089002545093e-06, 7.68089002545093e-06, 1.0972700036358473e-05, 1.0972700036358473e-05, 1.567528576622639e-05, 1.567528576622639e-05, 2.2393265380323416e-05, 2.2393265380323416e-05, 3.1990379114747744e-05, 3.1990379114747744e-05, 4.5700541592496775e-05, 4.5700541592496775e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:30:34,600] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=28.428471026888495, CurrSamplesPerSec=30.949381309350244, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [1530/2809]  eta: 0:12:36  lr: 0.000046  min_lr: 0.000000  loss: 4.2255 (4.2311)  class_acc: 0.2083 (0.1930)  loss_scale: 65536.0000 (63031.8485)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1032  max mem: 15572
Epoch: [8]  [1540/2809]  eta: 0:12:29  lr: 0.000046  min_lr: 0.000000  loss: 4.4061 (4.2316)  class_acc: 0.1667 (0.1928)  loss_scale: 65536.0000 (63048.0986)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1089  max mem: 15572
Epoch: [8]  [1550/2809]  eta: 0:12:24  lr: 0.000046  min_lr: 0.000000  loss: 4.2095 (4.2312)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (63064.1393)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1466  max mem: 15572
Epoch: [8]  [1560/2809]  eta: 0:12:18  lr: 0.000046  min_lr: 0.000000  loss: 4.1165 (4.2305)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (63079.9744)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1480  max mem: 15572
Epoch: [8]  [1570/2809]  eta: 0:12:11  lr: 0.000046  min_lr: 0.000000  loss: 4.1305 (4.2303)  class_acc: 0.2083 (0.1931)  loss_scale: 65536.0000 (63095.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0927  max mem: 15572
Epoch: [8]  [1580/2809]  eta: 0:12:06  lr: 0.000046  min_lr: 0.000000  loss: 4.2970 (4.2315)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (63111.0436)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1550  max mem: 15572
Epoch: [8]  [1590/2809]  eta: 0:12:00  lr: 0.000046  min_lr: 0.000000  loss: 4.3242 (4.2319)  class_acc: 0.1250 (0.1927)  loss_scale: 65536.0000 (63126.2854)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1593  max mem: 15572
[2025-01-15 18:31:15,110] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:31:15,111] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:31:15,895] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24071
[2025-01-15 18:31:15,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:31:15,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1600/2809]  eta: 0:11:54  lr: 0.000046  min_lr: 0.000000  loss: 4.3441 (4.2321)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63223.2055)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1236  max mem: 15572
Epoch: [8]  [1610/2809]  eta: 0:11:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1361 (4.2314)  class_acc: 0.2083 (0.1926)  loss_scale: 65536.0000 (63237.5618)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1727  max mem: 15572
Epoch: [8]  [1620/2809]  eta: 0:11:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2356 (4.2322)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63251.7409)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1642  max mem: 15572
Epoch: [8]  [1630/2809]  eta: 0:11:36  lr: 0.000046  min_lr: 0.000000  loss: 4.5064 (4.2340)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (63265.7462)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1248  max mem: 15572
Epoch: [8]  [1640/2809]  eta: 0:11:31  lr: 0.000046  min_lr: 0.000000  loss: 4.4049 (4.2333)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63279.5807)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1588  max mem: 15572
Epoch: [8]  [1650/2809]  eta: 0:11:25  lr: 0.000046  min_lr: 0.000000  loss: 4.2093 (4.2327)  class_acc: 0.2083 (0.1926)  loss_scale: 65536.0000 (63293.2477)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.1890  max mem: 15572
Epoch: [8]  [1660/2809]  eta: 0:11:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1157 (4.2319)  class_acc: 0.2083 (0.1928)  loss_scale: 65536.0000 (63306.7502)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.1983  max mem: 15572
Epoch: [8]  [1670/2809]  eta: 0:11:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1573 (4.2319)  class_acc: 0.2083 (0.1929)  loss_scale: 65536.0000 (63320.0910)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1378  max mem: 15572
Epoch: [8]  [1680/2809]  eta: 0:11:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2846 (4.2322)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (63333.2731)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1200  max mem: 15572
Epoch: [8]  [1690/2809]  eta: 0:11:02  lr: 0.000046  min_lr: 0.000000  loss: 4.3556 (4.2331)  class_acc: 0.1250 (0.1924)  loss_scale: 65536.0000 (63346.2992)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1595  max mem: 15572
Epoch: [8]  [1700/2809]  eta: 0:10:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2200 (4.2327)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63359.1723)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1601  max mem: 15572
Epoch: [8]  [1710/2809]  eta: 0:10:50  lr: 0.000046  min_lr: 0.000000  loss: 4.4525 (4.2338)  class_acc: 0.1250 (0.1923)  loss_scale: 65536.0000 (63371.8948)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1843  max mem: 15572
Epoch: [8]  [1720/2809]  eta: 0:10:45  lr: 0.000046  min_lr: 0.000000  loss: 4.2176 (4.2329)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (63384.4695)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1528  max mem: 15572
[2025-01-15 18:32:34,779] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:32:34,779] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1730/2809]  eta: 0:10:38  lr: 0.000046  min_lr: 0.000000  loss: 4.0932 (4.2331)  class_acc: 0.1667 (0.1923)  loss_scale: 65536.0000 (63510.4795)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1060  max mem: 15572
[2025-01-15 18:32:38,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24207
[2025-01-15 18:32:38,533] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:32:38,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1740/2809]  eta: 0:10:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3179 (4.2337)  class_acc: 0.1250 (0.1922)  loss_scale: 65536.0000 (63672.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0817  max mem: 15572
Epoch: [8]  [1750/2809]  eta: 0:10:26  lr: 0.000046  min_lr: 0.000000  loss: 4.4141 (4.2340)  class_acc: 0.1250 (0.1919)  loss_scale: 65536.0000 (63683.3261)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0994  max mem: 15572
Epoch: [8]  [1760/2809]  eta: 0:10:20  lr: 0.000046  min_lr: 0.000000  loss: 4.3142 (4.2329)  class_acc: 0.1667 (0.1921)  loss_scale: 65536.0000 (63693.8467)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1108  max mem: 15572
Epoch: [8]  [1770/2809]  eta: 0:10:14  lr: 0.000046  min_lr: 0.000000  loss: 4.1837 (4.2320)  class_acc: 0.1667 (0.1921)  loss_scale: 65536.0000 (63704.2484)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1062  max mem: 15572
Epoch: [8]  [1780/2809]  eta: 0:10:08  lr: 0.000046  min_lr: 0.000000  loss: 4.2418 (4.2326)  class_acc: 0.2083 (0.1922)  loss_scale: 65536.0000 (63714.5334)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1349  max mem: 15572
Epoch: [8]  [1790/2809]  eta: 0:10:02  lr: 0.000046  min_lr: 0.000000  loss: 4.2929 (4.2331)  class_acc: 0.1667 (0.1921)  loss_scale: 65536.0000 (63724.7035)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1619  max mem: 15572
Epoch: [8]  [1800/2809]  eta: 0:09:56  lr: 0.000046  min_lr: 0.000000  loss: 4.2929 (4.2333)  class_acc: 0.1667 (0.1920)  loss_scale: 65536.0000 (63734.7607)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1370  max mem: 15572
Epoch: [8]  [1810/2809]  eta: 0:09:50  lr: 0.000046  min_lr: 0.000000  loss: 4.2801 (4.2328)  class_acc: 0.2083 (0.1923)  loss_scale: 65536.0000 (63744.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0862  max mem: 15572
Epoch: [8]  [1820/2809]  eta: 0:09:44  lr: 0.000046  min_lr: 0.000000  loss: 4.1198 (4.2309)  class_acc: 0.2500 (0.1927)  loss_scale: 65536.0000 (63754.5437)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1370  max mem: 15572
Epoch: [8]  [1830/2809]  eta: 0:09:38  lr: 0.000046  min_lr: 0.000000  loss: 4.2547 (4.2323)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (63764.2731)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1760  max mem: 15572
Epoch: [8]  [1840/2809]  eta: 0:09:32  lr: 0.000046  min_lr: 0.000000  loss: 4.3406 (4.2324)  class_acc: 0.1250 (0.1923)  loss_scale: 65536.0000 (63773.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1600  max mem: 15572
Epoch: [8]  [1850/2809]  eta: 0:09:26  lr: 0.000046  min_lr: 0.000000  loss: 4.2023 (4.2319)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63783.4165)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1121  max mem: 15572
Epoch: [8]  [1860/2809]  eta: 0:09:20  lr: 0.000046  min_lr: 0.000000  loss: 4.1854 (4.2319)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (63792.8340)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1179  max mem: 15572
[2025-01-15 18:33:54,163] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:33:54,164] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:33:55,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24340
[2025-01-15 18:33:55,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:33:55,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [1870/2809]  eta: 0:09:15  lr: 0.000046  min_lr: 0.000000  loss: 4.2927 (4.2317)  class_acc: 0.1667 (0.1923)  loss_scale: 65536.0000 (63942.2598)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1850  max mem: 15572
Epoch: [8]  [1880/2809]  eta: 0:09:08  lr: 0.000046  min_lr: 0.000000  loss: 4.0031 (4.2309)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (63950.7326)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1425  max mem: 15572
Epoch: [8]  [1890/2809]  eta: 0:09:02  lr: 0.000046  min_lr: 0.000000  loss: 4.0031 (4.2300)  class_acc: 0.2083 (0.1927)  loss_scale: 65536.0000 (63959.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0900  max mem: 15572
Epoch: [8]  [1900/2809]  eta: 0:08:56  lr: 0.000046  min_lr: 0.000000  loss: 4.1884 (4.2296)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63967.4108)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0777  max mem: 15572
Epoch: [8]  [1910/2809]  eta: 0:08:51  lr: 0.000046  min_lr: 0.000000  loss: 4.0655 (4.2286)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (63975.6190)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1270  max mem: 15572
Epoch: [8]  [1920/2809]  eta: 0:08:45  lr: 0.000046  min_lr: 0.000000  loss: 4.1154 (4.2290)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (63983.7418)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1874  max mem: 15572
Epoch: [8]  [1930/2809]  eta: 0:08:39  lr: 0.000046  min_lr: 0.000000  loss: 4.1963 (4.2288)  class_acc: 0.1667 (0.1926)  loss_scale: 65536.0000 (63991.7804)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1381  max mem: 15572
Epoch: [8]  [1940/2809]  eta: 0:08:33  lr: 0.000046  min_lr: 0.000000  loss: 4.3401 (4.2295)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (63999.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0703  max mem: 15572
Epoch: [8]  [1950/2809]  eta: 0:08:27  lr: 0.000046  min_lr: 0.000000  loss: 4.3433 (4.2294)  class_acc: 0.1250 (0.1925)  loss_scale: 65536.0000 (64007.6105)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1281  max mem: 15572
Epoch: [8]  [1960/2809]  eta: 0:08:21  lr: 0.000046  min_lr: 0.000000  loss: 4.3433 (4.2299)  class_acc: 0.1250 (0.1924)  loss_scale: 65536.0000 (64015.4044)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1338  max mem: 15572
Epoch: [8]  [1970/2809]  eta: 0:08:15  lr: 0.000046  min_lr: 0.000000  loss: 4.0284 (4.2292)  class_acc: 0.1667 (0.1924)  loss_scale: 65536.0000 (64023.1192)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1159  max mem: 15572
Epoch: [8]  [1980/2809]  eta: 0:08:09  lr: 0.000046  min_lr: 0.000000  loss: 3.9971 (4.2286)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (64030.7562)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1749  max mem: 15572
Epoch: [8]  [1990/2809]  eta: 0:08:03  lr: 0.000046  min_lr: 0.000000  loss: 4.0602 (4.2282)  class_acc: 0.2083 (0.1926)  loss_scale: 65536.0000 (64038.3164)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1315  max mem: 15572
[2025-01-15 18:35:11,093] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:35:11,094] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2000/2809]  eta: 0:07:57  lr: 0.000046  min_lr: 0.000000  loss: 4.2497 (4.2290)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (64176.8076)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1232  max mem: 15572
[2025-01-15 18:35:14,149] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24476
[2025-01-15 18:35:14,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:35:14,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2010/2809]  eta: 0:07:51  lr: 0.000046  min_lr: 0.000000  loss: 4.4053 (4.2290)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (64281.3327)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1577  max mem: 15572
Epoch: [8]  [2020/2809]  eta: 0:07:46  lr: 0.000046  min_lr: 0.000000  loss: 4.2489 (4.2290)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (64287.5408)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1740  max mem: 15572
Epoch: [8]  [2030/2809]  eta: 0:07:40  lr: 0.000046  min_lr: 0.000000  loss: 4.2236 (4.2289)  class_acc: 0.2083 (0.1925)  loss_scale: 65536.0000 (64293.6878)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1688  max mem: 15572
Epoch: [8]  [2040/2809]  eta: 0:07:34  lr: 0.000046  min_lr: 0.000000  loss: 4.2510 (4.2292)  class_acc: 0.2083 (0.1926)  loss_scale: 65536.0000 (64299.7746)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1390  max mem: 15572
Epoch: [8]  [2050/2809]  eta: 0:07:28  lr: 0.000046  min_lr: 0.000000  loss: 4.1233 (4.2285)  class_acc: 0.2083 (0.1928)  loss_scale: 65536.0000 (64305.8020)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.1777  max mem: 15572
Epoch: [8]  [2060/2809]  eta: 0:07:22  lr: 0.000046  min_lr: 0.000000  loss: 4.1233 (4.2286)  class_acc: 0.2083 (0.1927)  loss_scale: 65536.0000 (64311.7710)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1579  max mem: 15572
Epoch: [8]  [2070/2809]  eta: 0:07:16  lr: 0.000046  min_lr: 0.000000  loss: 4.2419 (4.2287)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (64317.6823)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1389  max mem: 15572
Epoch: [8]  [2080/2809]  eta: 0:07:10  lr: 0.000046  min_lr: 0.000000  loss: 4.2651 (4.2294)  class_acc: 0.1667 (0.1925)  loss_scale: 65536.0000 (64323.5368)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1565  max mem: 15572
Epoch: [8]  [2090/2809]  eta: 0:07:05  lr: 0.000046  min_lr: 0.000000  loss: 4.1913 (4.2285)  class_acc: 0.2083 (0.1927)  loss_scale: 65536.0000 (64329.3352)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1566  max mem: 15572
Epoch: [8]  [2100/2809]  eta: 0:06:58  lr: 0.000046  min_lr: 0.000000  loss: 4.0150 (4.2279)  class_acc: 0.2083 (0.1928)  loss_scale: 65536.0000 (64335.0785)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1430  max mem: 15572
Epoch: [8]  [2110/2809]  eta: 0:06:52  lr: 0.000046  min_lr: 0.000000  loss: 4.0207 (4.2273)  class_acc: 0.1667 (0.1928)  loss_scale: 65536.0000 (64340.7674)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.1000  max mem: 15572
Epoch: [8]  [2120/2809]  eta: 0:06:47  lr: 0.000046  min_lr: 0.000000  loss: 4.0873 (4.2264)  class_acc: 0.2083 (0.1931)  loss_scale: 65536.0000 (64346.4026)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1373  max mem: 15572
Epoch: [8]  [2130/2809]  eta: 0:06:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1031 (4.2258)  class_acc: 0.2083 (0.1931)  loss_scale: 65536.0000 (64351.9850)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1662  max mem: 15572
[2025-01-15 18:36:32,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:36:32,705] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2140/2809]  eta: 0:06:35  lr: 0.000046  min_lr: 0.000000  loss: 4.0477 (4.2254)  class_acc: 0.1667 (0.1932)  loss_scale: 65536.0000 (64602.3951)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1690  max mem: 15572
[2025-01-15 18:36:36,912] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24613
[2025-01-15 18:36:36,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:36:36,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2150/2809]  eta: 0:06:29  lr: 0.000046  min_lr: 0.000000  loss: 4.2129 (4.2259)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (64606.7355)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1364  max mem: 15572
Epoch: [8]  [2160/2809]  eta: 0:06:23  lr: 0.000046  min_lr: 0.000000  loss: 4.3236 (4.2260)  class_acc: 0.1667 (0.1931)  loss_scale: 65536.0000 (64611.0356)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1490  max mem: 15572
Epoch: [8]  [2170/2809]  eta: 0:06:17  lr: 0.000046  min_lr: 0.000000  loss: 4.1997 (4.2253)  class_acc: 0.2083 (0.1933)  loss_scale: 65536.0000 (64615.2962)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1374  max mem: 15572
Epoch: [8]  [2180/2809]  eta: 0:06:11  lr: 0.000046  min_lr: 0.000000  loss: 4.1707 (4.2258)  class_acc: 0.2083 (0.1932)  loss_scale: 65536.0000 (64619.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0696  max mem: 15572
Epoch: [8]  [2190/2809]  eta: 0:06:05  lr: 0.000046  min_lr: 0.000000  loss: 4.2878 (4.2262)  class_acc: 0.1667 (0.1931)  loss_scale: 65536.0000 (64623.7006)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0813  max mem: 15572
Epoch: [8]  [2200/2809]  eta: 0:05:59  lr: 0.000046  min_lr: 0.000000  loss: 4.1654 (4.2259)  class_acc: 0.1667 (0.1931)  loss_scale: 65536.0000 (64627.8455)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1110  max mem: 15572
Epoch: [8]  [2210/2809]  eta: 0:05:53  lr: 0.000046  min_lr: 0.000000  loss: 4.2577 (4.2260)  class_acc: 0.2083 (0.1931)  loss_scale: 65536.0000 (64631.9530)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1087  max mem: 15572
Epoch: [8]  [2220/2809]  eta: 0:05:47  lr: 0.000046  min_lr: 0.000000  loss: 4.2075 (4.2253)  class_acc: 0.2083 (0.1933)  loss_scale: 65536.0000 (64636.0234)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1429  max mem: 15572
Epoch: [8]  [2230/2809]  eta: 0:05:41  lr: 0.000046  min_lr: 0.000000  loss: 4.1547 (4.2254)  class_acc: 0.2083 (0.1932)  loss_scale: 65536.0000 (64640.0574)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1376  max mem: 15572
Epoch: [8]  [2240/2809]  eta: 0:05:35  lr: 0.000046  min_lr: 0.000000  loss: 4.2321 (4.2250)  class_acc: 0.2083 (0.1933)  loss_scale: 65536.0000 (64644.0553)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0740  max mem: 15572
Epoch: [8]  [2250/2809]  eta: 0:05:29  lr: 0.000046  min_lr: 0.000000  loss: 4.1749 (4.2250)  class_acc: 0.1250 (0.1930)  loss_scale: 65536.0000 (64648.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0865  max mem: 15572
Epoch: [8]  [2260/2809]  eta: 0:05:23  lr: 0.000046  min_lr: 0.000000  loss: 4.2914 (4.2257)  class_acc: 0.1250 (0.1929)  loss_scale: 65536.0000 (64651.9452)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1150  max mem: 15572
[2025-01-15 18:37:50,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:37:50,598] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2270/2809]  eta: 0:05:17  lr: 0.000046  min_lr: 0.000000  loss: 4.3313 (4.2263)  class_acc: 0.1667 (0.1928)  loss_scale: 65536.0000 (64684.6957)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1417  max mem: 15572
[2025-01-15 18:37:53,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24747
[2025-01-15 18:37:53,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:37:53,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2280/2809]  eta: 0:05:11  lr: 0.000046  min_lr: 0.000000  loss: 4.2422 (4.2264)  class_acc: 0.1667 (0.1927)  loss_scale: 65536.0000 (64803.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.1054  max mem: 15572
Epoch: [8]  [2290/2809]  eta: 0:05:06  lr: 0.000046  min_lr: 0.000000  loss: 4.1830 (4.2260)  class_acc: 0.1667 (0.1929)  loss_scale: 65536.0000 (64806.5509)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1531  max mem: 15572
Epoch: [8]  [2300/2809]  eta: 0:05:00  lr: 0.000046  min_lr: 0.000000  loss: 4.2715 (4.2260)  class_acc: 0.1667 (0.1930)  loss_scale: 65536.0000 (64809.7210)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.2080  max mem: 15572
Epoch: [8]  [2310/2809]  eta: 0:04:54  lr: 0.000046  min_lr: 0.000000  loss: 4.0700 (4.2251)  class_acc: 0.2083 (0.1935)  loss_scale: 65536.0000 (64812.8637)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1483  max mem: 15572
Epoch: [8]  [2320/2809]  eta: 0:04:48  lr: 0.000046  min_lr: 0.000000  loss: 4.1073 (4.2248)  class_acc: 0.2083 (0.1934)  loss_scale: 65536.0000 (64815.9793)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1431  max mem: 15572
[2025-01-15 18:38:24,119] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24797
[2025-01-15 18:38:24,119] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:38:24,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [2330/2809]  eta: 0:04:42  lr: 0.000046  min_lr: 0.000000  loss: 4.2689 (4.2244)  class_acc: 0.1667 (0.1934)  loss_scale: 65536.0000 (64734.7233)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1720  max mem: 15572
Epoch: [8]  [2340/2809]  eta: 0:04:36  lr: 0.000046  min_lr: 0.000000  loss: 4.1216 (4.2237)  class_acc: 0.2083 (0.1936)  loss_scale: 32768.0000 (64598.1717)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1481  max mem: 15572
Epoch: [8]  [2350/2809]  eta: 0:04:30  lr: 0.000046  min_lr: 0.000000  loss: 4.0727 (4.2233)  class_acc: 0.2500 (0.1937)  loss_scale: 32768.0000 (64462.7818)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1375  max mem: 15572
Epoch: [8]  [2360/2809]  eta: 0:04:24  lr: 0.000045  min_lr: 0.000000  loss: 4.1196 (4.2227)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (64328.5388)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1569  max mem: 15572
Epoch: [8]  [2370/2809]  eta: 0:04:18  lr: 0.000045  min_lr: 0.000000  loss: 4.1411 (4.2226)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (64195.4281)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1145  max mem: 15572
Epoch: [8]  [2380/2809]  eta: 0:04:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1903 (4.2230)  class_acc: 0.2083 (0.1938)  loss_scale: 32768.0000 (64063.4355)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0996  max mem: 15572
Epoch: [8]  [2390/2809]  eta: 0:04:07  lr: 0.000045  min_lr: 0.000000  loss: 4.2211 (4.2227)  class_acc: 0.1667 (0.1937)  loss_scale: 32768.0000 (63932.5471)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1525  max mem: 15572
Epoch: [8]  [2400/2809]  eta: 0:04:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2211 (4.2233)  class_acc: 0.1667 (0.1937)  loss_scale: 32768.0000 (63802.7489)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1477  max mem: 15572
Epoch: [8]  [2410/2809]  eta: 0:03:55  lr: 0.000045  min_lr: 0.000000  loss: 4.3210 (4.2229)  class_acc: 0.1667 (0.1937)  loss_scale: 32768.0000 (63674.0274)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0940  max mem: 15572
Epoch: [8]  [2420/2809]  eta: 0:03:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2982 (4.2229)  class_acc: 0.1667 (0.1937)  loss_scale: 32768.0000 (63546.3693)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1217  max mem: 15572
Epoch: [8]  [2430/2809]  eta: 0:03:43  lr: 0.000045  min_lr: 0.000000  loss: 4.1968 (4.2219)  class_acc: 0.1667 (0.1939)  loss_scale: 32768.0000 (63419.7614)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1601  max mem: 15572
Epoch: [8]  [2440/2809]  eta: 0:03:37  lr: 0.000045  min_lr: 0.000000  loss: 4.0316 (4.2208)  class_acc: 0.2500 (0.1942)  loss_scale: 32768.0000 (63294.1909)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1457  max mem: 15572
Epoch: [8]  [2450/2809]  eta: 0:03:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2005 (4.2209)  class_acc: 0.2083 (0.1942)  loss_scale: 32768.0000 (63169.6450)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1110  max mem: 15572
[2025-01-15 18:39:38,873] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:39:38,874] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [2460/2809]  eta: 0:03:25  lr: 0.000045  min_lr: 0.000000  loss: 4.2969 (4.2213)  class_acc: 0.1667 (0.1941)  loss_scale: 32768.0000 (63139.3157)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0841  max mem: 15572
Epoch: [8]  [2470/2809]  eta: 0:03:19  lr: 0.000045  min_lr: 0.000000  loss: 4.2969 (4.2217)  class_acc: 0.1667 (0.1941)  loss_scale: 65536.0000 (63149.0150)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1201  max mem: 15572
Epoch: [8]  [2480/2809]  eta: 0:03:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2992 (4.2222)  class_acc: 0.1667 (0.1940)  loss_scale: 65536.0000 (63158.6360)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1350  max mem: 15572
Epoch: [8]  [2490/2809]  eta: 0:03:07  lr: 0.000045  min_lr: 0.000000  loss: 4.2880 (4.2228)  class_acc: 0.1667 (0.1941)  loss_scale: 65536.0000 (63168.1798)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1603  max mem: 15572
Epoch: [8]  [2500/2809]  eta: 0:03:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2880 (4.2228)  class_acc: 0.1667 (0.1941)  loss_scale: 65536.0000 (63177.6473)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1812  max mem: 15572
Epoch: [8]  [2510/2809]  eta: 0:02:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2945 (4.2232)  class_acc: 0.1667 (0.1939)  loss_scale: 65536.0000 (63187.0394)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0991  max mem: 15572
Epoch: [8]  [2520/2809]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000000  loss: 4.1862 (4.2226)  class_acc: 0.1667 (0.1940)  loss_scale: 65536.0000 (63196.3570)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0878  max mem: 15572
[2025-01-15 18:40:20,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=161, lr=[4.404113593984157e-07, 4.404113593984157e-07, 6.291590848548796e-07, 6.291590848548796e-07, 8.987986926498281e-07, 8.987986926498281e-07, 1.2839981323568972e-06, 1.2839981323568972e-06, 1.8342830462241392e-06, 1.8342830462241392e-06, 2.62040435174877e-06, 2.62040435174877e-06, 3.743434788212529e-06, 3.743434788212529e-06, 5.347763983160757e-06, 5.347763983160757e-06, 7.639662833086796e-06, 7.639662833086796e-06, 1.0913804047266852e-05, 1.0913804047266852e-05, 1.5591148638952645e-05, 1.5591148638952645e-05, 2.2273069484218067e-05, 2.2273069484218067e-05, 3.1818670691740094e-05, 3.1818670691740094e-05, 4.5455243845343e-05, 4.5455243845343e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:40:20,064] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=28.424528695827487, CurrSamplesPerSec=31.483844493791636, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [8]  [2530/2809]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2596 (4.2233)  class_acc: 0.1667 (0.1938)  loss_scale: 65536.0000 (63205.6009)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1345  max mem: 15572
Epoch: [8]  [2540/2809]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000000  loss: 4.3411 (4.2235)  class_acc: 0.1250 (0.1937)  loss_scale: 65536.0000 (63214.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1000  max mem: 15572
Epoch: [8]  [2550/2809]  eta: 0:02:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2202 (4.2227)  class_acc: 0.2083 (0.1939)  loss_scale: 65536.0000 (63223.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1012  max mem: 15572
Epoch: [8]  [2560/2809]  eta: 0:02:26  lr: 0.000045  min_lr: 0.000000  loss: 4.2532 (4.2231)  class_acc: 0.1667 (0.1938)  loss_scale: 65536.0000 (63232.8996)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1632  max mem: 15572
Epoch: [8]  [2570/2809]  eta: 0:02:20  lr: 0.000045  min_lr: 0.000000  loss: 4.2969 (4.2230)  class_acc: 0.1667 (0.1938)  loss_scale: 65536.0000 (63241.8576)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1151  max mem: 15572
Epoch: [8]  [2580/2809]  eta: 0:02:14  lr: 0.000045  min_lr: 0.000000  loss: 4.2109 (4.2226)  class_acc: 0.2083 (0.1939)  loss_scale: 65536.0000 (63250.7462)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0618  max mem: 15572
[2025-01-15 18:40:49,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:40:49,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:40:54,596] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25061
[2025-01-15 18:40:54,596] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:40:54,596] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2590/2809]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000000  loss: 4.1399 (4.2222)  class_acc: 0.2083 (0.1940)  loss_scale: 65536.0000 (63436.6222)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1221  max mem: 15572
Epoch: [8]  [2600/2809]  eta: 0:02:02  lr: 0.000045  min_lr: 0.000000  loss: 4.1399 (4.2221)  class_acc: 0.1667 (0.1939)  loss_scale: 65536.0000 (63444.6936)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1484  max mem: 15572
Epoch: [8]  [2610/2809]  eta: 0:01:57  lr: 0.000045  min_lr: 0.000000  loss: 4.2484 (4.2223)  class_acc: 0.1667 (0.1940)  loss_scale: 65536.0000 (63452.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1384  max mem: 15572
Epoch: [8]  [2620/2809]  eta: 0:01:51  lr: 0.000045  min_lr: 0.000000  loss: 4.1606 (4.2224)  class_acc: 0.1667 (0.1940)  loss_scale: 65536.0000 (63460.6517)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1178  max mem: 15572
Epoch: [8]  [2630/2809]  eta: 0:01:45  lr: 0.000045  min_lr: 0.000000  loss: 4.1386 (4.2223)  class_acc: 0.1667 (0.1938)  loss_scale: 65536.0000 (63468.5397)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0763  max mem: 15572
Epoch: [8]  [2640/2809]  eta: 0:01:39  lr: 0.000045  min_lr: 0.000000  loss: 4.0286 (4.2222)  class_acc: 0.1667 (0.1939)  loss_scale: 65536.0000 (63476.3680)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1098  max mem: 15572
Epoch: [8]  [2650/2809]  eta: 0:01:33  lr: 0.000045  min_lr: 0.000000  loss: 4.0286 (4.2212)  class_acc: 0.2917 (0.1941)  loss_scale: 65536.0000 (63484.1373)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1582  max mem: 15572
Epoch: [8]  [2660/2809]  eta: 0:01:27  lr: 0.000045  min_lr: 0.000000  loss: 4.0778 (4.2209)  class_acc: 0.2500 (0.1942)  loss_scale: 65536.0000 (63491.8482)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1031  max mem: 15572
Epoch: [8]  [2670/2809]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000000  loss: 4.2147 (4.2211)  class_acc: 0.1667 (0.1942)  loss_scale: 65536.0000 (63499.5013)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1004  max mem: 15572
Epoch: [8]  [2680/2809]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000000  loss: 4.2083 (4.2206)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (63507.0974)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1582  max mem: 15572
Epoch: [8]  [2690/2809]  eta: 0:01:09  lr: 0.000045  min_lr: 0.000000  loss: 4.0685 (4.2205)  class_acc: 0.2083 (0.1944)  loss_scale: 65536.0000 (63514.6369)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1401  max mem: 15572
Epoch: [8]  [2700/2809]  eta: 0:01:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2609 (4.2208)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (63522.1207)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1495  max mem: 15572
Epoch: [8]  [2710/2809]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3760 (4.2209)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (63529.5492)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1274  max mem: 15572
[2025-01-15 18:42:09,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:42:09,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [2720/2809]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000000  loss: 4.1427 (4.2198)  class_acc: 0.1667 (0.1945)  loss_scale: 65536.0000 (63609.1790)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1165  max mem: 15572
[2025-01-15 18:42:15,166] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25198
[2025-01-15 18:42:15,166] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:42:15,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [2730/2809]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1427 (4.2199)  class_acc: 0.2083 (0.1946)  loss_scale: 65536.0000 (63736.2197)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1875  max mem: 15572
Epoch: [8]  [2740/2809]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000000  loss: 4.4136 (4.2206)  class_acc: 0.2083 (0.1946)  loss_scale: 65536.0000 (63742.7858)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1384  max mem: 15572
Epoch: [8]  [2750/2809]  eta: 0:00:34  lr: 0.000045  min_lr: 0.000000  loss: 4.4136 (4.2208)  class_acc: 0.1667 (0.1945)  loss_scale: 65536.0000 (63749.3043)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1008  max mem: 15572
Epoch: [8]  [2760/2809]  eta: 0:00:28  lr: 0.000045  min_lr: 0.000000  loss: 4.3094 (4.2206)  class_acc: 0.1667 (0.1945)  loss_scale: 65536.0000 (63755.7754)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1538  max mem: 15572
Epoch: [8]  [2770/2809]  eta: 0:00:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2642 (4.2208)  class_acc: 0.2083 (0.1946)  loss_scale: 65536.0000 (63762.1999)  weight_decay: 0.0500 (0.0500)  time: 0.6692  data: 0.2131  max mem: 15572
Epoch: [8]  [2780/2809]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000000  loss: 4.2642 (4.2209)  class_acc: 0.1667 (0.1944)  loss_scale: 65536.0000 (63768.5782)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.2740  max mem: 15572
Epoch: [8]  [2790/2809]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.2484 (4.2209)  class_acc: 0.1250 (0.1944)  loss_scale: 65536.0000 (63774.9108)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1732  max mem: 15572
Epoch: [8]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.2436 (4.2207)  class_acc: 0.1667 (0.1944)  loss_scale: 65536.0000 (63781.1981)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0731  max mem: 15572
Epoch: [8]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2763 (4.2211)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (63786.1958)  weight_decay: 0.0500 (0.0500)  time: 0.4560  data: 0.0489  max mem: 15572
Epoch: [8] Total time: 0:27:32 (0.5883 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.2763 (4.2211)  class_acc: 0.1667 (0.1943)  loss_scale: 65536.0000 (63786.1958)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:19:53  loss: 0.4244 (0.4244)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.3887  data: 4.2263  max mem: 15572
Val:  [ 10/272]  eta: 0:03:26  loss: 3.9438 (3.3820)  acc1: 0.0000 (23.7374)  acc5: 22.2222 (32.8283)  time: 0.7893  data: 0.6146  max mem: 15572
Val:  [ 20/272]  eta: 0:02:16  loss: 3.4344 (3.3311)  acc1: 11.1111 (24.0741)  acc5: 44.4444 (44.4444)  time: 0.3488  data: 0.1724  max mem: 15572
Val:  [ 30/272]  eta: 0:01:57  loss: 3.4067 (3.4446)  acc1: 11.1111 (18.9964)  acc5: 50.0000 (45.5197)  time: 0.3168  data: 0.1141  max mem: 15572
Val:  [ 40/272]  eta: 0:01:44  loss: 3.3073 (3.3970)  acc1: 5.5556 (18.1572)  acc5: 55.5556 (48.6450)  time: 0.3575  data: 0.1294  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 3.1729 (3.3152)  acc1: 16.6667 (20.3704)  acc5: 55.5556 (52.2876)  time: 0.3072  data: 0.0780  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 2.2387 (3.1620)  acc1: 44.4444 (25.8652)  acc5: 72.2222 (55.2823)  time: 0.2885  data: 0.0755  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 2.2387 (3.0634)  acc1: 44.4444 (27.7778)  acc5: 77.7778 (58.8419)  time: 0.3280  data: 0.1233  max mem: 15572
Val:  [ 80/272]  eta: 0:01:13  loss: 2.8519 (3.0695)  acc1: 27.7778 (27.9150)  acc5: 77.7778 (58.3676)  time: 0.3301  data: 0.1128  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.6363 (3.1365)  acc1: 11.1111 (26.3126)  acc5: 38.8889 (56.5324)  time: 0.3135  data: 0.1033  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.6416 (3.1954)  acc1: 11.1111 (25.5776)  acc5: 38.8889 (55.7206)  time: 0.3196  data: 0.1228  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.6416 (3.2565)  acc1: 11.1111 (23.9740)  acc5: 44.4444 (53.9540)  time: 0.3453  data: 0.1303  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.8267 (3.2895)  acc1: 11.1111 (23.2782)  acc5: 44.4444 (53.3976)  time: 0.3367  data: 0.1242  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.4511 (3.2334)  acc1: 16.6667 (25.1060)  acc5: 50.0000 (54.3257)  time: 0.3083  data: 0.1216  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.8493 (3.2203)  acc1: 33.3333 (25.8865)  acc5: 55.5556 (54.3735)  time: 0.2841  data: 0.0984  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.1960 (3.2201)  acc1: 16.6667 (25.2391)  acc5: 66.6667 (55.3348)  time: 0.3074  data: 0.1035  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.0645 (3.1924)  acc1: 27.7778 (26.8116)  acc5: 72.2222 (56.5562)  time: 0.3356  data: 0.1204  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.2441 (3.2222)  acc1: 33.3333 (25.9584)  acc5: 61.1111 (55.7505)  time: 0.3346  data: 0.1346  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.2843 (3.2107)  acc1: 11.1111 (25.8134)  acc5: 50.0000 (56.4150)  time: 0.3758  data: 0.1883  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.3081 (3.2443)  acc1: 16.6667 (25.3054)  acc5: 50.0000 (55.3229)  time: 0.3725  data: 0.1868  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.0961 (3.2415)  acc1: 22.2222 (25.4561)  acc5: 44.4444 (55.7490)  time: 0.3278  data: 0.1440  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.8588 (3.2459)  acc1: 33.3333 (25.9084)  acc5: 72.2222 (56.0295)  time: 0.3073  data: 0.1232  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.1368 (3.2403)  acc1: 33.3333 (26.0181)  acc5: 66.6667 (56.1840)  time: 0.2975  data: 0.1092  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.5768 (3.2011)  acc1: 44.4444 (27.4892)  acc5: 72.2222 (57.1188)  time: 0.3318  data: 0.1434  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.0904 (3.1696)  acc1: 61.1111 (28.4924)  acc5: 77.7778 (58.1374)  time: 0.3357  data: 0.1545  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.8292 (3.1929)  acc1: 27.7778 (28.0434)  acc5: 66.6667 (57.5476)  time: 0.2263  data: 0.0630  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 2.4289 (3.1197)  acc1: 66.6667 (30.2682)  acc5: 77.7778 (58.8548)  time: 0.1514  data: 0.0003  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8914 (3.1228)  acc1: 66.6667 (30.0738)  acc5: 83.3333 (58.5281)  time: 0.1455  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8914 (3.1270)  acc1: 61.1111 (30.0430)  acc5: 83.3333 (58.4886)  time: 0.1395  data: 0.0002  max mem: 15572
Val: Total time: 0:01:27 (0.3235 s / it)
* Acc@1 30.043 Acc@5 58.489 loss 3.127
Accuracy of the network on the 4883 val videos: 30.0%
[2025-01-15 18:44:30,470] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 18:44:30,475] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 18:44:30,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 18:44:33,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 18:44:33,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.04%
Epoch: [9]  [   0/2809]  eta: 6:49:49  lr: 0.000045  min_lr: 0.000000  loss: 3.8854 (3.8854)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.7540  data: 8.3185  max mem: 15572
Epoch: [9]  [  10/2809]  eta: 1:04:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1422 (4.1213)  class_acc: 0.2083 (0.2273)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3826  data: 0.9326  max mem: 15572
Epoch: [9]  [  20/2809]  eta: 0:48:42  lr: 0.000045  min_lr: 0.000000  loss: 4.2611 (4.1616)  class_acc: 0.2083 (0.2401)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.2175  max mem: 15572
Epoch: [9]  [  30/2809]  eta: 0:44:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2611 (4.1841)  class_acc: 0.2083 (0.2177)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7134  data: 0.2667  max mem: 15572
Epoch: [9]  [  40/2809]  eta: 0:40:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2531 (4.1890)  class_acc: 0.2083 (0.2165)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7007  data: 0.2390  max mem: 15572
[2025-01-15 18:45:15,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:45:15,462] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:45:17,642] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25331
[2025-01-15 18:45:17,642] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:45:17,642] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [  50/2809]  eta: 0:39:31  lr: 0.000045  min_lr: 0.000000  loss: 4.0014 (4.1588)  class_acc: 0.2083 (0.2222)  loss_scale: 65536.0000 (70676.0784)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.2417  max mem: 15572
Epoch: [9]  [  60/2809]  eta: 0:37:54  lr: 0.000045  min_lr: 0.000000  loss: 4.2908 (4.1916)  class_acc: 0.1667 (0.2111)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.2436  max mem: 15572
Epoch: [9]  [  70/2809]  eta: 0:36:59  lr: 0.000045  min_lr: 0.000000  loss: 4.4062 (4.2090)  class_acc: 0.1667 (0.2019)  loss_scale: 65536.0000 (69228.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.2057  max mem: 15572
Epoch: [9]  [  80/2809]  eta: 0:35:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2153 (4.2124)  class_acc: 0.1250 (0.2011)  loss_scale: 65536.0000 (68772.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.1830  max mem: 15572
Epoch: [9]  [  90/2809]  eta: 0:35:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2153 (4.1981)  class_acc: 0.1667 (0.2028)  loss_scale: 65536.0000 (68416.7033)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.1693  max mem: 15572
Epoch: [9]  [ 100/2809]  eta: 0:34:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1962 (4.1907)  class_acc: 0.2083 (0.2063)  loss_scale: 65536.0000 (68131.4851)  weight_decay: 0.0500 (0.0500)  time: 0.7131  data: 0.2309  max mem: 15572
Epoch: [9]  [ 110/2809]  eta: 0:33:35  lr: 0.000045  min_lr: 0.000000  loss: 4.2102 (4.1873)  class_acc: 0.2083 (0.2080)  loss_scale: 65536.0000 (67897.6577)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1761  max mem: 15572
Epoch: [9]  [ 120/2809]  eta: 0:32:08  lr: 0.000045  min_lr: 0.000000  loss: 4.3269 (4.1991)  class_acc: 0.2083 (0.2025)  loss_scale: 65536.0000 (67702.4793)  weight_decay: 0.0500 (0.0500)  time: 0.4365  data: 0.0427  max mem: 15572
Epoch: [9]  [ 130/2809]  eta: 0:31:01  lr: 0.000045  min_lr: 0.000000  loss: 4.2582 (4.1809)  class_acc: 0.2083 (0.2071)  loss_scale: 65536.0000 (67537.0992)  weight_decay: 0.0500 (0.0500)  time: 0.4063  data: 0.0003  max mem: 15572
Epoch: [9]  [ 140/2809]  eta: 0:30:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0903 (4.1778)  class_acc: 0.2500 (0.2098)  loss_scale: 65536.0000 (67395.1773)  weight_decay: 0.0500 (0.0500)  time: 0.4258  data: 0.0004  max mem: 15572
Epoch: [9]  [ 150/2809]  eta: 0:29:39  lr: 0.000045  min_lr: 0.000000  loss: 4.3234 (4.1945)  class_acc: 0.2083 (0.2056)  loss_scale: 65536.0000 (67272.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0730  max mem: 15572
Epoch: [9]  [ 160/2809]  eta: 0:29:26  lr: 0.000045  min_lr: 0.000000  loss: 4.3759 (4.1979)  class_acc: 0.1667 (0.2063)  loss_scale: 65536.0000 (67164.2236)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1772  max mem: 15572
Epoch: [9]  [ 170/2809]  eta: 0:29:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2053 (4.2036)  class_acc: 0.1667 (0.2025)  loss_scale: 65536.0000 (67069.0058)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1675  max mem: 15572
[2025-01-15 18:46:32,172] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:46:32,172] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 180/2809]  eta: 0:28:45  lr: 0.000045  min_lr: 0.000000  loss: 4.1557 (4.1913)  class_acc: 0.1667 (0.2037)  loss_scale: 65536.0000 (67708.4641)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1362  max mem: 15572
Epoch: [9]  [ 190/2809]  eta: 0:28:26  lr: 0.000045  min_lr: 0.000000  loss: 4.0241 (4.1762)  class_acc: 0.2083 (0.2070)  loss_scale: 131072.0000 (71025.9267)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1447  max mem: 15572
[2025-01-15 18:46:42,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25477
[2025-01-15 18:46:42,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:46:42,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 200/2809]  eta: 0:28:16  lr: 0.000045  min_lr: 0.000000  loss: 4.2307 (4.1791)  class_acc: 0.2500 (0.2085)  loss_scale: 131072.0000 (72383.0448)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1565  max mem: 15572
Epoch: [9]  [ 210/2809]  eta: 0:28:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3496 (4.1888)  class_acc: 0.1250 (0.2044)  loss_scale: 65536.0000 (72058.5403)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1639  max mem: 15572
Epoch: [9]  [ 220/2809]  eta: 0:27:51  lr: 0.000045  min_lr: 0.000000  loss: 4.0330 (4.1774)  class_acc: 0.1250 (0.2055)  loss_scale: 65536.0000 (71763.4027)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1431  max mem: 15572
Epoch: [9]  [ 230/2809]  eta: 0:27:39  lr: 0.000045  min_lr: 0.000000  loss: 3.9866 (4.1748)  class_acc: 0.2500 (0.2073)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1223  max mem: 15572
Epoch: [9]  [ 240/2809]  eta: 0:27:15  lr: 0.000045  min_lr: 0.000000  loss: 4.0630 (4.1659)  class_acc: 0.2500 (0.2083)  loss_scale: 65536.0000 (71246.6058)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0894  max mem: 15572
Epoch: [9]  [ 250/2809]  eta: 0:27:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0621 (4.1571)  class_acc: 0.2083 (0.2097)  loss_scale: 65536.0000 (71019.0916)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0997  max mem: 15572
Epoch: [9]  [ 260/2809]  eta: 0:26:50  lr: 0.000045  min_lr: 0.000000  loss: 4.0768 (4.1570)  class_acc: 0.1667 (0.2091)  loss_scale: 65536.0000 (70809.0115)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1421  max mem: 15572
Epoch: [9]  [ 270/2809]  eta: 0:26:41  lr: 0.000045  min_lr: 0.000000  loss: 4.2388 (4.1584)  class_acc: 0.1667 (0.2074)  loss_scale: 65536.0000 (70614.4354)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1634  max mem: 15572
Epoch: [9]  [ 280/2809]  eta: 0:26:29  lr: 0.000045  min_lr: 0.000000  loss: 4.3509 (4.1589)  class_acc: 0.1667 (0.2070)  loss_scale: 65536.0000 (70433.7082)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1546  max mem: 15572
Epoch: [9]  [ 290/2809]  eta: 0:26:23  lr: 0.000045  min_lr: 0.000000  loss: 4.2566 (4.1634)  class_acc: 0.1667 (0.2059)  loss_scale: 65536.0000 (70265.4021)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1538  max mem: 15572
Epoch: [9]  [ 300/2809]  eta: 0:26:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2194 (4.1626)  class_acc: 0.1667 (0.2060)  loss_scale: 65536.0000 (70108.2791)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1630  max mem: 15572
Epoch: [9]  [ 310/2809]  eta: 0:25:59  lr: 0.000045  min_lr: 0.000000  loss: 4.2417 (4.1643)  class_acc: 0.1667 (0.2062)  loss_scale: 65536.0000 (69961.2605)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1197  max mem: 15572
Epoch: [9]  [ 320/2809]  eta: 0:25:49  lr: 0.000045  min_lr: 0.000000  loss: 4.0139 (4.1632)  class_acc: 0.1667 (0.2059)  loss_scale: 65536.0000 (69823.4019)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1191  max mem: 15572
[2025-01-15 18:47:56,757] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:47:56,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:47:57,178] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25607
[2025-01-15 18:47:57,179] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:47:57,179] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 330/2809]  eta: 0:25:34  lr: 0.000045  min_lr: 0.000000  loss: 4.0139 (4.1623)  class_acc: 0.2083 (0.2064)  loss_scale: 65536.0000 (69891.8671)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1186  max mem: 15572
Epoch: [9]  [ 340/2809]  eta: 0:25:27  lr: 0.000045  min_lr: 0.000000  loss: 4.1028 (4.1657)  class_acc: 0.1667 (0.2058)  loss_scale: 65536.0000 (69764.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1243  max mem: 15572
Epoch: [9]  [ 350/2809]  eta: 0:25:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4016 (4.1703)  class_acc: 0.1667 (0.2054)  loss_scale: 65536.0000 (69643.6695)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1114  max mem: 15572
Epoch: [9]  [ 360/2809]  eta: 0:24:59  lr: 0.000045  min_lr: 0.000000  loss: 4.2162 (4.1674)  class_acc: 0.1667 (0.2053)  loss_scale: 65536.0000 (69529.8837)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0655  max mem: 15572
Epoch: [9]  [ 370/2809]  eta: 0:24:53  lr: 0.000045  min_lr: 0.000000  loss: 3.9920 (4.1648)  class_acc: 0.1667 (0.2046)  loss_scale: 65536.0000 (69422.2318)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1210  max mem: 15572
Epoch: [9]  [ 380/2809]  eta: 0:24:49  lr: 0.000045  min_lr: 0.000000  loss: 4.0429 (4.1678)  class_acc: 0.2083 (0.2047)  loss_scale: 65536.0000 (69320.2310)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1911  max mem: 15572
Epoch: [9]  [ 390/2809]  eta: 0:24:39  lr: 0.000045  min_lr: 0.000000  loss: 4.1393 (4.1661)  class_acc: 0.2083 (0.2058)  loss_scale: 65536.0000 (69223.4476)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1484  max mem: 15572
Epoch: [9]  [ 400/2809]  eta: 0:24:30  lr: 0.000045  min_lr: 0.000000  loss: 3.9411 (4.1623)  class_acc: 0.2083 (0.2063)  loss_scale: 65536.0000 (69131.4913)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1058  max mem: 15572
Epoch: [9]  [ 410/2809]  eta: 0:24:23  lr: 0.000045  min_lr: 0.000000  loss: 3.9656 (4.1585)  class_acc: 0.2500 (0.2076)  loss_scale: 65536.0000 (69044.0097)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1263  max mem: 15572
Epoch: [9]  [ 420/2809]  eta: 0:24:19  lr: 0.000045  min_lr: 0.000000  loss: 4.0547 (4.1594)  class_acc: 0.2083 (0.2075)  loss_scale: 65536.0000 (68960.6841)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1646  max mem: 15572
Epoch: [9]  [ 430/2809]  eta: 0:24:10  lr: 0.000045  min_lr: 0.000000  loss: 4.2109 (4.1629)  class_acc: 0.1667 (0.2074)  loss_scale: 65536.0000 (68881.2251)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1655  max mem: 15572
Epoch: [9]  [ 440/2809]  eta: 0:24:05  lr: 0.000045  min_lr: 0.000000  loss: 4.3274 (4.1639)  class_acc: 0.1667 (0.2077)  loss_scale: 65536.0000 (68805.3696)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1561  max mem: 15572
Epoch: [9]  [ 450/2809]  eta: 0:23:57  lr: 0.000045  min_lr: 0.000000  loss: 4.2259 (4.1635)  class_acc: 0.1667 (0.2073)  loss_scale: 65536.0000 (68732.8780)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1614  max mem: 15572
[2025-01-15 18:49:11,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:49:11,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 460/2809]  eta: 0:23:49  lr: 0.000045  min_lr: 0.000000  loss: 4.1636 (4.1658)  class_acc: 0.1667 (0.2063)  loss_scale: 65536.0000 (69516.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1361  max mem: 15572
[2025-01-15 18:49:14,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25742
[2025-01-15 18:49:14,885] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:49:14,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 470/2809]  eta: 0:23:41  lr: 0.000045  min_lr: 0.000000  loss: 4.1636 (4.1676)  class_acc: 0.1250 (0.2051)  loss_scale: 65536.0000 (69431.9830)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1553  max mem: 15572
Epoch: [9]  [ 480/2809]  eta: 0:23:39  lr: 0.000045  min_lr: 0.000000  loss: 4.0317 (4.1673)  class_acc: 0.1250 (0.2056)  loss_scale: 65536.0000 (69350.9854)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.2178  max mem: 15572
Epoch: [9]  [ 490/2809]  eta: 0:23:31  lr: 0.000045  min_lr: 0.000000  loss: 3.9815 (4.1620)  class_acc: 0.2500 (0.2066)  loss_scale: 65536.0000 (69273.2872)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1977  max mem: 15572
Epoch: [9]  [ 500/2809]  eta: 0:23:19  lr: 0.000045  min_lr: 0.000000  loss: 3.9274 (4.1572)  class_acc: 0.2500 (0.2082)  loss_scale: 65536.0000 (69198.6906)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0993  max mem: 15572
Epoch: [9]  [ 510/2809]  eta: 0:23:14  lr: 0.000045  min_lr: 0.000000  loss: 3.9274 (4.1547)  class_acc: 0.2500 (0.2096)  loss_scale: 65536.0000 (69127.0137)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1137  max mem: 15572
Epoch: [9]  [ 520/2809]  eta: 0:23:07  lr: 0.000045  min_lr: 0.000000  loss: 4.0316 (4.1521)  class_acc: 0.2500 (0.2100)  loss_scale: 65536.0000 (69058.0883)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1349  max mem: 15572
Epoch: [9]  [ 530/2809]  eta: 0:22:58  lr: 0.000045  min_lr: 0.000000  loss: 4.0402 (4.1525)  class_acc: 0.2500 (0.2105)  loss_scale: 65536.0000 (68991.7589)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.0956  max mem: 15572
Epoch: [9]  [ 540/2809]  eta: 0:22:52  lr: 0.000045  min_lr: 0.000000  loss: 4.0406 (4.1516)  class_acc: 0.2083 (0.2107)  loss_scale: 65536.0000 (68927.8817)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1078  max mem: 15572
Epoch: [9]  [ 550/2809]  eta: 0:22:44  lr: 0.000045  min_lr: 0.000000  loss: 3.9732 (4.1499)  class_acc: 0.2083 (0.2118)  loss_scale: 65536.0000 (68866.3230)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1099  max mem: 15572
Epoch: [9]  [ 560/2809]  eta: 0:22:35  lr: 0.000045  min_lr: 0.000000  loss: 3.9732 (4.1501)  class_acc: 0.2083 (0.2119)  loss_scale: 65536.0000 (68806.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0897  max mem: 15572
Epoch: [9]  [ 570/2809]  eta: 0:22:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2413 (4.1541)  class_acc: 0.1667 (0.2116)  loss_scale: 65536.0000 (68749.6743)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1540  max mem: 15572
Epoch: [9]  [ 580/2809]  eta: 0:22:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2413 (4.1544)  class_acc: 0.2083 (0.2118)  loss_scale: 65536.0000 (68694.3614)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1604  max mem: 15572
[2025-01-15 18:50:30,526] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:50:30,526] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 590/2809]  eta: 0:22:18  lr: 0.000045  min_lr: 0.000000  loss: 4.2708 (4.1578)  class_acc: 0.2083 (0.2109)  loss_scale: 65536.0000 (68751.8105)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1321  max mem: 15572
Epoch: [9]  [ 600/2809]  eta: 0:22:12  lr: 0.000045  min_lr: 0.000000  loss: 4.2159 (4.1547)  class_acc: 0.2083 (0.2111)  loss_scale: 131072.0000 (69788.7521)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1639  max mem: 15572
[2025-01-15 18:50:39,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25887
[2025-01-15 18:50:39,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:50:39,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 610/2809]  eta: 0:22:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0136 (4.1531)  class_acc: 0.2500 (0.2113)  loss_scale: 131072.0000 (70255.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1373  max mem: 15572
Epoch: [9]  [ 620/2809]  eta: 0:21:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0742 (4.1520)  class_acc: 0.2083 (0.2119)  loss_scale: 65536.0000 (70179.4525)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1464  max mem: 15572
Epoch: [9]  [ 630/2809]  eta: 0:21:50  lr: 0.000045  min_lr: 0.000000  loss: 4.0742 (4.1501)  class_acc: 0.2500 (0.2133)  loss_scale: 65536.0000 (70105.8637)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1664  max mem: 15572
Epoch: [9]  [ 640/2809]  eta: 0:21:43  lr: 0.000045  min_lr: 0.000000  loss: 4.0319 (4.1484)  class_acc: 0.2500 (0.2134)  loss_scale: 65536.0000 (70034.5710)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1504  max mem: 15572
Epoch: [9]  [ 650/2809]  eta: 0:21:37  lr: 0.000045  min_lr: 0.000000  loss: 4.0722 (4.1501)  class_acc: 0.2083 (0.2130)  loss_scale: 65536.0000 (69965.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1505  max mem: 15572
Epoch: [9]  [ 660/2809]  eta: 0:21:34  lr: 0.000045  min_lr: 0.000000  loss: 4.1882 (4.1505)  class_acc: 0.2083 (0.2133)  loss_scale: 65536.0000 (69898.4569)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1940  max mem: 15572
Epoch: [9]  [ 670/2809]  eta: 0:21:26  lr: 0.000045  min_lr: 0.000000  loss: 4.1079 (4.1500)  class_acc: 0.1667 (0.2136)  loss_scale: 65536.0000 (69833.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1577  max mem: 15572
Epoch: [9]  [ 680/2809]  eta: 0:21:16  lr: 0.000045  min_lr: 0.000000  loss: 4.0459 (4.1487)  class_acc: 0.1667 (0.2140)  loss_scale: 65536.0000 (69770.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0456  max mem: 15572
Epoch: [9]  [ 690/2809]  eta: 0:21:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2387 (4.1511)  class_acc: 0.2083 (0.2134)  loss_scale: 65536.0000 (69709.0593)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1400  max mem: 15572
Epoch: [9]  [ 700/2809]  eta: 0:21:04  lr: 0.000045  min_lr: 0.000000  loss: 4.2315 (4.1491)  class_acc: 0.2083 (0.2137)  loss_scale: 65536.0000 (69649.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1595  max mem: 15572
Epoch: [9]  [ 710/2809]  eta: 0:20:58  lr: 0.000045  min_lr: 0.000000  loss: 3.9710 (4.1467)  class_acc: 0.2083 (0.2140)  loss_scale: 65536.0000 (69591.6737)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1015  max mem: 15572
[2025-01-15 18:51:43,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=168, lr=[4.3781711929090247e-07, 4.3781711929090247e-07, 6.254530275584321e-07, 6.254530275584321e-07, 8.935043250834747e-07, 8.935043250834747e-07, 1.2764347501192496e-06, 1.2764347501192496e-06, 1.8234782144560709e-06, 1.8234782144560709e-06, 2.604968877794387e-06, 2.604968877794387e-06, 3.721384111134839e-06, 3.721384111134839e-06, 5.3162630159069135e-06, 5.3162630159069135e-06, 7.59466145129559e-06, 7.59466145129559e-06, 1.0849516358993702e-05, 1.0849516358993702e-05, 1.5499309084276716e-05, 1.5499309084276716e-05, 2.2141870120395312e-05, 2.2141870120395312e-05, 3.163124302913616e-05, 3.163124302913616e-05, 4.518749004162309e-05, 4.518749004162309e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 18:51:43,775] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=28.431963230271325, CurrSamplesPerSec=27.981816028780354, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [ 720/2809]  eta: 0:20:49  lr: 0.000045  min_lr: 0.000000  loss: 3.9976 (4.1456)  class_acc: 0.2083 (0.2145)  loss_scale: 65536.0000 (69535.4230)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1128  max mem: 15572
Epoch: [9]  [ 730/2809]  eta: 0:20:42  lr: 0.000045  min_lr: 0.000000  loss: 4.1477 (4.1476)  class_acc: 0.2083 (0.2139)  loss_scale: 65536.0000 (69480.7114)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0794  max mem: 15572
[2025-01-15 18:51:54,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:51:54,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 740/2809]  eta: 0:20:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1477 (4.1459)  class_acc: 0.2083 (0.2143)  loss_scale: 65536.0000 (69958.1323)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1027  max mem: 15572
[2025-01-15 18:51:56,889] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26022
[2025-01-15 18:51:56,889] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:51:56,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 750/2809]  eta: 0:20:27  lr: 0.000045  min_lr: 0.000000  loss: 4.1902 (4.1496)  class_acc: 0.1667 (0.2136)  loss_scale: 65536.0000 (69899.2490)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1023  max mem: 15572
Epoch: [9]  [ 760/2809]  eta: 0:20:21  lr: 0.000045  min_lr: 0.000000  loss: 4.1385 (4.1478)  class_acc: 0.1667 (0.2140)  loss_scale: 65536.0000 (69841.9133)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1027  max mem: 15572
Epoch: [9]  [ 770/2809]  eta: 0:20:14  lr: 0.000045  min_lr: 0.000000  loss: 4.1166 (4.1479)  class_acc: 0.2083 (0.2137)  loss_scale: 65536.0000 (69786.0649)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1234  max mem: 15572
Epoch: [9]  [ 780/2809]  eta: 0:20:09  lr: 0.000045  min_lr: 0.000000  loss: 4.1166 (4.1460)  class_acc: 0.2083 (0.2139)  loss_scale: 65536.0000 (69731.6466)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1591  max mem: 15572
Epoch: [9]  [ 790/2809]  eta: 0:20:03  lr: 0.000045  min_lr: 0.000000  loss: 3.9652 (4.1457)  class_acc: 0.2083 (0.2133)  loss_scale: 65536.0000 (69678.6043)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1781  max mem: 15572
Epoch: [9]  [ 800/2809]  eta: 0:19:58  lr: 0.000045  min_lr: 0.000000  loss: 4.2067 (4.1459)  class_acc: 0.1667 (0.2132)  loss_scale: 65536.0000 (69626.8864)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1608  max mem: 15572
Epoch: [9]  [ 810/2809]  eta: 0:19:52  lr: 0.000045  min_lr: 0.000000  loss: 4.2067 (4.1470)  class_acc: 0.2083 (0.2130)  loss_scale: 65536.0000 (69576.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1351  max mem: 15572
Epoch: [9]  [ 820/2809]  eta: 0:19:48  lr: 0.000045  min_lr: 0.000000  loss: 4.2048 (4.1470)  class_acc: 0.2083 (0.2128)  loss_scale: 65536.0000 (69527.2302)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1862  max mem: 15572
Epoch: [9]  [ 830/2809]  eta: 0:19:40  lr: 0.000045  min_lr: 0.000000  loss: 4.2048 (4.1473)  class_acc: 0.1667 (0.2124)  loss_scale: 65536.0000 (69479.2010)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1656  max mem: 15572
Epoch: [9]  [ 840/2809]  eta: 0:19:35  lr: 0.000045  min_lr: 0.000000  loss: 3.9298 (4.1430)  class_acc: 0.2500 (0.2136)  loss_scale: 65536.0000 (69432.3139)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1181  max mem: 15572
Epoch: [9]  [ 850/2809]  eta: 0:19:28  lr: 0.000045  min_lr: 0.000000  loss: 3.9772 (4.1437)  class_acc: 0.2500 (0.2136)  loss_scale: 65536.0000 (69386.5288)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1451  max mem: 15572
Epoch: [9]  [ 860/2809]  eta: 0:19:21  lr: 0.000045  min_lr: 0.000000  loss: 4.1757 (4.1454)  class_acc: 0.1667 (0.2127)  loss_scale: 65536.0000 (69341.8072)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1201  max mem: 15572
[2025-01-15 18:53:12,802] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:53:12,802] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 870/2809]  eta: 0:19:14  lr: 0.000045  min_lr: 0.000000  loss: 4.1415 (4.1441)  class_acc: 0.1667 (0.2132)  loss_scale: 65536.0000 (69373.3548)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1133  max mem: 15572
[2025-01-15 18:53:15,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26156
[2025-01-15 18:53:15,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:53:15,081] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 880/2809]  eta: 0:19:10  lr: 0.000045  min_lr: 0.000000  loss: 4.0302 (4.1427)  class_acc: 0.2083 (0.2132)  loss_scale: 65536.0000 (69627.3507)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1751  max mem: 15572
Epoch: [9]  [ 890/2809]  eta: 0:19:04  lr: 0.000045  min_lr: 0.000000  loss: 4.1681 (4.1433)  class_acc: 0.2083 (0.2133)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1781  max mem: 15572
Epoch: [9]  [ 900/2809]  eta: 0:18:56  lr: 0.000045  min_lr: 0.000000  loss: 4.1793 (4.1422)  class_acc: 0.2083 (0.2135)  loss_scale: 65536.0000 (69536.5327)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1045  max mem: 15572
Epoch: [9]  [ 910/2809]  eta: 0:18:51  lr: 0.000045  min_lr: 0.000000  loss: 4.1562 (4.1432)  class_acc: 0.2083 (0.2136)  loss_scale: 65536.0000 (69492.6191)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1483  max mem: 15572
Epoch: [9]  [ 920/2809]  eta: 0:18:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2259 (4.1438)  class_acc: 0.2083 (0.2135)  loss_scale: 65536.0000 (69449.6591)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1386  max mem: 15572
Epoch: [9]  [ 930/2809]  eta: 0:18:38  lr: 0.000045  min_lr: 0.000000  loss: 4.1890 (4.1422)  class_acc: 0.2500 (0.2137)  loss_scale: 65536.0000 (69407.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0925  max mem: 15572
Epoch: [9]  [ 940/2809]  eta: 0:18:33  lr: 0.000045  min_lr: 0.000000  loss: 4.1294 (4.1420)  class_acc: 0.2500 (0.2140)  loss_scale: 65536.0000 (69366.4782)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1777  max mem: 15572
Epoch: [9]  [ 950/2809]  eta: 0:18:26  lr: 0.000045  min_lr: 0.000000  loss: 4.1294 (4.1404)  class_acc: 0.2083 (0.2145)  loss_scale: 65536.0000 (69326.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1653  max mem: 15572
Epoch: [9]  [ 960/2809]  eta: 0:18:20  lr: 0.000045  min_lr: 0.000000  loss: 4.0695 (4.1403)  class_acc: 0.2083 (0.2146)  loss_scale: 65536.0000 (69286.7596)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1147  max mem: 15572
Epoch: [9]  [ 970/2809]  eta: 0:18:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1354 (4.1417)  class_acc: 0.2083 (0.2142)  loss_scale: 65536.0000 (69248.1318)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1243  max mem: 15572
Epoch: [9]  [ 980/2809]  eta: 0:18:07  lr: 0.000045  min_lr: 0.000000  loss: 4.3189 (4.1436)  class_acc: 0.1667 (0.2139)  loss_scale: 65536.0000 (69210.2915)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1274  max mem: 15572
Epoch: [9]  [ 990/2809]  eta: 0:18:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2479 (4.1440)  class_acc: 0.1250 (0.2129)  loss_scale: 65536.0000 (69173.2149)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1107  max mem: 15572
Epoch: [9]  [1000/2809]  eta: 0:17:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2184 (4.1454)  class_acc: 0.1250 (0.2124)  loss_scale: 65536.0000 (69136.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1446  max mem: 15572
[2025-01-15 18:54:31,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:54:31,455] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1010/2809]  eta: 0:17:48  lr: 0.000045  min_lr: 0.000000  loss: 4.2164 (4.1464)  class_acc: 0.1667 (0.2122)  loss_scale: 65536.0000 (69555.0227)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1365  max mem: 15572
[2025-01-15 18:54:37,095] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26296
[2025-01-15 18:54:37,096] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:54:37,096] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1020/2809]  eta: 0:17:41  lr: 0.000045  min_lr: 0.000000  loss: 4.2164 (4.1473)  class_acc: 0.2083 (0.2121)  loss_scale: 131072.0000 (69772.4114)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0720  max mem: 15572
Epoch: [9]  [1030/2809]  eta: 0:17:35  lr: 0.000045  min_lr: 0.000000  loss: 4.0905 (4.1471)  class_acc: 0.1667 (0.2119)  loss_scale: 65536.0000 (69731.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1076  max mem: 15572
Epoch: [9]  [1040/2809]  eta: 0:17:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1226 (4.1474)  class_acc: 0.1667 (0.2120)  loss_scale: 65536.0000 (69691.0202)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1089  max mem: 15572
Epoch: [9]  [1050/2809]  eta: 0:17:23  lr: 0.000045  min_lr: 0.000000  loss: 4.1917 (4.1473)  class_acc: 0.1667 (0.2119)  loss_scale: 65536.0000 (69651.4862)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1192  max mem: 15572
Epoch: [9]  [1060/2809]  eta: 0:17:19  lr: 0.000045  min_lr: 0.000000  loss: 4.2777 (4.1494)  class_acc: 0.1667 (0.2115)  loss_scale: 65536.0000 (69612.6975)  weight_decay: 0.0500 (0.0500)  time: 0.6528  data: 0.1962  max mem: 15572
Epoch: [9]  [1070/2809]  eta: 0:17:13  lr: 0.000045  min_lr: 0.000000  loss: 4.2002 (4.1488)  class_acc: 0.1667 (0.2116)  loss_scale: 65536.0000 (69574.6331)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1665  max mem: 15572
Epoch: [9]  [1080/2809]  eta: 0:17:05  lr: 0.000045  min_lr: 0.000000  loss: 4.0839 (4.1488)  class_acc: 0.2083 (0.2118)  loss_scale: 65536.0000 (69537.2729)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0766  max mem: 15572
Epoch: [9]  [1090/2809]  eta: 0:16:59  lr: 0.000045  min_lr: 0.000000  loss: 4.1760 (4.1481)  class_acc: 0.2083 (0.2116)  loss_scale: 65536.0000 (69500.5976)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0778  max mem: 15572
Epoch: [9]  [1100/2809]  eta: 0:16:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1111 (4.1480)  class_acc: 0.2083 (0.2117)  loss_scale: 65536.0000 (69464.5886)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1268  max mem: 15572
Epoch: [9]  [1110/2809]  eta: 0:16:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1055 (4.1480)  class_acc: 0.1667 (0.2114)  loss_scale: 65536.0000 (69429.2277)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1389  max mem: 15572
Epoch: [9]  [1120/2809]  eta: 0:16:39  lr: 0.000045  min_lr: 0.000000  loss: 4.0808 (4.1473)  class_acc: 0.1667 (0.2113)  loss_scale: 65536.0000 (69394.4978)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0902  max mem: 15572
Epoch: [9]  [1130/2809]  eta: 0:16:33  lr: 0.000045  min_lr: 0.000000  loss: 4.0693 (4.1471)  class_acc: 0.2083 (0.2112)  loss_scale: 65536.0000 (69360.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1017  max mem: 15572
Epoch: [9]  [1140/2809]  eta: 0:16:28  lr: 0.000045  min_lr: 0.000000  loss: 4.1643 (4.1478)  class_acc: 0.2083 (0.2109)  loss_scale: 65536.0000 (69326.8642)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1563  max mem: 15572
[2025-01-15 18:55:51,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:55:51,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 18:55:54,313] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26430
[2025-01-15 18:55:54,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:55:54,313] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1150/2809]  eta: 0:16:20  lr: 0.000045  min_lr: 0.000000  loss: 4.1378 (4.1470)  class_acc: 0.2083 (0.2107)  loss_scale: 65536.0000 (69578.6203)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1012  max mem: 15572
Epoch: [9]  [1160/2809]  eta: 0:16:14  lr: 0.000045  min_lr: 0.000000  loss: 3.9711 (4.1456)  class_acc: 0.2083 (0.2110)  loss_scale: 65536.0000 (69543.8002)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0652  max mem: 15572
Epoch: [9]  [1170/2809]  eta: 0:16:08  lr: 0.000045  min_lr: 0.000000  loss: 4.0411 (4.1464)  class_acc: 0.2083 (0.2110)  loss_scale: 65536.0000 (69509.5747)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1216  max mem: 15572
Epoch: [9]  [1180/2809]  eta: 0:16:02  lr: 0.000045  min_lr: 0.000000  loss: 4.1813 (4.1465)  class_acc: 0.2083 (0.2115)  loss_scale: 65536.0000 (69475.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1309  max mem: 15572
Epoch: [9]  [1190/2809]  eta: 0:15:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0615 (4.1454)  class_acc: 0.2500 (0.2119)  loss_scale: 65536.0000 (69442.8480)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1355  max mem: 15572
Epoch: [9]  [1200/2809]  eta: 0:15:50  lr: 0.000045  min_lr: 0.000000  loss: 3.9342 (4.1449)  class_acc: 0.2500 (0.2118)  loss_scale: 65536.0000 (69410.3181)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1109  max mem: 15572
Epoch: [9]  [1210/2809]  eta: 0:15:44  lr: 0.000045  min_lr: 0.000000  loss: 4.0525 (4.1452)  class_acc: 0.2083 (0.2118)  loss_scale: 65536.0000 (69378.3254)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0735  max mem: 15572
Epoch: [9]  [1220/2809]  eta: 0:15:37  lr: 0.000045  min_lr: 0.000000  loss: 4.2867 (4.1476)  class_acc: 0.1667 (0.2111)  loss_scale: 65536.0000 (69346.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0850  max mem: 15572
Epoch: [9]  [1230/2809]  eta: 0:15:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2729 (4.1481)  class_acc: 0.1667 (0.2113)  loss_scale: 65536.0000 (69315.8993)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1138  max mem: 15572
Epoch: [9]  [1240/2809]  eta: 0:15:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2244 (4.1487)  class_acc: 0.2083 (0.2112)  loss_scale: 65536.0000 (69285.4408)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1308  max mem: 15572
Epoch: [9]  [1250/2809]  eta: 0:15:18  lr: 0.000045  min_lr: 0.000000  loss: 4.2311 (4.1502)  class_acc: 0.1667 (0.2110)  loss_scale: 65536.0000 (69255.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1273  max mem: 15572
Epoch: [9]  [1260/2809]  eta: 0:15:13  lr: 0.000045  min_lr: 0.000000  loss: 4.4054 (4.1525)  class_acc: 0.1667 (0.2107)  loss_scale: 65536.0000 (69225.9730)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1312  max mem: 15572
Epoch: [9]  [1270/2809]  eta: 0:15:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3592 (4.1531)  class_acc: 0.1667 (0.2106)  loss_scale: 65536.0000 (69196.9410)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1236  max mem: 15572
[2025-01-15 18:57:09,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:57:09,087] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1280/2809]  eta: 0:15:02  lr: 0.000045  min_lr: 0.000000  loss: 4.1366 (4.1519)  class_acc: 0.2083 (0.2112)  loss_scale: 65536.0000 (69321.8423)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1726  max mem: 15572
[2025-01-15 18:57:13,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26565
[2025-01-15 18:57:13,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 18:57:13,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1290/2809]  eta: 0:14:56  lr: 0.000045  min_lr: 0.000000  loss: 4.1470 (4.1531)  class_acc: 0.2500 (0.2110)  loss_scale: 65536.0000 (69444.8087)  weight_decay: 0.0500 (0.0500)  time: 0.6477  data: 0.1890  max mem: 15572
Epoch: [9]  [1300/2809]  eta: 0:14:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2323 (4.1536)  class_acc: 0.2083 (0.2110)  loss_scale: 65536.0000 (69414.7640)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1183  max mem: 15572
Epoch: [9]  [1310/2809]  eta: 0:14:43  lr: 0.000045  min_lr: 0.000000  loss: 4.1687 (4.1534)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (69385.1777)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1111  max mem: 15572
Epoch: [9]  [1320/2809]  eta: 0:14:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1720 (4.1539)  class_acc: 0.1667 (0.2107)  loss_scale: 65536.0000 (69356.0394)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1315  max mem: 15572
Epoch: [9]  [1330/2809]  eta: 0:14:32  lr: 0.000045  min_lr: 0.000000  loss: 4.1824 (4.1542)  class_acc: 0.2083 (0.2108)  loss_scale: 65536.0000 (69327.3388)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1587  max mem: 15572
Epoch: [9]  [1340/2809]  eta: 0:14:26  lr: 0.000045  min_lr: 0.000000  loss: 4.0000 (4.1532)  class_acc: 0.2500 (0.2112)  loss_scale: 65536.0000 (69299.0664)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1517  max mem: 15572
[2025-01-15 18:57:47,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26626
[2025-01-15 18:57:47,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 18:57:47,631] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [1350/2809]  eta: 0:14:19  lr: 0.000045  min_lr: 0.000000  loss: 3.9916 (4.1529)  class_acc: 0.2500 (0.2109)  loss_scale: 65536.0000 (69125.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0779  max mem: 15572
Epoch: [9]  [1360/2809]  eta: 0:14:14  lr: 0.000045  min_lr: 0.000000  loss: 3.8550 (4.1505)  class_acc: 0.2917 (0.2115)  loss_scale: 32768.0000 (68858.5452)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1412  max mem: 15572
Epoch: [9]  [1370/2809]  eta: 0:14:08  lr: 0.000045  min_lr: 0.000000  loss: 4.1271 (4.1518)  class_acc: 0.2083 (0.2111)  loss_scale: 32768.0000 (68595.3027)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1386  max mem: 15572
Epoch: [9]  [1380/2809]  eta: 0:14:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3067 (4.1530)  class_acc: 0.1250 (0.2107)  loss_scale: 32768.0000 (68335.8726)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1127  max mem: 15572
Epoch: [9]  [1390/2809]  eta: 0:13:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2490 (4.1532)  class_acc: 0.1250 (0.2105)  loss_scale: 32768.0000 (68080.1725)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1667  max mem: 15572
Epoch: [9]  [1400/2809]  eta: 0:13:51  lr: 0.000045  min_lr: 0.000000  loss: 4.2527 (4.1551)  class_acc: 0.1667 (0.2102)  loss_scale: 32768.0000 (67828.1228)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1653  max mem: 15572
Epoch: [9]  [1410/2809]  eta: 0:13:45  lr: 0.000045  min_lr: 0.000000  loss: 4.2074 (4.1553)  class_acc: 0.1667 (0.2101)  loss_scale: 32768.0000 (67579.6456)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1900  max mem: 15572
Epoch: [9]  [1420/2809]  eta: 0:13:40  lr: 0.000045  min_lr: 0.000000  loss: 4.0835 (4.1554)  class_acc: 0.1667 (0.2102)  loss_scale: 32768.0000 (67334.6657)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1800  max mem: 15572
Epoch: [9]  [1430/2809]  eta: 0:13:34  lr: 0.000045  min_lr: 0.000000  loss: 4.1335 (4.1556)  class_acc: 0.2083 (0.2103)  loss_scale: 32768.0000 (67093.1097)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1474  max mem: 15572
Epoch: [9]  [1440/2809]  eta: 0:13:28  lr: 0.000045  min_lr: 0.000000  loss: 4.2941 (4.1566)  class_acc: 0.2083 (0.2103)  loss_scale: 32768.0000 (66854.9063)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1309  max mem: 15572
Epoch: [9]  [1450/2809]  eta: 0:13:22  lr: 0.000045  min_lr: 0.000000  loss: 4.1906 (4.1562)  class_acc: 0.2500 (0.2105)  loss_scale: 32768.0000 (66619.9862)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1362  max mem: 15572
Epoch: [9]  [1460/2809]  eta: 0:13:16  lr: 0.000045  min_lr: 0.000000  loss: 4.0337 (4.1554)  class_acc: 0.2500 (0.2107)  loss_scale: 32768.0000 (66388.2820)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1640  max mem: 15572
Epoch: [9]  [1470/2809]  eta: 0:13:10  lr: 0.000045  min_lr: 0.000000  loss: 4.1062 (4.1551)  class_acc: 0.2083 (0.2108)  loss_scale: 32768.0000 (66159.7281)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1428  max mem: 15572
[2025-01-15 18:59:04,371] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 18:59:04,371] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [1480/2809]  eta: 0:13:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0463 (4.1535)  class_acc: 0.2083 (0.2111)  loss_scale: 32768.0000 (66089.1398)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1101  max mem: 15572
Epoch: [9]  [1490/2809]  eta: 0:12:58  lr: 0.000045  min_lr: 0.000000  loss: 4.0829 (4.1540)  class_acc: 0.2500 (0.2111)  loss_scale: 65536.0000 (66085.4299)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1651  max mem: 15572
Epoch: [9]  [1500/2809]  eta: 0:12:52  lr: 0.000045  min_lr: 0.000000  loss: 4.2560 (4.1544)  class_acc: 0.2500 (0.2111)  loss_scale: 65536.0000 (66081.7695)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1310  max mem: 15572
Epoch: [9]  [1510/2809]  eta: 0:12:46  lr: 0.000045  min_lr: 0.000000  loss: 4.1547 (4.1541)  class_acc: 0.2083 (0.2113)  loss_scale: 65536.0000 (66078.1575)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1200  max mem: 15572
Epoch: [9]  [1520/2809]  eta: 0:12:40  lr: 0.000045  min_lr: 0.000000  loss: 4.0543 (4.1539)  class_acc: 0.2083 (0.2112)  loss_scale: 65536.0000 (66074.5930)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1884  max mem: 15572
Epoch: [9]  [1530/2809]  eta: 0:12:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1978 (4.1550)  class_acc: 0.1250 (0.2107)  loss_scale: 65536.0000 (66071.0751)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1850  max mem: 15572
Epoch: [9]  [1540/2809]  eta: 0:12:29  lr: 0.000045  min_lr: 0.000000  loss: 4.2816 (4.1560)  class_acc: 0.1250 (0.2103)  loss_scale: 65536.0000 (66067.6029)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1356  max mem: 15572
Epoch: [9]  [1550/2809]  eta: 0:12:22  lr: 0.000045  min_lr: 0.000000  loss: 4.3354 (4.1568)  class_acc: 0.1667 (0.2100)  loss_scale: 65536.0000 (66064.1754)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0699  max mem: 15572
Epoch: [9]  [1560/2809]  eta: 0:12:16  lr: 0.000045  min_lr: 0.000000  loss: 4.2358 (4.1560)  class_acc: 0.1667 (0.2102)  loss_scale: 65536.0000 (66060.7918)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0789  max mem: 15572
Epoch: [9]  [1570/2809]  eta: 0:12:10  lr: 0.000045  min_lr: 0.000000  loss: 4.3477 (4.1571)  class_acc: 0.1667 (0.2099)  loss_scale: 65536.0000 (66057.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1055  max mem: 15572
Epoch: [9]  [1580/2809]  eta: 0:12:04  lr: 0.000045  min_lr: 0.000000  loss: 4.4498 (4.1587)  class_acc: 0.1250 (0.2094)  loss_scale: 65536.0000 (66054.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.0811  max mem: 15572
Epoch: [9]  [1590/2809]  eta: 0:11:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3678 (4.1596)  class_acc: 0.1250 (0.2091)  loss_scale: 65536.0000 (66050.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1367  max mem: 15572
Epoch: [9]  [1600/2809]  eta: 0:11:53  lr: 0.000045  min_lr: 0.000000  loss: 4.3434 (4.1602)  class_acc: 0.1250 (0.2089)  loss_scale: 65536.0000 (66047.6802)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.2031  max mem: 15572
[2025-01-15 19:00:19,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:00:19,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:00:24,251] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26891
[2025-01-15 19:00:24,252] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:00:24,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1610/2809]  eta: 0:11:46  lr: 0.000045  min_lr: 0.000000  loss: 4.2893 (4.1605)  class_acc: 0.1250 (0.2087)  loss_scale: 65536.0000 (66369.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1451  max mem: 15572
Epoch: [9]  [1620/2809]  eta: 0:11:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1906 (4.1611)  class_acc: 0.1667 (0.2086)  loss_scale: 65536.0000 (66364.8020)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0680  max mem: 15572
Epoch: [9]  [1630/2809]  eta: 0:11:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1499 (4.1606)  class_acc: 0.2083 (0.2088)  loss_scale: 65536.0000 (66359.7204)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1309  max mem: 15572
Epoch: [9]  [1640/2809]  eta: 0:11:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1499 (4.1612)  class_acc: 0.2500 (0.2090)  loss_scale: 65536.0000 (66354.7008)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.1998  max mem: 15572
Epoch: [9]  [1650/2809]  eta: 0:11:23  lr: 0.000045  min_lr: 0.000000  loss: 4.0650 (4.1598)  class_acc: 0.2083 (0.2092)  loss_scale: 65536.0000 (66349.7420)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1839  max mem: 15572
Epoch: [9]  [1660/2809]  eta: 0:11:17  lr: 0.000045  min_lr: 0.000000  loss: 3.9979 (4.1595)  class_acc: 0.2083 (0.2093)  loss_scale: 65536.0000 (66344.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1027  max mem: 15572
Epoch: [9]  [1670/2809]  eta: 0:11:10  lr: 0.000045  min_lr: 0.000000  loss: 4.1865 (4.1593)  class_acc: 0.2083 (0.2093)  loss_scale: 65536.0000 (66340.0024)  weight_decay: 0.0500 (0.0500)  time: 0.4866  data: 0.0379  max mem: 15572
Epoch: [9]  [1680/2809]  eta: 0:11:05  lr: 0.000045  min_lr: 0.000000  loss: 4.3400 (4.1603)  class_acc: 0.2083 (0.2092)  loss_scale: 65536.0000 (66335.2195)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1158  max mem: 15572
Epoch: [9]  [1690/2809]  eta: 0:10:59  lr: 0.000045  min_lr: 0.000000  loss: 4.1421 (4.1593)  class_acc: 0.2500 (0.2095)  loss_scale: 65536.0000 (66330.4932)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.2042  max mem: 15572
Epoch: [9]  [1700/2809]  eta: 0:10:54  lr: 0.000045  min_lr: 0.000000  loss: 4.1069 (4.1586)  class_acc: 0.2500 (0.2096)  loss_scale: 65536.0000 (66325.8225)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.2188  max mem: 15572
Epoch: [9]  [1710/2809]  eta: 0:10:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1224 (4.1589)  class_acc: 0.2083 (0.2094)  loss_scale: 65536.0000 (66321.2063)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1643  max mem: 15572
[2025-01-15 19:01:29,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=175, lr=[4.3500795337734684e-07, 4.3500795337734684e-07, 6.214399333962098e-07, 6.214399333962098e-07, 8.877713334231571e-07, 8.877713334231571e-07, 1.2682447620330817e-06, 1.2682447620330817e-06, 1.811778231475831e-06, 1.811778231475831e-06, 2.5882546163940444e-06, 2.5882546163940444e-06, 3.697506594848635e-06, 3.697506594848635e-06, 5.282152278355193e-06, 5.282152278355193e-06, 7.545931826221704e-06, 7.545931826221704e-06, 1.077990260888815e-05, 1.077990260888815e-05, 1.5399860869840216e-05, 1.5399860869840216e-05, 2.199980124262888e-05, 2.199980124262888e-05, 3.142828748946983e-05, 3.142828748946983e-05, 4.489755355638548e-05, 4.489755355638548e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:01:29,021] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=28.423645034656634, CurrSamplesPerSec=31.641293899836235, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [1720/2809]  eta: 0:10:42  lr: 0.000045  min_lr: 0.000000  loss: 4.0791 (4.1585)  class_acc: 0.2083 (0.2096)  loss_scale: 65536.0000 (66316.6438)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1305  max mem: 15572
Epoch: [9]  [1730/2809]  eta: 0:10:36  lr: 0.000045  min_lr: 0.000000  loss: 3.9241 (4.1575)  class_acc: 0.2500 (0.2099)  loss_scale: 65536.0000 (66312.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1512  max mem: 15572
[2025-01-15 19:01:41,796] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:01:41,797] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1740/2809]  eta: 0:10:31  lr: 0.000045  min_lr: 0.000000  loss: 3.9626 (4.1576)  class_acc: 0.2500 (0.2099)  loss_scale: 65536.0000 (66382.9615)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1723  max mem: 15572
[2025-01-15 19:01:46,336] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27030
[2025-01-15 19:01:46,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:01:46,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1750/2809]  eta: 0:10:24  lr: 0.000045  min_lr: 0.000000  loss: 4.2463 (4.1578)  class_acc: 0.1667 (0.2097)  loss_scale: 65536.0000 (66677.5465)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0954  max mem: 15572
Epoch: [9]  [1760/2809]  eta: 0:10:18  lr: 0.000045  min_lr: 0.000000  loss: 4.1558 (4.1570)  class_acc: 0.2083 (0.2100)  loss_scale: 65536.0000 (66671.0642)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0618  max mem: 15572
Epoch: [9]  [1770/2809]  eta: 0:10:12  lr: 0.000045  min_lr: 0.000000  loss: 4.0453 (4.1573)  class_acc: 0.2083 (0.2101)  loss_scale: 65536.0000 (66664.6550)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1531  max mem: 15572
Epoch: [9]  [1780/2809]  eta: 0:10:07  lr: 0.000045  min_lr: 0.000000  loss: 4.1619 (4.1579)  class_acc: 0.1250 (0.2095)  loss_scale: 65536.0000 (66658.3178)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1969  max mem: 15572
Epoch: [9]  [1790/2809]  eta: 0:10:01  lr: 0.000045  min_lr: 0.000000  loss: 4.1456 (4.1572)  class_acc: 0.1250 (0.2098)  loss_scale: 65536.0000 (66652.0514)  weight_decay: 0.0500 (0.0500)  time: 0.6458  data: 0.1882  max mem: 15572
Epoch: [9]  [1800/2809]  eta: 0:09:55  lr: 0.000045  min_lr: 0.000000  loss: 4.1232 (4.1568)  class_acc: 0.2500 (0.2100)  loss_scale: 65536.0000 (66645.8545)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1353  max mem: 15572
Epoch: [9]  [1810/2809]  eta: 0:09:49  lr: 0.000045  min_lr: 0.000000  loss: 4.2661 (4.1579)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (66639.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1179  max mem: 15572
Epoch: [9]  [1820/2809]  eta: 0:09:43  lr: 0.000045  min_lr: 0.000000  loss: 4.2236 (4.1567)  class_acc: 0.2083 (0.2102)  loss_scale: 65536.0000 (66633.6650)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1273  max mem: 15572
Epoch: [9]  [1830/2809]  eta: 0:09:37  lr: 0.000045  min_lr: 0.000000  loss: 4.2122 (4.1571)  class_acc: 0.2083 (0.2098)  loss_scale: 65536.0000 (66627.6701)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1160  max mem: 15572
Epoch: [9]  [1840/2809]  eta: 0:09:31  lr: 0.000045  min_lr: 0.000000  loss: 4.2858 (4.1573)  class_acc: 0.1250 (0.2097)  loss_scale: 65536.0000 (66621.7404)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0960  max mem: 15572
Epoch: [9]  [1850/2809]  eta: 0:09:25  lr: 0.000045  min_lr: 0.000000  loss: 4.0891 (4.1567)  class_acc: 0.1667 (0.2095)  loss_scale: 65536.0000 (66615.8747)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1527  max mem: 15572
Epoch: [9]  [1860/2809]  eta: 0:09:19  lr: 0.000045  min_lr: 0.000000  loss: 4.1577 (4.1566)  class_acc: 0.1667 (0.2096)  loss_scale: 65536.0000 (66610.0720)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1714  max mem: 15572
Epoch: [9]  [1870/2809]  eta: 0:09:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1577 (4.1575)  class_acc: 0.1667 (0.2095)  loss_scale: 65536.0000 (66604.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1065  max mem: 15572
[2025-01-15 19:03:02,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:03:02,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:03:03,520] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27161
[2025-01-15 19:03:03,520] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:03:03,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1880/2809]  eta: 0:09:07  lr: 0.000045  min_lr: 0.000000  loss: 4.1563 (4.1581)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (66668.3339)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0948  max mem: 15572
Epoch: [9]  [1890/2809]  eta: 0:09:02  lr: 0.000045  min_lr: 0.000000  loss: 4.2310 (4.1583)  class_acc: 0.1667 (0.2092)  loss_scale: 65536.0000 (66662.3458)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1312  max mem: 15572
Epoch: [9]  [1900/2809]  eta: 0:08:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2310 (4.1587)  class_acc: 0.1667 (0.2093)  loss_scale: 65536.0000 (66656.4208)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.1859  max mem: 15572
Epoch: [9]  [1910/2809]  eta: 0:08:50  lr: 0.000045  min_lr: 0.000000  loss: 4.0033 (4.1585)  class_acc: 0.2083 (0.2092)  loss_scale: 65536.0000 (66650.5578)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1424  max mem: 15572
Epoch: [9]  [1920/2809]  eta: 0:08:44  lr: 0.000045  min_lr: 0.000000  loss: 4.2062 (4.1593)  class_acc: 0.2083 (0.2091)  loss_scale: 65536.0000 (66644.7559)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1172  max mem: 15572
Epoch: [9]  [1930/2809]  eta: 0:08:38  lr: 0.000045  min_lr: 0.000000  loss: 4.3655 (4.1597)  class_acc: 0.1667 (0.2091)  loss_scale: 65536.0000 (66639.0140)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1640  max mem: 15572
Epoch: [9]  [1940/2809]  eta: 0:08:32  lr: 0.000045  min_lr: 0.000000  loss: 4.2361 (4.1595)  class_acc: 0.1667 (0.2091)  loss_scale: 65536.0000 (66633.3313)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1386  max mem: 15572
Epoch: [9]  [1950/2809]  eta: 0:08:26  lr: 0.000045  min_lr: 0.000000  loss: 4.0335 (4.1588)  class_acc: 0.2083 (0.2092)  loss_scale: 65536.0000 (66627.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0807  max mem: 15572
Epoch: [9]  [1960/2809]  eta: 0:08:20  lr: 0.000045  min_lr: 0.000000  loss: 4.0495 (4.1584)  class_acc: 0.2083 (0.2093)  loss_scale: 65536.0000 (66622.1397)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1225  max mem: 15572
Epoch: [9]  [1970/2809]  eta: 0:08:14  lr: 0.000045  min_lr: 0.000000  loss: 3.9258 (4.1569)  class_acc: 0.2500 (0.2096)  loss_scale: 65536.0000 (66616.6291)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1560  max mem: 15572
Epoch: [9]  [1980/2809]  eta: 0:08:08  lr: 0.000045  min_lr: 0.000000  loss: 4.0015 (4.1582)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (66611.1742)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1424  max mem: 15572
Epoch: [9]  [1990/2809]  eta: 0:08:02  lr: 0.000045  min_lr: 0.000000  loss: 4.3700 (4.1585)  class_acc: 0.1667 (0.2093)  loss_scale: 65536.0000 (66605.7740)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0896  max mem: 15572
Epoch: [9]  [2000/2809]  eta: 0:07:56  lr: 0.000045  min_lr: 0.000000  loss: 4.2224 (4.1590)  class_acc: 0.1667 (0.2091)  loss_scale: 65536.0000 (66600.4278)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1238  max mem: 15572
[2025-01-15 19:04:20,015] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:04:20,015] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [2010/2809]  eta: 0:07:51  lr: 0.000045  min_lr: 0.000000  loss: 4.2224 (4.1590)  class_acc: 0.1667 (0.2090)  loss_scale: 65536.0000 (66660.3123)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.2024  max mem: 15572
[2025-01-15 19:04:23,069] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27297
[2025-01-15 19:04:23,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:04:23,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2020/2809]  eta: 0:07:45  lr: 0.000045  min_lr: 0.000000  loss: 4.1548 (4.1593)  class_acc: 0.1667 (0.2090)  loss_scale: 65536.0000 (66816.8867)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1559  max mem: 15572
[2025-01-15 19:04:29,028] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27308
[2025-01-15 19:04:29,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:04:29,028] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2030/2809]  eta: 0:07:38  lr: 0.000045  min_lr: 0.000000  loss: 4.0639 (4.1589)  class_acc: 0.1667 (0.2089)  loss_scale: 65536.0000 (66746.0443)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0954  max mem: 15572
Epoch: [9]  [2040/2809]  eta: 0:07:32  lr: 0.000045  min_lr: 0.000000  loss: 4.1859 (4.1598)  class_acc: 0.1667 (0.2087)  loss_scale: 32768.0000 (66579.5669)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0635  max mem: 15572
Epoch: [9]  [2050/2809]  eta: 0:07:27  lr: 0.000045  min_lr: 0.000000  loss: 4.3358 (4.1603)  class_acc: 0.1667 (0.2089)  loss_scale: 32768.0000 (66414.7128)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1440  max mem: 15572
Epoch: [9]  [2060/2809]  eta: 0:07:20  lr: 0.000045  min_lr: 0.000000  loss: 4.1482 (4.1605)  class_acc: 0.1667 (0.2088)  loss_scale: 32768.0000 (66251.4585)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1137  max mem: 15572
Epoch: [9]  [2070/2809]  eta: 0:07:14  lr: 0.000045  min_lr: 0.000000  loss: 4.2812 (4.1607)  class_acc: 0.1250 (0.2086)  loss_scale: 32768.0000 (66089.7808)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0338  max mem: 15572
Epoch: [9]  [2080/2809]  eta: 0:07:09  lr: 0.000045  min_lr: 0.000000  loss: 4.3272 (4.1610)  class_acc: 0.1250 (0.2082)  loss_scale: 32768.0000 (65929.6569)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1339  max mem: 15572
Epoch: [9]  [2090/2809]  eta: 0:07:03  lr: 0.000045  min_lr: 0.000000  loss: 4.1711 (4.1602)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (65771.0646)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.2002  max mem: 15572
Epoch: [9]  [2100/2809]  eta: 0:06:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0821 (4.1604)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (65613.9819)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1594  max mem: 15572
Epoch: [9]  [2110/2809]  eta: 0:06:51  lr: 0.000045  min_lr: 0.000000  loss: 4.3149 (4.1614)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (65458.3875)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1258  max mem: 15572
Epoch: [9]  [2120/2809]  eta: 0:06:45  lr: 0.000045  min_lr: 0.000000  loss: 4.2633 (4.1612)  class_acc: 0.2083 (0.2084)  loss_scale: 32768.0000 (65304.2603)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1350  max mem: 15572
Epoch: [9]  [2130/2809]  eta: 0:06:39  lr: 0.000045  min_lr: 0.000000  loss: 4.1691 (4.1621)  class_acc: 0.2083 (0.2081)  loss_scale: 32768.0000 (65151.5795)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1311  max mem: 15572
Epoch: [9]  [2140/2809]  eta: 0:06:34  lr: 0.000045  min_lr: 0.000000  loss: 4.1318 (4.1614)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (65000.3251)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1627  max mem: 15572
Epoch: [9]  [2150/2809]  eta: 0:06:28  lr: 0.000045  min_lr: 0.000000  loss: 4.0045 (4.1612)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (64850.4770)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1672  max mem: 15572
[2025-01-15 19:05:45,194] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:05:45,194] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2160/2809]  eta: 0:06:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2117 (4.1616)  class_acc: 0.2083 (0.2084)  loss_scale: 32768.0000 (64777.8325)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1159  max mem: 15572
Epoch: [9]  [2170/2809]  eta: 0:06:16  lr: 0.000045  min_lr: 0.000000  loss: 4.1636 (4.1620)  class_acc: 0.2083 (0.2082)  loss_scale: 65536.0000 (64781.3247)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1609  max mem: 15572
Epoch: [9]  [2180/2809]  eta: 0:06:10  lr: 0.000045  min_lr: 0.000000  loss: 4.0917 (4.1612)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (64784.7850)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.1887  max mem: 15572
Epoch: [9]  [2190/2809]  eta: 0:06:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0930 (4.1617)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (64788.2136)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1559  max mem: 15572
Epoch: [9]  [2200/2809]  eta: 0:05:58  lr: 0.000045  min_lr: 0.000000  loss: 4.3083 (4.1629)  class_acc: 0.1667 (0.2081)  loss_scale: 65536.0000 (64791.6111)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1248  max mem: 15572
Epoch: [9]  [2210/2809]  eta: 0:05:52  lr: 0.000045  min_lr: 0.000000  loss: 4.2766 (4.1624)  class_acc: 0.1667 (0.2081)  loss_scale: 65536.0000 (64794.9778)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0515  max mem: 15572
Epoch: [9]  [2220/2809]  eta: 0:05:46  lr: 0.000045  min_lr: 0.000000  loss: 4.0422 (4.1619)  class_acc: 0.2500 (0.2083)  loss_scale: 65536.0000 (64798.3143)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0794  max mem: 15572
Epoch: [9]  [2230/2809]  eta: 0:05:41  lr: 0.000045  min_lr: 0.000000  loss: 4.1504 (4.1619)  class_acc: 0.2500 (0.2084)  loss_scale: 65536.0000 (64801.6208)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1967  max mem: 15572
Epoch: [9]  [2240/2809]  eta: 0:05:35  lr: 0.000045  min_lr: 0.000000  loss: 4.1504 (4.1610)  class_acc: 0.2083 (0.2085)  loss_scale: 65536.0000 (64804.8978)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.1927  max mem: 15572
Epoch: [9]  [2250/2809]  eta: 0:05:29  lr: 0.000045  min_lr: 0.000000  loss: 4.0560 (4.1612)  class_acc: 0.2083 (0.2084)  loss_scale: 65536.0000 (64808.1457)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1460  max mem: 15572
Epoch: [9]  [2260/2809]  eta: 0:05:23  lr: 0.000045  min_lr: 0.000000  loss: 4.2275 (4.1627)  class_acc: 0.1667 (0.2083)  loss_scale: 65536.0000 (64811.3649)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1466  max mem: 15572
Epoch: [9]  [2270/2809]  eta: 0:05:17  lr: 0.000045  min_lr: 0.000000  loss: 4.5229 (4.1635)  class_acc: 0.1250 (0.2081)  loss_scale: 65536.0000 (64814.5557)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1481  max mem: 15572
Epoch: [9]  [2280/2809]  eta: 0:05:11  lr: 0.000045  min_lr: 0.000000  loss: 4.3145 (4.1633)  class_acc: 0.1667 (0.2081)  loss_scale: 65536.0000 (64817.7185)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1548  max mem: 15572
[2025-01-15 19:07:00,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:07:00,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:07:02,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27568
[2025-01-15 19:07:02,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:07:02,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2290/2809]  eta: 0:05:05  lr: 0.000045  min_lr: 0.000000  loss: 4.0778 (4.1629)  class_acc: 0.2083 (0.2080)  loss_scale: 65536.0000 (64906.6713)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1403  max mem: 15572
Epoch: [9]  [2300/2809]  eta: 0:05:00  lr: 0.000045  min_lr: 0.000000  loss: 4.0825 (4.1627)  class_acc: 0.1667 (0.2080)  loss_scale: 65536.0000 (64909.4063)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1824  max mem: 15572
Epoch: [9]  [2310/2809]  eta: 0:04:53  lr: 0.000045  min_lr: 0.000000  loss: 4.1715 (4.1628)  class_acc: 0.1250 (0.2079)  loss_scale: 65536.0000 (64912.1177)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1458  max mem: 15572
Epoch: [9]  [2320/2809]  eta: 0:04:48  lr: 0.000045  min_lr: 0.000000  loss: 4.1514 (4.1620)  class_acc: 0.1250 (0.2082)  loss_scale: 65536.0000 (64914.8057)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1720  max mem: 15572
[2025-01-15 19:07:25,284] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27605
[2025-01-15 19:07:25,284] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:07:25,284] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2330/2809]  eta: 0:04:42  lr: 0.000045  min_lr: 0.000000  loss: 4.0210 (4.1619)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (64819.0682)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1517  max mem: 15572
Epoch: [9]  [2340/2809]  eta: 0:04:36  lr: 0.000045  min_lr: 0.000000  loss: 4.0588 (4.1614)  class_acc: 0.1667 (0.2081)  loss_scale: 32768.0000 (64682.1563)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0756  max mem: 15572
Epoch: [9]  [2350/2809]  eta: 0:04:30  lr: 0.000045  min_lr: 0.000000  loss: 4.0294 (4.1614)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (64546.4092)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1379  max mem: 15572
Epoch: [9]  [2360/2809]  eta: 0:04:24  lr: 0.000045  min_lr: 0.000000  loss: 4.0503 (4.1617)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (64411.8119)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1001  max mem: 15572
Epoch: [9]  [2370/2809]  eta: 0:04:18  lr: 0.000045  min_lr: 0.000000  loss: 4.0906 (4.1611)  class_acc: 0.2083 (0.2084)  loss_scale: 32768.0000 (64278.3501)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1188  max mem: 15572
Epoch: [9]  [2380/2809]  eta: 0:04:12  lr: 0.000045  min_lr: 0.000000  loss: 4.1545 (4.1622)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (64146.0092)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1630  max mem: 15572
Epoch: [9]  [2390/2809]  eta: 0:04:06  lr: 0.000045  min_lr: 0.000000  loss: 4.3152 (4.1621)  class_acc: 0.1667 (0.2083)  loss_scale: 32768.0000 (64014.7754)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1937  max mem: 15572
Epoch: [9]  [2400/2809]  eta: 0:04:01  lr: 0.000045  min_lr: 0.000000  loss: 4.0418 (4.1614)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (63884.6347)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.2395  max mem: 15572
Epoch: [9]  [2410/2809]  eta: 0:03:55  lr: 0.000045  min_lr: 0.000000  loss: 4.0418 (4.1615)  class_acc: 0.2083 (0.2083)  loss_scale: 32768.0000 (63755.5736)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1947  max mem: 15572
Epoch: [9]  [2420/2809]  eta: 0:03:49  lr: 0.000045  min_lr: 0.000000  loss: 4.3467 (4.1626)  class_acc: 0.1250 (0.2080)  loss_scale: 32768.0000 (63627.5787)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1437  max mem: 15572
Epoch: [9]  [2430/2809]  eta: 0:03:43  lr: 0.000045  min_lr: 0.000000  loss: 4.3667 (4.1628)  class_acc: 0.1250 (0.2077)  loss_scale: 32768.0000 (63500.6368)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1213  max mem: 15572
Epoch: [9]  [2440/2809]  eta: 0:03:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1794 (4.1625)  class_acc: 0.1667 (0.2077)  loss_scale: 32768.0000 (63374.7349)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1140  max mem: 15572
Epoch: [9]  [2450/2809]  eta: 0:03:31  lr: 0.000045  min_lr: 0.000000  loss: 4.1410 (4.1623)  class_acc: 0.2083 (0.2078)  loss_scale: 32768.0000 (63249.8605)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1127  max mem: 15572
[2025-01-15 19:08:40,096] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:08:40,096] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [2460/2809]  eta: 0:03:25  lr: 0.000045  min_lr: 0.000000  loss: 3.9776 (4.1609)  class_acc: 0.2500 (0.2082)  loss_scale: 32768.0000 (63232.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1417  max mem: 15572
Epoch: [9]  [2470/2809]  eta: 0:03:19  lr: 0.000045  min_lr: 0.000000  loss: 3.9939 (4.1617)  class_acc: 0.2500 (0.2081)  loss_scale: 65536.0000 (63241.8422)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0952  max mem: 15572
Epoch: [9]  [2480/2809]  eta: 0:03:13  lr: 0.000045  min_lr: 0.000000  loss: 4.1737 (4.1614)  class_acc: 0.1667 (0.2080)  loss_scale: 65536.0000 (63251.0891)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1368  max mem: 15572
Epoch: [9]  [2490/2809]  eta: 0:03:07  lr: 0.000045  min_lr: 0.000000  loss: 4.0927 (4.1612)  class_acc: 0.2083 (0.2080)  loss_scale: 65536.0000 (63260.2617)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2020  max mem: 15572
Epoch: [9]  [2500/2809]  eta: 0:03:02  lr: 0.000045  min_lr: 0.000000  loss: 3.9416 (4.1600)  class_acc: 0.2500 (0.2084)  loss_scale: 65536.0000 (63269.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1133  max mem: 15572
Epoch: [9]  [2510/2809]  eta: 0:02:56  lr: 0.000045  min_lr: 0.000000  loss: 3.8934 (4.1597)  class_acc: 0.2500 (0.2086)  loss_scale: 65536.0000 (63278.3879)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0910  max mem: 15572
Epoch: [9]  [2520/2809]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000000  loss: 4.1846 (4.1596)  class_acc: 0.2083 (0.2087)  loss_scale: 65536.0000 (63287.3431)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1364  max mem: 15572
Epoch: [9]  [2530/2809]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000000  loss: 4.1846 (4.1596)  class_acc: 0.1667 (0.2086)  loss_scale: 65536.0000 (63296.2276)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1801  max mem: 15572
Epoch: [9]  [2540/2809]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000000  loss: 4.2719 (4.1609)  class_acc: 0.1667 (0.2085)  loss_scale: 65536.0000 (63305.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1545  max mem: 15572
Epoch: [9]  [2550/2809]  eta: 0:02:32  lr: 0.000045  min_lr: 0.000000  loss: 4.3614 (4.1611)  class_acc: 0.2083 (0.2086)  loss_scale: 65536.0000 (63313.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1123  max mem: 15572
Epoch: [9]  [2560/2809]  eta: 0:02:26  lr: 0.000045  min_lr: 0.000000  loss: 4.2567 (4.1614)  class_acc: 0.2083 (0.2087)  loss_scale: 65536.0000 (63322.4647)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0850  max mem: 15572
Epoch: [9]  [2570/2809]  eta: 0:02:20  lr: 0.000045  min_lr: 0.000000  loss: 4.3944 (4.1623)  class_acc: 0.2083 (0.2085)  loss_scale: 65536.0000 (63331.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1053  max mem: 15572
Epoch: [9]  [2580/2809]  eta: 0:02:14  lr: 0.000045  min_lr: 0.000000  loss: 4.3944 (4.1624)  class_acc: 0.1667 (0.2085)  loss_scale: 65536.0000 (63339.6172)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1670  max mem: 15572
[2025-01-15 19:09:54,765] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:09:54,765] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:09:56,890] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27866
[2025-01-15 19:09:56,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:09:56,890] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2590/2809]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000000  loss: 4.3305 (4.1630)  class_acc: 0.2083 (0.2085)  loss_scale: 65536.0000 (63449.2690)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1717  max mem: 15572
Epoch: [9]  [2600/2809]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000000  loss: 4.2935 (4.1626)  class_acc: 0.2083 (0.2086)  loss_scale: 65536.0000 (63457.2918)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1255  max mem: 15572
Epoch: [9]  [2610/2809]  eta: 0:01:57  lr: 0.000045  min_lr: 0.000000  loss: 4.1498 (4.1628)  class_acc: 0.2083 (0.2086)  loss_scale: 65536.0000 (63465.2532)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1243  max mem: 15572
Epoch: [9]  [2620/2809]  eta: 0:01:51  lr: 0.000045  min_lr: 0.000000  loss: 4.1498 (4.1618)  class_acc: 0.1667 (0.2087)  loss_scale: 65536.0000 (63473.1538)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1247  max mem: 15572
Epoch: [9]  [2630/2809]  eta: 0:01:45  lr: 0.000045  min_lr: 0.000000  loss: 3.8927 (4.1608)  class_acc: 0.2500 (0.2089)  loss_scale: 65536.0000 (63480.9943)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1667  max mem: 15572
Epoch: [9]  [2640/2809]  eta: 0:01:39  lr: 0.000045  min_lr: 0.000000  loss: 3.9975 (4.1608)  class_acc: 0.1667 (0.2087)  loss_scale: 65536.0000 (63488.7755)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1523  max mem: 15572
Epoch: [9]  [2650/2809]  eta: 0:01:33  lr: 0.000045  min_lr: 0.000000  loss: 4.2605 (4.1610)  class_acc: 0.1250 (0.2087)  loss_scale: 65536.0000 (63496.4979)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1335  max mem: 15572
Epoch: [9]  [2660/2809]  eta: 0:01:27  lr: 0.000045  min_lr: 0.000000  loss: 4.1928 (4.1608)  class_acc: 0.2083 (0.2087)  loss_scale: 65536.0000 (63504.1623)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1672  max mem: 15572
Epoch: [9]  [2670/2809]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000000  loss: 4.1385 (4.1610)  class_acc: 0.1667 (0.2086)  loss_scale: 65536.0000 (63511.7694)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0922  max mem: 15572
Epoch: [9]  [2680/2809]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000000  loss: 4.1385 (4.1606)  class_acc: 0.2083 (0.2087)  loss_scale: 65536.0000 (63519.3197)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0338  max mem: 15572
Epoch: [9]  [2690/2809]  eta: 0:01:10  lr: 0.000045  min_lr: 0.000000  loss: 4.2636 (4.1612)  class_acc: 0.2083 (0.2084)  loss_scale: 65536.0000 (63526.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0952  max mem: 15572
Epoch: [9]  [2700/2809]  eta: 0:01:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0721 (4.1601)  class_acc: 0.2083 (0.2087)  loss_scale: 65536.0000 (63534.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1498  max mem: 15572
Epoch: [9]  [2710/2809]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000000  loss: 3.8661 (4.1594)  class_acc: 0.2500 (0.2088)  loss_scale: 65536.0000 (63541.6363)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.1692  max mem: 15572
[2025-01-15 19:11:13,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:11:13,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:11:14,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=182, lr=[4.319867297995155e-07, 4.319867297995155e-07, 6.171238997135936e-07, 6.171238997135936e-07, 8.816055710194195e-07, 8.816055710194195e-07, 1.2594365300277421e-06, 1.2594365300277421e-06, 1.7991950428967748e-06, 1.7991950428967748e-06, 2.5702786327096783e-06, 2.5702786327096783e-06, 3.6718266181566832e-06, 3.6718266181566832e-06, 5.2454665973666914e-06, 5.2454665973666914e-06, 7.493523710523844e-06, 7.493523710523844e-06, 1.0705033872176922e-05, 1.0705033872176922e-05, 1.5292905531681318e-05, 1.5292905531681318e-05, 2.1847007902401884e-05, 2.1847007902401884e-05, 3.121001128914555e-05, 3.121001128914555e-05, 4.4585730413065074e-05, 4.4585730413065074e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:11:14,845] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=28.419451533670212, CurrSamplesPerSec=25.27439014285865, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [9]  [2720/2809]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000000  loss: 4.0210 (4.1593)  class_acc: 0.2500 (0.2088)  loss_scale: 65536.0000 (63717.5627)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1006  max mem: 15572
[2025-01-15 19:11:19,660] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28008
[2025-01-15 19:11:19,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:11:19,660] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [2730/2809]  eta: 0:00:46  lr: 0.000045  min_lr: 0.000000  loss: 4.0415 (4.1586)  class_acc: 0.2083 (0.2090)  loss_scale: 131072.0000 (63868.2036)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0408  max mem: 15572
Epoch: [9]  [2740/2809]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000000  loss: 4.1788 (4.1585)  class_acc: 0.2083 (0.2091)  loss_scale: 65536.0000 (63874.2882)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1777  max mem: 15572
Epoch: [9]  [2750/2809]  eta: 0:00:34  lr: 0.000045  min_lr: 0.000000  loss: 4.0982 (4.1580)  class_acc: 0.2500 (0.2093)  loss_scale: 65536.0000 (63880.3286)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.2155  max mem: 15572
Epoch: [9]  [2760/2809]  eta: 0:00:28  lr: 0.000045  min_lr: 0.000000  loss: 4.2003 (4.1582)  class_acc: 0.1667 (0.2092)  loss_scale: 65536.0000 (63886.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1697  max mem: 15572
Epoch: [9]  [2770/2809]  eta: 0:00:22  lr: 0.000045  min_lr: 0.000000  loss: 4.2225 (4.1579)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (63892.2786)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1390  max mem: 15572
[2025-01-15 19:11:52,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28061
[2025-01-15 19:11:52,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:11:52,081] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [2780/2809]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000000  loss: 4.1786 (4.1580)  class_acc: 0.1667 (0.2094)  loss_scale: 65536.0000 (63886.4063)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0779  max mem: 15572
Epoch: [9]  [2790/2809]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000000  loss: 4.0816 (4.1574)  class_acc: 0.2083 (0.2096)  loss_scale: 32768.0000 (63774.9108)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0752  max mem: 15572
Epoch: [9]  [2800/2809]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000000  loss: 4.1844 (4.1579)  class_acc: 0.1667 (0.2094)  loss_scale: 32768.0000 (63664.2114)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0532  max mem: 15572
Epoch: [9]  [2808/2809]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 4.2264 (4.1585)  class_acc: 0.1667 (0.2093)  loss_scale: 32768.0000 (63576.2193)  weight_decay: 0.0500 (0.0500)  time: 0.4951  data: 0.0528  max mem: 15572
Epoch: [9] Total time: 0:27:32 (0.5882 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 4.2264 (4.1585)  class_acc: 0.1667 (0.2093)  loss_scale: 32768.0000 (63576.2193)  weight_decay: 0.0500 (0.0500)
[2025-01-15 19:12:06,010] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-01-15 19:12:06,013] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-9/mp_rank_00_model_states.pt
[2025-01-15 19:12:06,013] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-9/mp_rank_00_model_states.pt...
[2025-01-15 19:12:06,467] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-9/mp_rank_00_model_states.pt.
[2025-01-15 19:12:06,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/272]  eta: 0:20:24  loss: 0.4016 (0.4016)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.5019  data: 4.1717  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 3.8911 (3.2757)  acc1: 5.5556 (24.7475)  acc5: 33.3333 (40.4040)  time: 0.7726  data: 0.5503  max mem: 15572
Val:  [ 20/272]  eta: 0:02:20  loss: 3.3745 (3.2188)  acc1: 22.2222 (27.7778)  acc5: 55.5556 (50.2645)  time: 0.3589  data: 0.1510  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 3.3293 (3.3220)  acc1: 16.6667 (22.2222)  acc5: 61.1111 (49.4624)  time: 0.2632  data: 0.0619  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 3.1365 (3.2559)  acc1: 16.6667 (21.6802)  acc5: 61.1111 (53.6585)  time: 0.2755  data: 0.0742  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.9736 (3.2007)  acc1: 16.6667 (23.0937)  acc5: 66.6667 (55.5556)  time: 0.3574  data: 0.1528  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 2.0667 (3.0193)  acc1: 44.4444 (28.9617)  acc5: 77.7778 (58.3789)  time: 0.3146  data: 0.1096  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 2.0158 (2.9301)  acc1: 50.0000 (30.0469)  acc5: 83.3333 (61.6588)  time: 0.3166  data: 0.1094  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.5388 (2.9279)  acc1: 22.2222 (30.5898)  acc5: 77.7778 (61.4540)  time: 0.3434  data: 0.1364  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 3.5252 (3.0016)  acc1: 11.1111 (28.6325)  acc5: 50.0000 (59.6459)  time: 0.3045  data: 0.0995  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 3.6629 (3.0683)  acc1: 11.1111 (27.5028)  acc5: 50.0000 (58.6359)  time: 0.3074  data: 0.1087  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 3.6643 (3.1416)  acc1: 0.0000 (25.4254)  acc5: 44.4444 (57.1071)  time: 0.2916  data: 0.1037  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.7088 (3.1775)  acc1: 0.0000 (24.6556)  acc5: 44.4444 (56.5657)  time: 0.2899  data: 0.1122  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 3.1992 (3.1230)  acc1: 22.2222 (26.3783)  acc5: 55.5556 (57.5064)  time: 0.3183  data: 0.1309  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.5308 (3.1129)  acc1: 50.0000 (27.3838)  acc5: 61.1111 (57.4862)  time: 0.3693  data: 0.1663  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.1641 (3.1172)  acc1: 22.2222 (26.8948)  acc5: 61.1111 (57.7263)  time: 0.3763  data: 0.1706  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 3.0114 (3.0904)  acc1: 27.7778 (28.0538)  acc5: 66.6667 (58.9372)  time: 0.3477  data: 0.1452  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.9810 (3.1169)  acc1: 33.3333 (27.4854)  acc5: 66.6667 (58.3171)  time: 0.3126  data: 0.1236  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 3.0011 (3.1067)  acc1: 16.6667 (27.4401)  acc5: 61.1111 (58.8705)  time: 0.3052  data: 0.1207  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 3.2945 (3.1476)  acc1: 11.1111 (26.6143)  acc5: 55.5556 (57.5044)  time: 0.3726  data: 0.1851  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 3.2947 (3.1594)  acc1: 11.1111 (26.3682)  acc5: 44.4444 (57.3798)  time: 0.3638  data: 0.1702  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 3.0775 (3.1677)  acc1: 27.7778 (26.5403)  acc5: 66.6667 (57.4513)  time: 0.2973  data: 0.1028  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 3.1194 (3.1581)  acc1: 27.7778 (26.7220)  acc5: 61.1111 (57.7174)  time: 0.2833  data: 0.0866  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.2646 (3.1202)  acc1: 44.4444 (28.1145)  acc5: 77.7778 (58.6099)  time: 0.3062  data: 0.1000  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.9594 (3.0862)  acc1: 55.5556 (28.9534)  acc5: 83.3333 (59.5897)  time: 0.3370  data: 0.1328  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.6523 (3.1072)  acc1: 27.7778 (28.7074)  acc5: 72.2222 (59.0305)  time: 0.3181  data: 0.1268  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.1406 (3.0251)  acc1: 72.2222 (31.0558)  acc5: 77.7778 (60.4087)  time: 0.2962  data: 0.1138  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 2.0373 (3.0302)  acc1: 61.1111 (30.8528)  acc5: 83.3333 (60.2296)  time: 0.2667  data: 0.0956  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 2.0373 (3.0343)  acc1: 55.5556 (30.8622)  acc5: 83.3333 (60.2089)  time: 0.2600  data: 0.0956  max mem: 15572
Val: Total time: 0:01:30 (0.3342 s / it)
* Acc@1 30.862 Acc@5 60.209 loss 3.034
Accuracy of the network on the 4883 val videos: 30.9%
[2025-01-15 19:13:37,382] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 19:13:37,384] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 19:13:37,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 19:13:40,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 19:13:40,224] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.86%
Epoch: [10]  [   0/2809]  eta: 3:32:38  lr: 0.000045  min_lr: 0.000000  loss: 4.4051 (4.4051)  class_acc: 0.1667 (0.1667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5421  data: 4.1314  max mem: 15572
Epoch: [10]  [  10/2809]  eta: 0:36:21  lr: 0.000045  min_lr: 0.000000  loss: 3.9870 (4.0168)  class_acc: 0.2083 (0.2159)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7794  data: 0.3759  max mem: 15572
Epoch: [10]  [  20/2809]  eta: 0:30:53  lr: 0.000045  min_lr: 0.000000  loss: 3.9870 (4.0514)  class_acc: 0.2083 (0.2262)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4706  data: 0.0542  max mem: 15572
Epoch: [10]  [  30/2809]  eta: 0:30:31  lr: 0.000045  min_lr: 0.000000  loss: 4.0972 (4.0504)  class_acc: 0.2083 (0.2151)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1621  max mem: 15572
Epoch: [10]  [  40/2809]  eta: 0:31:19  lr: 0.000045  min_lr: 0.000000  loss: 4.0972 (4.0597)  class_acc: 0.2083 (0.2175)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.2519  max mem: 15572
Epoch: [10]  [  50/2809]  eta: 0:30:55  lr: 0.000045  min_lr: 0.000000  loss: 4.0097 (4.0277)  class_acc: 0.2083 (0.2239)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.2271  max mem: 15572
Epoch: [10]  [  60/2809]  eta: 0:30:24  lr: 0.000045  min_lr: 0.000000  loss: 4.1014 (4.0830)  class_acc: 0.1667 (0.2138)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1591  max mem: 15572
Epoch: [10]  [  70/2809]  eta: 0:31:29  lr: 0.000045  min_lr: 0.000000  loss: 4.1382 (4.0690)  class_acc: 0.1667 (0.2177)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7349  data: 0.2719  max mem: 15572
Epoch: [10]  [  80/2809]  eta: 0:31:18  lr: 0.000045  min_lr: 0.000000  loss: 4.0243 (4.0697)  class_acc: 0.2083 (0.2191)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7637  data: 0.3100  max mem: 15572
Epoch: [10]  [  90/2809]  eta: 0:31:09  lr: 0.000045  min_lr: 0.000000  loss: 4.0243 (4.0726)  class_acc: 0.2083 (0.2147)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6788  data: 0.2155  max mem: 15572
[2025-01-15 19:14:48,777] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:14:48,778] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 100/2809]  eta: 0:30:38  lr: 0.000045  min_lr: 0.000000  loss: 3.9449 (4.0546)  class_acc: 0.2083 (0.2215)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1702  max mem: 15572
Epoch: [10]  [ 110/2809]  eta: 0:31:05  lr: 0.000045  min_lr: 0.000000  loss: 3.9449 (4.0493)  class_acc: 0.2500 (0.2226)  loss_scale: 65536.0000 (36015.2793)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.2456  max mem: 15572
Epoch: [10]  [ 120/2809]  eta: 0:31:04  lr: 0.000045  min_lr: 0.000000  loss: 4.0831 (4.0554)  class_acc: 0.2083 (0.2231)  loss_scale: 65536.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  time: 0.7689  data: 0.3012  max mem: 15572
Epoch: [10]  [ 130/2809]  eta: 0:30:57  lr: 0.000045  min_lr: 0.000000  loss: 4.0312 (4.0407)  class_acc: 0.2917 (0.2296)  loss_scale: 65536.0000 (40522.2595)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.2310  max mem: 15572
Epoch: [10]  [ 140/2809]  eta: 0:30:30  lr: 0.000045  min_lr: 0.000000  loss: 4.0847 (4.0544)  class_acc: 0.2083 (0.2255)  loss_scale: 65536.0000 (42296.2837)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.1525  max mem: 15572
Epoch: [10]  [ 150/2809]  eta: 0:29:37  lr: 0.000045  min_lr: 0.000000  loss: 4.1526 (4.0543)  class_acc: 0.1667 (0.2221)  loss_scale: 65536.0000 (43835.3377)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0461  max mem: 15572
Epoch: [10]  [ 160/2809]  eta: 0:28:47  lr: 0.000045  min_lr: 0.000000  loss: 4.0911 (4.0498)  class_acc: 0.2083 (0.2241)  loss_scale: 65536.0000 (45183.2050)  weight_decay: 0.0500 (0.0500)  time: 0.4158  data: 0.0005  max mem: 15572
Epoch: [10]  [ 170/2809]  eta: 0:28:11  lr: 0.000045  min_lr: 0.000000  loss: 4.0911 (4.0604)  class_acc: 0.2083 (0.2217)  loss_scale: 65536.0000 (46373.4269)  weight_decay: 0.0500 (0.0500)  time: 0.4333  data: 0.0006  max mem: 15572
Epoch: [10]  [ 180/2809]  eta: 0:27:32  lr: 0.000044  min_lr: 0.000000  loss: 4.1634 (4.0679)  class_acc: 0.2083 (0.2212)  loss_scale: 65536.0000 (47432.1326)  weight_decay: 0.0500 (0.0500)  time: 0.4360  data: 0.0007  max mem: 15572
Epoch: [10]  [ 190/2809]  eta: 0:27:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1128 (4.0706)  class_acc: 0.2083 (0.2225)  loss_scale: 65536.0000 (48379.9791)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0006  max mem: 15572
Epoch: [10]  [ 200/2809]  eta: 0:26:43  lr: 0.000044  min_lr: 0.000000  loss: 4.0612 (4.0684)  class_acc: 0.2500 (0.2249)  loss_scale: 65536.0000 (49233.5124)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0433  max mem: 15572
Epoch: [10]  [ 210/2809]  eta: 0:26:34  lr: 0.000044  min_lr: 0.000000  loss: 3.9888 (4.0692)  class_acc: 0.2083 (0.2241)  loss_scale: 65536.0000 (50006.1422)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1215  max mem: 15572
Epoch: [10]  [ 220/2809]  eta: 0:26:24  lr: 0.000044  min_lr: 0.000000  loss: 4.0918 (4.0764)  class_acc: 0.1667 (0.2240)  loss_scale: 65536.0000 (50708.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1389  max mem: 15572
[2025-01-15 19:16:00,488] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:16:00,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:16:01,023] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28319
[2025-01-15 19:16:01,024] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:16:01,024] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 230/2809]  eta: 0:26:15  lr: 0.000044  min_lr: 0.000000  loss: 4.0455 (4.0695)  class_acc: 0.2083 (0.2255)  loss_scale: 65536.0000 (51634.4242)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1450  max mem: 15572
Epoch: [10]  [ 240/2809]  eta: 0:26:16  lr: 0.000044  min_lr: 0.000000  loss: 3.9127 (4.0698)  class_acc: 0.2083 (0.2253)  loss_scale: 65536.0000 (52211.2531)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1977  max mem: 15572
Epoch: [10]  [ 250/2809]  eta: 0:25:55  lr: 0.000044  min_lr: 0.000000  loss: 4.0864 (4.0715)  class_acc: 0.2083 (0.2253)  loss_scale: 65536.0000 (52742.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1225  max mem: 15572
Epoch: [10]  [ 260/2809]  eta: 0:25:54  lr: 0.000044  min_lr: 0.000000  loss: 4.0864 (4.0713)  class_acc: 0.2083 (0.2254)  loss_scale: 65536.0000 (53232.3065)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1182  max mem: 15572
Epoch: [10]  [ 270/2809]  eta: 0:25:41  lr: 0.000044  min_lr: 0.000000  loss: 4.3299 (4.0804)  class_acc: 0.1667 (0.2237)  loss_scale: 65536.0000 (53686.3173)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1649  max mem: 15572
Epoch: [10]  [ 280/2809]  eta: 0:25:31  lr: 0.000044  min_lr: 0.000000  loss: 4.3905 (4.0844)  class_acc: 0.1667 (0.2236)  loss_scale: 65536.0000 (54108.0142)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1201  max mem: 15572
Epoch: [10]  [ 290/2809]  eta: 0:25:26  lr: 0.000044  min_lr: 0.000000  loss: 4.1611 (4.0849)  class_acc: 0.2083 (0.2238)  loss_scale: 65536.0000 (54500.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1456  max mem: 15572
Epoch: [10]  [ 300/2809]  eta: 0:25:17  lr: 0.000044  min_lr: 0.000000  loss: 4.1648 (4.0869)  class_acc: 0.1667 (0.2229)  loss_scale: 65536.0000 (54867.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1261  max mem: 15572
Epoch: [10]  [ 310/2809]  eta: 0:25:07  lr: 0.000044  min_lr: 0.000000  loss: 3.9178 (4.0798)  class_acc: 0.2083 (0.2237)  loss_scale: 65536.0000 (55210.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0957  max mem: 15572
Epoch: [10]  [ 320/2809]  eta: 0:24:51  lr: 0.000044  min_lr: 0.000000  loss: 3.7872 (4.0773)  class_acc: 0.2500 (0.2252)  loss_scale: 65536.0000 (55532.0623)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0796  max mem: 15572
Epoch: [10]  [ 330/2809]  eta: 0:24:44  lr: 0.000044  min_lr: 0.000000  loss: 4.0520 (4.0808)  class_acc: 0.2083 (0.2249)  loss_scale: 65536.0000 (55834.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0951  max mem: 15572
Epoch: [10]  [ 340/2809]  eta: 0:24:37  lr: 0.000044  min_lr: 0.000000  loss: 4.1245 (4.0813)  class_acc: 0.2083 (0.2240)  loss_scale: 65536.0000 (56118.8035)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1369  max mem: 15572
Epoch: [10]  [ 350/2809]  eta: 0:24:40  lr: 0.000044  min_lr: 0.000000  loss: 4.0964 (4.0780)  class_acc: 0.2083 (0.2242)  loss_scale: 65536.0000 (56387.0997)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.2017  max mem: 15572
[2025-01-15 19:17:14,991] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:17:14,992] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 360/2809]  eta: 0:24:23  lr: 0.000044  min_lr: 0.000000  loss: 4.1493 (4.0810)  class_acc: 0.2083 (0.2243)  loss_scale: 65536.0000 (57185.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1397  max mem: 15572
[2025-01-15 19:17:19,421] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28455
[2025-01-15 19:17:19,422] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:17:19,422] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 370/2809]  eta: 0:24:16  lr: 0.000044  min_lr: 0.000000  loss: 4.2439 (4.0887)  class_acc: 0.1667 (0.2225)  loss_scale: 65536.0000 (58116.8302)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0699  max mem: 15572
Epoch: [10]  [ 380/2809]  eta: 0:24:16  lr: 0.000044  min_lr: 0.000000  loss: 4.3628 (4.0912)  class_acc: 0.1667 (0.2213)  loss_scale: 65536.0000 (58311.5591)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.1692  max mem: 15572
Epoch: [10]  [ 390/2809]  eta: 0:24:08  lr: 0.000044  min_lr: 0.000000  loss: 4.2166 (4.0958)  class_acc: 0.1667 (0.2206)  loss_scale: 65536.0000 (58496.3274)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1824  max mem: 15572
Epoch: [10]  [ 400/2809]  eta: 0:24:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2056 (4.0970)  class_acc: 0.1667 (0.2206)  loss_scale: 65536.0000 (58671.8803)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.2158  max mem: 15572
Epoch: [10]  [ 410/2809]  eta: 0:23:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0332 (4.0966)  class_acc: 0.2083 (0.2211)  loss_scale: 65536.0000 (58838.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1458  max mem: 15572
Epoch: [10]  [ 420/2809]  eta: 0:23:45  lr: 0.000044  min_lr: 0.000000  loss: 4.2116 (4.0999)  class_acc: 0.1667 (0.2201)  loss_scale: 65536.0000 (58997.9667)  weight_decay: 0.0500 (0.0500)  time: 0.4971  data: 0.0328  max mem: 15572
Epoch: [10]  [ 430/2809]  eta: 0:23:44  lr: 0.000044  min_lr: 0.000000  loss: 4.2211 (4.1018)  class_acc: 0.1667 (0.2189)  loss_scale: 65536.0000 (59149.6613)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1546  max mem: 15572
Epoch: [10]  [ 440/2809]  eta: 0:23:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1068 (4.0981)  class_acc: 0.2083 (0.2205)  loss_scale: 65536.0000 (59294.4762)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.2120  max mem: 15572
Epoch: [10]  [ 450/2809]  eta: 0:23:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1631 (4.1005)  class_acc: 0.2083 (0.2204)  loss_scale: 65536.0000 (59432.8692)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1566  max mem: 15572
Epoch: [10]  [ 460/2809]  eta: 0:23:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1980 (4.0995)  class_acc: 0.2083 (0.2207)  loss_scale: 65536.0000 (59565.2581)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.1736  max mem: 15572
Epoch: [10]  [ 470/2809]  eta: 0:23:29  lr: 0.000044  min_lr: 0.000000  loss: 4.1600 (4.1013)  class_acc: 0.2083 (0.2211)  loss_scale: 65536.0000 (59692.0255)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.2387  max mem: 15572
Epoch: [10]  [ 480/2809]  eta: 0:23:19  lr: 0.000044  min_lr: 0.000000  loss: 4.1928 (4.1034)  class_acc: 0.1667 (0.2203)  loss_scale: 65536.0000 (59813.5218)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1843  max mem: 15572
Epoch: [10]  [ 490/2809]  eta: 0:23:19  lr: 0.000044  min_lr: 0.000000  loss: 4.1537 (4.1007)  class_acc: 0.1667 (0.2211)  loss_scale: 65536.0000 (59930.0692)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1996  max mem: 15572
[2025-01-15 19:18:38,324] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:18:38,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:18:38,787] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28585
[2025-01-15 19:18:38,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:18:38,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 500/2809]  eta: 0:23:18  lr: 0.000044  min_lr: 0.000000  loss: 4.1823 (4.1029)  class_acc: 0.2083 (0.2207)  loss_scale: 65536.0000 (60172.7745)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.2795  max mem: 15572
Epoch: [10]  [ 510/2809]  eta: 0:23:09  lr: 0.000044  min_lr: 0.000000  loss: 4.1889 (4.1037)  class_acc: 0.2083 (0.2210)  loss_scale: 65536.0000 (60277.7299)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1713  max mem: 15572
Epoch: [10]  [ 520/2809]  eta: 0:23:04  lr: 0.000044  min_lr: 0.000000  loss: 4.1256 (4.1038)  class_acc: 0.2083 (0.2206)  loss_scale: 65536.0000 (60378.6564)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1281  max mem: 15572
Epoch: [10]  [ 530/2809]  eta: 0:23:00  lr: 0.000044  min_lr: 0.000000  loss: 4.2259 (4.1086)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (60475.7815)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1894  max mem: 15572
Epoch: [10]  [ 540/2809]  eta: 0:22:56  lr: 0.000044  min_lr: 0.000000  loss: 4.3157 (4.1081)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (60569.3161)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.2039  max mem: 15572
Epoch: [10]  [ 550/2809]  eta: 0:22:51  lr: 0.000044  min_lr: 0.000000  loss: 4.2900 (4.1102)  class_acc: 0.1667 (0.2191)  loss_scale: 65536.0000 (60659.4555)  weight_decay: 0.0500 (0.0500)  time: 0.6471  data: 0.1890  max mem: 15572
Epoch: [10]  [ 560/2809]  eta: 0:22:43  lr: 0.000044  min_lr: 0.000000  loss: 4.2397 (4.1097)  class_acc: 0.1667 (0.2187)  loss_scale: 65536.0000 (60746.3815)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1174  max mem: 15572
Epoch: [10]  [ 570/2809]  eta: 0:22:36  lr: 0.000044  min_lr: 0.000000  loss: 4.0759 (4.1115)  class_acc: 0.1667 (0.2180)  loss_scale: 65536.0000 (60830.2627)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.0925  max mem: 15572
Epoch: [10]  [ 580/2809]  eta: 0:22:29  lr: 0.000044  min_lr: 0.000000  loss: 4.0759 (4.1097)  class_acc: 0.2500 (0.2195)  loss_scale: 65536.0000 (60911.2565)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1300  max mem: 15572
Epoch: [10]  [ 590/2809]  eta: 0:22:20  lr: 0.000044  min_lr: 0.000000  loss: 4.1779 (4.1121)  class_acc: 0.2083 (0.2181)  loss_scale: 65536.0000 (60989.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1096  max mem: 15572
Epoch: [10]  [ 600/2809]  eta: 0:22:17  lr: 0.000044  min_lr: 0.000000  loss: 4.1620 (4.1109)  class_acc: 0.1667 (0.2184)  loss_scale: 65536.0000 (61065.1581)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1478  max mem: 15572
Epoch: [10]  [ 610/2809]  eta: 0:22:10  lr: 0.000044  min_lr: 0.000000  loss: 4.0586 (4.1125)  class_acc: 0.1667 (0.2173)  loss_scale: 65536.0000 (61138.3306)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1762  max mem: 15572
Epoch: [10]  [ 620/2809]  eta: 0:22:01  lr: 0.000044  min_lr: 0.000000  loss: 4.1908 (4.1137)  class_acc: 0.1667 (0.2167)  loss_scale: 65536.0000 (61209.1465)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0965  max mem: 15572
[2025-01-15 19:19:57,062] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:19:57,062] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:19:58,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28718
[2025-01-15 19:19:58,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:19:58,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 630/2809]  eta: 0:21:55  lr: 0.000044  min_lr: 0.000000  loss: 4.1767 (4.1134)  class_acc: 0.2083 (0.2168)  loss_scale: 65536.0000 (61693.1601)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1025  max mem: 15572
Epoch: [10]  [ 640/2809]  eta: 0:21:48  lr: 0.000044  min_lr: 0.000000  loss: 4.1625 (4.1136)  class_acc: 0.2500 (0.2172)  loss_scale: 65536.0000 (61753.1108)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1525  max mem: 15572
Epoch: [10]  [ 650/2809]  eta: 0:21:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1572 (4.1126)  class_acc: 0.2083 (0.2175)  loss_scale: 65536.0000 (61811.2197)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1180  max mem: 15572
Epoch: [10]  [ 660/2809]  eta: 0:21:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1552 (4.1110)  class_acc: 0.2083 (0.2179)  loss_scale: 65536.0000 (61867.5703)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1395  max mem: 15572
Epoch: [10]  [ 670/2809]  eta: 0:21:25  lr: 0.000044  min_lr: 0.000000  loss: 4.0024 (4.1090)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (61922.2414)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1382  max mem: 15572
Epoch: [10]  [ 680/2809]  eta: 0:21:20  lr: 0.000044  min_lr: 0.000000  loss: 4.0024 (4.1059)  class_acc: 0.2917 (0.2197)  loss_scale: 65536.0000 (61975.3069)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1354  max mem: 15572
Epoch: [10]  [ 690/2809]  eta: 0:21:12  lr: 0.000044  min_lr: 0.000000  loss: 4.0219 (4.1042)  class_acc: 0.2917 (0.2204)  loss_scale: 65536.0000 (62026.8365)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1272  max mem: 15572
Epoch: [10]  [ 700/2809]  eta: 0:21:04  lr: 0.000044  min_lr: 0.000000  loss: 4.0546 (4.1051)  class_acc: 0.2500 (0.2203)  loss_scale: 65536.0000 (62076.8959)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0790  max mem: 15572
Epoch: [10]  [ 710/2809]  eta: 0:20:55  lr: 0.000044  min_lr: 0.000000  loss: 4.1133 (4.1059)  class_acc: 0.1667 (0.2198)  loss_scale: 65536.0000 (62125.5471)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0611  max mem: 15572
[2025-01-15 19:20:50,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28810
[2025-01-15 19:20:50,488] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:20:50,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 720/2809]  eta: 0:20:45  lr: 0.000044  min_lr: 0.000000  loss: 4.0087 (4.1056)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (62127.4008)  weight_decay: 0.0500 (0.0500)  time: 0.4923  data: 0.0361  max mem: 15572
Epoch: [10]  [ 730/2809]  eta: 0:20:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1206 (4.1067)  class_acc: 0.1667 (0.2195)  loss_scale: 32768.0000 (61725.7674)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0747  max mem: 15572
Epoch: [10]  [ 740/2809]  eta: 0:20:31  lr: 0.000044  min_lr: 0.000000  loss: 4.1454 (4.1056)  class_acc: 0.2083 (0.2199)  loss_scale: 32768.0000 (61334.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1135  max mem: 15572
Epoch: [10]  [ 750/2809]  eta: 0:20:26  lr: 0.000044  min_lr: 0.000000  loss: 4.1415 (4.1061)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (60954.5885)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1561  max mem: 15572
Epoch: [10]  [ 760/2809]  eta: 0:20:20  lr: 0.000044  min_lr: 0.000000  loss: 4.2302 (4.1067)  class_acc: 0.2083 (0.2200)  loss_scale: 32768.0000 (60584.1997)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1778  max mem: 15572
Epoch: [10]  [ 770/2809]  eta: 0:20:16  lr: 0.000044  min_lr: 0.000000  loss: 4.3061 (4.1083)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (60223.4189)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.1796  max mem: 15572
Epoch: [10]  [ 780/2809]  eta: 0:20:08  lr: 0.000044  min_lr: 0.000000  loss: 4.3187 (4.1113)  class_acc: 0.1667 (0.2187)  loss_scale: 32768.0000 (59871.8771)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1584  max mem: 15572
Epoch: [10]  [ 790/2809]  eta: 0:20:04  lr: 0.000044  min_lr: 0.000000  loss: 4.1465 (4.1095)  class_acc: 0.2083 (0.2194)  loss_scale: 32768.0000 (59529.2238)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1606  max mem: 15572
Epoch: [10]  [ 800/2809]  eta: 0:19:58  lr: 0.000044  min_lr: 0.000000  loss: 4.0376 (4.1096)  class_acc: 0.2083 (0.2193)  loss_scale: 32768.0000 (59195.1261)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1962  max mem: 15572
Epoch: [10]  [ 810/2809]  eta: 0:19:53  lr: 0.000044  min_lr: 0.000000  loss: 4.0376 (4.1087)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (58869.2676)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1702  max mem: 15572
Epoch: [10]  [ 820/2809]  eta: 0:19:47  lr: 0.000044  min_lr: 0.000000  loss: 3.9020 (4.1071)  class_acc: 0.2500 (0.2199)  loss_scale: 32768.0000 (58551.3471)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1592  max mem: 15572
Epoch: [10]  [ 830/2809]  eta: 0:19:41  lr: 0.000044  min_lr: 0.000000  loss: 3.9784 (4.1071)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (58241.0782)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1487  max mem: 15572
Epoch: [10]  [ 840/2809]  eta: 0:19:35  lr: 0.000044  min_lr: 0.000000  loss: 4.1376 (4.1073)  class_acc: 0.1667 (0.2196)  loss_scale: 32768.0000 (57938.1879)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1418  max mem: 15572
[2025-01-15 19:22:07,763] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:22:07,764] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 850/2809]  eta: 0:19:28  lr: 0.000044  min_lr: 0.000000  loss: 4.1897 (4.1083)  class_acc: 0.1667 (0.2190)  loss_scale: 32768.0000 (57719.4266)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1327  max mem: 15572
Epoch: [10]  [ 860/2809]  eta: 0:19:23  lr: 0.000044  min_lr: 0.000000  loss: 4.0446 (4.1060)  class_acc: 0.1667 (0.2191)  loss_scale: 65536.0000 (57810.2114)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1455  max mem: 15572
Epoch: [10]  [ 870/2809]  eta: 0:19:16  lr: 0.000044  min_lr: 0.000000  loss: 3.9958 (4.1063)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (57898.9116)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1774  max mem: 15572
Epoch: [10]  [ 880/2809]  eta: 0:19:11  lr: 0.000044  min_lr: 0.000000  loss: 4.1633 (4.1069)  class_acc: 0.2083 (0.2192)  loss_scale: 65536.0000 (57985.5982)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1790  max mem: 15572
Epoch: [10]  [ 890/2809]  eta: 0:19:06  lr: 0.000044  min_lr: 0.000000  loss: 4.2547 (4.1094)  class_acc: 0.2083 (0.2191)  loss_scale: 65536.0000 (58070.3389)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1847  max mem: 15572
Epoch: [10]  [ 900/2809]  eta: 0:19:00  lr: 0.000044  min_lr: 0.000000  loss: 4.2126 (4.1090)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (58153.1987)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1601  max mem: 15572
[2025-01-15 19:22:44,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=189, lr=[4.287565332087948e-07, 4.287565332087948e-07, 6.125093331554213e-07, 6.125093331554213e-07, 8.750133330791733e-07, 8.750133330791733e-07, 1.250019047255962e-06, 1.250019047255962e-06, 1.7857414960799456e-06, 1.7857414960799456e-06, 2.551059280114208e-06, 2.551059280114208e-06, 3.6443704001631546e-06, 3.6443704001631546e-06, 5.2062434288045075e-06, 5.2062434288045075e-06, 7.437490612577868e-06, 7.437490612577868e-06, 1.0624986589396956e-05, 1.0624986589396956e-05, 1.5178552270567078e-05, 1.5178552270567078e-05, 2.1683646100810113e-05, 2.1683646100810113e-05, 3.097663728687159e-05, 3.097663728687159e-05, 4.4252338981245135e-05, 4.4252338981245135e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:22:44,296] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=28.418912029482996, CurrSamplesPerSec=28.23258897770299, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [ 910/2809]  eta: 0:18:54  lr: 0.000044  min_lr: 0.000000  loss: 4.1015 (4.1103)  class_acc: 0.1667 (0.2188)  loss_scale: 65536.0000 (58234.2393)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1406  max mem: 15572
Epoch: [10]  [ 920/2809]  eta: 0:18:48  lr: 0.000044  min_lr: 0.000000  loss: 4.1777 (4.1106)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (58313.5201)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1508  max mem: 15572
Epoch: [10]  [ 930/2809]  eta: 0:18:42  lr: 0.000044  min_lr: 0.000000  loss: 4.0998 (4.1101)  class_acc: 0.2083 (0.2186)  loss_scale: 65536.0000 (58391.0977)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1426  max mem: 15572
Epoch: [10]  [ 940/2809]  eta: 0:18:36  lr: 0.000044  min_lr: 0.000000  loss: 4.0574 (4.1102)  class_acc: 0.2083 (0.2184)  loss_scale: 65536.0000 (58467.0266)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1561  max mem: 15572
Epoch: [10]  [ 950/2809]  eta: 0:18:31  lr: 0.000044  min_lr: 0.000000  loss: 4.1634 (4.1104)  class_acc: 0.1667 (0.2184)  loss_scale: 65536.0000 (58541.3586)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1661  max mem: 15572
Epoch: [10]  [ 960/2809]  eta: 0:18:25  lr: 0.000044  min_lr: 0.000000  loss: 4.0743 (4.1095)  class_acc: 0.1667 (0.2185)  loss_scale: 65536.0000 (58614.1436)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1433  max mem: 15572
Epoch: [10]  [ 970/2809]  eta: 0:18:18  lr: 0.000044  min_lr: 0.000000  loss: 4.0645 (4.1086)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (58685.4295)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1371  max mem: 15572
[2025-01-15 19:23:24,218] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:23:24,218] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 980/2809]  eta: 0:18:13  lr: 0.000044  min_lr: 0.000000  loss: 4.2673 (4.1098)  class_acc: 0.2083 (0.2186)  loss_scale: 65536.0000 (59022.4832)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1439  max mem: 15572
[2025-01-15 19:23:28,876] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29074
[2025-01-15 19:23:28,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:23:28,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 990/2809]  eta: 0:18:06  lr: 0.000044  min_lr: 0.000000  loss: 4.2771 (4.1109)  class_acc: 0.1667 (0.2183)  loss_scale: 65536.0000 (59286.6034)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1110  max mem: 15572
Epoch: [10]  [1000/2809]  eta: 0:17:59  lr: 0.000044  min_lr: 0.000000  loss: 4.3160 (4.1126)  class_acc: 0.1667 (0.2179)  loss_scale: 65536.0000 (59349.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0838  max mem: 15572
Epoch: [10]  [1010/2809]  eta: 0:17:54  lr: 0.000044  min_lr: 0.000000  loss: 4.2805 (4.1124)  class_acc: 0.1667 (0.2179)  loss_scale: 65536.0000 (59410.2315)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1536  max mem: 15572
Epoch: [10]  [1020/2809]  eta: 0:17:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1699 (4.1130)  class_acc: 0.1667 (0.2177)  loss_scale: 65536.0000 (59470.2292)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1461  max mem: 15572
Epoch: [10]  [1030/2809]  eta: 0:17:42  lr: 0.000044  min_lr: 0.000000  loss: 4.0630 (4.1119)  class_acc: 0.2500 (0.2181)  loss_scale: 65536.0000 (59529.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1666  max mem: 15572
Epoch: [10]  [1040/2809]  eta: 0:17:36  lr: 0.000044  min_lr: 0.000000  loss: 3.9232 (4.1095)  class_acc: 0.2917 (0.2185)  loss_scale: 65536.0000 (59586.7666)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1707  max mem: 15572
Epoch: [10]  [1050/2809]  eta: 0:17:29  lr: 0.000044  min_lr: 0.000000  loss: 3.9875 (4.1103)  class_acc: 0.2500 (0.2186)  loss_scale: 65536.0000 (59643.3720)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.0910  max mem: 15572
Epoch: [10]  [1060/2809]  eta: 0:17:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1414 (4.1118)  class_acc: 0.2083 (0.2184)  loss_scale: 65536.0000 (59698.9105)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0575  max mem: 15572
Epoch: [10]  [1070/2809]  eta: 0:17:15  lr: 0.000044  min_lr: 0.000000  loss: 4.1113 (4.1119)  class_acc: 0.2500 (0.2187)  loss_scale: 65536.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0462  max mem: 15572
Epoch: [10]  [1080/2809]  eta: 0:17:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0490 (4.1117)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (59806.9047)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1036  max mem: 15572
Epoch: [10]  [1090/2809]  eta: 0:17:02  lr: 0.000044  min_lr: 0.000000  loss: 4.1281 (4.1110)  class_acc: 0.2500 (0.2191)  loss_scale: 65536.0000 (59859.4170)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.0994  max mem: 15572
Epoch: [10]  [1100/2809]  eta: 0:16:57  lr: 0.000044  min_lr: 0.000000  loss: 4.1763 (4.1114)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (59910.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1170  max mem: 15572
Epoch: [10]  [1110/2809]  eta: 0:16:50  lr: 0.000044  min_lr: 0.000000  loss: 4.0864 (4.1111)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (59961.6058)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1302  max mem: 15572
[2025-01-15 19:24:42,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:24:42,466] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1120/2809]  eta: 0:16:43  lr: 0.000044  min_lr: 0.000000  loss: 4.0745 (4.1100)  class_acc: 0.2083 (0.2191)  loss_scale: 65536.0000 (60479.0294)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0764  max mem: 15572
Epoch: [10]  [1130/2809]  eta: 0:16:37  lr: 0.000044  min_lr: 0.000000  loss: 4.1386 (4.1103)  class_acc: 0.1667 (0.2189)  loss_scale: 131072.0000 (61103.1936)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1163  max mem: 15572
[2025-01-15 19:24:52,652] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29221
[2025-01-15 19:24:52,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:24:52,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1140/2809]  eta: 0:16:31  lr: 0.000044  min_lr: 0.000000  loss: 4.1022 (4.1098)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (61142.0438)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1628  max mem: 15572
Epoch: [10]  [1150/2809]  eta: 0:16:25  lr: 0.000044  min_lr: 0.000000  loss: 4.0577 (4.1093)  class_acc: 0.2500 (0.2196)  loss_scale: 65536.0000 (61180.2189)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1397  max mem: 15572
Epoch: [10]  [1160/2809]  eta: 0:16:19  lr: 0.000044  min_lr: 0.000000  loss: 4.2272 (4.1098)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (61217.7364)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1335  max mem: 15572
Epoch: [10]  [1170/2809]  eta: 0:16:11  lr: 0.000044  min_lr: 0.000000  loss: 4.2845 (4.1112)  class_acc: 0.1667 (0.2191)  loss_scale: 65536.0000 (61254.6132)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0788  max mem: 15572
Epoch: [10]  [1180/2809]  eta: 0:16:06  lr: 0.000044  min_lr: 0.000000  loss: 4.0766 (4.1105)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (61290.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1142  max mem: 15572
Epoch: [10]  [1190/2809]  eta: 0:16:00  lr: 0.000044  min_lr: 0.000000  loss: 4.0766 (4.1111)  class_acc: 0.2083 (0.2192)  loss_scale: 65536.0000 (61326.5088)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1591  max mem: 15572
Epoch: [10]  [1200/2809]  eta: 0:15:53  lr: 0.000044  min_lr: 0.000000  loss: 4.2105 (4.1128)  class_acc: 0.1667 (0.2188)  loss_scale: 65536.0000 (61361.5587)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1095  max mem: 15572
Epoch: [10]  [1210/2809]  eta: 0:15:46  lr: 0.000044  min_lr: 0.000000  loss: 4.2708 (4.1142)  class_acc: 0.1250 (0.2186)  loss_scale: 65536.0000 (61396.0297)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1172  max mem: 15572
Epoch: [10]  [1220/2809]  eta: 0:15:41  lr: 0.000044  min_lr: 0.000000  loss: 4.2708 (4.1151)  class_acc: 0.2083 (0.2183)  loss_scale: 65536.0000 (61429.9361)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1401  max mem: 15572
Epoch: [10]  [1230/2809]  eta: 0:15:36  lr: 0.000044  min_lr: 0.000000  loss: 4.2165 (4.1161)  class_acc: 0.2083 (0.2180)  loss_scale: 65536.0000 (61463.2916)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.1899  max mem: 15572
Epoch: [10]  [1240/2809]  eta: 0:15:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1356 (4.1164)  class_acc: 0.2083 (0.2182)  loss_scale: 65536.0000 (61496.1096)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1525  max mem: 15572
Epoch: [10]  [1250/2809]  eta: 0:15:24  lr: 0.000044  min_lr: 0.000000  loss: 4.3815 (4.1185)  class_acc: 0.1667 (0.2175)  loss_scale: 65536.0000 (61528.4029)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1319  max mem: 15572
[2025-01-15 19:26:08,179] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:26:08,179] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1260/2809]  eta: 0:15:18  lr: 0.000044  min_lr: 0.000000  loss: 4.2030 (4.1179)  class_acc: 0.1667 (0.2179)  loss_scale: 65536.0000 (61612.1554)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1418  max mem: 15572
[2025-01-15 19:26:09,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29353
[2025-01-15 19:26:09,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:26:09,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1270/2809]  eta: 0:15:12  lr: 0.000044  min_lr: 0.000000  loss: 3.9814 (4.1168)  class_acc: 0.2500 (0.2181)  loss_scale: 65536.0000 (61746.1526)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1375  max mem: 15572
Epoch: [10]  [1280/2809]  eta: 0:15:06  lr: 0.000044  min_lr: 0.000000  loss: 3.9126 (4.1165)  class_acc: 0.2500 (0.2183)  loss_scale: 65536.0000 (61775.7377)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1461  max mem: 15572
Epoch: [10]  [1290/2809]  eta: 0:14:59  lr: 0.000044  min_lr: 0.000000  loss: 4.1671 (4.1166)  class_acc: 0.2500 (0.2185)  loss_scale: 65536.0000 (61804.8644)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1242  max mem: 15572
Epoch: [10]  [1300/2809]  eta: 0:14:53  lr: 0.000044  min_lr: 0.000000  loss: 3.9434 (4.1155)  class_acc: 0.2917 (0.2189)  loss_scale: 65536.0000 (61833.5434)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1362  max mem: 15572
Epoch: [10]  [1310/2809]  eta: 0:14:48  lr: 0.000044  min_lr: 0.000000  loss: 3.9196 (4.1154)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (61861.7849)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1436  max mem: 15572
Epoch: [10]  [1320/2809]  eta: 0:14:42  lr: 0.000044  min_lr: 0.000000  loss: 4.1833 (4.1160)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (61889.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1344  max mem: 15572
Epoch: [10]  [1330/2809]  eta: 0:14:37  lr: 0.000044  min_lr: 0.000000  loss: 4.1622 (4.1155)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (61916.9947)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1830  max mem: 15572
Epoch: [10]  [1340/2809]  eta: 0:14:30  lr: 0.000044  min_lr: 0.000000  loss: 4.0454 (4.1157)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (61943.9821)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1696  max mem: 15572
Epoch: [10]  [1350/2809]  eta: 0:14:24  lr: 0.000044  min_lr: 0.000000  loss: 4.0936 (4.1157)  class_acc: 0.1667 (0.2185)  loss_scale: 65536.0000 (61970.5699)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0786  max mem: 15572
Epoch: [10]  [1360/2809]  eta: 0:14:18  lr: 0.000044  min_lr: 0.000000  loss: 4.0940 (4.1144)  class_acc: 0.2083 (0.2187)  loss_scale: 65536.0000 (61996.7671)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1111  max mem: 15572
Epoch: [10]  [1370/2809]  eta: 0:14:12  lr: 0.000044  min_lr: 0.000000  loss: 4.1010 (4.1147)  class_acc: 0.2083 (0.2186)  loss_scale: 65536.0000 (62022.5821)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1205  max mem: 15572
Epoch: [10]  [1380/2809]  eta: 0:14:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1010 (4.1143)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (62048.0232)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1147  max mem: 15572
Epoch: [10]  [1390/2809]  eta: 0:14:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1251 (4.1147)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (62073.0985)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1665  max mem: 15572
[2025-01-15 19:27:25,942] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:27:25,942] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:27:30,857] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29489
[2025-01-15 19:27:30,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:27:30,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1400/2809]  eta: 0:13:55  lr: 0.000044  min_lr: 0.000000  loss: 4.2158 (4.1149)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (62425.2620)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1710  max mem: 15572
Epoch: [10]  [1410/2809]  eta: 0:13:48  lr: 0.000044  min_lr: 0.000000  loss: 4.2659 (4.1166)  class_acc: 0.1667 (0.2184)  loss_scale: 65536.0000 (62447.3083)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1239  max mem: 15572
Epoch: [10]  [1420/2809]  eta: 0:13:42  lr: 0.000044  min_lr: 0.000000  loss: 4.3010 (4.1172)  class_acc: 0.1667 (0.2182)  loss_scale: 65536.0000 (62469.0443)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1387  max mem: 15572
Epoch: [10]  [1430/2809]  eta: 0:13:37  lr: 0.000044  min_lr: 0.000000  loss: 4.0984 (4.1160)  class_acc: 0.2083 (0.2187)  loss_scale: 65536.0000 (62490.4766)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1856  max mem: 15572
Epoch: [10]  [1440/2809]  eta: 0:13:31  lr: 0.000044  min_lr: 0.000000  loss: 3.9746 (4.1157)  class_acc: 0.2500 (0.2187)  loss_scale: 65536.0000 (62511.6114)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1798  max mem: 15572
Epoch: [10]  [1450/2809]  eta: 0:13:25  lr: 0.000044  min_lr: 0.000000  loss: 4.1972 (4.1160)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (62532.4549)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1776  max mem: 15572
Epoch: [10]  [1460/2809]  eta: 0:13:19  lr: 0.000044  min_lr: 0.000000  loss: 4.2102 (4.1161)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (62553.0130)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1520  max mem: 15572
Epoch: [10]  [1470/2809]  eta: 0:13:14  lr: 0.000044  min_lr: 0.000000  loss: 4.2102 (4.1166)  class_acc: 0.1667 (0.2183)  loss_scale: 65536.0000 (62573.2916)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1667  max mem: 15572
Epoch: [10]  [1480/2809]  eta: 0:13:07  lr: 0.000044  min_lr: 0.000000  loss: 4.1789 (4.1172)  class_acc: 0.1667 (0.2182)  loss_scale: 65536.0000 (62593.2964)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1220  max mem: 15572
Epoch: [10]  [1490/2809]  eta: 0:13:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1107 (4.1179)  class_acc: 0.2500 (0.2185)  loss_scale: 65536.0000 (62613.0329)  weight_decay: 0.0500 (0.0500)  time: 0.4846  data: 0.0402  max mem: 15572
Epoch: [10]  [1500/2809]  eta: 0:12:55  lr: 0.000044  min_lr: 0.000000  loss: 4.1107 (4.1176)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (62632.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1002  max mem: 15572
Epoch: [10]  [1510/2809]  eta: 0:12:48  lr: 0.000044  min_lr: 0.000000  loss: 4.0754 (4.1179)  class_acc: 0.2083 (0.2186)  loss_scale: 65536.0000 (62651.7220)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1295  max mem: 15572
Epoch: [10]  [1520/2809]  eta: 0:12:43  lr: 0.000044  min_lr: 0.000000  loss: 4.2543 (4.1195)  class_acc: 0.1667 (0.2181)  loss_scale: 65536.0000 (62670.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1328  max mem: 15572
[2025-01-15 19:28:45,036] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:28:45,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1530/2809]  eta: 0:12:37  lr: 0.000044  min_lr: 0.000000  loss: 4.1310 (4.1177)  class_acc: 0.2083 (0.2185)  loss_scale: 65536.0000 (62817.8184)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1539  max mem: 15572
[2025-01-15 19:28:50,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29628
[2025-01-15 19:28:50,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:28:50,914] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1540/2809]  eta: 0:12:30  lr: 0.000044  min_lr: 0.000000  loss: 4.0381 (4.1168)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (63133.1551)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1083  max mem: 15572
Epoch: [10]  [1550/2809]  eta: 0:12:24  lr: 0.000044  min_lr: 0.000000  loss: 4.2739 (4.1184)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (63148.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.1028  max mem: 15572
Epoch: [10]  [1560/2809]  eta: 0:12:18  lr: 0.000044  min_lr: 0.000000  loss: 4.2937 (4.1186)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (63163.9411)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1120  max mem: 15572
Epoch: [10]  [1570/2809]  eta: 0:12:13  lr: 0.000044  min_lr: 0.000000  loss: 3.9664 (4.1179)  class_acc: 0.1667 (0.2189)  loss_scale: 65536.0000 (63179.0401)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1777  max mem: 15572
Epoch: [10]  [1580/2809]  eta: 0:12:06  lr: 0.000044  min_lr: 0.000000  loss: 3.9954 (4.1190)  class_acc: 0.1667 (0.2183)  loss_scale: 65536.0000 (63193.9481)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1796  max mem: 15572
Epoch: [10]  [1590/2809]  eta: 0:12:01  lr: 0.000044  min_lr: 0.000000  loss: 4.1583 (4.1199)  class_acc: 0.1250 (0.2180)  loss_scale: 65536.0000 (63208.6688)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1572  max mem: 15572
Epoch: [10]  [1600/2809]  eta: 0:11:55  lr: 0.000044  min_lr: 0.000000  loss: 4.2397 (4.1209)  class_acc: 0.1667 (0.2176)  loss_scale: 65536.0000 (63223.2055)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1477  max mem: 15572
Epoch: [10]  [1610/2809]  eta: 0:11:49  lr: 0.000044  min_lr: 0.000000  loss: 4.2397 (4.1203)  class_acc: 0.1667 (0.2176)  loss_scale: 65536.0000 (63237.5618)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1442  max mem: 15572
Epoch: [10]  [1620/2809]  eta: 0:11:44  lr: 0.000044  min_lr: 0.000000  loss: 4.2024 (4.1211)  class_acc: 0.2083 (0.2176)  loss_scale: 65536.0000 (63251.7409)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.2031  max mem: 15572
Epoch: [10]  [1630/2809]  eta: 0:11:38  lr: 0.000044  min_lr: 0.000000  loss: 4.2979 (4.1216)  class_acc: 0.2083 (0.2178)  loss_scale: 65536.0000 (63265.7462)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1721  max mem: 15572
Epoch: [10]  [1640/2809]  eta: 0:11:32  lr: 0.000044  min_lr: 0.000000  loss: 3.9631 (4.1207)  class_acc: 0.2083 (0.2178)  loss_scale: 65536.0000 (63279.5807)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1507  max mem: 15572
Epoch: [10]  [1650/2809]  eta: 0:11:26  lr: 0.000044  min_lr: 0.000000  loss: 3.9353 (4.1190)  class_acc: 0.2500 (0.2181)  loss_scale: 65536.0000 (63293.2477)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1801  max mem: 15572
Epoch: [10]  [1660/2809]  eta: 0:11:20  lr: 0.000044  min_lr: 0.000000  loss: 3.9543 (4.1187)  class_acc: 0.2500 (0.2182)  loss_scale: 65536.0000 (63306.7502)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1693  max mem: 15572
[2025-01-15 19:30:08,104] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:30:08,104] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:30:09,360] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29759
[2025-01-15 19:30:09,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:30:09,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1670/2809]  eta: 0:11:13  lr: 0.000044  min_lr: 0.000000  loss: 4.0516 (4.1186)  class_acc: 0.2500 (0.2184)  loss_scale: 65536.0000 (63398.5302)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0879  max mem: 15572
Epoch: [10]  [1680/2809]  eta: 0:11:08  lr: 0.000044  min_lr: 0.000000  loss: 4.0516 (4.1181)  class_acc: 0.2500 (0.2186)  loss_scale: 65536.0000 (63411.2457)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1184  max mem: 15572
Epoch: [10]  [1690/2809]  eta: 0:11:02  lr: 0.000044  min_lr: 0.000000  loss: 3.9847 (4.1169)  class_acc: 0.2500 (0.2188)  loss_scale: 65536.0000 (63423.8108)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1863  max mem: 15572
Epoch: [10]  [1700/2809]  eta: 0:10:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0023 (4.1168)  class_acc: 0.2083 (0.2188)  loss_scale: 65536.0000 (63436.2281)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1405  max mem: 15572
Epoch: [10]  [1710/2809]  eta: 0:10:49  lr: 0.000044  min_lr: 0.000000  loss: 3.9340 (4.1159)  class_acc: 0.2083 (0.2190)  loss_scale: 65536.0000 (63448.5003)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0809  max mem: 15572
Epoch: [10]  [1720/2809]  eta: 0:10:44  lr: 0.000044  min_lr: 0.000000  loss: 3.9619 (4.1156)  class_acc: 0.2500 (0.2191)  loss_scale: 65536.0000 (63460.6299)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1241  max mem: 15572
Epoch: [10]  [1730/2809]  eta: 0:10:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1824 (4.1158)  class_acc: 0.1667 (0.2189)  loss_scale: 65536.0000 (63472.6193)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1582  max mem: 15572
Epoch: [10]  [1740/2809]  eta: 0:10:32  lr: 0.000044  min_lr: 0.000000  loss: 4.1865 (4.1152)  class_acc: 0.1667 (0.2188)  loss_scale: 65536.0000 (63484.4710)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0900  max mem: 15572
Epoch: [10]  [1750/2809]  eta: 0:10:26  lr: 0.000044  min_lr: 0.000000  loss: 4.2033 (4.1163)  class_acc: 0.1667 (0.2186)  loss_scale: 65536.0000 (63496.1873)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1229  max mem: 15572
Epoch: [10]  [1760/2809]  eta: 0:10:20  lr: 0.000044  min_lr: 0.000000  loss: 4.1729 (4.1160)  class_acc: 0.2083 (0.2187)  loss_scale: 65536.0000 (63507.7706)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1461  max mem: 15572
Epoch: [10]  [1770/2809]  eta: 0:10:14  lr: 0.000044  min_lr: 0.000000  loss: 4.0979 (4.1161)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (63519.2230)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1273  max mem: 15572
Epoch: [10]  [1780/2809]  eta: 0:10:08  lr: 0.000044  min_lr: 0.000000  loss: 4.1037 (4.1164)  class_acc: 0.2083 (0.2189)  loss_scale: 65536.0000 (63530.5469)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1667  max mem: 15572
Epoch: [10]  [1790/2809]  eta: 0:10:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0538 (4.1148)  class_acc: 0.2083 (0.2195)  loss_scale: 65536.0000 (63541.7443)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1763  max mem: 15572
[2025-01-15 19:31:24,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:31:24,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:31:26,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29889
[2025-01-15 19:31:26,244] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:31:26,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1800/2809]  eta: 0:09:56  lr: 0.000044  min_lr: 0.000000  loss: 3.8007 (4.1147)  class_acc: 0.2500 (0.2197)  loss_scale: 65536.0000 (63589.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1754  max mem: 15572
Epoch: [10]  [1810/2809]  eta: 0:09:51  lr: 0.000044  min_lr: 0.000000  loss: 4.2170 (4.1143)  class_acc: 0.2500 (0.2198)  loss_scale: 65536.0000 (63599.9558)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1749  max mem: 15572
Epoch: [10]  [1820/2809]  eta: 0:09:45  lr: 0.000044  min_lr: 0.000000  loss: 4.2065 (4.1142)  class_acc: 0.2083 (0.2200)  loss_scale: 65536.0000 (63610.5876)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1580  max mem: 15572
Epoch: [10]  [1830/2809]  eta: 0:09:39  lr: 0.000044  min_lr: 0.000000  loss: 4.1494 (4.1141)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (63621.1032)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1434  max mem: 15572
Epoch: [10]  [1840/2809]  eta: 0:09:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1658 (4.1147)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (63631.5046)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1385  max mem: 15572
Epoch: [10]  [1850/2809]  eta: 0:09:27  lr: 0.000044  min_lr: 0.000000  loss: 4.1268 (4.1145)  class_acc: 0.2083 (0.2198)  loss_scale: 65536.0000 (63641.7936)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.1590  max mem: 15572
Epoch: [10]  [1860/2809]  eta: 0:09:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1777 (4.1151)  class_acc: 0.1667 (0.2195)  loss_scale: 65536.0000 (63651.9721)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.1849  max mem: 15572
Epoch: [10]  [1870/2809]  eta: 0:09:15  lr: 0.000044  min_lr: 0.000000  loss: 4.2190 (4.1158)  class_acc: 0.1667 (0.2193)  loss_scale: 65536.0000 (63662.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1250  max mem: 15572
Epoch: [10]  [1880/2809]  eta: 0:09:10  lr: 0.000044  min_lr: 0.000000  loss: 4.1831 (4.1162)  class_acc: 0.1667 (0.2191)  loss_scale: 65536.0000 (63672.0043)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1300  max mem: 15572
Epoch: [10]  [1890/2809]  eta: 0:09:04  lr: 0.000044  min_lr: 0.000000  loss: 4.0553 (4.1161)  class_acc: 0.2083 (0.2192)  loss_scale: 65536.0000 (63681.8614)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1710  max mem: 15572
Epoch: [10]  [1900/2809]  eta: 0:08:58  lr: 0.000044  min_lr: 0.000000  loss: 4.2415 (4.1164)  class_acc: 0.2083 (0.2193)  loss_scale: 65536.0000 (63691.6149)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1200  max mem: 15572
[2025-01-15 19:32:31,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=196, lr=[4.2532066161678034e-07, 4.2532066161678034e-07, 6.076009451668291e-07, 6.076009451668291e-07, 8.680013502383275e-07, 8.680013502383275e-07, 1.2400019289118964e-06, 1.2400019289118964e-06, 1.771431327016995e-06, 1.771431327016995e-06, 2.53061618145285e-06, 2.53061618145285e-06, 3.6151659735040717e-06, 3.6151659735040717e-06, 5.164522819291532e-06, 5.164522819291532e-06, 7.377889741845045e-06, 7.377889741845045e-06, 1.0539842488350066e-05, 1.0539842488350066e-05, 1.5056917840500093e-05, 1.5056917840500093e-05, 2.150988262928585e-05, 2.150988262928585e-05, 3.0728403756122646e-05, 3.0728403756122646e-05, 4.389771965160378e-05, 4.389771965160378e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:32:31,763] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=28.41460682584276, CurrSamplesPerSec=31.18617468518981, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [10]  [1910/2809]  eta: 0:08:52  lr: 0.000044  min_lr: 0.000000  loss: 4.2246 (4.1164)  class_acc: 0.2083 (0.2195)  loss_scale: 65536.0000 (63701.2664)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1142  max mem: 15572
Epoch: [10]  [1920/2809]  eta: 0:08:45  lr: 0.000044  min_lr: 0.000000  loss: 4.1158 (4.1164)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (63710.8173)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0951  max mem: 15572
[2025-01-15 19:32:40,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:32:40,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1930/2809]  eta: 0:08:40  lr: 0.000044  min_lr: 0.000000  loss: 4.1158 (4.1161)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (63822.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1419  max mem: 15572
[2025-01-15 19:32:44,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30023
[2025-01-15 19:32:44,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:32:44,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1940/2809]  eta: 0:08:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1217 (4.1160)  class_acc: 0.1667 (0.2195)  loss_scale: 65536.0000 (63898.4441)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1743  max mem: 15572
Epoch: [10]  [1950/2809]  eta: 0:08:27  lr: 0.000044  min_lr: 0.000000  loss: 4.0051 (4.1150)  class_acc: 0.2083 (0.2198)  loss_scale: 65536.0000 (63906.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1189  max mem: 15572
Epoch: [10]  [1960/2809]  eta: 0:08:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1643 (4.1157)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (63915.1453)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1673  max mem: 15572
Epoch: [10]  [1970/2809]  eta: 0:08:16  lr: 0.000044  min_lr: 0.000000  loss: 4.2522 (4.1163)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (63923.3688)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1720  max mem: 15572
Epoch: [10]  [1980/2809]  eta: 0:08:09  lr: 0.000044  min_lr: 0.000000  loss: 4.2493 (4.1163)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (63931.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0930  max mem: 15572
Epoch: [10]  [1990/2809]  eta: 0:08:04  lr: 0.000044  min_lr: 0.000000  loss: 4.0189 (4.1157)  class_acc: 0.2500 (0.2199)  loss_scale: 65536.0000 (63939.5681)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1310  max mem: 15572
Epoch: [10]  [2000/2809]  eta: 0:07:58  lr: 0.000044  min_lr: 0.000000  loss: 4.0314 (4.1154)  class_acc: 0.2500 (0.2201)  loss_scale: 65536.0000 (63947.5462)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.1921  max mem: 15572
Epoch: [10]  [2010/2809]  eta: 0:07:52  lr: 0.000044  min_lr: 0.000000  loss: 4.0639 (4.1155)  class_acc: 0.2083 (0.2200)  loss_scale: 65536.0000 (63955.4451)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.0900  max mem: 15572
[2025-01-15 19:33:36,095] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30110
[2025-01-15 19:33:36,096] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:33:36,096] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2020/2809]  eta: 0:07:46  lr: 0.000044  min_lr: 0.000000  loss: 4.1161 (4.1160)  class_acc: 0.1667 (0.2199)  loss_scale: 65536.0000 (63947.0520)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0824  max mem: 15572
Epoch: [10]  [2030/2809]  eta: 0:07:40  lr: 0.000044  min_lr: 0.000000  loss: 4.2364 (4.1159)  class_acc: 0.2500 (0.2199)  loss_scale: 32768.0000 (63793.5362)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1625  max mem: 15572
Epoch: [10]  [2040/2809]  eta: 0:07:34  lr: 0.000044  min_lr: 0.000000  loss: 4.2435 (4.1168)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (63641.5247)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1643  max mem: 15572
Epoch: [10]  [2050/2809]  eta: 0:07:28  lr: 0.000044  min_lr: 0.000000  loss: 4.3640 (4.1178)  class_acc: 0.1667 (0.2196)  loss_scale: 32768.0000 (63490.9956)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1543  max mem: 15572
Epoch: [10]  [2060/2809]  eta: 0:07:22  lr: 0.000044  min_lr: 0.000000  loss: 4.1548 (4.1176)  class_acc: 0.1667 (0.2196)  loss_scale: 32768.0000 (63341.9272)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1283  max mem: 15572
Epoch: [10]  [2070/2809]  eta: 0:07:16  lr: 0.000044  min_lr: 0.000000  loss: 4.1347 (4.1182)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (63194.2984)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0916  max mem: 15572
Epoch: [10]  [2080/2809]  eta: 0:07:11  lr: 0.000044  min_lr: 0.000000  loss: 4.2318 (4.1183)  class_acc: 0.2083 (0.2198)  loss_scale: 32768.0000 (63048.0884)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.1593  max mem: 15572
Epoch: [10]  [2090/2809]  eta: 0:07:05  lr: 0.000044  min_lr: 0.000000  loss: 4.1728 (4.1184)  class_acc: 0.2500 (0.2198)  loss_scale: 32768.0000 (62903.2769)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.2013  max mem: 15572
Epoch: [10]  [2100/2809]  eta: 0:06:59  lr: 0.000044  min_lr: 0.000000  loss: 4.1728 (4.1191)  class_acc: 0.2083 (0.2198)  loss_scale: 32768.0000 (62759.8439)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1022  max mem: 15572
Epoch: [10]  [2110/2809]  eta: 0:06:53  lr: 0.000044  min_lr: 0.000000  loss: 4.1699 (4.1197)  class_acc: 0.2083 (0.2198)  loss_scale: 32768.0000 (62617.7698)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1220  max mem: 15572
Epoch: [10]  [2120/2809]  eta: 0:06:47  lr: 0.000044  min_lr: 0.000000  loss: 4.1764 (4.1200)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (62477.0354)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1152  max mem: 15572
Epoch: [10]  [2130/2809]  eta: 0:06:41  lr: 0.000044  min_lr: 0.000000  loss: 4.1348 (4.1197)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (62337.6218)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0705  max mem: 15572
Epoch: [10]  [2140/2809]  eta: 0:06:34  lr: 0.000044  min_lr: 0.000000  loss: 3.8940 (4.1187)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (62199.5105)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0837  max mem: 15572
[2025-01-15 19:34:49,824] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:34:49,824] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2150/2809]  eta: 0:06:28  lr: 0.000044  min_lr: 0.000000  loss: 3.8675 (4.1189)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (62093.1511)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0876  max mem: 15572
Epoch: [10]  [2160/2809]  eta: 0:06:22  lr: 0.000044  min_lr: 0.000000  loss: 4.0814 (4.1190)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (62109.0828)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1540  max mem: 15572
Epoch: [10]  [2170/2809]  eta: 0:06:16  lr: 0.000044  min_lr: 0.000000  loss: 4.1135 (4.1198)  class_acc: 0.2083 (0.2195)  loss_scale: 65536.0000 (62124.8678)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1348  max mem: 15572
Epoch: [10]  [2180/2809]  eta: 0:06:10  lr: 0.000044  min_lr: 0.000000  loss: 4.1363 (4.1193)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (62140.5080)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1056  max mem: 15572
Epoch: [10]  [2190/2809]  eta: 0:06:05  lr: 0.000044  min_lr: 0.000000  loss: 4.0684 (4.1192)  class_acc: 0.2500 (0.2197)  loss_scale: 65536.0000 (62156.0055)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1793  max mem: 15572
Epoch: [10]  [2200/2809]  eta: 0:05:59  lr: 0.000044  min_lr: 0.000000  loss: 4.0330 (4.1186)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (62171.3621)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1756  max mem: 15572
Epoch: [10]  [2210/2809]  eta: 0:05:53  lr: 0.000044  min_lr: 0.000000  loss: 4.0330 (4.1185)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (62186.5798)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1258  max mem: 15572
Epoch: [10]  [2220/2809]  eta: 0:05:47  lr: 0.000044  min_lr: 0.000000  loss: 4.2326 (4.1188)  class_acc: 0.2083 (0.2197)  loss_scale: 65536.0000 (62201.6605)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1384  max mem: 15572
Epoch: [10]  [2230/2809]  eta: 0:05:41  lr: 0.000044  min_lr: 0.000000  loss: 4.1935 (4.1191)  class_acc: 0.1667 (0.2197)  loss_scale: 65536.0000 (62216.6060)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1175  max mem: 15572
Epoch: [10]  [2240/2809]  eta: 0:05:35  lr: 0.000044  min_lr: 0.000000  loss: 4.1832 (4.1193)  class_acc: 0.1667 (0.2195)  loss_scale: 65536.0000 (62231.4181)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1237  max mem: 15572
Epoch: [10]  [2250/2809]  eta: 0:05:29  lr: 0.000044  min_lr: 0.000000  loss: 4.1112 (4.1189)  class_acc: 0.2083 (0.2196)  loss_scale: 65536.0000 (62246.0986)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1729  max mem: 15572
[2025-01-15 19:35:50,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30343
[2025-01-15 19:35:50,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:35:50,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2260/2809]  eta: 0:05:23  lr: 0.000044  min_lr: 0.000000  loss: 4.1449 (4.1195)  class_acc: 0.2083 (0.2195)  loss_scale: 65536.0000 (62144.7077)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1640  max mem: 15572
Epoch: [10]  [2270/2809]  eta: 0:05:18  lr: 0.000044  min_lr: 0.000000  loss: 4.1143 (4.1192)  class_acc: 0.1667 (0.2195)  loss_scale: 32768.0000 (62015.3518)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1667  max mem: 15572
Epoch: [10]  [2280/2809]  eta: 0:05:12  lr: 0.000044  min_lr: 0.000000  loss: 4.0712 (4.1190)  class_acc: 0.2083 (0.2196)  loss_scale: 32768.0000 (61887.1302)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1629  max mem: 15572
Epoch: [10]  [2290/2809]  eta: 0:05:06  lr: 0.000044  min_lr: 0.000000  loss: 4.1067 (4.1189)  class_acc: 0.2500 (0.2197)  loss_scale: 32768.0000 (61760.0279)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1344  max mem: 15572
Epoch: [10]  [2300/2809]  eta: 0:05:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1401 (4.1185)  class_acc: 0.2500 (0.2197)  loss_scale: 32768.0000 (61634.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0983  max mem: 15572
Epoch: [10]  [2310/2809]  eta: 0:04:54  lr: 0.000044  min_lr: 0.000000  loss: 3.9741 (4.1186)  class_acc: 0.2083 (0.2197)  loss_scale: 32768.0000 (61509.1233)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.0947  max mem: 15572
Epoch: [10]  [2320/2809]  eta: 0:04:48  lr: 0.000044  min_lr: 0.000000  loss: 4.0411 (4.1189)  class_acc: 0.1667 (0.2197)  loss_scale: 32768.0000 (61385.2925)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0956  max mem: 15572
Epoch: [10]  [2330/2809]  eta: 0:04:42  lr: 0.000044  min_lr: 0.000000  loss: 4.0578 (4.1188)  class_acc: 0.2500 (0.2198)  loss_scale: 32768.0000 (61262.5242)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0963  max mem: 15572
Epoch: [10]  [2340/2809]  eta: 0:04:36  lr: 0.000044  min_lr: 0.000000  loss: 4.0216 (4.1191)  class_acc: 0.2500 (0.2200)  loss_scale: 32768.0000 (61140.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1163  max mem: 15572
Epoch: [10]  [2350/2809]  eta: 0:04:30  lr: 0.000044  min_lr: 0.000000  loss: 4.1685 (4.1193)  class_acc: 0.2500 (0.2200)  loss_scale: 32768.0000 (61020.1208)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.2050  max mem: 15572
Epoch: [10]  [2360/2809]  eta: 0:04:24  lr: 0.000044  min_lr: 0.000000  loss: 4.0408 (4.1188)  class_acc: 0.2500 (0.2201)  loss_scale: 32768.0000 (60900.4591)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.2073  max mem: 15572
Epoch: [10]  [2370/2809]  eta: 0:04:19  lr: 0.000044  min_lr: 0.000000  loss: 4.1326 (4.1188)  class_acc: 0.2083 (0.2202)  loss_scale: 32768.0000 (60781.8068)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1615  max mem: 15572
Epoch: [10]  [2380/2809]  eta: 0:04:13  lr: 0.000044  min_lr: 0.000000  loss: 4.0423 (4.1188)  class_acc: 0.2083 (0.2203)  loss_scale: 32768.0000 (60664.1512)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1383  max mem: 15572
[2025-01-15 19:37:07,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:37:07,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2390/2809]  eta: 0:04:07  lr: 0.000044  min_lr: 0.000000  loss: 4.0768 (4.1192)  class_acc: 0.2083 (0.2201)  loss_scale: 32768.0000 (60670.8223)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1064  max mem: 15572
Epoch: [10]  [2400/2809]  eta: 0:04:01  lr: 0.000044  min_lr: 0.000000  loss: 4.2135 (4.1195)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (60691.0854)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1621  max mem: 15572
Epoch: [10]  [2410/2809]  eta: 0:03:55  lr: 0.000044  min_lr: 0.000000  loss: 4.1829 (4.1194)  class_acc: 0.2083 (0.2203)  loss_scale: 65536.0000 (60711.1804)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1792  max mem: 15572
Epoch: [10]  [2420/2809]  eta: 0:03:49  lr: 0.000044  min_lr: 0.000000  loss: 4.1468 (4.1191)  class_acc: 0.2500 (0.2205)  loss_scale: 65536.0000 (60731.1095)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1999  max mem: 15572
Epoch: [10]  [2430/2809]  eta: 0:03:43  lr: 0.000044  min_lr: 0.000000  loss: 3.9031 (4.1178)  class_acc: 0.2500 (0.2208)  loss_scale: 65536.0000 (60750.8745)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1611  max mem: 15572
Epoch: [10]  [2440/2809]  eta: 0:03:37  lr: 0.000044  min_lr: 0.000000  loss: 3.7360 (4.1168)  class_acc: 0.2500 (0.2209)  loss_scale: 65536.0000 (60770.4777)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1497  max mem: 15572
Epoch: [10]  [2450/2809]  eta: 0:03:32  lr: 0.000044  min_lr: 0.000000  loss: 3.8891 (4.1165)  class_acc: 0.2083 (0.2210)  loss_scale: 65536.0000 (60789.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1527  max mem: 15572
Epoch: [10]  [2460/2809]  eta: 0:03:26  lr: 0.000044  min_lr: 0.000000  loss: 3.9331 (4.1161)  class_acc: 0.2500 (0.2212)  loss_scale: 65536.0000 (60809.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0859  max mem: 15572
Epoch: [10]  [2470/2809]  eta: 0:03:20  lr: 0.000044  min_lr: 0.000000  loss: 3.9335 (4.1159)  class_acc: 0.2500 (0.2212)  loss_scale: 65536.0000 (60828.3351)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1143  max mem: 15572
Epoch: [10]  [2480/2809]  eta: 0:03:14  lr: 0.000044  min_lr: 0.000000  loss: 4.0375 (4.1157)  class_acc: 0.2083 (0.2211)  loss_scale: 65536.0000 (60847.3100)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1288  max mem: 15572
[2025-01-15 19:38:07,801] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30574
[2025-01-15 19:38:07,802] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:38:07,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [2490/2809]  eta: 0:03:08  lr: 0.000044  min_lr: 0.000000  loss: 4.0375 (4.1157)  class_acc: 0.1667 (0.2211)  loss_scale: 65536.0000 (60774.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0968  max mem: 15572
Epoch: [10]  [2500/2809]  eta: 0:03:02  lr: 0.000044  min_lr: 0.000000  loss: 4.0945 (4.1159)  class_acc: 0.2500 (0.2211)  loss_scale: 32768.0000 (60662.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0896  max mem: 15572
Epoch: [10]  [2510/2809]  eta: 0:02:56  lr: 0.000044  min_lr: 0.000000  loss: 4.1479 (4.1165)  class_acc: 0.2083 (0.2210)  loss_scale: 32768.0000 (60550.9837)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1386  max mem: 15572
Epoch: [10]  [2520/2809]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000000  loss: 4.1688 (4.1165)  class_acc: 0.2083 (0.2210)  loss_scale: 32768.0000 (60440.7775)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1327  max mem: 15572
Epoch: [10]  [2530/2809]  eta: 0:02:44  lr: 0.000044  min_lr: 0.000000  loss: 4.2024 (4.1169)  class_acc: 0.2083 (0.2210)  loss_scale: 32768.0000 (60331.4421)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1550  max mem: 15572
Epoch: [10]  [2540/2809]  eta: 0:02:38  lr: 0.000044  min_lr: 0.000000  loss: 4.1991 (4.1168)  class_acc: 0.2083 (0.2211)  loss_scale: 32768.0000 (60222.9673)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1873  max mem: 15572
Epoch: [10]  [2550/2809]  eta: 0:02:32  lr: 0.000044  min_lr: 0.000000  loss: 4.0719 (4.1170)  class_acc: 0.2083 (0.2210)  loss_scale: 32768.0000 (60115.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0939  max mem: 15572
Epoch: [10]  [2560/2809]  eta: 0:02:26  lr: 0.000044  min_lr: 0.000000  loss: 4.0719 (4.1169)  class_acc: 0.2083 (0.2209)  loss_scale: 32768.0000 (60008.5592)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0914  max mem: 15572
Epoch: [10]  [2570/2809]  eta: 0:02:20  lr: 0.000044  min_lr: 0.000000  loss: 4.0622 (4.1170)  class_acc: 0.2500 (0.2211)  loss_scale: 32768.0000 (59902.6060)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1366  max mem: 15572
Epoch: [10]  [2580/2809]  eta: 0:02:15  lr: 0.000044  min_lr: 0.000000  loss: 4.0678 (4.1167)  class_acc: 0.2500 (0.2210)  loss_scale: 32768.0000 (59797.4738)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1588  max mem: 15572
Epoch: [10]  [2590/2809]  eta: 0:02:09  lr: 0.000044  min_lr: 0.000000  loss: 4.0303 (4.1160)  class_acc: 0.2083 (0.2212)  loss_scale: 32768.0000 (59693.1532)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1747  max mem: 15572
Epoch: [10]  [2600/2809]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000000  loss: 3.7895 (4.1152)  class_acc: 0.2500 (0.2214)  loss_scale: 32768.0000 (59589.6348)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1449  max mem: 15572
Epoch: [10]  [2610/2809]  eta: 0:01:57  lr: 0.000044  min_lr: 0.000000  loss: 3.8724 (4.1150)  class_acc: 0.2500 (0.2214)  loss_scale: 32768.0000 (59486.9092)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1547  max mem: 15572
[2025-01-15 19:39:22,909] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:39:22,909] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [2620/2809]  eta: 0:01:51  lr: 0.000044  min_lr: 0.000000  loss: 4.2215 (4.1160)  class_acc: 0.2083 (0.2212)  loss_scale: 32768.0000 (59484.9844)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.1929  max mem: 15572
Epoch: [10]  [2630/2809]  eta: 0:01:45  lr: 0.000044  min_lr: 0.000000  loss: 4.2594 (4.1160)  class_acc: 0.2083 (0.2212)  loss_scale: 65536.0000 (59507.9833)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1375  max mem: 15572
Epoch: [10]  [2640/2809]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000000  loss: 4.2219 (4.1164)  class_acc: 0.2083 (0.2210)  loss_scale: 65536.0000 (59530.8080)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1163  max mem: 15572
Epoch: [10]  [2650/2809]  eta: 0:01:33  lr: 0.000044  min_lr: 0.000000  loss: 4.1716 (4.1163)  class_acc: 0.1667 (0.2209)  loss_scale: 65536.0000 (59553.4606)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1544  max mem: 15572
Epoch: [10]  [2660/2809]  eta: 0:01:27  lr: 0.000044  min_lr: 0.000000  loss: 4.0897 (4.1163)  class_acc: 0.2083 (0.2211)  loss_scale: 65536.0000 (59575.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1535  max mem: 15572
Epoch: [10]  [2670/2809]  eta: 0:01:21  lr: 0.000044  min_lr: 0.000000  loss: 4.1075 (4.1165)  class_acc: 0.2500 (0.2211)  loss_scale: 65536.0000 (59598.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1686  max mem: 15572
Epoch: [10]  [2680/2809]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000000  loss: 4.2732 (4.1171)  class_acc: 0.1250 (0.2208)  loss_scale: 65536.0000 (59620.4043)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.2083  max mem: 15572
Epoch: [10]  [2690/2809]  eta: 0:01:10  lr: 0.000044  min_lr: 0.000000  loss: 4.0952 (4.1171)  class_acc: 0.2083 (0.2209)  loss_scale: 65536.0000 (59642.3872)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1625  max mem: 15572
Epoch: [10]  [2700/2809]  eta: 0:01:04  lr: 0.000044  min_lr: 0.000000  loss: 4.1280 (4.1176)  class_acc: 0.2083 (0.2209)  loss_scale: 65536.0000 (59664.2073)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1099  max mem: 15572
Epoch: [10]  [2710/2809]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000000  loss: 4.2603 (4.1184)  class_acc: 0.2083 (0.2207)  loss_scale: 65536.0000 (59685.8665)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1541  max mem: 15572
Epoch: [10]  [2720/2809]  eta: 0:00:52  lr: 0.000044  min_lr: 0.000000  loss: 4.2525 (4.1185)  class_acc: 0.1667 (0.2207)  loss_scale: 65536.0000 (59707.3664)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1697  max mem: 15572
Epoch: [10]  [2730/2809]  eta: 0:00:46  lr: 0.000044  min_lr: 0.000000  loss: 4.2525 (4.1193)  class_acc: 0.1667 (0.2205)  loss_scale: 65536.0000 (59728.7089)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1770  max mem: 15572
Epoch: [10]  [2740/2809]  eta: 0:00:40  lr: 0.000044  min_lr: 0.000000  loss: 4.3155 (4.1195)  class_acc: 0.2083 (0.2205)  loss_scale: 65536.0000 (59749.8957)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.1003  max mem: 15572
[2025-01-15 19:40:38,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:40:38,342] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:40:40,778] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30836
[2025-01-15 19:40:40,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:40:40,778] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [2750/2809]  eta: 0:00:34  lr: 0.000044  min_lr: 0.000000  loss: 4.1655 (4.1194)  class_acc: 0.2083 (0.2205)  loss_scale: 65536.0000 (59890.0414)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0997  max mem: 15572
Epoch: [10]  [2760/2809]  eta: 0:00:28  lr: 0.000044  min_lr: 0.000000  loss: 4.1216 (4.1194)  class_acc: 0.2083 (0.2205)  loss_scale: 65536.0000 (59910.4904)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1570  max mem: 15572
Epoch: [10]  [2770/2809]  eta: 0:00:23  lr: 0.000044  min_lr: 0.000000  loss: 4.3848 (4.1204)  class_acc: 0.1667 (0.2203)  loss_scale: 65536.0000 (59930.7918)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1348  max mem: 15572
Epoch: [10]  [2780/2809]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000000  loss: 4.2941 (4.1210)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (59950.9471)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1724  max mem: 15572
Epoch: [10]  [2790/2809]  eta: 0:00:11  lr: 0.000044  min_lr: 0.000000  loss: 4.2817 (4.1209)  class_acc: 0.1667 (0.2203)  loss_scale: 65536.0000 (59970.9581)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1191  max mem: 15572
Epoch: [10]  [2800/2809]  eta: 0:00:05  lr: 0.000044  min_lr: 0.000000  loss: 4.2817 (4.1211)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (59990.8261)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0453  max mem: 15572
Epoch: [10]  [2808/2809]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000000  loss: 4.1206 (4.1210)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (60006.6187)  weight_decay: 0.0500 (0.0500)  time: 0.4418  data: 0.0216  max mem: 15572
Epoch: [10] Total time: 0:27:35 (0.5894 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000000  loss: 4.1206 (4.1210)  class_acc: 0.1667 (0.2202)  loss_scale: 65536.0000 (60006.6187)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:56  loss: 0.3899 (0.3899)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.2812  data: 4.9650  max mem: 15572
Val:  [ 10/272]  eta: 0:03:19  loss: 3.8409 (3.2005)  acc1: 0.0000 (24.2424)  acc5: 33.3333 (42.9293)  time: 0.7633  data: 0.5556  max mem: 15572
Val:  [ 20/272]  eta: 0:02:08  loss: 3.2215 (3.1386)  acc1: 27.7778 (27.2487)  acc5: 50.0000 (52.3810)  time: 0.2733  data: 0.0832  max mem: 15572
Val:  [ 30/272]  eta: 0:01:53  loss: 3.1225 (3.2070)  acc1: 22.2222 (24.9104)  acc5: 61.1111 (53.9427)  time: 0.3052  data: 0.1110  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 3.0537 (3.1580)  acc1: 16.6667 (24.3902)  acc5: 66.6667 (57.1816)  time: 0.3612  data: 0.1547  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.9991 (3.0779)  acc1: 16.6667 (26.7974)  acc5: 66.6667 (60.2397)  time: 0.2817  data: 0.0817  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 2.0460 (2.8991)  acc1: 55.5556 (32.4226)  acc5: 83.3333 (62.5683)  time: 0.2767  data: 0.0813  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.8206 (2.8230)  acc1: 55.5556 (33.4898)  acc5: 83.3333 (64.8670)  time: 0.3143  data: 0.1216  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.5237 (2.8265)  acc1: 27.7778 (33.8134)  acc5: 77.7778 (64.8148)  time: 0.3344  data: 0.1467  max mem: 15572
Val:  [ 90/272]  eta: 0:01:07  loss: 3.2135 (2.8773)  acc1: 22.2222 (32.1123)  acc5: 61.1111 (64.2247)  time: 0.3705  data: 0.1815  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 3.2135 (2.9232)  acc1: 22.2222 (31.5182)  acc5: 61.1111 (63.6414)  time: 0.3655  data: 0.1647  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.3722 (2.9967)  acc1: 0.0000 (29.1792)  acc5: 50.0000 (61.6617)  time: 0.3512  data: 0.1406  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.5582 (3.0342)  acc1: 5.5556 (28.1910)  acc5: 50.0000 (60.8815)  time: 0.3061  data: 0.1123  max mem: 15572
Val:  [130/272]  eta: 0:00:51  loss: 3.1534 (2.9899)  acc1: 22.2222 (29.4741)  acc5: 61.1111 (61.5352)  time: 0.3194  data: 0.1263  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 2.6569 (2.9790)  acc1: 33.3333 (30.4571)  acc5: 61.1111 (61.4263)  time: 0.3538  data: 0.1537  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 3.0117 (2.9781)  acc1: 27.7778 (30.0589)  acc5: 61.1111 (61.7734)  time: 0.3301  data: 0.1425  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.8241 (2.9591)  acc1: 33.3333 (31.1939)  acc5: 72.2222 (62.6639)  time: 0.3168  data: 0.1399  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 2.8911 (2.9824)  acc1: 33.3333 (30.6043)  acc5: 66.6667 (62.0858)  time: 0.3394  data: 0.1451  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 2.8963 (2.9693)  acc1: 22.2222 (30.6630)  acc5: 66.6667 (62.7993)  time: 0.3543  data: 0.1522  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.9592 (3.0090)  acc1: 22.2222 (29.8429)  acc5: 55.5556 (61.4892)  time: 0.3427  data: 0.1563  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 3.0813 (3.0158)  acc1: 5.5556 (29.4638)  acc5: 50.0000 (61.6363)  time: 0.2796  data: 0.0961  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.7629 (3.0160)  acc1: 27.7778 (30.1474)  acc5: 72.2222 (61.8483)  time: 0.2443  data: 0.0482  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.9021 (3.0053)  acc1: 38.8889 (30.4424)  acc5: 72.2222 (62.2423)  time: 0.2885  data: 0.0927  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.1220 (2.9715)  acc1: 50.0000 (31.8182)  acc5: 77.7778 (63.0111)  time: 0.2876  data: 0.1027  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 2.0456 (2.9478)  acc1: 55.5556 (32.6187)  acc5: 83.3333 (63.6238)  time: 0.3043  data: 0.1084  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.8699 (2.9652)  acc1: 33.3333 (32.3152)  acc5: 66.6667 (63.1031)  time: 0.3228  data: 0.1284  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 2.0882 (2.8926)  acc1: 66.6667 (34.5040)  acc5: 77.7778 (64.2401)  time: 0.2822  data: 0.0926  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8713 (2.8936)  acc1: 55.5556 (34.1328)  acc5: 83.3333 (64.1451)  time: 0.2226  data: 0.0488  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8713 (2.8984)  acc1: 50.0000 (34.0979)  acc5: 83.3333 (64.1204)  time: 0.2169  data: 0.0488  max mem: 15572
Val: Total time: 0:01:29 (0.3287 s / it)
* Acc@1 34.098 Acc@5 64.120 loss 2.898
Accuracy of the network on the 4883 val videos: 34.1%
[2025-01-15 19:42:45,390] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 19:42:45,392] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 19:42:45,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 19:42:48,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 19:42:48,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.10%
Epoch: [11]  [   0/2809]  eta: 5:23:16  lr: 0.000044  min_lr: 0.000000  loss: 4.0998 (4.0998)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.9053  data: 6.4415  max mem: 15572
Epoch: [11]  [  10/2809]  eta: 0:52:36  lr: 0.000044  min_lr: 0.000000  loss: 4.2259 (4.2672)  class_acc: 0.1667 (0.1629)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1277  data: 0.6999  max mem: 15572
Epoch: [11]  [  20/2809]  eta: 0:36:16  lr: 0.000044  min_lr: 0.000000  loss: 4.2259 (4.2312)  class_acc: 0.1667 (0.1845)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0630  max mem: 15572
Epoch: [11]  [  30/2809]  eta: 0:31:47  lr: 0.000044  min_lr: 0.000000  loss: 4.0885 (4.1843)  class_acc: 0.2083 (0.2043)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4439  data: 0.0006  max mem: 15572
Epoch: [11]  [  40/2809]  eta: 0:29:13  lr: 0.000044  min_lr: 0.000000  loss: 4.1069 (4.1622)  class_acc: 0.1667 (0.2063)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4788  data: 0.0009  max mem: 15572
Epoch: [11]  [  50/2809]  eta: 0:27:40  lr: 0.000044  min_lr: 0.000000  loss: 4.1523 (4.1358)  class_acc: 0.2083 (0.2149)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4705  data: 0.0009  max mem: 15572
Epoch: [11]  [  60/2809]  eta: 0:27:53  lr: 0.000044  min_lr: 0.000000  loss: 4.1069 (4.1101)  class_acc: 0.2500 (0.2193)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1015  max mem: 15572
[2025-01-15 19:43:30,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:43:30,275] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:43:31,110] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30967
[2025-01-15 19:43:31,110] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:43:31,110] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [  70/2809]  eta: 0:28:49  lr: 0.000044  min_lr: 0.000000  loss: 4.0616 (4.1131)  class_acc: 0.1667 (0.2165)  loss_scale: 65536.0000 (67382.0845)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.2605  max mem: 15572
Epoch: [11]  [  80/2809]  eta: 0:29:07  lr: 0.000044  min_lr: 0.000000  loss: 4.0155 (4.1177)  class_acc: 0.1667 (0.2145)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.7368  data: 0.2836  max mem: 15572
Epoch: [11]  [  90/2809]  eta: 0:29:35  lr: 0.000044  min_lr: 0.000000  loss: 3.9859 (4.0944)  class_acc: 0.2500 (0.2253)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.7291  data: 0.2785  max mem: 15572
[2025-01-15 19:43:54,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=202, lr=[4.216826230280262e-07, 4.216826230280262e-07, 6.024037471828946e-07, 6.024037471828946e-07, 8.605767816898495e-07, 8.605767816898495e-07, 1.2293954024140709e-06, 1.2293954024140709e-06, 1.7562791463058155e-06, 1.7562791463058155e-06, 2.508970209008308e-06, 2.508970209008308e-06, 3.5842431557261544e-06, 3.5842431557261544e-06, 5.120347365323078e-06, 5.120347365323078e-06, 7.31478195046154e-06, 7.31478195046154e-06, 1.0449688500659345e-05, 1.0449688500659345e-05, 1.492812642951335e-05, 1.492812642951335e-05, 2.1325894899304786e-05, 2.1325894899304786e-05, 3.046556414186398e-05, 3.046556414186398e-05, 4.352223448837712e-05, 4.352223448837712e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:43:54,520] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=28.416127010105605, CurrSamplesPerSec=31.983122513017143, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [ 100/2809]  eta: 0:29:40  lr: 0.000044  min_lr: 0.000000  loss: 3.9783 (4.0995)  class_acc: 0.2917 (0.2236)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.7269  data: 0.2713  max mem: 15572
Epoch: [11]  [ 110/2809]  eta: 0:29:43  lr: 0.000044  min_lr: 0.000000  loss: 3.9757 (4.0756)  class_acc: 0.2083 (0.2305)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.2299  max mem: 15572
Epoch: [11]  [ 120/2809]  eta: 0:29:54  lr: 0.000044  min_lr: 0.000000  loss: 3.9757 (4.0641)  class_acc: 0.2917 (0.2362)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.2553  max mem: 15572
Epoch: [11]  [ 130/2809]  eta: 0:30:05  lr: 0.000044  min_lr: 0.000000  loss: 4.0994 (4.0782)  class_acc: 0.2083 (0.2341)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.7484  data: 0.2893  max mem: 15572
Epoch: [11]  [ 140/2809]  eta: 0:29:56  lr: 0.000044  min_lr: 0.000000  loss: 4.0994 (4.0779)  class_acc: 0.2500 (0.2373)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.2576  max mem: 15572
Epoch: [11]  [ 150/2809]  eta: 0:29:52  lr: 0.000044  min_lr: 0.000000  loss: 4.2231 (4.0891)  class_acc: 0.2083 (0.2354)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.6760  data: 0.2215  max mem: 15572
Epoch: [11]  [ 160/2809]  eta: 0:29:10  lr: 0.000043  min_lr: 0.000000  loss: 4.3549 (4.1004)  class_acc: 0.1667 (0.2296)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1326  max mem: 15572
Epoch: [11]  [ 170/2809]  eta: 0:28:22  lr: 0.000043  min_lr: 0.000000  loss: 4.0909 (4.0919)  class_acc: 0.2083 (0.2320)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.4250  data: 0.0219  max mem: 15572
Epoch: [11]  [ 180/2809]  eta: 0:27:50  lr: 0.000043  min_lr: 0.000000  loss: 4.0532 (4.0932)  class_acc: 0.2083 (0.2304)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.4290  data: 0.0004  max mem: 15572
Epoch: [11]  [ 190/2809]  eta: 0:27:27  lr: 0.000043  min_lr: 0.000000  loss: 4.0794 (4.0948)  class_acc: 0.2083 (0.2299)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0376  max mem: 15572
[2025-01-15 19:44:53,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:44:53,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:44:54,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31098
[2025-01-15 19:44:54,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:44:54,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 200/2809]  eta: 0:27:31  lr: 0.000043  min_lr: 0.000000  loss: 4.2310 (4.1066)  class_acc: 0.2083 (0.2282)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1535  max mem: 15572
Epoch: [11]  [ 210/2809]  eta: 0:27:22  lr: 0.000043  min_lr: 0.000000  loss: 4.3070 (4.1094)  class_acc: 0.1667 (0.2273)  loss_scale: 65536.0000 (66778.3886)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.1928  max mem: 15572
Epoch: [11]  [ 220/2809]  eta: 0:27:17  lr: 0.000043  min_lr: 0.000000  loss: 4.1940 (4.1116)  class_acc: 0.1667 (0.2264)  loss_scale: 65536.0000 (66722.1719)  weight_decay: 0.0500 (0.0500)  time: 0.6265  data: 0.1628  max mem: 15572
Epoch: [11]  [ 230/2809]  eta: 0:26:54  lr: 0.000043  min_lr: 0.000000  loss: 4.3062 (4.1195)  class_acc: 0.1667 (0.2247)  loss_scale: 65536.0000 (66670.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1037  max mem: 15572
Epoch: [11]  [ 240/2809]  eta: 0:26:46  lr: 0.000043  min_lr: 0.000000  loss: 4.2712 (4.1244)  class_acc: 0.1667 (0.2232)  loss_scale: 65536.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0893  max mem: 15572
Epoch: [11]  [ 250/2809]  eta: 0:26:38  lr: 0.000043  min_lr: 0.000000  loss: 4.1973 (4.1184)  class_acc: 0.2083 (0.2243)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1476  max mem: 15572
Epoch: [11]  [ 260/2809]  eta: 0:26:24  lr: 0.000043  min_lr: 0.000000  loss: 4.0215 (4.1180)  class_acc: 0.2083 (0.2240)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1304  max mem: 15572
Epoch: [11]  [ 270/2809]  eta: 0:26:16  lr: 0.000043  min_lr: 0.000000  loss: 4.0272 (4.1156)  class_acc: 0.1667 (0.2236)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1387  max mem: 15572
Epoch: [11]  [ 280/2809]  eta: 0:26:11  lr: 0.000043  min_lr: 0.000000  loss: 4.0272 (4.1116)  class_acc: 0.2083 (0.2242)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1846  max mem: 15572
Epoch: [11]  [ 290/2809]  eta: 0:26:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1190 (4.1094)  class_acc: 0.2500 (0.2254)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.2259  max mem: 15572
Epoch: [11]  [ 300/2809]  eta: 0:25:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0052 (4.1084)  class_acc: 0.2500 (0.2258)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1890  max mem: 15572
Epoch: [11]  [ 310/2809]  eta: 0:25:49  lr: 0.000043  min_lr: 0.000000  loss: 4.0052 (4.1048)  class_acc: 0.2083 (0.2266)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1304  max mem: 15572
[2025-01-15 19:46:03,119] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31213
[2025-01-15 19:46:03,120] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:46:03,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 320/2809]  eta: 0:25:39  lr: 0.000043  min_lr: 0.000000  loss: 3.9831 (4.1005)  class_acc: 0.2083 (0.2270)  loss_scale: 65536.0000 (65638.0810)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1225  max mem: 15572
Epoch: [11]  [ 330/2809]  eta: 0:25:33  lr: 0.000043  min_lr: 0.000000  loss: 3.9831 (4.1007)  class_acc: 0.2083 (0.2268)  loss_scale: 32768.0000 (64645.0272)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1216  max mem: 15572
Epoch: [11]  [ 340/2809]  eta: 0:25:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0828 (4.0998)  class_acc: 0.2083 (0.2272)  loss_scale: 32768.0000 (63710.2170)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1479  max mem: 15572
Epoch: [11]  [ 350/2809]  eta: 0:25:19  lr: 0.000043  min_lr: 0.000000  loss: 4.1086 (4.0997)  class_acc: 0.2083 (0.2277)  loss_scale: 32768.0000 (62828.6724)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1368  max mem: 15572
Epoch: [11]  [ 360/2809]  eta: 0:25:14  lr: 0.000043  min_lr: 0.000000  loss: 4.1087 (4.1022)  class_acc: 0.2083 (0.2274)  loss_scale: 32768.0000 (61995.9668)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1383  max mem: 15572
Epoch: [11]  [ 370/2809]  eta: 0:25:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0565 (4.0977)  class_acc: 0.2083 (0.2278)  loss_scale: 32768.0000 (61208.1509)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1220  max mem: 15572
Epoch: [11]  [ 380/2809]  eta: 0:24:51  lr: 0.000043  min_lr: 0.000000  loss: 4.1006 (4.1016)  class_acc: 0.2083 (0.2274)  loss_scale: 32768.0000 (60461.6903)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0847  max mem: 15572
Epoch: [11]  [ 390/2809]  eta: 0:24:51  lr: 0.000043  min_lr: 0.000000  loss: 4.1033 (4.1016)  class_acc: 0.2500 (0.2274)  loss_scale: 32768.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1613  max mem: 15572
Epoch: [11]  [ 400/2809]  eta: 0:24:41  lr: 0.000043  min_lr: 0.000000  loss: 3.9398 (4.0972)  class_acc: 0.2500 (0.2281)  loss_scale: 32768.0000 (59080.4589)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1671  max mem: 15572
Epoch: [11]  [ 410/2809]  eta: 0:24:34  lr: 0.000043  min_lr: 0.000000  loss: 4.0144 (4.0982)  class_acc: 0.2083 (0.2281)  loss_scale: 32768.0000 (58440.2530)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1268  max mem: 15572
Epoch: [11]  [ 420/2809]  eta: 0:24:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0519 (4.0946)  class_acc: 0.2083 (0.2276)  loss_scale: 32768.0000 (57830.4608)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.1695  max mem: 15572
Epoch: [11]  [ 430/2809]  eta: 0:24:19  lr: 0.000043  min_lr: 0.000000  loss: 4.0954 (4.0979)  class_acc: 0.2083 (0.2277)  loss_scale: 32768.0000 (57248.9652)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1497  max mem: 15572
Epoch: [11]  [ 440/2809]  eta: 0:24:07  lr: 0.000043  min_lr: 0.000000  loss: 4.1755 (4.1001)  class_acc: 0.1667 (0.2269)  loss_scale: 32768.0000 (56693.8413)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0752  max mem: 15572
[2025-01-15 19:47:19,127] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:47:19,127] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 450/2809]  eta: 0:24:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0874 (4.0971)  class_acc: 0.2083 (0.2278)  loss_scale: 32768.0000 (56744.5854)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1141  max mem: 15572
Epoch: [11]  [ 460/2809]  eta: 0:23:56  lr: 0.000043  min_lr: 0.000000  loss: 4.0880 (4.0988)  class_acc: 0.2500 (0.2278)  loss_scale: 65536.0000 (56935.2885)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1905  max mem: 15572
Epoch: [11]  [ 470/2809]  eta: 0:23:52  lr: 0.000043  min_lr: 0.000000  loss: 4.1721 (4.0997)  class_acc: 0.2083 (0.2277)  loss_scale: 65536.0000 (57117.8938)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.2154  max mem: 15572
Epoch: [11]  [ 480/2809]  eta: 0:23:45  lr: 0.000043  min_lr: 0.000000  loss: 4.1506 (4.0975)  class_acc: 0.2083 (0.2282)  loss_scale: 65536.0000 (57292.9064)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1906  max mem: 15572
Epoch: [11]  [ 490/2809]  eta: 0:23:33  lr: 0.000043  min_lr: 0.000000  loss: 4.2373 (4.1002)  class_acc: 0.2083 (0.2275)  loss_scale: 65536.0000 (57460.7902)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0796  max mem: 15572
Epoch: [11]  [ 500/2809]  eta: 0:23:29  lr: 0.000043  min_lr: 0.000000  loss: 4.1971 (4.0991)  class_acc: 0.2083 (0.2280)  loss_scale: 65536.0000 (57621.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0980  max mem: 15572
Epoch: [11]  [ 510/2809]  eta: 0:23:19  lr: 0.000043  min_lr: 0.000000  loss: 3.9866 (4.0977)  class_acc: 0.2500 (0.2283)  loss_scale: 65536.0000 (57776.8454)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1329  max mem: 15572
Epoch: [11]  [ 520/2809]  eta: 0:23:14  lr: 0.000043  min_lr: 0.000000  loss: 3.9449 (4.0961)  class_acc: 0.2500 (0.2292)  loss_scale: 65536.0000 (57925.7735)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1168  max mem: 15572
Epoch: [11]  [ 530/2809]  eta: 0:23:09  lr: 0.000043  min_lr: 0.000000  loss: 4.0770 (4.0966)  class_acc: 0.2500 (0.2294)  loss_scale: 65536.0000 (58069.0923)  weight_decay: 0.0500 (0.0500)  time: 0.6376  data: 0.1746  max mem: 15572
Epoch: [11]  [ 540/2809]  eta: 0:22:58  lr: 0.000043  min_lr: 0.000000  loss: 4.1827 (4.0979)  class_acc: 0.2083 (0.2291)  loss_scale: 65536.0000 (58207.1128)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1420  max mem: 15572
Epoch: [11]  [ 550/2809]  eta: 0:22:52  lr: 0.000043  min_lr: 0.000000  loss: 4.0533 (4.0954)  class_acc: 0.2500 (0.2298)  loss_scale: 65536.0000 (58340.1234)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1148  max mem: 15572
Epoch: [11]  [ 560/2809]  eta: 0:22:47  lr: 0.000043  min_lr: 0.000000  loss: 3.9812 (4.0937)  class_acc: 0.2500 (0.2302)  loss_scale: 65536.0000 (58468.3922)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1769  max mem: 15572
Epoch: [11]  [ 570/2809]  eta: 0:22:40  lr: 0.000043  min_lr: 0.000000  loss: 4.0274 (4.0961)  class_acc: 0.2083 (0.2297)  loss_scale: 65536.0000 (58592.1681)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1709  max mem: 15572
[2025-01-15 19:48:35,793] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:48:35,793] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:48:37,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31474
[2025-01-15 19:48:37,551] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:48:37,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 580/2809]  eta: 0:22:39  lr: 0.000043  min_lr: 0.000000  loss: 4.0276 (4.0939)  class_acc: 0.1667 (0.2301)  loss_scale: 65536.0000 (59162.8778)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.2160  max mem: 15572
Epoch: [11]  [ 590/2809]  eta: 0:22:31  lr: 0.000043  min_lr: 0.000000  loss: 4.1057 (4.0954)  class_acc: 0.2500 (0.2298)  loss_scale: 65536.0000 (59270.7140)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2027  max mem: 15572
Epoch: [11]  [ 600/2809]  eta: 0:22:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0368 (4.0944)  class_acc: 0.2083 (0.2303)  loss_scale: 65536.0000 (59374.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1471  max mem: 15572
Epoch: [11]  [ 610/2809]  eta: 0:22:18  lr: 0.000043  min_lr: 0.000000  loss: 4.0539 (4.0949)  class_acc: 0.2500 (0.2300)  loss_scale: 65536.0000 (59475.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1343  max mem: 15572
[2025-01-15 19:49:02,524] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31514
[2025-01-15 19:49:02,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 19:49:02,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 620/2809]  eta: 0:22:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1368 (4.0966)  class_acc: 0.1667 (0.2298)  loss_scale: 65536.0000 (59256.7858)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0847  max mem: 15572
Epoch: [11]  [ 630/2809]  eta: 0:22:06  lr: 0.000043  min_lr: 0.000000  loss: 4.3003 (4.1010)  class_acc: 0.1667 (0.2287)  loss_scale: 32768.0000 (58836.9952)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1541  max mem: 15572
Epoch: [11]  [ 640/2809]  eta: 0:21:58  lr: 0.000043  min_lr: 0.000000  loss: 4.2433 (4.1011)  class_acc: 0.2083 (0.2287)  loss_scale: 32768.0000 (58430.3027)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1797  max mem: 15572
Epoch: [11]  [ 650/2809]  eta: 0:21:51  lr: 0.000043  min_lr: 0.000000  loss: 4.0817 (4.0998)  class_acc: 0.2083 (0.2286)  loss_scale: 32768.0000 (58036.1045)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1215  max mem: 15572
Epoch: [11]  [ 660/2809]  eta: 0:21:46  lr: 0.000043  min_lr: 0.000000  loss: 3.8706 (4.0952)  class_acc: 0.2917 (0.2298)  loss_scale: 32768.0000 (57653.8336)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1367  max mem: 15572
Epoch: [11]  [ 670/2809]  eta: 0:21:41  lr: 0.000043  min_lr: 0.000000  loss: 4.2223 (4.0998)  class_acc: 0.2500 (0.2289)  loss_scale: 32768.0000 (57282.9568)  weight_decay: 0.0500 (0.0500)  time: 0.6381  data: 0.1801  max mem: 15572
Epoch: [11]  [ 680/2809]  eta: 0:21:32  lr: 0.000043  min_lr: 0.000000  loss: 4.3534 (4.1050)  class_acc: 0.1667 (0.2278)  loss_scale: 32768.0000 (56922.9721)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1321  max mem: 15572
Epoch: [11]  [ 690/2809]  eta: 0:21:28  lr: 0.000043  min_lr: 0.000000  loss: 4.2133 (4.1018)  class_acc: 0.2083 (0.2282)  loss_scale: 32768.0000 (56573.4067)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1621  max mem: 15572
Epoch: [11]  [ 700/2809]  eta: 0:21:17  lr: 0.000043  min_lr: 0.000000  loss: 4.0479 (4.1030)  class_acc: 0.2500 (0.2282)  loss_scale: 32768.0000 (56233.8146)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1260  max mem: 15572
Epoch: [11]  [ 710/2809]  eta: 0:21:16  lr: 0.000043  min_lr: 0.000000  loss: 3.7361 (4.0989)  class_acc: 0.2917 (0.2295)  loss_scale: 32768.0000 (55903.7750)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1390  max mem: 15572
Epoch: [11]  [ 720/2809]  eta: 0:21:07  lr: 0.000043  min_lr: 0.000000  loss: 3.7273 (4.0991)  class_acc: 0.2917 (0.2302)  loss_scale: 32768.0000 (55582.8904)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.2011  max mem: 15572
Epoch: [11]  [ 730/2809]  eta: 0:21:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0082 (4.0963)  class_acc: 0.2500 (0.2305)  loss_scale: 32768.0000 (55270.7852)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1492  max mem: 15572
Epoch: [11]  [ 740/2809]  eta: 0:20:54  lr: 0.000043  min_lr: 0.000000  loss: 4.1018 (4.0979)  class_acc: 0.2083 (0.2299)  loss_scale: 32768.0000 (54967.1039)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1343  max mem: 15572
[2025-01-15 19:50:19,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:50:19,493] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [ 750/2809]  eta: 0:20:46  lr: 0.000043  min_lr: 0.000000  loss: 4.1307 (4.0971)  class_acc: 0.1667 (0.2297)  loss_scale: 32768.0000 (54976.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1018  max mem: 15572
Epoch: [11]  [ 760/2809]  eta: 0:20:40  lr: 0.000043  min_lr: 0.000000  loss: 4.0028 (4.0954)  class_acc: 0.2500 (0.2301)  loss_scale: 65536.0000 (55115.6899)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1220  max mem: 15572
Epoch: [11]  [ 770/2809]  eta: 0:20:32  lr: 0.000043  min_lr: 0.000000  loss: 4.0028 (4.0974)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (55250.8431)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1245  max mem: 15572
Epoch: [11]  [ 780/2809]  eta: 0:20:26  lr: 0.000043  min_lr: 0.000000  loss: 4.2307 (4.0973)  class_acc: 0.2083 (0.2297)  loss_scale: 65536.0000 (55382.5352)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1434  max mem: 15572
Epoch: [11]  [ 790/2809]  eta: 0:20:18  lr: 0.000043  min_lr: 0.000000  loss: 4.1895 (4.0970)  class_acc: 0.2083 (0.2300)  loss_scale: 65536.0000 (55510.8976)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1294  max mem: 15572
Epoch: [11]  [ 800/2809]  eta: 0:20:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0905 (4.0972)  class_acc: 0.2500 (0.2301)  loss_scale: 65536.0000 (55636.0549)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1432  max mem: 15572
Epoch: [11]  [ 810/2809]  eta: 0:20:05  lr: 0.000043  min_lr: 0.000000  loss: 4.1864 (4.0983)  class_acc: 0.2083 (0.2300)  loss_scale: 65536.0000 (55758.1258)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1348  max mem: 15572
Epoch: [11]  [ 820/2809]  eta: 0:19:58  lr: 0.000043  min_lr: 0.000000  loss: 4.3090 (4.0981)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (55877.2229)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0892  max mem: 15572
Epoch: [11]  [ 830/2809]  eta: 0:19:53  lr: 0.000043  min_lr: 0.000000  loss: 4.0934 (4.0990)  class_acc: 0.2083 (0.2295)  loss_scale: 65536.0000 (55993.4537)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1728  max mem: 15572
Epoch: [11]  [ 840/2809]  eta: 0:19:46  lr: 0.000043  min_lr: 0.000000  loss: 4.0904 (4.0974)  class_acc: 0.2083 (0.2293)  loss_scale: 65536.0000 (56106.9203)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1839  max mem: 15572
Epoch: [11]  [ 850/2809]  eta: 0:19:41  lr: 0.000043  min_lr: 0.000000  loss: 4.0647 (4.0963)  class_acc: 0.2083 (0.2294)  loss_scale: 65536.0000 (56217.7203)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1664  max mem: 15572
Epoch: [11]  [ 860/2809]  eta: 0:19:35  lr: 0.000043  min_lr: 0.000000  loss: 4.0841 (4.0960)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (56325.9466)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1687  max mem: 15572
Epoch: [11]  [ 870/2809]  eta: 0:19:28  lr: 0.000043  min_lr: 0.000000  loss: 4.3128 (4.0979)  class_acc: 0.1667 (0.2291)  loss_scale: 65536.0000 (56431.6877)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1286  max mem: 15572
[2025-01-15 19:51:34,207] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:51:34,208] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:51:34,665] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31772
[2025-01-15 19:51:34,665] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:51:34,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 880/2809]  eta: 0:19:22  lr: 0.000043  min_lr: 0.000000  loss: 4.2162 (4.0982)  class_acc: 0.1667 (0.2290)  loss_scale: 65536.0000 (56609.4166)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1339  max mem: 15572
Epoch: [11]  [ 890/2809]  eta: 0:19:16  lr: 0.000043  min_lr: 0.000000  loss: 4.2162 (4.1003)  class_acc: 0.2083 (0.2285)  loss_scale: 65536.0000 (56709.6027)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1598  max mem: 15572
Epoch: [11]  [ 900/2809]  eta: 0:19:10  lr: 0.000043  min_lr: 0.000000  loss: 4.3153 (4.1021)  class_acc: 0.2083 (0.2282)  loss_scale: 65536.0000 (56807.5649)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1501  max mem: 15572
Epoch: [11]  [ 910/2809]  eta: 0:19:03  lr: 0.000043  min_lr: 0.000000  loss: 4.0975 (4.0997)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (56903.3765)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1163  max mem: 15572
Epoch: [11]  [ 920/2809]  eta: 0:18:58  lr: 0.000043  min_lr: 0.000000  loss: 4.0869 (4.0999)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (56997.1075)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1435  max mem: 15572
Epoch: [11]  [ 930/2809]  eta: 0:18:52  lr: 0.000043  min_lr: 0.000000  loss: 4.2267 (4.1019)  class_acc: 0.2083 (0.2287)  loss_scale: 65536.0000 (57088.8249)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1686  max mem: 15572
Epoch: [11]  [ 940/2809]  eta: 0:18:45  lr: 0.000043  min_lr: 0.000000  loss: 4.0625 (4.1007)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (57178.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1259  max mem: 15572
Epoch: [11]  [ 950/2809]  eta: 0:18:40  lr: 0.000043  min_lr: 0.000000  loss: 4.0625 (4.1019)  class_acc: 0.1667 (0.2282)  loss_scale: 65536.0000 (57266.4732)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1668  max mem: 15572
Epoch: [11]  [ 960/2809]  eta: 0:18:34  lr: 0.000043  min_lr: 0.000000  loss: 4.0378 (4.0991)  class_acc: 0.1667 (0.2288)  loss_scale: 65536.0000 (57352.5245)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1943  max mem: 15572
Epoch: [11]  [ 970/2809]  eta: 0:18:27  lr: 0.000043  min_lr: 0.000000  loss: 4.0587 (4.0990)  class_acc: 0.2083 (0.2287)  loss_scale: 65536.0000 (57436.8033)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1502  max mem: 15572
Epoch: [11]  [ 980/2809]  eta: 0:18:21  lr: 0.000043  min_lr: 0.000000  loss: 4.1991 (4.1014)  class_acc: 0.1667 (0.2283)  loss_scale: 65536.0000 (57519.3639)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1156  max mem: 15572
Epoch: [11]  [ 990/2809]  eta: 0:18:14  lr: 0.000043  min_lr: 0.000000  loss: 4.2016 (4.1013)  class_acc: 0.1667 (0.2282)  loss_scale: 65536.0000 (57600.2583)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.0930  max mem: 15572
Epoch: [11]  [1000/2809]  eta: 0:18:10  lr: 0.000043  min_lr: 0.000000  loss: 3.8645 (4.0986)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (57679.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1517  max mem: 15572
[2025-01-15 19:52:52,635] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:52:52,636] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:52:57,325] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31908
[2025-01-15 19:52:57,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:52:57,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1010/2809]  eta: 0:18:03  lr: 0.000043  min_lr: 0.000000  loss: 4.0089 (4.1005)  class_acc: 0.2083 (0.2283)  loss_scale: 65536.0000 (58211.0069)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1695  max mem: 15572
Epoch: [11]  [1020/2809]  eta: 0:17:56  lr: 0.000043  min_lr: 0.000000  loss: 4.2028 (4.1014)  class_acc: 0.1667 (0.2281)  loss_scale: 65536.0000 (58282.7502)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1265  max mem: 15572
Epoch: [11]  [1030/2809]  eta: 0:17:51  lr: 0.000043  min_lr: 0.000000  loss: 4.0601 (4.1012)  class_acc: 0.2083 (0.2278)  loss_scale: 65536.0000 (58353.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1371  max mem: 15572
Epoch: [11]  [1040/2809]  eta: 0:17:45  lr: 0.000043  min_lr: 0.000000  loss: 4.0601 (4.1013)  class_acc: 0.2083 (0.2277)  loss_scale: 65536.0000 (58422.1018)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1472  max mem: 15572
Epoch: [11]  [1050/2809]  eta: 0:17:39  lr: 0.000043  min_lr: 0.000000  loss: 4.1451 (4.1004)  class_acc: 0.2083 (0.2278)  loss_scale: 65536.0000 (58489.7888)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1384  max mem: 15572
Epoch: [11]  [1060/2809]  eta: 0:17:32  lr: 0.000043  min_lr: 0.000000  loss: 4.2181 (4.1004)  class_acc: 0.2083 (0.2279)  loss_scale: 65536.0000 (58556.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1370  max mem: 15572
Epoch: [11]  [1070/2809]  eta: 0:17:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0531 (4.0985)  class_acc: 0.2500 (0.2285)  loss_scale: 65536.0000 (58621.3707)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1308  max mem: 15572
Epoch: [11]  [1080/2809]  eta: 0:17:19  lr: 0.000043  min_lr: 0.000000  loss: 3.9674 (4.0976)  class_acc: 0.2083 (0.2285)  loss_scale: 65536.0000 (58685.3358)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1300  max mem: 15572
Epoch: [11]  [1090/2809]  eta: 0:17:13  lr: 0.000043  min_lr: 0.000000  loss: 3.9674 (4.0962)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (58748.1283)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1152  max mem: 15572
[2025-01-15 19:53:49,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=208, lr=[4.1784613185839324e-07, 4.1784613185839324e-07, 5.969230455119903e-07, 5.969230455119903e-07, 8.527472078742721e-07, 8.527472078742721e-07, 1.218210296963246e-06, 1.218210296963246e-06, 1.7403004242332084e-06, 1.7403004242332084e-06, 2.486143463190298e-06, 2.486143463190298e-06, 3.551633518843283e-06, 3.551633518843283e-06, 5.073762169776119e-06, 5.073762169776119e-06, 7.248231671108742e-06, 7.248231671108742e-06, 1.035461667301249e-05, 1.035461667301249e-05, 1.4792309532874983e-05, 1.4792309532874983e-05, 2.1131870761249978e-05, 2.1131870761249978e-05, 3.0188386801785686e-05, 3.0188386801785686e-05, 4.312626685969384e-05, 4.312626685969384e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 19:53:49,599] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=28.410943151510534, CurrSamplesPerSec=29.02148834479144, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [1100/2809]  eta: 0:17:05  lr: 0.000043  min_lr: 0.000000  loss: 3.9684 (4.0949)  class_acc: 0.2500 (0.2290)  loss_scale: 65536.0000 (58809.7802)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0759  max mem: 15572
Epoch: [11]  [1110/2809]  eta: 0:16:59  lr: 0.000043  min_lr: 0.000000  loss: 4.0542 (4.0948)  class_acc: 0.1667 (0.2285)  loss_scale: 65536.0000 (58870.3222)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0901  max mem: 15572
Epoch: [11]  [1120/2809]  eta: 0:16:51  lr: 0.000043  min_lr: 0.000000  loss: 4.3049 (4.0973)  class_acc: 0.1667 (0.2279)  loss_scale: 65536.0000 (58929.7841)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0879  max mem: 15572
Epoch: [11]  [1130/2809]  eta: 0:16:46  lr: 0.000043  min_lr: 0.000000  loss: 4.2605 (4.0965)  class_acc: 0.1667 (0.2278)  loss_scale: 65536.0000 (58988.1945)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1078  max mem: 15572
[2025-01-15 19:54:10,088] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:54:10,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:54:12,277] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32039
[2025-01-15 19:54:12,278] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:54:12,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1140/2809]  eta: 0:16:39  lr: 0.000043  min_lr: 0.000000  loss: 3.9514 (4.0951)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (59160.4557)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1487  max mem: 15572
Epoch: [11]  [1150/2809]  eta: 0:16:35  lr: 0.000043  min_lr: 0.000000  loss: 4.0103 (4.0946)  class_acc: 0.2500 (0.2280)  loss_scale: 65536.0000 (59215.8471)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1794  max mem: 15572
Epoch: [11]  [1160/2809]  eta: 0:16:28  lr: 0.000043  min_lr: 0.000000  loss: 4.0125 (4.0947)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (59270.2842)  weight_decay: 0.0500 (0.0500)  time: 0.6368  data: 0.1807  max mem: 15572
Epoch: [11]  [1170/2809]  eta: 0:16:21  lr: 0.000043  min_lr: 0.000000  loss: 4.1302 (4.0954)  class_acc: 0.1667 (0.2278)  loss_scale: 65536.0000 (59323.7916)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1060  max mem: 15572
Epoch: [11]  [1180/2809]  eta: 0:16:16  lr: 0.000043  min_lr: 0.000000  loss: 4.2779 (4.0974)  class_acc: 0.1667 (0.2273)  loss_scale: 65536.0000 (59376.3929)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.0967  max mem: 15572
Epoch: [11]  [1190/2809]  eta: 0:16:10  lr: 0.000043  min_lr: 0.000000  loss: 4.1064 (4.0961)  class_acc: 0.2083 (0.2280)  loss_scale: 65536.0000 (59428.1108)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1403  max mem: 15572
Epoch: [11]  [1200/2809]  eta: 0:16:03  lr: 0.000043  min_lr: 0.000000  loss: 3.9742 (4.0953)  class_acc: 0.2500 (0.2281)  loss_scale: 65536.0000 (59478.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1117  max mem: 15572
Epoch: [11]  [1210/2809]  eta: 0:15:57  lr: 0.000043  min_lr: 0.000000  loss: 3.9815 (4.0949)  class_acc: 0.2500 (0.2286)  loss_scale: 65536.0000 (59528.9843)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1074  max mem: 15572
Epoch: [11]  [1220/2809]  eta: 0:15:51  lr: 0.000043  min_lr: 0.000000  loss: 4.0850 (4.0963)  class_acc: 0.2500 (0.2284)  loss_scale: 65536.0000 (59578.1818)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1404  max mem: 15572
Epoch: [11]  [1230/2809]  eta: 0:15:45  lr: 0.000043  min_lr: 0.000000  loss: 4.2715 (4.0974)  class_acc: 0.1667 (0.2282)  loss_scale: 65536.0000 (59626.5800)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1627  max mem: 15572
Epoch: [11]  [1240/2809]  eta: 0:15:38  lr: 0.000043  min_lr: 0.000000  loss: 4.1708 (4.0973)  class_acc: 0.2083 (0.2281)  loss_scale: 65536.0000 (59674.1982)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1157  max mem: 15572
Epoch: [11]  [1250/2809]  eta: 0:15:32  lr: 0.000043  min_lr: 0.000000  loss: 4.1348 (4.0975)  class_acc: 0.2500 (0.2283)  loss_scale: 65536.0000 (59721.0552)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0792  max mem: 15572
Epoch: [11]  [1260/2809]  eta: 0:15:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0435 (4.0964)  class_acc: 0.2500 (0.2287)  loss_scale: 65536.0000 (59767.1689)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1464  max mem: 15572
[2025-01-15 19:55:27,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:55:27,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1270/2809]  eta: 0:15:19  lr: 0.000043  min_lr: 0.000000  loss: 3.9580 (4.0975)  class_acc: 0.2083 (0.2287)  loss_scale: 65536.0000 (59915.6821)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1399  max mem: 15572
[2025-01-15 19:55:28,770] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32170
[2025-01-15 19:55:28,770] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:55:28,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1280/2809]  eta: 0:15:13  lr: 0.000043  min_lr: 0.000000  loss: 4.3773 (4.0987)  class_acc: 0.2083 (0.2284)  loss_scale: 65536.0000 (59959.5566)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1539  max mem: 15572
Epoch: [11]  [1290/2809]  eta: 0:15:08  lr: 0.000043  min_lr: 0.000000  loss: 4.3058 (4.0994)  class_acc: 0.2083 (0.2284)  loss_scale: 65536.0000 (60002.7514)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.2112  max mem: 15572
Epoch: [11]  [1300/2809]  eta: 0:15:03  lr: 0.000043  min_lr: 0.000000  loss: 4.0055 (4.0980)  class_acc: 0.2083 (0.2285)  loss_scale: 65536.0000 (60045.2821)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.1911  max mem: 15572
Epoch: [11]  [1310/2809]  eta: 0:14:56  lr: 0.000043  min_lr: 0.000000  loss: 4.0043 (4.0991)  class_acc: 0.2500 (0.2287)  loss_scale: 65536.0000 (60087.1640)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1194  max mem: 15572
Epoch: [11]  [1320/2809]  eta: 0:14:49  lr: 0.000043  min_lr: 0.000000  loss: 4.1635 (4.0998)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (60128.4118)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.1019  max mem: 15572
Epoch: [11]  [1330/2809]  eta: 0:14:43  lr: 0.000043  min_lr: 0.000000  loss: 4.1688 (4.0999)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (60169.0398)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1171  max mem: 15572
Epoch: [11]  [1340/2809]  eta: 0:14:38  lr: 0.000043  min_lr: 0.000000  loss: 4.0826 (4.0979)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (60209.0619)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1822  max mem: 15572
Epoch: [11]  [1350/2809]  eta: 0:14:31  lr: 0.000043  min_lr: 0.000000  loss: 4.0446 (4.0972)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (60248.4915)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1521  max mem: 15572
Epoch: [11]  [1360/2809]  eta: 0:14:25  lr: 0.000043  min_lr: 0.000000  loss: 4.1998 (4.0975)  class_acc: 0.2083 (0.2288)  loss_scale: 65536.0000 (60287.3417)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1090  max mem: 15572
Epoch: [11]  [1370/2809]  eta: 0:14:20  lr: 0.000043  min_lr: 0.000000  loss: 4.3073 (4.0983)  class_acc: 0.1667 (0.2287)  loss_scale: 65536.0000 (60325.6251)  weight_decay: 0.0500 (0.0500)  time: 0.6310  data: 0.1873  max mem: 15572
Epoch: [11]  [1380/2809]  eta: 0:14:14  lr: 0.000043  min_lr: 0.000000  loss: 4.1871 (4.0993)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (60363.3541)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1663  max mem: 15572
Epoch: [11]  [1390/2809]  eta: 0:14:08  lr: 0.000043  min_lr: 0.000000  loss: 4.0756 (4.0991)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (60400.5406)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1229  max mem: 15572
[2025-01-15 19:56:46,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:56:46,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1400/2809]  eta: 0:14:02  lr: 0.000043  min_lr: 0.000000  loss: 4.1972 (4.1000)  class_acc: 0.2083 (0.2283)  loss_scale: 65536.0000 (60483.9743)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1377  max mem: 15572
[2025-01-15 19:56:47,242] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32301
[2025-01-15 19:56:47,242] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:56:47,242] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1410/2809]  eta: 0:13:55  lr: 0.000043  min_lr: 0.000000  loss: 4.1972 (4.0997)  class_acc: 0.2083 (0.2285)  loss_scale: 65536.0000 (60566.2254)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1107  max mem: 15572
Epoch: [11]  [1420/2809]  eta: 0:13:49  lr: 0.000043  min_lr: 0.000000  loss: 4.1991 (4.1006)  class_acc: 0.2500 (0.2285)  loss_scale: 65536.0000 (60601.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1267  max mem: 15572
Epoch: [11]  [1430/2809]  eta: 0:13:43  lr: 0.000043  min_lr: 0.000000  loss: 4.1981 (4.1005)  class_acc: 0.2500 (0.2284)  loss_scale: 65536.0000 (60635.6841)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1583  max mem: 15572
Epoch: [11]  [1440/2809]  eta: 0:13:38  lr: 0.000043  min_lr: 0.000000  loss: 4.0793 (4.1002)  class_acc: 0.2083 (0.2284)  loss_scale: 65536.0000 (60669.6905)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1794  max mem: 15572
Epoch: [11]  [1450/2809]  eta: 0:13:31  lr: 0.000043  min_lr: 0.000000  loss: 4.0793 (4.1008)  class_acc: 0.2083 (0.2279)  loss_scale: 65536.0000 (60703.2281)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1217  max mem: 15572
Epoch: [11]  [1460/2809]  eta: 0:13:25  lr: 0.000043  min_lr: 0.000000  loss: 4.1895 (4.1008)  class_acc: 0.1250 (0.2275)  loss_scale: 65536.0000 (60736.3066)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0985  max mem: 15572
Epoch: [11]  [1470/2809]  eta: 0:13:19  lr: 0.000043  min_lr: 0.000000  loss: 4.0016 (4.0993)  class_acc: 0.2083 (0.2276)  loss_scale: 65536.0000 (60768.9354)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1624  max mem: 15572
Epoch: [11]  [1480/2809]  eta: 0:13:14  lr: 0.000043  min_lr: 0.000000  loss: 3.8761 (4.0993)  class_acc: 0.2500 (0.2276)  loss_scale: 65536.0000 (60801.1236)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.2038  max mem: 15572
Epoch: [11]  [1490/2809]  eta: 0:13:07  lr: 0.000043  min_lr: 0.000000  loss: 4.0128 (4.1002)  class_acc: 0.2083 (0.2273)  loss_scale: 65536.0000 (60832.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1601  max mem: 15572
Epoch: [11]  [1500/2809]  eta: 0:13:01  lr: 0.000043  min_lr: 0.000000  loss: 4.0391 (4.0991)  class_acc: 0.2083 (0.2275)  loss_scale: 65536.0000 (60864.2132)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0819  max mem: 15572
Epoch: [11]  [1510/2809]  eta: 0:12:54  lr: 0.000043  min_lr: 0.000000  loss: 4.1560 (4.1002)  class_acc: 0.2083 (0.2271)  loss_scale: 65536.0000 (60895.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0689  max mem: 15572
Epoch: [11]  [1520/2809]  eta: 0:12:48  lr: 0.000043  min_lr: 0.000000  loss: 4.2092 (4.0990)  class_acc: 0.2083 (0.2277)  loss_scale: 65536.0000 (60925.6437)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0653  max mem: 15572
Epoch: [11]  [1530/2809]  eta: 0:12:42  lr: 0.000043  min_lr: 0.000000  loss: 3.9244 (4.0980)  class_acc: 0.2917 (0.2281)  loss_scale: 65536.0000 (60955.7570)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1426  max mem: 15572
[2025-01-15 19:58:02,381] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:58:02,381] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:58:05,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32436
[2025-01-15 19:58:05,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:58:05,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1540/2809]  eta: 0:12:36  lr: 0.000043  min_lr: 0.000000  loss: 3.9244 (4.0976)  class_acc: 0.2500 (0.2282)  loss_scale: 65536.0000 (61240.6489)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1678  max mem: 15572
Epoch: [11]  [1550/2809]  eta: 0:12:30  lr: 0.000043  min_lr: 0.000000  loss: 3.9766 (4.0976)  class_acc: 0.2083 (0.2280)  loss_scale: 65536.0000 (61268.3430)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1383  max mem: 15572
Epoch: [11]  [1560/2809]  eta: 0:12:24  lr: 0.000043  min_lr: 0.000000  loss: 4.0469 (4.0975)  class_acc: 0.2083 (0.2277)  loss_scale: 65536.0000 (61295.6823)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1099  max mem: 15572
Epoch: [11]  [1570/2809]  eta: 0:12:18  lr: 0.000043  min_lr: 0.000000  loss: 4.0469 (4.0974)  class_acc: 0.2083 (0.2277)  loss_scale: 65536.0000 (61322.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1068  max mem: 15572
Epoch: [11]  [1580/2809]  eta: 0:12:12  lr: 0.000043  min_lr: 0.000000  loss: 4.2183 (4.0980)  class_acc: 0.1667 (0.2276)  loss_scale: 65536.0000 (61349.3232)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.0932  max mem: 15572
Epoch: [11]  [1590/2809]  eta: 0:12:06  lr: 0.000043  min_lr: 0.000000  loss: 4.2183 (4.0980)  class_acc: 0.1667 (0.2274)  loss_scale: 65536.0000 (61375.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1073  max mem: 15572
Epoch: [11]  [1600/2809]  eta: 0:12:00  lr: 0.000043  min_lr: 0.000000  loss: 4.1137 (4.0985)  class_acc: 0.2083 (0.2275)  loss_scale: 65536.0000 (61401.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1673  max mem: 15572
Epoch: [11]  [1610/2809]  eta: 0:11:53  lr: 0.000043  min_lr: 0.000000  loss: 4.1978 (4.0993)  class_acc: 0.2083 (0.2273)  loss_scale: 65536.0000 (61427.2874)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1266  max mem: 15572
Epoch: [11]  [1620/2809]  eta: 0:11:47  lr: 0.000043  min_lr: 0.000000  loss: 4.0461 (4.0983)  class_acc: 0.2083 (0.2276)  loss_scale: 65536.0000 (61452.6342)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1068  max mem: 15572
Epoch: [11]  [1630/2809]  eta: 0:11:41  lr: 0.000043  min_lr: 0.000000  loss: 3.8762 (4.0964)  class_acc: 0.2917 (0.2283)  loss_scale: 65536.0000 (61477.6701)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1370  max mem: 15572
Epoch: [11]  [1640/2809]  eta: 0:11:36  lr: 0.000043  min_lr: 0.000000  loss: 3.9256 (4.0966)  class_acc: 0.2917 (0.2284)  loss_scale: 65536.0000 (61502.4010)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1711  max mem: 15572
Epoch: [11]  [1650/2809]  eta: 0:11:29  lr: 0.000043  min_lr: 0.000000  loss: 4.0664 (4.0964)  class_acc: 0.2083 (0.2283)  loss_scale: 65536.0000 (61526.8322)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1754  max mem: 15572
Epoch: [11]  [1660/2809]  eta: 0:11:24  lr: 0.000043  min_lr: 0.000000  loss: 4.0398 (4.0955)  class_acc: 0.2083 (0.2284)  loss_scale: 65536.0000 (61550.9693)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1688  max mem: 15572
[2025-01-15 19:59:20,857] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 19:59:20,857] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 19:59:21,259] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32566
[2025-01-15 19:59:21,260] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 19:59:21,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1670/2809]  eta: 0:11:18  lr: 0.000043  min_lr: 0.000000  loss: 3.9748 (4.0959)  class_acc: 0.2083 (0.2283)  loss_scale: 65536.0000 (61614.0371)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1770  max mem: 15572
Epoch: [11]  [1680/2809]  eta: 0:11:12  lr: 0.000043  min_lr: 0.000000  loss: 4.3883 (4.0975)  class_acc: 0.2083 (0.2280)  loss_scale: 65536.0000 (61637.3682)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1675  max mem: 15572
Epoch: [11]  [1690/2809]  eta: 0:11:05  lr: 0.000043  min_lr: 0.000000  loss: 4.1502 (4.0963)  class_acc: 0.2500 (0.2284)  loss_scale: 65536.0000 (61660.4234)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1179  max mem: 15572
Epoch: [11]  [1700/2809]  eta: 0:10:59  lr: 0.000043  min_lr: 0.000000  loss: 3.8738 (4.0954)  class_acc: 0.2500 (0.2284)  loss_scale: 65536.0000 (61683.2075)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0929  max mem: 15572
Epoch: [11]  [1710/2809]  eta: 0:10:53  lr: 0.000043  min_lr: 0.000000  loss: 4.1102 (4.0957)  class_acc: 0.2083 (0.2283)  loss_scale: 65536.0000 (61705.7253)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1219  max mem: 15572
Epoch: [11]  [1720/2809]  eta: 0:10:47  lr: 0.000043  min_lr: 0.000000  loss: 4.1102 (4.0948)  class_acc: 0.2083 (0.2286)  loss_scale: 65536.0000 (61727.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1010  max mem: 15572
Epoch: [11]  [1730/2809]  eta: 0:10:41  lr: 0.000043  min_lr: 0.000000  loss: 4.0173 (4.0939)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (61749.9804)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0735  max mem: 15572
Epoch: [11]  [1740/2809]  eta: 0:10:35  lr: 0.000043  min_lr: 0.000000  loss: 4.2568 (4.0951)  class_acc: 0.2500 (0.2288)  loss_scale: 65536.0000 (61771.7266)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1575  max mem: 15572
Epoch: [11]  [1750/2809]  eta: 0:10:30  lr: 0.000043  min_lr: 0.000000  loss: 4.3100 (4.0946)  class_acc: 0.2083 (0.2291)  loss_scale: 65536.0000 (61793.2244)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.2542  max mem: 15572
Epoch: [11]  [1760/2809]  eta: 0:10:24  lr: 0.000043  min_lr: 0.000000  loss: 3.9920 (4.0941)  class_acc: 0.2083 (0.2292)  loss_scale: 65536.0000 (61814.4781)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.2089  max mem: 15572
Epoch: [11]  [1770/2809]  eta: 0:10:18  lr: 0.000043  min_lr: 0.000000  loss: 4.1389 (4.0947)  class_acc: 0.1667 (0.2288)  loss_scale: 65536.0000 (61835.4918)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1866  max mem: 15572
Epoch: [11]  [1780/2809]  eta: 0:10:13  lr: 0.000043  min_lr: 0.000000  loss: 4.1601 (4.0947)  class_acc: 0.1667 (0.2286)  loss_scale: 65536.0000 (61856.2695)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.1944  max mem: 15572
Epoch: [11]  [1790/2809]  eta: 0:10:07  lr: 0.000043  min_lr: 0.000000  loss: 3.9564 (4.0933)  class_acc: 0.2500 (0.2288)  loss_scale: 65536.0000 (61876.8152)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.2158  max mem: 15572
[2025-01-15 20:00:39,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:00:39,909] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1800/2809]  eta: 0:10:01  lr: 0.000043  min_lr: 0.000000  loss: 3.8205 (4.0910)  class_acc: 0.2917 (0.2292)  loss_scale: 65536.0000 (62079.0761)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1593  max mem: 15572
[2025-01-15 20:00:44,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32705
[2025-01-15 20:00:44,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:00:44,706] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1810/2809]  eta: 0:09:55  lr: 0.000043  min_lr: 0.000000  loss: 3.7622 (4.0901)  class_acc: 0.2917 (0.2292)  loss_scale: 65536.0000 (62279.1033)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.1004  max mem: 15572
Epoch: [11]  [1820/2809]  eta: 0:09:49  lr: 0.000043  min_lr: 0.000000  loss: 3.9557 (4.0905)  class_acc: 0.2500 (0.2291)  loss_scale: 65536.0000 (62296.9885)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1522  max mem: 15572
Epoch: [11]  [1830/2809]  eta: 0:09:43  lr: 0.000043  min_lr: 0.000000  loss: 4.2216 (4.0910)  class_acc: 0.2083 (0.2289)  loss_scale: 65536.0000 (62314.6783)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1551  max mem: 15572
Epoch: [11]  [1840/2809]  eta: 0:09:38  lr: 0.000043  min_lr: 0.000000  loss: 4.1890 (4.0906)  class_acc: 0.2083 (0.2290)  loss_scale: 65536.0000 (62332.1760)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.1900  max mem: 15572
Epoch: [11]  [1850/2809]  eta: 0:09:32  lr: 0.000043  min_lr: 0.000000  loss: 4.0690 (4.0904)  class_acc: 0.2500 (0.2289)  loss_scale: 65536.0000 (62349.4846)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.2062  max mem: 15572
Epoch: [11]  [1860/2809]  eta: 0:09:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0275 (4.0908)  class_acc: 0.2083 (0.2291)  loss_scale: 65536.0000 (62366.6072)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1499  max mem: 15572
Epoch: [11]  [1870/2809]  eta: 0:09:19  lr: 0.000043  min_lr: 0.000000  loss: 4.0269 (4.0906)  class_acc: 0.2083 (0.2292)  loss_scale: 65536.0000 (62383.5468)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.0961  max mem: 15572
Epoch: [11]  [1880/2809]  eta: 0:09:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0269 (4.0903)  class_acc: 0.2083 (0.2294)  loss_scale: 65536.0000 (62400.3062)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1451  max mem: 15572
Epoch: [11]  [1890/2809]  eta: 0:09:08  lr: 0.000043  min_lr: 0.000000  loss: 4.0528 (4.0906)  class_acc: 0.2917 (0.2296)  loss_scale: 65536.0000 (62416.8884)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1821  max mem: 15572
Epoch: [11]  [1900/2809]  eta: 0:09:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0559 (4.0907)  class_acc: 0.2500 (0.2297)  loss_scale: 65536.0000 (62433.2962)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1417  max mem: 15572
Epoch: [11]  [1910/2809]  eta: 0:08:55  lr: 0.000043  min_lr: 0.000000  loss: 4.1065 (4.0911)  class_acc: 0.2083 (0.2296)  loss_scale: 65536.0000 (62449.5322)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1090  max mem: 15572
Epoch: [11]  [1920/2809]  eta: 0:08:50  lr: 0.000043  min_lr: 0.000000  loss: 4.0875 (4.0908)  class_acc: 0.2500 (0.2298)  loss_scale: 65536.0000 (62465.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1485  max mem: 15572
Epoch: [11]  [1930/2809]  eta: 0:08:43  lr: 0.000043  min_lr: 0.000000  loss: 4.0875 (4.0919)  class_acc: 0.2083 (0.2297)  loss_scale: 65536.0000 (62481.4997)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1426  max mem: 15572
[2025-01-15 20:02:02,189] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:02:02,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:02:03,512] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32837
[2025-01-15 20:02:03,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:02:03,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1940/2809]  eta: 0:08:38  lr: 0.000043  min_lr: 0.000000  loss: 4.2561 (4.0916)  class_acc: 0.1667 (0.2298)  loss_scale: 65536.0000 (62598.5286)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1229  max mem: 15572
Epoch: [11]  [1950/2809]  eta: 0:08:32  lr: 0.000043  min_lr: 0.000000  loss: 3.9344 (4.0909)  class_acc: 0.2500 (0.2301)  loss_scale: 65536.0000 (62613.5848)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1379  max mem: 15572
Epoch: [11]  [1960/2809]  eta: 0:08:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0256 (4.0908)  class_acc: 0.2500 (0.2302)  loss_scale: 65536.0000 (62628.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1132  max mem: 15572
Epoch: [11]  [1970/2809]  eta: 0:08:20  lr: 0.000043  min_lr: 0.000000  loss: 4.0590 (4.0909)  class_acc: 0.2500 (0.2302)  loss_scale: 65536.0000 (62643.2390)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1256  max mem: 15572
Epoch: [11]  [1980/2809]  eta: 0:08:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0294 (4.0909)  class_acc: 0.2500 (0.2303)  loss_scale: 65536.0000 (62657.8415)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1376  max mem: 15572
Epoch: [11]  [1990/2809]  eta: 0:08:08  lr: 0.000043  min_lr: 0.000000  loss: 4.0061 (4.0902)  class_acc: 0.2500 (0.2302)  loss_scale: 65536.0000 (62672.2973)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1479  max mem: 15572
Epoch: [11]  [2000/2809]  eta: 0:08:02  lr: 0.000043  min_lr: 0.000000  loss: 4.1812 (4.0917)  class_acc: 0.1667 (0.2299)  loss_scale: 65536.0000 (62686.6087)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1548  max mem: 15572
Epoch: [11]  [2010/2809]  eta: 0:07:56  lr: 0.000043  min_lr: 0.000000  loss: 4.1812 (4.0914)  class_acc: 0.1667 (0.2298)  loss_scale: 65536.0000 (62700.7777)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1979  max mem: 15572
Epoch: [11]  [2020/2809]  eta: 0:07:50  lr: 0.000043  min_lr: 0.000000  loss: 3.9867 (4.0905)  class_acc: 0.2500 (0.2302)  loss_scale: 65536.0000 (62714.8065)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.1903  max mem: 15572
Epoch: [11]  [2030/2809]  eta: 0:07:44  lr: 0.000043  min_lr: 0.000000  loss: 3.9413 (4.0895)  class_acc: 0.2917 (0.2307)  loss_scale: 65536.0000 (62728.6972)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1534  max mem: 15572
Epoch: [11]  [2040/2809]  eta: 0:07:38  lr: 0.000043  min_lr: 0.000000  loss: 4.0336 (4.0903)  class_acc: 0.2917 (0.2307)  loss_scale: 65536.0000 (62742.4517)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1357  max mem: 15572
Epoch: [11]  [2050/2809]  eta: 0:07:32  lr: 0.000043  min_lr: 0.000000  loss: 4.2031 (4.0904)  class_acc: 0.2500 (0.2306)  loss_scale: 65536.0000 (62756.0722)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0893  max mem: 15572
Epoch: [11]  [2060/2809]  eta: 0:07:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0053 (4.0903)  class_acc: 0.2500 (0.2307)  loss_scale: 65536.0000 (62769.5604)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0703  max mem: 15572
[2025-01-15 20:03:19,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:03:19,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2070/2809]  eta: 0:07:19  lr: 0.000043  min_lr: 0.000000  loss: 4.0036 (4.0904)  class_acc: 0.2083 (0.2307)  loss_scale: 65536.0000 (62909.4969)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0674  max mem: 15572
[2025-01-15 20:03:24,530] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32975
[2025-01-15 20:03:24,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:03:24,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2080/2809]  eta: 0:07:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0593 (4.0907)  class_acc: 0.2083 (0.2307)  loss_scale: 65536.0000 (63079.5810)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1310  max mem: 15572
[2025-01-15 20:03:29,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32982
[2025-01-15 20:03:29,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:03:29,935] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [2090/2809]  eta: 0:07:08  lr: 0.000043  min_lr: 0.000000  loss: 4.0157 (4.0900)  class_acc: 0.2083 (0.2310)  loss_scale: 65536.0000 (62965.9608)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1446  max mem: 15572
[2025-01-15 20:03:40,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=217, lr=[4.1381510514265254e-07, 4.1381510514265254e-07, 5.911644359180752e-07, 5.911644359180752e-07, 8.445206227401074e-07, 8.445206227401074e-07, 1.2064580324858679e-06, 1.2064580324858679e-06, 1.7235114749798113e-06, 1.7235114749798113e-06, 2.462159249971159e-06, 2.462159249971159e-06, 3.517370357101656e-06, 3.517370357101656e-06, 5.024814795859509e-06, 5.024814795859509e-06, 7.17830685122787e-06, 7.17830685122787e-06, 1.0254724073182673e-05, 1.0254724073182673e-05, 1.464960581883239e-05, 1.464960581883239e-05, 2.09280083126177e-05, 2.09280083126177e-05, 2.9897154732311005e-05, 2.9897154732311005e-05, 4.271022104615858e-05, 4.271022104615858e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:03:40,115] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=28.404329550301828, CurrSamplesPerSec=30.294501303710337, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [11]  [2100/2809]  eta: 0:07:02  lr: 0.000043  min_lr: 0.000000  loss: 3.9671 (4.0897)  class_acc: 0.2500 (0.2309)  loss_scale: 32768.0000 (62822.2294)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.0894  max mem: 15572
Epoch: [11]  [2110/2809]  eta: 0:06:55  lr: 0.000043  min_lr: 0.000000  loss: 4.1244 (4.0903)  class_acc: 0.1667 (0.2309)  loss_scale: 32768.0000 (62679.8598)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0490  max mem: 15572
Epoch: [11]  [2120/2809]  eta: 0:06:49  lr: 0.000043  min_lr: 0.000000  loss: 4.0357 (4.0898)  class_acc: 0.2083 (0.2310)  loss_scale: 32768.0000 (62538.8326)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0233  max mem: 15572
Epoch: [11]  [2130/2809]  eta: 0:06:43  lr: 0.000043  min_lr: 0.000000  loss: 3.9599 (4.0901)  class_acc: 0.2083 (0.2310)  loss_scale: 32768.0000 (62399.1290)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0664  max mem: 15572
Epoch: [11]  [2140/2809]  eta: 0:06:37  lr: 0.000043  min_lr: 0.000000  loss: 4.0904 (4.0900)  class_acc: 0.2500 (0.2310)  loss_scale: 32768.0000 (62260.7305)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.1002  max mem: 15572
Epoch: [11]  [2150/2809]  eta: 0:06:31  lr: 0.000043  min_lr: 0.000000  loss: 4.1661 (4.0905)  class_acc: 0.1667 (0.2310)  loss_scale: 32768.0000 (62123.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1872  max mem: 15572
Epoch: [11]  [2160/2809]  eta: 0:06:25  lr: 0.000043  min_lr: 0.000000  loss: 4.1664 (4.0910)  class_acc: 0.2500 (0.2313)  loss_scale: 32768.0000 (61987.7760)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1551  max mem: 15572
Epoch: [11]  [2170/2809]  eta: 0:06:19  lr: 0.000043  min_lr: 0.000000  loss: 4.1330 (4.0901)  class_acc: 0.2500 (0.2314)  loss_scale: 32768.0000 (61853.1847)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0922  max mem: 15572
Epoch: [11]  [2180/2809]  eta: 0:06:13  lr: 0.000043  min_lr: 0.000000  loss: 4.0815 (4.0902)  class_acc: 0.2083 (0.2314)  loss_scale: 32768.0000 (61719.8276)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.2030  max mem: 15572
Epoch: [11]  [2190/2809]  eta: 0:06:07  lr: 0.000043  min_lr: 0.000000  loss: 4.1800 (4.0903)  class_acc: 0.2083 (0.2312)  loss_scale: 32768.0000 (61587.6878)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1358  max mem: 15572
Epoch: [11]  [2200/2809]  eta: 0:06:01  lr: 0.000043  min_lr: 0.000000  loss: 4.2397 (4.0903)  class_acc: 0.2083 (0.2313)  loss_scale: 32768.0000 (61456.7488)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0481  max mem: 15572
Epoch: [11]  [2210/2809]  eta: 0:05:55  lr: 0.000043  min_lr: 0.000000  loss: 4.2681 (4.0910)  class_acc: 0.1667 (0.2310)  loss_scale: 32768.0000 (61326.9941)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1103  max mem: 15572
[2025-01-15 20:04:41,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:04:41,871] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [2220/2809]  eta: 0:05:49  lr: 0.000043  min_lr: 0.000000  loss: 4.3304 (4.0917)  class_acc: 0.1250 (0.2308)  loss_scale: 32768.0000 (61331.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.2038  max mem: 15572
Epoch: [11]  [2230/2809]  eta: 0:05:43  lr: 0.000043  min_lr: 0.000000  loss: 4.0997 (4.0912)  class_acc: 0.2083 (0.2311)  loss_scale: 65536.0000 (61350.0385)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.2073  max mem: 15572
Epoch: [11]  [2240/2809]  eta: 0:05:37  lr: 0.000043  min_lr: 0.000000  loss: 3.9241 (4.0897)  class_acc: 0.2500 (0.2314)  loss_scale: 65536.0000 (61368.7175)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1217  max mem: 15572
Epoch: [11]  [2250/2809]  eta: 0:05:31  lr: 0.000043  min_lr: 0.000000  loss: 3.7925 (4.0892)  class_acc: 0.2500 (0.2315)  loss_scale: 65536.0000 (61387.2306)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1101  max mem: 15572
Epoch: [11]  [2260/2809]  eta: 0:05:26  lr: 0.000043  min_lr: 0.000000  loss: 3.8084 (4.0886)  class_acc: 0.2917 (0.2318)  loss_scale: 65536.0000 (61405.5798)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1704  max mem: 15572
Epoch: [11]  [2270/2809]  eta: 0:05:19  lr: 0.000043  min_lr: 0.000000  loss: 3.9802 (4.0885)  class_acc: 0.2917 (0.2320)  loss_scale: 65536.0000 (61423.7675)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1474  max mem: 15572
Epoch: [11]  [2280/2809]  eta: 0:05:14  lr: 0.000043  min_lr: 0.000000  loss: 4.0689 (4.0880)  class_acc: 0.2500 (0.2322)  loss_scale: 65536.0000 (61441.7957)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1335  max mem: 15572
Epoch: [11]  [2290/2809]  eta: 0:05:08  lr: 0.000043  min_lr: 0.000000  loss: 4.1321 (4.0880)  class_acc: 0.2083 (0.2322)  loss_scale: 65536.0000 (61459.6665)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.2046  max mem: 15572
Epoch: [11]  [2300/2809]  eta: 0:05:02  lr: 0.000043  min_lr: 0.000000  loss: 4.0784 (4.0878)  class_acc: 0.2083 (0.2323)  loss_scale: 65536.0000 (61477.3820)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1509  max mem: 15572
Epoch: [11]  [2310/2809]  eta: 0:04:56  lr: 0.000043  min_lr: 0.000000  loss: 3.8383 (4.0877)  class_acc: 0.2500 (0.2323)  loss_scale: 65536.0000 (61494.9442)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1227  max mem: 15572
Epoch: [11]  [2320/2809]  eta: 0:04:50  lr: 0.000043  min_lr: 0.000000  loss: 3.8737 (4.0872)  class_acc: 0.2083 (0.2323)  loss_scale: 65536.0000 (61512.3550)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1422  max mem: 15572
Epoch: [11]  [2330/2809]  eta: 0:04:44  lr: 0.000043  min_lr: 0.000000  loss: 3.8593 (4.0868)  class_acc: 0.2083 (0.2323)  loss_scale: 65536.0000 (61529.6165)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1203  max mem: 15572
[2025-01-15 20:05:59,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:05:59,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2340/2809]  eta: 0:04:38  lr: 0.000043  min_lr: 0.000000  loss: 4.0208 (4.0870)  class_acc: 0.2083 (0.2321)  loss_scale: 65536.0000 (61574.7253)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.0890  max mem: 15572
[2025-01-15 20:05:59,565] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33240
[2025-01-15 20:05:59,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:05:59,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2350/2809]  eta: 0:04:32  lr: 0.000043  min_lr: 0.000000  loss: 4.1032 (4.0869)  class_acc: 0.1667 (0.2321)  loss_scale: 65536.0000 (61591.5746)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1006  max mem: 15572
Epoch: [11]  [2360/2809]  eta: 0:04:26  lr: 0.000043  min_lr: 0.000000  loss: 4.0647 (4.0870)  class_acc: 0.2083 (0.2321)  loss_scale: 65536.0000 (61608.2812)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1174  max mem: 15572
Epoch: [11]  [2370/2809]  eta: 0:04:20  lr: 0.000043  min_lr: 0.000000  loss: 4.1317 (4.0883)  class_acc: 0.2083 (0.2319)  loss_scale: 65536.0000 (61624.8469)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0860  max mem: 15572
Epoch: [11]  [2380/2809]  eta: 0:04:14  lr: 0.000043  min_lr: 0.000000  loss: 4.1876 (4.0882)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (61641.2734)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0981  max mem: 15572
Epoch: [11]  [2390/2809]  eta: 0:04:08  lr: 0.000043  min_lr: 0.000000  loss: 4.1022 (4.0884)  class_acc: 0.2083 (0.2317)  loss_scale: 65536.0000 (61657.5625)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0937  max mem: 15572
Epoch: [11]  [2400/2809]  eta: 0:04:02  lr: 0.000043  min_lr: 0.000000  loss: 4.1635 (4.0889)  class_acc: 0.1667 (0.2315)  loss_scale: 65536.0000 (61673.7160)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1391  max mem: 15572
Epoch: [11]  [2410/2809]  eta: 0:03:56  lr: 0.000043  min_lr: 0.000000  loss: 4.1519 (4.0891)  class_acc: 0.1667 (0.2314)  loss_scale: 65536.0000 (61689.7354)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1935  max mem: 15572
Epoch: [11]  [2420/2809]  eta: 0:03:50  lr: 0.000043  min_lr: 0.000000  loss: 4.0918 (4.0894)  class_acc: 0.1667 (0.2313)  loss_scale: 65536.0000 (61705.6225)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1252  max mem: 15572
Epoch: [11]  [2430/2809]  eta: 0:03:44  lr: 0.000043  min_lr: 0.000000  loss: 4.1347 (4.0897)  class_acc: 0.2083 (0.2313)  loss_scale: 65536.0000 (61721.3789)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1560  max mem: 15572
Epoch: [11]  [2440/2809]  eta: 0:03:38  lr: 0.000043  min_lr: 0.000000  loss: 4.1347 (4.0891)  class_acc: 0.2500 (0.2315)  loss_scale: 65536.0000 (61737.0061)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1573  max mem: 15572
Epoch: [11]  [2450/2809]  eta: 0:03:32  lr: 0.000043  min_lr: 0.000000  loss: 3.8952 (4.0887)  class_acc: 0.2917 (0.2318)  loss_scale: 65536.0000 (61752.5059)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1073  max mem: 15572
Epoch: [11]  [2460/2809]  eta: 0:03:26  lr: 0.000043  min_lr: 0.000000  loss: 3.8792 (4.0882)  class_acc: 0.2917 (0.2319)  loss_scale: 65536.0000 (61767.8797)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1286  max mem: 15572
[2025-01-15 20:07:15,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:07:15,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2470/2809]  eta: 0:03:21  lr: 0.000043  min_lr: 0.000000  loss: 3.9606 (4.0880)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (61809.6512)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.1618  max mem: 15572
[2025-01-15 20:07:15,974] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33371
[2025-01-15 20:07:15,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:07:15,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2480/2809]  eta: 0:03:15  lr: 0.000043  min_lr: 0.000000  loss: 4.0409 (4.0875)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (61851.0859)  weight_decay: 0.0500 (0.0500)  time: 0.6488  data: 0.1758  max mem: 15572
Epoch: [11]  [2490/2809]  eta: 0:03:09  lr: 0.000043  min_lr: 0.000000  loss: 4.1831 (4.0878)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (61865.8788)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1066  max mem: 15572
Epoch: [11]  [2500/2809]  eta: 0:03:03  lr: 0.000043  min_lr: 0.000000  loss: 4.1384 (4.0878)  class_acc: 0.2500 (0.2319)  loss_scale: 65536.0000 (61880.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1458  max mem: 15572
Epoch: [11]  [2510/2809]  eta: 0:02:57  lr: 0.000043  min_lr: 0.000000  loss: 4.0477 (4.0877)  class_acc: 0.2500 (0.2319)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.1954  max mem: 15572
Epoch: [11]  [2520/2809]  eta: 0:02:51  lr: 0.000043  min_lr: 0.000000  loss: 4.1175 (4.0877)  class_acc: 0.2500 (0.2319)  loss_scale: 65536.0000 (61909.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1489  max mem: 15572
Epoch: [11]  [2530/2809]  eta: 0:02:45  lr: 0.000043  min_lr: 0.000000  loss: 4.1175 (4.0881)  class_acc: 0.2083 (0.2320)  loss_scale: 65536.0000 (61923.8815)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1125  max mem: 15572
Epoch: [11]  [2540/2809]  eta: 0:02:39  lr: 0.000043  min_lr: 0.000000  loss: 4.1228 (4.0873)  class_acc: 0.2500 (0.2320)  loss_scale: 65536.0000 (61938.0968)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1378  max mem: 15572
Epoch: [11]  [2550/2809]  eta: 0:02:33  lr: 0.000043  min_lr: 0.000000  loss: 4.1160 (4.0873)  class_acc: 0.2083 (0.2321)  loss_scale: 65536.0000 (61952.2007)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.2129  max mem: 15572
Epoch: [11]  [2560/2809]  eta: 0:02:27  lr: 0.000043  min_lr: 0.000000  loss: 4.0537 (4.0869)  class_acc: 0.2500 (0.2322)  loss_scale: 65536.0000 (61966.1945)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.2158  max mem: 15572
Epoch: [11]  [2570/2809]  eta: 0:02:21  lr: 0.000043  min_lr: 0.000000  loss: 4.0537 (4.0875)  class_acc: 0.2500 (0.2321)  loss_scale: 65536.0000 (61980.0793)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1502  max mem: 15572
Epoch: [11]  [2580/2809]  eta: 0:02:16  lr: 0.000043  min_lr: 0.000000  loss: 4.1636 (4.0875)  class_acc: 0.2083 (0.2320)  loss_scale: 65536.0000 (61993.8566)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1632  max mem: 15572
Epoch: [11]  [2590/2809]  eta: 0:02:10  lr: 0.000042  min_lr: 0.000000  loss: 4.1385 (4.0880)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (62007.5276)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1422  max mem: 15572
Epoch: [11]  [2600/2809]  eta: 0:02:04  lr: 0.000042  min_lr: 0.000000  loss: 4.1487 (4.0882)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (62021.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0983  max mem: 15572
[2025-01-15 20:08:33,756] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:08:33,757] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [2610/2809]  eta: 0:01:58  lr: 0.000042  min_lr: 0.000000  loss: 4.1988 (4.0884)  class_acc: 0.2083 (0.2319)  loss_scale: 65536.0000 (62285.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1381  max mem: 15572
[2025-01-15 20:08:41,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33515
[2025-01-15 20:08:41,933] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:08:41,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2620/2809]  eta: 0:01:52  lr: 0.000042  min_lr: 0.000000  loss: 4.2602 (4.0890)  class_acc: 0.2083 (0.2318)  loss_scale: 131072.0000 (62422.9775)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.2188  max mem: 15572
Epoch: [11]  [2630/2809]  eta: 0:01:46  lr: 0.000042  min_lr: 0.000000  loss: 4.2602 (4.0897)  class_acc: 0.2083 (0.2318)  loss_scale: 65536.0000 (62434.8096)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.2342  max mem: 15572
Epoch: [11]  [2640/2809]  eta: 0:01:40  lr: 0.000042  min_lr: 0.000000  loss: 4.0535 (4.0891)  class_acc: 0.2500 (0.2321)  loss_scale: 65536.0000 (62446.5521)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1588  max mem: 15572
Epoch: [11]  [2650/2809]  eta: 0:01:34  lr: 0.000042  min_lr: 0.000000  loss: 4.1091 (4.0889)  class_acc: 0.2083 (0.2321)  loss_scale: 65536.0000 (62458.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1353  max mem: 15572
Epoch: [11]  [2660/2809]  eta: 0:01:28  lr: 0.000042  min_lr: 0.000000  loss: 4.1556 (4.0896)  class_acc: 0.2083 (0.2321)  loss_scale: 65536.0000 (62469.7723)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1057  max mem: 15572
Epoch: [11]  [2670/2809]  eta: 0:01:22  lr: 0.000042  min_lr: 0.000000  loss: 4.3012 (4.0893)  class_acc: 0.2500 (0.2323)  loss_scale: 65536.0000 (62481.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1001  max mem: 15572
Epoch: [11]  [2680/2809]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000000  loss: 4.1402 (4.0897)  class_acc: 0.2500 (0.2323)  loss_scale: 65536.0000 (62492.6460)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0967  max mem: 15572
Epoch: [11]  [2690/2809]  eta: 0:01:10  lr: 0.000042  min_lr: 0.000000  loss: 4.1049 (4.0896)  class_acc: 0.2083 (0.2322)  loss_scale: 65536.0000 (62503.9554)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0867  max mem: 15572
Epoch: [11]  [2700/2809]  eta: 0:01:04  lr: 0.000042  min_lr: 0.000000  loss: 4.0969 (4.0898)  class_acc: 0.2500 (0.2324)  loss_scale: 65536.0000 (62515.1810)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1630  max mem: 15572
Epoch: [11]  [2710/2809]  eta: 0:00:58  lr: 0.000042  min_lr: 0.000000  loss: 4.0793 (4.0895)  class_acc: 0.2500 (0.2325)  loss_scale: 65536.0000 (62526.3239)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2072  max mem: 15572
Epoch: [11]  [2720/2809]  eta: 0:00:52  lr: 0.000042  min_lr: 0.000000  loss: 4.0060 (4.0896)  class_acc: 0.2083 (0.2324)  loss_scale: 65536.0000 (62537.3848)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1588  max mem: 15572
Epoch: [11]  [2730/2809]  eta: 0:00:46  lr: 0.000042  min_lr: 0.000000  loss: 4.3063 (4.0905)  class_acc: 0.2083 (0.2322)  loss_scale: 65536.0000 (62548.3647)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1478  max mem: 15572
Epoch: [11]  [2740/2809]  eta: 0:00:40  lr: 0.000042  min_lr: 0.000000  loss: 3.8850 (4.0895)  class_acc: 0.2083 (0.2324)  loss_scale: 65536.0000 (62559.2645)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1677  max mem: 15572
[2025-01-15 20:10:00,023] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:10:00,023] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:10:01,006] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33646
[2025-01-15 20:10:01,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:10:01,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [2750/2809]  eta: 0:00:35  lr: 0.000042  min_lr: 0.000000  loss: 3.8054 (4.0889)  class_acc: 0.2500 (0.2325)  loss_scale: 65536.0000 (62617.7303)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1449  max mem: 15572
Epoch: [11]  [2760/2809]  eta: 0:00:29  lr: 0.000042  min_lr: 0.000000  loss: 3.9558 (4.0890)  class_acc: 0.2500 (0.2325)  loss_scale: 65536.0000 (62628.2999)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1432  max mem: 15572
Epoch: [11]  [2770/2809]  eta: 0:00:23  lr: 0.000042  min_lr: 0.000000  loss: 3.9558 (4.0878)  class_acc: 0.2500 (0.2328)  loss_scale: 65536.0000 (62638.7932)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1304  max mem: 15572
Epoch: [11]  [2780/2809]  eta: 0:00:17  lr: 0.000042  min_lr: 0.000000  loss: 4.0196 (4.0882)  class_acc: 0.2500 (0.2328)  loss_scale: 65536.0000 (62649.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1247  max mem: 15572
Epoch: [11]  [2790/2809]  eta: 0:00:11  lr: 0.000042  min_lr: 0.000000  loss: 3.8792 (4.0869)  class_acc: 0.2917 (0.2332)  loss_scale: 65536.0000 (62659.5543)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1163  max mem: 15572
Epoch: [11]  [2800/2809]  eta: 0:00:05  lr: 0.000042  min_lr: 0.000000  loss: 3.7486 (4.0875)  class_acc: 0.2500 (0.2330)  loss_scale: 65536.0000 (62669.8236)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0631  max mem: 15572
Epoch: [11]  [2808/2809]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000000  loss: 4.0211 (4.0873)  class_acc: 0.2083 (0.2330)  loss_scale: 65536.0000 (62677.9865)  weight_decay: 0.0500 (0.0500)  time: 0.4680  data: 0.0629  max mem: 15572
Epoch: [11] Total time: 0:27:46 (0.5933 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000000  loss: 4.0211 (4.0873)  class_acc: 0.2083 (0.2330)  loss_scale: 65536.0000 (62677.9865)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:14:35  loss: 0.4635 (0.4635)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.2200  data: 3.0253  max mem: 15572
Val:  [ 10/272]  eta: 0:02:57  loss: 3.6273 (3.1616)  acc1: 5.5556 (26.2626)  acc5: 33.3333 (43.9394)  time: 0.6790  data: 0.4719  max mem: 15572
Val:  [ 20/272]  eta: 0:01:58  loss: 3.2906 (3.1237)  acc1: 16.6667 (25.6614)  acc5: 50.0000 (50.7937)  time: 0.3330  data: 0.1334  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 3.2956 (3.2379)  acc1: 16.6667 (21.6846)  acc5: 55.5556 (52.3297)  time: 0.3062  data: 0.1178  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 3.0382 (3.1724)  acc1: 16.6667 (22.4932)  acc5: 61.1111 (56.7751)  time: 0.3528  data: 0.1634  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.8901 (3.0691)  acc1: 22.2222 (24.9455)  acc5: 72.2222 (60.6754)  time: 0.3255  data: 0.1266  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.9463 (2.9108)  acc1: 44.4444 (30.6011)  acc5: 83.3333 (63.2969)  time: 0.3025  data: 0.0926  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.8923 (2.8138)  acc1: 61.1111 (32.6291)  acc5: 77.7778 (65.4147)  time: 0.3341  data: 0.1246  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.5249 (2.8279)  acc1: 33.3333 (32.9904)  acc5: 72.2222 (65.1578)  time: 0.3500  data: 0.1428  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.0813 (2.8673)  acc1: 22.2222 (32.4176)  acc5: 66.6667 (64.5910)  time: 0.2953  data: 0.0873  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.1787 (2.9232)  acc1: 22.2222 (31.5732)  acc5: 61.1111 (63.7514)  time: 0.2911  data: 0.0916  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 3.3610 (2.9913)  acc1: 11.1111 (29.7297)  acc5: 50.0000 (61.9119)  time: 0.3701  data: 0.1688  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.5904 (3.0222)  acc1: 5.5556 (29.0634)  acc5: 44.4444 (61.2948)  time: 0.3822  data: 0.1611  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 3.0251 (2.9704)  acc1: 27.7778 (30.7040)  acc5: 66.6667 (62.1289)  time: 0.3282  data: 0.1123  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.4215 (2.9534)  acc1: 38.8889 (31.4421)  acc5: 72.2222 (62.0567)  time: 0.2916  data: 0.1012  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 3.0855 (2.9589)  acc1: 22.2222 (30.7579)  acc5: 66.6667 (62.5460)  time: 0.3275  data: 0.1334  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.8877 (2.9397)  acc1: 27.7778 (31.9186)  acc5: 77.7778 (63.4231)  time: 0.3709  data: 0.1585  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 3.0803 (2.9757)  acc1: 22.2222 (30.8967)  acc5: 66.6667 (62.6381)  time: 0.3457  data: 0.1280  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 3.0489 (2.9588)  acc1: 22.2222 (31.1848)  acc5: 66.6667 (63.4438)  time: 0.3523  data: 0.1455  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.9307 (2.9969)  acc1: 22.2222 (30.3083)  acc5: 61.1111 (62.0128)  time: 0.3448  data: 0.1443  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.9307 (3.0021)  acc1: 16.6667 (30.1824)  acc5: 50.0000 (61.9127)  time: 0.3014  data: 0.0934  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.7695 (3.0002)  acc1: 38.8889 (30.9900)  acc5: 72.2222 (62.1906)  time: 0.2765  data: 0.0650  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.7855 (2.9913)  acc1: 44.4444 (31.1966)  acc5: 66.6667 (62.4183)  time: 0.3248  data: 0.1154  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 2.2802 (2.9542)  acc1: 50.0000 (32.6840)  acc5: 72.2222 (63.0832)  time: 0.3638  data: 0.1628  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 2.1069 (2.9240)  acc1: 50.0000 (33.2411)  acc5: 83.3333 (64.0387)  time: 0.3293  data: 0.1361  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4353 (2.9388)  acc1: 38.8889 (32.9128)  acc5: 77.7778 (63.6565)  time: 0.2980  data: 0.0992  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.8525 (2.8608)  acc1: 77.7778 (35.1213)  acc5: 77.7778 (64.8148)  time: 0.2796  data: 0.0743  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.8525 (2.8629)  acc1: 61.1111 (34.9323)  acc5: 83.3333 (64.6166)  time: 0.2059  data: 0.0310  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8786 (2.8675)  acc1: 61.1111 (34.9375)  acc5: 83.3333 (64.5914)  time: 0.1928  data: 0.0309  max mem: 15572
Val: Total time: 0:01:30 (0.3316 s / it)
* Acc@1 34.938 Acc@5 64.591 loss 2.868
Accuracy of the network on the 4883 val videos: 34.9%
[2025-01-15 20:12:04,971] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 20:12:04,974] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 20:12:04,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 20:12:07,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 20:12:07,591] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.94%
Epoch: [12]  [   0/2809]  eta: 5:24:58  lr: 0.000042  min_lr: 0.000000  loss: 4.3043 (4.3043)  class_acc: 0.1667 (0.1667)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.9414  data: 6.3989  max mem: 15572
Epoch: [12]  [  10/2809]  eta: 0:59:02  lr: 0.000042  min_lr: 0.000000  loss: 4.0427 (3.9995)  class_acc: 0.2500 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2655  data: 0.8109  max mem: 15572
Epoch: [12]  [  20/2809]  eta: 0:46:48  lr: 0.000042  min_lr: 0.000000  loss: 3.9413 (3.9521)  class_acc: 0.2500 (0.2857)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.2574  max mem: 15572
Epoch: [12]  [  30/2809]  eta: 0:40:28  lr: 0.000042  min_lr: 0.000000  loss: 4.0578 (4.0099)  class_acc: 0.2083 (0.2594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.1989  max mem: 15572
Epoch: [12]  [  40/2809]  eta: 0:38:52  lr: 0.000042  min_lr: 0.000000  loss: 4.1502 (4.0165)  class_acc: 0.2083 (0.2581)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6697  data: 0.1866  max mem: 15572
Epoch: [12]  [  50/2809]  eta: 0:37:15  lr: 0.000042  min_lr: 0.000000  loss: 4.0744 (4.0107)  class_acc: 0.2083 (0.2565)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7121  data: 0.2301  max mem: 15572
Epoch: [12]  [  60/2809]  eta: 0:35:43  lr: 0.000042  min_lr: 0.000000  loss: 3.9833 (4.0038)  class_acc: 0.2500 (0.2630)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.1895  max mem: 15572
[2025-01-15 20:12:58,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:12:58,734] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:12:59,234] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33776
[2025-01-15 20:12:59,234] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:12:59,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [  70/2809]  eta: 0:34:58  lr: 0.000042  min_lr: 0.000000  loss: 4.0783 (4.0129)  class_acc: 0.2500 (0.2570)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6531  data: 0.1702  max mem: 15572
Epoch: [12]  [  80/2809]  eta: 0:33:57  lr: 0.000042  min_lr: 0.000000  loss: 3.9599 (4.0058)  class_acc: 0.2083 (0.2536)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1572  max mem: 15572
Epoch: [12]  [  90/2809]  eta: 0:33:07  lr: 0.000042  min_lr: 0.000000  loss: 3.9575 (4.0048)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1329  max mem: 15572
Epoch: [12]  [ 100/2809]  eta: 0:33:05  lr: 0.000042  min_lr: 0.000000  loss: 4.1709 (4.0362)  class_acc: 0.2083 (0.2446)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.2202  max mem: 15572
Epoch: [12]  [ 110/2809]  eta: 0:32:52  lr: 0.000042  min_lr: 0.000000  loss: 4.2877 (4.0502)  class_acc: 0.1667 (0.2399)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.2723  max mem: 15572
Epoch: [12]  [ 120/2809]  eta: 0:32:26  lr: 0.000042  min_lr: 0.000000  loss: 4.2140 (4.0437)  class_acc: 0.2083 (0.2410)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.2130  max mem: 15572
Epoch: [12]  [ 130/2809]  eta: 0:31:29  lr: 0.000042  min_lr: 0.000000  loss: 4.0987 (4.0486)  class_acc: 0.2083 (0.2389)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1230  max mem: 15572
Epoch: [12]  [ 140/2809]  eta: 0:30:30  lr: 0.000042  min_lr: 0.000000  loss: 4.1807 (4.0610)  class_acc: 0.1667 (0.2343)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.4560  data: 0.0429  max mem: 15572
Epoch: [12]  [ 150/2809]  eta: 0:29:39  lr: 0.000042  min_lr: 0.000000  loss: 4.2325 (4.0747)  class_acc: 0.1667 (0.2318)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.4316  data: 0.0136  max mem: 15572
Epoch: [12]  [ 160/2809]  eta: 0:28:56  lr: 0.000042  min_lr: 0.000000  loss: 4.2007 (4.0760)  class_acc: 0.2083 (0.2340)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.4408  data: 0.0007  max mem: 15572
Epoch: [12]  [ 170/2809]  eta: 0:28:25  lr: 0.000042  min_lr: 0.000000  loss: 4.1948 (4.0797)  class_acc: 0.2500 (0.2344)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.4748  data: 0.0217  max mem: 15572
Epoch: [12]  [ 180/2809]  eta: 0:28:20  lr: 0.000042  min_lr: 0.000000  loss: 4.2097 (4.0808)  class_acc: 0.2500 (0.2314)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1146  max mem: 15572
Epoch: [12]  [ 190/2809]  eta: 0:27:59  lr: 0.000042  min_lr: 0.000000  loss: 4.1307 (4.0790)  class_acc: 0.2083 (0.2317)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1536  max mem: 15572
[2025-01-15 20:14:14,256] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:14:14,257] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:14:14,712] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33906
[2025-01-15 20:14:14,712] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:14:14,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 200/2809]  eta: 0:27:46  lr: 0.000042  min_lr: 0.000000  loss: 4.0889 (4.0749)  class_acc: 0.2500 (0.2326)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1298  max mem: 15572
[2025-01-15 20:14:21,678] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33918
[2025-01-15 20:14:21,679] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:14:21,680] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 210/2809]  eta: 0:27:29  lr: 0.000042  min_lr: 0.000000  loss: 4.0510 (4.0682)  class_acc: 0.2500 (0.2340)  loss_scale: 65536.0000 (66001.8957)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1251  max mem: 15572
Epoch: [12]  [ 220/2809]  eta: 0:27:20  lr: 0.000042  min_lr: 0.000000  loss: 4.0396 (4.0687)  class_acc: 0.2083 (0.2342)  loss_scale: 32768.0000 (64498.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1429  max mem: 15572
Epoch: [12]  [ 230/2809]  eta: 0:27:07  lr: 0.000042  min_lr: 0.000000  loss: 4.0527 (4.0688)  class_acc: 0.2500 (0.2359)  loss_scale: 32768.0000 (63124.5022)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1477  max mem: 15572
Epoch: [12]  [ 240/2809]  eta: 0:26:55  lr: 0.000042  min_lr: 0.000000  loss: 4.0661 (4.0656)  class_acc: 0.2500 (0.2369)  loss_scale: 32768.0000 (61864.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1205  max mem: 15572
Epoch: [12]  [ 250/2809]  eta: 0:26:33  lr: 0.000042  min_lr: 0.000000  loss: 3.8338 (4.0540)  class_acc: 0.2083 (0.2372)  loss_scale: 32768.0000 (60705.6574)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0606  max mem: 15572
Epoch: [12]  [ 260/2809]  eta: 0:26:14  lr: 0.000042  min_lr: 0.000000  loss: 3.8168 (4.0505)  class_acc: 0.2500 (0.2377)  loss_scale: 32768.0000 (59635.2490)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0174  max mem: 15572
Epoch: [12]  [ 270/2809]  eta: 0:25:58  lr: 0.000042  min_lr: 0.000000  loss: 4.2448 (4.0605)  class_acc: 0.1667 (0.2352)  loss_scale: 32768.0000 (58643.8376)  weight_decay: 0.0500 (0.0500)  time: 0.5036  data: 0.0447  max mem: 15572
Epoch: [12]  [ 280/2809]  eta: 0:25:58  lr: 0.000042  min_lr: 0.000000  loss: 4.2435 (4.0591)  class_acc: 0.1667 (0.2358)  loss_scale: 32768.0000 (57722.9893)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1272  max mem: 15572
Epoch: [12]  [ 290/2809]  eta: 0:25:52  lr: 0.000042  min_lr: 0.000000  loss: 4.0518 (4.0609)  class_acc: 0.2083 (0.2358)  loss_scale: 32768.0000 (56865.4296)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1889  max mem: 15572
[2025-01-15 20:15:07,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=224, lr=[4.095936585352162e-07, 4.095936585352162e-07, 5.851337979074517e-07, 5.851337979074517e-07, 8.359054255820739e-07, 8.359054255820739e-07, 1.1941506079743914e-06, 1.1941506079743914e-06, 1.7059294399634163e-06, 1.7059294399634163e-06, 2.437042057090595e-06, 2.437042057090595e-06, 3.4814886529865642e-06, 3.4814886529865642e-06, 4.973555218552235e-06, 4.973555218552235e-06, 7.10507888364605e-06, 7.10507888364605e-06, 1.0150112690922931e-05, 1.0150112690922931e-05, 1.4500160987032758e-05, 1.4500160987032758e-05, 2.0714515695761083e-05, 2.0714515695761083e-05, 2.9592165279658694e-05, 2.9592165279658694e-05, 4.227452182808385e-05, 4.227452182808385e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:15:07,528] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=28.39761887032301, CurrSamplesPerSec=28.215226978546927, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [ 300/2809]  eta: 0:25:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0927 (4.0594)  class_acc: 0.2083 (0.2360)  loss_scale: 32768.0000 (56064.8505)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1000  max mem: 15572
Epoch: [12]  [ 310/2809]  eta: 0:25:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0668 (4.0582)  class_acc: 0.2500 (0.2378)  loss_scale: 32768.0000 (55315.7556)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1397  max mem: 15572
Epoch: [12]  [ 320/2809]  eta: 0:25:24  lr: 0.000042  min_lr: 0.000000  loss: 3.9044 (4.0529)  class_acc: 0.2500 (0.2392)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1807  max mem: 15572
Epoch: [12]  [ 330/2809]  eta: 0:25:16  lr: 0.000042  min_lr: 0.000000  loss: 4.0512 (4.0581)  class_acc: 0.2083 (0.2382)  loss_scale: 32768.0000 (53953.3535)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1335  max mem: 15572
[2025-01-15 20:15:34,997] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:15:34,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 340/2809]  eta: 0:25:03  lr: 0.000042  min_lr: 0.000000  loss: 4.0988 (4.0596)  class_acc: 0.1667 (0.2384)  loss_scale: 32768.0000 (53524.2698)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1120  max mem: 15572
Epoch: [12]  [ 350/2809]  eta: 0:25:00  lr: 0.000042  min_lr: 0.000000  loss: 3.9471 (4.0565)  class_acc: 0.2500 (0.2386)  loss_scale: 65536.0000 (53866.4843)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1203  max mem: 15572
Epoch: [12]  [ 360/2809]  eta: 0:24:48  lr: 0.000042  min_lr: 0.000000  loss: 4.0169 (4.0594)  class_acc: 0.2500 (0.2389)  loss_scale: 65536.0000 (54189.7396)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1158  max mem: 15572
Epoch: [12]  [ 370/2809]  eta: 0:24:43  lr: 0.000042  min_lr: 0.000000  loss: 4.0944 (4.0588)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (54495.5687)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1040  max mem: 15572
Epoch: [12]  [ 380/2809]  eta: 0:24:36  lr: 0.000042  min_lr: 0.000000  loss: 3.9222 (4.0550)  class_acc: 0.2083 (0.2399)  loss_scale: 65536.0000 (54785.3438)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1568  max mem: 15572
Epoch: [12]  [ 390/2809]  eta: 0:24:27  lr: 0.000042  min_lr: 0.000000  loss: 4.1127 (4.0574)  class_acc: 0.2500 (0.2398)  loss_scale: 65536.0000 (55060.2967)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1445  max mem: 15572
Epoch: [12]  [ 400/2809]  eta: 0:24:21  lr: 0.000042  min_lr: 0.000000  loss: 3.8756 (4.0531)  class_acc: 0.2917 (0.2412)  loss_scale: 65536.0000 (55321.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1424  max mem: 15572
Epoch: [12]  [ 410/2809]  eta: 0:24:11  lr: 0.000042  min_lr: 0.000000  loss: 4.1066 (4.0565)  class_acc: 0.2500 (0.2399)  loss_scale: 65536.0000 (55570.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1287  max mem: 15572
Epoch: [12]  [ 420/2809]  eta: 0:24:07  lr: 0.000042  min_lr: 0.000000  loss: 4.1408 (4.0585)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (55806.7838)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1511  max mem: 15572
Epoch: [12]  [ 430/2809]  eta: 0:24:00  lr: 0.000042  min_lr: 0.000000  loss: 4.0698 (4.0593)  class_acc: 0.2500 (0.2396)  loss_scale: 65536.0000 (56032.5197)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1888  max mem: 15572
Epoch: [12]  [ 440/2809]  eta: 0:23:53  lr: 0.000042  min_lr: 0.000000  loss: 3.9528 (4.0564)  class_acc: 0.2500 (0.2402)  loss_scale: 65536.0000 (56248.0181)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1655  max mem: 15572
Epoch: [12]  [ 450/2809]  eta: 0:23:46  lr: 0.000042  min_lr: 0.000000  loss: 4.0823 (4.0609)  class_acc: 0.2500 (0.2397)  loss_scale: 65536.0000 (56453.9601)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1372  max mem: 15572
Epoch: [12]  [ 460/2809]  eta: 0:23:35  lr: 0.000042  min_lr: 0.000000  loss: 4.1879 (4.0567)  class_acc: 0.2500 (0.2406)  loss_scale: 65536.0000 (56650.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1091  max mem: 15572
[2025-01-15 20:16:50,263] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:16:50,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 470/2809]  eta: 0:23:29  lr: 0.000042  min_lr: 0.000000  loss: 3.8180 (4.0504)  class_acc: 0.2500 (0.2414)  loss_scale: 65536.0000 (57396.1783)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1260  max mem: 15572
[2025-01-15 20:16:52,959] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34181
[2025-01-15 20:16:52,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:16:52,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 480/2809]  eta: 0:23:29  lr: 0.000042  min_lr: 0.000000  loss: 4.1962 (4.0549)  class_acc: 0.2500 (0.2408)  loss_scale: 65536.0000 (57837.9044)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.2197  max mem: 15572
Epoch: [12]  [ 490/2809]  eta: 0:23:15  lr: 0.000042  min_lr: 0.000000  loss: 4.1962 (4.0484)  class_acc: 0.2500 (0.2423)  loss_scale: 65536.0000 (57994.6884)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1412  max mem: 15572
Epoch: [12]  [ 500/2809]  eta: 0:23:07  lr: 0.000042  min_lr: 0.000000  loss: 3.8022 (4.0473)  class_acc: 0.2917 (0.2429)  loss_scale: 65536.0000 (58145.2136)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0385  max mem: 15572
Epoch: [12]  [ 510/2809]  eta: 0:23:00  lr: 0.000042  min_lr: 0.000000  loss: 3.9590 (4.0432)  class_acc: 0.2083 (0.2436)  loss_scale: 65536.0000 (58289.8474)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1145  max mem: 15572
Epoch: [12]  [ 520/2809]  eta: 0:22:55  lr: 0.000042  min_lr: 0.000000  loss: 3.8493 (4.0389)  class_acc: 0.2083 (0.2432)  loss_scale: 65536.0000 (58428.9290)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1556  max mem: 15572
Epoch: [12]  [ 530/2809]  eta: 0:22:45  lr: 0.000042  min_lr: 0.000000  loss: 3.9956 (4.0411)  class_acc: 0.2083 (0.2425)  loss_scale: 65536.0000 (58562.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1164  max mem: 15572
Epoch: [12]  [ 540/2809]  eta: 0:22:38  lr: 0.000042  min_lr: 0.000000  loss: 4.1083 (4.0410)  class_acc: 0.2083 (0.2425)  loss_scale: 65536.0000 (58691.6673)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1196  max mem: 15572
Epoch: [12]  [ 550/2809]  eta: 0:22:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0478 (4.0390)  class_acc: 0.2500 (0.2437)  loss_scale: 65536.0000 (58815.8838)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1656  max mem: 15572
Epoch: [12]  [ 560/2809]  eta: 0:22:25  lr: 0.000042  min_lr: 0.000000  loss: 3.9934 (4.0410)  class_acc: 0.2917 (0.2429)  loss_scale: 65536.0000 (58935.6720)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1237  max mem: 15572
Epoch: [12]  [ 570/2809]  eta: 0:22:20  lr: 0.000042  min_lr: 0.000000  loss: 4.1679 (4.0403)  class_acc: 0.2083 (0.2433)  loss_scale: 65536.0000 (59051.2644)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1265  max mem: 15572
Epoch: [12]  [ 580/2809]  eta: 0:22:11  lr: 0.000042  min_lr: 0.000000  loss: 4.0012 (4.0412)  class_acc: 0.2500 (0.2433)  loss_scale: 65536.0000 (59162.8778)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1124  max mem: 15572
Epoch: [12]  [ 590/2809]  eta: 0:22:01  lr: 0.000042  min_lr: 0.000000  loss: 4.0544 (4.0392)  class_acc: 0.2500 (0.2439)  loss_scale: 65536.0000 (59270.7140)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0580  max mem: 15572
Epoch: [12]  [ 600/2809]  eta: 0:21:53  lr: 0.000042  min_lr: 0.000000  loss: 4.0924 (4.0426)  class_acc: 0.2500 (0.2432)  loss_scale: 65536.0000 (59374.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0756  max mem: 15572
[2025-01-15 20:18:06,078] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:18:06,078] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:18:07,352] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34313
[2025-01-15 20:18:07,352] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:18:07,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 610/2809]  eta: 0:21:45  lr: 0.000042  min_lr: 0.000000  loss: 4.0924 (4.0420)  class_acc: 0.2083 (0.2434)  loss_scale: 65536.0000 (59797.5777)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.1069  max mem: 15572
Epoch: [12]  [ 620/2809]  eta: 0:21:36  lr: 0.000042  min_lr: 0.000000  loss: 4.2051 (4.0446)  class_acc: 0.2083 (0.2427)  loss_scale: 65536.0000 (59889.9839)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.1096  max mem: 15572
Epoch: [12]  [ 630/2809]  eta: 0:21:31  lr: 0.000042  min_lr: 0.000000  loss: 4.2051 (4.0446)  class_acc: 0.1667 (0.2425)  loss_scale: 65536.0000 (59979.4612)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1476  max mem: 15572
Epoch: [12]  [ 640/2809]  eta: 0:21:26  lr: 0.000042  min_lr: 0.000000  loss: 4.0517 (4.0432)  class_acc: 0.1667 (0.2419)  loss_scale: 65536.0000 (60066.1466)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1794  max mem: 15572
Epoch: [12]  [ 650/2809]  eta: 0:21:20  lr: 0.000042  min_lr: 0.000000  loss: 4.0517 (4.0459)  class_acc: 0.2083 (0.2414)  loss_scale: 65536.0000 (60150.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1618  max mem: 15572
Epoch: [12]  [ 660/2809]  eta: 0:21:13  lr: 0.000042  min_lr: 0.000000  loss: 4.1515 (4.0480)  class_acc: 0.2083 (0.2412)  loss_scale: 65536.0000 (60231.6490)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1301  max mem: 15572
Epoch: [12]  [ 670/2809]  eta: 0:21:06  lr: 0.000042  min_lr: 0.000000  loss: 4.1933 (4.0488)  class_acc: 0.2083 (0.2413)  loss_scale: 65536.0000 (60310.7004)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1138  max mem: 15572
Epoch: [12]  [ 680/2809]  eta: 0:20:59  lr: 0.000042  min_lr: 0.000000  loss: 4.0158 (4.0490)  class_acc: 0.2500 (0.2411)  loss_scale: 65536.0000 (60387.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1292  max mem: 15572
Epoch: [12]  [ 690/2809]  eta: 0:20:53  lr: 0.000042  min_lr: 0.000000  loss: 4.0158 (4.0462)  class_acc: 0.2500 (0.2420)  loss_scale: 65536.0000 (60461.9392)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1250  max mem: 15572
Epoch: [12]  [ 700/2809]  eta: 0:20:47  lr: 0.000042  min_lr: 0.000000  loss: 4.0331 (4.0470)  class_acc: 0.2500 (0.2420)  loss_scale: 65536.0000 (60534.3224)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1316  max mem: 15572
Epoch: [12]  [ 710/2809]  eta: 0:20:40  lr: 0.000042  min_lr: 0.000000  loss: 4.0360 (4.0444)  class_acc: 0.2917 (0.2429)  loss_scale: 65536.0000 (60604.6695)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1147  max mem: 15572
Epoch: [12]  [ 720/2809]  eta: 0:20:32  lr: 0.000042  min_lr: 0.000000  loss: 4.0551 (4.0439)  class_acc: 0.2500 (0.2427)  loss_scale: 65536.0000 (60673.0652)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0932  max mem: 15572
Epoch: [12]  [ 730/2809]  eta: 0:20:28  lr: 0.000042  min_lr: 0.000000  loss: 4.1241 (4.0462)  class_acc: 0.2083 (0.2420)  loss_scale: 65536.0000 (60739.5896)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1550  max mem: 15572
[2025-01-15 20:19:21,707] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:19:21,708] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 740/2809]  eta: 0:20:19  lr: 0.000042  min_lr: 0.000000  loss: 4.1794 (4.0473)  class_acc: 0.1667 (0.2417)  loss_scale: 65536.0000 (61423.4170)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1247  max mem: 15572
[2025-01-15 20:19:25,150] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34449
[2025-01-15 20:19:25,150] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:19:25,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 750/2809]  eta: 0:20:13  lr: 0.000042  min_lr: 0.000000  loss: 4.0021 (4.0475)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (61478.1784)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0985  max mem: 15572
Epoch: [12]  [ 760/2809]  eta: 0:20:07  lr: 0.000042  min_lr: 0.000000  loss: 3.9578 (4.0451)  class_acc: 0.2917 (0.2429)  loss_scale: 65536.0000 (61531.5007)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1207  max mem: 15572
Epoch: [12]  [ 770/2809]  eta: 0:20:00  lr: 0.000042  min_lr: 0.000000  loss: 3.8493 (4.0431)  class_acc: 0.2917 (0.2438)  loss_scale: 65536.0000 (61583.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0942  max mem: 15572
Epoch: [12]  [ 780/2809]  eta: 0:19:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0665 (4.0450)  class_acc: 0.2083 (0.2436)  loss_scale: 65536.0000 (61634.0487)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1348  max mem: 15572
Epoch: [12]  [ 790/2809]  eta: 0:19:50  lr: 0.000042  min_lr: 0.000000  loss: 4.2120 (4.0471)  class_acc: 0.2083 (0.2432)  loss_scale: 65536.0000 (61683.3780)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1468  max mem: 15572
Epoch: [12]  [ 800/2809]  eta: 0:19:44  lr: 0.000042  min_lr: 0.000000  loss: 4.1057 (4.0457)  class_acc: 0.2083 (0.2434)  loss_scale: 65536.0000 (61731.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1354  max mem: 15572
Epoch: [12]  [ 810/2809]  eta: 0:19:38  lr: 0.000042  min_lr: 0.000000  loss: 4.1057 (4.0476)  class_acc: 0.2500 (0.2431)  loss_scale: 65536.0000 (61778.3872)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1380  max mem: 15572
Epoch: [12]  [ 820/2809]  eta: 0:19:31  lr: 0.000042  min_lr: 0.000000  loss: 3.9984 (4.0444)  class_acc: 0.2500 (0.2433)  loss_scale: 65536.0000 (61824.1559)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1279  max mem: 15572
Epoch: [12]  [ 830/2809]  eta: 0:19:30  lr: 0.000042  min_lr: 0.000000  loss: 3.9050 (4.0461)  class_acc: 0.2500 (0.2429)  loss_scale: 65536.0000 (61868.8231)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.2160  max mem: 15572
[2025-01-15 20:20:21,305] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34543
[2025-01-15 20:20:21,305] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:20:21,305] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 840/2809]  eta: 0:19:20  lr: 0.000042  min_lr: 0.000000  loss: 4.0562 (4.0444)  class_acc: 0.2083 (0.2431)  loss_scale: 65536.0000 (61678.6492)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1580  max mem: 15572
Epoch: [12]  [ 850/2809]  eta: 0:19:11  lr: 0.000042  min_lr: 0.000000  loss: 4.0562 (4.0460)  class_acc: 0.2083 (0.2422)  loss_scale: 32768.0000 (61338.9236)  weight_decay: 0.0500 (0.0500)  time: 0.4471  data: 0.0006  max mem: 15572
Epoch: [12]  [ 860/2809]  eta: 0:19:06  lr: 0.000042  min_lr: 0.000000  loss: 3.9458 (4.0444)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (61007.0894)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0785  max mem: 15572
Epoch: [12]  [ 870/2809]  eta: 0:19:00  lr: 0.000042  min_lr: 0.000000  loss: 3.8900 (4.0452)  class_acc: 0.2500 (0.2430)  loss_scale: 32768.0000 (60682.8749)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1465  max mem: 15572
Epoch: [12]  [ 880/2809]  eta: 0:18:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0358 (4.0443)  class_acc: 0.1667 (0.2430)  loss_scale: 32768.0000 (60366.0204)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1836  max mem: 15572
Epoch: [12]  [ 890/2809]  eta: 0:18:48  lr: 0.000042  min_lr: 0.000000  loss: 4.1376 (4.0433)  class_acc: 0.2083 (0.2431)  loss_scale: 32768.0000 (60056.2783)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1273  max mem: 15572
Epoch: [12]  [ 900/2809]  eta: 0:18:42  lr: 0.000042  min_lr: 0.000000  loss: 4.0713 (4.0414)  class_acc: 0.2500 (0.2434)  loss_scale: 32768.0000 (59753.4118)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0650  max mem: 15572
Epoch: [12]  [ 910/2809]  eta: 0:18:33  lr: 0.000042  min_lr: 0.000000  loss: 3.9532 (4.0400)  class_acc: 0.2500 (0.2437)  loss_scale: 32768.0000 (59457.1943)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0531  max mem: 15572
Epoch: [12]  [ 920/2809]  eta: 0:18:25  lr: 0.000042  min_lr: 0.000000  loss: 3.9507 (4.0384)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (59167.4093)  weight_decay: 0.0500 (0.0500)  time: 0.4803  data: 0.0005  max mem: 15572
Epoch: [12]  [ 930/2809]  eta: 0:18:17  lr: 0.000042  min_lr: 0.000000  loss: 4.0255 (4.0385)  class_acc: 0.2500 (0.2436)  loss_scale: 32768.0000 (58883.8496)  weight_decay: 0.0500 (0.0500)  time: 0.4719  data: 0.0006  max mem: 15572
Epoch: [12]  [ 940/2809]  eta: 0:18:10  lr: 0.000042  min_lr: 0.000000  loss: 3.9660 (4.0374)  class_acc: 0.2500 (0.2438)  loss_scale: 32768.0000 (58606.3167)  weight_decay: 0.0500 (0.0500)  time: 0.4805  data: 0.0007  max mem: 15572
Epoch: [12]  [ 950/2809]  eta: 0:18:04  lr: 0.000042  min_lr: 0.000000  loss: 3.8446 (4.0349)  class_acc: 0.2500 (0.2442)  loss_scale: 32768.0000 (58334.6204)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0564  max mem: 15572
Epoch: [12]  [ 960/2809]  eta: 0:17:57  lr: 0.000042  min_lr: 0.000000  loss: 4.0515 (4.0367)  class_acc: 0.2500 (0.2437)  loss_scale: 32768.0000 (58068.5786)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1072  max mem: 15572
[2025-01-15 20:21:29,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:21:29,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [ 970/2809]  eta: 0:17:49  lr: 0.000042  min_lr: 0.000000  loss: 4.0515 (4.0350)  class_acc: 0.2500 (0.2439)  loss_scale: 32768.0000 (58044.2430)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0515  max mem: 15572
Epoch: [12]  [ 980/2809]  eta: 0:17:43  lr: 0.000042  min_lr: 0.000000  loss: 3.9403 (4.0363)  class_acc: 0.2500 (0.2438)  loss_scale: 65536.0000 (58120.6116)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0631  max mem: 15572
Epoch: [12]  [ 990/2809]  eta: 0:17:38  lr: 0.000042  min_lr: 0.000000  loss: 4.1718 (4.0377)  class_acc: 0.2083 (0.2437)  loss_scale: 65536.0000 (58195.4390)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1388  max mem: 15572
Epoch: [12]  [1000/2809]  eta: 0:17:33  lr: 0.000042  min_lr: 0.000000  loss: 4.1867 (4.0409)  class_acc: 0.2083 (0.2431)  loss_scale: 65536.0000 (58268.7712)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1724  max mem: 15572
Epoch: [12]  [1010/2809]  eta: 0:17:27  lr: 0.000042  min_lr: 0.000000  loss: 4.1777 (4.0424)  class_acc: 0.2083 (0.2429)  loss_scale: 65536.0000 (58340.6528)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1535  max mem: 15572
Epoch: [12]  [1020/2809]  eta: 0:17:20  lr: 0.000042  min_lr: 0.000000  loss: 4.1167 (4.0426)  class_acc: 0.2083 (0.2427)  loss_scale: 65536.0000 (58411.1263)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1192  max mem: 15572
Epoch: [12]  [1030/2809]  eta: 0:17:14  lr: 0.000042  min_lr: 0.000000  loss: 4.0260 (4.0413)  class_acc: 0.2083 (0.2426)  loss_scale: 65536.0000 (58480.2328)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1146  max mem: 15572
Epoch: [12]  [1040/2809]  eta: 0:17:08  lr: 0.000042  min_lr: 0.000000  loss: 4.0414 (4.0424)  class_acc: 0.2083 (0.2426)  loss_scale: 65536.0000 (58548.0115)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0907  max mem: 15572
Epoch: [12]  [1050/2809]  eta: 0:17:01  lr: 0.000042  min_lr: 0.000000  loss: 4.1327 (4.0431)  class_acc: 0.2500 (0.2425)  loss_scale: 65536.0000 (58614.5005)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0846  max mem: 15572
Epoch: [12]  [1060/2809]  eta: 0:16:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0651 (4.0433)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (58679.7361)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1105  max mem: 15572
Epoch: [12]  [1070/2809]  eta: 0:16:51  lr: 0.000042  min_lr: 0.000000  loss: 4.0657 (4.0443)  class_acc: 0.2083 (0.2419)  loss_scale: 65536.0000 (58743.7535)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.1340  max mem: 15572
Epoch: [12]  [1080/2809]  eta: 0:16:44  lr: 0.000042  min_lr: 0.000000  loss: 4.2541 (4.0447)  class_acc: 0.1667 (0.2419)  loss_scale: 65536.0000 (58806.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1303  max mem: 15572
Epoch: [12]  [1090/2809]  eta: 0:16:38  lr: 0.000042  min_lr: 0.000000  loss: 4.0393 (4.0438)  class_acc: 0.2917 (0.2425)  loss_scale: 65536.0000 (58868.2676)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1193  max mem: 15572
[2025-01-15 20:22:42,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:22:42,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:22:46,418] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34806
[2025-01-15 20:22:46,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:22:46,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1100/2809]  eta: 0:16:32  lr: 0.000042  min_lr: 0.000000  loss: 3.9057 (4.0430)  class_acc: 0.2917 (0.2427)  loss_scale: 65536.0000 (59285.9728)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1241  max mem: 15572
Epoch: [12]  [1110/2809]  eta: 0:16:25  lr: 0.000042  min_lr: 0.000000  loss: 4.0011 (4.0435)  class_acc: 0.2500 (0.2426)  loss_scale: 65536.0000 (59342.2286)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.1012  max mem: 15572
Epoch: [12]  [1120/2809]  eta: 0:16:18  lr: 0.000042  min_lr: 0.000000  loss: 4.0806 (4.0428)  class_acc: 0.2083 (0.2427)  loss_scale: 65536.0000 (59397.4808)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0917  max mem: 15572
Epoch: [12]  [1130/2809]  eta: 0:16:13  lr: 0.000042  min_lr: 0.000000  loss: 4.1641 (4.0435)  class_acc: 0.2083 (0.2425)  loss_scale: 65536.0000 (59451.7560)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1588  max mem: 15572
Epoch: [12]  [1140/2809]  eta: 0:16:07  lr: 0.000042  min_lr: 0.000000  loss: 4.1309 (4.0433)  class_acc: 0.2500 (0.2430)  loss_scale: 65536.0000 (59505.0798)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1465  max mem: 15572
Epoch: [12]  [1150/2809]  eta: 0:16:01  lr: 0.000042  min_lr: 0.000000  loss: 4.1022 (4.0440)  class_acc: 0.2500 (0.2428)  loss_scale: 65536.0000 (59557.4770)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1057  max mem: 15572
Epoch: [12]  [1160/2809]  eta: 0:15:54  lr: 0.000042  min_lr: 0.000000  loss: 4.1022 (4.0446)  class_acc: 0.2083 (0.2426)  loss_scale: 65536.0000 (59608.9716)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1067  max mem: 15572
Epoch: [12]  [1170/2809]  eta: 0:15:47  lr: 0.000042  min_lr: 0.000000  loss: 4.1255 (4.0456)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (59659.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0671  max mem: 15572
Epoch: [12]  [1180/2809]  eta: 0:15:42  lr: 0.000042  min_lr: 0.000000  loss: 4.1053 (4.0442)  class_acc: 0.2500 (0.2423)  loss_scale: 65536.0000 (59709.3446)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1299  max mem: 15572
Epoch: [12]  [1190/2809]  eta: 0:15:36  lr: 0.000042  min_lr: 0.000000  loss: 4.1230 (4.0453)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (59758.2670)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.1574  max mem: 15572
Epoch: [12]  [1200/2809]  eta: 0:15:30  lr: 0.000042  min_lr: 0.000000  loss: 4.0284 (4.0444)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (59806.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1107  max mem: 15572
Epoch: [12]  [1210/2809]  eta: 0:15:26  lr: 0.000042  min_lr: 0.000000  loss: 3.9448 (4.0445)  class_acc: 0.1667 (0.2419)  loss_scale: 65536.0000 (59853.6879)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1667  max mem: 15572
Epoch: [12]  [1220/2809]  eta: 0:15:19  lr: 0.000042  min_lr: 0.000000  loss: 3.9778 (4.0440)  class_acc: 0.2083 (0.2423)  loss_scale: 65536.0000 (59900.2260)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1522  max mem: 15572
[2025-01-15 20:23:58,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:23:58,422] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [1230/2809]  eta: 0:15:13  lr: 0.000042  min_lr: 0.000000  loss: 4.0954 (4.0443)  class_acc: 0.2083 (0.2420)  loss_scale: 65536.0000 (60158.9602)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0863  max mem: 15572
[2025-01-15 20:24:02,085] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34941
[2025-01-15 20:24:02,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:24:02,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1240/2809]  eta: 0:15:08  lr: 0.000042  min_lr: 0.000000  loss: 4.0772 (4.0456)  class_acc: 0.2083 (0.2419)  loss_scale: 65536.0000 (60307.9065)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1572  max mem: 15572
Epoch: [12]  [1250/2809]  eta: 0:15:03  lr: 0.000042  min_lr: 0.000000  loss: 4.2089 (4.0473)  class_acc: 0.2083 (0.2412)  loss_scale: 65536.0000 (60349.6978)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.2032  max mem: 15572
Epoch: [12]  [1260/2809]  eta: 0:14:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0889 (4.0466)  class_acc: 0.2083 (0.2414)  loss_scale: 65536.0000 (60390.8263)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1065  max mem: 15572
Epoch: [12]  [1270/2809]  eta: 0:14:50  lr: 0.000042  min_lr: 0.000000  loss: 3.9568 (4.0451)  class_acc: 0.2917 (0.2419)  loss_scale: 65536.0000 (60431.3076)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.1028  max mem: 15572
Epoch: [12]  [1280/2809]  eta: 0:14:45  lr: 0.000042  min_lr: 0.000000  loss: 4.0769 (4.0469)  class_acc: 0.1667 (0.2413)  loss_scale: 65536.0000 (60471.1569)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1737  max mem: 15572
Epoch: [12]  [1290/2809]  eta: 0:14:38  lr: 0.000042  min_lr: 0.000000  loss: 4.3072 (4.0487)  class_acc: 0.1250 (0.2408)  loss_scale: 65536.0000 (60510.3888)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1188  max mem: 15572
[2025-01-15 20:24:35,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=230, lr=[4.051861021080781e-07, 4.051861021080781e-07, 5.788372887258259e-07, 5.788372887258259e-07, 8.269104124654657e-07, 8.269104124654657e-07, 1.1813005892363797e-06, 1.1813005892363797e-06, 1.6875722703376853e-06, 1.6875722703376853e-06, 2.4108175290538364e-06, 2.4108175290538364e-06, 3.4440250415054805e-06, 3.4440250415054805e-06, 4.920035773579258e-06, 4.920035773579258e-06, 7.028622533684655e-06, 7.028622533684655e-06, 1.0040889333835223e-05, 1.0040889333835223e-05, 1.4344127619764603e-05, 1.4344127619764603e-05, 2.0491610885378006e-05, 2.0491610885378006e-05, 2.9273729836254297e-05, 2.9273729836254297e-05, 4.181961405179186e-05, 4.181961405179186e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:24:35,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=28.399141254468365, CurrSamplesPerSec=24.592763426632608, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [12]  [1300/2809]  eta: 0:14:31  lr: 0.000042  min_lr: 0.000000  loss: 4.0935 (4.0473)  class_acc: 0.2500 (0.2414)  loss_scale: 65536.0000 (60549.0177)  weight_decay: 0.0500 (0.0500)  time: 0.4751  data: 0.0281  max mem: 15572
Epoch: [12]  [1310/2809]  eta: 0:14:25  lr: 0.000042  min_lr: 0.000000  loss: 3.7476 (4.0465)  class_acc: 0.2917 (0.2415)  loss_scale: 65536.0000 (60587.0572)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0493  max mem: 15572
Epoch: [12]  [1320/2809]  eta: 0:14:19  lr: 0.000042  min_lr: 0.000000  loss: 4.1248 (4.0474)  class_acc: 0.2083 (0.2412)  loss_scale: 65536.0000 (60624.5208)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1324  max mem: 15572
Epoch: [12]  [1330/2809]  eta: 0:14:15  lr: 0.000042  min_lr: 0.000000  loss: 4.2002 (4.0478)  class_acc: 0.1667 (0.2409)  loss_scale: 65536.0000 (60661.4215)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.2258  max mem: 15572
Epoch: [12]  [1340/2809]  eta: 0:14:08  lr: 0.000042  min_lr: 0.000000  loss: 3.9239 (4.0461)  class_acc: 0.2083 (0.2408)  loss_scale: 65536.0000 (60697.7718)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1705  max mem: 15572
Epoch: [12]  [1350/2809]  eta: 0:14:02  lr: 0.000042  min_lr: 0.000000  loss: 3.9489 (4.0465)  class_acc: 0.2500 (0.2409)  loss_scale: 65536.0000 (60733.5840)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0519  max mem: 15572
Epoch: [12]  [1360/2809]  eta: 0:13:56  lr: 0.000042  min_lr: 0.000000  loss: 4.0566 (4.0463)  class_acc: 0.2500 (0.2410)  loss_scale: 65536.0000 (60768.8699)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0953  max mem: 15572
[2025-01-15 20:25:14,488] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:25:14,488] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:25:16,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35074
[2025-01-15 20:25:16,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:25:16,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1370/2809]  eta: 0:13:50  lr: 0.000042  min_lr: 0.000000  loss: 4.0087 (4.0457)  class_acc: 0.2083 (0.2407)  loss_scale: 65536.0000 (60994.8476)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1511  max mem: 15572
Epoch: [12]  [1380/2809]  eta: 0:13:45  lr: 0.000042  min_lr: 0.000000  loss: 4.0720 (4.0466)  class_acc: 0.2083 (0.2407)  loss_scale: 65536.0000 (61027.7306)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1717  max mem: 15572
Epoch: [12]  [1390/2809]  eta: 0:13:38  lr: 0.000042  min_lr: 0.000000  loss: 4.0720 (4.0467)  class_acc: 0.2500 (0.2407)  loss_scale: 65536.0000 (61060.1409)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1146  max mem: 15572
Epoch: [12]  [1400/2809]  eta: 0:13:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0236 (4.0465)  class_acc: 0.2500 (0.2407)  loss_scale: 65536.0000 (61092.0885)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1090  max mem: 15572
Epoch: [12]  [1410/2809]  eta: 0:13:28  lr: 0.000042  min_lr: 0.000000  loss: 4.1503 (4.0475)  class_acc: 0.2083 (0.2406)  loss_scale: 65536.0000 (61123.5833)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.2026  max mem: 15572
Epoch: [12]  [1420/2809]  eta: 0:13:22  lr: 0.000042  min_lr: 0.000000  loss: 4.1676 (4.0478)  class_acc: 0.2083 (0.2404)  loss_scale: 65536.0000 (61154.6348)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1645  max mem: 15572
Epoch: [12]  [1430/2809]  eta: 0:13:16  lr: 0.000042  min_lr: 0.000000  loss: 4.2778 (4.0491)  class_acc: 0.2083 (0.2404)  loss_scale: 65536.0000 (61185.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1083  max mem: 15572
Epoch: [12]  [1440/2809]  eta: 0:13:09  lr: 0.000042  min_lr: 0.000000  loss: 4.3040 (4.0499)  class_acc: 0.2083 (0.2400)  loss_scale: 65536.0000 (61215.4448)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0862  max mem: 15572
Epoch: [12]  [1450/2809]  eta: 0:13:03  lr: 0.000042  min_lr: 0.000000  loss: 4.2119 (4.0495)  class_acc: 0.2083 (0.2399)  loss_scale: 65536.0000 (61245.2212)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0889  max mem: 15572
Epoch: [12]  [1460/2809]  eta: 0:12:58  lr: 0.000042  min_lr: 0.000000  loss: 4.1178 (4.0503)  class_acc: 0.2083 (0.2398)  loss_scale: 65536.0000 (61274.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1467  max mem: 15572
Epoch: [12]  [1470/2809]  eta: 0:12:53  lr: 0.000042  min_lr: 0.000000  loss: 4.0756 (4.0502)  class_acc: 0.2500 (0.2397)  loss_scale: 65536.0000 (61303.5595)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.2016  max mem: 15572
Epoch: [12]  [1480/2809]  eta: 0:12:47  lr: 0.000042  min_lr: 0.000000  loss: 4.1908 (4.0517)  class_acc: 0.1667 (0.2392)  loss_scale: 65536.0000 (61332.1377)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.2195  max mem: 15572
Epoch: [12]  [1490/2809]  eta: 0:12:41  lr: 0.000042  min_lr: 0.000000  loss: 4.2342 (4.0530)  class_acc: 0.1667 (0.2391)  loss_scale: 65536.0000 (61360.3327)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1447  max mem: 15572
[2025-01-15 20:26:31,579] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:26:31,579] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:26:34,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35207
[2025-01-15 20:26:34,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:26:34,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1500/2809]  eta: 0:12:36  lr: 0.000042  min_lr: 0.000000  loss: 4.2342 (4.0536)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (61562.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1085  max mem: 15572
Epoch: [12]  [1510/2809]  eta: 0:12:30  lr: 0.000042  min_lr: 0.000000  loss: 4.1695 (4.0547)  class_acc: 0.2083 (0.2388)  loss_scale: 65536.0000 (61589.0933)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1397  max mem: 15572
Epoch: [12]  [1520/2809]  eta: 0:12:24  lr: 0.000042  min_lr: 0.000000  loss: 4.1090 (4.0543)  class_acc: 0.2083 (0.2391)  loss_scale: 65536.0000 (61615.0427)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1073  max mem: 15572
Epoch: [12]  [1530/2809]  eta: 0:12:17  lr: 0.000042  min_lr: 0.000000  loss: 4.0051 (4.0534)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (61640.6532)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0733  max mem: 15572
Epoch: [12]  [1540/2809]  eta: 0:12:12  lr: 0.000042  min_lr: 0.000000  loss: 4.0963 (4.0547)  class_acc: 0.2083 (0.2389)  loss_scale: 65536.0000 (61665.9312)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0976  max mem: 15572
Epoch: [12]  [1550/2809]  eta: 0:12:05  lr: 0.000042  min_lr: 0.000000  loss: 4.2074 (4.0551)  class_acc: 0.2083 (0.2391)  loss_scale: 65536.0000 (61690.8833)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0841  max mem: 15572
Epoch: [12]  [1560/2809]  eta: 0:11:59  lr: 0.000042  min_lr: 0.000000  loss: 4.1111 (4.0556)  class_acc: 0.2083 (0.2391)  loss_scale: 65536.0000 (61715.5157)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.0926  max mem: 15572
Epoch: [12]  [1570/2809]  eta: 0:11:54  lr: 0.000042  min_lr: 0.000000  loss: 4.0120 (4.0544)  class_acc: 0.2083 (0.2392)  loss_scale: 65536.0000 (61739.8345)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1343  max mem: 15572
Epoch: [12]  [1580/2809]  eta: 0:11:48  lr: 0.000042  min_lr: 0.000000  loss: 3.9581 (4.0544)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (61763.8457)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1561  max mem: 15572
Epoch: [12]  [1590/2809]  eta: 0:11:43  lr: 0.000042  min_lr: 0.000000  loss: 3.9691 (4.0544)  class_acc: 0.2083 (0.2390)  loss_scale: 65536.0000 (61787.5550)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1544  max mem: 15572
Epoch: [12]  [1600/2809]  eta: 0:11:37  lr: 0.000042  min_lr: 0.000000  loss: 3.9691 (4.0548)  class_acc: 0.2500 (0.2387)  loss_scale: 65536.0000 (61810.9681)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1458  max mem: 15572
Epoch: [12]  [1610/2809]  eta: 0:11:31  lr: 0.000042  min_lr: 0.000000  loss: 3.8919 (4.0536)  class_acc: 0.2500 (0.2392)  loss_scale: 65536.0000 (61834.0906)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1447  max mem: 15572
Epoch: [12]  [1620/2809]  eta: 0:11:25  lr: 0.000042  min_lr: 0.000000  loss: 4.0877 (4.0544)  class_acc: 0.2500 (0.2390)  loss_scale: 65536.0000 (61856.9278)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0860  max mem: 15572
[2025-01-15 20:27:47,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:27:47,722] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:27:48,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35338
[2025-01-15 20:27:48,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:27:48,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1630/2809]  eta: 0:11:19  lr: 0.000042  min_lr: 0.000000  loss: 4.1342 (4.0534)  class_acc: 0.2917 (0.2394)  loss_scale: 65536.0000 (61959.8479)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1031  max mem: 15572
Epoch: [12]  [1640/2809]  eta: 0:11:13  lr: 0.000042  min_lr: 0.000000  loss: 3.9401 (4.0527)  class_acc: 0.2917 (0.2396)  loss_scale: 65536.0000 (61981.6405)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1301  max mem: 15572
Epoch: [12]  [1650/2809]  eta: 0:11:08  lr: 0.000042  min_lr: 0.000000  loss: 3.9401 (4.0519)  class_acc: 0.2917 (0.2397)  loss_scale: 65536.0000 (62003.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1112  max mem: 15572
Epoch: [12]  [1660/2809]  eta: 0:11:02  lr: 0.000042  min_lr: 0.000000  loss: 3.9568 (4.0521)  class_acc: 0.2500 (0.2398)  loss_scale: 65536.0000 (62024.4383)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1243  max mem: 15572
Epoch: [12]  [1670/2809]  eta: 0:10:56  lr: 0.000042  min_lr: 0.000000  loss: 3.9568 (4.0514)  class_acc: 0.2500 (0.2398)  loss_scale: 65536.0000 (62045.4530)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1473  max mem: 15572
Epoch: [12]  [1680/2809]  eta: 0:10:50  lr: 0.000042  min_lr: 0.000000  loss: 4.0235 (4.0512)  class_acc: 0.2083 (0.2400)  loss_scale: 65536.0000 (62066.2177)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1414  max mem: 15572
Epoch: [12]  [1690/2809]  eta: 0:10:45  lr: 0.000042  min_lr: 0.000000  loss: 3.7435 (4.0496)  class_acc: 0.2083 (0.2403)  loss_scale: 65536.0000 (62086.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1287  max mem: 15572
Epoch: [12]  [1700/2809]  eta: 0:10:39  lr: 0.000042  min_lr: 0.000000  loss: 3.7412 (4.0485)  class_acc: 0.2500 (0.2406)  loss_scale: 65536.0000 (62107.0147)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1312  max mem: 15572
Epoch: [12]  [1710/2809]  eta: 0:10:33  lr: 0.000042  min_lr: 0.000000  loss: 4.0907 (4.0489)  class_acc: 0.2083 (0.2408)  loss_scale: 65536.0000 (62127.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1366  max mem: 15572
Epoch: [12]  [1720/2809]  eta: 0:10:27  lr: 0.000042  min_lr: 0.000000  loss: 4.1798 (4.0485)  class_acc: 0.2500 (0.2411)  loss_scale: 65536.0000 (62146.8635)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1465  max mem: 15572
Epoch: [12]  [1730/2809]  eta: 0:10:21  lr: 0.000042  min_lr: 0.000000  loss: 4.1867 (4.0489)  class_acc: 0.2500 (0.2409)  loss_scale: 65536.0000 (62166.4425)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0851  max mem: 15572
Epoch: [12]  [1740/2809]  eta: 0:10:15  lr: 0.000042  min_lr: 0.000000  loss: 4.2098 (4.0483)  class_acc: 0.2500 (0.2412)  loss_scale: 65536.0000 (62185.7967)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0611  max mem: 15572
Epoch: [12]  [1750/2809]  eta: 0:10:09  lr: 0.000042  min_lr: 0.000000  loss: 4.0590 (4.0476)  class_acc: 0.2500 (0.2413)  loss_scale: 65536.0000 (62204.9298)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1614  max mem: 15572
[2025-01-15 20:29:03,358] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:29:03,359] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [1760/2809]  eta: 0:10:04  lr: 0.000042  min_lr: 0.000000  loss: 3.9932 (4.0469)  class_acc: 0.2500 (0.2416)  loss_scale: 65536.0000 (62298.2760)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.2353  max mem: 15572
[2025-01-15 20:29:06,296] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35474
[2025-01-15 20:29:06,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:29:06,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1770/2809]  eta: 0:09:58  lr: 0.000042  min_lr: 0.000000  loss: 3.9740 (4.0473)  class_acc: 0.2500 (0.2416)  loss_scale: 65536.0000 (62501.5833)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1756  max mem: 15572
Epoch: [12]  [1780/2809]  eta: 0:09:53  lr: 0.000042  min_lr: 0.000000  loss: 4.1303 (4.0489)  class_acc: 0.2083 (0.2413)  loss_scale: 65536.0000 (62518.6210)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1407  max mem: 15572
Epoch: [12]  [1790/2809]  eta: 0:09:47  lr: 0.000042  min_lr: 0.000000  loss: 4.2338 (4.0486)  class_acc: 0.2083 (0.2414)  loss_scale: 65536.0000 (62535.4685)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1075  max mem: 15572
Epoch: [12]  [1800/2809]  eta: 0:09:41  lr: 0.000042  min_lr: 0.000000  loss: 4.0056 (4.0486)  class_acc: 0.2083 (0.2414)  loss_scale: 65536.0000 (62552.1288)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1188  max mem: 15572
Epoch: [12]  [1810/2809]  eta: 0:09:35  lr: 0.000042  min_lr: 0.000000  loss: 4.0867 (4.0492)  class_acc: 0.2083 (0.2414)  loss_scale: 65536.0000 (62568.6052)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1719  max mem: 15572
Epoch: [12]  [1820/2809]  eta: 0:09:30  lr: 0.000042  min_lr: 0.000000  loss: 4.0867 (4.0489)  class_acc: 0.2083 (0.2416)  loss_scale: 65536.0000 (62584.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1446  max mem: 15572
Epoch: [12]  [1830/2809]  eta: 0:09:25  lr: 0.000042  min_lr: 0.000000  loss: 3.9553 (4.0482)  class_acc: 0.2917 (0.2419)  loss_scale: 65536.0000 (62601.0180)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1997  max mem: 15572
Epoch: [12]  [1840/2809]  eta: 0:09:18  lr: 0.000042  min_lr: 0.000000  loss: 3.9886 (4.0485)  class_acc: 0.2500 (0.2419)  loss_scale: 65536.0000 (62616.9603)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1453  max mem: 15572
Epoch: [12]  [1850/2809]  eta: 0:09:13  lr: 0.000042  min_lr: 0.000000  loss: 4.0498 (4.0481)  class_acc: 0.2500 (0.2423)  loss_scale: 65536.0000 (62632.7304)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1141  max mem: 15572
Epoch: [12]  [1860/2809]  eta: 0:09:06  lr: 0.000042  min_lr: 0.000000  loss: 3.9311 (4.0479)  class_acc: 0.2500 (0.2425)  loss_scale: 65536.0000 (62648.3310)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1024  max mem: 15572
Epoch: [12]  [1870/2809]  eta: 0:09:01  lr: 0.000042  min_lr: 0.000000  loss: 4.0731 (4.0489)  class_acc: 0.2083 (0.2422)  loss_scale: 65536.0000 (62663.7648)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0847  max mem: 15572
Epoch: [12]  [1880/2809]  eta: 0:08:55  lr: 0.000042  min_lr: 0.000000  loss: 4.1036 (4.0486)  class_acc: 0.2083 (0.2422)  loss_scale: 65536.0000 (62679.0346)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0966  max mem: 15572
Epoch: [12]  [1890/2809]  eta: 0:08:49  lr: 0.000042  min_lr: 0.000000  loss: 4.0973 (4.0493)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (62694.1428)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0825  max mem: 15572
[2025-01-15 20:30:19,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:30:19,781] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:30:21,662] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35605
[2025-01-15 20:30:21,662] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:30:21,662] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1900/2809]  eta: 0:08:43  lr: 0.000042  min_lr: 0.000000  loss: 4.2874 (4.0508)  class_acc: 0.1667 (0.2417)  loss_scale: 65536.0000 (62778.0410)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1186  max mem: 15572
Epoch: [12]  [1910/2809]  eta: 0:08:37  lr: 0.000042  min_lr: 0.000000  loss: 4.2704 (4.0512)  class_acc: 0.1667 (0.2415)  loss_scale: 65536.0000 (62792.4731)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0743  max mem: 15572
Epoch: [12]  [1920/2809]  eta: 0:08:31  lr: 0.000042  min_lr: 0.000000  loss: 4.1826 (4.0523)  class_acc: 0.1667 (0.2413)  loss_scale: 65536.0000 (62806.7548)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0531  max mem: 15572
Epoch: [12]  [1930/2809]  eta: 0:08:25  lr: 0.000042  min_lr: 0.000000  loss: 4.1993 (4.0528)  class_acc: 0.2083 (0.2411)  loss_scale: 65536.0000 (62820.8887)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0824  max mem: 15572
Epoch: [12]  [1940/2809]  eta: 0:08:19  lr: 0.000042  min_lr: 0.000000  loss: 3.8415 (4.0517)  class_acc: 0.2500 (0.2412)  loss_scale: 65536.0000 (62834.8769)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0896  max mem: 15572
Epoch: [12]  [1950/2809]  eta: 0:08:13  lr: 0.000042  min_lr: 0.000000  loss: 3.9127 (4.0521)  class_acc: 0.2500 (0.2411)  loss_scale: 65536.0000 (62848.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1215  max mem: 15572
Epoch: [12]  [1960/2809]  eta: 0:08:07  lr: 0.000042  min_lr: 0.000000  loss: 3.9744 (4.0506)  class_acc: 0.2500 (0.2414)  loss_scale: 65536.0000 (62862.4253)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1580  max mem: 15572
Epoch: [12]  [1970/2809]  eta: 0:08:02  lr: 0.000042  min_lr: 0.000000  loss: 3.8858 (4.0496)  class_acc: 0.2917 (0.2417)  loss_scale: 65536.0000 (62875.9899)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1246  max mem: 15572
Epoch: [12]  [1980/2809]  eta: 0:07:56  lr: 0.000041  min_lr: 0.000000  loss: 4.0398 (4.0497)  class_acc: 0.2500 (0.2418)  loss_scale: 65536.0000 (62889.4175)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1053  max mem: 15572
Epoch: [12]  [1990/2809]  eta: 0:07:50  lr: 0.000041  min_lr: 0.000000  loss: 4.1631 (4.0503)  class_acc: 0.2500 (0.2417)  loss_scale: 65536.0000 (62902.7102)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1326  max mem: 15572
Epoch: [12]  [2000/2809]  eta: 0:07:44  lr: 0.000041  min_lr: 0.000000  loss: 4.1631 (4.0506)  class_acc: 0.2083 (0.2416)  loss_scale: 65536.0000 (62915.8701)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1541  max mem: 15572
Epoch: [12]  [2010/2809]  eta: 0:07:38  lr: 0.000041  min_lr: 0.000000  loss: 3.8686 (4.0498)  class_acc: 0.2083 (0.2416)  loss_scale: 65536.0000 (62928.8991)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1130  max mem: 15572
Epoch: [12]  [2020/2809]  eta: 0:07:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0078 (4.0506)  class_acc: 0.2083 (0.2415)  loss_scale: 65536.0000 (62941.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0805  max mem: 15572
[2025-01-15 20:31:31,450] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:31:31,450] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [2030/2809]  eta: 0:07:27  lr: 0.000041  min_lr: 0.000000  loss: 3.9514 (4.0491)  class_acc: 0.2917 (0.2418)  loss_scale: 65536.0000 (63115.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1443  max mem: 15572
[2025-01-15 20:31:36,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35740
[2025-01-15 20:31:36,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:31:36,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2040/2809]  eta: 0:07:21  lr: 0.000041  min_lr: 0.000000  loss: 3.9514 (4.0498)  class_acc: 0.2083 (0.2418)  loss_scale: 65536.0000 (63159.8785)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1460  max mem: 15572
Epoch: [12]  [2050/2809]  eta: 0:07:15  lr: 0.000041  min_lr: 0.000000  loss: 4.1868 (4.0499)  class_acc: 0.2083 (0.2419)  loss_scale: 65536.0000 (63171.4637)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1069  max mem: 15572
Epoch: [12]  [2060/2809]  eta: 0:07:09  lr: 0.000041  min_lr: 0.000000  loss: 3.9211 (4.0479)  class_acc: 0.2917 (0.2422)  loss_scale: 65536.0000 (63182.9364)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1119  max mem: 15572
Epoch: [12]  [2070/2809]  eta: 0:07:04  lr: 0.000041  min_lr: 0.000000  loss: 3.9966 (4.0483)  class_acc: 0.2500 (0.2421)  loss_scale: 65536.0000 (63194.2984)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1316  max mem: 15572
Epoch: [12]  [2080/2809]  eta: 0:06:58  lr: 0.000041  min_lr: 0.000000  loss: 4.1587 (4.0490)  class_acc: 0.2500 (0.2421)  loss_scale: 65536.0000 (63205.5512)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1067  max mem: 15572
Epoch: [12]  [2090/2809]  eta: 0:06:52  lr: 0.000041  min_lr: 0.000000  loss: 4.0588 (4.0495)  class_acc: 0.2500 (0.2420)  loss_scale: 65536.0000 (63216.6963)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1094  max mem: 15572
Epoch: [12]  [2100/2809]  eta: 0:06:46  lr: 0.000041  min_lr: 0.000000  loss: 4.0078 (4.0487)  class_acc: 0.2500 (0.2422)  loss_scale: 65536.0000 (63227.7354)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1434  max mem: 15572
Epoch: [12]  [2110/2809]  eta: 0:06:41  lr: 0.000041  min_lr: 0.000000  loss: 4.1036 (4.0488)  class_acc: 0.2500 (0.2422)  loss_scale: 65536.0000 (63238.6698)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1533  max mem: 15572
Epoch: [12]  [2120/2809]  eta: 0:06:35  lr: 0.000041  min_lr: 0.000000  loss: 4.1037 (4.0497)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (63249.5012)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1842  max mem: 15572
Epoch: [12]  [2130/2809]  eta: 0:06:30  lr: 0.000041  min_lr: 0.000000  loss: 4.1043 (4.0497)  class_acc: 0.2083 (0.2420)  loss_scale: 65536.0000 (63260.2309)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1808  max mem: 15572
Epoch: [12]  [2140/2809]  eta: 0:06:24  lr: 0.000041  min_lr: 0.000000  loss: 4.0261 (4.0491)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (63270.8603)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1736  max mem: 15572
Epoch: [12]  [2150/2809]  eta: 0:06:18  lr: 0.000041  min_lr: 0.000000  loss: 3.8996 (4.0484)  class_acc: 0.2083 (0.2422)  loss_scale: 65536.0000 (63281.3910)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1479  max mem: 15572
Epoch: [12]  [2160/2809]  eta: 0:06:12  lr: 0.000041  min_lr: 0.000000  loss: 4.0134 (4.0480)  class_acc: 0.2083 (0.2422)  loss_scale: 65536.0000 (63291.8242)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1162  max mem: 15572
[2025-01-15 20:32:50,405] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:32:50,405] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:32:52,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35873
[2025-01-15 20:32:52,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:32:52,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2170/2809]  eta: 0:06:07  lr: 0.000041  min_lr: 0.000000  loss: 4.0159 (4.0483)  class_acc: 0.2083 (0.2422)  loss_scale: 65536.0000 (63422.9093)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1403  max mem: 15572
Epoch: [12]  [2180/2809]  eta: 0:06:01  lr: 0.000041  min_lr: 0.000000  loss: 4.0116 (4.0486)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (63432.5979)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1436  max mem: 15572
Epoch: [12]  [2190/2809]  eta: 0:05:55  lr: 0.000041  min_lr: 0.000000  loss: 3.9209 (4.0481)  class_acc: 0.2083 (0.2421)  loss_scale: 65536.0000 (63442.1981)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0775  max mem: 15572
Epoch: [12]  [2200/2809]  eta: 0:05:49  lr: 0.000041  min_lr: 0.000000  loss: 4.0197 (4.0485)  class_acc: 0.2083 (0.2420)  loss_scale: 65536.0000 (63451.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1366  max mem: 15572
Epoch: [12]  [2210/2809]  eta: 0:05:43  lr: 0.000041  min_lr: 0.000000  loss: 3.9768 (4.0477)  class_acc: 0.2500 (0.2423)  loss_scale: 65536.0000 (63461.1379)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1487  max mem: 15572
Epoch: [12]  [2220/2809]  eta: 0:05:38  lr: 0.000041  min_lr: 0.000000  loss: 3.9152 (4.0474)  class_acc: 0.2917 (0.2425)  loss_scale: 65536.0000 (63470.4800)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0830  max mem: 15572
Epoch: [12]  [2230/2809]  eta: 0:05:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0542 (4.0474)  class_acc: 0.2500 (0.2425)  loss_scale: 65536.0000 (63479.7382)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0947  max mem: 15572
Epoch: [12]  [2240/2809]  eta: 0:05:26  lr: 0.000041  min_lr: 0.000000  loss: 4.1221 (4.0480)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (63488.9139)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1238  max mem: 15572
Epoch: [12]  [2250/2809]  eta: 0:05:20  lr: 0.000041  min_lr: 0.000000  loss: 4.1221 (4.0479)  class_acc: 0.2083 (0.2424)  loss_scale: 65536.0000 (63498.0080)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1730  max mem: 15572
Epoch: [12]  [2260/2809]  eta: 0:05:14  lr: 0.000041  min_lr: 0.000000  loss: 4.0005 (4.0471)  class_acc: 0.2917 (0.2426)  loss_scale: 65536.0000 (63507.0217)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1168  max mem: 15572
Epoch: [12]  [2270/2809]  eta: 0:05:09  lr: 0.000041  min_lr: 0.000000  loss: 3.7839 (4.0461)  class_acc: 0.2500 (0.2426)  loss_scale: 65536.0000 (63515.9560)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0849  max mem: 15572
Epoch: [12]  [2280/2809]  eta: 0:05:03  lr: 0.000041  min_lr: 0.000000  loss: 3.9649 (4.0456)  class_acc: 0.2083 (0.2427)  loss_scale: 65536.0000 (63524.8119)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1560  max mem: 15572
Epoch: [12]  [2290/2809]  eta: 0:04:57  lr: 0.000041  min_lr: 0.000000  loss: 3.9865 (4.0452)  class_acc: 0.2083 (0.2426)  loss_scale: 65536.0000 (63533.5906)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1517  max mem: 15572
[2025-01-15 20:34:03,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=237, lr=[4.0059693595025643e-07, 4.0059693595025643e-07, 5.72281337071795e-07, 5.72281337071795e-07, 8.175447672454215e-07, 8.175447672454215e-07, 1.167921096064888e-06, 1.167921096064888e-06, 1.6684587086641256e-06, 1.6684587086641256e-06, 2.383512440948751e-06, 2.383512440948751e-06, 3.4050177727839304e-06, 3.4050177727839304e-06, 4.8643111039770435e-06, 4.8643111039770435e-06, 6.949015862824348e-06, 6.949015862824348e-06, 9.927165518320499e-06, 9.927165518320499e-06, 1.418166502617214e-05, 1.418166502617214e-05, 2.0259521465960202e-05, 2.0259521465960202e-05, 2.894217352280029e-05, 2.894217352280029e-05, 4.134596217542899e-05, 4.134596217542899e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:34:03,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=28.411986885628195, CurrSamplesPerSec=23.663866557652515, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-15 20:34:04,985] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:34:04,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [2300/2809]  eta: 0:04:51  lr: 0.000041  min_lr: 0.000000  loss: 4.0051 (4.0441)  class_acc: 0.2083 (0.2429)  loss_scale: 65536.0000 (63741.6636)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1062  max mem: 15572
[2025-01-15 20:34:08,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36009
[2025-01-15 20:34:08,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:34:08,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2310/2809]  eta: 0:04:46  lr: 0.000041  min_lr: 0.000000  loss: 3.8990 (4.0438)  class_acc: 0.3333 (0.2430)  loss_scale: 65536.0000 (63749.4280)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1268  max mem: 15572
Epoch: [12]  [2320/2809]  eta: 0:04:40  lr: 0.000041  min_lr: 0.000000  loss: 3.9063 (4.0434)  class_acc: 0.2500 (0.2432)  loss_scale: 65536.0000 (63757.1254)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1268  max mem: 15572
[2025-01-15 20:34:21,409] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36032
[2025-01-15 20:34:21,409] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:34:21,409] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [2330/2809]  eta: 0:04:34  lr: 0.000041  min_lr: 0.000000  loss: 4.1166 (4.0430)  class_acc: 0.2500 (0.2433)  loss_scale: 65536.0000 (63666.3544)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.1097  max mem: 15572
Epoch: [12]  [2340/2809]  eta: 0:04:28  lr: 0.000041  min_lr: 0.000000  loss: 4.1802 (4.0441)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (63534.3665)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1064  max mem: 15572
Epoch: [12]  [2350/2809]  eta: 0:04:23  lr: 0.000041  min_lr: 0.000000  loss: 4.1950 (4.0435)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (63403.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0729  max mem: 15572
Epoch: [12]  [2360/2809]  eta: 0:04:17  lr: 0.000041  min_lr: 0.000000  loss: 4.0364 (4.0435)  class_acc: 0.2500 (0.2432)  loss_scale: 32768.0000 (63273.7450)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1280  max mem: 15572
Epoch: [12]  [2370/2809]  eta: 0:04:11  lr: 0.000041  min_lr: 0.000000  loss: 4.1597 (4.0437)  class_acc: 0.2500 (0.2432)  loss_scale: 32768.0000 (63145.0831)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1741  max mem: 15572
Epoch: [12]  [2380/2809]  eta: 0:04:06  lr: 0.000041  min_lr: 0.000000  loss: 4.1239 (4.0441)  class_acc: 0.2083 (0.2430)  loss_scale: 32768.0000 (63017.5019)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1445  max mem: 15572
Epoch: [12]  [2390/2809]  eta: 0:04:00  lr: 0.000041  min_lr: 0.000000  loss: 3.9890 (4.0433)  class_acc: 0.2083 (0.2431)  loss_scale: 32768.0000 (62890.9879)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1431  max mem: 15572
Epoch: [12]  [2400/2809]  eta: 0:03:54  lr: 0.000041  min_lr: 0.000000  loss: 3.9890 (4.0432)  class_acc: 0.2083 (0.2432)  loss_scale: 32768.0000 (62765.5277)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1294  max mem: 15572
Epoch: [12]  [2410/2809]  eta: 0:03:48  lr: 0.000041  min_lr: 0.000000  loss: 3.9686 (4.0422)  class_acc: 0.2917 (0.2434)  loss_scale: 32768.0000 (62641.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1100  max mem: 15572
Epoch: [12]  [2420/2809]  eta: 0:03:43  lr: 0.000041  min_lr: 0.000000  loss: 3.8562 (4.0418)  class_acc: 0.2500 (0.2433)  loss_scale: 32768.0000 (62517.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.0805  max mem: 15572
Epoch: [12]  [2430/2809]  eta: 0:03:37  lr: 0.000041  min_lr: 0.000000  loss: 3.9800 (4.0416)  class_acc: 0.2083 (0.2435)  loss_scale: 32768.0000 (62395.3402)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0712  max mem: 15572
Epoch: [12]  [2440/2809]  eta: 0:03:31  lr: 0.000041  min_lr: 0.000000  loss: 4.0463 (4.0417)  class_acc: 0.2083 (0.2435)  loss_scale: 32768.0000 (62273.9664)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0799  max mem: 15572
Epoch: [12]  [2450/2809]  eta: 0:03:25  lr: 0.000041  min_lr: 0.000000  loss: 4.1114 (4.0421)  class_acc: 0.2500 (0.2435)  loss_scale: 32768.0000 (62153.5830)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1104  max mem: 15572
[2025-01-15 20:35:35,270] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:35:35,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [2460/2809]  eta: 0:03:20  lr: 0.000041  min_lr: 0.000000  loss: 4.0392 (4.0416)  class_acc: 0.2500 (0.2436)  loss_scale: 32768.0000 (62140.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.0996  max mem: 15572
Epoch: [12]  [2470/2809]  eta: 0:03:14  lr: 0.000041  min_lr: 0.000000  loss: 4.0191 (4.0416)  class_acc: 0.2500 (0.2436)  loss_scale: 65536.0000 (62154.4379)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0581  max mem: 15572
Epoch: [12]  [2480/2809]  eta: 0:03:08  lr: 0.000041  min_lr: 0.000000  loss: 4.1030 (4.0414)  class_acc: 0.2083 (0.2435)  loss_scale: 65536.0000 (62168.0677)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0383  max mem: 15572
Epoch: [12]  [2490/2809]  eta: 0:03:02  lr: 0.000041  min_lr: 0.000000  loss: 4.1030 (4.0417)  class_acc: 0.2083 (0.2435)  loss_scale: 65536.0000 (62181.5881)  weight_decay: 0.0500 (0.0500)  time: 0.4756  data: 0.0096  max mem: 15572
Epoch: [12]  [2500/2809]  eta: 0:02:56  lr: 0.000041  min_lr: 0.000000  loss: 3.9509 (4.0414)  class_acc: 0.2917 (0.2437)  loss_scale: 65536.0000 (62195.0004)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.0918  max mem: 15572
Epoch: [12]  [2510/2809]  eta: 0:02:51  lr: 0.000041  min_lr: 0.000000  loss: 3.9251 (4.0413)  class_acc: 0.2917 (0.2438)  loss_scale: 65536.0000 (62208.3059)  weight_decay: 0.0500 (0.0500)  time: 0.6295  data: 0.1635  max mem: 15572
Epoch: [12]  [2520/2809]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000000  loss: 3.8988 (4.0409)  class_acc: 0.2917 (0.2439)  loss_scale: 65536.0000 (62221.5058)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1625  max mem: 15572
Epoch: [12]  [2530/2809]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000000  loss: 3.9738 (4.0404)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62234.6013)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1534  max mem: 15572
Epoch: [12]  [2540/2809]  eta: 0:02:34  lr: 0.000041  min_lr: 0.000000  loss: 4.0913 (4.0409)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (62247.5939)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.1516  max mem: 15572
Epoch: [12]  [2550/2809]  eta: 0:02:28  lr: 0.000041  min_lr: 0.000000  loss: 4.2515 (4.0408)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (62260.4845)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0803  max mem: 15572
Epoch: [12]  [2560/2809]  eta: 0:02:22  lr: 0.000041  min_lr: 0.000000  loss: 4.0671 (4.0417)  class_acc: 0.2083 (0.2437)  loss_scale: 65536.0000 (62273.2745)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0600  max mem: 15572
Epoch: [12]  [2570/2809]  eta: 0:02:16  lr: 0.000041  min_lr: 0.000000  loss: 3.9691 (4.0413)  class_acc: 0.2500 (0.2438)  loss_scale: 65536.0000 (62285.9650)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0601  max mem: 15572
Epoch: [12]  [2580/2809]  eta: 0:02:11  lr: 0.000041  min_lr: 0.000000  loss: 3.9778 (4.0420)  class_acc: 0.2500 (0.2437)  loss_scale: 65536.0000 (62298.5571)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0641  max mem: 15572
[2025-01-15 20:36:47,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:36:47,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:36:48,546] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36291
[2025-01-15 20:36:48,547] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:36:48,547] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2590/2809]  eta: 0:02:05  lr: 0.000041  min_lr: 0.000000  loss: 3.9464 (4.0412)  class_acc: 0.2500 (0.2438)  loss_scale: 65536.0000 (62361.6395)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1568  max mem: 15572
Epoch: [12]  [2600/2809]  eta: 0:01:59  lr: 0.000041  min_lr: 0.000000  loss: 3.8443 (4.0408)  class_acc: 0.2083 (0.2438)  loss_scale: 65536.0000 (62373.8439)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1827  max mem: 15572
Epoch: [12]  [2610/2809]  eta: 0:01:54  lr: 0.000041  min_lr: 0.000000  loss: 3.8354 (4.0406)  class_acc: 0.2083 (0.2437)  loss_scale: 65536.0000 (62385.9548)  weight_decay: 0.0500 (0.0500)  time: 0.6680  data: 0.2383  max mem: 15572
Epoch: [12]  [2620/2809]  eta: 0:01:48  lr: 0.000041  min_lr: 0.000000  loss: 4.0174 (4.0405)  class_acc: 0.2083 (0.2438)  loss_scale: 65536.0000 (62397.9733)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1489  max mem: 15572
Epoch: [12]  [2630/2809]  eta: 0:01:42  lr: 0.000041  min_lr: 0.000000  loss: 3.8659 (4.0393)  class_acc: 0.2500 (0.2441)  loss_scale: 65536.0000 (62409.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0611  max mem: 15572
Epoch: [12]  [2640/2809]  eta: 0:01:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8659 (4.0390)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62421.7372)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1296  max mem: 15572
Epoch: [12]  [2650/2809]  eta: 0:01:31  lr: 0.000041  min_lr: 0.000000  loss: 4.0502 (4.0388)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62433.4847)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1202  max mem: 15572
Epoch: [12]  [2660/2809]  eta: 0:01:25  lr: 0.000041  min_lr: 0.000000  loss: 4.1441 (4.0391)  class_acc: 0.2083 (0.2440)  loss_scale: 65536.0000 (62445.1439)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0517  max mem: 15572
Epoch: [12]  [2670/2809]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000000  loss: 4.1441 (4.0390)  class_acc: 0.1667 (0.2439)  loss_scale: 65536.0000 (62456.7158)  weight_decay: 0.0500 (0.0500)  time: 0.4765  data: 0.0181  max mem: 15572
Epoch: [12]  [2680/2809]  eta: 0:01:13  lr: 0.000041  min_lr: 0.000000  loss: 3.8640 (4.0384)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62468.2014)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0492  max mem: 15572
Epoch: [12]  [2690/2809]  eta: 0:01:08  lr: 0.000041  min_lr: 0.000000  loss: 3.9049 (4.0389)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (62479.6016)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.0872  max mem: 15572
Epoch: [12]  [2700/2809]  eta: 0:01:02  lr: 0.000041  min_lr: 0.000000  loss: 4.2394 (4.0395)  class_acc: 0.1667 (0.2438)  loss_scale: 65536.0000 (62490.9174)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1086  max mem: 15572
Epoch: [12]  [2710/2809]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000000  loss: 4.0654 (4.0389)  class_acc: 0.2917 (0.2440)  loss_scale: 65536.0000 (62502.1498)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1190  max mem: 15572
[2025-01-15 20:38:01,010] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:38:01,011] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:38:01,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36422
[2025-01-15 20:38:01,809] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:38:01,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [2720/2809]  eta: 0:00:50  lr: 0.000041  min_lr: 0.000000  loss: 4.0550 (4.0390)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62561.4700)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1331  max mem: 15572
Epoch: [12]  [2730/2809]  eta: 0:00:45  lr: 0.000041  min_lr: 0.000000  loss: 4.1246 (4.0399)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (62572.3618)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1428  max mem: 15572
Epoch: [12]  [2740/2809]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000000  loss: 4.1246 (4.0399)  class_acc: 0.2083 (0.2439)  loss_scale: 65536.0000 (62583.1740)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0828  max mem: 15572
Epoch: [12]  [2750/2809]  eta: 0:00:33  lr: 0.000041  min_lr: 0.000000  loss: 3.9567 (4.0397)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62593.9077)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0652  max mem: 15572
Epoch: [12]  [2760/2809]  eta: 0:00:28  lr: 0.000041  min_lr: 0.000000  loss: 3.9189 (4.0392)  class_acc: 0.2500 (0.2440)  loss_scale: 65536.0000 (62604.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0834  max mem: 15572
Epoch: [12]  [2770/2809]  eta: 0:00:22  lr: 0.000041  min_lr: 0.000000  loss: 3.8936 (4.0386)  class_acc: 0.2500 (0.2441)  loss_scale: 65536.0000 (62615.1425)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0777  max mem: 15572
Epoch: [12]  [2780/2809]  eta: 0:00:16  lr: 0.000041  min_lr: 0.000000  loss: 3.8345 (4.0381)  class_acc: 0.3333 (0.2445)  loss_scale: 65536.0000 (62625.6455)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1364  max mem: 15572
Epoch: [12]  [2790/2809]  eta: 0:00:10  lr: 0.000041  min_lr: 0.000000  loss: 3.9902 (4.0384)  class_acc: 0.2083 (0.2443)  loss_scale: 65536.0000 (62636.0731)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1298  max mem: 15572
Epoch: [12]  [2800/2809]  eta: 0:00:05  lr: 0.000041  min_lr: 0.000000  loss: 4.1756 (4.0387)  class_acc: 0.1667 (0.2441)  loss_scale: 65536.0000 (62646.4263)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0962  max mem: 15572
Epoch: [12]  [2808/2809]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000000  loss: 4.1987 (4.0387)  class_acc: 0.2083 (0.2442)  loss_scale: 65536.0000 (62654.6557)  weight_decay: 0.0500 (0.0500)  time: 0.4726  data: 0.0503  max mem: 15572
Epoch: [12] Total time: 0:26:46 (0.5719 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000000  loss: 4.1987 (4.0387)  class_acc: 0.2083 (0.2442)  loss_scale: 65536.0000 (62654.6557)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:21:30  loss: 0.4262 (0.4262)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.7429  data: 4.4379  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 3.3587 (2.9672)  acc1: 22.2222 (29.2929)  acc5: 44.4444 (50.0000)  time: 0.7739  data: 0.5549  max mem: 15572
Val:  [ 20/272]  eta: 0:02:14  loss: 3.0505 (2.9510)  acc1: 27.7778 (32.0106)  acc5: 55.5556 (56.8783)  time: 0.3219  data: 0.1150  max mem: 15572
Val:  [ 30/272]  eta: 0:01:50  loss: 2.9944 (3.0097)  acc1: 27.7778 (29.9283)  acc5: 66.6667 (59.1398)  time: 0.2848  data: 0.0891  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 2.9491 (2.9811)  acc1: 27.7778 (28.9973)  acc5: 72.2222 (61.7886)  time: 0.3281  data: 0.1315  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 2.7492 (2.8915)  acc1: 27.7778 (31.5904)  acc5: 72.2222 (64.2702)  time: 0.3553  data: 0.1330  max mem: 15572
Val:  [ 60/272]  eta: 0:01:26  loss: 1.7263 (2.7425)  acc1: 55.5556 (35.8834)  acc5: 83.3333 (66.3934)  time: 0.3578  data: 0.1341  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 1.7391 (2.6607)  acc1: 55.5556 (37.4022)  acc5: 83.3333 (68.6228)  time: 0.3327  data: 0.1296  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 2.4270 (2.6615)  acc1: 38.8889 (37.6543)  acc5: 77.7778 (68.3813)  time: 0.3347  data: 0.1406  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 2.9588 (2.7024)  acc1: 27.7778 (36.9353)  acc5: 66.6667 (68.1319)  time: 0.3190  data: 0.1322  max mem: 15572
Val:  [100/272]  eta: 0:01:04  loss: 2.9588 (2.7474)  acc1: 27.7778 (36.3586)  acc5: 72.2222 (67.6568)  time: 0.3002  data: 0.1145  max mem: 15572
Val:  [110/272]  eta: 0:00:59  loss: 3.0662 (2.8237)  acc1: 5.5556 (33.8338)  acc5: 55.5556 (65.8659)  time: 0.3185  data: 0.1279  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 3.5084 (2.8535)  acc1: 5.5556 (33.1038)  acc5: 50.0000 (65.1974)  time: 0.3224  data: 0.1307  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 2.8746 (2.8176)  acc1: 27.7778 (34.4784)  acc5: 66.6667 (65.7761)  time: 0.3083  data: 0.1260  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 2.5279 (2.8136)  acc1: 33.3333 (34.7518)  acc5: 72.2222 (65.7210)  time: 0.2633  data: 0.0891  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.8012 (2.8034)  acc1: 33.3333 (34.6210)  acc5: 72.2222 (66.4459)  time: 0.2719  data: 0.0932  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.5111 (2.7892)  acc1: 38.8889 (35.4382)  acc5: 77.7778 (67.0462)  time: 0.3044  data: 0.1083  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.8001 (2.8218)  acc1: 27.7778 (34.6004)  acc5: 72.2222 (66.3418)  time: 0.3141  data: 0.1049  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.8001 (2.8094)  acc1: 27.7778 (34.7759)  acc5: 72.2222 (66.9122)  time: 0.3085  data: 0.0994  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.7851 (2.8500)  acc1: 27.7778 (33.7696)  acc5: 66.6667 (65.5323)  time: 0.3417  data: 0.1346  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.9968 (2.8666)  acc1: 22.2222 (33.4715)  acc5: 55.5556 (65.1465)  time: 0.3137  data: 0.1196  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.6803 (2.8719)  acc1: 33.3333 (33.8599)  acc5: 72.2222 (65.1395)  time: 0.2594  data: 0.0715  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.6754 (2.8607)  acc1: 44.4444 (34.3389)  acc5: 72.2222 (65.2841)  time: 0.2738  data: 0.0743  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 2.2186 (2.8248)  acc1: 50.0000 (35.7143)  acc5: 72.2222 (65.7287)  time: 0.2933  data: 0.0998  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.9937 (2.7999)  acc1: 50.0000 (36.1457)  acc5: 83.3333 (66.4131)  time: 0.3346  data: 0.1437  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5363 (2.8176)  acc1: 22.2222 (35.3918)  acc5: 72.2222 (66.0248)  time: 0.3381  data: 0.1284  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.7860 (2.7467)  acc1: 72.2222 (37.5053)  acc5: 83.3333 (67.0498)  time: 0.3184  data: 0.1262  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.7860 (2.7460)  acc1: 55.5556 (37.2079)  acc5: 83.3333 (67.1792)  time: 0.2292  data: 0.0736  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.8279 (2.7498)  acc1: 55.5556 (37.1903)  acc5: 83.3333 (67.1718)  time: 0.2217  data: 0.0735  max mem: 15572
Val: Total time: 0:01:28 (0.3246 s / it)
* Acc@1 37.190 Acc@5 67.172 loss 2.750
Accuracy of the network on the 4883 val videos: 37.2%
[2025-01-15 20:40:22,372] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 20:40:22,375] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 20:40:22,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 20:40:25,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 20:40:25,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.19%
Epoch: [13]  [   0/2809]  eta: 3:16:34  lr: 0.000041  min_lr: 0.000000  loss: 4.2780 (4.2780)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1988  data: 3.7578  max mem: 15572
Epoch: [13]  [  10/2809]  eta: 0:37:30  lr: 0.000041  min_lr: 0.000000  loss: 4.2780 (4.1685)  class_acc: 0.2083 (0.2462)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.3725  max mem: 15572
Epoch: [13]  [  20/2809]  eta: 0:32:33  lr: 0.000041  min_lr: 0.000000  loss: 4.0994 (4.0538)  class_acc: 0.2500 (0.2599)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0842  max mem: 15572
Epoch: [13]  [  30/2809]  eta: 0:34:12  lr: 0.000041  min_lr: 0.000000  loss: 3.9978 (4.0262)  class_acc: 0.2500 (0.2594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7026  data: 0.2515  max mem: 15572
[2025-01-15 20:40:50,142] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:40:50,142] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:40:52,405] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36556
[2025-01-15 20:40:52,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:40:52,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [  40/2809]  eta: 0:32:38  lr: 0.000041  min_lr: 0.000000  loss: 3.8779 (3.9927)  class_acc: 0.2500 (0.2652)  loss_scale: 65536.0000 (73528.1951)  weight_decay: 0.0500 (0.0500)  time: 0.7148  data: 0.2646  max mem: 15572
Epoch: [13]  [  50/2809]  eta: 0:31:11  lr: 0.000041  min_lr: 0.000000  loss: 3.8528 (3.9884)  class_acc: 0.2500 (0.2516)  loss_scale: 65536.0000 (71961.0980)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1408  max mem: 15572
Epoch: [13]  [  60/2809]  eta: 0:31:38  lr: 0.000041  min_lr: 0.000000  loss: 4.0039 (3.9944)  class_acc: 0.2500 (0.2555)  loss_scale: 65536.0000 (70907.8033)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.2142  max mem: 15572
Epoch: [13]  [  70/2809]  eta: 0:31:02  lr: 0.000041  min_lr: 0.000000  loss: 4.0994 (4.0231)  class_acc: 0.2500 (0.2477)  loss_scale: 65536.0000 (70151.2113)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.2312  max mem: 15572
Epoch: [13]  [  80/2809]  eta: 0:30:28  lr: 0.000041  min_lr: 0.000000  loss: 4.1766 (4.0340)  class_acc: 0.2083 (0.2485)  loss_scale: 65536.0000 (69581.4321)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1523  max mem: 15572
Epoch: [13]  [  90/2809]  eta: 0:30:19  lr: 0.000041  min_lr: 0.000000  loss: 4.1780 (4.0398)  class_acc: 0.2083 (0.2459)  loss_scale: 65536.0000 (69136.8791)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.1734  max mem: 15572
Epoch: [13]  [ 100/2809]  eta: 0:29:59  lr: 0.000041  min_lr: 0.000000  loss: 4.0091 (4.0366)  class_acc: 0.2083 (0.2442)  loss_scale: 65536.0000 (68780.3564)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1871  max mem: 15572
[2025-01-15 20:41:35,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36620
[2025-01-15 20:41:35,081] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:41:35,081] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 110/2809]  eta: 0:29:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8201 (4.0067)  class_acc: 0.2917 (0.2500)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1734  max mem: 15572
Epoch: [13]  [ 120/2809]  eta: 0:29:41  lr: 0.000041  min_lr: 0.000000  loss: 3.7924 (3.9957)  class_acc: 0.2500 (0.2490)  loss_scale: 32768.0000 (63369.5207)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.2047  max mem: 15572
Epoch: [13]  [ 130/2809]  eta: 0:29:31  lr: 0.000041  min_lr: 0.000000  loss: 3.8061 (3.9889)  class_acc: 0.2500 (0.2506)  loss_scale: 32768.0000 (61033.5267)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.2071  max mem: 15572
Epoch: [13]  [ 140/2809]  eta: 0:29:15  lr: 0.000041  min_lr: 0.000000  loss: 4.1467 (4.0140)  class_acc: 0.2083 (0.2467)  loss_scale: 32768.0000 (59028.8794)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.2001  max mem: 15572
Epoch: [13]  [ 150/2809]  eta: 0:28:23  lr: 0.000041  min_lr: 0.000000  loss: 4.1467 (4.0106)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (57289.7483)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.1132  max mem: 15572
Epoch: [13]  [ 160/2809]  eta: 0:27:47  lr: 0.000041  min_lr: 0.000000  loss: 3.9334 (3.9966)  class_acc: 0.2917 (0.2500)  loss_scale: 32768.0000 (55766.6584)  weight_decay: 0.0500 (0.0500)  time: 0.4308  data: 0.0005  max mem: 15572
Epoch: [13]  [ 170/2809]  eta: 0:27:18  lr: 0.000041  min_lr: 0.000000  loss: 3.9032 (3.9898)  class_acc: 0.2500 (0.2527)  loss_scale: 32768.0000 (54421.7076)  weight_decay: 0.0500 (0.0500)  time: 0.4724  data: 0.0006  max mem: 15572
Epoch: [13]  [ 180/2809]  eta: 0:26:47  lr: 0.000041  min_lr: 0.000000  loss: 3.7833 (3.9849)  class_acc: 0.2500 (0.2521)  loss_scale: 32768.0000 (53225.3702)  weight_decay: 0.0500 (0.0500)  time: 0.4643  data: 0.0006  max mem: 15572
Epoch: [13]  [ 190/2809]  eta: 0:26:21  lr: 0.000041  min_lr: 0.000000  loss: 3.7703 (3.9825)  class_acc: 0.2083 (0.2509)  loss_scale: 32768.0000 (52154.3037)  weight_decay: 0.0500 (0.0500)  time: 0.4568  data: 0.0172  max mem: 15572
Epoch: [13]  [ 200/2809]  eta: 0:26:10  lr: 0.000041  min_lr: 0.000000  loss: 3.7942 (3.9757)  class_acc: 0.2500 (0.2527)  loss_scale: 32768.0000 (51189.8109)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0773  max mem: 15572
Epoch: [13]  [ 210/2809]  eta: 0:26:15  lr: 0.000041  min_lr: 0.000000  loss: 3.9661 (3.9847)  class_acc: 0.2500 (0.2494)  loss_scale: 32768.0000 (50316.7393)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1753  max mem: 15572
Epoch: [13]  [ 220/2809]  eta: 0:26:11  lr: 0.000041  min_lr: 0.000000  loss: 4.1140 (3.9778)  class_acc: 0.2500 (0.2519)  loss_scale: 32768.0000 (49522.6787)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.2078  max mem: 15572
Epoch: [13]  [ 230/2809]  eta: 0:25:58  lr: 0.000041  min_lr: 0.000000  loss: 3.8901 (3.9755)  class_acc: 0.3333 (0.2536)  loss_scale: 32768.0000 (48797.3680)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1422  max mem: 15572
[2025-01-15 20:42:45,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:42:45,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 240/2809]  eta: 0:25:42  lr: 0.000041  min_lr: 0.000000  loss: 4.1634 (3.9848)  class_acc: 0.2500 (0.2516)  loss_scale: 32768.0000 (49355.9502)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0873  max mem: 15572
Epoch: [13]  [ 250/2809]  eta: 0:25:30  lr: 0.000041  min_lr: 0.000000  loss: 4.1929 (3.9913)  class_acc: 0.1667 (0.2515)  loss_scale: 65536.0000 (50000.5737)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0933  max mem: 15572
Epoch: [13]  [ 260/2809]  eta: 0:25:21  lr: 0.000041  min_lr: 0.000000  loss: 4.0727 (3.9895)  class_acc: 0.2500 (0.2542)  loss_scale: 65536.0000 (50595.8008)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1340  max mem: 15572
Epoch: [13]  [ 270/2809]  eta: 0:25:10  lr: 0.000041  min_lr: 0.000000  loss: 3.8931 (3.9861)  class_acc: 0.2917 (0.2560)  loss_scale: 65536.0000 (51147.0996)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1307  max mem: 15572
Epoch: [13]  [ 280/2809]  eta: 0:25:03  lr: 0.000041  min_lr: 0.000000  loss: 3.8737 (3.9851)  class_acc: 0.2500 (0.2555)  loss_scale: 65536.0000 (51659.1601)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1118  max mem: 15572
Epoch: [13]  [ 290/2809]  eta: 0:24:53  lr: 0.000041  min_lr: 0.000000  loss: 3.9427 (3.9865)  class_acc: 0.2083 (0.2542)  loss_scale: 65536.0000 (52136.0275)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1103  max mem: 15572
Epoch: [13]  [ 300/2809]  eta: 0:24:40  lr: 0.000041  min_lr: 0.000000  loss: 4.0003 (3.9815)  class_acc: 0.2500 (0.2565)  loss_scale: 65536.0000 (52581.2093)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0854  max mem: 15572
Epoch: [13]  [ 310/2809]  eta: 0:24:38  lr: 0.000041  min_lr: 0.000000  loss: 4.0835 (3.9908)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (52997.7621)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1264  max mem: 15572
[2025-01-15 20:43:34,645] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36836
[2025-01-15 20:43:34,645] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:43:34,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 320/2809]  eta: 0:24:32  lr: 0.000041  min_lr: 0.000000  loss: 4.0480 (3.9846)  class_acc: 0.2500 (0.2558)  loss_scale: 65536.0000 (53184.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1595  max mem: 15572
Epoch: [13]  [ 330/2809]  eta: 0:24:23  lr: 0.000041  min_lr: 0.000000  loss: 3.8849 (3.9858)  class_acc: 0.2917 (0.2569)  loss_scale: 32768.0000 (52567.3958)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1233  max mem: 15572
Epoch: [13]  [ 340/2809]  eta: 0:24:17  lr: 0.000041  min_lr: 0.000000  loss: 3.9936 (3.9888)  class_acc: 0.2500 (0.2566)  loss_scale: 32768.0000 (51986.7683)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1320  max mem: 15572
Epoch: [13]  [ 350/2809]  eta: 0:24:06  lr: 0.000041  min_lr: 0.000000  loss: 4.0063 (3.9874)  class_acc: 0.2500 (0.2570)  loss_scale: 32768.0000 (51439.2251)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1116  max mem: 15572
Epoch: [13]  [ 360/2809]  eta: 0:23:58  lr: 0.000041  min_lr: 0.000000  loss: 3.9568 (3.9856)  class_acc: 0.2500 (0.2576)  loss_scale: 32768.0000 (50922.0166)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0979  max mem: 15572
Epoch: [13]  [ 370/2809]  eta: 0:23:53  lr: 0.000041  min_lr: 0.000000  loss: 3.9967 (3.9922)  class_acc: 0.2500 (0.2566)  loss_scale: 32768.0000 (50432.6900)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1408  max mem: 15572
Epoch: [13]  [ 380/2809]  eta: 0:23:51  lr: 0.000041  min_lr: 0.000000  loss: 4.1924 (3.9926)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (49969.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1833  max mem: 15572
Epoch: [13]  [ 390/2809]  eta: 0:23:41  lr: 0.000041  min_lr: 0.000000  loss: 3.9088 (3.9926)  class_acc: 0.2917 (0.2579)  loss_scale: 32768.0000 (49529.1253)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1431  max mem: 15572
Epoch: [13]  [ 400/2809]  eta: 0:23:35  lr: 0.000041  min_lr: 0.000000  loss: 3.9875 (3.9930)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (49111.1421)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0878  max mem: 15572
Epoch: [13]  [ 410/2809]  eta: 0:23:31  lr: 0.000041  min_lr: 0.000000  loss: 3.9875 (3.9916)  class_acc: 0.2500 (0.2569)  loss_scale: 32768.0000 (48713.4988)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1466  max mem: 15572
Epoch: [13]  [ 420/2809]  eta: 0:23:24  lr: 0.000041  min_lr: 0.000000  loss: 4.2000 (3.9949)  class_acc: 0.2500 (0.2572)  loss_scale: 32768.0000 (48334.7458)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1621  max mem: 15572
Epoch: [13]  [ 430/2809]  eta: 0:23:16  lr: 0.000041  min_lr: 0.000000  loss: 4.1876 (3.9936)  class_acc: 0.2500 (0.2568)  loss_scale: 32768.0000 (47973.5684)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1165  max mem: 15572
Epoch: [13]  [ 440/2809]  eta: 0:23:10  lr: 0.000041  min_lr: 0.000000  loss: 3.8744 (3.9900)  class_acc: 0.2500 (0.2574)  loss_scale: 32768.0000 (47628.7710)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1214  max mem: 15572
[2025-01-15 20:44:49,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:44:49,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 450/2809]  eta: 0:23:06  lr: 0.000041  min_lr: 0.000000  loss: 3.9569 (3.9937)  class_acc: 0.2083 (0.2556)  loss_scale: 32768.0000 (47517.2328)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1506  max mem: 15572
Epoch: [13]  [ 460/2809]  eta: 0:22:52  lr: 0.000041  min_lr: 0.000000  loss: 3.9569 (3.9881)  class_acc: 0.2083 (0.2577)  loss_scale: 65536.0000 (47908.0954)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0805  max mem: 15572
Epoch: [13]  [ 470/2809]  eta: 0:22:48  lr: 0.000041  min_lr: 0.000000  loss: 4.0771 (3.9893)  class_acc: 0.2917 (0.2583)  loss_scale: 65536.0000 (48282.3609)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0907  max mem: 15572
Epoch: [13]  [ 480/2809]  eta: 0:22:41  lr: 0.000041  min_lr: 0.000000  loss: 4.1374 (3.9918)  class_acc: 0.2917 (0.2579)  loss_scale: 65536.0000 (48641.0644)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1409  max mem: 15572
[2025-01-15 20:45:07,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=244, lr=[3.958308455732292e-07, 3.958308455732292e-07, 5.654726365331847e-07, 5.654726365331847e-07, 8.078180521902639e-07, 8.078180521902639e-07, 1.1540257888432343e-06, 1.1540257888432343e-06, 1.6486082697760488e-06, 1.6486082697760488e-06, 2.355154671108641e-06, 2.355154671108641e-06, 3.3645066730123448e-06, 3.3645066730123448e-06, 4.806438104303351e-06, 4.806438104303351e-06, 6.866340149004786e-06, 6.866340149004786e-06, 9.809057355721125e-06, 9.809057355721125e-06, 1.4012939079601606e-05, 1.4012939079601606e-05, 2.001848439943087e-05, 2.001848439943087e-05, 2.8597834856329814e-05, 2.8597834856329814e-05, 4.085404979475688e-05, 4.085404979475688e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:45:07,331] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=28.406799393778467, CurrSamplesPerSec=24.339321490486796, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [ 490/2809]  eta: 0:22:33  lr: 0.000041  min_lr: 0.000000  loss: 4.0723 (3.9919)  class_acc: 0.2500 (0.2576)  loss_scale: 65536.0000 (48985.1568)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0978  max mem: 15572
Epoch: [13]  [ 500/2809]  eta: 0:22:28  lr: 0.000041  min_lr: 0.000000  loss: 3.9703 (3.9914)  class_acc: 0.2500 (0.2580)  loss_scale: 65536.0000 (49315.5130)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1278  max mem: 15572
Epoch: [13]  [ 510/2809]  eta: 0:22:20  lr: 0.000041  min_lr: 0.000000  loss: 3.9363 (3.9905)  class_acc: 0.2500 (0.2578)  loss_scale: 65536.0000 (49632.9393)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1363  max mem: 15572
Epoch: [13]  [ 520/2809]  eta: 0:22:13  lr: 0.000041  min_lr: 0.000000  loss: 3.9703 (3.9934)  class_acc: 0.2083 (0.2572)  loss_scale: 65536.0000 (49938.1804)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1091  max mem: 15572
Epoch: [13]  [ 530/2809]  eta: 0:22:03  lr: 0.000041  min_lr: 0.000000  loss: 3.8729 (3.9890)  class_acc: 0.2500 (0.2579)  loss_scale: 65536.0000 (50231.9247)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0797  max mem: 15572
Epoch: [13]  [ 540/2809]  eta: 0:21:56  lr: 0.000041  min_lr: 0.000000  loss: 3.8452 (3.9915)  class_acc: 0.2500 (0.2571)  loss_scale: 65536.0000 (50514.8096)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0808  max mem: 15572
Epoch: [13]  [ 550/2809]  eta: 0:21:49  lr: 0.000041  min_lr: 0.000000  loss: 4.0322 (3.9932)  class_acc: 0.2083 (0.2567)  loss_scale: 65536.0000 (50787.4265)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1155  max mem: 15572
Epoch: [13]  [ 560/2809]  eta: 0:21:43  lr: 0.000041  min_lr: 0.000000  loss: 4.1992 (3.9957)  class_acc: 0.2083 (0.2555)  loss_scale: 65536.0000 (51050.3244)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1375  max mem: 15572
Epoch: [13]  [ 570/2809]  eta: 0:21:35  lr: 0.000041  min_lr: 0.000000  loss: 3.9787 (3.9941)  class_acc: 0.2083 (0.2558)  loss_scale: 65536.0000 (51304.0140)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1159  max mem: 15572
[2025-01-15 20:45:58,243] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:45:58,244] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:45:59,235] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37095
[2025-01-15 20:45:59,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:45:59,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 580/2809]  eta: 0:21:24  lr: 0.000041  min_lr: 0.000000  loss: 3.8790 (3.9919)  class_acc: 0.2917 (0.2565)  loss_scale: 65536.0000 (51774.5680)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0397  max mem: 15572
Epoch: [13]  [ 590/2809]  eta: 0:21:20  lr: 0.000041  min_lr: 0.000000  loss: 3.8790 (3.9928)  class_acc: 0.2500 (0.2562)  loss_scale: 65536.0000 (52007.4179)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0816  max mem: 15572
Epoch: [13]  [ 600/2809]  eta: 0:21:11  lr: 0.000041  min_lr: 0.000000  loss: 4.0808 (3.9943)  class_acc: 0.2500 (0.2563)  loss_scale: 65536.0000 (52232.5191)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0966  max mem: 15572
Epoch: [13]  [ 610/2809]  eta: 0:21:03  lr: 0.000041  min_lr: 0.000000  loss: 4.1253 (3.9942)  class_acc: 0.2083 (0.2561)  loss_scale: 65536.0000 (52450.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0639  max mem: 15572
Epoch: [13]  [ 620/2809]  eta: 0:21:00  lr: 0.000041  min_lr: 0.000000  loss: 4.1335 (3.9966)  class_acc: 0.2083 (0.2550)  loss_scale: 65536.0000 (52660.9726)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1311  max mem: 15572
Epoch: [13]  [ 630/2809]  eta: 0:20:56  lr: 0.000041  min_lr: 0.000000  loss: 4.1840 (3.9977)  class_acc: 0.1667 (0.2544)  loss_scale: 65536.0000 (52865.0143)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1777  max mem: 15572
[2025-01-15 20:46:33,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37156
[2025-01-15 20:46:33,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 20:46:33,660] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 640/2809]  eta: 0:20:48  lr: 0.000041  min_lr: 0.000000  loss: 4.0657 (3.9998)  class_acc: 0.2500 (0.2540)  loss_scale: 65536.0000 (52960.4493)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1269  max mem: 15572
Epoch: [13]  [ 650/2809]  eta: 0:20:42  lr: 0.000041  min_lr: 0.000000  loss: 3.9840 (3.9987)  class_acc: 0.2500 (0.2543)  loss_scale: 32768.0000 (52650.2734)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0829  max mem: 15572
Epoch: [13]  [ 660/2809]  eta: 0:20:37  lr: 0.000041  min_lr: 0.000000  loss: 3.9840 (3.9996)  class_acc: 0.2083 (0.2536)  loss_scale: 32768.0000 (52349.4826)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1460  max mem: 15572
Epoch: [13]  [ 670/2809]  eta: 0:20:29  lr: 0.000041  min_lr: 0.000000  loss: 4.0835 (3.9994)  class_acc: 0.2083 (0.2540)  loss_scale: 32768.0000 (52057.6572)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1280  max mem: 15572
Epoch: [13]  [ 680/2809]  eta: 0:20:20  lr: 0.000041  min_lr: 0.000000  loss: 3.8960 (3.9981)  class_acc: 0.2500 (0.2537)  loss_scale: 32768.0000 (51774.4023)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0475  max mem: 15572
Epoch: [13]  [ 690/2809]  eta: 0:20:16  lr: 0.000041  min_lr: 0.000000  loss: 3.7673 (3.9979)  class_acc: 0.2500 (0.2536)  loss_scale: 32768.0000 (51499.3459)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1057  max mem: 15572
Epoch: [13]  [ 700/2809]  eta: 0:20:09  lr: 0.000041  min_lr: 0.000000  loss: 4.1085 (4.0002)  class_acc: 0.2083 (0.2529)  loss_scale: 32768.0000 (51232.1369)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1359  max mem: 15572
Epoch: [13]  [ 710/2809]  eta: 0:20:02  lr: 0.000041  min_lr: 0.000000  loss: 4.0731 (3.9996)  class_acc: 0.2083 (0.2528)  loss_scale: 32768.0000 (50972.4444)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0929  max mem: 15572
Epoch: [13]  [ 720/2809]  eta: 0:19:55  lr: 0.000041  min_lr: 0.000000  loss: 3.8897 (3.9981)  class_acc: 0.2500 (0.2526)  loss_scale: 32768.0000 (50719.9556)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1071  max mem: 15572
Epoch: [13]  [ 730/2809]  eta: 0:19:50  lr: 0.000041  min_lr: 0.000000  loss: 3.7692 (3.9960)  class_acc: 0.2500 (0.2531)  loss_scale: 32768.0000 (50474.3748)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1399  max mem: 15572
Epoch: [13]  [ 740/2809]  eta: 0:19:46  lr: 0.000041  min_lr: 0.000000  loss: 3.7764 (3.9964)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (50235.4224)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1895  max mem: 15572
Epoch: [13]  [ 750/2809]  eta: 0:19:42  lr: 0.000041  min_lr: 0.000000  loss: 4.2405 (4.0010)  class_acc: 0.2083 (0.2523)  loss_scale: 32768.0000 (50002.8336)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1934  max mem: 15572
Epoch: [13]  [ 760/2809]  eta: 0:19:32  lr: 0.000041  min_lr: 0.000000  loss: 4.3029 (4.0007)  class_acc: 0.1667 (0.2522)  loss_scale: 32768.0000 (49776.3574)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0852  max mem: 15572
[2025-01-15 20:47:46,206] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:47:46,206] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 770/2809]  eta: 0:19:28  lr: 0.000041  min_lr: 0.000000  loss: 4.1004 (4.0023)  class_acc: 0.2500 (0.2522)  loss_scale: 32768.0000 (49683.2581)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1020  max mem: 15572
Epoch: [13]  [ 780/2809]  eta: 0:19:22  lr: 0.000041  min_lr: 0.000000  loss: 4.1432 (4.0052)  class_acc: 0.2083 (0.2520)  loss_scale: 65536.0000 (49886.2382)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1602  max mem: 15572
Epoch: [13]  [ 790/2809]  eta: 0:19:15  lr: 0.000041  min_lr: 0.000000  loss: 4.1035 (4.0056)  class_acc: 0.2083 (0.2522)  loss_scale: 65536.0000 (50084.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1022  max mem: 15572
Epoch: [13]  [ 800/2809]  eta: 0:19:09  lr: 0.000041  min_lr: 0.000000  loss: 4.1109 (4.0080)  class_acc: 0.2083 (0.2514)  loss_scale: 65536.0000 (50276.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1019  max mem: 15572
Epoch: [13]  [ 810/2809]  eta: 0:19:02  lr: 0.000041  min_lr: 0.000000  loss: 4.2246 (4.0099)  class_acc: 0.1667 (0.2508)  loss_scale: 65536.0000 (50465.1443)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0863  max mem: 15572
Epoch: [13]  [ 820/2809]  eta: 0:18:58  lr: 0.000041  min_lr: 0.000000  loss: 4.2246 (4.0114)  class_acc: 0.2083 (0.2507)  loss_scale: 65536.0000 (50648.7113)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1193  max mem: 15572
Epoch: [13]  [ 830/2809]  eta: 0:18:53  lr: 0.000041  min_lr: 0.000000  loss: 4.2420 (4.0125)  class_acc: 0.2500 (0.2511)  loss_scale: 65536.0000 (50827.8604)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1746  max mem: 15572
Epoch: [13]  [ 840/2809]  eta: 0:18:47  lr: 0.000041  min_lr: 0.000000  loss: 4.2577 (4.0139)  class_acc: 0.2500 (0.2506)  loss_scale: 65536.0000 (51002.7491)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1477  max mem: 15572
Epoch: [13]  [ 850/2809]  eta: 0:18:39  lr: 0.000041  min_lr: 0.000000  loss: 4.0675 (4.0124)  class_acc: 0.2500 (0.2512)  loss_scale: 65536.0000 (51173.5276)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0727  max mem: 15572
Epoch: [13]  [ 860/2809]  eta: 0:18:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8856 (4.0101)  class_acc: 0.2500 (0.2516)  loss_scale: 65536.0000 (51340.3391)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1292  max mem: 15572
Epoch: [13]  [ 870/2809]  eta: 0:18:30  lr: 0.000041  min_lr: 0.000000  loss: 3.8593 (4.0109)  class_acc: 0.2083 (0.2513)  loss_scale: 65536.0000 (51503.3203)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1754  max mem: 15572
Epoch: [13]  [ 880/2809]  eta: 0:18:23  lr: 0.000041  min_lr: 0.000000  loss: 4.2193 (4.0126)  class_acc: 0.1667 (0.2511)  loss_scale: 65536.0000 (51662.6016)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1121  max mem: 15572
Epoch: [13]  [ 890/2809]  eta: 0:18:19  lr: 0.000041  min_lr: 0.000000  loss: 3.9793 (4.0117)  class_acc: 0.2083 (0.2507)  loss_scale: 65536.0000 (51818.3075)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1514  max mem: 15572
[2025-01-15 20:48:58,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:48:58,917] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:48:59,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37415
[2025-01-15 20:48:59,811] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:48:59,811] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 900/2809]  eta: 0:18:11  lr: 0.000041  min_lr: 0.000000  loss: 3.8929 (4.0112)  class_acc: 0.2083 (0.2511)  loss_scale: 65536.0000 (52116.0311)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1140  max mem: 15572
Epoch: [13]  [ 910/2809]  eta: 0:18:06  lr: 0.000041  min_lr: 0.000000  loss: 4.0093 (4.0122)  class_acc: 0.2500 (0.2510)  loss_scale: 65536.0000 (52263.3414)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0985  max mem: 15572
Epoch: [13]  [ 920/2809]  eta: 0:17:58  lr: 0.000041  min_lr: 0.000000  loss: 3.9462 (4.0105)  class_acc: 0.2500 (0.2511)  loss_scale: 65536.0000 (52407.4528)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0787  max mem: 15572
Epoch: [13]  [ 930/2809]  eta: 0:17:53  lr: 0.000041  min_lr: 0.000000  loss: 3.9841 (4.0111)  class_acc: 0.2083 (0.2508)  loss_scale: 65536.0000 (52548.4683)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0908  max mem: 15572
Epoch: [13]  [ 940/2809]  eta: 0:17:47  lr: 0.000041  min_lr: 0.000000  loss: 4.0019 (4.0095)  class_acc: 0.2500 (0.2513)  loss_scale: 65536.0000 (52686.4867)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1549  max mem: 15572
Epoch: [13]  [ 950/2809]  eta: 0:17:42  lr: 0.000041  min_lr: 0.000000  loss: 3.9847 (4.0100)  class_acc: 0.2500 (0.2513)  loss_scale: 65536.0000 (52821.6025)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1460  max mem: 15572
Epoch: [13]  [ 960/2809]  eta: 0:17:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8634 (4.0071)  class_acc: 0.2500 (0.2516)  loss_scale: 65536.0000 (52953.9063)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1440  max mem: 15572
Epoch: [13]  [ 970/2809]  eta: 0:17:31  lr: 0.000041  min_lr: 0.000000  loss: 3.9176 (4.0068)  class_acc: 0.2500 (0.2519)  loss_scale: 65536.0000 (53083.4851)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1326  max mem: 15572
Epoch: [13]  [ 980/2809]  eta: 0:17:25  lr: 0.000041  min_lr: 0.000000  loss: 3.9716 (4.0045)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (53210.4220)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1432  max mem: 15572
Epoch: [13]  [ 990/2809]  eta: 0:17:20  lr: 0.000041  min_lr: 0.000000  loss: 4.0535 (4.0051)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (53334.7972)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1648  max mem: 15572
Epoch: [13]  [1000/2809]  eta: 0:17:13  lr: 0.000041  min_lr: 0.000000  loss: 4.0535 (4.0036)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (53456.6873)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0920  max mem: 15572
Epoch: [13]  [1010/2809]  eta: 0:17:08  lr: 0.000041  min_lr: 0.000000  loss: 3.8227 (4.0029)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (53576.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0906  max mem: 15572
Epoch: [13]  [1020/2809]  eta: 0:17:02  lr: 0.000041  min_lr: 0.000000  loss: 3.8822 (4.0030)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (53693.3046)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1326  max mem: 15572
[2025-01-15 20:50:13,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:50:13,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1030/2809]  eta: 0:16:57  lr: 0.000041  min_lr: 0.000000  loss: 4.1857 (4.0039)  class_acc: 0.2083 (0.2530)  loss_scale: 65536.0000 (54062.4326)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1479  max mem: 15572
[2025-01-15 20:50:18,840] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37553
[2025-01-15 20:50:18,841] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:50:18,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1040/2809]  eta: 0:16:52  lr: 0.000041  min_lr: 0.000000  loss: 3.9033 (4.0029)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (54487.4236)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1643  max mem: 15572
Epoch: [13]  [1050/2809]  eta: 0:16:46  lr: 0.000041  min_lr: 0.000000  loss: 3.9042 (4.0032)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (54592.5480)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1298  max mem: 15572
Epoch: [13]  [1060/2809]  eta: 0:16:40  lr: 0.000041  min_lr: 0.000000  loss: 4.0346 (4.0021)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (54695.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1228  max mem: 15572
Epoch: [13]  [1070/2809]  eta: 0:16:36  lr: 0.000041  min_lr: 0.000000  loss: 3.8034 (4.0010)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (54796.9076)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1436  max mem: 15572
Epoch: [13]  [1080/2809]  eta: 0:16:29  lr: 0.000041  min_lr: 0.000000  loss: 3.8610 (4.0003)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (54896.2516)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1275  max mem: 15572
Epoch: [13]  [1090/2809]  eta: 0:16:22  lr: 0.000041  min_lr: 0.000000  loss: 4.0388 (4.0011)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (54993.7745)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0823  max mem: 15572
Epoch: [13]  [1100/2809]  eta: 0:16:15  lr: 0.000041  min_lr: 0.000000  loss: 3.9028 (3.9994)  class_acc: 0.2500 (0.2542)  loss_scale: 65536.0000 (55089.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0542  max mem: 15572
Epoch: [13]  [1110/2809]  eta: 0:16:09  lr: 0.000041  min_lr: 0.000000  loss: 3.9099 (3.9989)  class_acc: 0.2500 (0.2541)  loss_scale: 65536.0000 (55183.5536)  weight_decay: 0.0500 (0.0500)  time: 0.5079  data: 0.0533  max mem: 15572
Epoch: [13]  [1120/2809]  eta: 0:16:03  lr: 0.000041  min_lr: 0.000000  loss: 4.1650 (4.0008)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (55275.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1082  max mem: 15572
Epoch: [13]  [1130/2809]  eta: 0:15:58  lr: 0.000041  min_lr: 0.000000  loss: 4.2986 (4.0017)  class_acc: 0.2083 (0.2539)  loss_scale: 65536.0000 (55366.6207)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1482  max mem: 15572
Epoch: [13]  [1140/2809]  eta: 0:15:52  lr: 0.000041  min_lr: 0.000000  loss: 4.2148 (4.0036)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (55455.7476)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1272  max mem: 15572
Epoch: [13]  [1150/2809]  eta: 0:15:47  lr: 0.000041  min_lr: 0.000000  loss: 4.0500 (4.0024)  class_acc: 0.1667 (0.2538)  loss_scale: 65536.0000 (55543.3258)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1297  max mem: 15572
Epoch: [13]  [1160/2809]  eta: 0:15:41  lr: 0.000041  min_lr: 0.000000  loss: 3.8704 (4.0029)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (55629.3953)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1192  max mem: 15572
[2025-01-15 20:51:31,259] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:51:31,259] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1170/2809]  eta: 0:15:34  lr: 0.000041  min_lr: 0.000000  loss: 3.9524 (4.0027)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (56049.7899)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0687  max mem: 15572
[2025-01-15 20:51:36,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37690
[2025-01-15 20:51:36,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:51:36,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1180/2809]  eta: 0:15:29  lr: 0.000041  min_lr: 0.000000  loss: 4.0205 (4.0039)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (56241.0974)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1325  max mem: 15572
Epoch: [13]  [1190/2809]  eta: 0:15:23  lr: 0.000040  min_lr: 0.000000  loss: 4.2157 (4.0065)  class_acc: 0.2083 (0.2528)  loss_scale: 65536.0000 (56319.1402)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1475  max mem: 15572
Epoch: [13]  [1200/2809]  eta: 0:15:17  lr: 0.000040  min_lr: 0.000000  loss: 4.2567 (4.0075)  class_acc: 0.2083 (0.2526)  loss_scale: 65536.0000 (56395.8834)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1068  max mem: 15572
Epoch: [13]  [1210/2809]  eta: 0:15:13  lr: 0.000040  min_lr: 0.000000  loss: 4.1205 (4.0075)  class_acc: 0.2083 (0.2526)  loss_scale: 65536.0000 (56471.3592)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1477  max mem: 15572
Epoch: [13]  [1220/2809]  eta: 0:15:05  lr: 0.000040  min_lr: 0.000000  loss: 3.9241 (4.0061)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (56545.5987)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0941  max mem: 15572
Epoch: [13]  [1230/2809]  eta: 0:15:01  lr: 0.000040  min_lr: 0.000000  loss: 3.6873 (4.0023)  class_acc: 0.3333 (0.2542)  loss_scale: 65536.0000 (56618.6320)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1507  max mem: 15572
Epoch: [13]  [1240/2809]  eta: 0:14:56  lr: 0.000040  min_lr: 0.000000  loss: 3.6873 (4.0014)  class_acc: 0.2917 (0.2541)  loss_scale: 65536.0000 (56690.4883)  weight_decay: 0.0500 (0.0500)  time: 0.6540  data: 0.2013  max mem: 15572
Epoch: [13]  [1250/2809]  eta: 0:14:49  lr: 0.000040  min_lr: 0.000000  loss: 4.0469 (4.0032)  class_acc: 0.2083 (0.2536)  loss_scale: 65536.0000 (56761.1958)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0957  max mem: 15572
Epoch: [13]  [1260/2809]  eta: 0:14:43  lr: 0.000040  min_lr: 0.000000  loss: 4.1551 (4.0043)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (56830.7819)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0951  max mem: 15572
Epoch: [13]  [1270/2809]  eta: 0:14:37  lr: 0.000040  min_lr: 0.000000  loss: 3.7991 (4.0025)  class_acc: 0.2917 (0.2537)  loss_scale: 65536.0000 (56899.2730)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0820  max mem: 15572
Epoch: [13]  [1280/2809]  eta: 0:14:31  lr: 0.000040  min_lr: 0.000000  loss: 4.0403 (4.0055)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (56966.6948)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1322  max mem: 15572
Epoch: [13]  [1290/2809]  eta: 0:14:27  lr: 0.000040  min_lr: 0.000000  loss: 4.2184 (4.0036)  class_acc: 0.2083 (0.2537)  loss_scale: 65536.0000 (57033.0720)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.2452  max mem: 15572
Epoch: [13]  [1300/2809]  eta: 0:14:21  lr: 0.000040  min_lr: 0.000000  loss: 3.9795 (4.0038)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (57098.4289)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1742  max mem: 15572
[2025-01-15 20:52:50,086] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:52:50,086] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:52:50,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37820
[2025-01-15 20:52:50,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:52:50,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1310/2809]  eta: 0:14:15  lr: 0.000040  min_lr: 0.000000  loss: 3.9278 (4.0027)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (57212.7780)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0800  max mem: 15572
Epoch: [13]  [1320/2809]  eta: 0:14:09  lr: 0.000040  min_lr: 0.000000  loss: 3.8650 (4.0016)  class_acc: 0.2500 (0.2541)  loss_scale: 65536.0000 (57275.7850)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1407  max mem: 15572
Epoch: [13]  [1330/2809]  eta: 0:14:04  lr: 0.000040  min_lr: 0.000000  loss: 3.9826 (4.0024)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (57337.8452)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1663  max mem: 15572
Epoch: [13]  [1340/2809]  eta: 0:13:58  lr: 0.000040  min_lr: 0.000000  loss: 4.1426 (4.0036)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (57398.9799)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1085  max mem: 15572
Epoch: [13]  [1350/2809]  eta: 0:13:52  lr: 0.000040  min_lr: 0.000000  loss: 4.1497 (4.0034)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (57459.2095)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0839  max mem: 15572
Epoch: [13]  [1360/2809]  eta: 0:13:45  lr: 0.000040  min_lr: 0.000000  loss: 4.0611 (4.0045)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (57518.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0674  max mem: 15572
Epoch: [13]  [1370/2809]  eta: 0:13:40  lr: 0.000040  min_lr: 0.000000  loss: 4.2411 (4.0058)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (57577.0328)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0939  max mem: 15572
Epoch: [13]  [1380/2809]  eta: 0:13:35  lr: 0.000040  min_lr: 0.000000  loss: 4.1680 (4.0058)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (57634.6647)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1646  max mem: 15572
Epoch: [13]  [1390/2809]  eta: 0:13:29  lr: 0.000040  min_lr: 0.000000  loss: 4.0638 (4.0053)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (57691.4680)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1253  max mem: 15572
Epoch: [13]  [1400/2809]  eta: 0:13:24  lr: 0.000040  min_lr: 0.000000  loss: 4.0076 (4.0041)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (57747.4604)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1318  max mem: 15572
Epoch: [13]  [1410/2809]  eta: 0:13:18  lr: 0.000040  min_lr: 0.000000  loss: 4.0270 (4.0045)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (57802.6591)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1528  max mem: 15572
Epoch: [13]  [1420/2809]  eta: 0:13:12  lr: 0.000040  min_lr: 0.000000  loss: 3.9796 (4.0044)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (57857.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0909  max mem: 15572
Epoch: [13]  [1430/2809]  eta: 0:13:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9498 (4.0033)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (57910.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1034  max mem: 15572
[2025-01-15 20:54:04,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:54:04,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:54:04,998] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37950
[2025-01-15 20:54:04,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:54:04,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1440/2809]  eta: 0:13:01  lr: 0.000040  min_lr: 0.000000  loss: 3.9103 (4.0021)  class_acc: 0.2917 (0.2535)  loss_scale: 65536.0000 (58009.1381)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1530  max mem: 15572
Epoch: [13]  [1450/2809]  eta: 0:12:55  lr: 0.000040  min_lr: 0.000000  loss: 3.9374 (4.0027)  class_acc: 0.2917 (0.2532)  loss_scale: 65536.0000 (58061.0117)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1197  max mem: 15572
Epoch: [13]  [1460/2809]  eta: 0:12:49  lr: 0.000040  min_lr: 0.000000  loss: 4.0300 (4.0033)  class_acc: 0.2083 (0.2530)  loss_scale: 65536.0000 (58112.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0816  max mem: 15572
Epoch: [13]  [1470/2809]  eta: 0:12:44  lr: 0.000040  min_lr: 0.000000  loss: 4.0308 (4.0033)  class_acc: 0.2083 (0.2529)  loss_scale: 65536.0000 (58162.6431)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1361  max mem: 15572
Epoch: [13]  [1480/2809]  eta: 0:12:38  lr: 0.000040  min_lr: 0.000000  loss: 4.0308 (4.0039)  class_acc: 0.2083 (0.2528)  loss_scale: 65536.0000 (58212.4294)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1786  max mem: 15572
[2025-01-15 20:54:34,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=251, lr=[3.9089269712705475e-07, 3.9089269712705475e-07, 5.584181387529355e-07, 5.584181387529355e-07, 7.977401982184794e-07, 7.977401982184794e-07, 1.1396288545978277e-06, 1.1396288545978277e-06, 1.6280412208540395e-06, 1.6280412208540395e-06, 2.325773172648628e-06, 2.325773172648628e-06, 3.3225331037837544e-06, 3.3225331037837544e-06, 4.746475862548221e-06, 4.746475862548221e-06, 6.780679803640316e-06, 6.780679803640316e-06, 9.686685433771882e-06, 9.686685433771882e-06, 1.3838122048245544e-05, 1.3838122048245544e-05, 1.9768745783207923e-05, 1.9768745783207923e-05, 2.8241065404582747e-05, 2.8241065404582747e-05, 4.034437914940393e-05, 4.034437914940393e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 20:54:34,287] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=28.414910276657004, CurrSamplesPerSec=30.72900263749145, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [1490/2809]  eta: 0:12:33  lr: 0.000040  min_lr: 0.000000  loss: 3.9575 (4.0028)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (58261.5480)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1904  max mem: 15572
Epoch: [13]  [1500/2809]  eta: 0:12:28  lr: 0.000040  min_lr: 0.000000  loss: 3.9147 (4.0023)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (58310.0120)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1686  max mem: 15572
Epoch: [13]  [1510/2809]  eta: 0:12:22  lr: 0.000040  min_lr: 0.000000  loss: 4.0953 (4.0029)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (58357.8345)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1774  max mem: 15572
Epoch: [13]  [1520/2809]  eta: 0:12:17  lr: 0.000040  min_lr: 0.000000  loss: 4.1702 (4.0029)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (58405.0283)  weight_decay: 0.0500 (0.0500)  time: 0.6465  data: 0.2194  max mem: 15572
Epoch: [13]  [1530/2809]  eta: 0:12:11  lr: 0.000040  min_lr: 0.000000  loss: 4.0407 (4.0028)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (58451.6055)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1311  max mem: 15572
Epoch: [13]  [1540/2809]  eta: 0:12:06  lr: 0.000040  min_lr: 0.000000  loss: 3.9638 (4.0033)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (58497.5782)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1239  max mem: 15572
Epoch: [13]  [1550/2809]  eta: 0:12:01  lr: 0.000040  min_lr: 0.000000  loss: 3.9362 (4.0026)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (58542.9581)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.2065  max mem: 15572
Epoch: [13]  [1560/2809]  eta: 0:11:55  lr: 0.000040  min_lr: 0.000000  loss: 3.9261 (4.0022)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (58587.7566)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1681  max mem: 15572
[2025-01-15 20:55:21,787] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:55:21,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 20:55:23,038] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38082
[2025-01-15 20:55:23,038] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:55:23,038] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1570/2809]  eta: 0:11:49  lr: 0.000040  min_lr: 0.000000  loss: 4.0214 (4.0021)  class_acc: 0.2917 (0.2536)  loss_scale: 65536.0000 (58757.1330)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1022  max mem: 15572
Epoch: [13]  [1580/2809]  eta: 0:11:44  lr: 0.000040  min_lr: 0.000000  loss: 4.0214 (4.0031)  class_acc: 0.2917 (0.2537)  loss_scale: 65536.0000 (58800.0101)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1346  max mem: 15572
Epoch: [13]  [1590/2809]  eta: 0:11:38  lr: 0.000040  min_lr: 0.000000  loss: 4.0565 (4.0035)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (58842.3482)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1501  max mem: 15572
Epoch: [13]  [1600/2809]  eta: 0:11:32  lr: 0.000040  min_lr: 0.000000  loss: 4.0565 (4.0042)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (58884.1574)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1291  max mem: 15572
Epoch: [13]  [1610/2809]  eta: 0:11:27  lr: 0.000040  min_lr: 0.000000  loss: 4.1460 (4.0047)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (58925.4475)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1839  max mem: 15572
Epoch: [13]  [1620/2809]  eta: 0:11:21  lr: 0.000040  min_lr: 0.000000  loss: 4.0952 (4.0047)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (58966.2283)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1280  max mem: 15572
Epoch: [13]  [1630/2809]  eta: 0:11:16  lr: 0.000040  min_lr: 0.000000  loss: 4.0803 (4.0051)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (59006.5089)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1201  max mem: 15572
Epoch: [13]  [1640/2809]  eta: 0:11:09  lr: 0.000040  min_lr: 0.000000  loss: 4.0803 (4.0056)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (59046.2986)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1235  max mem: 15572
Epoch: [13]  [1650/2809]  eta: 0:11:03  lr: 0.000040  min_lr: 0.000000  loss: 4.1395 (4.0062)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (59085.6063)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0872  max mem: 15572
Epoch: [13]  [1660/2809]  eta: 0:10:58  lr: 0.000040  min_lr: 0.000000  loss: 3.9991 (4.0050)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (59124.4407)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1405  max mem: 15572
Epoch: [13]  [1670/2809]  eta: 0:10:53  lr: 0.000040  min_lr: 0.000000  loss: 3.9673 (4.0047)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (59162.8103)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1875  max mem: 15572
Epoch: [13]  [1680/2809]  eta: 0:10:47  lr: 0.000040  min_lr: 0.000000  loss: 4.0833 (4.0051)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (59200.7234)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.1827  max mem: 15572
Epoch: [13]  [1690/2809]  eta: 0:10:42  lr: 0.000040  min_lr: 0.000000  loss: 4.1930 (4.0056)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (59238.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1460  max mem: 15572
[2025-01-15 20:56:38,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:56:38,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1700/2809]  eta: 0:10:35  lr: 0.000040  min_lr: 0.000000  loss: 4.1335 (4.0062)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (59544.9077)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.1038  max mem: 15572
[2025-01-15 20:56:40,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38218
[2025-01-15 20:56:40,970] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:56:40,972] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1710/2809]  eta: 0:10:30  lr: 0.000040  min_lr: 0.000000  loss: 4.1263 (4.0062)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (59579.9229)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1194  max mem: 15572
Epoch: [13]  [1720/2809]  eta: 0:10:24  lr: 0.000040  min_lr: 0.000000  loss: 4.1238 (4.0072)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (59614.5311)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1538  max mem: 15572
Epoch: [13]  [1730/2809]  eta: 0:10:18  lr: 0.000040  min_lr: 0.000000  loss: 4.1034 (4.0080)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (59648.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1399  max mem: 15572
Epoch: [13]  [1740/2809]  eta: 0:10:12  lr: 0.000040  min_lr: 0.000000  loss: 4.0614 (4.0087)  class_acc: 0.2500 (0.2528)  loss_scale: 65536.0000 (59682.5549)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1394  max mem: 15572
Epoch: [13]  [1750/2809]  eta: 0:10:07  lr: 0.000040  min_lr: 0.000000  loss: 3.9950 (4.0087)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (59715.9840)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1166  max mem: 15572
Epoch: [13]  [1760/2809]  eta: 0:10:01  lr: 0.000040  min_lr: 0.000000  loss: 4.0262 (4.0097)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (59749.0335)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1311  max mem: 15572
Epoch: [13]  [1770/2809]  eta: 0:09:55  lr: 0.000040  min_lr: 0.000000  loss: 4.1718 (4.0105)  class_acc: 0.2500 (0.2528)  loss_scale: 65536.0000 (59781.7098)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1285  max mem: 15572
Epoch: [13]  [1780/2809]  eta: 0:09:49  lr: 0.000040  min_lr: 0.000000  loss: 4.0217 (4.0094)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (59814.0191)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1390  max mem: 15572
Epoch: [13]  [1790/2809]  eta: 0:09:44  lr: 0.000040  min_lr: 0.000000  loss: 3.9380 (4.0087)  class_acc: 0.2917 (0.2534)  loss_scale: 65536.0000 (59845.9676)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.1872  max mem: 15572
Epoch: [13]  [1800/2809]  eta: 0:09:38  lr: 0.000040  min_lr: 0.000000  loss: 3.8977 (4.0086)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (59877.5614)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1350  max mem: 15572
Epoch: [13]  [1810/2809]  eta: 0:09:32  lr: 0.000040  min_lr: 0.000000  loss: 3.7577 (4.0078)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (59908.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0751  max mem: 15572
Epoch: [13]  [1820/2809]  eta: 0:09:26  lr: 0.000040  min_lr: 0.000000  loss: 3.8441 (4.0073)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (59939.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1194  max mem: 15572
[2025-01-15 20:57:56,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:57:56,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1830/2809]  eta: 0:09:21  lr: 0.000040  min_lr: 0.000000  loss: 3.9567 (4.0073)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (60006.0644)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1623  max mem: 15572
[2025-01-15 20:57:56,419] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38348
[2025-01-15 20:57:56,419] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:57:56,419] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1840/2809]  eta: 0:09:15  lr: 0.000040  min_lr: 0.000000  loss: 4.0165 (4.0077)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (60036.1021)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1515  max mem: 15572
Epoch: [13]  [1850/2809]  eta: 0:09:10  lr: 0.000040  min_lr: 0.000000  loss: 3.9008 (4.0071)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (60065.8152)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1463  max mem: 15572
Epoch: [13]  [1860/2809]  eta: 0:09:04  lr: 0.000040  min_lr: 0.000000  loss: 4.0271 (4.0071)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (60095.2090)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1600  max mem: 15572
Epoch: [13]  [1870/2809]  eta: 0:08:58  lr: 0.000040  min_lr: 0.000000  loss: 3.9909 (4.0069)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (60124.2886)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0793  max mem: 15572
Epoch: [13]  [1880/2809]  eta: 0:08:51  lr: 0.000040  min_lr: 0.000000  loss: 3.9909 (4.0077)  class_acc: 0.1667 (0.2527)  loss_scale: 65536.0000 (60153.0590)  weight_decay: 0.0500 (0.0500)  time: 0.4466  data: 0.0134  max mem: 15572
Epoch: [13]  [1890/2809]  eta: 0:08:45  lr: 0.000040  min_lr: 0.000000  loss: 4.1157 (4.0080)  class_acc: 0.2083 (0.2526)  loss_scale: 65536.0000 (60181.5251)  weight_decay: 0.0500 (0.0500)  time: 0.4652  data: 0.0296  max mem: 15572
Epoch: [13]  [1900/2809]  eta: 0:08:40  lr: 0.000040  min_lr: 0.000000  loss: 4.1020 (4.0090)  class_acc: 0.2083 (0.2524)  loss_scale: 65536.0000 (60209.6917)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0815  max mem: 15572
Epoch: [13]  [1910/2809]  eta: 0:08:34  lr: 0.000040  min_lr: 0.000000  loss: 4.1020 (4.0096)  class_acc: 0.2083 (0.2524)  loss_scale: 65536.0000 (60237.5636)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1201  max mem: 15572
Epoch: [13]  [1920/2809]  eta: 0:08:28  lr: 0.000040  min_lr: 0.000000  loss: 4.0732 (4.0100)  class_acc: 0.2083 (0.2523)  loss_scale: 65536.0000 (60265.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1477  max mem: 15572
Epoch: [13]  [1930/2809]  eta: 0:08:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9842 (4.0099)  class_acc: 0.2083 (0.2525)  loss_scale: 65536.0000 (60292.4412)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1330  max mem: 15572
Epoch: [13]  [1940/2809]  eta: 0:08:16  lr: 0.000040  min_lr: 0.000000  loss: 3.9241 (4.0091)  class_acc: 0.2917 (0.2526)  loss_scale: 65536.0000 (60319.4560)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0906  max mem: 15572
Epoch: [13]  [1950/2809]  eta: 0:08:11  lr: 0.000040  min_lr: 0.000000  loss: 3.8373 (4.0089)  class_acc: 0.2500 (0.2525)  loss_scale: 65536.0000 (60346.1937)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1157  max mem: 15572
[2025-01-15 20:59:08,279] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 20:59:08,280] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [1960/2809]  eta: 0:08:05  lr: 0.000040  min_lr: 0.000000  loss: 3.9075 (4.0088)  class_acc: 0.2083 (0.2525)  loss_scale: 65536.0000 (60406.0785)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1883  max mem: 15572
[2025-01-15 20:59:11,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38479
[2025-01-15 20:59:11,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 20:59:11,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1970/2809]  eta: 0:08:00  lr: 0.000040  min_lr: 0.000000  loss: 3.9637 (4.0088)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (60465.3557)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.2251  max mem: 15572
Epoch: [13]  [1980/2809]  eta: 0:07:54  lr: 0.000040  min_lr: 0.000000  loss: 4.0835 (4.0081)  class_acc: 0.2917 (0.2526)  loss_scale: 65536.0000 (60490.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1513  max mem: 15572
Epoch: [13]  [1990/2809]  eta: 0:07:48  lr: 0.000040  min_lr: 0.000000  loss: 4.1981 (4.0093)  class_acc: 0.2083 (0.2522)  loss_scale: 65536.0000 (60516.2913)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.0975  max mem: 15572
Epoch: [13]  [2000/2809]  eta: 0:07:43  lr: 0.000040  min_lr: 0.000000  loss: 4.4193 (4.0109)  class_acc: 0.1250 (0.2517)  loss_scale: 65536.0000 (60541.3773)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1098  max mem: 15572
Epoch: [13]  [2010/2809]  eta: 0:07:37  lr: 0.000040  min_lr: 0.000000  loss: 4.2683 (4.0106)  class_acc: 0.1250 (0.2516)  loss_scale: 65536.0000 (60566.2138)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1002  max mem: 15572
Epoch: [13]  [2020/2809]  eta: 0:07:31  lr: 0.000040  min_lr: 0.000000  loss: 3.8816 (4.0107)  class_acc: 0.1667 (0.2514)  loss_scale: 65536.0000 (60590.8046)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1179  max mem: 15572
Epoch: [13]  [2030/2809]  eta: 0:07:25  lr: 0.000040  min_lr: 0.000000  loss: 3.9478 (4.0106)  class_acc: 0.2083 (0.2513)  loss_scale: 65536.0000 (60615.1531)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1353  max mem: 15572
Epoch: [13]  [2040/2809]  eta: 0:07:20  lr: 0.000040  min_lr: 0.000000  loss: 3.9368 (4.0101)  class_acc: 0.2500 (0.2516)  loss_scale: 65536.0000 (60639.2631)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1306  max mem: 15572
Epoch: [13]  [2050/2809]  eta: 0:07:14  lr: 0.000040  min_lr: 0.000000  loss: 3.9368 (4.0091)  class_acc: 0.2917 (0.2521)  loss_scale: 65536.0000 (60663.1380)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1444  max mem: 15572
Epoch: [13]  [2060/2809]  eta: 0:07:09  lr: 0.000040  min_lr: 0.000000  loss: 4.0049 (4.0099)  class_acc: 0.2917 (0.2521)  loss_scale: 65536.0000 (60686.7812)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.2007  max mem: 15572
Epoch: [13]  [2070/2809]  eta: 0:07:03  lr: 0.000040  min_lr: 0.000000  loss: 4.0296 (4.0092)  class_acc: 0.2500 (0.2524)  loss_scale: 65536.0000 (60710.1960)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.2201  max mem: 15572
Epoch: [13]  [2080/2809]  eta: 0:06:58  lr: 0.000040  min_lr: 0.000000  loss: 3.9033 (4.0086)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (60733.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1442  max mem: 15572
Epoch: [13]  [2090/2809]  eta: 0:06:52  lr: 0.000040  min_lr: 0.000000  loss: 4.0409 (4.0085)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (60756.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0718  max mem: 15572
[2025-01-15 21:00:24,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:00:24,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2100/2809]  eta: 0:06:46  lr: 0.000040  min_lr: 0.000000  loss: 4.0561 (4.0087)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (61091.0309)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0899  max mem: 15572
[2025-01-15 21:00:30,796] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38618
[2025-01-15 21:00:30,796] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:00:30,796] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2110/2809]  eta: 0:06:40  lr: 0.000040  min_lr: 0.000000  loss: 3.9598 (4.0086)  class_acc: 0.2500 (0.2528)  loss_scale: 65536.0000 (61112.0872)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1306  max mem: 15572
Epoch: [13]  [2120/2809]  eta: 0:06:34  lr: 0.000040  min_lr: 0.000000  loss: 4.0004 (4.0086)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (61132.9448)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0878  max mem: 15572
Epoch: [13]  [2130/2809]  eta: 0:06:29  lr: 0.000040  min_lr: 0.000000  loss: 3.9763 (4.0087)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (61153.6068)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1129  max mem: 15572
Epoch: [13]  [2140/2809]  eta: 0:06:23  lr: 0.000040  min_lr: 0.000000  loss: 3.9509 (4.0085)  class_acc: 0.2083 (0.2525)  loss_scale: 65536.0000 (61174.0757)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1489  max mem: 15572
Epoch: [13]  [2150/2809]  eta: 0:06:17  lr: 0.000040  min_lr: 0.000000  loss: 3.9509 (4.0076)  class_acc: 0.2500 (0.2528)  loss_scale: 65536.0000 (61194.3543)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1336  max mem: 15572
Epoch: [13]  [2160/2809]  eta: 0:06:11  lr: 0.000040  min_lr: 0.000000  loss: 3.9812 (4.0077)  class_acc: 0.2917 (0.2530)  loss_scale: 65536.0000 (61214.4452)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1416  max mem: 15572
Epoch: [13]  [2170/2809]  eta: 0:06:06  lr: 0.000040  min_lr: 0.000000  loss: 4.0858 (4.0076)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (61234.3510)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1352  max mem: 15572
Epoch: [13]  [2180/2809]  eta: 0:06:00  lr: 0.000040  min_lr: 0.000000  loss: 4.0858 (4.0072)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61254.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1126  max mem: 15572
Epoch: [13]  [2190/2809]  eta: 0:05:54  lr: 0.000040  min_lr: 0.000000  loss: 4.0388 (4.0078)  class_acc: 0.2917 (0.2532)  loss_scale: 65536.0000 (61273.6175)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1000  max mem: 15572
Epoch: [13]  [2200/2809]  eta: 0:05:48  lr: 0.000040  min_lr: 0.000000  loss: 4.0345 (4.0072)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61292.9832)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0794  max mem: 15572
Epoch: [13]  [2210/2809]  eta: 0:05:42  lr: 0.000040  min_lr: 0.000000  loss: 3.9333 (4.0076)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61312.1737)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0944  max mem: 15572
Epoch: [13]  [2220/2809]  eta: 0:05:37  lr: 0.000040  min_lr: 0.000000  loss: 4.1627 (4.0087)  class_acc: 0.2083 (0.2530)  loss_scale: 65536.0000 (61331.1914)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1801  max mem: 15572
[2025-01-15 21:01:42,988] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:01:42,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [2230/2809]  eta: 0:05:31  lr: 0.000040  min_lr: 0.000000  loss: 4.1112 (4.0080)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (61379.4137)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1346  max mem: 15572
[2025-01-15 21:01:45,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38749
[2025-01-15 21:01:45,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:01:45,562] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2240/2809]  eta: 0:05:25  lr: 0.000040  min_lr: 0.000000  loss: 4.0684 (4.0086)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (61427.2057)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0937  max mem: 15572
Epoch: [13]  [2250/2809]  eta: 0:05:19  lr: 0.000040  min_lr: 0.000000  loss: 4.0684 (4.0083)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (61445.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1173  max mem: 15572
Epoch: [13]  [2260/2809]  eta: 0:05:14  lr: 0.000040  min_lr: 0.000000  loss: 3.9192 (4.0074)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61463.5506)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1009  max mem: 15572
Epoch: [13]  [2270/2809]  eta: 0:05:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9196 (4.0071)  class_acc: 0.2917 (0.2535)  loss_scale: 65536.0000 (61481.4830)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1439  max mem: 15572
Epoch: [13]  [2280/2809]  eta: 0:05:02  lr: 0.000040  min_lr: 0.000000  loss: 4.2050 (4.0077)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (61499.2582)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1222  max mem: 15572
Epoch: [13]  [2290/2809]  eta: 0:04:56  lr: 0.000040  min_lr: 0.000000  loss: 4.1842 (4.0076)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (61516.8782)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0580  max mem: 15572
Epoch: [13]  [2300/2809]  eta: 0:04:51  lr: 0.000040  min_lr: 0.000000  loss: 3.8255 (4.0068)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (61534.3451)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0733  max mem: 15572
Epoch: [13]  [2310/2809]  eta: 0:04:45  lr: 0.000040  min_lr: 0.000000  loss: 3.6110 (4.0056)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61551.6608)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1743  max mem: 15572
Epoch: [13]  [2320/2809]  eta: 0:04:39  lr: 0.000040  min_lr: 0.000000  loss: 3.7847 (4.0057)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61568.8272)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1976  max mem: 15572
Epoch: [13]  [2330/2809]  eta: 0:04:34  lr: 0.000040  min_lr: 0.000000  loss: 3.9102 (4.0053)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61585.8464)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1413  max mem: 15572
Epoch: [13]  [2340/2809]  eta: 0:04:28  lr: 0.000040  min_lr: 0.000000  loss: 3.8959 (4.0047)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (61602.7202)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1160  max mem: 15572
Epoch: [13]  [2350/2809]  eta: 0:04:22  lr: 0.000040  min_lr: 0.000000  loss: 3.8625 (4.0050)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (61619.4504)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1183  max mem: 15572
Epoch: [13]  [2360/2809]  eta: 0:04:17  lr: 0.000040  min_lr: 0.000000  loss: 4.1676 (4.0057)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (61636.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1425  max mem: 15572
[2025-01-15 21:02:57,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:02:57,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:02:58,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38880
[2025-01-15 21:02:58,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:02:58,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2370/2809]  eta: 0:04:11  lr: 0.000040  min_lr: 0.000000  loss: 4.2407 (4.0063)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61707.7689)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0759  max mem: 15572
Epoch: [13]  [2380/2809]  eta: 0:04:05  lr: 0.000040  min_lr: 0.000000  loss: 4.1295 (4.0067)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61723.8471)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0546  max mem: 15572
Epoch: [13]  [2390/2809]  eta: 0:03:59  lr: 0.000040  min_lr: 0.000000  loss: 4.0982 (4.0072)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (61739.7909)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1626  max mem: 15572
Epoch: [13]  [2400/2809]  eta: 0:03:54  lr: 0.000040  min_lr: 0.000000  loss: 4.0757 (4.0076)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (61755.6018)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1574  max mem: 15572
Epoch: [13]  [2410/2809]  eta: 0:03:48  lr: 0.000040  min_lr: 0.000000  loss: 4.0519 (4.0073)  class_acc: 0.2083 (0.2530)  loss_scale: 65536.0000 (61771.2816)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0969  max mem: 15572
Epoch: [13]  [2420/2809]  eta: 0:03:42  lr: 0.000040  min_lr: 0.000000  loss: 3.9469 (4.0072)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (61786.8319)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0945  max mem: 15572
Epoch: [13]  [2430/2809]  eta: 0:03:36  lr: 0.000040  min_lr: 0.000000  loss: 3.9469 (4.0072)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (61802.2542)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1200  max mem: 15572
Epoch: [13]  [2440/2809]  eta: 0:03:31  lr: 0.000040  min_lr: 0.000000  loss: 4.1394 (4.0076)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (61817.5502)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1179  max mem: 15572
Epoch: [13]  [2450/2809]  eta: 0:03:25  lr: 0.000040  min_lr: 0.000000  loss: 4.1026 (4.0071)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (61832.7213)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0846  max mem: 15572
Epoch: [13]  [2460/2809]  eta: 0:03:19  lr: 0.000040  min_lr: 0.000000  loss: 3.9304 (4.0072)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (61847.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1312  max mem: 15572
Epoch: [13]  [2470/2809]  eta: 0:03:13  lr: 0.000040  min_lr: 0.000000  loss: 4.0162 (4.0074)  class_acc: 0.2917 (0.2532)  loss_scale: 65536.0000 (61862.6953)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1193  max mem: 15572
Epoch: [13]  [2480/2809]  eta: 0:03:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9548 (4.0073)  class_acc: 0.2917 (0.2533)  loss_scale: 65536.0000 (61877.5010)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1009  max mem: 15572
[2025-01-15 21:04:07,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=258, lr=[3.8578753243206146e-07, 3.8578753243206146e-07, 5.511250463315164e-07, 5.511250463315164e-07, 7.873214947593093e-07, 7.873214947593093e-07, 1.1247449925132992e-06, 1.1247449925132992e-06, 1.6067785607332844e-06, 1.6067785607332844e-06, 2.295397943904692e-06, 2.295397943904692e-06, 3.2791399198638463e-06, 3.2791399198638463e-06, 4.684485599805495e-06, 4.684485599805495e-06, 6.692122285436421e-06, 6.692122285436421e-06, 9.560174693480603e-06, 9.560174693480603e-06, 1.3657392419258004e-05, 1.3657392419258004e-05, 1.951056059894001e-05, 1.951056059894001e-05, 2.7872229427057156e-05, 2.7872229427057156e-05, 3.9817470610081654e-05, 3.9817470610081654e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:04:07,912] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=28.42106013778916, CurrSamplesPerSec=21.56584780828662, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [13]  [2490/2809]  eta: 0:03:02  lr: 0.000040  min_lr: 0.000000  loss: 3.8249 (4.0072)  class_acc: 0.2917 (0.2534)  loss_scale: 65536.0000 (61892.1879)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.1735  max mem: 15572
[2025-01-15 21:04:12,763] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:04:12,764] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:04:13,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39012
[2025-01-15 21:04:13,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:04:13,967] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2500/2809]  eta: 0:02:56  lr: 0.000040  min_lr: 0.000000  loss: 3.8249 (4.0068)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61985.3691)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1157  max mem: 15572
Epoch: [13]  [2510/2809]  eta: 0:02:51  lr: 0.000040  min_lr: 0.000000  loss: 4.0641 (4.0073)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (61999.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1265  max mem: 15572
Epoch: [13]  [2520/2809]  eta: 0:02:45  lr: 0.000040  min_lr: 0.000000  loss: 4.3396 (4.0078)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (62013.5375)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1771  max mem: 15572
Epoch: [13]  [2530/2809]  eta: 0:02:39  lr: 0.000040  min_lr: 0.000000  loss: 4.2013 (4.0084)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (62027.4548)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1660  max mem: 15572
Epoch: [13]  [2540/2809]  eta: 0:02:33  lr: 0.000040  min_lr: 0.000000  loss: 4.0080 (4.0072)  class_acc: 0.3333 (0.2539)  loss_scale: 65536.0000 (62041.2625)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1402  max mem: 15572
Epoch: [13]  [2550/2809]  eta: 0:02:28  lr: 0.000040  min_lr: 0.000000  loss: 4.0080 (4.0074)  class_acc: 0.2917 (0.2537)  loss_scale: 65536.0000 (62054.9620)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0907  max mem: 15572
Epoch: [13]  [2560/2809]  eta: 0:02:22  lr: 0.000040  min_lr: 0.000000  loss: 4.0171 (4.0072)  class_acc: 0.2083 (0.2538)  loss_scale: 65536.0000 (62068.5545)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1368  max mem: 15572
Epoch: [13]  [2570/2809]  eta: 0:02:16  lr: 0.000040  min_lr: 0.000000  loss: 3.9546 (4.0069)  class_acc: 0.2917 (0.2540)  loss_scale: 65536.0000 (62082.0412)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1165  max mem: 15572
Epoch: [13]  [2580/2809]  eta: 0:02:11  lr: 0.000040  min_lr: 0.000000  loss: 3.9129 (4.0070)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (62095.4235)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0923  max mem: 15572
Epoch: [13]  [2590/2809]  eta: 0:02:05  lr: 0.000040  min_lr: 0.000000  loss: 4.1600 (4.0078)  class_acc: 0.2083 (0.2537)  loss_scale: 65536.0000 (62108.7024)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1056  max mem: 15572
Epoch: [13]  [2600/2809]  eta: 0:01:59  lr: 0.000040  min_lr: 0.000000  loss: 4.2584 (4.0080)  class_acc: 0.1667 (0.2534)  loss_scale: 65536.0000 (62121.8793)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0704  max mem: 15572
Epoch: [13]  [2610/2809]  eta: 0:01:53  lr: 0.000040  min_lr: 0.000000  loss: 4.1160 (4.0081)  class_acc: 0.1667 (0.2532)  loss_scale: 65536.0000 (62134.9552)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0852  max mem: 15572
Epoch: [13]  [2620/2809]  eta: 0:01:48  lr: 0.000040  min_lr: 0.000000  loss: 4.1160 (4.0077)  class_acc: 0.1667 (0.2532)  loss_scale: 65536.0000 (62147.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1329  max mem: 15572
[2025-01-15 21:05:28,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:05:28,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:05:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39147
[2025-01-15 21:05:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:05:30,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2630/2809]  eta: 0:01:42  lr: 0.000040  min_lr: 0.000000  loss: 4.0301 (4.0076)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (62310.2638)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1456  max mem: 15572
Epoch: [13]  [2640/2809]  eta: 0:01:36  lr: 0.000040  min_lr: 0.000000  loss: 3.8812 (4.0068)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (62322.4778)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1485  max mem: 15572
Epoch: [13]  [2650/2809]  eta: 0:01:30  lr: 0.000040  min_lr: 0.000000  loss: 3.8282 (4.0068)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (62334.5998)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.1558  max mem: 15572
Epoch: [13]  [2660/2809]  eta: 0:01:25  lr: 0.000040  min_lr: 0.000000  loss: 4.1249 (4.0073)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (62346.6306)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0973  max mem: 15572
Epoch: [13]  [2670/2809]  eta: 0:01:19  lr: 0.000040  min_lr: 0.000000  loss: 4.1249 (4.0069)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (62358.5713)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1101  max mem: 15572
Epoch: [13]  [2680/2809]  eta: 0:01:13  lr: 0.000040  min_lr: 0.000000  loss: 3.9035 (4.0070)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (62370.4230)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1600  max mem: 15572
Epoch: [13]  [2690/2809]  eta: 0:01:08  lr: 0.000040  min_lr: 0.000000  loss: 3.9035 (4.0068)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (62382.1865)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1036  max mem: 15572
Epoch: [13]  [2700/2809]  eta: 0:01:02  lr: 0.000040  min_lr: 0.000000  loss: 4.1715 (4.0076)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (62393.8630)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1229  max mem: 15572
Epoch: [13]  [2710/2809]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000000  loss: 4.0471 (4.0070)  class_acc: 0.2917 (0.2534)  loss_scale: 65536.0000 (62405.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1224  max mem: 15572
Epoch: [13]  [2720/2809]  eta: 0:00:50  lr: 0.000040  min_lr: 0.000000  loss: 3.9124 (4.0067)  class_acc: 0.2917 (0.2535)  loss_scale: 65536.0000 (62416.9585)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.0984  max mem: 15572
Epoch: [13]  [2730/2809]  eta: 0:00:45  lr: 0.000040  min_lr: 0.000000  loss: 3.8806 (4.0061)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (62428.3793)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1130  max mem: 15572
Epoch: [13]  [2740/2809]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000000  loss: 3.8422 (4.0067)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (62439.7169)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1254  max mem: 15572
Epoch: [13]  [2750/2809]  eta: 0:00:33  lr: 0.000040  min_lr: 0.000000  loss: 3.9414 (4.0065)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (62450.9720)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0765  max mem: 15572
[2025-01-15 21:06:44,479] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:06:44,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:06:44,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39277
[2025-01-15 21:06:44,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:06:44,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [2760/2809]  eta: 0:00:28  lr: 0.000040  min_lr: 0.000000  loss: 4.1028 (4.0069)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (62485.8819)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0318  max mem: 15572
Epoch: [13]  [2770/2809]  eta: 0:00:22  lr: 0.000040  min_lr: 0.000000  loss: 3.9489 (4.0066)  class_acc: 0.2083 (0.2534)  loss_scale: 65536.0000 (62496.8892)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.0485  max mem: 15572
[2025-01-15 21:06:52,204] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39290
[2025-01-15 21:06:52,204] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:06:52,204] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [2780/2809]  eta: 0:00:16  lr: 0.000040  min_lr: 0.000000  loss: 3.7494 (4.0050)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (62413.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0333  max mem: 15572
Epoch: [13]  [2790/2809]  eta: 0:00:10  lr: 0.000040  min_lr: 0.000000  loss: 3.9100 (4.0053)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (62307.3364)  weight_decay: 0.0500 (0.0500)  time: 0.4948  data: 0.0470  max mem: 15572
Epoch: [13]  [2800/2809]  eta: 0:00:05  lr: 0.000040  min_lr: 0.000000  loss: 4.0128 (4.0049)  class_acc: 0.2500 (0.2537)  loss_scale: 32768.0000 (62201.8765)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1139  max mem: 15572
Epoch: [13]  [2808/2809]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000000  loss: 3.9100 (4.0046)  class_acc: 0.2917 (0.2538)  loss_scale: 32768.0000 (62118.0491)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0833  max mem: 15572
Epoch: [13] Total time: 0:26:44 (0.5712 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000000  loss: 3.9100 (4.0046)  class_acc: 0.2917 (0.2538)  loss_scale: 32768.0000 (62118.0491)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:23  loss: 0.4769 (0.4769)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.0560  data: 3.8671  max mem: 15572
Val:  [ 10/272]  eta: 0:03:10  loss: 3.1260 (2.8315)  acc1: 16.6667 (29.7980)  acc5: 55.5556 (54.0404)  time: 0.7254  data: 0.5244  max mem: 15572
Val:  [ 20/272]  eta: 0:02:15  loss: 3.0509 (2.8022)  acc1: 27.7778 (32.8042)  acc5: 61.1111 (60.0529)  time: 0.3636  data: 0.1672  max mem: 15572
Val:  [ 30/272]  eta: 0:01:52  loss: 3.0109 (2.9060)  acc1: 27.7778 (28.6738)  acc5: 61.1111 (60.5735)  time: 0.3226  data: 0.1260  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 2.8158 (2.8831)  acc1: 22.2222 (28.4553)  acc5: 72.2222 (63.4146)  time: 0.2708  data: 0.0744  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.7045 (2.8210)  acc1: 33.3333 (30.3922)  acc5: 72.2222 (65.7952)  time: 0.2600  data: 0.0705  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.8839 (2.6766)  acc1: 55.5556 (34.8816)  acc5: 77.7778 (67.5774)  time: 0.3116  data: 0.1126  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.8030 (2.5828)  acc1: 61.1111 (37.1674)  acc5: 83.3333 (70.1878)  time: 0.3296  data: 0.1184  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.3522 (2.5880)  acc1: 44.4444 (37.5857)  acc5: 77.7778 (69.8903)  time: 0.3133  data: 0.1152  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 3.0658 (2.6510)  acc1: 27.7778 (36.8132)  acc5: 66.6667 (68.8034)  time: 0.3330  data: 0.1456  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 3.0658 (2.7002)  acc1: 27.7778 (36.4136)  acc5: 66.6667 (68.1518)  time: 0.3231  data: 0.1329  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 3.1366 (2.7746)  acc1: 11.1111 (34.2843)  acc5: 61.1111 (66.7167)  time: 0.3041  data: 0.1170  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.2499 (2.8134)  acc1: 11.1111 (33.2874)  acc5: 55.5556 (65.9780)  time: 0.3221  data: 0.1442  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.8192 (2.7708)  acc1: 27.7778 (34.7328)  acc5: 66.6667 (66.4546)  time: 0.2733  data: 0.0843  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 2.0728 (2.7566)  acc1: 44.4444 (35.3822)  acc5: 72.2222 (66.4303)  time: 0.3074  data: 0.1110  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.9139 (2.7744)  acc1: 27.7778 (34.5475)  acc5: 61.1111 (66.5195)  time: 0.3716  data: 0.1889  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.7889 (2.7551)  acc1: 33.3333 (35.5072)  acc5: 72.2222 (67.2533)  time: 0.3360  data: 0.1474  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6577 (2.7793)  acc1: 38.8889 (34.6979)  acc5: 72.2222 (66.6667)  time: 0.3222  data: 0.1307  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.7120 (2.7720)  acc1: 22.2222 (34.6532)  acc5: 66.6667 (66.9429)  time: 0.3546  data: 0.1676  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7133 (2.8127)  acc1: 22.2222 (33.5660)  acc5: 66.6667 (65.7941)  time: 0.3232  data: 0.1361  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.7587 (2.8207)  acc1: 22.2222 (33.1399)  acc5: 66.6667 (65.8928)  time: 0.2808  data: 0.1061  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.4472 (2.8171)  acc1: 38.8889 (33.7546)  acc5: 77.7778 (66.0874)  time: 0.2534  data: 0.0922  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.6080 (2.8054)  acc1: 50.0000 (33.9869)  acc5: 77.7778 (66.1890)  time: 0.1845  data: 0.0265  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9974 (2.7629)  acc1: 55.5556 (35.4016)  acc5: 77.7778 (66.8831)  time: 0.1665  data: 0.0005  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 1.8493 (2.7395)  acc1: 50.0000 (35.7308)  acc5: 88.8889 (67.4274)  time: 0.1764  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.7572 (2.7560)  acc1: 38.8889 (35.4360)  acc5: 66.6667 (67.0208)  time: 0.1880  data: 0.0007  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.5781 (2.6869)  acc1: 72.2222 (37.4628)  acc5: 83.3333 (67.9651)  time: 0.2547  data: 0.0623  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5448 (2.6840)  acc1: 66.6667 (37.3104)  acc5: 88.8889 (68.1017)  time: 0.2778  data: 0.0998  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5448 (2.6877)  acc1: 55.5556 (37.3131)  acc5: 88.8889 (68.0729)  time: 0.2723  data: 0.0997  max mem: 15572
Val: Total time: 0:01:23 (0.3056 s / it)
* Acc@1 37.313 Acc@5 68.073 loss 2.688
Accuracy of the network on the 4883 val videos: 37.3%
[2025-01-15 21:08:32,764] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 21:08:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 21:08:32,773] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 21:08:36,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 21:08:36,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.31%
Epoch: [14]  [   0/2809]  eta: 8:17:15  lr: 0.000040  min_lr: 0.000000  loss: 4.0551 (4.0551)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 10.6215  data: 10.2010  max mem: 15572
Epoch: [14]  [  10/2809]  eta: 1:16:37  lr: 0.000040  min_lr: 0.000000  loss: 4.1947 (4.0935)  class_acc: 0.2083 (0.1894)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6425  data: 1.1842  max mem: 15572
Epoch: [14]  [  20/2809]  eta: 0:55:08  lr: 0.000040  min_lr: 0.000000  loss: 3.7395 (3.9202)  class_acc: 0.2500 (0.2560)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.2626  max mem: 15572
Epoch: [14]  [  30/2809]  eta: 0:47:38  lr: 0.000040  min_lr: 0.000000  loss: 3.8152 (3.9717)  class_acc: 0.2083 (0.2406)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.2324  max mem: 15572
Epoch: [14]  [  40/2809]  eta: 0:43:16  lr: 0.000040  min_lr: 0.000000  loss: 4.0293 (4.0056)  class_acc: 0.2083 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.2108  max mem: 15572
Epoch: [14]  [  50/2809]  eta: 0:40:53  lr: 0.000040  min_lr: 0.000000  loss: 3.9605 (3.9362)  class_acc: 0.2917 (0.2631)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.2199  max mem: 15572
Epoch: [14]  [  60/2809]  eta: 0:38:22  lr: 0.000040  min_lr: 0.000000  loss: 3.8552 (3.9348)  class_acc: 0.2917 (0.2643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.1679  max mem: 15572
Epoch: [14]  [  70/2809]  eta: 0:37:46  lr: 0.000040  min_lr: 0.000000  loss: 4.0191 (3.9493)  class_acc: 0.2500 (0.2647)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.2160  max mem: 15572
Epoch: [14]  [  80/2809]  eta: 0:36:40  lr: 0.000040  min_lr: 0.000000  loss: 4.2097 (3.9888)  class_acc: 0.2083 (0.2521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.2472  max mem: 15572
Epoch: [14]  [  90/2809]  eta: 0:34:26  lr: 0.000040  min_lr: 0.000000  loss: 4.1387 (3.9846)  class_acc: 0.2083 (0.2550)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0791  max mem: 15572
[2025-01-15 21:09:46,621] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:09:46,621] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 100/2809]  eta: 0:32:40  lr: 0.000040  min_lr: 0.000000  loss: 4.0392 (3.9997)  class_acc: 0.2500 (0.2541)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3898  data: 0.0003  max mem: 15572
Epoch: [14]  [ 110/2809]  eta: 0:31:30  lr: 0.000040  min_lr: 0.000000  loss: 4.0988 (4.0026)  class_acc: 0.2083 (0.2511)  loss_scale: 65536.0000 (38081.7297)  weight_decay: 0.0500 (0.0500)  time: 0.4295  data: 0.0005  max mem: 15572
Epoch: [14]  [ 120/2809]  eta: 0:30:31  lr: 0.000040  min_lr: 0.000000  loss: 4.1193 (4.0060)  class_acc: 0.2500 (0.2541)  loss_scale: 65536.0000 (40350.6777)  weight_decay: 0.0500 (0.0500)  time: 0.4644  data: 0.0006  max mem: 15572
Epoch: [14]  [ 130/2809]  eta: 0:29:38  lr: 0.000040  min_lr: 0.000000  loss: 4.1552 (4.0133)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (42273.2214)  weight_decay: 0.0500 (0.0500)  time: 0.4611  data: 0.0007  max mem: 15572
Epoch: [14]  [ 140/2809]  eta: 0:28:54  lr: 0.000040  min_lr: 0.000000  loss: 4.1200 (4.0215)  class_acc: 0.1667 (0.2482)  loss_scale: 65536.0000 (43923.0638)  weight_decay: 0.0500 (0.0500)  time: 0.4609  data: 0.0108  max mem: 15572
Epoch: [14]  [ 150/2809]  eta: 0:28:33  lr: 0.000040  min_lr: 0.000000  loss: 3.9447 (4.0231)  class_acc: 0.2083 (0.2470)  loss_scale: 65536.0000 (45354.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0528  max mem: 15572
Epoch: [14]  [ 160/2809]  eta: 0:28:11  lr: 0.000040  min_lr: 0.000000  loss: 3.9443 (4.0211)  class_acc: 0.2500 (0.2469)  loss_scale: 65536.0000 (46607.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0924  max mem: 15572
Epoch: [14]  [ 170/2809]  eta: 0:27:41  lr: 0.000040  min_lr: 0.000000  loss: 3.8663 (4.0124)  class_acc: 0.2917 (0.2522)  loss_scale: 65536.0000 (47714.8070)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0680  max mem: 15572
Epoch: [14]  [ 180/2809]  eta: 0:27:25  lr: 0.000040  min_lr: 0.000000  loss: 4.0484 (4.0172)  class_acc: 0.2500 (0.2502)  loss_scale: 65536.0000 (48699.4033)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0683  max mem: 15572
Epoch: [14]  [ 190/2809]  eta: 0:27:14  lr: 0.000040  min_lr: 0.000000  loss: 4.1496 (4.0268)  class_acc: 0.2083 (0.2476)  loss_scale: 65536.0000 (49580.9005)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1114  max mem: 15572
Epoch: [14]  [ 200/2809]  eta: 0:27:04  lr: 0.000040  min_lr: 0.000000  loss: 4.0709 (4.0317)  class_acc: 0.2083 (0.2444)  loss_scale: 65536.0000 (50374.6866)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1574  max mem: 15572
Epoch: [14]  [ 210/2809]  eta: 0:26:50  lr: 0.000040  min_lr: 0.000000  loss: 4.0950 (4.0293)  class_acc: 0.2083 (0.2445)  loss_scale: 65536.0000 (51093.2322)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1527  max mem: 15572
Epoch: [14]  [ 220/2809]  eta: 0:26:44  lr: 0.000040  min_lr: 0.000000  loss: 3.6714 (4.0165)  class_acc: 0.2917 (0.2474)  loss_scale: 65536.0000 (51746.7511)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1448  max mem: 15572
[2025-01-15 21:10:53,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:10:53,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:10:56,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39551
[2025-01-15 21:10:56,549] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:10:56,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 230/2809]  eta: 0:26:29  lr: 0.000040  min_lr: 0.000000  loss: 3.6776 (4.0082)  class_acc: 0.2917 (0.2486)  loss_scale: 65536.0000 (53478.5108)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1437  max mem: 15572
Epoch: [14]  [ 240/2809]  eta: 0:26:11  lr: 0.000040  min_lr: 0.000000  loss: 4.0485 (4.0079)  class_acc: 0.2083 (0.2486)  loss_scale: 65536.0000 (53978.8216)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0891  max mem: 15572
Epoch: [14]  [ 250/2809]  eta: 0:26:04  lr: 0.000040  min_lr: 0.000000  loss: 4.1049 (4.0104)  class_acc: 0.2500 (0.2488)  loss_scale: 65536.0000 (54439.2669)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1346  max mem: 15572
Epoch: [14]  [ 260/2809]  eta: 0:26:00  lr: 0.000040  min_lr: 0.000000  loss: 4.1910 (4.0162)  class_acc: 0.2500 (0.2487)  loss_scale: 65536.0000 (54864.4291)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.2012  max mem: 15572
Epoch: [14]  [ 270/2809]  eta: 0:25:42  lr: 0.000039  min_lr: 0.000000  loss: 4.1939 (4.0179)  class_acc: 0.2500 (0.2505)  loss_scale: 65536.0000 (55258.2140)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1267  max mem: 15572
Epoch: [14]  [ 280/2809]  eta: 0:25:37  lr: 0.000039  min_lr: 0.000000  loss: 4.1226 (4.0203)  class_acc: 0.2083 (0.2481)  loss_scale: 65536.0000 (55623.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1179  max mem: 15572
Epoch: [14]  [ 290/2809]  eta: 0:25:29  lr: 0.000039  min_lr: 0.000000  loss: 4.0835 (4.0231)  class_acc: 0.2083 (0.2477)  loss_scale: 65536.0000 (55964.5911)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1713  max mem: 15572
Epoch: [14]  [ 300/2809]  eta: 0:25:20  lr: 0.000039  min_lr: 0.000000  loss: 4.0122 (4.0204)  class_acc: 0.2917 (0.2478)  loss_scale: 65536.0000 (56282.5781)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1444  max mem: 15572
Epoch: [14]  [ 310/2809]  eta: 0:25:12  lr: 0.000039  min_lr: 0.000000  loss: 4.0357 (4.0244)  class_acc: 0.2500 (0.2473)  loss_scale: 65536.0000 (56580.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1360  max mem: 15572
Epoch: [14]  [ 320/2809]  eta: 0:25:03  lr: 0.000039  min_lr: 0.000000  loss: 4.1282 (4.0277)  class_acc: 0.2083 (0.2479)  loss_scale: 65536.0000 (56859.1153)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1333  max mem: 15572
Epoch: [14]  [ 330/2809]  eta: 0:24:52  lr: 0.000039  min_lr: 0.000000  loss: 4.1802 (4.0298)  class_acc: 0.2083 (0.2479)  loss_scale: 65536.0000 (57121.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1084  max mem: 15572
Epoch: [14]  [ 340/2809]  eta: 0:24:49  lr: 0.000039  min_lr: 0.000000  loss: 4.1251 (4.0297)  class_acc: 0.2500 (0.2471)  loss_scale: 65536.0000 (57368.0235)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1445  max mem: 15572
Epoch: [14]  [ 350/2809]  eta: 0:24:41  lr: 0.000039  min_lr: 0.000000  loss: 4.1508 (4.0340)  class_acc: 0.2083 (0.2453)  loss_scale: 65536.0000 (57600.7293)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1629  max mem: 15572
[2025-01-15 21:12:11,607] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:12:11,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:12:13,053] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39683
[2025-01-15 21:12:13,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:12:13,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 360/2809]  eta: 0:24:38  lr: 0.000039  min_lr: 0.000000  loss: 4.0252 (4.0336)  class_acc: 0.2083 (0.2457)  loss_scale: 65536.0000 (58365.1634)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1636  max mem: 15572
Epoch: [14]  [ 370/2809]  eta: 0:24:30  lr: 0.000039  min_lr: 0.000000  loss: 4.0107 (4.0352)  class_acc: 0.2083 (0.2445)  loss_scale: 65536.0000 (58558.4474)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1527  max mem: 15572
Epoch: [14]  [ 380/2809]  eta: 0:24:15  lr: 0.000039  min_lr: 0.000000  loss: 3.9316 (4.0273)  class_acc: 0.2500 (0.2462)  loss_scale: 65536.0000 (58741.5853)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0544  max mem: 15572
Epoch: [14]  [ 390/2809]  eta: 0:24:05  lr: 0.000039  min_lr: 0.000000  loss: 3.9236 (4.0269)  class_acc: 0.2500 (0.2469)  loss_scale: 65536.0000 (58915.3555)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0564  max mem: 15572
Epoch: [14]  [ 400/2809]  eta: 0:23:58  lr: 0.000039  min_lr: 0.000000  loss: 4.0864 (4.0257)  class_acc: 0.2500 (0.2476)  loss_scale: 65536.0000 (59080.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1185  max mem: 15572
Epoch: [14]  [ 410/2809]  eta: 0:23:49  lr: 0.000039  min_lr: 0.000000  loss: 4.0260 (4.0208)  class_acc: 0.2917 (0.2481)  loss_scale: 65536.0000 (59237.5280)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1200  max mem: 15572
Epoch: [14]  [ 420/2809]  eta: 0:23:43  lr: 0.000039  min_lr: 0.000000  loss: 3.9788 (4.0213)  class_acc: 0.2500 (0.2487)  loss_scale: 65536.0000 (59387.1354)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1344  max mem: 15572
Epoch: [14]  [ 430/2809]  eta: 0:23:38  lr: 0.000039  min_lr: 0.000000  loss: 3.9788 (4.0184)  class_acc: 0.2500 (0.2493)  loss_scale: 65536.0000 (59529.8005)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1616  max mem: 15572
Epoch: [14]  [ 440/2809]  eta: 0:23:29  lr: 0.000039  min_lr: 0.000000  loss: 3.9039 (4.0177)  class_acc: 0.2500 (0.2498)  loss_scale: 65536.0000 (59665.9955)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1461  max mem: 15572
Epoch: [14]  [ 450/2809]  eta: 0:23:22  lr: 0.000039  min_lr: 0.000000  loss: 3.9039 (4.0151)  class_acc: 0.2500 (0.2504)  loss_scale: 65536.0000 (59796.1508)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1252  max mem: 15572
Epoch: [14]  [ 460/2809]  eta: 0:23:16  lr: 0.000039  min_lr: 0.000000  loss: 4.0017 (4.0103)  class_acc: 0.2917 (0.2521)  loss_scale: 65536.0000 (59920.6594)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1338  max mem: 15572
Epoch: [14]  [ 470/2809]  eta: 0:23:09  lr: 0.000039  min_lr: 0.000000  loss: 4.0195 (4.0100)  class_acc: 0.2917 (0.2524)  loss_scale: 65536.0000 (60039.8811)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1448  max mem: 15572
Epoch: [14]  [ 480/2809]  eta: 0:23:04  lr: 0.000039  min_lr: 0.000000  loss: 4.0193 (4.0082)  class_acc: 0.2500 (0.2532)  loss_scale: 65536.0000 (60154.1455)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1687  max mem: 15572
[2025-01-15 21:13:27,109] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:13:27,110] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:13:28,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39816
[2025-01-15 21:13:28,899] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:13:28,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 490/2809]  eta: 0:23:01  lr: 0.000039  min_lr: 0.000000  loss: 4.1298 (4.0132)  class_acc: 0.2083 (0.2520)  loss_scale: 65536.0000 (60797.6538)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1968  max mem: 15572
Epoch: [14]  [ 500/2809]  eta: 0:22:50  lr: 0.000039  min_lr: 0.000000  loss: 4.0463 (4.0091)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (60892.2315)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1257  max mem: 15572
Epoch: [14]  [ 510/2809]  eta: 0:22:41  lr: 0.000039  min_lr: 0.000000  loss: 3.9796 (4.0113)  class_acc: 0.2500 (0.2524)  loss_scale: 65536.0000 (60983.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0748  max mem: 15572
Epoch: [14]  [ 520/2809]  eta: 0:22:35  lr: 0.000039  min_lr: 0.000000  loss: 3.9796 (4.0086)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (61070.4952)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1345  max mem: 15572
Epoch: [14]  [ 530/2809]  eta: 0:22:26  lr: 0.000039  min_lr: 0.000000  loss: 3.9322 (4.0090)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (61154.5913)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1065  max mem: 15572
Epoch: [14]  [ 540/2809]  eta: 0:22:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0667 (4.0106)  class_acc: 0.2083 (0.2518)  loss_scale: 65536.0000 (61235.5786)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0616  max mem: 15572
Epoch: [14]  [ 550/2809]  eta: 0:22:07  lr: 0.000039  min_lr: 0.000000  loss: 4.0667 (4.0115)  class_acc: 0.2500 (0.2523)  loss_scale: 65536.0000 (61313.6261)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0657  max mem: 15572
Epoch: [14]  [ 560/2809]  eta: 0:22:01  lr: 0.000039  min_lr: 0.000000  loss: 3.9269 (4.0068)  class_acc: 0.2917 (0.2538)  loss_scale: 65536.0000 (61388.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0975  max mem: 15572
Epoch: [14]  [ 570/2809]  eta: 0:21:51  lr: 0.000039  min_lr: 0.000000  loss: 3.7357 (4.0029)  class_acc: 0.2917 (0.2545)  loss_scale: 65536.0000 (61461.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0974  max mem: 15572
Epoch: [14]  [ 580/2809]  eta: 0:21:44  lr: 0.000039  min_lr: 0.000000  loss: 3.7411 (4.0027)  class_acc: 0.2500 (0.2544)  loss_scale: 65536.0000 (61531.6489)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0905  max mem: 15572
Epoch: [14]  [ 590/2809]  eta: 0:21:37  lr: 0.000039  min_lr: 0.000000  loss: 4.0580 (4.0024)  class_acc: 0.2083 (0.2541)  loss_scale: 65536.0000 (61599.4044)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1293  max mem: 15572
Epoch: [14]  [ 600/2809]  eta: 0:21:29  lr: 0.000039  min_lr: 0.000000  loss: 4.0481 (4.0044)  class_acc: 0.2083 (0.2531)  loss_scale: 65536.0000 (61664.9052)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1142  max mem: 15572
Epoch: [14]  [ 610/2809]  eta: 0:21:23  lr: 0.000039  min_lr: 0.000000  loss: 4.0481 (4.0038)  class_acc: 0.2083 (0.2525)  loss_scale: 65536.0000 (61728.2619)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1028  max mem: 15572
[2025-01-15 21:14:38,469] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:14:38,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 620/2809]  eta: 0:21:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0176 (4.0062)  class_acc: 0.2083 (0.2517)  loss_scale: 65536.0000 (62000.6441)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1192  max mem: 15572
[2025-01-15 21:14:41,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39949
[2025-01-15 21:14:41,416] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:14:41,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 630/2809]  eta: 0:21:10  lr: 0.000039  min_lr: 0.000000  loss: 4.0249 (4.0061)  class_acc: 0.2500 (0.2517)  loss_scale: 65536.0000 (62264.3930)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1246  max mem: 15572
Epoch: [14]  [ 640/2809]  eta: 0:21:04  lr: 0.000039  min_lr: 0.000000  loss: 4.0206 (4.0037)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (62315.4321)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1399  max mem: 15572
Epoch: [14]  [ 650/2809]  eta: 0:20:56  lr: 0.000039  min_lr: 0.000000  loss: 3.9785 (4.0042)  class_acc: 0.2500 (0.2526)  loss_scale: 65536.0000 (62364.9032)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1011  max mem: 15572
Epoch: [14]  [ 660/2809]  eta: 0:20:48  lr: 0.000039  min_lr: 0.000000  loss: 4.1411 (4.0064)  class_acc: 0.2083 (0.2523)  loss_scale: 65536.0000 (62412.8775)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0309  max mem: 15572
Epoch: [14]  [ 670/2809]  eta: 0:20:38  lr: 0.000039  min_lr: 0.000000  loss: 4.0238 (4.0060)  class_acc: 0.2500 (0.2524)  loss_scale: 65536.0000 (62459.4218)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0092  max mem: 15572
[2025-01-15 21:15:07,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=266, lr=[3.805205638311788e-07, 3.805205638311788e-07, 5.436008054731127e-07, 5.436008054731127e-07, 7.765725792473039e-07, 7.765725792473039e-07, 1.10938939892472e-06, 1.10938939892472e-06, 1.5848419984638858e-06, 1.5848419984638858e-06, 2.2640599978055513e-06, 2.2640599978055513e-06, 3.234371425436502e-06, 3.234371425436502e-06, 4.620530607766432e-06, 4.620530607766432e-06, 6.600758011094902e-06, 6.600758011094902e-06, 9.429654301564147e-06, 9.429654301564147e-06, 1.347093471652021e-05, 1.347093471652021e-05, 1.924419245217173e-05, 1.924419245217173e-05, 2.7491703503102474e-05, 2.7491703503102474e-05, 3.927386214728925e-05, 3.927386214728925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:15:07,277] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=28.422116312155225, CurrSamplesPerSec=22.90293396408002, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [ 680/2809]  eta: 0:20:31  lr: 0.000039  min_lr: 0.000000  loss: 4.0238 (4.0066)  class_acc: 0.2500 (0.2522)  loss_scale: 65536.0000 (62504.5991)  weight_decay: 0.0500 (0.0500)  time: 0.4881  data: 0.0310  max mem: 15572
Epoch: [14]  [ 690/2809]  eta: 0:20:23  lr: 0.000039  min_lr: 0.000000  loss: 4.1086 (4.0038)  class_acc: 0.2917 (0.2529)  loss_scale: 65536.0000 (62548.4689)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0518  max mem: 15572
Epoch: [14]  [ 700/2809]  eta: 0:20:14  lr: 0.000039  min_lr: 0.000000  loss: 3.8146 (4.0010)  class_acc: 0.2917 (0.2531)  loss_scale: 65536.0000 (62591.0870)  weight_decay: 0.0500 (0.0500)  time: 0.4913  data: 0.0285  max mem: 15572
Epoch: [14]  [ 710/2809]  eta: 0:20:10  lr: 0.000039  min_lr: 0.000000  loss: 3.6554 (3.9980)  class_acc: 0.2917 (0.2535)  loss_scale: 65536.0000 (62632.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0897  max mem: 15572
Epoch: [14]  [ 720/2809]  eta: 0:20:05  lr: 0.000039  min_lr: 0.000000  loss: 3.9306 (3.9981)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (62672.7767)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.1801  max mem: 15572
Epoch: [14]  [ 730/2809]  eta: 0:20:01  lr: 0.000039  min_lr: 0.000000  loss: 3.9559 (3.9970)  class_acc: 0.2083 (0.2529)  loss_scale: 65536.0000 (62711.9453)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.2089  max mem: 15572
Epoch: [14]  [ 740/2809]  eta: 0:19:55  lr: 0.000039  min_lr: 0.000000  loss: 4.0555 (3.9971)  class_acc: 0.2500 (0.2537)  loss_scale: 65536.0000 (62750.0567)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1793  max mem: 15572
Epoch: [14]  [ 750/2809]  eta: 0:19:53  lr: 0.000039  min_lr: 0.000000  loss: 4.0682 (3.9980)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (62787.1531)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1953  max mem: 15572
[2025-01-15 21:15:53,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:15:53,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:15:56,830] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40086
[2025-01-15 21:15:56,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:15:56,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 760/2809]  eta: 0:19:45  lr: 0.000039  min_lr: 0.000000  loss: 4.0013 (3.9972)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (63512.2208)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1512  max mem: 15572
Epoch: [14]  [ 770/2809]  eta: 0:19:37  lr: 0.000039  min_lr: 0.000000  loss: 3.9808 (3.9972)  class_acc: 0.2500 (0.2537)  loss_scale: 65536.0000 (63538.4695)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0730  max mem: 15572
Epoch: [14]  [ 780/2809]  eta: 0:19:33  lr: 0.000039  min_lr: 0.000000  loss: 4.0526 (3.9963)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (63564.0461)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1487  max mem: 15572
Epoch: [14]  [ 790/2809]  eta: 0:19:28  lr: 0.000039  min_lr: 0.000000  loss: 4.0915 (3.9976)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (63588.9760)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1903  max mem: 15572
Epoch: [14]  [ 800/2809]  eta: 0:19:20  lr: 0.000039  min_lr: 0.000000  loss: 4.0406 (3.9967)  class_acc: 0.2500 (0.2538)  loss_scale: 65536.0000 (63613.2834)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1268  max mem: 15572
Epoch: [14]  [ 810/2809]  eta: 0:19:15  lr: 0.000039  min_lr: 0.000000  loss: 3.9457 (3.9968)  class_acc: 0.2500 (0.2543)  loss_scale: 65536.0000 (63636.9914)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1210  max mem: 15572
Epoch: [14]  [ 820/2809]  eta: 0:19:07  lr: 0.000039  min_lr: 0.000000  loss: 4.0791 (3.9986)  class_acc: 0.2083 (0.2539)  loss_scale: 65536.0000 (63660.1218)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0967  max mem: 15572
Epoch: [14]  [ 830/2809]  eta: 0:18:59  lr: 0.000039  min_lr: 0.000000  loss: 4.0858 (3.9979)  class_acc: 0.2083 (0.2539)  loss_scale: 65536.0000 (63682.6955)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0491  max mem: 15572
Epoch: [14]  [ 840/2809]  eta: 0:18:52  lr: 0.000039  min_lr: 0.000000  loss: 4.0759 (3.9986)  class_acc: 0.2083 (0.2535)  loss_scale: 65536.0000 (63704.7325)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0820  max mem: 15572
Epoch: [14]  [ 850/2809]  eta: 0:18:47  lr: 0.000039  min_lr: 0.000000  loss: 3.9442 (3.9991)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (63726.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1332  max mem: 15572
Epoch: [14]  [ 860/2809]  eta: 0:18:41  lr: 0.000039  min_lr: 0.000000  loss: 4.1420 (4.0002)  class_acc: 0.2083 (0.2528)  loss_scale: 65536.0000 (63747.2706)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1555  max mem: 15572
Epoch: [14]  [ 870/2809]  eta: 0:18:33  lr: 0.000039  min_lr: 0.000000  loss: 3.9146 (3.9979)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (63767.8071)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.1104  max mem: 15572
Epoch: [14]  [ 880/2809]  eta: 0:18:30  lr: 0.000039  min_lr: 0.000000  loss: 3.8734 (3.9983)  class_acc: 0.2917 (0.2535)  loss_scale: 65536.0000 (63787.8774)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1697  max mem: 15572
[2025-01-15 21:17:08,775] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:17:08,775] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 890/2809]  eta: 0:18:23  lr: 0.000039  min_lr: 0.000000  loss: 4.0325 (3.9988)  class_acc: 0.2500 (0.2535)  loss_scale: 65536.0000 (63954.6038)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1853  max mem: 15572
[2025-01-15 21:17:11,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40219
[2025-01-15 21:17:11,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:17:11,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 900/2809]  eta: 0:18:18  lr: 0.000039  min_lr: 0.000000  loss: 3.9351 (3.9974)  class_acc: 0.2917 (0.2543)  loss_scale: 65536.0000 (64117.6293)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1265  max mem: 15572
[2025-01-15 21:17:19,812] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40233
[2025-01-15 21:17:19,812] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:17:19,813] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 910/2809]  eta: 0:18:13  lr: 0.000039  min_lr: 0.000000  loss: 3.9311 (3.9974)  class_acc: 0.2500 (0.2539)  loss_scale: 65536.0000 (63989.3216)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1394  max mem: 15572
Epoch: [14]  [ 920/2809]  eta: 0:18:06  lr: 0.000039  min_lr: 0.000000  loss: 3.9171 (3.9952)  class_acc: 0.2500 (0.2545)  loss_scale: 32768.0000 (63650.3279)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1086  max mem: 15572
Epoch: [14]  [ 930/2809]  eta: 0:18:01  lr: 0.000039  min_lr: 0.000000  loss: 3.9528 (3.9960)  class_acc: 0.2500 (0.2544)  loss_scale: 32768.0000 (63318.6165)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1174  max mem: 15572
Epoch: [14]  [ 940/2809]  eta: 0:17:52  lr: 0.000039  min_lr: 0.000000  loss: 4.2183 (3.9979)  class_acc: 0.2083 (0.2539)  loss_scale: 32768.0000 (62993.9554)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0866  max mem: 15572
Epoch: [14]  [ 950/2809]  eta: 0:17:47  lr: 0.000039  min_lr: 0.000000  loss: 4.1250 (3.9993)  class_acc: 0.2500 (0.2537)  loss_scale: 32768.0000 (62676.1220)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0843  max mem: 15572
Epoch: [14]  [ 960/2809]  eta: 0:17:41  lr: 0.000039  min_lr: 0.000000  loss: 4.1250 (4.0012)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (62364.9032)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1357  max mem: 15572
Epoch: [14]  [ 970/2809]  eta: 0:17:34  lr: 0.000039  min_lr: 0.000000  loss: 4.1745 (4.0023)  class_acc: 0.2083 (0.2535)  loss_scale: 32768.0000 (62060.0947)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.1085  max mem: 15572
Epoch: [14]  [ 980/2809]  eta: 0:17:29  lr: 0.000039  min_lr: 0.000000  loss: 4.1010 (4.0024)  class_acc: 0.2083 (0.2533)  loss_scale: 32768.0000 (61761.5005)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1300  max mem: 15572
Epoch: [14]  [ 990/2809]  eta: 0:17:23  lr: 0.000039  min_lr: 0.000000  loss: 4.0529 (4.0033)  class_acc: 0.1667 (0.2531)  loss_scale: 32768.0000 (61468.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1268  max mem: 15572
Epoch: [14]  [1000/2809]  eta: 0:17:17  lr: 0.000039  min_lr: 0.000000  loss: 3.9558 (4.0011)  class_acc: 0.2500 (0.2535)  loss_scale: 32768.0000 (61182.2098)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1110  max mem: 15572
Epoch: [14]  [1010/2809]  eta: 0:17:09  lr: 0.000039  min_lr: 0.000000  loss: 3.9121 (4.0008)  class_acc: 0.2500 (0.2533)  loss_scale: 32768.0000 (60901.1592)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0634  max mem: 15572
Epoch: [14]  [1020/2809]  eta: 0:17:03  lr: 0.000039  min_lr: 0.000000  loss: 3.9518 (3.9994)  class_acc: 0.2917 (0.2539)  loss_scale: 32768.0000 (60625.6141)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0768  max mem: 15572
Epoch: [14]  [1030/2809]  eta: 0:16:57  lr: 0.000039  min_lr: 0.000000  loss: 3.9130 (3.9986)  class_acc: 0.2917 (0.2538)  loss_scale: 32768.0000 (60355.4142)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1295  max mem: 15572
[2025-01-15 21:18:31,115] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:18:31,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1040/2809]  eta: 0:16:52  lr: 0.000039  min_lr: 0.000000  loss: 3.9377 (3.9988)  class_acc: 0.2500 (0.2537)  loss_scale: 32768.0000 (60247.7925)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1321  max mem: 15572
Epoch: [14]  [1050/2809]  eta: 0:16:45  lr: 0.000039  min_lr: 0.000000  loss: 4.0056 (3.9998)  class_acc: 0.2083 (0.2536)  loss_scale: 65536.0000 (60298.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1160  max mem: 15572
Epoch: [14]  [1060/2809]  eta: 0:16:40  lr: 0.000039  min_lr: 0.000000  loss: 4.0175 (4.0009)  class_acc: 0.1667 (0.2531)  loss_scale: 65536.0000 (60347.4760)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0849  max mem: 15572
Epoch: [14]  [1070/2809]  eta: 0:16:34  lr: 0.000039  min_lr: 0.000000  loss: 4.0039 (4.0003)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (60395.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1251  max mem: 15572
Epoch: [14]  [1080/2809]  eta: 0:16:29  lr: 0.000039  min_lr: 0.000000  loss: 3.7997 (3.9987)  class_acc: 0.2917 (0.2536)  loss_scale: 65536.0000 (60443.4709)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1310  max mem: 15572
Epoch: [14]  [1090/2809]  eta: 0:16:22  lr: 0.000039  min_lr: 0.000000  loss: 3.7508 (3.9973)  class_acc: 0.2917 (0.2539)  loss_scale: 65536.0000 (60490.1485)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0878  max mem: 15572
Epoch: [14]  [1100/2809]  eta: 0:16:16  lr: 0.000039  min_lr: 0.000000  loss: 3.9064 (3.9980)  class_acc: 0.2917 (0.2540)  loss_scale: 65536.0000 (60535.9782)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1118  max mem: 15572
Epoch: [14]  [1110/2809]  eta: 0:16:10  lr: 0.000039  min_lr: 0.000000  loss: 4.0536 (3.9972)  class_acc: 0.2500 (0.2542)  loss_scale: 65536.0000 (60580.9829)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1122  max mem: 15572
Epoch: [14]  [1120/2809]  eta: 0:16:03  lr: 0.000039  min_lr: 0.000000  loss: 3.7676 (3.9968)  class_acc: 0.2500 (0.2542)  loss_scale: 65536.0000 (60625.1847)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0700  max mem: 15572
Epoch: [14]  [1130/2809]  eta: 0:15:57  lr: 0.000039  min_lr: 0.000000  loss: 4.0831 (3.9982)  class_acc: 0.2083 (0.2539)  loss_scale: 65536.0000 (60668.6048)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0908  max mem: 15572
Epoch: [14]  [1140/2809]  eta: 0:15:51  lr: 0.000039  min_lr: 0.000000  loss: 4.1430 (4.0000)  class_acc: 0.2083 (0.2540)  loss_scale: 65536.0000 (60711.2638)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1296  max mem: 15572
Epoch: [14]  [1150/2809]  eta: 0:15:45  lr: 0.000039  min_lr: 0.000000  loss: 4.0551 (3.9994)  class_acc: 0.2500 (0.2541)  loss_scale: 65536.0000 (60753.1816)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1178  max mem: 15572
Epoch: [14]  [1160/2809]  eta: 0:15:39  lr: 0.000039  min_lr: 0.000000  loss: 4.0513 (4.0011)  class_acc: 0.2083 (0.2537)  loss_scale: 65536.0000 (60794.3773)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0958  max mem: 15572
[2025-01-15 21:19:41,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:19:41,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:19:42,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40493
[2025-01-15 21:19:42,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:19:42,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1170/2809]  eta: 0:15:33  lr: 0.000039  min_lr: 0.000000  loss: 4.0761 (4.0016)  class_acc: 0.2083 (0.2536)  loss_scale: 65536.0000 (61002.7669)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1216  max mem: 15572
Epoch: [14]  [1180/2809]  eta: 0:15:28  lr: 0.000039  min_lr: 0.000000  loss: 4.0009 (4.0014)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (61041.1516)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1638  max mem: 15572
Epoch: [14]  [1190/2809]  eta: 0:15:22  lr: 0.000039  min_lr: 0.000000  loss: 3.9899 (4.0021)  class_acc: 0.2500 (0.2537)  loss_scale: 65536.0000 (61078.8917)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1512  max mem: 15572
Epoch: [14]  [1200/2809]  eta: 0:15:17  lr: 0.000039  min_lr: 0.000000  loss: 4.1188 (4.0033)  class_acc: 0.2083 (0.2532)  loss_scale: 65536.0000 (61116.0033)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1395  max mem: 15572
Epoch: [14]  [1210/2809]  eta: 0:15:10  lr: 0.000039  min_lr: 0.000000  loss: 4.0206 (4.0022)  class_acc: 0.2083 (0.2533)  loss_scale: 65536.0000 (61152.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1128  max mem: 15572
Epoch: [14]  [1220/2809]  eta: 0:15:04  lr: 0.000039  min_lr: 0.000000  loss: 3.9081 (4.0008)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61188.4029)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0903  max mem: 15572
Epoch: [14]  [1230/2809]  eta: 0:14:58  lr: 0.000039  min_lr: 0.000000  loss: 3.9196 (4.0004)  class_acc: 0.2500 (0.2536)  loss_scale: 65536.0000 (61223.7206)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1130  max mem: 15572
Epoch: [14]  [1240/2809]  eta: 0:14:52  lr: 0.000039  min_lr: 0.000000  loss: 4.0248 (4.0017)  class_acc: 0.2500 (0.2534)  loss_scale: 65536.0000 (61258.4690)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1109  max mem: 15572
Epoch: [14]  [1250/2809]  eta: 0:14:45  lr: 0.000039  min_lr: 0.000000  loss: 4.0075 (4.0020)  class_acc: 0.2500 (0.2533)  loss_scale: 65536.0000 (61292.6619)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0789  max mem: 15572
Epoch: [14]  [1260/2809]  eta: 0:14:40  lr: 0.000039  min_lr: 0.000000  loss: 4.0686 (4.0035)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (61326.3125)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1051  max mem: 15572
Epoch: [14]  [1270/2809]  eta: 0:14:35  lr: 0.000039  min_lr: 0.000000  loss: 4.1696 (4.0044)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (61359.4335)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1890  max mem: 15572
Epoch: [14]  [1280/2809]  eta: 0:14:29  lr: 0.000039  min_lr: 0.000000  loss: 3.8738 (4.0030)  class_acc: 0.2500 (0.2530)  loss_scale: 65536.0000 (61392.0375)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1661  max mem: 15572
Epoch: [14]  [1290/2809]  eta: 0:14:23  lr: 0.000039  min_lr: 0.000000  loss: 4.0495 (4.0033)  class_acc: 0.2500 (0.2527)  loss_scale: 65536.0000 (61424.1363)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0770  max mem: 15572
[2025-01-15 21:20:54,706] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:20:54,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:20:55,099] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40623
[2025-01-15 21:20:55,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:20:55,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1300/2809]  eta: 0:14:18  lr: 0.000039  min_lr: 0.000000  loss: 4.0694 (4.0039)  class_acc: 0.2500 (0.2528)  loss_scale: 65536.0000 (61506.1153)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0911  max mem: 15572
Epoch: [14]  [1310/2809]  eta: 0:14:12  lr: 0.000039  min_lr: 0.000000  loss: 4.0203 (4.0035)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (61536.8543)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1555  max mem: 15572
Epoch: [14]  [1320/2809]  eta: 0:14:06  lr: 0.000039  min_lr: 0.000000  loss: 4.0352 (4.0030)  class_acc: 0.2500 (0.2531)  loss_scale: 65536.0000 (61567.1279)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1152  max mem: 15572
Epoch: [14]  [1330/2809]  eta: 0:14:01  lr: 0.000039  min_lr: 0.000000  loss: 4.0352 (4.0038)  class_acc: 0.2083 (0.2528)  loss_scale: 65536.0000 (61596.9467)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1090  max mem: 15572
Epoch: [14]  [1340/2809]  eta: 0:13:54  lr: 0.000039  min_lr: 0.000000  loss: 4.1087 (4.0044)  class_acc: 0.2500 (0.2529)  loss_scale: 65536.0000 (61626.3207)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1019  max mem: 15572
Epoch: [14]  [1350/2809]  eta: 0:13:49  lr: 0.000039  min_lr: 0.000000  loss: 4.0375 (4.0043)  class_acc: 0.2917 (0.2534)  loss_scale: 65536.0000 (61655.2598)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1064  max mem: 15572
Epoch: [14]  [1360/2809]  eta: 0:13:43  lr: 0.000039  min_lr: 0.000000  loss: 3.6991 (4.0024)  class_acc: 0.2917 (0.2538)  loss_scale: 65536.0000 (61683.7737)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1461  max mem: 15572
Epoch: [14]  [1370/2809]  eta: 0:13:37  lr: 0.000039  min_lr: 0.000000  loss: 3.6991 (4.0009)  class_acc: 0.2917 (0.2541)  loss_scale: 65536.0000 (61711.8716)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0931  max mem: 15572
Epoch: [14]  [1380/2809]  eta: 0:13:31  lr: 0.000039  min_lr: 0.000000  loss: 3.7591 (3.9985)  class_acc: 0.2917 (0.2544)  loss_scale: 65536.0000 (61739.5626)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0801  max mem: 15572
Epoch: [14]  [1390/2809]  eta: 0:13:25  lr: 0.000039  min_lr: 0.000000  loss: 3.8002 (3.9989)  class_acc: 0.2500 (0.2545)  loss_scale: 65536.0000 (61766.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1123  max mem: 15572
[2025-01-15 21:21:51,096] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40724
[2025-01-15 21:21:51,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:21:51,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1400/2809]  eta: 0:13:19  lr: 0.000039  min_lr: 0.000000  loss: 3.9815 (3.9996)  class_acc: 0.2500 (0.2544)  loss_scale: 65536.0000 (61723.5917)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.0915  max mem: 15572
Epoch: [14]  [1410/2809]  eta: 0:13:13  lr: 0.000039  min_lr: 0.000000  loss: 3.9017 (3.9977)  class_acc: 0.2917 (0.2550)  loss_scale: 32768.0000 (61518.3785)  weight_decay: 0.0500 (0.0500)  time: 0.5008  data: 0.0611  max mem: 15572
Epoch: [14]  [1420/2809]  eta: 0:13:08  lr: 0.000039  min_lr: 0.000000  loss: 3.7308 (3.9982)  class_acc: 0.2500 (0.2548)  loss_scale: 32768.0000 (61316.0535)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1427  max mem: 15572
Epoch: [14]  [1430/2809]  eta: 0:13:02  lr: 0.000039  min_lr: 0.000000  loss: 4.1230 (3.9972)  class_acc: 0.2500 (0.2548)  loss_scale: 32768.0000 (61116.5563)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1579  max mem: 15572
Epoch: [14]  [1440/2809]  eta: 0:12:56  lr: 0.000039  min_lr: 0.000000  loss: 3.8220 (3.9965)  class_acc: 0.2500 (0.2550)  loss_scale: 32768.0000 (60919.8279)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0973  max mem: 15572
Epoch: [14]  [1450/2809]  eta: 0:12:51  lr: 0.000039  min_lr: 0.000000  loss: 3.8094 (3.9949)  class_acc: 0.2500 (0.2553)  loss_scale: 32768.0000 (60725.8112)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1543  max mem: 15572
Epoch: [14]  [1460/2809]  eta: 0:12:46  lr: 0.000039  min_lr: 0.000000  loss: 3.9420 (3.9939)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (60534.4504)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1710  max mem: 15572
Epoch: [14]  [1470/2809]  eta: 0:12:41  lr: 0.000039  min_lr: 0.000000  loss: 3.9424 (3.9938)  class_acc: 0.2500 (0.2555)  loss_scale: 32768.0000 (60345.6914)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1892  max mem: 15572
Epoch: [14]  [1480/2809]  eta: 0:12:35  lr: 0.000039  min_lr: 0.000000  loss: 3.9424 (3.9950)  class_acc: 0.2083 (0.2552)  loss_scale: 32768.0000 (60159.4814)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1903  max mem: 15572
Epoch: [14]  [1490/2809]  eta: 0:12:29  lr: 0.000039  min_lr: 0.000000  loss: 3.9332 (3.9947)  class_acc: 0.2083 (0.2553)  loss_scale: 32768.0000 (59975.7693)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1028  max mem: 15572
Epoch: [14]  [1500/2809]  eta: 0:12:23  lr: 0.000039  min_lr: 0.000000  loss: 3.9332 (3.9941)  class_acc: 0.2500 (0.2554)  loss_scale: 32768.0000 (59794.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0855  max mem: 15572
Epoch: [14]  [1510/2809]  eta: 0:12:17  lr: 0.000039  min_lr: 0.000000  loss: 3.9588 (3.9945)  class_acc: 0.2083 (0.2551)  loss_scale: 32768.0000 (59615.6400)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0979  max mem: 15572
Epoch: [14]  [1520/2809]  eta: 0:12:11  lr: 0.000039  min_lr: 0.000000  loss: 4.0077 (3.9936)  class_acc: 0.2083 (0.2555)  loss_scale: 32768.0000 (59439.1269)  weight_decay: 0.0500 (0.0500)  time: 0.5311  data: 0.0888  max mem: 15572
[2025-01-15 21:23:04,034] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:23:04,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1530/2809]  eta: 0:12:05  lr: 0.000039  min_lr: 0.000000  loss: 3.9694 (3.9941)  class_acc: 0.2500 (0.2553)  loss_scale: 32768.0000 (59350.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0906  max mem: 15572
Epoch: [14]  [1540/2809]  eta: 0:11:59  lr: 0.000039  min_lr: 0.000000  loss: 3.9694 (3.9943)  class_acc: 0.2500 (0.2553)  loss_scale: 65536.0000 (59390.6710)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1178  max mem: 15572
Epoch: [14]  [1550/2809]  eta: 0:11:54  lr: 0.000039  min_lr: 0.000000  loss: 3.9161 (3.9938)  class_acc: 0.2917 (0.2553)  loss_scale: 65536.0000 (59430.2927)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1693  max mem: 15572
Epoch: [14]  [1560/2809]  eta: 0:11:49  lr: 0.000039  min_lr: 0.000000  loss: 3.8421 (3.9925)  class_acc: 0.2917 (0.2557)  loss_scale: 65536.0000 (59469.4068)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.2071  max mem: 15572
Epoch: [14]  [1570/2809]  eta: 0:11:43  lr: 0.000039  min_lr: 0.000000  loss: 3.7771 (3.9914)  class_acc: 0.2917 (0.2559)  loss_scale: 65536.0000 (59508.0229)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1723  max mem: 15572
Epoch: [14]  [1580/2809]  eta: 0:11:37  lr: 0.000039  min_lr: 0.000000  loss: 4.0703 (3.9920)  class_acc: 0.2917 (0.2562)  loss_scale: 65536.0000 (59546.1505)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0609  max mem: 15572
Epoch: [14]  [1590/2809]  eta: 0:11:32  lr: 0.000039  min_lr: 0.000000  loss: 4.1637 (3.9929)  class_acc: 0.2917 (0.2560)  loss_scale: 65536.0000 (59583.7989)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0751  max mem: 15572
Epoch: [14]  [1600/2809]  eta: 0:11:25  lr: 0.000039  min_lr: 0.000000  loss: 4.1276 (3.9922)  class_acc: 0.2500 (0.2561)  loss_scale: 65536.0000 (59620.9769)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0847  max mem: 15572
Epoch: [14]  [1610/2809]  eta: 0:11:20  lr: 0.000039  min_lr: 0.000000  loss: 3.6799 (3.9912)  class_acc: 0.2500 (0.2562)  loss_scale: 65536.0000 (59657.6934)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1428  max mem: 15572
Epoch: [14]  [1620/2809]  eta: 0:11:14  lr: 0.000039  min_lr: 0.000000  loss: 3.7657 (3.9904)  class_acc: 0.2917 (0.2565)  loss_scale: 65536.0000 (59693.9568)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1655  max mem: 15572
Epoch: [14]  [1630/2809]  eta: 0:11:08  lr: 0.000039  min_lr: 0.000000  loss: 3.9079 (3.9896)  class_acc: 0.2917 (0.2567)  loss_scale: 65536.0000 (59729.7756)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0823  max mem: 15572
Epoch: [14]  [1640/2809]  eta: 0:11:03  lr: 0.000039  min_lr: 0.000000  loss: 3.9322 (3.9900)  class_acc: 0.2500 (0.2567)  loss_scale: 65536.0000 (59765.1578)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1026  max mem: 15572
Epoch: [14]  [1650/2809]  eta: 0:10:57  lr: 0.000039  min_lr: 0.000000  loss: 3.9607 (3.9901)  class_acc: 0.2500 (0.2569)  loss_scale: 65536.0000 (59800.1114)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1467  max mem: 15572
[2025-01-15 21:24:16,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:24:16,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:24:16,968] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40982
[2025-01-15 21:24:16,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:24:16,968] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1660/2809]  eta: 0:10:52  lr: 0.000039  min_lr: 0.000000  loss: 3.9001 (3.9892)  class_acc: 0.2500 (0.2571)  loss_scale: 65536.0000 (59874.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1895  max mem: 15572
Epoch: [14]  [1670/2809]  eta: 0:10:46  lr: 0.000039  min_lr: 0.000000  loss: 3.8672 (3.9895)  class_acc: 0.2500 (0.2571)  loss_scale: 65536.0000 (59907.9832)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0959  max mem: 15572
[2025-01-15 21:24:26,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=273, lr=[3.750971688681664e-07, 3.750971688681664e-07, 5.358530983830948e-07, 5.358530983830948e-07, 7.655044262615642e-07, 7.655044262615642e-07, 1.0935777518022345e-06, 1.0935777518022345e-06, 1.5622539311460496e-06, 1.5622539311460496e-06, 2.231791330208642e-06, 2.231791330208642e-06, 3.188273328869489e-06, 3.188273328869489e-06, 4.55467618409927e-06, 4.55467618409927e-06, 6.506680262998958e-06, 6.506680262998958e-06, 9.295257518569941e-06, 9.295257518569941e-06, 1.3278939312242773e-05, 1.3278939312242773e-05, 1.8969913303203963e-05, 1.8969913303203963e-05, 2.7099876147434235e-05, 2.7099876147434235e-05, 3.871410878204891e-05, 3.871410878204891e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:24:26,463] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=28.434686407475294, CurrSamplesPerSec=27.394488951396376, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [1680/2809]  eta: 0:10:40  lr: 0.000039  min_lr: 0.000000  loss: 3.9583 (3.9888)  class_acc: 0.2500 (0.2573)  loss_scale: 65536.0000 (59941.4634)  weight_decay: 0.0500 (0.0500)  time: 0.4894  data: 0.0432  max mem: 15572
Epoch: [14]  [1690/2809]  eta: 0:10:34  lr: 0.000039  min_lr: 0.000000  loss: 3.9660 (3.9888)  class_acc: 0.2500 (0.2574)  loss_scale: 65536.0000 (59974.5476)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0781  max mem: 15572
Epoch: [14]  [1700/2809]  eta: 0:10:28  lr: 0.000039  min_lr: 0.000000  loss: 4.1587 (3.9894)  class_acc: 0.2500 (0.2573)  loss_scale: 65536.0000 (60007.2428)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0686  max mem: 15572
[2025-01-15 21:24:46,320] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41036
[2025-01-15 21:24:46,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:24:46,321] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [1710/2809]  eta: 0:10:22  lr: 0.000039  min_lr: 0.000000  loss: 4.0417 (3.9892)  class_acc: 0.2083 (0.2573)  loss_scale: 65536.0000 (60020.4044)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0821  max mem: 15572
Epoch: [14]  [1720/2809]  eta: 0:10:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0417 (3.9899)  class_acc: 0.2083 (0.2570)  loss_scale: 32768.0000 (59862.0523)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1192  max mem: 15572
Epoch: [14]  [1730/2809]  eta: 0:10:11  lr: 0.000039  min_lr: 0.000000  loss: 4.0427 (3.9892)  class_acc: 0.2500 (0.2573)  loss_scale: 32768.0000 (59705.5298)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1322  max mem: 15572
Epoch: [14]  [1740/2809]  eta: 0:10:06  lr: 0.000039  min_lr: 0.000000  loss: 4.0427 (3.9897)  class_acc: 0.2500 (0.2571)  loss_scale: 32768.0000 (59550.8053)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1423  max mem: 15572
Epoch: [14]  [1750/2809]  eta: 0:09:59  lr: 0.000039  min_lr: 0.000000  loss: 4.1074 (3.9905)  class_acc: 0.1667 (0.2567)  loss_scale: 32768.0000 (59397.8481)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0810  max mem: 15572
Epoch: [14]  [1760/2809]  eta: 0:09:53  lr: 0.000039  min_lr: 0.000000  loss: 4.0758 (3.9905)  class_acc: 0.2083 (0.2566)  loss_scale: 32768.0000 (59246.6281)  weight_decay: 0.0500 (0.0500)  time: 0.4968  data: 0.0433  max mem: 15572
Epoch: [14]  [1770/2809]  eta: 0:09:48  lr: 0.000039  min_lr: 0.000000  loss: 4.1452 (3.9919)  class_acc: 0.2083 (0.2563)  loss_scale: 32768.0000 (59097.1158)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0851  max mem: 15572
Epoch: [14]  [1780/2809]  eta: 0:09:43  lr: 0.000039  min_lr: 0.000000  loss: 4.0706 (3.9921)  class_acc: 0.2083 (0.2562)  loss_scale: 32768.0000 (58949.2824)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1625  max mem: 15572
Epoch: [14]  [1790/2809]  eta: 0:09:37  lr: 0.000039  min_lr: 0.000000  loss: 4.0706 (3.9930)  class_acc: 0.2500 (0.2561)  loss_scale: 32768.0000 (58803.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1450  max mem: 15572
Epoch: [14]  [1800/2809]  eta: 0:09:30  lr: 0.000039  min_lr: 0.000000  loss: 4.1419 (3.9936)  class_acc: 0.2083 (0.2559)  loss_scale: 32768.0000 (58658.5408)  weight_decay: 0.0500 (0.0500)  time: 0.4755  data: 0.0298  max mem: 15572
Epoch: [14]  [1810/2809]  eta: 0:09:24  lr: 0.000039  min_lr: 0.000000  loss: 4.1792 (3.9950)  class_acc: 0.1667 (0.2557)  loss_scale: 32768.0000 (58515.5781)  weight_decay: 0.0500 (0.0500)  time: 0.4827  data: 0.0257  max mem: 15572
Epoch: [14]  [1820/2809]  eta: 0:09:19  lr: 0.000039  min_lr: 0.000000  loss: 4.1792 (3.9946)  class_acc: 0.2500 (0.2559)  loss_scale: 32768.0000 (58374.1856)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.0919  max mem: 15572
Epoch: [14]  [1830/2809]  eta: 0:09:14  lr: 0.000039  min_lr: 0.000000  loss: 3.8970 (3.9938)  class_acc: 0.2500 (0.2559)  loss_scale: 32768.0000 (58234.3375)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1603  max mem: 15572
[2025-01-15 21:25:59,327] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:25:59,328] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [1840/2809]  eta: 0:09:08  lr: 0.000039  min_lr: 0.000000  loss: 3.7989 (3.9926)  class_acc: 0.2500 (0.2562)  loss_scale: 32768.0000 (58131.6067)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1600  max mem: 15572
Epoch: [14]  [1850/2809]  eta: 0:09:03  lr: 0.000039  min_lr: 0.000000  loss: 3.8449 (3.9933)  class_acc: 0.2500 (0.2560)  loss_scale: 65536.0000 (58171.6089)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1396  max mem: 15572
Epoch: [14]  [1860/2809]  eta: 0:08:57  lr: 0.000039  min_lr: 0.000000  loss: 4.0579 (3.9931)  class_acc: 0.2083 (0.2560)  loss_scale: 65536.0000 (58211.1811)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1045  max mem: 15572
Epoch: [14]  [1870/2809]  eta: 0:08:51  lr: 0.000039  min_lr: 0.000000  loss: 4.0579 (3.9938)  class_acc: 0.2083 (0.2557)  loss_scale: 65536.0000 (58250.3303)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0987  max mem: 15572
Epoch: [14]  [1880/2809]  eta: 0:08:46  lr: 0.000039  min_lr: 0.000000  loss: 4.1783 (3.9931)  class_acc: 0.2083 (0.2559)  loss_scale: 65536.0000 (58289.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1466  max mem: 15572
Epoch: [14]  [1890/2809]  eta: 0:08:40  lr: 0.000039  min_lr: 0.000000  loss: 3.9281 (3.9932)  class_acc: 0.2917 (0.2561)  loss_scale: 65536.0000 (58327.3866)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1233  max mem: 15572
Epoch: [14]  [1900/2809]  eta: 0:08:34  lr: 0.000039  min_lr: 0.000000  loss: 3.9281 (3.9931)  class_acc: 0.2917 (0.2564)  loss_scale: 65536.0000 (58365.3067)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0843  max mem: 15572
Epoch: [14]  [1910/2809]  eta: 0:08:29  lr: 0.000039  min_lr: 0.000000  loss: 3.8202 (3.9913)  class_acc: 0.2917 (0.2571)  loss_scale: 65536.0000 (58402.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1482  max mem: 15572
Epoch: [14]  [1920/2809]  eta: 0:08:23  lr: 0.000039  min_lr: 0.000000  loss: 3.8923 (3.9916)  class_acc: 0.2917 (0.2569)  loss_scale: 65536.0000 (58439.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1476  max mem: 15572
Epoch: [14]  [1930/2809]  eta: 0:08:17  lr: 0.000039  min_lr: 0.000000  loss: 4.0476 (3.9911)  class_acc: 0.2500 (0.2570)  loss_scale: 65536.0000 (58476.7105)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0613  max mem: 15572
Epoch: [14]  [1940/2809]  eta: 0:08:11  lr: 0.000039  min_lr: 0.000000  loss: 3.9708 (3.9910)  class_acc: 0.2917 (0.2572)  loss_scale: 65536.0000 (58513.0799)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0675  max mem: 15572
Epoch: [14]  [1950/2809]  eta: 0:08:05  lr: 0.000039  min_lr: 0.000000  loss: 4.0960 (3.9919)  class_acc: 0.2083 (0.2569)  loss_scale: 65536.0000 (58549.0764)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0895  max mem: 15572
Epoch: [14]  [1960/2809]  eta: 0:08:00  lr: 0.000039  min_lr: 0.000000  loss: 4.0960 (3.9924)  class_acc: 0.2083 (0.2571)  loss_scale: 65536.0000 (58584.7058)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1441  max mem: 15572
[2025-01-15 21:27:11,822] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:27:11,823] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [1970/2809]  eta: 0:07:55  lr: 0.000039  min_lr: 0.000000  loss: 4.1315 (3.9925)  class_acc: 0.2500 (0.2570)  loss_scale: 65536.0000 (58752.9741)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1789  max mem: 15572
[2025-01-15 21:27:17,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41302
[2025-01-15 21:27:17,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:27:17,381] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [1980/2809]  eta: 0:07:49  lr: 0.000039  min_lr: 0.000000  loss: 4.1315 (3.9922)  class_acc: 0.2500 (0.2570)  loss_scale: 65536.0000 (58952.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1586  max mem: 15572
Epoch: [14]  [1990/2809]  eta: 0:07:43  lr: 0.000039  min_lr: 0.000000  loss: 3.9943 (3.9915)  class_acc: 0.2917 (0.2573)  loss_scale: 65536.0000 (58985.6916)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1380  max mem: 15572
Epoch: [14]  [2000/2809]  eta: 0:07:38  lr: 0.000039  min_lr: 0.000000  loss: 4.0083 (3.9908)  class_acc: 0.2917 (0.2574)  loss_scale: 65536.0000 (59018.4268)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1289  max mem: 15572
Epoch: [14]  [2010/2809]  eta: 0:07:32  lr: 0.000039  min_lr: 0.000000  loss: 3.9069 (3.9902)  class_acc: 0.2500 (0.2573)  loss_scale: 65536.0000 (59050.8364)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0860  max mem: 15572
Epoch: [14]  [2020/2809]  eta: 0:07:26  lr: 0.000039  min_lr: 0.000000  loss: 3.7802 (3.9889)  class_acc: 0.2917 (0.2578)  loss_scale: 65536.0000 (59082.9253)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1111  max mem: 15572
Epoch: [14]  [2030/2809]  eta: 0:07:21  lr: 0.000039  min_lr: 0.000000  loss: 3.8863 (3.9883)  class_acc: 0.2917 (0.2578)  loss_scale: 65536.0000 (59114.6982)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1776  max mem: 15572
Epoch: [14]  [2040/2809]  eta: 0:07:15  lr: 0.000039  min_lr: 0.000000  loss: 4.0283 (3.9886)  class_acc: 0.2083 (0.2578)  loss_scale: 65536.0000 (59146.1597)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1348  max mem: 15572
Epoch: [14]  [2050/2809]  eta: 0:07:09  lr: 0.000038  min_lr: 0.000000  loss: 4.3608 (3.9898)  class_acc: 0.2083 (0.2575)  loss_scale: 65536.0000 (59177.3145)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0776  max mem: 15572
Epoch: [14]  [2060/2809]  eta: 0:07:04  lr: 0.000038  min_lr: 0.000000  loss: 4.2166 (3.9889)  class_acc: 0.2500 (0.2577)  loss_scale: 65536.0000 (59208.1669)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1147  max mem: 15572
Epoch: [14]  [2070/2809]  eta: 0:06:58  lr: 0.000038  min_lr: 0.000000  loss: 3.7012 (3.9876)  class_acc: 0.3333 (0.2582)  loss_scale: 65536.0000 (59238.7214)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1440  max mem: 15572
Epoch: [14]  [2080/2809]  eta: 0:06:52  lr: 0.000038  min_lr: 0.000000  loss: 3.7341 (3.9868)  class_acc: 0.3333 (0.2586)  loss_scale: 65536.0000 (59268.9822)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1446  max mem: 15572
[2025-01-15 21:28:16,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41407
[2025-01-15 21:28:16,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:28:16,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [2090/2809]  eta: 0:06:47  lr: 0.000038  min_lr: 0.000000  loss: 3.9224 (3.9866)  class_acc: 0.2917 (0.2587)  loss_scale: 32768.0000 (59142.2439)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1252  max mem: 15572
Epoch: [14]  [2100/2809]  eta: 0:06:41  lr: 0.000038  min_lr: 0.000000  loss: 3.9063 (3.9859)  class_acc: 0.2917 (0.2589)  loss_scale: 32768.0000 (59016.7120)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1422  max mem: 15572
Epoch: [14]  [2110/2809]  eta: 0:06:35  lr: 0.000038  min_lr: 0.000000  loss: 3.8776 (3.9856)  class_acc: 0.2917 (0.2592)  loss_scale: 32768.0000 (58892.3695)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1182  max mem: 15572
Epoch: [14]  [2120/2809]  eta: 0:06:30  lr: 0.000038  min_lr: 0.000000  loss: 3.9773 (3.9861)  class_acc: 0.2500 (0.2590)  loss_scale: 32768.0000 (58769.1994)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.1026  max mem: 15572
Epoch: [14]  [2130/2809]  eta: 0:06:24  lr: 0.000038  min_lr: 0.000000  loss: 3.9861 (3.9857)  class_acc: 0.2500 (0.2593)  loss_scale: 32768.0000 (58647.1854)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1654  max mem: 15572
Epoch: [14]  [2140/2809]  eta: 0:06:18  lr: 0.000038  min_lr: 0.000000  loss: 3.9372 (3.9853)  class_acc: 0.2917 (0.2595)  loss_scale: 32768.0000 (58526.3111)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1253  max mem: 15572
Epoch: [14]  [2150/2809]  eta: 0:06:12  lr: 0.000038  min_lr: 0.000000  loss: 3.9713 (3.9847)  class_acc: 0.2917 (0.2597)  loss_scale: 32768.0000 (58406.5607)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1194  max mem: 15572
Epoch: [14]  [2160/2809]  eta: 0:06:07  lr: 0.000038  min_lr: 0.000000  loss: 4.0519 (3.9849)  class_acc: 0.2500 (0.2597)  loss_scale: 32768.0000 (58287.9186)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1132  max mem: 15572
Epoch: [14]  [2170/2809]  eta: 0:06:01  lr: 0.000038  min_lr: 0.000000  loss: 3.8845 (3.9843)  class_acc: 0.2500 (0.2599)  loss_scale: 32768.0000 (58170.3694)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0865  max mem: 15572
Epoch: [14]  [2180/2809]  eta: 0:05:55  lr: 0.000038  min_lr: 0.000000  loss: 3.8845 (3.9841)  class_acc: 0.2500 (0.2597)  loss_scale: 32768.0000 (58053.8982)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0902  max mem: 15572
Epoch: [14]  [2190/2809]  eta: 0:05:50  lr: 0.000038  min_lr: 0.000000  loss: 4.0954 (3.9850)  class_acc: 0.2083 (0.2595)  loss_scale: 32768.0000 (57938.4902)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1266  max mem: 15572
Epoch: [14]  [2200/2809]  eta: 0:05:44  lr: 0.000038  min_lr: 0.000000  loss: 4.0558 (3.9847)  class_acc: 0.2500 (0.2594)  loss_scale: 32768.0000 (57824.1308)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1485  max mem: 15572
[2025-01-15 21:29:29,215] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:29:29,215] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [2210/2809]  eta: 0:05:39  lr: 0.000038  min_lr: 0.000000  loss: 3.8755 (3.9846)  class_acc: 0.2917 (0.2596)  loss_scale: 32768.0000 (57725.6264)  weight_decay: 0.0500 (0.0500)  time: 0.6550  data: 0.2202  max mem: 15572
Epoch: [14]  [2220/2809]  eta: 0:05:33  lr: 0.000038  min_lr: 0.000000  loss: 3.9788 (3.9846)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (57760.7924)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1536  max mem: 15572
Epoch: [14]  [2230/2809]  eta: 0:05:27  lr: 0.000038  min_lr: 0.000000  loss: 4.0357 (3.9848)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (57795.6432)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0764  max mem: 15572
Epoch: [14]  [2240/2809]  eta: 0:05:22  lr: 0.000038  min_lr: 0.000000  loss: 4.1655 (3.9858)  class_acc: 0.2500 (0.2594)  loss_scale: 65536.0000 (57830.1830)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1436  max mem: 15572
Epoch: [14]  [2250/2809]  eta: 0:05:16  lr: 0.000038  min_lr: 0.000000  loss: 4.1175 (3.9860)  class_acc: 0.2500 (0.2596)  loss_scale: 65536.0000 (57864.4158)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0775  max mem: 15572
Epoch: [14]  [2260/2809]  eta: 0:05:10  lr: 0.000038  min_lr: 0.000000  loss: 4.0631 (3.9868)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (57898.3459)  weight_decay: 0.0500 (0.0500)  time: 0.4776  data: 0.0248  max mem: 15572
Epoch: [14]  [2270/2809]  eta: 0:05:04  lr: 0.000038  min_lr: 0.000000  loss: 4.0631 (3.9865)  class_acc: 0.2500 (0.2595)  loss_scale: 65536.0000 (57931.9771)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.0932  max mem: 15572
Epoch: [14]  [2280/2809]  eta: 0:04:59  lr: 0.000038  min_lr: 0.000000  loss: 3.8362 (3.9857)  class_acc: 0.2500 (0.2596)  loss_scale: 65536.0000 (57965.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0928  max mem: 15572
Epoch: [14]  [2290/2809]  eta: 0:04:53  lr: 0.000038  min_lr: 0.000000  loss: 3.8279 (3.9862)  class_acc: 0.2500 (0.2596)  loss_scale: 65536.0000 (57998.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1726  max mem: 15572
Epoch: [14]  [2300/2809]  eta: 0:04:47  lr: 0.000038  min_lr: 0.000000  loss: 4.0127 (3.9863)  class_acc: 0.2500 (0.2596)  loss_scale: 65536.0000 (58031.1169)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1897  max mem: 15572
Epoch: [14]  [2310/2809]  eta: 0:04:42  lr: 0.000038  min_lr: 0.000000  loss: 3.8955 (3.9861)  class_acc: 0.2500 (0.2598)  loss_scale: 65536.0000 (58063.5915)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1207  max mem: 15572
Epoch: [14]  [2320/2809]  eta: 0:04:36  lr: 0.000038  min_lr: 0.000000  loss: 3.8911 (3.9856)  class_acc: 0.2917 (0.2601)  loss_scale: 65536.0000 (58095.7863)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0801  max mem: 15572
Epoch: [14]  [2330/2809]  eta: 0:04:30  lr: 0.000038  min_lr: 0.000000  loss: 3.8955 (3.9852)  class_acc: 0.2917 (0.2602)  loss_scale: 65536.0000 (58127.7048)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0148  max mem: 15572
[2025-01-15 21:30:39,611] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:30:39,611] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [2340/2809]  eta: 0:04:25  lr: 0.000038  min_lr: 0.000000  loss: 3.8955 (3.9845)  class_acc: 0.2917 (0.2603)  loss_scale: 65536.0000 (58243.3353)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0760  max mem: 15572
[2025-01-15 21:30:43,811] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41667
[2025-01-15 21:30:43,812] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:30:43,812] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2350/2809]  eta: 0:04:19  lr: 0.000038  min_lr: 0.000000  loss: 3.9586 (3.9846)  class_acc: 0.2083 (0.2603)  loss_scale: 65536.0000 (58274.3547)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.1969  max mem: 15572
Epoch: [14]  [2360/2809]  eta: 0:04:14  lr: 0.000038  min_lr: 0.000000  loss: 4.0516 (3.9846)  class_acc: 0.2083 (0.2603)  loss_scale: 65536.0000 (58305.1114)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1817  max mem: 15572
Epoch: [14]  [2370/2809]  eta: 0:04:08  lr: 0.000038  min_lr: 0.000000  loss: 3.9921 (3.9848)  class_acc: 0.2917 (0.2605)  loss_scale: 65536.0000 (58335.6086)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.0937  max mem: 15572
Epoch: [14]  [2380/2809]  eta: 0:04:02  lr: 0.000038  min_lr: 0.000000  loss: 3.9921 (3.9844)  class_acc: 0.2500 (0.2604)  loss_scale: 65536.0000 (58365.8496)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1134  max mem: 15572
Epoch: [14]  [2390/2809]  eta: 0:03:57  lr: 0.000038  min_lr: 0.000000  loss: 3.8129 (3.9838)  class_acc: 0.2500 (0.2607)  loss_scale: 65536.0000 (58395.8377)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1524  max mem: 15572
Epoch: [14]  [2400/2809]  eta: 0:03:51  lr: 0.000038  min_lr: 0.000000  loss: 3.9013 (3.9835)  class_acc: 0.2917 (0.2606)  loss_scale: 65536.0000 (58425.5760)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0868  max mem: 15572
Epoch: [14]  [2410/2809]  eta: 0:03:45  lr: 0.000038  min_lr: 0.000000  loss: 3.7828 (3.9829)  class_acc: 0.2500 (0.2608)  loss_scale: 65536.0000 (58455.0676)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0815  max mem: 15572
Epoch: [14]  [2420/2809]  eta: 0:03:39  lr: 0.000038  min_lr: 0.000000  loss: 3.8002 (3.9822)  class_acc: 0.2917 (0.2611)  loss_scale: 65536.0000 (58484.3156)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.0856  max mem: 15572
Epoch: [14]  [2430/2809]  eta: 0:03:34  lr: 0.000038  min_lr: 0.000000  loss: 3.8386 (3.9812)  class_acc: 0.2917 (0.2613)  loss_scale: 65536.0000 (58513.3229)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0900  max mem: 15572
Epoch: [14]  [2440/2809]  eta: 0:03:28  lr: 0.000038  min_lr: 0.000000  loss: 3.8386 (3.9806)  class_acc: 0.2500 (0.2613)  loss_scale: 65536.0000 (58542.0926)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1456  max mem: 15572
Epoch: [14]  [2450/2809]  eta: 0:03:22  lr: 0.000038  min_lr: 0.000000  loss: 3.8697 (3.9797)  class_acc: 0.2917 (0.2615)  loss_scale: 65536.0000 (58570.6275)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0983  max mem: 15572
Epoch: [14]  [2460/2809]  eta: 0:03:17  lr: 0.000038  min_lr: 0.000000  loss: 3.8743 (3.9800)  class_acc: 0.2083 (0.2613)  loss_scale: 65536.0000 (58598.9305)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1077  max mem: 15572
[2025-01-15 21:31:54,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:31:54,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [2470/2809]  eta: 0:03:11  lr: 0.000038  min_lr: 0.000000  loss: 3.9906 (3.9793)  class_acc: 0.2083 (0.2614)  loss_scale: 65536.0000 (58653.5265)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1305  max mem: 15572
[2025-01-15 21:31:56,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41799
[2025-01-15 21:31:56,712] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:31:56,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2480/2809]  eta: 0:03:05  lr: 0.000038  min_lr: 0.000000  loss: 3.7342 (3.9788)  class_acc: 0.2917 (0.2614)  loss_scale: 65536.0000 (58734.0975)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1158  max mem: 15572
Epoch: [14]  [2490/2809]  eta: 0:03:00  lr: 0.000038  min_lr: 0.000000  loss: 3.8565 (3.9791)  class_acc: 0.2500 (0.2613)  loss_scale: 65536.0000 (58761.4035)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1500  max mem: 15572
Epoch: [14]  [2500/2809]  eta: 0:02:54  lr: 0.000038  min_lr: 0.000000  loss: 3.8565 (3.9781)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (58788.4910)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1578  max mem: 15572
Epoch: [14]  [2510/2809]  eta: 0:02:49  lr: 0.000038  min_lr: 0.000000  loss: 3.8686 (3.9780)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (58815.3628)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1220  max mem: 15572
Epoch: [14]  [2520/2809]  eta: 0:02:43  lr: 0.000038  min_lr: 0.000000  loss: 3.9053 (3.9783)  class_acc: 0.2083 (0.2611)  loss_scale: 65536.0000 (58842.0214)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1231  max mem: 15572
Epoch: [14]  [2530/2809]  eta: 0:02:37  lr: 0.000038  min_lr: 0.000000  loss: 4.0807 (3.9785)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (58868.4694)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.0992  max mem: 15572
Epoch: [14]  [2540/2809]  eta: 0:02:32  lr: 0.000038  min_lr: 0.000000  loss: 3.9848 (3.9781)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (58894.7092)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0983  max mem: 15572
Epoch: [14]  [2550/2809]  eta: 0:02:26  lr: 0.000038  min_lr: 0.000000  loss: 3.9935 (3.9785)  class_acc: 0.2500 (0.2612)  loss_scale: 65536.0000 (58920.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1363  max mem: 15572
Epoch: [14]  [2560/2809]  eta: 0:02:20  lr: 0.000038  min_lr: 0.000000  loss: 4.1083 (3.9788)  class_acc: 0.2083 (0.2611)  loss_scale: 65536.0000 (58946.5740)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0801  max mem: 15572
Epoch: [14]  [2570/2809]  eta: 0:02:15  lr: 0.000038  min_lr: 0.000000  loss: 3.8261 (3.9784)  class_acc: 0.2500 (0.2613)  loss_scale: 65536.0000 (58972.2038)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0867  max mem: 15572
Epoch: [14]  [2580/2809]  eta: 0:02:09  lr: 0.000038  min_lr: 0.000000  loss: 4.0427 (3.9790)  class_acc: 0.2500 (0.2611)  loss_scale: 65536.0000 (58997.6350)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1751  max mem: 15572
Epoch: [14]  [2590/2809]  eta: 0:02:03  lr: 0.000038  min_lr: 0.000000  loss: 4.0423 (3.9784)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (59022.8699)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1845  max mem: 15572
Epoch: [14]  [2600/2809]  eta: 0:01:58  lr: 0.000038  min_lr: 0.000000  loss: 3.8619 (3.9778)  class_acc: 0.2500 (0.2615)  loss_scale: 65536.0000 (59047.9108)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1112  max mem: 15572
[2025-01-15 21:33:09,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:33:09,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:33:10,236] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41930
[2025-01-15 21:33:10,236] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:33:10,237] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2610/2809]  eta: 0:01:52  lr: 0.000038  min_lr: 0.000000  loss: 3.8619 (3.9775)  class_acc: 0.2083 (0.2614)  loss_scale: 65536.0000 (59122.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0508  max mem: 15572
Epoch: [14]  [2620/2809]  eta: 0:01:46  lr: 0.000038  min_lr: 0.000000  loss: 3.9506 (3.9776)  class_acc: 0.2083 (0.2614)  loss_scale: 65536.0000 (59147.4277)  weight_decay: 0.0500 (0.0500)  time: 0.4810  data: 0.0454  max mem: 15572
Epoch: [14]  [2630/2809]  eta: 0:01:41  lr: 0.000038  min_lr: 0.000000  loss: 3.9000 (3.9774)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (59171.7096)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1206  max mem: 15572
Epoch: [14]  [2640/2809]  eta: 0:01:35  lr: 0.000038  min_lr: 0.000000  loss: 3.8598 (3.9773)  class_acc: 0.2500 (0.2615)  loss_scale: 65536.0000 (59195.8076)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1847  max mem: 15572
Epoch: [14]  [2650/2809]  eta: 0:01:29  lr: 0.000038  min_lr: 0.000000  loss: 3.8598 (3.9769)  class_acc: 0.2917 (0.2617)  loss_scale: 65536.0000 (59219.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1202  max mem: 15572
Epoch: [14]  [2660/2809]  eta: 0:01:24  lr: 0.000038  min_lr: 0.000000  loss: 4.1094 (3.9774)  class_acc: 0.2500 (0.2615)  loss_scale: 65536.0000 (59243.4604)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0807  max mem: 15572
Epoch: [14]  [2670/2809]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000000  loss: 4.1688 (3.9777)  class_acc: 0.2083 (0.2615)  loss_scale: 65536.0000 (59267.0191)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1186  max mem: 15572
[2025-01-15 21:33:49,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=279, lr=[3.695228847971735e-07, 3.695228847971735e-07, 5.278898354245336e-07, 5.278898354245336e-07, 7.541283363207624e-07, 7.541283363207624e-07, 1.0773261947439463e-06, 1.0773261947439463e-06, 1.5390374210627805e-06, 1.5390374210627805e-06, 2.1986248872325438e-06, 2.1986248872325438e-06, 3.140892696046491e-06, 3.140892696046491e-06, 4.4869895657807015e-06, 4.4869895657807015e-06, 6.409985093972431e-06, 6.409985093972431e-06, 9.15712156281776e-06, 9.15712156281776e-06, 1.30816022325968e-05, 1.30816022325968e-05, 1.8688003189424e-05, 1.8688003189424e-05, 2.669714741346286e-05, 2.669714741346286e-05, 3.813878201923266e-05, 3.813878201923266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:33:49,896] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=28.436788246068424, CurrSamplesPerSec=29.5326033255216, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [14]  [2680/2809]  eta: 0:01:12  lr: 0.000038  min_lr: 0.000000  loss: 4.0732 (3.9775)  class_acc: 0.2917 (0.2617)  loss_scale: 65536.0000 (59290.4021)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1468  max mem: 15572
Epoch: [14]  [2690/2809]  eta: 0:01:07  lr: 0.000038  min_lr: 0.000000  loss: 3.9250 (3.9772)  class_acc: 0.2917 (0.2618)  loss_scale: 65536.0000 (59313.6113)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1162  max mem: 15572
Epoch: [14]  [2700/2809]  eta: 0:01:01  lr: 0.000038  min_lr: 0.000000  loss: 4.0628 (3.9776)  class_acc: 0.2500 (0.2618)  loss_scale: 65536.0000 (59336.6486)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1545  max mem: 15572
Epoch: [14]  [2710/2809]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000000  loss: 4.1286 (3.9780)  class_acc: 0.2083 (0.2617)  loss_scale: 65536.0000 (59359.5160)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1439  max mem: 15572
Epoch: [14]  [2720/2809]  eta: 0:00:50  lr: 0.000038  min_lr: 0.000000  loss: 4.1429 (3.9789)  class_acc: 0.2083 (0.2615)  loss_scale: 65536.0000 (59382.2154)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0842  max mem: 15572
Epoch: [14]  [2730/2809]  eta: 0:00:44  lr: 0.000038  min_lr: 0.000000  loss: 4.1930 (3.9798)  class_acc: 0.2083 (0.2614)  loss_scale: 65536.0000 (59404.7484)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1696  max mem: 15572
[2025-01-15 21:34:23,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:34:23,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:34:24,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42060
[2025-01-15 21:34:24,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:34:24,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [2740/2809]  eta: 0:00:39  lr: 0.000038  min_lr: 0.000000  loss: 4.0977 (3.9791)  class_acc: 0.2917 (0.2616)  loss_scale: 65536.0000 (59451.0266)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1583  max mem: 15572
Epoch: [14]  [2750/2809]  eta: 0:00:33  lr: 0.000038  min_lr: 0.000000  loss: 3.9373 (3.9793)  class_acc: 0.2917 (0.2616)  loss_scale: 65536.0000 (59473.1458)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0685  max mem: 15572
Epoch: [14]  [2760/2809]  eta: 0:00:27  lr: 0.000038  min_lr: 0.000000  loss: 3.9373 (3.9783)  class_acc: 0.2917 (0.2620)  loss_scale: 65536.0000 (59495.1047)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1225  max mem: 15572
Epoch: [14]  [2770/2809]  eta: 0:00:22  lr: 0.000038  min_lr: 0.000000  loss: 3.9181 (3.9786)  class_acc: 0.2500 (0.2620)  loss_scale: 65536.0000 (59516.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1260  max mem: 15572
Epoch: [14]  [2780/2809]  eta: 0:00:16  lr: 0.000038  min_lr: 0.000000  loss: 3.9302 (3.9785)  class_acc: 0.2500 (0.2622)  loss_scale: 65536.0000 (59538.5487)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1655  max mem: 15572
Epoch: [14]  [2790/2809]  eta: 0:00:10  lr: 0.000038  min_lr: 0.000000  loss: 3.9952 (3.9790)  class_acc: 0.2500 (0.2622)  loss_scale: 65536.0000 (59560.0373)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.2099  max mem: 15572
Epoch: [14]  [2800/2809]  eta: 0:00:05  lr: 0.000038  min_lr: 0.000000  loss: 4.1643 (3.9794)  class_acc: 0.2500 (0.2621)  loss_scale: 65536.0000 (59581.3724)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1927  max mem: 15572
Epoch: [14]  [2808/2809]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 4.1643 (3.9797)  class_acc: 0.2500 (0.2620)  loss_scale: 65536.0000 (59598.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.1127  max mem: 15572
Epoch: [14] Total time: 0:26:29 (0.5659 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 4.1643 (3.9797)  class_acc: 0.2500 (0.2620)  loss_scale: 65536.0000 (59598.3311)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:22:45  loss: 0.4045 (0.4045)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.0211  data: 4.8256  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 3.0051 (2.6514)  acc1: 27.7778 (32.3232)  acc5: 66.6667 (62.6263)  time: 0.7684  data: 0.5787  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 2.8671 (2.7305)  acc1: 27.7778 (34.9206)  acc5: 66.6667 (65.0794)  time: 0.3198  data: 0.1188  max mem: 15572
Val:  [ 30/272]  eta: 0:01:49  loss: 2.8671 (2.8099)  acc1: 33.3333 (32.4373)  acc5: 66.6667 (64.5161)  time: 0.2785  data: 0.0578  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.9962 (2.8376)  acc1: 22.2222 (30.4878)  acc5: 66.6667 (65.7182)  time: 0.2564  data: 0.0358  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 2.8677 (2.7613)  acc1: 22.2222 (32.6797)  acc5: 72.2222 (67.9739)  time: 0.2696  data: 0.0692  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.6778 (2.6056)  acc1: 55.5556 (37.3406)  acc5: 83.3333 (69.3989)  time: 0.3025  data: 0.1192  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 1.6282 (2.5214)  acc1: 66.6667 (39.5149)  acc5: 83.3333 (71.2833)  time: 0.3085  data: 0.1308  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.3551 (2.5312)  acc1: 38.8889 (39.3004)  acc5: 77.7778 (71.2620)  time: 0.3113  data: 0.1347  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.8437 (2.5769)  acc1: 33.3333 (38.5226)  acc5: 66.6667 (70.8181)  time: 0.3179  data: 0.1341  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.8437 (2.6205)  acc1: 33.3333 (38.1738)  acc5: 72.2222 (70.2420)  time: 0.3294  data: 0.1358  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.9100 (2.7029)  acc1: 11.1111 (35.9359)  acc5: 61.1111 (68.6687)  time: 0.3354  data: 0.1267  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.3458 (2.7453)  acc1: 11.1111 (34.7567)  acc5: 55.5556 (67.6309)  time: 0.2964  data: 0.0872  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.6942 (2.7031)  acc1: 27.7778 (35.8779)  acc5: 66.6667 (68.3206)  time: 0.3260  data: 0.1261  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.8871 (2.6852)  acc1: 50.0000 (36.6430)  acc5: 77.7778 (68.6367)  time: 0.3595  data: 0.1697  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.6984 (2.6907)  acc1: 33.3333 (36.1295)  acc5: 72.2222 (68.9110)  time: 0.2972  data: 0.1076  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.6937 (2.6688)  acc1: 33.3333 (37.3016)  acc5: 72.2222 (69.4962)  time: 0.2674  data: 0.0683  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.7513 (2.6978)  acc1: 33.3333 (36.7771)  acc5: 72.2222 (68.8434)  time: 0.2840  data: 0.0962  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.6562 (2.6842)  acc1: 33.3333 (37.0473)  acc5: 72.2222 (69.3370)  time: 0.3147  data: 0.1351  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7645 (2.7300)  acc1: 22.2222 (35.7184)  acc5: 66.6667 (67.9174)  time: 0.3317  data: 0.1526  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.8138 (2.7403)  acc1: 22.2222 (35.6551)  acc5: 55.5556 (67.8552)  time: 0.3047  data: 0.1275  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.2832 (2.7328)  acc1: 38.8889 (36.2823)  acc5: 77.7778 (68.0885)  time: 0.3296  data: 0.1340  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.2473 (2.7193)  acc1: 38.8889 (36.6516)  acc5: 77.7778 (68.2755)  time: 0.3697  data: 0.1610  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9540 (2.6847)  acc1: 55.5556 (37.8547)  acc5: 77.7778 (68.8552)  time: 0.2763  data: 0.0812  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7386 (2.6659)  acc1: 61.1111 (38.4278)  acc5: 83.3333 (69.2946)  time: 0.1791  data: 0.0006  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.7433 (2.6794)  acc1: 38.8889 (37.8707)  acc5: 72.2222 (68.9907)  time: 0.1829  data: 0.0089  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.6646 (2.6089)  acc1: 72.2222 (39.8680)  acc5: 83.3333 (69.9021)  time: 0.1931  data: 0.0149  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4611 (2.6082)  acc1: 66.6667 (39.6269)  acc5: 88.8889 (69.8647)  time: 0.1914  data: 0.0225  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4611 (2.6122)  acc1: 61.1111 (39.6273)  acc5: 88.8889 (69.8341)  time: 0.1795  data: 0.0164  max mem: 15572
Val: Total time: 0:01:23 (0.3071 s / it)
* Acc@1 39.627 Acc@5 69.834 loss 2.612
Accuracy of the network on the 4883 val videos: 39.6%
[2025-01-15 21:36:29,462] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 21:36:29,471] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 21:36:29,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 21:36:32,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 21:36:32,532] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.63%
Epoch: [15]  [   0/2809]  eta: 6:18:28  lr: 0.000038  min_lr: 0.000000  loss: 3.9133 (3.9133)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0843  data: 7.6583  max mem: 15572
Epoch: [15]  [  10/2809]  eta: 1:03:19  lr: 0.000038  min_lr: 0.000000  loss: 3.9133 (3.8595)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3573  data: 0.9113  max mem: 15572
Epoch: [15]  [  20/2809]  eta: 0:51:31  lr: 0.000038  min_lr: 0.000000  loss: 4.0952 (4.0234)  class_acc: 0.2083 (0.2540)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7596  data: 0.2995  max mem: 15572
Epoch: [15]  [  30/2809]  eta: 0:46:16  lr: 0.000038  min_lr: 0.000000  loss: 3.9426 (3.9318)  class_acc: 0.2083 (0.2581)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8019  data: 0.3458  max mem: 15572
Epoch: [15]  [  40/2809]  eta: 0:42:26  lr: 0.000038  min_lr: 0.000000  loss: 3.8468 (3.9413)  class_acc: 0.2083 (0.2510)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7214  data: 0.2848  max mem: 15572
Epoch: [15]  [  50/2809]  eta: 0:39:55  lr: 0.000038  min_lr: 0.000000  loss: 3.9180 (3.9469)  class_acc: 0.2500 (0.2590)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2307  max mem: 15572
[2025-01-15 21:37:18,494] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:37:18,495] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:37:19,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42192
[2025-01-15 21:37:19,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:37:19,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [  60/2809]  eta: 0:37:31  lr: 0.000038  min_lr: 0.000000  loss: 4.1681 (3.9965)  class_acc: 0.2083 (0.2459)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1701  max mem: 15572
Epoch: [15]  [  70/2809]  eta: 0:35:28  lr: 0.000038  min_lr: 0.000000  loss: 4.2469 (4.0144)  class_acc: 0.1667 (0.2441)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1068  max mem: 15572
Epoch: [15]  [  80/2809]  eta: 0:34:46  lr: 0.000038  min_lr: 0.000000  loss: 3.8668 (4.0033)  class_acc: 0.2083 (0.2449)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1517  max mem: 15572
Epoch: [15]  [  90/2809]  eta: 0:34:19  lr: 0.000038  min_lr: 0.000000  loss: 3.8668 (4.0089)  class_acc: 0.2083 (0.2473)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.2248  max mem: 15572
Epoch: [15]  [ 100/2809]  eta: 0:33:09  lr: 0.000038  min_lr: 0.000000  loss: 4.1440 (4.0261)  class_acc: 0.2083 (0.2475)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1807  max mem: 15572
Epoch: [15]  [ 110/2809]  eta: 0:31:38  lr: 0.000038  min_lr: 0.000000  loss: 4.1374 (4.0075)  class_acc: 0.2917 (0.2545)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.4591  data: 0.0608  max mem: 15572
Epoch: [15]  [ 120/2809]  eta: 0:30:34  lr: 0.000038  min_lr: 0.000000  loss: 3.8326 (3.9977)  class_acc: 0.2917 (0.2552)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.4177  data: 0.0006  max mem: 15572
Epoch: [15]  [ 130/2809]  eta: 0:29:40  lr: 0.000038  min_lr: 0.000000  loss: 3.9212 (3.9902)  class_acc: 0.2500 (0.2567)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.4484  data: 0.0006  max mem: 15572
Epoch: [15]  [ 140/2809]  eta: 0:29:08  lr: 0.000038  min_lr: 0.000000  loss: 3.9212 (3.9736)  class_acc: 0.2917 (0.2603)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.4933  data: 0.0413  max mem: 15572
Epoch: [15]  [ 150/2809]  eta: 0:28:52  lr: 0.000038  min_lr: 0.000000  loss: 3.9450 (3.9740)  class_acc: 0.3333 (0.2630)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1284  max mem: 15572
Epoch: [15]  [ 160/2809]  eta: 0:28:16  lr: 0.000038  min_lr: 0.000000  loss: 3.9003 (3.9660)  class_acc: 0.2500 (0.2635)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.1159  max mem: 15572
Epoch: [15]  [ 170/2809]  eta: 0:27:56  lr: 0.000038  min_lr: 0.000000  loss: 3.9003 (3.9703)  class_acc: 0.2500 (0.2629)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0736  max mem: 15572
Epoch: [15]  [ 180/2809]  eta: 0:27:42  lr: 0.000038  min_lr: 0.000000  loss: 3.9215 (3.9657)  class_acc: 0.2500 (0.2647)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1094  max mem: 15572
[2025-01-15 21:38:30,899] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:38:30,899] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:38:32,696] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42325
[2025-01-15 21:38:32,696] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:38:32,697] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 190/2809]  eta: 0:27:26  lr: 0.000038  min_lr: 0.000000  loss: 3.9215 (3.9729)  class_acc: 0.2500 (0.2629)  loss_scale: 65536.0000 (67937.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1275  max mem: 15572
Epoch: [15]  [ 200/2809]  eta: 0:27:10  lr: 0.000038  min_lr: 0.000000  loss: 3.9292 (3.9679)  class_acc: 0.2083 (0.2639)  loss_scale: 65536.0000 (67818.3483)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1079  max mem: 15572
Epoch: [15]  [ 210/2809]  eta: 0:26:52  lr: 0.000038  min_lr: 0.000000  loss: 3.9292 (3.9706)  class_acc: 0.2083 (0.2634)  loss_scale: 65536.0000 (67710.1801)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0975  max mem: 15572
Epoch: [15]  [ 220/2809]  eta: 0:26:47  lr: 0.000038  min_lr: 0.000000  loss: 3.9693 (3.9623)  class_acc: 0.2917 (0.2651)  loss_scale: 65536.0000 (67611.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1555  max mem: 15572
Epoch: [15]  [ 230/2809]  eta: 0:26:46  lr: 0.000038  min_lr: 0.000000  loss: 3.9111 (3.9532)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (67521.9394)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.2211  max mem: 15572
Epoch: [15]  [ 240/2809]  eta: 0:26:24  lr: 0.000038  min_lr: 0.000000  loss: 3.9433 (3.9536)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (67439.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1332  max mem: 15572
Epoch: [15]  [ 250/2809]  eta: 0:26:14  lr: 0.000038  min_lr: 0.000000  loss: 4.0015 (3.9526)  class_acc: 0.2083 (0.2658)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0887  max mem: 15572
Epoch: [15]  [ 260/2809]  eta: 0:26:02  lr: 0.000038  min_lr: 0.000000  loss: 4.0771 (3.9604)  class_acc: 0.2083 (0.2634)  loss_scale: 65536.0000 (67293.6705)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1375  max mem: 15572
Epoch: [15]  [ 270/2809]  eta: 0:25:45  lr: 0.000038  min_lr: 0.000000  loss: 4.0486 (3.9604)  class_acc: 0.2083 (0.2637)  loss_scale: 65536.0000 (67228.8118)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0920  max mem: 15572
Epoch: [15]  [ 280/2809]  eta: 0:25:37  lr: 0.000038  min_lr: 0.000000  loss: 3.8782 (3.9559)  class_acc: 0.2500 (0.2647)  loss_scale: 65536.0000 (67168.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1084  max mem: 15572
Epoch: [15]  [ 290/2809]  eta: 0:25:34  lr: 0.000038  min_lr: 0.000000  loss: 3.8172 (3.9567)  class_acc: 0.2500 (0.2652)  loss_scale: 65536.0000 (67112.4674)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1887  max mem: 15572
Epoch: [15]  [ 300/2809]  eta: 0:25:29  lr: 0.000038  min_lr: 0.000000  loss: 4.0095 (3.9539)  class_acc: 0.2917 (0.2656)  loss_scale: 65536.0000 (67060.0930)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.2036  max mem: 15572
Epoch: [15]  [ 310/2809]  eta: 0:25:28  lr: 0.000038  min_lr: 0.000000  loss: 3.8047 (3.9484)  class_acc: 0.2917 (0.2671)  loss_scale: 65536.0000 (67011.0868)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.2086  max mem: 15572
[2025-01-15 21:39:48,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:39:48,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:39:48,844] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42455
[2025-01-15 21:39:48,845] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:39:48,845] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 320/2809]  eta: 0:25:21  lr: 0.000038  min_lr: 0.000000  loss: 3.8524 (3.9497)  class_acc: 0.2917 (0.2667)  loss_scale: 65536.0000 (67169.2960)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1905  max mem: 15572
Epoch: [15]  [ 330/2809]  eta: 0:25:12  lr: 0.000038  min_lr: 0.000000  loss: 4.1007 (3.9577)  class_acc: 0.1667 (0.2637)  loss_scale: 65536.0000 (67119.9517)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1424  max mem: 15572
Epoch: [15]  [ 340/2809]  eta: 0:25:01  lr: 0.000038  min_lr: 0.000000  loss: 4.0640 (3.9548)  class_acc: 0.2083 (0.2655)  loss_scale: 65536.0000 (67073.5015)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1250  max mem: 15572
Epoch: [15]  [ 350/2809]  eta: 0:24:46  lr: 0.000038  min_lr: 0.000000  loss: 3.9283 (3.9586)  class_acc: 0.2917 (0.2656)  loss_scale: 65536.0000 (67029.6980)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0913  max mem: 15572
Epoch: [15]  [ 360/2809]  eta: 0:24:38  lr: 0.000038  min_lr: 0.000000  loss: 4.0725 (3.9555)  class_acc: 0.2500 (0.2657)  loss_scale: 65536.0000 (66988.3213)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.1001  max mem: 15572
Epoch: [15]  [ 370/2809]  eta: 0:24:30  lr: 0.000038  min_lr: 0.000000  loss: 4.2496 (3.9657)  class_acc: 0.2083 (0.2645)  loss_scale: 65536.0000 (66949.1752)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1289  max mem: 15572
Epoch: [15]  [ 380/2809]  eta: 0:24:26  lr: 0.000038  min_lr: 0.000000  loss: 4.2100 (3.9656)  class_acc: 0.2083 (0.2635)  loss_scale: 65536.0000 (66912.0840)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1351  max mem: 15572
Epoch: [15]  [ 390/2809]  eta: 0:24:14  lr: 0.000038  min_lr: 0.000000  loss: 4.0049 (3.9673)  class_acc: 0.2083 (0.2625)  loss_scale: 65536.0000 (66876.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1244  max mem: 15572
Epoch: [15]  [ 400/2809]  eta: 0:24:09  lr: 0.000038  min_lr: 0.000000  loss: 3.9935 (3.9654)  class_acc: 0.2500 (0.2642)  loss_scale: 65536.0000 (66843.4514)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1321  max mem: 15572
Epoch: [15]  [ 410/2809]  eta: 0:24:00  lr: 0.000038  min_lr: 0.000000  loss: 4.1388 (3.9680)  class_acc: 0.2500 (0.2624)  loss_scale: 65536.0000 (66811.6399)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1343  max mem: 15572
Epoch: [15]  [ 420/2809]  eta: 0:23:53  lr: 0.000038  min_lr: 0.000000  loss: 3.8140 (3.9615)  class_acc: 0.2500 (0.2646)  loss_scale: 65536.0000 (66781.3397)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1224  max mem: 15572
Epoch: [15]  [ 430/2809]  eta: 0:23:49  lr: 0.000038  min_lr: 0.000000  loss: 3.4902 (3.9500)  class_acc: 0.3750 (0.2676)  loss_scale: 65536.0000 (66752.4455)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1761  max mem: 15572
Epoch: [15]  [ 440/2809]  eta: 0:23:42  lr: 0.000038  min_lr: 0.000000  loss: 3.6757 (3.9503)  class_acc: 0.2917 (0.2670)  loss_scale: 65536.0000 (66724.8617)  weight_decay: 0.0500 (0.0500)  time: 0.6085  data: 0.1784  max mem: 15572
[2025-01-15 21:41:02,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:41:02,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 450/2809]  eta: 0:23:32  lr: 0.000038  min_lr: 0.000000  loss: 4.0066 (3.9514)  class_acc: 0.2500 (0.2676)  loss_scale: 65536.0000 (66989.1264)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1188  max mem: 15572
[2025-01-15 21:41:03,256] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42586
[2025-01-15 21:41:03,256] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:41:03,256] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 460/2809]  eta: 0:23:20  lr: 0.000038  min_lr: 0.000000  loss: 3.8856 (3.9492)  class_acc: 0.2917 (0.2675)  loss_scale: 65536.0000 (66957.6052)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0659  max mem: 15572
Epoch: [15]  [ 470/2809]  eta: 0:23:07  lr: 0.000038  min_lr: 0.000000  loss: 3.8285 (3.9460)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (66927.4225)  weight_decay: 0.0500 (0.0500)  time: 0.4661  data: 0.0288  max mem: 15572
Epoch: [15]  [ 480/2809]  eta: 0:22:56  lr: 0.000038  min_lr: 0.000000  loss: 3.7397 (3.9425)  class_acc: 0.2500 (0.2691)  loss_scale: 65536.0000 (66898.4948)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0402  max mem: 15572
Epoch: [15]  [ 490/2809]  eta: 0:22:46  lr: 0.000038  min_lr: 0.000000  loss: 3.8308 (3.9431)  class_acc: 0.2500 (0.2688)  loss_scale: 65536.0000 (66870.7454)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0609  max mem: 15572
Epoch: [15]  [ 500/2809]  eta: 0:22:42  lr: 0.000038  min_lr: 0.000000  loss: 4.0998 (3.9476)  class_acc: 0.1667 (0.2675)  loss_scale: 65536.0000 (66844.1038)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1188  max mem: 15572
Epoch: [15]  [ 510/2809]  eta: 0:22:36  lr: 0.000038  min_lr: 0.000000  loss: 4.0951 (3.9480)  class_acc: 0.2083 (0.2679)  loss_scale: 65536.0000 (66818.5049)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1790  max mem: 15572
Epoch: [15]  [ 520/2809]  eta: 0:22:30  lr: 0.000038  min_lr: 0.000000  loss: 4.1523 (3.9515)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (66793.8887)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1555  max mem: 15572
Epoch: [15]  [ 530/2809]  eta: 0:22:23  lr: 0.000038  min_lr: 0.000000  loss: 4.0631 (3.9493)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (66770.1996)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1327  max mem: 15572
Epoch: [15]  [ 540/2809]  eta: 0:22:16  lr: 0.000038  min_lr: 0.000000  loss: 3.7412 (3.9474)  class_acc: 0.2917 (0.2681)  loss_scale: 65536.0000 (66747.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1187  max mem: 15572
Epoch: [15]  [ 550/2809]  eta: 0:22:05  lr: 0.000038  min_lr: 0.000000  loss: 3.8848 (3.9482)  class_acc: 0.2083 (0.2673)  loss_scale: 65536.0000 (66725.4011)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0718  max mem: 15572
Epoch: [15]  [ 560/2809]  eta: 0:22:00  lr: 0.000038  min_lr: 0.000000  loss: 3.9012 (3.9443)  class_acc: 0.2500 (0.2679)  loss_scale: 65536.0000 (66704.1996)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1178  max mem: 15572
Epoch: [15]  [ 570/2809]  eta: 0:21:50  lr: 0.000038  min_lr: 0.000000  loss: 3.8684 (3.9448)  class_acc: 0.2500 (0.2668)  loss_scale: 65536.0000 (66683.7408)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1109  max mem: 15572
[2025-01-15 21:42:12,745] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:42:12,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 580/2809]  eta: 0:21:44  lr: 0.000038  min_lr: 0.000000  loss: 3.8716 (3.9468)  class_acc: 0.2083 (0.2666)  loss_scale: 65536.0000 (66776.7849)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0835  max mem: 15572
[2025-01-15 21:42:13,779] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42717
[2025-01-15 21:42:13,780] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:42:13,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 590/2809]  eta: 0:21:40  lr: 0.000038  min_lr: 0.000000  loss: 3.8716 (3.9453)  class_acc: 0.3333 (0.2676)  loss_scale: 65536.0000 (66866.6802)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1614  max mem: 15572
Epoch: [15]  [ 600/2809]  eta: 0:21:30  lr: 0.000038  min_lr: 0.000000  loss: 4.1018 (3.9499)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (66844.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1028  max mem: 15572
Epoch: [15]  [ 610/2809]  eta: 0:21:26  lr: 0.000038  min_lr: 0.000000  loss: 4.1282 (3.9516)  class_acc: 0.2500 (0.2667)  loss_scale: 65536.0000 (66823.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1090  max mem: 15572
Epoch: [15]  [ 620/2809]  eta: 0:21:20  lr: 0.000038  min_lr: 0.000000  loss: 4.0976 (3.9529)  class_acc: 0.2500 (0.2666)  loss_scale: 65536.0000 (66802.3961)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1558  max mem: 15572
Epoch: [15]  [ 630/2809]  eta: 0:21:16  lr: 0.000038  min_lr: 0.000000  loss: 4.0252 (3.9541)  class_acc: 0.2500 (0.2662)  loss_scale: 65536.0000 (66782.3265)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1585  max mem: 15572
Epoch: [15]  [ 640/2809]  eta: 0:21:09  lr: 0.000038  min_lr: 0.000000  loss: 4.0252 (3.9549)  class_acc: 0.2083 (0.2658)  loss_scale: 65536.0000 (66762.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1659  max mem: 15572
Epoch: [15]  [ 650/2809]  eta: 0:21:02  lr: 0.000038  min_lr: 0.000000  loss: 3.9000 (3.9517)  class_acc: 0.2083 (0.2662)  loss_scale: 65536.0000 (66744.0369)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1189  max mem: 15572
Epoch: [15]  [ 660/2809]  eta: 0:20:54  lr: 0.000038  min_lr: 0.000000  loss: 3.8201 (3.9505)  class_acc: 0.2917 (0.2668)  loss_scale: 65536.0000 (66725.7610)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0853  max mem: 15572
Epoch: [15]  [ 670/2809]  eta: 0:20:48  lr: 0.000038  min_lr: 0.000000  loss: 4.0698 (3.9528)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (66708.0298)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0985  max mem: 15572
Epoch: [15]  [ 680/2809]  eta: 0:20:40  lr: 0.000038  min_lr: 0.000000  loss: 4.1182 (3.9523)  class_acc: 0.2083 (0.2666)  loss_scale: 65536.0000 (66690.8194)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1086  max mem: 15572
Epoch: [15]  [ 690/2809]  eta: 0:20:37  lr: 0.000038  min_lr: 0.000000  loss: 4.1782 (3.9565)  class_acc: 0.2500 (0.2662)  loss_scale: 65536.0000 (66674.1071)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1549  max mem: 15572
Epoch: [15]  [ 700/2809]  eta: 0:20:29  lr: 0.000038  min_lr: 0.000000  loss: 4.1499 (3.9574)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (66657.8716)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1614  max mem: 15572
Epoch: [15]  [ 710/2809]  eta: 0:20:23  lr: 0.000038  min_lr: 0.000000  loss: 3.8977 (3.9554)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (66642.0928)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1060  max mem: 15572
[2025-01-15 21:43:27,531] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:43:27,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:43:27,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42847
[2025-01-15 21:43:27,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:43:27,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 720/2809]  eta: 0:20:17  lr: 0.000038  min_lr: 0.000000  loss: 3.8708 (3.9547)  class_acc: 0.2500 (0.2667)  loss_scale: 65536.0000 (66717.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1286  max mem: 15572
Epoch: [15]  [ 730/2809]  eta: 0:20:11  lr: 0.000038  min_lr: 0.000000  loss: 4.0643 (3.9539)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (66701.4829)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1141  max mem: 15572
Epoch: [15]  [ 740/2809]  eta: 0:20:04  lr: 0.000038  min_lr: 0.000000  loss: 4.0824 (3.9549)  class_acc: 0.2500 (0.2660)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.0963  max mem: 15572
Epoch: [15]  [ 750/2809]  eta: 0:19:56  lr: 0.000038  min_lr: 0.000000  loss: 3.8315 (3.9527)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (66670.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0764  max mem: 15572
[2025-01-15 21:43:52,487] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42892
[2025-01-15 21:43:52,487] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:43:52,487] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 760/2809]  eta: 0:19:48  lr: 0.000038  min_lr: 0.000000  loss: 3.7359 (3.9526)  class_acc: 0.2083 (0.2660)  loss_scale: 65536.0000 (66483.3009)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0604  max mem: 15572
Epoch: [15]  [ 770/2809]  eta: 0:19:43  lr: 0.000038  min_lr: 0.000000  loss: 4.0187 (3.9546)  class_acc: 0.2083 (0.2654)  loss_scale: 32768.0000 (66046.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1293  max mem: 15572
Epoch: [15]  [ 780/2809]  eta: 0:19:37  lr: 0.000038  min_lr: 0.000000  loss: 4.1568 (3.9532)  class_acc: 0.2500 (0.2660)  loss_scale: 32768.0000 (65619.9129)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1500  max mem: 15572
Epoch: [15]  [ 790/2809]  eta: 0:19:31  lr: 0.000038  min_lr: 0.000000  loss: 3.5419 (3.9502)  class_acc: 0.3333 (0.2665)  loss_scale: 32768.0000 (65204.5917)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1310  max mem: 15572
Epoch: [15]  [ 800/2809]  eta: 0:19:25  lr: 0.000038  min_lr: 0.000000  loss: 3.7934 (3.9503)  class_acc: 0.2500 (0.2660)  loss_scale: 32768.0000 (64799.6404)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1386  max mem: 15572
Epoch: [15]  [ 810/2809]  eta: 0:19:20  lr: 0.000038  min_lr: 0.000000  loss: 3.5501 (3.9454)  class_acc: 0.2917 (0.2674)  loss_scale: 32768.0000 (64404.6757)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1431  max mem: 15572
Epoch: [15]  [ 820/2809]  eta: 0:19:14  lr: 0.000038  min_lr: 0.000000  loss: 3.6792 (3.9438)  class_acc: 0.3333 (0.2679)  loss_scale: 32768.0000 (64019.3325)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1427  max mem: 15572
Epoch: [15]  [ 830/2809]  eta: 0:19:05  lr: 0.000038  min_lr: 0.000000  loss: 3.8810 (3.9438)  class_acc: 0.2917 (0.2677)  loss_scale: 32768.0000 (63643.2635)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0657  max mem: 15572
Epoch: [15]  [ 840/2809]  eta: 0:18:57  lr: 0.000038  min_lr: 0.000000  loss: 3.9663 (3.9425)  class_acc: 0.2917 (0.2682)  loss_scale: 32768.0000 (63276.1379)  weight_decay: 0.0500 (0.0500)  time: 0.4775  data: 0.0374  max mem: 15572
Epoch: [15]  [ 850/2809]  eta: 0:18:52  lr: 0.000038  min_lr: 0.000000  loss: 3.9292 (3.9427)  class_acc: 0.2917 (0.2684)  loss_scale: 32768.0000 (62917.6404)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1166  max mem: 15572
Epoch: [15]  [ 860/2809]  eta: 0:18:44  lr: 0.000038  min_lr: 0.000000  loss: 3.9292 (3.9399)  class_acc: 0.2917 (0.2691)  loss_scale: 32768.0000 (62567.4704)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0882  max mem: 15572
[2025-01-15 21:44:51,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=287, lr=[3.6380340292923513e-07, 3.6380340292923513e-07, 5.197191470417645e-07, 5.197191470417645e-07, 7.42455924345378e-07, 7.42455924345378e-07, 1.0606513204933972e-06, 1.0606513204933972e-06, 1.5152161721334248e-06, 1.5152161721334248e-06, 2.1645945316191783e-06, 2.1645945316191783e-06, 3.0922779023131118e-06, 3.0922779023131118e-06, 4.417539860447303e-06, 4.417539860447303e-06, 6.310771229210433e-06, 6.310771229210433e-06, 9.01538747030062e-06, 9.01538747030062e-06, 1.2879124957572313e-05, 1.2879124957572313e-05, 1.839874993938902e-05, 1.839874993938902e-05, 2.628392848484146e-05, 2.628392848484146e-05, 3.754846926405923e-05, 3.754846926405923e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:44:51,384] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=28.450292195869462, CurrSamplesPerSec=26.330817871307044, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [ 870/2809]  eta: 0:18:38  lr: 0.000038  min_lr: 0.000000  loss: 4.0229 (3.9428)  class_acc: 0.2917 (0.2691)  loss_scale: 32768.0000 (62225.3410)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0783  max mem: 15572
Epoch: [15]  [ 880/2809]  eta: 0:18:31  lr: 0.000038  min_lr: 0.000000  loss: 4.0185 (3.9432)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (61890.9784)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1047  max mem: 15572
[2025-01-15 21:45:04,125] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:45:04,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [ 890/2809]  eta: 0:18:24  lr: 0.000038  min_lr: 0.000000  loss: 3.9545 (3.9419)  class_acc: 0.2500 (0.2692)  loss_scale: 32768.0000 (61748.0045)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0701  max mem: 15572
Epoch: [15]  [ 900/2809]  eta: 0:18:20  lr: 0.000038  min_lr: 0.000000  loss: 4.1670 (3.9427)  class_acc: 0.2083 (0.2690)  loss_scale: 65536.0000 (61790.0466)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1293  max mem: 15572
Epoch: [15]  [ 910/2809]  eta: 0:18:14  lr: 0.000038  min_lr: 0.000000  loss: 4.1774 (3.9457)  class_acc: 0.2083 (0.2685)  loss_scale: 65536.0000 (61831.1658)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1632  max mem: 15572
Epoch: [15]  [ 920/2809]  eta: 0:18:07  lr: 0.000038  min_lr: 0.000000  loss: 4.0955 (3.9454)  class_acc: 0.2083 (0.2683)  loss_scale: 65536.0000 (61871.3920)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1143  max mem: 15572
Epoch: [15]  [ 930/2809]  eta: 0:18:04  lr: 0.000038  min_lr: 0.000000  loss: 3.9575 (3.9457)  class_acc: 0.2500 (0.2684)  loss_scale: 65536.0000 (61910.7540)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1742  max mem: 15572
Epoch: [15]  [ 940/2809]  eta: 0:17:58  lr: 0.000038  min_lr: 0.000000  loss: 4.0444 (3.9471)  class_acc: 0.2500 (0.2679)  loss_scale: 65536.0000 (61949.2795)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1808  max mem: 15572
Epoch: [15]  [ 950/2809]  eta: 0:17:51  lr: 0.000037  min_lr: 0.000000  loss: 4.1347 (3.9475)  class_acc: 0.2917 (0.2682)  loss_scale: 65536.0000 (61986.9947)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0975  max mem: 15572
Epoch: [15]  [ 960/2809]  eta: 0:17:45  lr: 0.000037  min_lr: 0.000000  loss: 3.9524 (3.9466)  class_acc: 0.2917 (0.2686)  loss_scale: 65536.0000 (62023.9251)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.1061  max mem: 15572
Epoch: [15]  [ 970/2809]  eta: 0:17:39  lr: 0.000037  min_lr: 0.000000  loss: 3.9024 (3.9459)  class_acc: 0.2500 (0.2682)  loss_scale: 65536.0000 (62060.0947)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1241  max mem: 15572
Epoch: [15]  [ 980/2809]  eta: 0:17:34  lr: 0.000037  min_lr: 0.000000  loss: 4.1376 (3.9488)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (62095.5270)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1662  max mem: 15572
Epoch: [15]  [ 990/2809]  eta: 0:17:28  lr: 0.000037  min_lr: 0.000000  loss: 3.9916 (3.9469)  class_acc: 0.2917 (0.2683)  loss_scale: 65536.0000 (62130.2442)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1632  max mem: 15572
Epoch: [15]  [1000/2809]  eta: 0:17:21  lr: 0.000037  min_lr: 0.000000  loss: 3.7193 (3.9446)  class_acc: 0.2917 (0.2690)  loss_scale: 65536.0000 (62164.2677)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0925  max mem: 15572
Epoch: [15]  [1010/2809]  eta: 0:17:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9293 (3.9456)  class_acc: 0.2500 (0.2686)  loss_scale: 65536.0000 (62197.6182)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1169  max mem: 15572
[2025-01-15 21:46:18,449] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:46:18,449] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:46:20,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43153
[2025-01-15 21:46:20,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:46:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1020/2809]  eta: 0:17:10  lr: 0.000037  min_lr: 0.000000  loss: 4.1465 (3.9454)  class_acc: 0.2500 (0.2684)  loss_scale: 65536.0000 (62487.0676)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1528  max mem: 15572
Epoch: [15]  [1030/2809]  eta: 0:17:04  lr: 0.000037  min_lr: 0.000000  loss: 3.9139 (3.9441)  class_acc: 0.2917 (0.2690)  loss_scale: 65536.0000 (62516.6402)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1191  max mem: 15572
Epoch: [15]  [1040/2809]  eta: 0:16:59  lr: 0.000037  min_lr: 0.000000  loss: 4.1299 (3.9472)  class_acc: 0.2500 (0.2686)  loss_scale: 65536.0000 (62545.6446)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1209  max mem: 15572
Epoch: [15]  [1050/2809]  eta: 0:16:53  lr: 0.000037  min_lr: 0.000000  loss: 4.2229 (3.9485)  class_acc: 0.2083 (0.2679)  loss_scale: 65536.0000 (62574.0971)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1304  max mem: 15572
Epoch: [15]  [1060/2809]  eta: 0:16:47  lr: 0.000037  min_lr: 0.000000  loss: 4.1383 (3.9494)  class_acc: 0.2083 (0.2678)  loss_scale: 65536.0000 (62602.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1250  max mem: 15572
Epoch: [15]  [1070/2809]  eta: 0:16:39  lr: 0.000037  min_lr: 0.000000  loss: 4.1313 (3.9504)  class_acc: 0.2083 (0.2678)  loss_scale: 65536.0000 (62629.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0731  max mem: 15572
Epoch: [15]  [1080/2809]  eta: 0:16:33  lr: 0.000037  min_lr: 0.000000  loss: 3.8779 (3.9507)  class_acc: 0.2500 (0.2675)  loss_scale: 65536.0000 (62656.2960)  weight_decay: 0.0500 (0.0500)  time: 0.4967  data: 0.0712  max mem: 15572
Epoch: [15]  [1090/2809]  eta: 0:16:28  lr: 0.000037  min_lr: 0.000000  loss: 4.0452 (3.9513)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (62682.6911)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1293  max mem: 15572
Epoch: [15]  [1100/2809]  eta: 0:16:21  lr: 0.000037  min_lr: 0.000000  loss: 4.1007 (3.9519)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (62708.6067)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1134  max mem: 15572
Epoch: [15]  [1110/2809]  eta: 0:16:16  lr: 0.000037  min_lr: 0.000000  loss: 4.1656 (3.9540)  class_acc: 0.2500 (0.2671)  loss_scale: 65536.0000 (62734.0558)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1378  max mem: 15572
Epoch: [15]  [1120/2809]  eta: 0:16:11  lr: 0.000037  min_lr: 0.000000  loss: 4.1656 (3.9562)  class_acc: 0.2083 (0.2667)  loss_scale: 65536.0000 (62759.0508)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.1716  max mem: 15572
Epoch: [15]  [1130/2809]  eta: 0:16:05  lr: 0.000037  min_lr: 0.000000  loss: 4.1005 (3.9570)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (62783.6039)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1367  max mem: 15572
Epoch: [15]  [1140/2809]  eta: 0:15:59  lr: 0.000037  min_lr: 0.000000  loss: 4.0146 (3.9571)  class_acc: 0.2500 (0.2666)  loss_scale: 65536.0000 (62807.7266)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1241  max mem: 15572
[2025-01-15 21:47:33,328] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:47:33,328] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1150/2809]  eta: 0:15:54  lr: 0.000037  min_lr: 0.000000  loss: 3.9099 (3.9584)  class_acc: 0.2917 (0.2664)  loss_scale: 65536.0000 (63059.1833)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1321  max mem: 15572
[2025-01-15 21:47:35,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43286
[2025-01-15 21:47:35,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:47:35,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1160/2809]  eta: 0:15:48  lr: 0.000037  min_lr: 0.000000  loss: 4.2445 (3.9603)  class_acc: 0.2083 (0.2659)  loss_scale: 65536.0000 (63080.5168)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1552  max mem: 15572
Epoch: [15]  [1170/2809]  eta: 0:15:42  lr: 0.000037  min_lr: 0.000000  loss: 4.2445 (3.9588)  class_acc: 0.2500 (0.2661)  loss_scale: 65536.0000 (63101.4859)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1300  max mem: 15572
Epoch: [15]  [1180/2809]  eta: 0:15:37  lr: 0.000037  min_lr: 0.000000  loss: 4.1757 (3.9610)  class_acc: 0.2500 (0.2658)  loss_scale: 65536.0000 (63122.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1257  max mem: 15572
Epoch: [15]  [1190/2809]  eta: 0:15:31  lr: 0.000037  min_lr: 0.000000  loss: 4.0705 (3.9600)  class_acc: 0.2500 (0.2661)  loss_scale: 65536.0000 (63142.3678)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1361  max mem: 15572
Epoch: [15]  [1200/2809]  eta: 0:15:26  lr: 0.000037  min_lr: 0.000000  loss: 4.0324 (3.9605)  class_acc: 0.2917 (0.2662)  loss_scale: 65536.0000 (63162.2981)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1607  max mem: 15572
Epoch: [15]  [1210/2809]  eta: 0:15:21  lr: 0.000037  min_lr: 0.000000  loss: 4.2002 (3.9620)  class_acc: 0.2083 (0.2659)  loss_scale: 65536.0000 (63181.8993)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1944  max mem: 15572
Epoch: [15]  [1220/2809]  eta: 0:15:14  lr: 0.000037  min_lr: 0.000000  loss: 4.0598 (3.9615)  class_acc: 0.2500 (0.2660)  loss_scale: 65536.0000 (63201.1794)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1299  max mem: 15572
Epoch: [15]  [1230/2809]  eta: 0:15:09  lr: 0.000037  min_lr: 0.000000  loss: 3.9127 (3.9609)  class_acc: 0.2500 (0.2656)  loss_scale: 65536.0000 (63220.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.0942  max mem: 15572
Epoch: [15]  [1240/2809]  eta: 0:15:02  lr: 0.000037  min_lr: 0.000000  loss: 3.9127 (3.9604)  class_acc: 0.2500 (0.2661)  loss_scale: 65536.0000 (63238.8074)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0905  max mem: 15572
Epoch: [15]  [1250/2809]  eta: 0:14:57  lr: 0.000037  min_lr: 0.000000  loss: 4.0001 (3.9611)  class_acc: 0.2917 (0.2662)  loss_scale: 65536.0000 (63257.1703)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1371  max mem: 15572
Epoch: [15]  [1260/2809]  eta: 0:14:51  lr: 0.000037  min_lr: 0.000000  loss: 4.0211 (3.9600)  class_acc: 0.2917 (0.2667)  loss_scale: 65536.0000 (63275.2419)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1552  max mem: 15572
Epoch: [15]  [1270/2809]  eta: 0:14:47  lr: 0.000037  min_lr: 0.000000  loss: 3.9886 (3.9610)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (63293.0291)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.1827  max mem: 15572
[2025-01-15 21:48:50,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:48:50,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1280/2809]  eta: 0:14:40  lr: 0.000037  min_lr: 0.000000  loss: 4.0033 (3.9607)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (63361.6987)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1671  max mem: 15572
[2025-01-15 21:48:51,739] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43417
[2025-01-15 21:48:51,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:48:51,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1290/2809]  eta: 0:14:35  lr: 0.000037  min_lr: 0.000000  loss: 4.0835 (3.9621)  class_acc: 0.2500 (0.2660)  loss_scale: 65536.0000 (63429.3044)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1490  max mem: 15572
Epoch: [15]  [1300/2809]  eta: 0:14:29  lr: 0.000037  min_lr: 0.000000  loss: 4.1279 (3.9635)  class_acc: 0.2083 (0.2661)  loss_scale: 65536.0000 (63445.4973)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1337  max mem: 15572
Epoch: [15]  [1310/2809]  eta: 0:14:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9637 (3.9635)  class_acc: 0.2917 (0.2664)  loss_scale: 65536.0000 (63461.4432)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0699  max mem: 15572
Epoch: [15]  [1320/2809]  eta: 0:14:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9660 (3.9648)  class_acc: 0.2083 (0.2660)  loss_scale: 65536.0000 (63477.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1095  max mem: 15572
Epoch: [15]  [1330/2809]  eta: 0:14:11  lr: 0.000037  min_lr: 0.000000  loss: 3.9808 (3.9640)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (63492.6161)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1671  max mem: 15572
[2025-01-15 21:49:22,842] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43472
[2025-01-15 21:49:22,843] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 21:49:22,844] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [1340/2809]  eta: 0:14:05  lr: 0.000037  min_lr: 0.000000  loss: 3.8872 (3.9638)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (63410.1119)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1519  max mem: 15572
Epoch: [15]  [1350/2809]  eta: 0:14:00  lr: 0.000037  min_lr: 0.000000  loss: 3.8863 (3.9621)  class_acc: 0.2500 (0.2668)  loss_scale: 32768.0000 (63183.3013)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1192  max mem: 15572
Epoch: [15]  [1360/2809]  eta: 0:13:54  lr: 0.000037  min_lr: 0.000000  loss: 3.6893 (3.9617)  class_acc: 0.2500 (0.2666)  loss_scale: 32768.0000 (62959.8237)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1253  max mem: 15572
Epoch: [15]  [1370/2809]  eta: 0:13:47  lr: 0.000037  min_lr: 0.000000  loss: 3.7422 (3.9618)  class_acc: 0.2083 (0.2664)  loss_scale: 32768.0000 (62739.6061)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.1031  max mem: 15572
Epoch: [15]  [1380/2809]  eta: 0:13:42  lr: 0.000037  min_lr: 0.000000  loss: 3.9305 (3.9623)  class_acc: 0.2500 (0.2665)  loss_scale: 32768.0000 (62522.5778)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1400  max mem: 15572
Epoch: [15]  [1390/2809]  eta: 0:13:37  lr: 0.000037  min_lr: 0.000000  loss: 4.0622 (3.9633)  class_acc: 0.2500 (0.2662)  loss_scale: 32768.0000 (62308.6700)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1957  max mem: 15572
Epoch: [15]  [1400/2809]  eta: 0:13:31  lr: 0.000037  min_lr: 0.000000  loss: 3.9849 (3.9625)  class_acc: 0.2500 (0.2662)  loss_scale: 32768.0000 (62097.8158)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1489  max mem: 15572
Epoch: [15]  [1410/2809]  eta: 0:13:25  lr: 0.000037  min_lr: 0.000000  loss: 3.6821 (3.9617)  class_acc: 0.2500 (0.2662)  loss_scale: 32768.0000 (61889.9504)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1277  max mem: 15572
Epoch: [15]  [1420/2809]  eta: 0:13:19  lr: 0.000037  min_lr: 0.000000  loss: 4.0459 (3.9636)  class_acc: 0.2083 (0.2656)  loss_scale: 32768.0000 (61685.0106)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1351  max mem: 15572
Epoch: [15]  [1430/2809]  eta: 0:13:13  lr: 0.000037  min_lr: 0.000000  loss: 4.0928 (3.9630)  class_acc: 0.2083 (0.2658)  loss_scale: 32768.0000 (61482.9350)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0717  max mem: 15572
Epoch: [15]  [1440/2809]  eta: 0:13:07  lr: 0.000037  min_lr: 0.000000  loss: 3.9601 (3.9624)  class_acc: 0.2500 (0.2660)  loss_scale: 32768.0000 (61283.6641)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0666  max mem: 15572
Epoch: [15]  [1450/2809]  eta: 0:13:01  lr: 0.000037  min_lr: 0.000000  loss: 3.9726 (3.9621)  class_acc: 0.2500 (0.2661)  loss_scale: 32768.0000 (61087.1399)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1336  max mem: 15572
Epoch: [15]  [1460/2809]  eta: 0:12:56  lr: 0.000037  min_lr: 0.000000  loss: 3.9632 (3.9622)  class_acc: 0.2917 (0.2665)  loss_scale: 32768.0000 (60893.3060)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1613  max mem: 15572
[2025-01-15 21:50:36,350] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:50:36,350] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [1470/2809]  eta: 0:12:49  lr: 0.000037  min_lr: 0.000000  loss: 3.9632 (3.9628)  class_acc: 0.2917 (0.2666)  loss_scale: 32768.0000 (60813.4874)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0812  max mem: 15572
Epoch: [15]  [1480/2809]  eta: 0:12:43  lr: 0.000037  min_lr: 0.000000  loss: 3.9165 (3.9622)  class_acc: 0.2500 (0.2667)  loss_scale: 65536.0000 (60845.3747)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0666  max mem: 15572
Epoch: [15]  [1490/2809]  eta: 0:12:37  lr: 0.000037  min_lr: 0.000000  loss: 3.7563 (3.9594)  class_acc: 0.3333 (0.2674)  loss_scale: 65536.0000 (60876.8343)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1089  max mem: 15572
Epoch: [15]  [1500/2809]  eta: 0:12:31  lr: 0.000037  min_lr: 0.000000  loss: 3.7563 (3.9605)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (60907.8748)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0723  max mem: 15572
Epoch: [15]  [1510/2809]  eta: 0:12:24  lr: 0.000037  min_lr: 0.000000  loss: 4.0413 (3.9598)  class_acc: 0.2500 (0.2674)  loss_scale: 65536.0000 (60938.5043)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0661  max mem: 15572
Epoch: [15]  [1520/2809]  eta: 0:12:20  lr: 0.000037  min_lr: 0.000000  loss: 4.0070 (3.9609)  class_acc: 0.2500 (0.2673)  loss_scale: 65536.0000 (60968.7311)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1462  max mem: 15572
Epoch: [15]  [1530/2809]  eta: 0:12:13  lr: 0.000037  min_lr: 0.000000  loss: 4.0070 (3.9609)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (60998.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1103  max mem: 15572
Epoch: [15]  [1540/2809]  eta: 0:12:08  lr: 0.000037  min_lr: 0.000000  loss: 4.0839 (3.9619)  class_acc: 0.2500 (0.2669)  loss_scale: 65536.0000 (61028.0078)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1233  max mem: 15572
Epoch: [15]  [1550/2809]  eta: 0:12:01  lr: 0.000037  min_lr: 0.000000  loss: 4.0839 (3.9634)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (61057.0729)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1234  max mem: 15572
Epoch: [15]  [1560/2809]  eta: 0:11:55  lr: 0.000037  min_lr: 0.000000  loss: 4.3289 (3.9647)  class_acc: 0.2083 (0.2662)  loss_scale: 65536.0000 (61085.7655)  weight_decay: 0.0500 (0.0500)  time: 0.4693  data: 0.0334  max mem: 15572
Epoch: [15]  [1570/2809]  eta: 0:11:49  lr: 0.000037  min_lr: 0.000000  loss: 4.1153 (3.9643)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (61114.0929)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0976  max mem: 15572
Epoch: [15]  [1580/2809]  eta: 0:11:44  lr: 0.000037  min_lr: 0.000000  loss: 4.1153 (3.9643)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (61142.0620)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1381  max mem: 15572
Epoch: [15]  [1590/2809]  eta: 0:11:37  lr: 0.000037  min_lr: 0.000000  loss: 4.0608 (3.9643)  class_acc: 0.2500 (0.2668)  loss_scale: 65536.0000 (61169.6794)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0939  max mem: 15572
[2025-01-15 21:51:46,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:51:46,673] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:51:48,021] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43732
[2025-01-15 21:51:48,021] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:51:48,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1600/2809]  eta: 0:11:31  lr: 0.000037  min_lr: 0.000000  loss: 3.8887 (3.9632)  class_acc: 0.2917 (0.2671)  loss_scale: 65536.0000 (61319.7552)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0861  max mem: 15572
Epoch: [15]  [1610/2809]  eta: 0:11:26  lr: 0.000037  min_lr: 0.000000  loss: 4.0132 (3.9640)  class_acc: 0.2917 (0.2671)  loss_scale: 65536.0000 (61345.9268)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1406  max mem: 15572
Epoch: [15]  [1620/2809]  eta: 0:11:20  lr: 0.000037  min_lr: 0.000000  loss: 4.0267 (3.9638)  class_acc: 0.2500 (0.2671)  loss_scale: 65536.0000 (61371.7754)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1493  max mem: 15572
Epoch: [15]  [1630/2809]  eta: 0:11:14  lr: 0.000037  min_lr: 0.000000  loss: 3.8444 (3.9632)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (61397.3072)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0917  max mem: 15572
Epoch: [15]  [1640/2809]  eta: 0:11:08  lr: 0.000037  min_lr: 0.000000  loss: 3.8691 (3.9631)  class_acc: 0.2917 (0.2672)  loss_scale: 65536.0000 (61422.5277)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0713  max mem: 15572
Epoch: [15]  [1650/2809]  eta: 0:11:03  lr: 0.000037  min_lr: 0.000000  loss: 3.9691 (3.9635)  class_acc: 0.2500 (0.2670)  loss_scale: 65536.0000 (61447.4428)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1635  max mem: 15572
Epoch: [15]  [1660/2809]  eta: 0:10:57  lr: 0.000037  min_lr: 0.000000  loss: 3.9377 (3.9626)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (61472.0578)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1400  max mem: 15572
Epoch: [15]  [1670/2809]  eta: 0:10:51  lr: 0.000037  min_lr: 0.000000  loss: 3.8213 (3.9616)  class_acc: 0.2917 (0.2673)  loss_scale: 65536.0000 (61496.3782)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0958  max mem: 15572
Epoch: [15]  [1680/2809]  eta: 0:10:45  lr: 0.000037  min_lr: 0.000000  loss: 3.9780 (3.9620)  class_acc: 0.2917 (0.2674)  loss_scale: 65536.0000 (61520.4093)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0757  max mem: 15572
Epoch: [15]  [1690/2809]  eta: 0:10:39  lr: 0.000037  min_lr: 0.000000  loss: 3.9859 (3.9632)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (61544.1561)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0422  max mem: 15572
Epoch: [15]  [1700/2809]  eta: 0:10:33  lr: 0.000037  min_lr: 0.000000  loss: 3.9682 (3.9623)  class_acc: 0.2917 (0.2673)  loss_scale: 65536.0000 (61567.6238)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1455  max mem: 15572
Epoch: [15]  [1710/2809]  eta: 0:10:28  lr: 0.000037  min_lr: 0.000000  loss: 3.9817 (3.9628)  class_acc: 0.2500 (0.2669)  loss_scale: 65536.0000 (61590.8171)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1867  max mem: 15572
Epoch: [15]  [1720/2809]  eta: 0:10:22  lr: 0.000037  min_lr: 0.000000  loss: 4.0571 (3.9624)  class_acc: 0.2500 (0.2671)  loss_scale: 65536.0000 (61613.7408)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1324  max mem: 15572
[2025-01-15 21:53:00,274] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:53:00,274] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:53:00,751] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43862
[2025-01-15 21:53:00,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:53:00,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1730/2809]  eta: 0:10:16  lr: 0.000037  min_lr: 0.000000  loss: 3.8231 (3.9613)  class_acc: 0.2917 (0.2675)  loss_scale: 65536.0000 (61674.2600)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0746  max mem: 15572
Epoch: [15]  [1740/2809]  eta: 0:10:10  lr: 0.000037  min_lr: 0.000000  loss: 3.8231 (3.9611)  class_acc: 0.3333 (0.2676)  loss_scale: 65536.0000 (61696.4411)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0749  max mem: 15572
Epoch: [15]  [1750/2809]  eta: 0:10:04  lr: 0.000037  min_lr: 0.000000  loss: 3.7213 (3.9607)  class_acc: 0.2917 (0.2676)  loss_scale: 65536.0000 (61718.3689)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1406  max mem: 15572
Epoch: [15]  [1760/2809]  eta: 0:09:59  lr: 0.000037  min_lr: 0.000000  loss: 3.6684 (3.9585)  class_acc: 0.2917 (0.2681)  loss_scale: 65536.0000 (61740.0477)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1259  max mem: 15572
Epoch: [15]  [1770/2809]  eta: 0:09:53  lr: 0.000037  min_lr: 0.000000  loss: 3.8239 (3.9591)  class_acc: 0.2917 (0.2681)  loss_scale: 65536.0000 (61761.4816)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1218  max mem: 15572
Epoch: [15]  [1780/2809]  eta: 0:09:47  lr: 0.000037  min_lr: 0.000000  loss: 4.0449 (3.9601)  class_acc: 0.2083 (0.2678)  loss_scale: 65536.0000 (61782.6749)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1170  max mem: 15572
Epoch: [15]  [1790/2809]  eta: 0:09:42  lr: 0.000037  min_lr: 0.000000  loss: 3.9198 (3.9601)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (61803.6315)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1296  max mem: 15572
Epoch: [15]  [1800/2809]  eta: 0:09:36  lr: 0.000037  min_lr: 0.000000  loss: 3.9963 (3.9608)  class_acc: 0.2083 (0.2675)  loss_scale: 65536.0000 (61824.3554)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1361  max mem: 15572
Epoch: [15]  [1810/2809]  eta: 0:09:30  lr: 0.000037  min_lr: 0.000000  loss: 4.0770 (3.9609)  class_acc: 0.2083 (0.2676)  loss_scale: 65536.0000 (61844.8504)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1337  max mem: 15572
Epoch: [15]  [1820/2809]  eta: 0:09:24  lr: 0.000037  min_lr: 0.000000  loss: 3.9616 (3.9609)  class_acc: 0.2917 (0.2678)  loss_scale: 65536.0000 (61865.1203)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0922  max mem: 15572
Epoch: [15]  [1830/2809]  eta: 0:09:18  lr: 0.000037  min_lr: 0.000000  loss: 3.9257 (3.9611)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (61885.1688)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0903  max mem: 15572
Epoch: [15]  [1840/2809]  eta: 0:09:13  lr: 0.000037  min_lr: 0.000000  loss: 4.0438 (3.9617)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (61904.9995)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.1709  max mem: 15572
Epoch: [15]  [1850/2809]  eta: 0:09:08  lr: 0.000037  min_lr: 0.000000  loss: 4.0438 (3.9621)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (61924.6159)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1822  max mem: 15572
[2025-01-15 21:54:14,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:54:14,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:54:16,140] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43994
[2025-01-15 21:54:16,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:54:16,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1860/2809]  eta: 0:09:02  lr: 0.000037  min_lr: 0.000000  loss: 4.1537 (3.9632)  class_acc: 0.1667 (0.2675)  loss_scale: 65536.0000 (62049.6679)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1280  max mem: 15572
[2025-01-15 21:54:20,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=294, lr=[3.5794456282147766e-07, 3.5794456282147766e-07, 5.113493754592539e-07, 5.113493754592539e-07, 7.304991077989342e-07, 7.304991077989342e-07, 1.0435701539984775e-06, 1.0435701539984775e-06, 1.4908145057121107e-06, 1.4908145057121107e-06, 2.1297350081601583e-06, 2.1297350081601583e-06, 3.0424785830859404e-06, 3.0424785830859404e-06, 4.346397975837058e-06, 4.346397975837058e-06, 6.2091399654815116e-06, 6.2091399654815116e-06, 8.870199950687875e-06, 8.870199950687875e-06, 1.2671714215268393e-05, 1.2671714215268393e-05, 1.8102448878954847e-05, 1.8102448878954847e-05, 2.5860641255649787e-05, 2.5860641255649787e-05, 3.694377322235684e-05, 3.694377322235684e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 21:54:20,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=28.455626070882197, CurrSamplesPerSec=28.983802310796616, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [15]  [1870/2809]  eta: 0:08:57  lr: 0.000037  min_lr: 0.000000  loss: 4.1730 (3.9637)  class_acc: 0.1667 (0.2672)  loss_scale: 65536.0000 (62068.3014)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1562  max mem: 15572
Epoch: [15]  [1880/2809]  eta: 0:08:51  lr: 0.000037  min_lr: 0.000000  loss: 3.9898 (3.9628)  class_acc: 0.2500 (0.2675)  loss_scale: 65536.0000 (62086.7368)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1867  max mem: 15572
Epoch: [15]  [1890/2809]  eta: 0:08:45  lr: 0.000037  min_lr: 0.000000  loss: 3.8174 (3.9624)  class_acc: 0.2917 (0.2675)  loss_scale: 65536.0000 (62104.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1021  max mem: 15572
Epoch: [15]  [1900/2809]  eta: 0:08:39  lr: 0.000037  min_lr: 0.000000  loss: 3.8798 (3.9627)  class_acc: 0.2500 (0.2676)  loss_scale: 65536.0000 (62123.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1224  max mem: 15572
Epoch: [15]  [1910/2809]  eta: 0:08:34  lr: 0.000037  min_lr: 0.000000  loss: 3.9340 (3.9628)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (62140.8854)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1517  max mem: 15572
Epoch: [15]  [1920/2809]  eta: 0:08:28  lr: 0.000037  min_lr: 0.000000  loss: 4.0009 (3.9639)  class_acc: 0.2500 (0.2675)  loss_scale: 65536.0000 (62158.5591)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1150  max mem: 15572
Epoch: [15]  [1930/2809]  eta: 0:08:22  lr: 0.000037  min_lr: 0.000000  loss: 4.0356 (3.9641)  class_acc: 0.2500 (0.2675)  loss_scale: 65536.0000 (62176.0497)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1195  max mem: 15572
Epoch: [15]  [1940/2809]  eta: 0:08:16  lr: 0.000037  min_lr: 0.000000  loss: 4.0356 (3.9646)  class_acc: 0.2500 (0.2674)  loss_scale: 65536.0000 (62193.3601)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1088  max mem: 15572
Epoch: [15]  [1950/2809]  eta: 0:08:10  lr: 0.000037  min_lr: 0.000000  loss: 3.9607 (3.9647)  class_acc: 0.1667 (0.2670)  loss_scale: 65536.0000 (62210.4931)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1052  max mem: 15572
Epoch: [15]  [1960/2809]  eta: 0:08:05  lr: 0.000037  min_lr: 0.000000  loss: 3.8650 (3.9640)  class_acc: 0.2083 (0.2672)  loss_scale: 65536.0000 (62227.4513)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1794  max mem: 15572
Epoch: [15]  [1970/2809]  eta: 0:07:59  lr: 0.000037  min_lr: 0.000000  loss: 3.9608 (3.9646)  class_acc: 0.2500 (0.2672)  loss_scale: 65536.0000 (62244.2374)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1443  max mem: 15572
Epoch: [15]  [1980/2809]  eta: 0:07:54  lr: 0.000037  min_lr: 0.000000  loss: 4.1161 (3.9658)  class_acc: 0.2083 (0.2670)  loss_scale: 65536.0000 (62260.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1104  max mem: 15572
[2025-01-15 21:55:30,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:55:30,357] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1990/2809]  eta: 0:07:48  lr: 0.000037  min_lr: 0.000000  loss: 4.0689 (3.9661)  class_acc: 0.2917 (0.2672)  loss_scale: 65536.0000 (62376.0522)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0890  max mem: 15572
[2025-01-15 21:55:31,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44126
[2025-01-15 21:55:31,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:55:31,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2000/2809]  eta: 0:07:41  lr: 0.000037  min_lr: 0.000000  loss: 3.9256 (3.9652)  class_acc: 0.3333 (0.2674)  loss_scale: 65536.0000 (62391.8441)  weight_decay: 0.0500 (0.0500)  time: 0.4637  data: 0.0189  max mem: 15572
Epoch: [15]  [2010/2809]  eta: 0:07:36  lr: 0.000037  min_lr: 0.000000  loss: 3.9006 (3.9647)  class_acc: 0.3333 (0.2678)  loss_scale: 65536.0000 (62407.4789)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0958  max mem: 15572
Epoch: [15]  [2020/2809]  eta: 0:07:30  lr: 0.000037  min_lr: 0.000000  loss: 3.9265 (3.9646)  class_acc: 0.3333 (0.2681)  loss_scale: 65536.0000 (62422.9589)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1178  max mem: 15572
Epoch: [15]  [2030/2809]  eta: 0:07:25  lr: 0.000037  min_lr: 0.000000  loss: 4.1316 (3.9652)  class_acc: 0.2917 (0.2681)  loss_scale: 65536.0000 (62438.2866)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1307  max mem: 15572
Epoch: [15]  [2040/2809]  eta: 0:07:19  lr: 0.000037  min_lr: 0.000000  loss: 4.1636 (3.9659)  class_acc: 0.2083 (0.2679)  loss_scale: 65536.0000 (62453.4640)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1368  max mem: 15572
Epoch: [15]  [2050/2809]  eta: 0:07:13  lr: 0.000037  min_lr: 0.000000  loss: 4.0735 (3.9659)  class_acc: 0.1667 (0.2678)  loss_scale: 65536.0000 (62468.4934)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0923  max mem: 15572
Epoch: [15]  [2060/2809]  eta: 0:07:07  lr: 0.000037  min_lr: 0.000000  loss: 4.0250 (3.9670)  class_acc: 0.2083 (0.2676)  loss_scale: 65536.0000 (62483.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0873  max mem: 15572
Epoch: [15]  [2070/2809]  eta: 0:07:01  lr: 0.000037  min_lr: 0.000000  loss: 4.0680 (3.9668)  class_acc: 0.2500 (0.2676)  loss_scale: 65536.0000 (62498.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0733  max mem: 15572
Epoch: [15]  [2080/2809]  eta: 0:06:56  lr: 0.000037  min_lr: 0.000000  loss: 3.8646 (3.9662)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (62512.7150)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1376  max mem: 15572
Epoch: [15]  [2090/2809]  eta: 0:06:50  lr: 0.000037  min_lr: 0.000000  loss: 3.8646 (3.9667)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (62527.1736)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1583  max mem: 15572
Epoch: [15]  [2100/2809]  eta: 0:06:44  lr: 0.000037  min_lr: 0.000000  loss: 3.9975 (3.9667)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (62541.4945)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.1031  max mem: 15572
Epoch: [15]  [2110/2809]  eta: 0:06:38  lr: 0.000037  min_lr: 0.000000  loss: 3.9617 (3.9662)  class_acc: 0.2917 (0.2680)  loss_scale: 65536.0000 (62555.6798)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0943  max mem: 15572
[2025-01-15 21:56:43,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:56:43,639] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [2120/2809]  eta: 0:06:33  lr: 0.000037  min_lr: 0.000000  loss: 3.9399 (3.9663)  class_acc: 0.3333 (0.2682)  loss_scale: 65536.0000 (62600.6299)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1361  max mem: 15572
[2025-01-15 21:56:48,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44261
[2025-01-15 21:56:48,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:56:48,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2130/2809]  eta: 0:06:27  lr: 0.000037  min_lr: 0.000000  loss: 3.9399 (3.9652)  class_acc: 0.2917 (0.2685)  loss_scale: 65536.0000 (62768.1727)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1864  max mem: 15572
Epoch: [15]  [2140/2809]  eta: 0:06:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9961 (3.9652)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (62781.1004)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1675  max mem: 15572
Epoch: [15]  [2150/2809]  eta: 0:06:16  lr: 0.000037  min_lr: 0.000000  loss: 4.0088 (3.9649)  class_acc: 0.2500 (0.2684)  loss_scale: 65536.0000 (62793.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1052  max mem: 15572
Epoch: [15]  [2160/2809]  eta: 0:06:10  lr: 0.000037  min_lr: 0.000000  loss: 3.9897 (3.9649)  class_acc: 0.2500 (0.2684)  loss_scale: 65536.0000 (62806.5969)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1359  max mem: 15572
Epoch: [15]  [2170/2809]  eta: 0:06:05  lr: 0.000037  min_lr: 0.000000  loss: 4.1409 (3.9657)  class_acc: 0.2083 (0.2680)  loss_scale: 65536.0000 (62819.1690)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1660  max mem: 15572
Epoch: [15]  [2180/2809]  eta: 0:05:59  lr: 0.000037  min_lr: 0.000000  loss: 4.1170 (3.9654)  class_acc: 0.2083 (0.2680)  loss_scale: 65536.0000 (62831.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1472  max mem: 15572
Epoch: [15]  [2190/2809]  eta: 0:05:53  lr: 0.000037  min_lr: 0.000000  loss: 3.7956 (3.9648)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (62843.9690)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1491  max mem: 15572
Epoch: [15]  [2200/2809]  eta: 0:05:47  lr: 0.000037  min_lr: 0.000000  loss: 3.7956 (3.9648)  class_acc: 0.2917 (0.2683)  loss_scale: 65536.0000 (62856.1999)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0923  max mem: 15572
Epoch: [15]  [2210/2809]  eta: 0:05:42  lr: 0.000037  min_lr: 0.000000  loss: 3.9283 (3.9643)  class_acc: 0.2917 (0.2684)  loss_scale: 65536.0000 (62868.3202)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1197  max mem: 15572
Epoch: [15]  [2220/2809]  eta: 0:05:36  lr: 0.000037  min_lr: 0.000000  loss: 4.0997 (3.9655)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (62880.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1259  max mem: 15572
Epoch: [15]  [2230/2809]  eta: 0:05:31  lr: 0.000037  min_lr: 0.000000  loss: 4.1932 (3.9662)  class_acc: 0.2083 (0.2682)  loss_scale: 65536.0000 (62892.2349)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1512  max mem: 15572
Epoch: [15]  [2240/2809]  eta: 0:05:25  lr: 0.000037  min_lr: 0.000000  loss: 4.1105 (3.9659)  class_acc: 0.2500 (0.2682)  loss_scale: 65536.0000 (62904.0321)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1630  max mem: 15572
Epoch: [15]  [2250/2809]  eta: 0:05:19  lr: 0.000037  min_lr: 0.000000  loss: 4.0973 (3.9664)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (62915.7246)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1089  max mem: 15572
[2025-01-15 21:58:03,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:58:03,064] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:58:03,560] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44391
[2025-01-15 21:58:03,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:58:03,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2260/2809]  eta: 0:05:13  lr: 0.000037  min_lr: 0.000000  loss: 4.0519 (3.9668)  class_acc: 0.2083 (0.2678)  loss_scale: 65536.0000 (62956.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1125  max mem: 15572
Epoch: [15]  [2270/2809]  eta: 0:05:08  lr: 0.000037  min_lr: 0.000000  loss: 3.9780 (3.9665)  class_acc: 0.1667 (0.2678)  loss_scale: 65536.0000 (62967.6583)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1183  max mem: 15572
Epoch: [15]  [2280/2809]  eta: 0:05:02  lr: 0.000037  min_lr: 0.000000  loss: 3.8565 (3.9658)  class_acc: 0.2083 (0.2678)  loss_scale: 65536.0000 (62978.9180)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1225  max mem: 15572
Epoch: [15]  [2290/2809]  eta: 0:04:56  lr: 0.000037  min_lr: 0.000000  loss: 3.8565 (3.9659)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (62990.0794)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1376  max mem: 15572
Epoch: [15]  [2300/2809]  eta: 0:04:51  lr: 0.000037  min_lr: 0.000000  loss: 3.9855 (3.9658)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (63001.1439)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1636  max mem: 15572
Epoch: [15]  [2310/2809]  eta: 0:04:45  lr: 0.000037  min_lr: 0.000000  loss: 3.9672 (3.9654)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63012.1125)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.1782  max mem: 15572
Epoch: [15]  [2320/2809]  eta: 0:04:39  lr: 0.000037  min_lr: 0.000000  loss: 3.7938 (3.9647)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63022.9866)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.1589  max mem: 15572
Epoch: [15]  [2330/2809]  eta: 0:04:33  lr: 0.000037  min_lr: 0.000000  loss: 3.8351 (3.9647)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (63033.7675)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0534  max mem: 15572
Epoch: [15]  [2340/2809]  eta: 0:04:28  lr: 0.000037  min_lr: 0.000000  loss: 4.0906 (3.9648)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (63044.4562)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0592  max mem: 15572
Epoch: [15]  [2350/2809]  eta: 0:04:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9447 (3.9645)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (63055.0540)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1072  max mem: 15572
Epoch: [15]  [2360/2809]  eta: 0:04:16  lr: 0.000037  min_lr: 0.000000  loss: 3.9447 (3.9649)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (63065.5620)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0815  max mem: 15572
Epoch: [15]  [2370/2809]  eta: 0:04:10  lr: 0.000037  min_lr: 0.000000  loss: 4.0147 (3.9647)  class_acc: 0.2500 (0.2676)  loss_scale: 65536.0000 (63075.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0846  max mem: 15572
Epoch: [15]  [2380/2809]  eta: 0:04:04  lr: 0.000037  min_lr: 0.000000  loss: 3.8950 (3.9642)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (63086.3133)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0810  max mem: 15572
[2025-01-15 21:59:15,084] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 21:59:15,084] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 21:59:16,970] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44523
[2025-01-15 21:59:16,970] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 21:59:16,970] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2390/2809]  eta: 0:03:59  lr: 0.000037  min_lr: 0.000000  loss: 3.8052 (3.9635)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (63178.7871)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0835  max mem: 15572
Epoch: [15]  [2400/2809]  eta: 0:03:53  lr: 0.000037  min_lr: 0.000000  loss: 3.7908 (3.9628)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (63188.6047)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1654  max mem: 15572
Epoch: [15]  [2410/2809]  eta: 0:03:47  lr: 0.000037  min_lr: 0.000000  loss: 4.1784 (3.9639)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (63198.3409)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1386  max mem: 15572
Epoch: [15]  [2420/2809]  eta: 0:03:41  lr: 0.000037  min_lr: 0.000000  loss: 4.1951 (3.9636)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (63207.9967)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0538  max mem: 15572
Epoch: [15]  [2430/2809]  eta: 0:03:36  lr: 0.000037  min_lr: 0.000000  loss: 3.9055 (3.9638)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63217.5730)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0950  max mem: 15572
Epoch: [15]  [2440/2809]  eta: 0:03:30  lr: 0.000037  min_lr: 0.000000  loss: 3.9362 (3.9634)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63227.0709)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1324  max mem: 15572
Epoch: [15]  [2450/2809]  eta: 0:03:24  lr: 0.000037  min_lr: 0.000000  loss: 3.9362 (3.9638)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (63236.4912)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1101  max mem: 15572
Epoch: [15]  [2460/2809]  eta: 0:03:19  lr: 0.000037  min_lr: 0.000000  loss: 3.9731 (3.9642)  class_acc: 0.2083 (0.2676)  loss_scale: 65536.0000 (63245.8350)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1347  max mem: 15572
Epoch: [15]  [2470/2809]  eta: 0:03:13  lr: 0.000037  min_lr: 0.000000  loss: 3.9677 (3.9641)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (63255.1032)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1454  max mem: 15572
Epoch: [15]  [2480/2809]  eta: 0:03:07  lr: 0.000037  min_lr: 0.000000  loss: 3.9611 (3.9642)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (63264.2967)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1381  max mem: 15572
Epoch: [15]  [2490/2809]  eta: 0:03:01  lr: 0.000037  min_lr: 0.000000  loss: 3.9611 (3.9640)  class_acc: 0.2917 (0.2678)  loss_scale: 65536.0000 (63273.4163)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1003  max mem: 15572
Epoch: [15]  [2500/2809]  eta: 0:02:56  lr: 0.000037  min_lr: 0.000000  loss: 3.9666 (3.9640)  class_acc: 0.2500 (0.2678)  loss_scale: 65536.0000 (63282.4630)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.0992  max mem: 15572
Epoch: [15]  [2510/2809]  eta: 0:02:50  lr: 0.000037  min_lr: 0.000000  loss: 4.0718 (3.9644)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (63291.4377)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1525  max mem: 15572
[2025-01-15 22:00:30,872] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:00:30,872] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:00:31,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44654
[2025-01-15 22:00:31,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:00:31,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2520/2809]  eta: 0:02:44  lr: 0.000037  min_lr: 0.000000  loss: 4.1513 (3.9649)  class_acc: 0.2083 (0.2677)  loss_scale: 65536.0000 (63352.3332)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1499  max mem: 15572
Epoch: [15]  [2530/2809]  eta: 0:02:39  lr: 0.000037  min_lr: 0.000000  loss: 4.1513 (3.9647)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (63360.9609)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1461  max mem: 15572
Epoch: [15]  [2540/2809]  eta: 0:02:33  lr: 0.000037  min_lr: 0.000000  loss: 3.6062 (3.9635)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63369.5207)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1112  max mem: 15572
Epoch: [15]  [2550/2809]  eta: 0:02:27  lr: 0.000037  min_lr: 0.000000  loss: 3.7591 (3.9638)  class_acc: 0.2917 (0.2679)  loss_scale: 65536.0000 (63378.0133)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1062  max mem: 15572
Epoch: [15]  [2560/2809]  eta: 0:02:22  lr: 0.000037  min_lr: 0.000000  loss: 3.9939 (3.9635)  class_acc: 0.2500 (0.2679)  loss_scale: 65536.0000 (63386.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1248  max mem: 15572
Epoch: [15]  [2570/2809]  eta: 0:02:16  lr: 0.000037  min_lr: 0.000000  loss: 3.8961 (3.9636)  class_acc: 0.2500 (0.2680)  loss_scale: 65536.0000 (63394.8005)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0948  max mem: 15572
Epoch: [15]  [2580/2809]  eta: 0:02:10  lr: 0.000037  min_lr: 0.000000  loss: 4.0180 (3.9638)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (63403.0965)  weight_decay: 0.0500 (0.0500)  time: 0.4797  data: 0.0393  max mem: 15572
Epoch: [15]  [2590/2809]  eta: 0:02:04  lr: 0.000036  min_lr: 0.000000  loss: 4.0180 (3.9641)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (63411.3284)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0834  max mem: 15572
Epoch: [15]  [2600/2809]  eta: 0:01:59  lr: 0.000036  min_lr: 0.000000  loss: 4.0603 (3.9644)  class_acc: 0.2083 (0.2680)  loss_scale: 65536.0000 (63419.4971)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1348  max mem: 15572
Epoch: [15]  [2610/2809]  eta: 0:01:53  lr: 0.000036  min_lr: 0.000000  loss: 4.0603 (3.9643)  class_acc: 0.2500 (0.2681)  loss_scale: 65536.0000 (63427.6032)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.0986  max mem: 15572
Epoch: [15]  [2620/2809]  eta: 0:01:47  lr: 0.000036  min_lr: 0.000000  loss: 3.8780 (3.9640)  class_acc: 0.2500 (0.2682)  loss_scale: 65536.0000 (63435.6475)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1534  max mem: 15572
Epoch: [15]  [2630/2809]  eta: 0:01:42  lr: 0.000036  min_lr: 0.000000  loss: 3.8780 (3.9642)  class_acc: 0.2500 (0.2682)  loss_scale: 65536.0000 (63443.6306)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1597  max mem: 15572
Epoch: [15]  [2640/2809]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000000  loss: 3.9521 (3.9641)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (63451.5532)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1064  max mem: 15572
[2025-01-15 22:01:44,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:01:44,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [2650/2809]  eta: 0:01:30  lr: 0.000036  min_lr: 0.000000  loss: 4.0101 (3.9644)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (63533.5798)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.0963  max mem: 15572
[2025-01-15 22:01:48,980] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44789
[2025-01-15 22:01:48,980] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:01:48,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [2660/2809]  eta: 0:01:25  lr: 0.000036  min_lr: 0.000000  loss: 3.9380 (3.9646)  class_acc: 0.2083 (0.2681)  loss_scale: 65536.0000 (63614.9899)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1255  max mem: 15572
Epoch: [15]  [2670/2809]  eta: 0:01:19  lr: 0.000036  min_lr: 0.000000  loss: 3.9793 (3.9647)  class_acc: 0.2083 (0.2681)  loss_scale: 65536.0000 (63622.1820)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1615  max mem: 15572
Epoch: [15]  [2680/2809]  eta: 0:01:13  lr: 0.000036  min_lr: 0.000000  loss: 3.9596 (3.9642)  class_acc: 0.2917 (0.2682)  loss_scale: 65536.0000 (63629.3204)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1841  max mem: 15572
Epoch: [15]  [2690/2809]  eta: 0:01:07  lr: 0.000036  min_lr: 0.000000  loss: 3.8487 (3.9633)  class_acc: 0.2917 (0.2684)  loss_scale: 65536.0000 (63636.4058)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1448  max mem: 15572
Epoch: [15]  [2700/2809]  eta: 0:01:02  lr: 0.000036  min_lr: 0.000000  loss: 3.9228 (3.9639)  class_acc: 0.2917 (0.2683)  loss_scale: 65536.0000 (63643.4387)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1366  max mem: 15572
Epoch: [15]  [2710/2809]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000000  loss: 3.9228 (3.9630)  class_acc: 0.2917 (0.2685)  loss_scale: 65536.0000 (63650.4198)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0974  max mem: 15572
[2025-01-15 22:02:25,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44852
[2025-01-15 22:02:25,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:02:25,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [2720/2809]  eta: 0:00:50  lr: 0.000036  min_lr: 0.000000  loss: 3.7707 (3.9629)  class_acc: 0.2917 (0.2685)  loss_scale: 65536.0000 (63609.1790)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0431  max mem: 15572
Epoch: [15]  [2730/2809]  eta: 0:00:45  lr: 0.000036  min_lr: 0.000000  loss: 3.5703 (3.9618)  class_acc: 0.3333 (0.2687)  loss_scale: 32768.0000 (63496.2490)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0591  max mem: 15572
Epoch: [15]  [2740/2809]  eta: 0:00:39  lr: 0.000036  min_lr: 0.000000  loss: 3.6317 (3.9614)  class_acc: 0.3333 (0.2687)  loss_scale: 32768.0000 (63384.1430)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.0669  max mem: 15572
Epoch: [15]  [2750/2809]  eta: 0:00:33  lr: 0.000036  min_lr: 0.000000  loss: 3.8691 (3.9615)  class_acc: 0.2500 (0.2688)  loss_scale: 32768.0000 (63272.8521)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0507  max mem: 15572
Epoch: [15]  [2760/2809]  eta: 0:00:27  lr: 0.000036  min_lr: 0.000000  loss: 3.8807 (3.9608)  class_acc: 0.2500 (0.2689)  loss_scale: 32768.0000 (63162.3673)  weight_decay: 0.0500 (0.0500)  time: 0.4786  data: 0.0248  max mem: 15572
Epoch: [15]  [2770/2809]  eta: 0:00:22  lr: 0.000036  min_lr: 0.000000  loss: 3.7922 (3.9605)  class_acc: 0.2917 (0.2691)  loss_scale: 32768.0000 (63052.6799)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0816  max mem: 15572
Epoch: [15]  [2780/2809]  eta: 0:00:16  lr: 0.000036  min_lr: 0.000000  loss: 3.8405 (3.9600)  class_acc: 0.3333 (0.2694)  loss_scale: 32768.0000 (62943.7814)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1017  max mem: 15572
Epoch: [15]  [2790/2809]  eta: 0:00:10  lr: 0.000036  min_lr: 0.000000  loss: 3.8405 (3.9597)  class_acc: 0.3333 (0.2695)  loss_scale: 32768.0000 (62835.6632)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1391  max mem: 15572
Epoch: [15]  [2800/2809]  eta: 0:00:05  lr: 0.000036  min_lr: 0.000000  loss: 3.7678 (3.9594)  class_acc: 0.2917 (0.2696)  loss_scale: 32768.0000 (62728.3170)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1587  max mem: 15572
Epoch: [15]  [2808/2809]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8203 (3.9591)  class_acc: 0.2917 (0.2696)  loss_scale: 32768.0000 (62642.9904)  weight_decay: 0.0500 (0.0500)  time: 0.4778  data: 0.0723  max mem: 15572
Epoch: [15] Total time: 0:26:41 (0.5701 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000000  loss: 3.8203 (3.9591)  class_acc: 0.2917 (0.2696)  loss_scale: 32768.0000 (62642.9904)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:27  loss: 0.3107 (0.3107)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.5119  data: 4.2860  max mem: 15572
Val:  [ 10/272]  eta: 0:03:08  loss: 3.0801 (2.8053)  acc1: 22.2222 (31.3131)  acc5: 61.1111 (58.5859)  time: 0.7202  data: 0.5155  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 2.9813 (2.7833)  acc1: 27.7778 (34.3915)  acc5: 61.1111 (63.2275)  time: 0.3308  data: 0.1174  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.8510 (2.8429)  acc1: 33.3333 (31.1828)  acc5: 66.6667 (64.6953)  time: 0.2829  data: 0.0776  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 2.8510 (2.8383)  acc1: 22.2222 (29.9458)  acc5: 72.2222 (66.5312)  time: 0.2910  data: 0.0978  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.7134 (2.7467)  acc1: 33.3333 (33.2244)  acc5: 72.2222 (68.4096)  time: 0.3278  data: 0.1357  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.6902 (2.5943)  acc1: 55.5556 (37.5228)  acc5: 77.7778 (70.0364)  time: 0.3101  data: 0.1113  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.7604 (2.5090)  acc1: 55.5556 (39.0454)  acc5: 83.3333 (71.9092)  time: 0.3007  data: 0.1056  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 2.3621 (2.5329)  acc1: 38.8889 (39.0947)  acc5: 77.7778 (71.1934)  time: 0.3070  data: 0.1220  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.9850 (2.5902)  acc1: 33.3333 (38.5836)  acc5: 66.6667 (70.2076)  time: 0.2880  data: 0.0867  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.8859 (2.6251)  acc1: 33.3333 (38.5589)  acc5: 66.6667 (69.5820)  time: 0.2796  data: 0.0859  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.8817 (2.6952)  acc1: 16.6667 (36.6867)  acc5: 61.1111 (68.0180)  time: 0.3395  data: 0.1483  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 3.1923 (2.7341)  acc1: 16.6667 (35.6290)  acc5: 55.5556 (67.3095)  time: 0.3748  data: 0.1720  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.6319 (2.6853)  acc1: 33.3333 (37.1925)  acc5: 72.2222 (67.9389)  time: 0.3439  data: 0.1286  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.9783 (2.6688)  acc1: 44.4444 (37.7463)  acc5: 77.7778 (68.0851)  time: 0.3061  data: 0.0862  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.8064 (2.6751)  acc1: 27.7778 (36.8653)  acc5: 72.2222 (68.3223)  time: 0.2999  data: 0.0957  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.6772 (2.6540)  acc1: 33.3333 (37.7502)  acc5: 77.7778 (69.0131)  time: 0.2895  data: 0.1009  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6686 (2.6824)  acc1: 38.8889 (37.1020)  acc5: 72.2222 (68.3561)  time: 0.3418  data: 0.1551  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.6514 (2.6677)  acc1: 27.7778 (36.8017)  acc5: 66.6667 (68.9994)  time: 0.2966  data: 0.1093  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.5547 (2.7045)  acc1: 27.7778 (36.0675)  acc5: 72.2222 (67.8592)  time: 0.2039  data: 0.0232  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.5525 (2.7097)  acc1: 27.7778 (35.9038)  acc5: 72.2222 (67.8275)  time: 0.1957  data: 0.0259  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.2383 (2.6973)  acc1: 50.0000 (36.7299)  acc5: 77.7778 (68.0621)  time: 0.1796  data: 0.0131  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.1821 (2.6849)  acc1: 50.0000 (37.0538)  acc5: 77.7778 (68.3509)  time: 0.2552  data: 0.0643  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9469 (2.6530)  acc1: 55.5556 (38.2395)  acc5: 83.3333 (68.8793)  time: 0.3306  data: 0.1265  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7914 (2.6340)  acc1: 61.1111 (38.7506)  acc5: 83.3333 (69.3638)  time: 0.3356  data: 0.1303  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.6918 (2.6488)  acc1: 27.7778 (38.1363)  acc5: 77.7778 (69.2120)  time: 0.3341  data: 0.1269  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.6118 (2.5779)  acc1: 77.7778 (40.1447)  acc5: 83.3333 (70.2001)  time: 0.3537  data: 0.1524  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5508 (2.5776)  acc1: 66.6667 (39.8934)  acc5: 88.8889 (70.1517)  time: 0.2666  data: 0.0836  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5508 (2.5826)  acc1: 55.5556 (39.8730)  acc5: 88.8889 (70.1208)  time: 0.2445  data: 0.0687  max mem: 15572
Val: Total time: 0:01:25 (0.3134 s / it)
* Acc@1 39.873 Acc@5 70.121 loss 2.583
Accuracy of the network on the 4883 val videos: 39.9%
[2025-01-15 22:04:39,294] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 22:04:39,298] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 22:04:39,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 22:04:42,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 22:04:42,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 39.87%
Epoch: [16]  [   0/2809]  eta: 7:13:10  lr: 0.000036  min_lr: 0.000000  loss: 2.8806 (2.8806)  class_acc: 0.5833 (0.5833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.2525  data: 8.7460  max mem: 15572
Epoch: [16]  [  10/2809]  eta: 1:05:22  lr: 0.000036  min_lr: 0.000000  loss: 4.1112 (3.8930)  class_acc: 0.2500 (0.3144)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4013  data: 0.9444  max mem: 15572
Epoch: [16]  [  20/2809]  eta: 0:47:28  lr: 0.000036  min_lr: 0.000000  loss: 4.1073 (3.9011)  class_acc: 0.2500 (0.3155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1617  max mem: 15572
Epoch: [16]  [  30/2809]  eta: 0:41:41  lr: 0.000036  min_lr: 0.000000  loss: 3.9486 (3.9486)  class_acc: 0.2500 (0.2823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1653  max mem: 15572
[2025-01-15 22:05:15,323] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:05:15,324] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [  40/2809]  eta: 0:39:40  lr: 0.000036  min_lr: 0.000000  loss: 3.9290 (3.9381)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.2263  max mem: 15572
Epoch: [16]  [  50/2809]  eta: 0:39:26  lr: 0.000036  min_lr: 0.000000  loss: 3.8546 (3.9155)  class_acc: 0.2917 (0.2900)  loss_scale: 65536.0000 (41763.1373)  weight_decay: 0.0500 (0.0500)  time: 0.7922  data: 0.3383  max mem: 15572
[2025-01-15 22:05:28,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=301, lr=[3.519523463149651e-07, 3.519523463149651e-07, 5.027890661642359e-07, 5.027890661642359e-07, 7.18270094520337e-07, 7.18270094520337e-07, 1.026100135029053e-06, 1.026100135029053e-06, 1.46585733575579e-06, 1.46585733575579e-06, 2.094081908222557e-06, 2.094081908222557e-06, 2.991545583175082e-06, 2.991545583175082e-06, 4.273636547392975e-06, 4.273636547392975e-06, 6.105195067704249e-06, 6.105195067704249e-06, 8.7217072395775e-06, 8.7217072395775e-06, 1.2459581770825e-05, 1.2459581770825e-05, 1.779940252975e-05, 1.779940252975e-05, 2.5427717899642862e-05, 2.5427717899642862e-05, 3.632531128520409e-05, 3.632531128520409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:05:29,000] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=28.451846355689664, CurrSamplesPerSec=24.971210868099465, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [  60/2809]  eta: 0:36:54  lr: 0.000036  min_lr: 0.000000  loss: 4.0114 (3.9336)  class_acc: 0.2917 (0.2883)  loss_scale: 65536.0000 (45660.3279)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.2335  max mem: 15572
Epoch: [16]  [  70/2809]  eta: 0:35:49  lr: 0.000036  min_lr: 0.000000  loss: 4.0791 (3.9543)  class_acc: 0.2500 (0.2805)  loss_scale: 65536.0000 (48459.7183)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1379  max mem: 15572
Epoch: [16]  [  80/2809]  eta: 0:33:55  lr: 0.000036  min_lr: 0.000000  loss: 4.0209 (3.9416)  class_acc: 0.2500 (0.2793)  loss_scale: 65536.0000 (50567.9012)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1400  max mem: 15572
Epoch: [16]  [  90/2809]  eta: 0:32:05  lr: 0.000036  min_lr: 0.000000  loss: 3.9208 (3.9448)  class_acc: 0.2917 (0.2816)  loss_scale: 65536.0000 (52212.7473)  weight_decay: 0.0500 (0.0500)  time: 0.4370  data: 0.0382  max mem: 15572
Epoch: [16]  [ 100/2809]  eta: 0:30:59  lr: 0.000036  min_lr: 0.000000  loss: 3.9317 (3.9429)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (53531.8812)  weight_decay: 0.0500 (0.0500)  time: 0.4457  data: 0.0006  max mem: 15572
Epoch: [16]  [ 110/2809]  eta: 0:29:49  lr: 0.000036  min_lr: 0.000000  loss: 3.9678 (3.9404)  class_acc: 0.2500 (0.2819)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.4569  data: 0.0007  max mem: 15572
Epoch: [16]  [ 120/2809]  eta: 0:28:54  lr: 0.000036  min_lr: 0.000000  loss: 3.9865 (3.9377)  class_acc: 0.2917 (0.2827)  loss_scale: 65536.0000 (55516.0331)  weight_decay: 0.0500 (0.0500)  time: 0.4351  data: 0.0008  max mem: 15572
Epoch: [16]  [ 130/2809]  eta: 0:28:40  lr: 0.000036  min_lr: 0.000000  loss: 4.0165 (3.9448)  class_acc: 0.2500 (0.2812)  loss_scale: 65536.0000 (56280.9160)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0758  max mem: 15572
Epoch: [16]  [ 140/2809]  eta: 0:28:32  lr: 0.000036  min_lr: 0.000000  loss: 4.0936 (3.9534)  class_acc: 0.2083 (0.2763)  loss_scale: 65536.0000 (56937.3050)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.1686  max mem: 15572
Epoch: [16]  [ 150/2809]  eta: 0:28:21  lr: 0.000036  min_lr: 0.000000  loss: 4.0226 (3.9541)  class_acc: 0.2500 (0.2809)  loss_scale: 65536.0000 (57506.7550)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1817  max mem: 15572
Epoch: [16]  [ 160/2809]  eta: 0:27:47  lr: 0.000036  min_lr: 0.000000  loss: 4.0010 (3.9487)  class_acc: 0.2917 (0.2816)  loss_scale: 65536.0000 (58005.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.1075  max mem: 15572
[2025-01-15 22:06:27,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:06:27,229] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:06:28,085] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45111
[2025-01-15 22:06:28,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:06:28,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 170/2809]  eta: 0:27:25  lr: 0.000036  min_lr: 0.000000  loss: 4.0996 (3.9599)  class_acc: 0.2500 (0.2797)  loss_scale: 65536.0000 (59212.3509)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0658  max mem: 15572
Epoch: [16]  [ 180/2809]  eta: 0:27:15  lr: 0.000036  min_lr: 0.000000  loss: 4.2306 (3.9672)  class_acc: 0.2083 (0.2756)  loss_scale: 65536.0000 (59561.7238)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1379  max mem: 15572
Epoch: [16]  [ 190/2809]  eta: 0:27:01  lr: 0.000036  min_lr: 0.000000  loss: 4.1215 (3.9687)  class_acc: 0.2083 (0.2744)  loss_scale: 65536.0000 (59874.5131)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1534  max mem: 15572
Epoch: [16]  [ 200/2809]  eta: 0:26:39  lr: 0.000036  min_lr: 0.000000  loss: 4.0664 (3.9716)  class_acc: 0.2083 (0.2711)  loss_scale: 65536.0000 (60156.1791)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0970  max mem: 15572
Epoch: [16]  [ 210/2809]  eta: 0:26:35  lr: 0.000036  min_lr: 0.000000  loss: 4.0208 (3.9777)  class_acc: 0.2083 (0.2686)  loss_scale: 65536.0000 (60411.1469)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1340  max mem: 15572
Epoch: [16]  [ 220/2809]  eta: 0:26:21  lr: 0.000036  min_lr: 0.000000  loss: 4.0208 (3.9702)  class_acc: 0.2083 (0.2702)  loss_scale: 65536.0000 (60643.0407)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1582  max mem: 15572
Epoch: [16]  [ 230/2809]  eta: 0:26:14  lr: 0.000036  min_lr: 0.000000  loss: 4.0832 (3.9775)  class_acc: 0.2083 (0.2675)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1291  max mem: 15572
Epoch: [16]  [ 240/2809]  eta: 0:25:50  lr: 0.000036  min_lr: 0.000000  loss: 4.1579 (3.9852)  class_acc: 0.2083 (0.2664)  loss_scale: 65536.0000 (61049.0954)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0704  max mem: 15572
Epoch: [16]  [ 250/2809]  eta: 0:25:38  lr: 0.000036  min_lr: 0.000000  loss: 4.1073 (3.9801)  class_acc: 0.2500 (0.2666)  loss_scale: 65536.0000 (61227.8566)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0383  max mem: 15572
Epoch: [16]  [ 260/2809]  eta: 0:25:34  lr: 0.000036  min_lr: 0.000000  loss: 3.8741 (3.9785)  class_acc: 0.2500 (0.2663)  loss_scale: 65536.0000 (61392.9195)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1294  max mem: 15572
Epoch: [16]  [ 270/2809]  eta: 0:25:30  lr: 0.000036  min_lr: 0.000000  loss: 3.9039 (3.9804)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (61545.8007)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1913  max mem: 15572
Epoch: [16]  [ 280/2809]  eta: 0:25:23  lr: 0.000036  min_lr: 0.000000  loss: 4.0801 (3.9807)  class_acc: 0.2500 (0.2662)  loss_scale: 65536.0000 (61687.8007)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1726  max mem: 15572
Epoch: [16]  [ 290/2809]  eta: 0:25:10  lr: 0.000036  min_lr: 0.000000  loss: 4.0801 (3.9781)  class_acc: 0.2500 (0.2652)  loss_scale: 65536.0000 (61820.0412)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1250  max mem: 15572
[2025-01-15 22:07:40,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:07:40,057] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:07:42,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45243
[2025-01-15 22:07:42,416] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:07:42,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 300/2809]  eta: 0:24:59  lr: 0.000036  min_lr: 0.000000  loss: 3.6696 (3.9683)  class_acc: 0.2917 (0.2670)  loss_scale: 65536.0000 (62596.6777)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.1070  max mem: 15572
Epoch: [16]  [ 310/2809]  eta: 0:24:41  lr: 0.000036  min_lr: 0.000000  loss: 3.6696 (3.9658)  class_acc: 0.3333 (0.2663)  loss_scale: 65536.0000 (62691.1897)  weight_decay: 0.0500 (0.0500)  time: 0.4972  data: 0.0548  max mem: 15572
Epoch: [16]  [ 320/2809]  eta: 0:24:30  lr: 0.000036  min_lr: 0.000000  loss: 4.0780 (3.9703)  class_acc: 0.2500 (0.2665)  loss_scale: 65536.0000 (62779.8131)  weight_decay: 0.0500 (0.0500)  time: 0.4854  data: 0.0489  max mem: 15572
Epoch: [16]  [ 330/2809]  eta: 0:24:24  lr: 0.000036  min_lr: 0.000000  loss: 4.0673 (3.9664)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (62863.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1372  max mem: 15572
Epoch: [16]  [ 340/2809]  eta: 0:24:25  lr: 0.000036  min_lr: 0.000000  loss: 4.0121 (3.9681)  class_acc: 0.2500 (0.2686)  loss_scale: 65536.0000 (62941.4663)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.2123  max mem: 15572
Epoch: [16]  [ 350/2809]  eta: 0:24:10  lr: 0.000036  min_lr: 0.000000  loss: 4.0557 (3.9725)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (63015.3846)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1351  max mem: 15572
Epoch: [16]  [ 360/2809]  eta: 0:24:07  lr: 0.000036  min_lr: 0.000000  loss: 4.2282 (3.9811)  class_acc: 0.2083 (0.2659)  loss_scale: 65536.0000 (63085.2078)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1045  max mem: 15572
Epoch: [16]  [ 370/2809]  eta: 0:23:59  lr: 0.000036  min_lr: 0.000000  loss: 4.2710 (3.9848)  class_acc: 0.2083 (0.2659)  loss_scale: 65536.0000 (63151.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1637  max mem: 15572
Epoch: [16]  [ 380/2809]  eta: 0:23:50  lr: 0.000036  min_lr: 0.000000  loss: 3.8768 (3.9800)  class_acc: 0.2500 (0.2662)  loss_scale: 65536.0000 (63213.8583)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1391  max mem: 15572
Epoch: [16]  [ 390/2809]  eta: 0:23:41  lr: 0.000036  min_lr: 0.000000  loss: 3.9079 (3.9801)  class_acc: 0.2917 (0.2664)  loss_scale: 65536.0000 (63273.2481)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1289  max mem: 15572
Epoch: [16]  [ 400/2809]  eta: 0:23:33  lr: 0.000036  min_lr: 0.000000  loss: 4.0549 (3.9817)  class_acc: 0.2083 (0.2670)  loss_scale: 65536.0000 (63329.6758)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1120  max mem: 15572
Epoch: [16]  [ 410/2809]  eta: 0:23:29  lr: 0.000036  min_lr: 0.000000  loss: 3.8822 (3.9719)  class_acc: 0.2500 (0.2682)  loss_scale: 65536.0000 (63383.3577)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1175  max mem: 15572
Epoch: [16]  [ 420/2809]  eta: 0:23:18  lr: 0.000036  min_lr: 0.000000  loss: 3.5828 (3.9703)  class_acc: 0.2500 (0.2690)  loss_scale: 65536.0000 (63434.4893)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0980  max mem: 15572
[2025-01-15 22:08:52,776] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:08:52,776] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:08:54,060] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45373
[2025-01-15 22:08:54,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:08:54,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 430/2809]  eta: 0:23:08  lr: 0.000036  min_lr: 0.000000  loss: 3.7753 (3.9663)  class_acc: 0.2500 (0.2704)  loss_scale: 65536.0000 (63635.3039)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0803  max mem: 15572
Epoch: [16]  [ 440/2809]  eta: 0:23:09  lr: 0.000036  min_lr: 0.000000  loss: 3.6587 (3.9631)  class_acc: 0.2500 (0.2710)  loss_scale: 65536.0000 (63678.4036)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1703  max mem: 15572
Epoch: [16]  [ 450/2809]  eta: 0:23:07  lr: 0.000036  min_lr: 0.000000  loss: 4.0925 (3.9676)  class_acc: 0.2500 (0.2708)  loss_scale: 65536.0000 (63719.5920)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.2365  max mem: 15572
Epoch: [16]  [ 460/2809]  eta: 0:22:54  lr: 0.000036  min_lr: 0.000000  loss: 4.1535 (3.9702)  class_acc: 0.2083 (0.2707)  loss_scale: 65536.0000 (63758.9935)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1276  max mem: 15572
Epoch: [16]  [ 470/2809]  eta: 0:22:46  lr: 0.000036  min_lr: 0.000000  loss: 3.8500 (3.9661)  class_acc: 0.3333 (0.2717)  loss_scale: 65536.0000 (63796.7219)  weight_decay: 0.0500 (0.0500)  time: 0.4991  data: 0.0610  max mem: 15572
Epoch: [16]  [ 480/2809]  eta: 0:22:39  lr: 0.000036  min_lr: 0.000000  loss: 3.8287 (3.9665)  class_acc: 0.3333 (0.2715)  loss_scale: 65536.0000 (63832.8815)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1103  max mem: 15572
Epoch: [16]  [ 490/2809]  eta: 0:22:34  lr: 0.000036  min_lr: 0.000000  loss: 3.9233 (3.9636)  class_acc: 0.2917 (0.2714)  loss_scale: 65536.0000 (63867.5682)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1478  max mem: 15572
Epoch: [16]  [ 500/2809]  eta: 0:22:26  lr: 0.000036  min_lr: 0.000000  loss: 3.9233 (3.9634)  class_acc: 0.2917 (0.2720)  loss_scale: 65536.0000 (63900.8703)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1447  max mem: 15572
Epoch: [16]  [ 510/2809]  eta: 0:22:22  lr: 0.000036  min_lr: 0.000000  loss: 3.8290 (3.9604)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (63932.8689)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1632  max mem: 15572
Epoch: [16]  [ 520/2809]  eta: 0:22:16  lr: 0.000036  min_lr: 0.000000  loss: 3.9141 (3.9594)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (63963.6392)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1834  max mem: 15572
Epoch: [16]  [ 530/2809]  eta: 0:22:09  lr: 0.000036  min_lr: 0.000000  loss: 3.9141 (3.9549)  class_acc: 0.2500 (0.2733)  loss_scale: 65536.0000 (63993.2505)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1406  max mem: 15572
Epoch: [16]  [ 540/2809]  eta: 0:22:04  lr: 0.000036  min_lr: 0.000000  loss: 4.0166 (3.9583)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (64021.7671)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1444  max mem: 15572
Epoch: [16]  [ 550/2809]  eta: 0:21:54  lr: 0.000036  min_lr: 0.000000  loss: 4.0464 (3.9560)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (64049.2486)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.1100  max mem: 15572
[2025-01-15 22:10:07,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:10:07,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 560/2809]  eta: 0:21:48  lr: 0.000036  min_lr: 0.000000  loss: 3.8045 (3.9520)  class_acc: 0.2500 (0.2743)  loss_scale: 65536.0000 (64426.2103)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.1006  max mem: 15572
[2025-01-15 22:10:09,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45505
[2025-01-15 22:10:09,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:10:09,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 570/2809]  eta: 0:21:40  lr: 0.000036  min_lr: 0.000000  loss: 3.6777 (3.9525)  class_acc: 0.2500 (0.2736)  loss_scale: 65536.0000 (64445.6462)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1162  max mem: 15572
Epoch: [16]  [ 580/2809]  eta: 0:21:36  lr: 0.000036  min_lr: 0.000000  loss: 3.9537 (3.9527)  class_acc: 0.2083 (0.2735)  loss_scale: 65536.0000 (64464.4131)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1312  max mem: 15572
Epoch: [16]  [ 590/2809]  eta: 0:21:31  lr: 0.000036  min_lr: 0.000000  loss: 3.4975 (3.9445)  class_acc: 0.2917 (0.2747)  loss_scale: 65536.0000 (64482.5448)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1864  max mem: 15572
Epoch: [16]  [ 600/2809]  eta: 0:21:23  lr: 0.000036  min_lr: 0.000000  loss: 3.6744 (3.9454)  class_acc: 0.2917 (0.2745)  loss_scale: 65536.0000 (64500.0732)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1504  max mem: 15572
Epoch: [16]  [ 610/2809]  eta: 0:21:16  lr: 0.000036  min_lr: 0.000000  loss: 3.9251 (3.9461)  class_acc: 0.2500 (0.2740)  loss_scale: 65536.0000 (64517.0278)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.1038  max mem: 15572
Epoch: [16]  [ 620/2809]  eta: 0:21:08  lr: 0.000036  min_lr: 0.000000  loss: 4.0209 (3.9472)  class_acc: 0.2500 (0.2740)  loss_scale: 65536.0000 (64533.4364)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0798  max mem: 15572
Epoch: [16]  [ 630/2809]  eta: 0:21:00  lr: 0.000036  min_lr: 0.000000  loss: 3.8434 (3.9451)  class_acc: 0.2500 (0.2743)  loss_scale: 65536.0000 (64549.3249)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0943  max mem: 15572
Epoch: [16]  [ 640/2809]  eta: 0:20:54  lr: 0.000036  min_lr: 0.000000  loss: 3.6665 (3.9433)  class_acc: 0.2917 (0.2746)  loss_scale: 65536.0000 (64564.7176)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1330  max mem: 15572
Epoch: [16]  [ 650/2809]  eta: 0:20:48  lr: 0.000036  min_lr: 0.000000  loss: 3.9435 (3.9435)  class_acc: 0.2917 (0.2748)  loss_scale: 65536.0000 (64579.6375)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1468  max mem: 15572
Epoch: [16]  [ 660/2809]  eta: 0:20:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9892 (3.9452)  class_acc: 0.2083 (0.2742)  loss_scale: 65536.0000 (64594.1059)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1772  max mem: 15572
Epoch: [16]  [ 670/2809]  eta: 0:20:36  lr: 0.000036  min_lr: 0.000000  loss: 4.0686 (3.9451)  class_acc: 0.2500 (0.2747)  loss_scale: 65536.0000 (64608.1431)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1161  max mem: 15572
Epoch: [16]  [ 680/2809]  eta: 0:20:30  lr: 0.000036  min_lr: 0.000000  loss: 3.9135 (3.9430)  class_acc: 0.2917 (0.2751)  loss_scale: 65536.0000 (64621.7680)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0796  max mem: 15572
[2025-01-15 22:11:21,497] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:11:21,497] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 690/2809]  eta: 0:20:21  lr: 0.000036  min_lr: 0.000000  loss: 3.9135 (3.9443)  class_acc: 0.2500 (0.2746)  loss_scale: 65536.0000 (64729.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0907  max mem: 15572
[2025-01-15 22:11:24,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45641
[2025-01-15 22:11:24,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:11:24,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 700/2809]  eta: 0:20:16  lr: 0.000036  min_lr: 0.000000  loss: 3.8109 (3.9398)  class_acc: 0.2500 (0.2754)  loss_scale: 65536.0000 (65302.2767)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.1099  max mem: 15572
Epoch: [16]  [ 710/2809]  eta: 0:20:08  lr: 0.000036  min_lr: 0.000000  loss: 3.7526 (3.9402)  class_acc: 0.3333 (0.2752)  loss_scale: 65536.0000 (65305.5640)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1164  max mem: 15572
Epoch: [16]  [ 720/2809]  eta: 0:20:01  lr: 0.000036  min_lr: 0.000000  loss: 4.0772 (3.9430)  class_acc: 0.2083 (0.2744)  loss_scale: 65536.0000 (65308.7601)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0665  max mem: 15572
Epoch: [16]  [ 730/2809]  eta: 0:19:55  lr: 0.000036  min_lr: 0.000000  loss: 4.2647 (3.9474)  class_acc: 0.2083 (0.2733)  loss_scale: 65536.0000 (65311.8687)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0937  max mem: 15572
Epoch: [16]  [ 740/2809]  eta: 0:19:49  lr: 0.000036  min_lr: 0.000000  loss: 4.2346 (3.9484)  class_acc: 0.2083 (0.2729)  loss_scale: 65536.0000 (65314.8934)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1229  max mem: 15572
Epoch: [16]  [ 750/2809]  eta: 0:19:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9688 (3.9485)  class_acc: 0.2917 (0.2729)  loss_scale: 65536.0000 (65317.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1446  max mem: 15572
Epoch: [16]  [ 760/2809]  eta: 0:19:39  lr: 0.000036  min_lr: 0.000000  loss: 4.0122 (3.9505)  class_acc: 0.2500 (0.2722)  loss_scale: 65536.0000 (65320.7043)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1721  max mem: 15572
Epoch: [16]  [ 770/2809]  eta: 0:19:32  lr: 0.000036  min_lr: 0.000000  loss: 3.9967 (3.9462)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (65323.4968)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1090  max mem: 15572
Epoch: [16]  [ 780/2809]  eta: 0:19:28  lr: 0.000036  min_lr: 0.000000  loss: 3.9448 (3.9473)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (65326.2177)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1159  max mem: 15572
Epoch: [16]  [ 790/2809]  eta: 0:19:21  lr: 0.000036  min_lr: 0.000000  loss: 4.0644 (3.9470)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (65328.8698)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1261  max mem: 15572
Epoch: [16]  [ 800/2809]  eta: 0:19:15  lr: 0.000036  min_lr: 0.000000  loss: 4.0644 (3.9479)  class_acc: 0.2500 (0.2728)  loss_scale: 65536.0000 (65331.4557)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0863  max mem: 15572
Epoch: [16]  [ 810/2809]  eta: 0:19:08  lr: 0.000036  min_lr: 0.000000  loss: 3.9833 (3.9468)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (65333.9778)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1016  max mem: 15572
Epoch: [16]  [ 820/2809]  eta: 0:19:01  lr: 0.000036  min_lr: 0.000000  loss: 3.9882 (3.9472)  class_acc: 0.2500 (0.2725)  loss_scale: 65536.0000 (65336.4385)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0861  max mem: 15572
[2025-01-15 22:12:36,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:12:36,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 830/2809]  eta: 0:18:56  lr: 0.000036  min_lr: 0.000000  loss: 3.9882 (3.9459)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (65733.1600)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1222  max mem: 15572
[2025-01-15 22:12:40,744] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45775
[2025-01-15 22:12:40,744] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:12:40,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 840/2809]  eta: 0:18:48  lr: 0.000036  min_lr: 0.000000  loss: 3.8855 (3.9466)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (65730.8157)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1046  max mem: 15572
Epoch: [16]  [ 850/2809]  eta: 0:18:41  lr: 0.000036  min_lr: 0.000000  loss: 3.9316 (3.9465)  class_acc: 0.2500 (0.2725)  loss_scale: 65536.0000 (65728.5264)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0614  max mem: 15572
Epoch: [16]  [ 860/2809]  eta: 0:18:35  lr: 0.000036  min_lr: 0.000000  loss: 3.7841 (3.9444)  class_acc: 0.2500 (0.2726)  loss_scale: 65536.0000 (65726.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0911  max mem: 15572
Epoch: [16]  [ 870/2809]  eta: 0:18:29  lr: 0.000036  min_lr: 0.000000  loss: 3.7755 (3.9442)  class_acc: 0.2917 (0.2726)  loss_scale: 65536.0000 (65724.1056)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1213  max mem: 15572
Epoch: [16]  [ 880/2809]  eta: 0:18:21  lr: 0.000036  min_lr: 0.000000  loss: 3.8119 (3.9437)  class_acc: 0.2500 (0.2726)  loss_scale: 65536.0000 (65721.9705)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0949  max mem: 15572
Epoch: [16]  [ 890/2809]  eta: 0:18:16  lr: 0.000036  min_lr: 0.000000  loss: 3.9371 (3.9454)  class_acc: 0.2500 (0.2722)  loss_scale: 65536.0000 (65719.8833)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0804  max mem: 15572
Epoch: [16]  [ 900/2809]  eta: 0:18:10  lr: 0.000036  min_lr: 0.000000  loss: 3.9371 (3.9446)  class_acc: 0.2500 (0.2720)  loss_scale: 65536.0000 (65717.8424)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.0966  max mem: 15572
Epoch: [16]  [ 910/2809]  eta: 0:18:06  lr: 0.000036  min_lr: 0.000000  loss: 3.9451 (3.9445)  class_acc: 0.2083 (0.2717)  loss_scale: 65536.0000 (65715.8463)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1653  max mem: 15572
Epoch: [16]  [ 920/2809]  eta: 0:17:59  lr: 0.000036  min_lr: 0.000000  loss: 3.8055 (3.9410)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (65713.8936)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1523  max mem: 15572
Epoch: [16]  [ 930/2809]  eta: 0:17:52  lr: 0.000036  min_lr: 0.000000  loss: 3.5802 (3.9383)  class_acc: 0.3333 (0.2730)  loss_scale: 65536.0000 (65711.9828)  weight_decay: 0.0500 (0.0500)  time: 0.4970  data: 0.0544  max mem: 15572
Epoch: [16]  [ 940/2809]  eta: 0:17:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9691 (3.9399)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (65710.1126)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0585  max mem: 15572
Epoch: [16]  [ 950/2809]  eta: 0:17:40  lr: 0.000036  min_lr: 0.000000  loss: 4.0260 (3.9393)  class_acc: 0.2500 (0.2728)  loss_scale: 65536.0000 (65708.2818)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1057  max mem: 15572
[2025-01-15 22:13:51,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:13:51,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 960/2809]  eta: 0:17:35  lr: 0.000036  min_lr: 0.000000  loss: 3.8705 (3.9372)  class_acc: 0.3333 (0.2737)  loss_scale: 65536.0000 (65774.6847)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1475  max mem: 15572
[2025-01-15 22:13:53,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45906
[2025-01-15 22:13:53,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:13:53,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 970/2809]  eta: 0:17:28  lr: 0.000036  min_lr: 0.000000  loss: 3.8952 (3.9354)  class_acc: 0.3333 (0.2736)  loss_scale: 65536.0000 (65839.7199)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1183  max mem: 15572
Epoch: [16]  [ 980/2809]  eta: 0:17:23  lr: 0.000036  min_lr: 0.000000  loss: 3.9693 (3.9379)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (65836.6239)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1296  max mem: 15572
Epoch: [16]  [ 990/2809]  eta: 0:17:17  lr: 0.000036  min_lr: 0.000000  loss: 4.1152 (3.9398)  class_acc: 0.2500 (0.2728)  loss_scale: 65536.0000 (65833.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1118  max mem: 15572
Epoch: [16]  [1000/2809]  eta: 0:17:10  lr: 0.000036  min_lr: 0.000000  loss: 4.1565 (3.9413)  class_acc: 0.2500 (0.2726)  loss_scale: 65536.0000 (65830.6174)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0654  max mem: 15572
Epoch: [16]  [1010/2809]  eta: 0:17:05  lr: 0.000036  min_lr: 0.000000  loss: 4.0912 (3.9414)  class_acc: 0.2083 (0.2725)  loss_scale: 65536.0000 (65827.7033)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1101  max mem: 15572
Epoch: [16]  [1020/2809]  eta: 0:17:00  lr: 0.000036  min_lr: 0.000000  loss: 3.9611 (3.9405)  class_acc: 0.2917 (0.2728)  loss_scale: 65536.0000 (65824.8462)  weight_decay: 0.0500 (0.0500)  time: 0.6263  data: 0.1818  max mem: 15572
Epoch: [16]  [1030/2809]  eta: 0:16:54  lr: 0.000036  min_lr: 0.000000  loss: 4.1229 (3.9440)  class_acc: 0.2083 (0.2716)  loss_scale: 65536.0000 (65822.0446)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1616  max mem: 15572
Epoch: [16]  [1040/2809]  eta: 0:16:49  lr: 0.000036  min_lr: 0.000000  loss: 4.3555 (3.9436)  class_acc: 0.1667 (0.2713)  loss_scale: 65536.0000 (65819.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1358  max mem: 15572
Epoch: [16]  [1050/2809]  eta: 0:16:43  lr: 0.000036  min_lr: 0.000000  loss: 3.9313 (3.9445)  class_acc: 0.2083 (0.2711)  loss_scale: 65536.0000 (65816.6013)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1437  max mem: 15572
[2025-01-15 22:14:46,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=308, lr=[3.458328714272749e-07, 3.458328714272749e-07, 4.940469591818214e-07, 4.940469591818214e-07, 7.057813702597449e-07, 7.057813702597449e-07, 1.0082591003710642e-06, 1.0082591003710642e-06, 1.4403701433872347e-06, 1.4403701433872347e-06, 2.057671633410335e-06, 2.057671633410335e-06, 2.9395309048719076e-06, 2.9395309048719076e-06, 4.199329864102725e-06, 4.199329864102725e-06, 5.999042663003894e-06, 5.999042663003894e-06, 8.570060947148422e-06, 8.570060947148422e-06, 1.2242944210212029e-05, 1.2242944210212029e-05, 1.7489920300302902e-05, 1.7489920300302902e-05, 2.4985600429004146e-05, 2.4985600429004146e-05, 3.5693714898577354e-05, 3.5693714898577354e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:14:46,078] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=28.464673001130343, CurrSamplesPerSec=31.343912984723378, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [1060/2809]  eta: 0:16:38  lr: 0.000036  min_lr: 0.000000  loss: 3.9565 (3.9443)  class_acc: 0.2500 (0.2713)  loss_scale: 65536.0000 (65813.9566)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1669  max mem: 15572
Epoch: [16]  [1070/2809]  eta: 0:16:32  lr: 0.000036  min_lr: 0.000000  loss: 3.9664 (3.9434)  class_acc: 0.2500 (0.2718)  loss_scale: 65536.0000 (65811.3613)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1722  max mem: 15572
Epoch: [16]  [1080/2809]  eta: 0:16:27  lr: 0.000036  min_lr: 0.000000  loss: 3.9664 (3.9433)  class_acc: 0.2917 (0.2721)  loss_scale: 65536.0000 (65808.8141)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1477  max mem: 15572
Epoch: [16]  [1090/2809]  eta: 0:16:21  lr: 0.000036  min_lr: 0.000000  loss: 4.0421 (3.9444)  class_acc: 0.2917 (0.2720)  loss_scale: 65536.0000 (65806.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1162  max mem: 15572
[2025-01-15 22:15:06,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:15:06,870] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:15:08,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46039
[2025-01-15 22:15:08,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:15:08,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-15 22:15:11,131] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46044
[2025-01-15 22:15:11,132] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:15:11,132] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1100/2809]  eta: 0:16:14  lr: 0.000036  min_lr: 0.000000  loss: 4.0421 (3.9425)  class_acc: 0.2917 (0.2726)  loss_scale: 65536.0000 (66012.1926)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0714  max mem: 15572
Epoch: [16]  [1110/2809]  eta: 0:16:08  lr: 0.000036  min_lr: 0.000000  loss: 3.7019 (3.9420)  class_acc: 0.2917 (0.2733)  loss_scale: 32768.0000 (65712.9649)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0906  max mem: 15572
Epoch: [16]  [1120/2809]  eta: 0:16:03  lr: 0.000036  min_lr: 0.000000  loss: 3.8470 (3.9415)  class_acc: 0.2917 (0.2734)  loss_scale: 32768.0000 (65419.0758)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1561  max mem: 15572
Epoch: [16]  [1130/2809]  eta: 0:15:58  lr: 0.000036  min_lr: 0.000000  loss: 3.9076 (3.9405)  class_acc: 0.2917 (0.2735)  loss_scale: 32768.0000 (65130.3837)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.1860  max mem: 15572
Epoch: [16]  [1140/2809]  eta: 0:15:53  lr: 0.000036  min_lr: 0.000000  loss: 4.0823 (3.9428)  class_acc: 0.2083 (0.2728)  loss_scale: 32768.0000 (64846.7520)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1454  max mem: 15572
Epoch: [16]  [1150/2809]  eta: 0:15:47  lr: 0.000036  min_lr: 0.000000  loss: 4.1328 (3.9442)  class_acc: 0.2083 (0.2720)  loss_scale: 32768.0000 (64568.0487)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1182  max mem: 15572
Epoch: [16]  [1160/2809]  eta: 0:15:41  lr: 0.000036  min_lr: 0.000000  loss: 4.0816 (3.9445)  class_acc: 0.2083 (0.2724)  loss_scale: 32768.0000 (64294.1464)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1142  max mem: 15572
Epoch: [16]  [1170/2809]  eta: 0:15:35  lr: 0.000036  min_lr: 0.000000  loss: 3.9177 (3.9450)  class_acc: 0.2917 (0.2722)  loss_scale: 32768.0000 (64024.9223)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1117  max mem: 15572
Epoch: [16]  [1180/2809]  eta: 0:15:30  lr: 0.000036  min_lr: 0.000000  loss: 3.9602 (3.9449)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (63760.2574)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1540  max mem: 15572
Epoch: [16]  [1190/2809]  eta: 0:15:25  lr: 0.000036  min_lr: 0.000000  loss: 3.9184 (3.9443)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (63500.0369)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.1782  max mem: 15572
Epoch: [16]  [1200/2809]  eta: 0:15:19  lr: 0.000036  min_lr: 0.000000  loss: 3.8941 (3.9455)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (63244.1499)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1331  max mem: 15572
Epoch: [16]  [1210/2809]  eta: 0:15:13  lr: 0.000036  min_lr: 0.000000  loss: 3.7528 (3.9435)  class_acc: 0.3333 (0.2737)  loss_scale: 32768.0000 (62992.4889)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1128  max mem: 15572
Epoch: [16]  [1220/2809]  eta: 0:15:07  lr: 0.000036  min_lr: 0.000000  loss: 3.7109 (3.9419)  class_acc: 0.3333 (0.2737)  loss_scale: 32768.0000 (62744.9500)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1246  max mem: 15572
[2025-01-15 22:16:26,770] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:16:26,770] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1230/2809]  eta: 0:15:02  lr: 0.000036  min_lr: 0.000000  loss: 3.9629 (3.9438)  class_acc: 0.2500 (0.2734)  loss_scale: 32768.0000 (62554.6710)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1653  max mem: 15572
[2025-01-15 22:16:30,160] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46181
[2025-01-15 22:16:30,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:16:30,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1240/2809]  eta: 0:14:55  lr: 0.000036  min_lr: 0.000000  loss: 4.1389 (3.9434)  class_acc: 0.2917 (0.2736)  loss_scale: 32768.0000 (62473.0766)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0972  max mem: 15572
Epoch: [16]  [1250/2809]  eta: 0:14:50  lr: 0.000036  min_lr: 0.000000  loss: 4.1275 (3.9458)  class_acc: 0.2083 (0.2732)  loss_scale: 32768.0000 (62235.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0873  max mem: 15572
Epoch: [16]  [1260/2809]  eta: 0:14:45  lr: 0.000036  min_lr: 0.000000  loss: 3.9751 (3.9444)  class_acc: 0.2083 (0.2737)  loss_scale: 32768.0000 (62001.9413)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1701  max mem: 15572
Epoch: [16]  [1270/2809]  eta: 0:14:37  lr: 0.000036  min_lr: 0.000000  loss: 3.7439 (3.9451)  class_acc: 0.2917 (0.2735)  loss_scale: 32768.0000 (61771.9339)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0836  max mem: 15572
Epoch: [16]  [1280/2809]  eta: 0:14:31  lr: 0.000036  min_lr: 0.000000  loss: 4.0735 (3.9452)  class_acc: 0.2500 (0.2736)  loss_scale: 32768.0000 (61545.5176)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0550  max mem: 15572
Epoch: [16]  [1290/2809]  eta: 0:14:26  lr: 0.000036  min_lr: 0.000000  loss: 4.0253 (3.9454)  class_acc: 0.2917 (0.2736)  loss_scale: 32768.0000 (61322.6088)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1350  max mem: 15572
Epoch: [16]  [1300/2809]  eta: 0:14:20  lr: 0.000036  min_lr: 0.000000  loss: 3.8808 (3.9456)  class_acc: 0.2500 (0.2735)  loss_scale: 32768.0000 (61103.1268)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1540  max mem: 15572
Epoch: [16]  [1310/2809]  eta: 0:14:13  lr: 0.000036  min_lr: 0.000000  loss: 3.9622 (3.9451)  class_acc: 0.2500 (0.2739)  loss_scale: 32768.0000 (60886.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0938  max mem: 15572
Epoch: [16]  [1320/2809]  eta: 0:14:08  lr: 0.000036  min_lr: 0.000000  loss: 3.9910 (3.9457)  class_acc: 0.2917 (0.2740)  loss_scale: 32768.0000 (60674.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.1055  max mem: 15572
Epoch: [16]  [1330/2809]  eta: 0:14:01  lr: 0.000036  min_lr: 0.000000  loss: 3.9369 (3.9454)  class_acc: 0.2917 (0.2740)  loss_scale: 32768.0000 (60464.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0856  max mem: 15572
Epoch: [16]  [1340/2809]  eta: 0:13:55  lr: 0.000036  min_lr: 0.000000  loss: 3.9595 (3.9466)  class_acc: 0.2500 (0.2736)  loss_scale: 32768.0000 (60257.9329)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0557  max mem: 15572
Epoch: [16]  [1350/2809]  eta: 0:13:49  lr: 0.000036  min_lr: 0.000000  loss: 4.1259 (3.9479)  class_acc: 0.2083 (0.2733)  loss_scale: 32768.0000 (60054.4545)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1118  max mem: 15572
Epoch: [16]  [1360/2809]  eta: 0:13:44  lr: 0.000035  min_lr: 0.000000  loss: 4.0898 (3.9472)  class_acc: 0.2500 (0.2731)  loss_scale: 32768.0000 (59853.9662)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1676  max mem: 15572
[2025-01-15 22:17:42,508] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:17:42,508] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1370/2809]  eta: 0:13:39  lr: 0.000035  min_lr: 0.000000  loss: 3.6394 (3.9462)  class_acc: 0.2500 (0.2732)  loss_scale: 32768.0000 (59775.9066)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1913  max mem: 15572
Epoch: [16]  [1380/2809]  eta: 0:13:34  lr: 0.000035  min_lr: 0.000000  loss: 3.7935 (3.9463)  class_acc: 0.2917 (0.2733)  loss_scale: 65536.0000 (59817.6162)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1626  max mem: 15572
Epoch: [16]  [1390/2809]  eta: 0:13:28  lr: 0.000035  min_lr: 0.000000  loss: 3.9828 (3.9471)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (59858.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1525  max mem: 15572
Epoch: [16]  [1400/2809]  eta: 0:13:22  lr: 0.000035  min_lr: 0.000000  loss: 4.0232 (3.9473)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (59899.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1146  max mem: 15572
Epoch: [16]  [1410/2809]  eta: 0:13:16  lr: 0.000035  min_lr: 0.000000  loss: 4.0232 (3.9477)  class_acc: 0.2083 (0.2728)  loss_scale: 65536.0000 (59939.1977)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.1050  max mem: 15572
Epoch: [16]  [1420/2809]  eta: 0:13:10  lr: 0.000035  min_lr: 0.000000  loss: 4.0282 (3.9485)  class_acc: 0.2500 (0.2727)  loss_scale: 65536.0000 (59978.5841)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1400  max mem: 15572
Epoch: [16]  [1430/2809]  eta: 0:13:04  lr: 0.000035  min_lr: 0.000000  loss: 4.0641 (3.9500)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (60017.4200)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0915  max mem: 15572
Epoch: [16]  [1440/2809]  eta: 0:12:58  lr: 0.000035  min_lr: 0.000000  loss: 3.9116 (3.9498)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (60055.7169)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0687  max mem: 15572
Epoch: [16]  [1450/2809]  eta: 0:12:52  lr: 0.000035  min_lr: 0.000000  loss: 3.8801 (3.9506)  class_acc: 0.2500 (0.2722)  loss_scale: 65536.0000 (60093.4859)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1361  max mem: 15572
Epoch: [16]  [1460/2809]  eta: 0:12:46  lr: 0.000035  min_lr: 0.000000  loss: 4.0678 (3.9500)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (60130.7379)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1133  max mem: 15572
Epoch: [16]  [1470/2809]  eta: 0:12:41  lr: 0.000035  min_lr: 0.000000  loss: 4.0740 (3.9503)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (60167.4833)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1027  max mem: 15572
Epoch: [16]  [1480/2809]  eta: 0:12:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0125 (3.9510)  class_acc: 0.2917 (0.2726)  loss_scale: 65536.0000 (60203.7326)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1605  max mem: 15572
Epoch: [16]  [1490/2809]  eta: 0:12:29  lr: 0.000035  min_lr: 0.000000  loss: 3.9170 (3.9510)  class_acc: 0.2083 (0.2727)  loss_scale: 65536.0000 (60239.4956)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1282  max mem: 15572
[2025-01-15 22:18:53,318] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:18:53,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:18:53,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46439
[2025-01-15 22:18:53,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:18:53,946] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1500/2809]  eta: 0:12:23  lr: 0.000035  min_lr: 0.000000  loss: 3.7559 (3.9501)  class_acc: 0.3333 (0.2732)  loss_scale: 65536.0000 (60318.4437)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0632  max mem: 15572
Epoch: [16]  [1510/2809]  eta: 0:12:17  lr: 0.000035  min_lr: 0.000000  loss: 3.7559 (3.9509)  class_acc: 0.3333 (0.2729)  loss_scale: 65536.0000 (60352.9742)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0704  max mem: 15572
Epoch: [16]  [1520/2809]  eta: 0:12:11  lr: 0.000035  min_lr: 0.000000  loss: 4.0088 (3.9512)  class_acc: 0.2083 (0.2726)  loss_scale: 65536.0000 (60387.0506)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.1131  max mem: 15572
Epoch: [16]  [1530/2809]  eta: 0:12:06  lr: 0.000035  min_lr: 0.000000  loss: 4.0088 (3.9519)  class_acc: 0.2500 (0.2726)  loss_scale: 65536.0000 (60420.6819)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1477  max mem: 15572
Epoch: [16]  [1540/2809]  eta: 0:12:00  lr: 0.000035  min_lr: 0.000000  loss: 3.9178 (3.9502)  class_acc: 0.2917 (0.2730)  loss_scale: 65536.0000 (60453.8767)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1567  max mem: 15572
Epoch: [16]  [1550/2809]  eta: 0:11:55  lr: 0.000035  min_lr: 0.000000  loss: 3.6507 (3.9479)  class_acc: 0.3333 (0.2735)  loss_scale: 65536.0000 (60486.6435)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1429  max mem: 15572
Epoch: [16]  [1560/2809]  eta: 0:11:49  lr: 0.000035  min_lr: 0.000000  loss: 3.6856 (3.9489)  class_acc: 0.2917 (0.2733)  loss_scale: 65536.0000 (60518.9904)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1222  max mem: 15572
Epoch: [16]  [1570/2809]  eta: 0:11:43  lr: 0.000035  min_lr: 0.000000  loss: 4.0004 (3.9478)  class_acc: 0.2083 (0.2730)  loss_scale: 65536.0000 (60550.9255)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1303  max mem: 15572
Epoch: [16]  [1580/2809]  eta: 0:11:38  lr: 0.000035  min_lr: 0.000000  loss: 3.8845 (3.9475)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (60582.4567)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1255  max mem: 15572
Epoch: [16]  [1590/2809]  eta: 0:11:33  lr: 0.000035  min_lr: 0.000000  loss: 4.1890 (3.9488)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (60613.5915)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1616  max mem: 15572
Epoch: [16]  [1600/2809]  eta: 0:11:26  lr: 0.000035  min_lr: 0.000000  loss: 4.0951 (3.9488)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (60644.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1086  max mem: 15572
Epoch: [16]  [1610/2809]  eta: 0:11:20  lr: 0.000035  min_lr: 0.000000  loss: 4.0663 (3.9484)  class_acc: 0.2917 (0.2730)  loss_scale: 65536.0000 (60674.7014)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0509  max mem: 15572
Epoch: [16]  [1620/2809]  eta: 0:11:14  lr: 0.000035  min_lr: 0.000000  loss: 4.0427 (3.9472)  class_acc: 0.2917 (0.2733)  loss_scale: 65536.0000 (60704.6909)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1105  max mem: 15572
[2025-01-15 22:20:06,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:20:06,058] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:20:06,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46569
[2025-01-15 22:20:06,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:20:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1630/2809]  eta: 0:11:08  lr: 0.000035  min_lr: 0.000000  loss: 4.1556 (3.9486)  class_acc: 0.2083 (0.2731)  loss_scale: 65536.0000 (60774.4942)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0878  max mem: 15572
Epoch: [16]  [1640/2809]  eta: 0:11:03  lr: 0.000035  min_lr: 0.000000  loss: 4.1469 (3.9488)  class_acc: 0.2083 (0.2732)  loss_scale: 65536.0000 (60803.5101)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0997  max mem: 15572
Epoch: [16]  [1650/2809]  eta: 0:10:57  lr: 0.000035  min_lr: 0.000000  loss: 3.9526 (3.9485)  class_acc: 0.2500 (0.2735)  loss_scale: 65536.0000 (60832.1744)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1243  max mem: 15572
Epoch: [16]  [1660/2809]  eta: 0:10:51  lr: 0.000035  min_lr: 0.000000  loss: 3.9526 (3.9488)  class_acc: 0.2917 (0.2736)  loss_scale: 65536.0000 (60860.4937)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1151  max mem: 15572
Epoch: [16]  [1670/2809]  eta: 0:10:46  lr: 0.000035  min_lr: 0.000000  loss: 3.9748 (3.9486)  class_acc: 0.2917 (0.2738)  loss_scale: 65536.0000 (60888.4740)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1384  max mem: 15572
Epoch: [16]  [1680/2809]  eta: 0:10:40  lr: 0.000035  min_lr: 0.000000  loss: 3.6399 (3.9475)  class_acc: 0.2500 (0.2740)  loss_scale: 65536.0000 (60916.1214)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1222  max mem: 15572
Epoch: [16]  [1690/2809]  eta: 0:10:34  lr: 0.000035  min_lr: 0.000000  loss: 3.8137 (3.9470)  class_acc: 0.2917 (0.2742)  loss_scale: 65536.0000 (60943.4418)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1192  max mem: 15572
Epoch: [16]  [1700/2809]  eta: 0:10:29  lr: 0.000035  min_lr: 0.000000  loss: 3.9250 (3.9480)  class_acc: 0.2917 (0.2740)  loss_scale: 65536.0000 (60970.4409)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1270  max mem: 15572
Epoch: [16]  [1710/2809]  eta: 0:10:23  lr: 0.000035  min_lr: 0.000000  loss: 3.9250 (3.9476)  class_acc: 0.2500 (0.2742)  loss_scale: 65536.0000 (60997.1245)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.0862  max mem: 15572
Epoch: [16]  [1720/2809]  eta: 0:10:17  lr: 0.000035  min_lr: 0.000000  loss: 3.8061 (3.9476)  class_acc: 0.2500 (0.2742)  loss_scale: 65536.0000 (61023.4980)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.0984  max mem: 15572
Epoch: [16]  [1730/2809]  eta: 0:10:11  lr: 0.000035  min_lr: 0.000000  loss: 4.0707 (3.9478)  class_acc: 0.2083 (0.2740)  loss_scale: 65536.0000 (61049.5667)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1002  max mem: 15572
Epoch: [16]  [1740/2809]  eta: 0:10:06  lr: 0.000035  min_lr: 0.000000  loss: 4.0133 (3.9479)  class_acc: 0.2083 (0.2737)  loss_scale: 65536.0000 (61075.3360)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1168  max mem: 15572
Epoch: [16]  [1750/2809]  eta: 0:10:00  lr: 0.000035  min_lr: 0.000000  loss: 3.9379 (3.9479)  class_acc: 0.2083 (0.2736)  loss_scale: 65536.0000 (61100.8110)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1493  max mem: 15572
[2025-01-15 22:21:18,754] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:21:18,755] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:21:20,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46701
[2025-01-15 22:21:20,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:21:20,419] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1760/2809]  eta: 0:09:54  lr: 0.000035  min_lr: 0.000000  loss: 3.9133 (3.9462)  class_acc: 0.2917 (0.2738)  loss_scale: 65536.0000 (61237.6422)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0997  max mem: 15572
Epoch: [16]  [1770/2809]  eta: 0:09:49  lr: 0.000035  min_lr: 0.000000  loss: 3.9133 (3.9458)  class_acc: 0.2917 (0.2739)  loss_scale: 65536.0000 (61261.9130)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.1072  max mem: 15572
Epoch: [16]  [1780/2809]  eta: 0:09:43  lr: 0.000035  min_lr: 0.000000  loss: 3.9442 (3.9457)  class_acc: 0.2917 (0.2737)  loss_scale: 65536.0000 (61285.9113)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1497  max mem: 15572
Epoch: [16]  [1790/2809]  eta: 0:09:37  lr: 0.000035  min_lr: 0.000000  loss: 3.9861 (3.9455)  class_acc: 0.2917 (0.2737)  loss_scale: 65536.0000 (61309.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1311  max mem: 15572
Epoch: [16]  [1800/2809]  eta: 0:09:32  lr: 0.000035  min_lr: 0.000000  loss: 3.8578 (3.9448)  class_acc: 0.2500 (0.2738)  loss_scale: 65536.0000 (61333.1083)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1472  max mem: 15572
Epoch: [16]  [1810/2809]  eta: 0:09:26  lr: 0.000035  min_lr: 0.000000  loss: 3.7057 (3.9425)  class_acc: 0.2917 (0.2743)  loss_scale: 65536.0000 (61356.3158)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1435  max mem: 15572
Epoch: [16]  [1820/2809]  eta: 0:09:21  lr: 0.000035  min_lr: 0.000000  loss: 3.9355 (3.9431)  class_acc: 0.2917 (0.2742)  loss_scale: 65536.0000 (61379.2685)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1442  max mem: 15572
Epoch: [16]  [1830/2809]  eta: 0:09:15  lr: 0.000035  min_lr: 0.000000  loss: 4.1392 (3.9434)  class_acc: 0.2083 (0.2740)  loss_scale: 65536.0000 (61401.9705)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1475  max mem: 15572
Epoch: [16]  [1840/2809]  eta: 0:09:09  lr: 0.000035  min_lr: 0.000000  loss: 4.0828 (3.9441)  class_acc: 0.2083 (0.2738)  loss_scale: 65536.0000 (61424.4259)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1418  max mem: 15572
Epoch: [16]  [1850/2809]  eta: 0:09:04  lr: 0.000035  min_lr: 0.000000  loss: 4.1400 (3.9452)  class_acc: 0.2083 (0.2736)  loss_scale: 65536.0000 (61446.6386)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1711  max mem: 15572
[2025-01-15 22:22:17,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46798
[2025-01-15 22:22:17,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:22:17,117] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [1860/2809]  eta: 0:08:58  lr: 0.000035  min_lr: 0.000000  loss: 3.9998 (3.9448)  class_acc: 0.2083 (0.2734)  loss_scale: 65536.0000 (61345.3584)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1455  max mem: 15572
Epoch: [16]  [1870/2809]  eta: 0:08:53  lr: 0.000035  min_lr: 0.000000  loss: 3.8667 (3.9448)  class_acc: 0.2500 (0.2736)  loss_scale: 32768.0000 (61192.6200)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1322  max mem: 15572
Epoch: [16]  [1880/2809]  eta: 0:08:47  lr: 0.000035  min_lr: 0.000000  loss: 4.1202 (3.9463)  class_acc: 0.2500 (0.2733)  loss_scale: 32768.0000 (61041.5056)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1066  max mem: 15572
Epoch: [16]  [1890/2809]  eta: 0:08:41  lr: 0.000035  min_lr: 0.000000  loss: 4.1561 (3.9469)  class_acc: 0.2083 (0.2731)  loss_scale: 32768.0000 (60891.9894)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1544  max mem: 15572
Epoch: [16]  [1900/2809]  eta: 0:08:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0431 (3.9469)  class_acc: 0.2083 (0.2731)  loss_scale: 32768.0000 (60744.0463)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1190  max mem: 15572
Epoch: [16]  [1910/2809]  eta: 0:08:29  lr: 0.000035  min_lr: 0.000000  loss: 3.8434 (3.9461)  class_acc: 0.2500 (0.2731)  loss_scale: 32768.0000 (60597.6515)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0516  max mem: 15572
Epoch: [16]  [1920/2809]  eta: 0:08:24  lr: 0.000035  min_lr: 0.000000  loss: 3.8434 (3.9466)  class_acc: 0.2500 (0.2730)  loss_scale: 32768.0000 (60452.7808)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1106  max mem: 15572
Epoch: [16]  [1930/2809]  eta: 0:08:18  lr: 0.000035  min_lr: 0.000000  loss: 4.1137 (3.9462)  class_acc: 0.2083 (0.2729)  loss_scale: 32768.0000 (60309.4107)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1041  max mem: 15572
Epoch: [16]  [1940/2809]  eta: 0:08:12  lr: 0.000035  min_lr: 0.000000  loss: 4.0964 (3.9468)  class_acc: 0.2083 (0.2728)  loss_scale: 32768.0000 (60167.5178)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1086  max mem: 15572
Epoch: [16]  [1950/2809]  eta: 0:08:06  lr: 0.000035  min_lr: 0.000000  loss: 4.1879 (3.9479)  class_acc: 0.2083 (0.2725)  loss_scale: 32768.0000 (60027.0794)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1069  max mem: 15572
Epoch: [16]  [1960/2809]  eta: 0:08:01  lr: 0.000035  min_lr: 0.000000  loss: 4.0000 (3.9472)  class_acc: 0.2500 (0.2727)  loss_scale: 32768.0000 (59888.0734)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1251  max mem: 15572
Epoch: [16]  [1970/2809]  eta: 0:07:55  lr: 0.000035  min_lr: 0.000000  loss: 3.7832 (3.9465)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (59750.4779)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1553  max mem: 15572
Epoch: [16]  [1980/2809]  eta: 0:07:50  lr: 0.000035  min_lr: 0.000000  loss: 3.8561 (3.9466)  class_acc: 0.2083 (0.2722)  loss_scale: 32768.0000 (59614.2716)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1175  max mem: 15572
[2025-01-15 22:23:28,877] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:23:28,878] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [1990/2809]  eta: 0:07:44  lr: 0.000035  min_lr: 0.000000  loss: 3.9115 (3.9461)  class_acc: 0.2500 (0.2725)  loss_scale: 32768.0000 (59611.0979)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1161  max mem: 15572
Epoch: [16]  [2000/2809]  eta: 0:07:38  lr: 0.000035  min_lr: 0.000000  loss: 4.0850 (3.9475)  class_acc: 0.2083 (0.2723)  loss_scale: 65536.0000 (59640.7076)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1212  max mem: 15572
Epoch: [16]  [2010/2809]  eta: 0:07:33  lr: 0.000035  min_lr: 0.000000  loss: 4.2259 (3.9484)  class_acc: 0.2083 (0.2720)  loss_scale: 65536.0000 (59670.0229)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1007  max mem: 15572
Epoch: [16]  [2020/2809]  eta: 0:07:27  lr: 0.000035  min_lr: 0.000000  loss: 4.1326 (3.9492)  class_acc: 0.2083 (0.2719)  loss_scale: 65536.0000 (59699.0480)  weight_decay: 0.0500 (0.0500)  time: 0.6120  data: 0.1654  max mem: 15572
Epoch: [16]  [2030/2809]  eta: 0:07:22  lr: 0.000035  min_lr: 0.000000  loss: 3.7763 (3.9476)  class_acc: 0.3333 (0.2723)  loss_scale: 65536.0000 (59727.7873)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.2079  max mem: 15572
Epoch: [16]  [2040/2809]  eta: 0:07:16  lr: 0.000035  min_lr: 0.000000  loss: 3.8248 (3.9478)  class_acc: 0.3333 (0.2723)  loss_scale: 65536.0000 (59756.2450)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1961  max mem: 15572
Epoch: [16]  [2050/2809]  eta: 0:07:11  lr: 0.000035  min_lr: 0.000000  loss: 3.9584 (3.9469)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (59784.4252)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1740  max mem: 15572
[2025-01-15 22:24:11,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=315, lr=[3.395923861060386e-07, 3.395923861060386e-07, 4.851319801514837e-07, 4.851319801514837e-07, 6.930456859306911e-07, 6.930456859306911e-07, 9.90065265615273e-07, 9.90065265615273e-07, 1.4143789508789615e-06, 1.4143789508789615e-06, 2.0205413583985166e-06, 2.0205413583985166e-06, 2.886487654855024e-06, 2.886487654855024e-06, 4.123553792650034e-06, 4.123553792650034e-06, 5.890791132357192e-06, 5.890791132357192e-06, 8.415415903367418e-06, 8.415415903367418e-06, 1.2022022719096311e-05, 1.2022022719096311e-05, 1.717431817013759e-05, 1.717431817013759e-05, 2.4534740243053702e-05, 2.4534740243053702e-05, 3.504962891864815e-05, 3.504962891864815e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:24:11,015] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=28.47341702968233, CurrSamplesPerSec=29.11295897827445, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [16]  [2060/2809]  eta: 0:07:05  lr: 0.000035  min_lr: 0.000000  loss: 3.9766 (3.9480)  class_acc: 0.2917 (0.2724)  loss_scale: 65536.0000 (59812.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1192  max mem: 15572
Epoch: [16]  [2070/2809]  eta: 0:06:59  lr: 0.000035  min_lr: 0.000000  loss: 4.0256 (3.9483)  class_acc: 0.2917 (0.2724)  loss_scale: 65536.0000 (59839.9691)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0742  max mem: 15572
Epoch: [16]  [2080/2809]  eta: 0:06:53  lr: 0.000035  min_lr: 0.000000  loss: 4.0042 (3.9488)  class_acc: 0.2083 (0.2723)  loss_scale: 65536.0000 (59867.3407)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0865  max mem: 15572
Epoch: [16]  [2090/2809]  eta: 0:06:48  lr: 0.000035  min_lr: 0.000000  loss: 3.9383 (3.9490)  class_acc: 0.2917 (0.2723)  loss_scale: 65536.0000 (59894.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.1837  max mem: 15572
Epoch: [16]  [2100/2809]  eta: 0:06:42  lr: 0.000035  min_lr: 0.000000  loss: 3.9532 (3.9492)  class_acc: 0.2500 (0.2723)  loss_scale: 65536.0000 (59921.3022)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1245  max mem: 15572
Epoch: [16]  [2110/2809]  eta: 0:06:36  lr: 0.000035  min_lr: 0.000000  loss: 3.8422 (3.9488)  class_acc: 0.2917 (0.2725)  loss_scale: 65536.0000 (59947.8996)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0622  max mem: 15572
[2025-01-15 22:24:42,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:24:42,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:24:44,499] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47059
[2025-01-15 22:24:44,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:24:44,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [2120/2809]  eta: 0:06:31  lr: 0.000035  min_lr: 0.000000  loss: 3.8280 (3.9483)  class_acc: 0.2917 (0.2726)  loss_scale: 65536.0000 (60097.8406)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1218  max mem: 15572
Epoch: [16]  [2130/2809]  eta: 0:06:25  lr: 0.000035  min_lr: 0.000000  loss: 3.8361 (3.9476)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (60123.3599)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1520  max mem: 15572
Epoch: [16]  [2140/2809]  eta: 0:06:19  lr: 0.000035  min_lr: 0.000000  loss: 3.8368 (3.9475)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (60148.6408)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1262  max mem: 15572
Epoch: [16]  [2150/2809]  eta: 0:06:14  lr: 0.000035  min_lr: 0.000000  loss: 3.9572 (3.9473)  class_acc: 0.2500 (0.2725)  loss_scale: 65536.0000 (60173.6867)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1154  max mem: 15572
Epoch: [16]  [2160/2809]  eta: 0:06:08  lr: 0.000035  min_lr: 0.000000  loss: 3.9107 (3.9466)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (60198.5007)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1767  max mem: 15572
Epoch: [16]  [2170/2809]  eta: 0:06:02  lr: 0.000035  min_lr: 0.000000  loss: 3.9550 (3.9476)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (60223.0861)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0957  max mem: 15572
Epoch: [16]  [2180/2809]  eta: 0:05:57  lr: 0.000035  min_lr: 0.000000  loss: 3.6721 (3.9459)  class_acc: 0.2917 (0.2733)  loss_scale: 65536.0000 (60247.4461)  weight_decay: 0.0500 (0.0500)  time: 0.4907  data: 0.0328  max mem: 15572
Epoch: [16]  [2190/2809]  eta: 0:05:51  lr: 0.000035  min_lr: 0.000000  loss: 3.6721 (3.9464)  class_acc: 0.3750 (0.2732)  loss_scale: 65536.0000 (60271.5838)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0725  max mem: 15572
Epoch: [16]  [2200/2809]  eta: 0:05:46  lr: 0.000035  min_lr: 0.000000  loss: 4.1635 (3.9476)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (60295.5020)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1508  max mem: 15572
Epoch: [16]  [2210/2809]  eta: 0:05:40  lr: 0.000035  min_lr: 0.000000  loss: 4.0173 (3.9467)  class_acc: 0.2500 (0.2733)  loss_scale: 65536.0000 (60319.2040)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1280  max mem: 15572
Epoch: [16]  [2220/2809]  eta: 0:05:34  lr: 0.000035  min_lr: 0.000000  loss: 3.9693 (3.9474)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (60342.6925)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1064  max mem: 15572
Epoch: [16]  [2230/2809]  eta: 0:05:28  lr: 0.000035  min_lr: 0.000000  loss: 3.9348 (3.9463)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (60365.9704)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0899  max mem: 15572
Epoch: [16]  [2240/2809]  eta: 0:05:23  lr: 0.000035  min_lr: 0.000000  loss: 3.8894 (3.9471)  class_acc: 0.2500 (0.2729)  loss_scale: 65536.0000 (60389.0406)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.0906  max mem: 15572
[2025-01-15 22:25:58,089] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:25:58,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:25:58,475] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47189
[2025-01-15 22:25:58,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:25:58,476] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [2250/2809]  eta: 0:05:17  lr: 0.000035  min_lr: 0.000000  loss: 3.8290 (3.9446)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (60441.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0901  max mem: 15572
Epoch: [16]  [2260/2809]  eta: 0:05:11  lr: 0.000035  min_lr: 0.000000  loss: 3.6266 (3.9445)  class_acc: 0.3333 (0.2733)  loss_scale: 65536.0000 (60463.5542)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0886  max mem: 15572
Epoch: [16]  [2270/2809]  eta: 0:05:05  lr: 0.000035  min_lr: 0.000000  loss: 4.0091 (3.9448)  class_acc: 0.2500 (0.2732)  loss_scale: 65536.0000 (60485.8899)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1493  max mem: 15572
Epoch: [16]  [2280/2809]  eta: 0:05:00  lr: 0.000035  min_lr: 0.000000  loss: 4.1751 (3.9459)  class_acc: 0.2083 (0.2729)  loss_scale: 65536.0000 (60508.0298)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.0765  max mem: 15572
Epoch: [16]  [2290/2809]  eta: 0:04:54  lr: 0.000035  min_lr: 0.000000  loss: 4.1290 (3.9454)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (60529.9764)  weight_decay: 0.0500 (0.0500)  time: 0.5049  data: 0.0456  max mem: 15572
Epoch: [16]  [2300/2809]  eta: 0:04:48  lr: 0.000035  min_lr: 0.000000  loss: 3.9998 (3.9457)  class_acc: 0.2083 (0.2729)  loss_scale: 65536.0000 (60551.7323)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0606  max mem: 15572
Epoch: [16]  [2310/2809]  eta: 0:04:43  lr: 0.000035  min_lr: 0.000000  loss: 3.9949 (3.9450)  class_acc: 0.1667 (0.2729)  loss_scale: 65536.0000 (60573.2999)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1487  max mem: 15572
Epoch: [16]  [2320/2809]  eta: 0:04:37  lr: 0.000035  min_lr: 0.000000  loss: 4.0184 (3.9460)  class_acc: 0.2083 (0.2728)  loss_scale: 65536.0000 (60594.6816)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1484  max mem: 15572
[2025-01-15 22:26:41,928] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47268
[2025-01-15 22:26:41,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:26:41,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2330/2809]  eta: 0:04:31  lr: 0.000035  min_lr: 0.000000  loss: 4.1228 (3.9460)  class_acc: 0.2917 (0.2728)  loss_scale: 65536.0000 (60517.4775)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0647  max mem: 15572
Epoch: [16]  [2340/2809]  eta: 0:04:25  lr: 0.000035  min_lr: 0.000000  loss: 4.1015 (3.9459)  class_acc: 0.2917 (0.2730)  loss_scale: 32768.0000 (60398.9406)  weight_decay: 0.0500 (0.0500)  time: 0.5016  data: 0.0614  max mem: 15572
Epoch: [16]  [2350/2809]  eta: 0:04:19  lr: 0.000035  min_lr: 0.000000  loss: 4.0840 (3.9461)  class_acc: 0.2917 (0.2729)  loss_scale: 32768.0000 (60281.4122)  weight_decay: 0.0500 (0.0500)  time: 0.4918  data: 0.0429  max mem: 15572
Epoch: [16]  [2360/2809]  eta: 0:04:14  lr: 0.000035  min_lr: 0.000000  loss: 3.9879 (3.9454)  class_acc: 0.2500 (0.2730)  loss_scale: 32768.0000 (60164.8793)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0796  max mem: 15572
Epoch: [16]  [2370/2809]  eta: 0:04:08  lr: 0.000035  min_lr: 0.000000  loss: 4.0743 (3.9460)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (60049.3294)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0742  max mem: 15572
Epoch: [16]  [2380/2809]  eta: 0:04:02  lr: 0.000035  min_lr: 0.000000  loss: 4.0743 (3.9458)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (59934.7501)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0649  max mem: 15572
Epoch: [16]  [2390/2809]  eta: 0:03:57  lr: 0.000035  min_lr: 0.000000  loss: 4.0839 (3.9459)  class_acc: 0.2500 (0.2728)  loss_scale: 32768.0000 (59821.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1091  max mem: 15572
Epoch: [16]  [2400/2809]  eta: 0:03:51  lr: 0.000035  min_lr: 0.000000  loss: 4.1193 (3.9467)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (59708.4548)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1296  max mem: 15572
Epoch: [16]  [2410/2809]  eta: 0:03:45  lr: 0.000035  min_lr: 0.000000  loss: 4.0982 (3.9473)  class_acc: 0.2500 (0.2724)  loss_scale: 32768.0000 (59596.7151)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1071  max mem: 15572
Epoch: [16]  [2420/2809]  eta: 0:03:40  lr: 0.000035  min_lr: 0.000000  loss: 4.0811 (3.9474)  class_acc: 0.2500 (0.2725)  loss_scale: 32768.0000 (59485.8984)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1165  max mem: 15572
Epoch: [16]  [2430/2809]  eta: 0:03:34  lr: 0.000035  min_lr: 0.000000  loss: 3.8811 (3.9468)  class_acc: 0.2500 (0.2724)  loss_scale: 32768.0000 (59375.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2047  max mem: 15572
Epoch: [16]  [2440/2809]  eta: 0:03:28  lr: 0.000035  min_lr: 0.000000  loss: 3.8811 (3.9464)  class_acc: 0.2500 (0.2725)  loss_scale: 32768.0000 (59266.9889)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1544  max mem: 15572
Epoch: [16]  [2450/2809]  eta: 0:03:23  lr: 0.000035  min_lr: 0.000000  loss: 3.9371 (3.9465)  class_acc: 0.2083 (0.2724)  loss_scale: 32768.0000 (59158.8739)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1401  max mem: 15572
[2025-01-15 22:27:54,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:27:54,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2460/2809]  eta: 0:03:17  lr: 0.000035  min_lr: 0.000000  loss: 4.1578 (3.9468)  class_acc: 0.2083 (0.2724)  loss_scale: 32768.0000 (59158.1568)  weight_decay: 0.0500 (0.0500)  time: 0.6106  data: 0.1580  max mem: 15572
Epoch: [16]  [2470/2809]  eta: 0:03:12  lr: 0.000035  min_lr: 0.000000  loss: 4.0431 (3.9463)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (59183.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1335  max mem: 15572
Epoch: [16]  [2480/2809]  eta: 0:03:06  lr: 0.000035  min_lr: 0.000000  loss: 3.9338 (3.9465)  class_acc: 0.2500 (0.2724)  loss_scale: 65536.0000 (59209.5703)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1318  max mem: 15572
Epoch: [16]  [2490/2809]  eta: 0:03:00  lr: 0.000035  min_lr: 0.000000  loss: 4.1602 (3.9470)  class_acc: 0.2083 (0.2722)  loss_scale: 65536.0000 (59234.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1450  max mem: 15572
Epoch: [16]  [2500/2809]  eta: 0:02:55  lr: 0.000035  min_lr: 0.000000  loss: 4.1935 (3.9470)  class_acc: 0.2083 (0.2720)  loss_scale: 65536.0000 (59260.1615)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1413  max mem: 15572
Epoch: [16]  [2510/2809]  eta: 0:02:49  lr: 0.000035  min_lr: 0.000000  loss: 3.8164 (3.9466)  class_acc: 0.2500 (0.2721)  loss_scale: 65536.0000 (59285.1549)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0741  max mem: 15572
Epoch: [16]  [2520/2809]  eta: 0:02:43  lr: 0.000035  min_lr: 0.000000  loss: 3.7999 (3.9458)  class_acc: 0.2917 (0.2722)  loss_scale: 65536.0000 (59309.9500)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1282  max mem: 15572
Epoch: [16]  [2530/2809]  eta: 0:02:38  lr: 0.000035  min_lr: 0.000000  loss: 4.1026 (3.9466)  class_acc: 0.2500 (0.2720)  loss_scale: 65536.0000 (59334.5492)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1400  max mem: 15572
[2025-01-15 22:28:42,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47482
[2025-01-15 22:28:42,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:28:42,417] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [2540/2809]  eta: 0:02:32  lr: 0.000035  min_lr: 0.000000  loss: 4.1026 (3.9463)  class_acc: 0.2500 (0.2720)  loss_scale: 65536.0000 (59320.2676)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0948  max mem: 15572
Epoch: [16]  [2550/2809]  eta: 0:02:26  lr: 0.000035  min_lr: 0.000000  loss: 4.0750 (3.9466)  class_acc: 0.2917 (0.2720)  loss_scale: 32768.0000 (59216.1819)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1405  max mem: 15572
Epoch: [16]  [2560/2809]  eta: 0:02:21  lr: 0.000035  min_lr: 0.000000  loss: 4.0750 (3.9467)  class_acc: 0.2917 (0.2720)  loss_scale: 32768.0000 (59112.9090)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1182  max mem: 15572
Epoch: [16]  [2570/2809]  eta: 0:02:15  lr: 0.000035  min_lr: 0.000000  loss: 3.9952 (3.9468)  class_acc: 0.2500 (0.2718)  loss_scale: 32768.0000 (59010.4395)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1223  max mem: 15572
Epoch: [16]  [2580/2809]  eta: 0:02:09  lr: 0.000035  min_lr: 0.000000  loss: 4.0662 (3.9471)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (58908.7640)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1592  max mem: 15572
Epoch: [16]  [2590/2809]  eta: 0:02:04  lr: 0.000035  min_lr: 0.000000  loss: 3.9832 (3.9472)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (58807.8734)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1295  max mem: 15572
Epoch: [16]  [2600/2809]  eta: 0:01:58  lr: 0.000035  min_lr: 0.000000  loss: 3.9314 (3.9470)  class_acc: 0.2917 (0.2718)  loss_scale: 32768.0000 (58707.7586)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1263  max mem: 15572
Epoch: [16]  [2610/2809]  eta: 0:01:52  lr: 0.000035  min_lr: 0.000000  loss: 3.8202 (3.9466)  class_acc: 0.2917 (0.2719)  loss_scale: 32768.0000 (58608.4106)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.1699  max mem: 15572
Epoch: [16]  [2620/2809]  eta: 0:01:47  lr: 0.000035  min_lr: 0.000000  loss: 4.1133 (3.9472)  class_acc: 0.2083 (0.2717)  loss_scale: 32768.0000 (58509.8207)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.1872  max mem: 15572
Epoch: [16]  [2630/2809]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000000  loss: 4.1480 (3.9478)  class_acc: 0.2083 (0.2716)  loss_scale: 32768.0000 (58411.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0843  max mem: 15572
Epoch: [16]  [2640/2809]  eta: 0:01:35  lr: 0.000035  min_lr: 0.000000  loss: 4.0889 (3.9475)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (58314.8807)  weight_decay: 0.0500 (0.0500)  time: 0.5121  data: 0.0566  max mem: 15572
Epoch: [16]  [2650/2809]  eta: 0:01:30  lr: 0.000035  min_lr: 0.000000  loss: 3.9955 (3.9472)  class_acc: 0.2917 (0.2718)  loss_scale: 32768.0000 (58218.5138)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1135  max mem: 15572
Epoch: [16]  [2660/2809]  eta: 0:01:24  lr: 0.000035  min_lr: 0.000000  loss: 3.9964 (3.9478)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (58122.8711)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0933  max mem: 15572
[2025-01-15 22:29:58,213] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:29:58,213] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [2670/2809]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000000  loss: 4.0565 (3.9477)  class_acc: 0.2500 (0.2717)  loss_scale: 32768.0000 (58077.0168)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1296  max mem: 15572
Epoch: [16]  [2680/2809]  eta: 0:01:13  lr: 0.000035  min_lr: 0.000000  loss: 4.0131 (3.9482)  class_acc: 0.2083 (0.2715)  loss_scale: 65536.0000 (58104.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1557  max mem: 15572
Epoch: [16]  [2690/2809]  eta: 0:01:07  lr: 0.000035  min_lr: 0.000000  loss: 3.9932 (3.9477)  class_acc: 0.2500 (0.2716)  loss_scale: 65536.0000 (58132.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0879  max mem: 15572
Epoch: [16]  [2700/2809]  eta: 0:01:01  lr: 0.000035  min_lr: 0.000000  loss: 3.9932 (3.9483)  class_acc: 0.2917 (0.2715)  loss_scale: 65536.0000 (58159.8638)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1122  max mem: 15572
Epoch: [16]  [2710/2809]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000000  loss: 3.6905 (3.9472)  class_acc: 0.2500 (0.2718)  loss_scale: 65536.0000 (58187.0719)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1348  max mem: 15572
Epoch: [16]  [2720/2809]  eta: 0:00:50  lr: 0.000035  min_lr: 0.000000  loss: 3.8614 (3.9471)  class_acc: 0.2917 (0.2719)  loss_scale: 65536.0000 (58214.0801)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1053  max mem: 15572
Epoch: [16]  [2730/2809]  eta: 0:00:44  lr: 0.000035  min_lr: 0.000000  loss: 3.9232 (3.9469)  class_acc: 0.2917 (0.2720)  loss_scale: 65536.0000 (58240.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1164  max mem: 15572
Epoch: [16]  [2740/2809]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000000  loss: 3.9232 (3.9465)  class_acc: 0.2500 (0.2719)  loss_scale: 65536.0000 (58267.5053)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.1137  max mem: 15572
Epoch: [16]  [2750/2809]  eta: 0:00:33  lr: 0.000035  min_lr: 0.000000  loss: 4.2656 (3.9471)  class_acc: 0.2083 (0.2719)  loss_scale: 65536.0000 (58293.9266)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1594  max mem: 15572
Epoch: [16]  [2760/2809]  eta: 0:00:27  lr: 0.000035  min_lr: 0.000000  loss: 4.2656 (3.9478)  class_acc: 0.1667 (0.2717)  loss_scale: 65536.0000 (58320.1565)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1054  max mem: 15572
Epoch: [16]  [2770/2809]  eta: 0:00:22  lr: 0.000035  min_lr: 0.000000  loss: 4.2231 (3.9485)  class_acc: 0.1667 (0.2716)  loss_scale: 65536.0000 (58346.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.0944  max mem: 15572
Epoch: [16]  [2780/2809]  eta: 0:00:16  lr: 0.000035  min_lr: 0.000000  loss: 4.2762 (3.9491)  class_acc: 0.1667 (0.2714)  loss_scale: 65536.0000 (58372.0503)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.0944  max mem: 15572
Epoch: [16]  [2790/2809]  eta: 0:00:10  lr: 0.000035  min_lr: 0.000000  loss: 4.0104 (3.9483)  class_acc: 0.2917 (0.2717)  loss_scale: 65536.0000 (58397.7184)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.0994  max mem: 15572
[2025-01-15 22:31:11,188] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:31:11,188] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:31:13,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47744
[2025-01-15 22:31:13,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:31:13,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [2800/2809]  eta: 0:00:05  lr: 0.000035  min_lr: 0.000000  loss: 3.9349 (3.9484)  class_acc: 0.2917 (0.2716)  loss_scale: 65536.0000 (58540.1899)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0993  max mem: 15572
Epoch: [16]  [2808/2809]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000000  loss: 4.0135 (3.9489)  class_acc: 0.2500 (0.2715)  loss_scale: 65536.0000 (58560.1139)  weight_decay: 0.0500 (0.0500)  time: 0.4258  data: 0.0006  max mem: 15572
Epoch: [16] Total time: 0:26:33 (0.5674 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000000  loss: 4.0135 (3.9489)  class_acc: 0.2500 (0.2715)  loss_scale: 65536.0000 (58560.1139)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:28:12  loss: 0.3487 (0.3487)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.2221  data: 6.0218  max mem: 15572
Val:  [ 10/272]  eta: 0:03:35  loss: 2.9336 (2.6512)  acc1: 27.7778 (32.3232)  acc5: 66.6667 (62.1212)  time: 0.8227  data: 0.6390  max mem: 15572
Val:  [ 20/272]  eta: 0:02:17  loss: 2.8455 (2.6462)  acc1: 38.8889 (37.5661)  acc5: 66.6667 (66.1376)  time: 0.2609  data: 0.0734  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 2.7432 (2.7515)  acc1: 38.8889 (34.2294)  acc5: 66.6667 (65.4122)  time: 0.2247  data: 0.0236  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 2.7432 (2.7223)  acc1: 27.7778 (33.4688)  acc5: 72.2222 (67.4797)  time: 0.2654  data: 0.0518  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 2.5919 (2.6227)  acc1: 33.3333 (36.0566)  acc5: 72.2222 (70.1525)  time: 0.3206  data: 0.1015  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.6817 (2.5065)  acc1: 55.5556 (39.7086)  acc5: 83.3333 (71.3115)  time: 0.3157  data: 0.1055  max mem: 15572
Val:  [ 70/272]  eta: 0:01:18  loss: 1.6817 (2.4293)  acc1: 61.1111 (42.3318)  acc5: 83.3333 (72.7700)  time: 0.3732  data: 0.1611  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 2.1709 (2.4368)  acc1: 50.0000 (42.5240)  acc5: 77.7778 (72.4966)  time: 0.3573  data: 0.1471  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.7164 (2.4798)  acc1: 38.8889 (41.6361)  acc5: 72.2222 (72.4664)  time: 0.2531  data: 0.0570  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 2.7122 (2.5165)  acc1: 38.8889 (41.3641)  acc5: 72.2222 (72.0572)  time: 0.3503  data: 0.1448  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 2.8963 (2.5996)  acc1: 11.1111 (38.9890)  acc5: 61.1111 (70.5205)  time: 0.3906  data: 0.1733  max mem: 15572
Val:  [120/272]  eta: 0:00:54  loss: 3.3250 (2.6488)  acc1: 11.1111 (37.7870)  acc5: 50.0000 (69.6051)  time: 0.2895  data: 0.0789  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.6725 (2.6089)  acc1: 33.3333 (39.1009)  acc5: 72.2222 (70.1442)  time: 0.2844  data: 0.0867  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.8684 (2.5997)  acc1: 50.0000 (39.5981)  acc5: 77.7778 (70.1734)  time: 0.3055  data: 0.0990  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.6584 (2.6012)  acc1: 33.3333 (38.9993)  acc5: 72.2222 (70.6402)  time: 0.3412  data: 0.1339  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.6132 (2.5847)  acc1: 38.8889 (39.9586)  acc5: 72.2222 (71.1180)  time: 0.3088  data: 0.1134  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6922 (2.6107)  acc1: 38.8889 (39.0838)  acc5: 72.2222 (70.5653)  time: 0.2821  data: 0.0867  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.7260 (2.6035)  acc1: 22.2222 (38.6433)  acc5: 66.6667 (70.9638)  time: 0.3102  data: 0.1204  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.7181 (2.6467)  acc1: 22.2222 (37.6091)  acc5: 66.6667 (69.5462)  time: 0.2544  data: 0.0725  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.7181 (2.6479)  acc1: 27.7778 (37.7833)  acc5: 66.6667 (69.6241)  time: 0.1844  data: 0.0126  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.2074 (2.6420)  acc1: 44.4444 (38.3360)  acc5: 77.7778 (69.6419)  time: 0.1679  data: 0.0005  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.3656 (2.6360)  acc1: 50.0000 (38.5872)  acc5: 72.2222 (69.6833)  time: 0.1862  data: 0.0006  max mem: 15572
Val:  [230/272]  eta: 0:00:12  loss: 2.0629 (2.6013)  acc1: 55.5556 (39.7547)  acc5: 77.7778 (70.2742)  time: 0.2149  data: 0.0008  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 1.9113 (2.5819)  acc1: 55.5556 (39.9954)  acc5: 83.3333 (70.7699)  time: 0.2953  data: 0.0854  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.4574 (2.5935)  acc1: 27.7778 (39.3094)  acc5: 77.7778 (70.6286)  time: 0.3666  data: 0.1731  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.3698 (2.5277)  acc1: 66.6667 (41.2729)  acc5: 88.8889 (71.5411)  time: 0.3083  data: 0.1150  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4594 (2.5262)  acc1: 66.6667 (41.0004)  acc5: 88.8889 (71.5867)  time: 0.2537  data: 0.0713  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4594 (2.5306)  acc1: 55.5556 (40.9789)  acc5: 88.8889 (71.5544)  time: 0.2469  data: 0.0712  max mem: 15572
Val: Total time: 0:01:24 (0.3094 s / it)
* Acc@1 40.979 Acc@5 71.554 loss 2.531
Accuracy of the network on the 4883 val videos: 41.0%
[2025-01-15 22:32:40,835] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 22:32:40,838] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 22:32:40,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 22:32:44,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 22:32:44,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.98%
Epoch: [17]  [   0/2809]  eta: 8:34:13  lr: 0.000035  min_lr: 0.000000  loss: 3.9224 (3.9224)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.9839  data: 10.5364  max mem: 15572
Epoch: [17]  [  10/2809]  eta: 1:19:05  lr: 0.000035  min_lr: 0.000000  loss: 3.9984 (3.9754)  class_acc: 0.2500 (0.2614)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6955  data: 1.2312  max mem: 15572
Epoch: [17]  [  20/2809]  eta: 0:53:19  lr: 0.000035  min_lr: 0.000000  loss: 3.9208 (3.9481)  class_acc: 0.2083 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.1853  max mem: 15572
Epoch: [17]  [  30/2809]  eta: 0:47:26  lr: 0.000035  min_lr: 0.000000  loss: 3.9138 (3.9389)  class_acc: 0.2083 (0.2527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.1987  max mem: 15572
Epoch: [17]  [  40/2809]  eta: 0:42:51  lr: 0.000035  min_lr: 0.000000  loss: 4.0411 (3.9682)  class_acc: 0.2083 (0.2510)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.2449  max mem: 15572
Epoch: [17]  [  50/2809]  eta: 0:40:13  lr: 0.000035  min_lr: 0.000000  loss: 3.9651 (3.9531)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1808  max mem: 15572
Epoch: [17]  [  60/2809]  eta: 0:38:28  lr: 0.000035  min_lr: 0.000000  loss: 3.9495 (3.9502)  class_acc: 0.2917 (0.2589)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2047  max mem: 15572
Epoch: [17]  [  70/2809]  eta: 0:37:17  lr: 0.000035  min_lr: 0.000000  loss: 3.9969 (3.9511)  class_acc: 0.2500 (0.2559)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.2046  max mem: 15572
Epoch: [17]  [  80/2809]  eta: 0:35:17  lr: 0.000035  min_lr: 0.000000  loss: 4.0352 (3.9455)  class_acc: 0.2083 (0.2557)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1538  max mem: 15572
Epoch: [17]  [  90/2809]  eta: 0:33:15  lr: 0.000034  min_lr: 0.000000  loss: 3.9832 (3.9275)  class_acc: 0.2500 (0.2596)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4393  data: 0.0546  max mem: 15572
Epoch: [17]  [ 100/2809]  eta: 0:31:54  lr: 0.000034  min_lr: 0.000000  loss: 3.9832 (3.9396)  class_acc: 0.2500 (0.2628)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4259  data: 0.0010  max mem: 15572
Epoch: [17]  [ 110/2809]  eta: 0:30:43  lr: 0.000034  min_lr: 0.000000  loss: 4.0632 (3.9541)  class_acc: 0.2500 (0.2598)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4523  data: 0.0010  max mem: 15572
[2025-01-15 22:34:04,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:34:04,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 120/2809]  eta: 0:29:48  lr: 0.000034  min_lr: 0.000000  loss: 4.1069 (3.9556)  class_acc: 0.1667 (0.2610)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.4553  data: 0.0006  max mem: 15572
[2025-01-15 22:34:10,355] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47881
[2025-01-15 22:34:10,355] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:34:10,355] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 130/2809]  eta: 0:29:33  lr: 0.000034  min_lr: 0.000000  loss: 4.0707 (3.9546)  class_acc: 0.1667 (0.2573)  loss_scale: 65536.0000 (69538.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0981  max mem: 15572
Epoch: [17]  [ 140/2809]  eta: 0:28:58  lr: 0.000034  min_lr: 0.000000  loss: 3.9417 (3.9489)  class_acc: 0.2500 (0.2586)  loss_scale: 65536.0000 (69254.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1419  max mem: 15572
Epoch: [17]  [ 150/2809]  eta: 0:28:42  lr: 0.000034  min_lr: 0.000000  loss: 3.9462 (3.9538)  class_acc: 0.2917 (0.2586)  loss_scale: 65536.0000 (69008.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1238  max mem: 15572
Epoch: [17]  [ 160/2809]  eta: 0:28:33  lr: 0.000034  min_lr: 0.000000  loss: 4.0338 (3.9537)  class_acc: 0.2917 (0.2583)  loss_scale: 65536.0000 (68792.4472)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1794  max mem: 15572
Epoch: [17]  [ 170/2809]  eta: 0:28:30  lr: 0.000034  min_lr: 0.000000  loss: 3.9430 (3.9413)  class_acc: 0.2500 (0.2600)  loss_scale: 65536.0000 (68602.0117)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.2093  max mem: 15572
Epoch: [17]  [ 180/2809]  eta: 0:28:12  lr: 0.000034  min_lr: 0.000000  loss: 3.9001 (3.9426)  class_acc: 0.2083 (0.2610)  loss_scale: 65536.0000 (68432.6188)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1754  max mem: 15572
Epoch: [17]  [ 190/2809]  eta: 0:27:49  lr: 0.000034  min_lr: 0.000000  loss: 4.1262 (3.9512)  class_acc: 0.2500 (0.2613)  loss_scale: 65536.0000 (68280.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1076  max mem: 15572
Epoch: [17]  [ 200/2809]  eta: 0:27:29  lr: 0.000034  min_lr: 0.000000  loss: 4.0093 (3.9433)  class_acc: 0.2917 (0.2633)  loss_scale: 65536.0000 (68144.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0883  max mem: 15572
Epoch: [17]  [ 210/2809]  eta: 0:27:22  lr: 0.000034  min_lr: 0.000000  loss: 3.8335 (3.9426)  class_acc: 0.2917 (0.2646)  loss_scale: 65536.0000 (68020.7773)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1323  max mem: 15572
Epoch: [17]  [ 220/2809]  eta: 0:26:57  lr: 0.000034  min_lr: 0.000000  loss: 3.8381 (3.9338)  class_acc: 0.2917 (0.2660)  loss_scale: 65536.0000 (67908.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0992  max mem: 15572
Epoch: [17]  [ 230/2809]  eta: 0:26:46  lr: 0.000034  min_lr: 0.000000  loss: 3.9136 (3.9346)  class_acc: 0.2500 (0.2655)  loss_scale: 65536.0000 (67805.6450)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0937  max mem: 15572
Epoch: [17]  [ 240/2809]  eta: 0:26:40  lr: 0.000034  min_lr: 0.000000  loss: 4.0081 (3.9456)  class_acc: 0.2500 (0.2640)  loss_scale: 65536.0000 (67711.4689)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1557  max mem: 15572
[2025-01-15 22:35:18,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=321, lr=[3.332372618498235e-07, 3.332372618498235e-07, 4.7605323121403363e-07, 4.7605323121403363e-07, 6.800760445914767e-07, 6.800760445914767e-07, 9.715372065592525e-07, 9.715372065592525e-07, 1.3879102950846464e-06, 1.3879102950846464e-06, 1.9827289929780666e-06, 1.9827289929780666e-06, 2.8324699899686663e-06, 2.8324699899686663e-06, 4.046385699955239e-06, 4.046385699955239e-06, 5.780550999936054e-06, 5.780550999936054e-06, 8.25792999990865e-06, 8.25792999990865e-06, 1.1797042857012358e-05, 1.1797042857012358e-05, 1.6852918367160514e-05, 1.6852918367160514e-05, 2.4075597667372162e-05, 2.4075597667372162e-05, 3.4393710953388806e-05, 3.4393710953388806e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:35:18,179] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=28.468397689604608, CurrSamplesPerSec=30.664973740967746, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [ 250/2809]  eta: 0:26:22  lr: 0.000034  min_lr: 0.000000  loss: 4.0296 (3.9449)  class_acc: 0.2083 (0.2644)  loss_scale: 65536.0000 (67624.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1202  max mem: 15572
[2025-01-15 22:35:24,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:35:24,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 260/2809]  eta: 0:26:09  lr: 0.000034  min_lr: 0.000000  loss: 4.1467 (3.9524)  class_acc: 0.2500 (0.2639)  loss_scale: 65536.0000 (68549.1494)  weight_decay: 0.0500 (0.0500)  time: 0.5280  data: 0.0934  max mem: 15572
[2025-01-15 22:35:26,925] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48015
[2025-01-15 22:35:26,925] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:35:26,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 270/2809]  eta: 0:25:54  lr: 0.000034  min_lr: 0.000000  loss: 3.9798 (3.9529)  class_acc: 0.2917 (0.2640)  loss_scale: 65536.0000 (68679.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0886  max mem: 15572
Epoch: [17]  [ 280/2809]  eta: 0:25:43  lr: 0.000034  min_lr: 0.000000  loss: 3.9679 (3.9530)  class_acc: 0.2500 (0.2648)  loss_scale: 65536.0000 (68567.9146)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1071  max mem: 15572
Epoch: [17]  [ 290/2809]  eta: 0:25:28  lr: 0.000034  min_lr: 0.000000  loss: 3.8875 (3.9473)  class_acc: 0.2917 (0.2665)  loss_scale: 65536.0000 (68463.7251)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.1021  max mem: 15572
Epoch: [17]  [ 300/2809]  eta: 0:25:20  lr: 0.000034  min_lr: 0.000000  loss: 3.9738 (3.9567)  class_acc: 0.2500 (0.2638)  loss_scale: 65536.0000 (68366.4585)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1112  max mem: 15572
Epoch: [17]  [ 310/2809]  eta: 0:25:09  lr: 0.000034  min_lr: 0.000000  loss: 4.1395 (3.9582)  class_acc: 0.2500 (0.2634)  loss_scale: 65536.0000 (68275.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1416  max mem: 15572
Epoch: [17]  [ 320/2809]  eta: 0:24:59  lr: 0.000034  min_lr: 0.000000  loss: 4.0241 (3.9536)  class_acc: 0.2500 (0.2649)  loss_scale: 65536.0000 (68190.1059)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1190  max mem: 15572
Epoch: [17]  [ 330/2809]  eta: 0:24:51  lr: 0.000034  min_lr: 0.000000  loss: 3.8134 (3.9477)  class_acc: 0.3333 (0.2671)  loss_scale: 65536.0000 (68109.9215)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1231  max mem: 15572
Epoch: [17]  [ 340/2809]  eta: 0:24:42  lr: 0.000034  min_lr: 0.000000  loss: 3.9106 (3.9440)  class_acc: 0.2917 (0.2676)  loss_scale: 65536.0000 (68034.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1326  max mem: 15572
Epoch: [17]  [ 350/2809]  eta: 0:24:39  lr: 0.000034  min_lr: 0.000000  loss: 3.9862 (3.9442)  class_acc: 0.2500 (0.2677)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1741  max mem: 15572
Epoch: [17]  [ 360/2809]  eta: 0:24:29  lr: 0.000034  min_lr: 0.000000  loss: 4.1293 (3.9479)  class_acc: 0.2917 (0.2677)  loss_scale: 65536.0000 (67896.0222)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1600  max mem: 15572
Epoch: [17]  [ 370/2809]  eta: 0:24:19  lr: 0.000034  min_lr: 0.000000  loss: 4.0311 (3.9474)  class_acc: 0.2917 (0.2691)  loss_scale: 65536.0000 (67832.4097)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1061  max mem: 15572
Epoch: [17]  [ 380/2809]  eta: 0:24:15  lr: 0.000034  min_lr: 0.000000  loss: 4.0146 (3.9486)  class_acc: 0.2500 (0.2683)  loss_scale: 65536.0000 (67772.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1541  max mem: 15572
Epoch: [17]  [ 390/2809]  eta: 0:24:14  lr: 0.000034  min_lr: 0.000000  loss: 4.0306 (3.9508)  class_acc: 0.2083 (0.2682)  loss_scale: 65536.0000 (67714.9463)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.1998  max mem: 15572
[2025-01-15 22:36:40,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:36:40,122] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:36:41,629] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48146
[2025-01-15 22:36:41,629] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:36:41,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 400/2809]  eta: 0:24:03  lr: 0.000034  min_lr: 0.000000  loss: 4.0306 (3.9478)  class_acc: 0.2500 (0.2676)  loss_scale: 65536.0000 (67987.4713)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1337  max mem: 15572
Epoch: [17]  [ 410/2809]  eta: 0:24:01  lr: 0.000034  min_lr: 0.000000  loss: 3.9467 (3.9474)  class_acc: 0.2083 (0.2673)  loss_scale: 65536.0000 (67927.8248)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1545  max mem: 15572
Epoch: [17]  [ 420/2809]  eta: 0:23:46  lr: 0.000034  min_lr: 0.000000  loss: 3.8377 (3.9442)  class_acc: 0.2083 (0.2671)  loss_scale: 65536.0000 (67871.0119)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1237  max mem: 15572
Epoch: [17]  [ 430/2809]  eta: 0:23:38  lr: 0.000034  min_lr: 0.000000  loss: 3.7293 (3.9395)  class_acc: 0.2917 (0.2691)  loss_scale: 65536.0000 (67816.8353)  weight_decay: 0.0500 (0.0500)  time: 0.5025  data: 0.0587  max mem: 15572
Epoch: [17]  [ 440/2809]  eta: 0:23:31  lr: 0.000034  min_lr: 0.000000  loss: 3.9535 (3.9409)  class_acc: 0.2917 (0.2697)  loss_scale: 65536.0000 (67765.1156)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1377  max mem: 15572
Epoch: [17]  [ 450/2809]  eta: 0:23:22  lr: 0.000034  min_lr: 0.000000  loss: 3.8843 (3.9334)  class_acc: 0.2917 (0.2717)  loss_scale: 65536.0000 (67715.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1112  max mem: 15572
Epoch: [17]  [ 460/2809]  eta: 0:23:12  lr: 0.000034  min_lr: 0.000000  loss: 3.6267 (3.9285)  class_acc: 0.2917 (0.2727)  loss_scale: 65536.0000 (67668.4078)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0603  max mem: 15572
Epoch: [17]  [ 470/2809]  eta: 0:23:04  lr: 0.000034  min_lr: 0.000000  loss: 3.8939 (3.9291)  class_acc: 0.2917 (0.2732)  loss_scale: 65536.0000 (67623.1338)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0912  max mem: 15572
Epoch: [17]  [ 480/2809]  eta: 0:22:56  lr: 0.000034  min_lr: 0.000000  loss: 3.9988 (3.9317)  class_acc: 0.2917 (0.2732)  loss_scale: 65536.0000 (67579.7422)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1035  max mem: 15572
Epoch: [17]  [ 490/2809]  eta: 0:22:51  lr: 0.000034  min_lr: 0.000000  loss: 3.9710 (3.9336)  class_acc: 0.2917 (0.2731)  loss_scale: 65536.0000 (67538.1181)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1150  max mem: 15572
Epoch: [17]  [ 500/2809]  eta: 0:22:41  lr: 0.000034  min_lr: 0.000000  loss: 4.1757 (3.9388)  class_acc: 0.2083 (0.2716)  loss_scale: 65536.0000 (67498.1557)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0968  max mem: 15572
Epoch: [17]  [ 510/2809]  eta: 0:22:33  lr: 0.000034  min_lr: 0.000000  loss: 4.1771 (3.9346)  class_acc: 0.2083 (0.2728)  loss_scale: 65536.0000 (67459.7573)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0612  max mem: 15572
Epoch: [17]  [ 520/2809]  eta: 0:22:27  lr: 0.000034  min_lr: 0.000000  loss: 4.1402 (3.9381)  class_acc: 0.2500 (0.2713)  loss_scale: 65536.0000 (67422.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1016  max mem: 15572
[2025-01-15 22:37:53,414] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:37:53,415] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:37:53,842] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48276
[2025-01-15 22:37:53,843] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:37:53,843] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 530/2809]  eta: 0:22:20  lr: 0.000034  min_lr: 0.000000  loss: 4.2033 (3.9398)  class_acc: 0.2083 (0.2713)  loss_scale: 65536.0000 (67510.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1252  max mem: 15572
Epoch: [17]  [ 540/2809]  eta: 0:22:13  lr: 0.000034  min_lr: 0.000000  loss: 4.1844 (3.9437)  class_acc: 0.2500 (0.2702)  loss_scale: 65536.0000 (67474.2181)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1259  max mem: 15572
Epoch: [17]  [ 550/2809]  eta: 0:22:06  lr: 0.000034  min_lr: 0.000000  loss: 3.8522 (3.9385)  class_acc: 0.2500 (0.2710)  loss_scale: 65536.0000 (67439.0417)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1284  max mem: 15572
Epoch: [17]  [ 560/2809]  eta: 0:21:59  lr: 0.000034  min_lr: 0.000000  loss: 3.7321 (3.9372)  class_acc: 0.3333 (0.2725)  loss_scale: 65536.0000 (67405.1194)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1153  max mem: 15572
[2025-01-15 22:38:15,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48315
[2025-01-15 22:38:15,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 22:38:15,672] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 570/2809]  eta: 0:21:51  lr: 0.000034  min_lr: 0.000000  loss: 3.7596 (3.9322)  class_acc: 0.3333 (0.2734)  loss_scale: 65536.0000 (66855.9019)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0978  max mem: 15572
Epoch: [17]  [ 580/2809]  eta: 0:21:43  lr: 0.000034  min_lr: 0.000000  loss: 3.8582 (3.9346)  class_acc: 0.2500 (0.2729)  loss_scale: 32768.0000 (66269.1910)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0900  max mem: 15572
Epoch: [17]  [ 590/2809]  eta: 0:21:39  lr: 0.000034  min_lr: 0.000000  loss: 4.1217 (3.9376)  class_acc: 0.2083 (0.2726)  loss_scale: 32768.0000 (65702.3350)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1467  max mem: 15572
Epoch: [17]  [ 600/2809]  eta: 0:21:34  lr: 0.000034  min_lr: 0.000000  loss: 4.0250 (3.9366)  class_acc: 0.2083 (0.2720)  loss_scale: 32768.0000 (65154.3428)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1806  max mem: 15572
Epoch: [17]  [ 610/2809]  eta: 0:21:30  lr: 0.000034  min_lr: 0.000000  loss: 3.9108 (3.9343)  class_acc: 0.2500 (0.2724)  loss_scale: 32768.0000 (64624.2881)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1672  max mem: 15572
Epoch: [17]  [ 620/2809]  eta: 0:21:23  lr: 0.000034  min_lr: 0.000000  loss: 3.9108 (3.9348)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (64111.3043)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1520  max mem: 15572
Epoch: [17]  [ 630/2809]  eta: 0:21:15  lr: 0.000034  min_lr: 0.000000  loss: 3.8794 (3.9339)  class_acc: 0.2500 (0.2731)  loss_scale: 32768.0000 (63614.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1042  max mem: 15572
Epoch: [17]  [ 640/2809]  eta: 0:21:09  lr: 0.000034  min_lr: 0.000000  loss: 3.9646 (3.9342)  class_acc: 0.2500 (0.2730)  loss_scale: 32768.0000 (63133.3541)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.1149  max mem: 15572
Epoch: [17]  [ 650/2809]  eta: 0:21:01  lr: 0.000034  min_lr: 0.000000  loss: 4.0018 (3.9371)  class_acc: 0.2500 (0.2724)  loss_scale: 32768.0000 (62666.9124)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1160  max mem: 15572
Epoch: [17]  [ 660/2809]  eta: 0:20:54  lr: 0.000034  min_lr: 0.000000  loss: 4.0014 (3.9370)  class_acc: 0.2500 (0.2726)  loss_scale: 32768.0000 (62214.5840)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0981  max mem: 15572
Epoch: [17]  [ 670/2809]  eta: 0:20:48  lr: 0.000034  min_lr: 0.000000  loss: 4.0844 (3.9389)  class_acc: 0.2500 (0.2724)  loss_scale: 32768.0000 (61775.7377)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1245  max mem: 15572
Epoch: [17]  [ 680/2809]  eta: 0:20:43  lr: 0.000034  min_lr: 0.000000  loss: 4.0951 (3.9388)  class_acc: 0.2500 (0.2725)  loss_scale: 32768.0000 (61349.7797)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1507  max mem: 15572
Epoch: [17]  [ 690/2809]  eta: 0:20:38  lr: 0.000034  min_lr: 0.000000  loss: 4.2188 (3.9438)  class_acc: 0.2083 (0.2716)  loss_scale: 32768.0000 (60936.1505)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1759  max mem: 15572
[2025-01-15 22:39:29,147] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:39:29,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [ 700/2809]  eta: 0:20:32  lr: 0.000034  min_lr: 0.000000  loss: 4.1712 (3.9456)  class_acc: 0.2083 (0.2712)  loss_scale: 32768.0000 (61001.7689)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1633  max mem: 15572
Epoch: [17]  [ 710/2809]  eta: 0:20:25  lr: 0.000034  min_lr: 0.000000  loss: 4.0140 (3.9463)  class_acc: 0.2500 (0.2714)  loss_scale: 65536.0000 (61065.5415)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1263  max mem: 15572
Epoch: [17]  [ 720/2809]  eta: 0:20:21  lr: 0.000034  min_lr: 0.000000  loss: 4.0634 (3.9477)  class_acc: 0.2500 (0.2712)  loss_scale: 65536.0000 (61127.5451)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1774  max mem: 15572
Epoch: [17]  [ 730/2809]  eta: 0:20:11  lr: 0.000034  min_lr: 0.000000  loss: 4.1164 (3.9470)  class_acc: 0.2500 (0.2713)  loss_scale: 65536.0000 (61187.8523)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1193  max mem: 15572
Epoch: [17]  [ 740/2809]  eta: 0:20:05  lr: 0.000034  min_lr: 0.000000  loss: 4.0135 (3.9476)  class_acc: 0.2917 (0.2714)  loss_scale: 65536.0000 (61246.5317)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0695  max mem: 15572
Epoch: [17]  [ 750/2809]  eta: 0:19:57  lr: 0.000034  min_lr: 0.000000  loss: 3.8423 (3.9493)  class_acc: 0.2500 (0.2711)  loss_scale: 65536.0000 (61303.6485)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0988  max mem: 15572
Epoch: [17]  [ 760/2809]  eta: 0:19:52  lr: 0.000034  min_lr: 0.000000  loss: 3.9273 (3.9481)  class_acc: 0.2500 (0.2716)  loss_scale: 65536.0000 (61359.2641)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1138  max mem: 15572
Epoch: [17]  [ 770/2809]  eta: 0:19:46  lr: 0.000034  min_lr: 0.000000  loss: 3.9011 (3.9469)  class_acc: 0.2917 (0.2715)  loss_scale: 65536.0000 (61413.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1521  max mem: 15572
Epoch: [17]  [ 780/2809]  eta: 0:19:39  lr: 0.000034  min_lr: 0.000000  loss: 3.6763 (3.9455)  class_acc: 0.2917 (0.2716)  loss_scale: 65536.0000 (61466.2228)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1309  max mem: 15572
Epoch: [17]  [ 790/2809]  eta: 0:19:33  lr: 0.000034  min_lr: 0.000000  loss: 3.7627 (3.9459)  class_acc: 0.2083 (0.2710)  loss_scale: 65536.0000 (61517.6738)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1385  max mem: 15572
Epoch: [17]  [ 800/2809]  eta: 0:19:28  lr: 0.000034  min_lr: 0.000000  loss: 3.8537 (3.9435)  class_acc: 0.2500 (0.2714)  loss_scale: 65536.0000 (61567.8402)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1530  max mem: 15572
Epoch: [17]  [ 810/2809]  eta: 0:19:22  lr: 0.000034  min_lr: 0.000000  loss: 3.8807 (3.9445)  class_acc: 0.2500 (0.2711)  loss_scale: 65536.0000 (61616.7694)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1413  max mem: 15572
[2025-01-15 22:40:41,297] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:40:41,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:40:41,844] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48573
[2025-01-15 22:40:41,845] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:40:41,845] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 820/2809]  eta: 0:19:15  lr: 0.000034  min_lr: 0.000000  loss: 3.8488 (3.9425)  class_acc: 0.2917 (0.2716)  loss_scale: 65536.0000 (61744.3313)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1181  max mem: 15572
Epoch: [17]  [ 830/2809]  eta: 0:19:09  lr: 0.000034  min_lr: 0.000000  loss: 3.9325 (3.9434)  class_acc: 0.2500 (0.2713)  loss_scale: 65536.0000 (61789.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1065  max mem: 15572
Epoch: [17]  [ 840/2809]  eta: 0:19:04  lr: 0.000034  min_lr: 0.000000  loss: 3.9871 (3.9430)  class_acc: 0.2500 (0.2717)  loss_scale: 65536.0000 (61834.5018)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1285  max mem: 15572
Epoch: [17]  [ 850/2809]  eta: 0:18:55  lr: 0.000034  min_lr: 0.000000  loss: 3.9314 (3.9432)  class_acc: 0.2917 (0.2714)  loss_scale: 65536.0000 (61877.9976)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0804  max mem: 15572
Epoch: [17]  [ 860/2809]  eta: 0:18:48  lr: 0.000034  min_lr: 0.000000  loss: 3.9087 (3.9427)  class_acc: 0.2500 (0.2716)  loss_scale: 65536.0000 (61920.4832)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0612  max mem: 15572
Epoch: [17]  [ 870/2809]  eta: 0:18:42  lr: 0.000034  min_lr: 0.000000  loss: 4.1802 (3.9436)  class_acc: 0.2500 (0.2714)  loss_scale: 65536.0000 (61961.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1082  max mem: 15572
Epoch: [17]  [ 880/2809]  eta: 0:18:37  lr: 0.000034  min_lr: 0.000000  loss: 4.0386 (3.9453)  class_acc: 0.2500 (0.2711)  loss_scale: 65536.0000 (62002.5607)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1421  max mem: 15572
Epoch: [17]  [ 890/2809]  eta: 0:18:30  lr: 0.000034  min_lr: 0.000000  loss: 3.9742 (3.9432)  class_acc: 0.2500 (0.2709)  loss_scale: 65536.0000 (62042.2177)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1469  max mem: 15572
Epoch: [17]  [ 900/2809]  eta: 0:18:25  lr: 0.000034  min_lr: 0.000000  loss: 3.8307 (3.9450)  class_acc: 0.2917 (0.2708)  loss_scale: 65536.0000 (62080.9945)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1368  max mem: 15572
Epoch: [17]  [ 910/2809]  eta: 0:18:18  lr: 0.000034  min_lr: 0.000000  loss: 3.9653 (3.9427)  class_acc: 0.2917 (0.2711)  loss_scale: 65536.0000 (62118.9199)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1118  max mem: 15572
Epoch: [17]  [ 920/2809]  eta: 0:18:11  lr: 0.000034  min_lr: 0.000000  loss: 3.8615 (3.9413)  class_acc: 0.2917 (0.2716)  loss_scale: 65536.0000 (62156.0217)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0824  max mem: 15572
Epoch: [17]  [ 930/2809]  eta: 0:18:07  lr: 0.000034  min_lr: 0.000000  loss: 3.9262 (3.9411)  class_acc: 0.2917 (0.2717)  loss_scale: 65536.0000 (62192.3265)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1656  max mem: 15572
Epoch: [17]  [ 940/2809]  eta: 0:18:00  lr: 0.000034  min_lr: 0.000000  loss: 3.9262 (3.9396)  class_acc: 0.2917 (0.2721)  loss_scale: 65536.0000 (62227.8597)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1508  max mem: 15572
[2025-01-15 22:41:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:41:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 950/2809]  eta: 0:17:56  lr: 0.000034  min_lr: 0.000000  loss: 3.9414 (3.9404)  class_acc: 0.2500 (0.2717)  loss_scale: 65536.0000 (62400.4711)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1635  max mem: 15572
[2025-01-15 22:41:57,405] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48705
[2025-01-15 22:41:57,405] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:41:57,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 960/2809]  eta: 0:17:50  lr: 0.000034  min_lr: 0.000000  loss: 3.9082 (3.9379)  class_acc: 0.2500 (0.2726)  loss_scale: 65536.0000 (62501.2945)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1727  max mem: 15572
Epoch: [17]  [ 970/2809]  eta: 0:17:45  lr: 0.000034  min_lr: 0.000000  loss: 3.6805 (3.9357)  class_acc: 0.3750 (0.2735)  loss_scale: 65536.0000 (62532.5479)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1333  max mem: 15572
Epoch: [17]  [ 980/2809]  eta: 0:17:37  lr: 0.000034  min_lr: 0.000000  loss: 3.8356 (3.9358)  class_acc: 0.3333 (0.2737)  loss_scale: 65536.0000 (62563.1641)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0928  max mem: 15572
Epoch: [17]  [ 990/2809]  eta: 0:17:30  lr: 0.000034  min_lr: 0.000000  loss: 3.8799 (3.9352)  class_acc: 0.2500 (0.2740)  loss_scale: 65536.0000 (62593.1625)  weight_decay: 0.0500 (0.0500)  time: 0.4796  data: 0.0521  max mem: 15572
Epoch: [17]  [1000/2809]  eta: 0:17:24  lr: 0.000034  min_lr: 0.000000  loss: 4.0470 (3.9380)  class_acc: 0.2500 (0.2731)  loss_scale: 65536.0000 (62622.5614)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1144  max mem: 15572
Epoch: [17]  [1010/2809]  eta: 0:17:18  lr: 0.000034  min_lr: 0.000000  loss: 4.0600 (3.9378)  class_acc: 0.2083 (0.2732)  loss_scale: 65536.0000 (62651.3788)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1546  max mem: 15572
Epoch: [17]  [1020/2809]  eta: 0:17:14  lr: 0.000034  min_lr: 0.000000  loss: 3.9573 (3.9342)  class_acc: 0.2917 (0.2743)  loss_scale: 65536.0000 (62679.6317)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1802  max mem: 15572
Epoch: [17]  [1030/2809]  eta: 0:17:09  lr: 0.000034  min_lr: 0.000000  loss: 3.8317 (3.9344)  class_acc: 0.2917 (0.2745)  loss_scale: 65536.0000 (62707.3366)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.1717  max mem: 15572
Epoch: [17]  [1040/2809]  eta: 0:17:01  lr: 0.000034  min_lr: 0.000000  loss: 3.8430 (3.9325)  class_acc: 0.2917 (0.2751)  loss_scale: 65536.0000 (62734.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0844  max mem: 15572
Epoch: [17]  [1050/2809]  eta: 0:16:54  lr: 0.000034  min_lr: 0.000000  loss: 3.9073 (3.9346)  class_acc: 0.2500 (0.2747)  loss_scale: 65536.0000 (62761.1646)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0468  max mem: 15572
Epoch: [17]  [1060/2809]  eta: 0:16:48  lr: 0.000034  min_lr: 0.000000  loss: 4.0821 (3.9357)  class_acc: 0.2500 (0.2743)  loss_scale: 65536.0000 (62787.3176)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.1041  max mem: 15572
Epoch: [17]  [1070/2809]  eta: 0:16:41  lr: 0.000034  min_lr: 0.000000  loss: 3.8000 (3.9331)  class_acc: 0.2500 (0.2747)  loss_scale: 65536.0000 (62812.9823)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1097  max mem: 15572
Epoch: [17]  [1080/2809]  eta: 0:16:37  lr: 0.000034  min_lr: 0.000000  loss: 3.6819 (3.9312)  class_acc: 0.2917 (0.2749)  loss_scale: 65536.0000 (62838.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1554  max mem: 15572
[2025-01-15 22:43:10,064] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:43:10,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:43:11,711] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48837
[2025-01-15 22:43:11,712] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:43:11,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1090/2809]  eta: 0:16:31  lr: 0.000034  min_lr: 0.000000  loss: 3.8380 (3.9303)  class_acc: 0.2917 (0.2754)  loss_scale: 65536.0000 (63043.1091)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1796  max mem: 15572
Epoch: [17]  [1100/2809]  eta: 0:16:25  lr: 0.000034  min_lr: 0.000000  loss: 3.8380 (3.9286)  class_acc: 0.2500 (0.2756)  loss_scale: 65536.0000 (63065.7511)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1345  max mem: 15572
Epoch: [17]  [1110/2809]  eta: 0:16:19  lr: 0.000034  min_lr: 0.000000  loss: 3.9079 (3.9284)  class_acc: 0.2083 (0.2753)  loss_scale: 65536.0000 (63087.9856)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1196  max mem: 15572
Epoch: [17]  [1120/2809]  eta: 0:16:14  lr: 0.000034  min_lr: 0.000000  loss: 3.7376 (3.9269)  class_acc: 0.2083 (0.2753)  loss_scale: 65536.0000 (63109.8234)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1343  max mem: 15572
Epoch: [17]  [1130/2809]  eta: 0:16:09  lr: 0.000034  min_lr: 0.000000  loss: 3.9632 (3.9295)  class_acc: 0.2500 (0.2751)  loss_scale: 65536.0000 (63131.2750)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1581  max mem: 15572
Epoch: [17]  [1140/2809]  eta: 0:16:03  lr: 0.000034  min_lr: 0.000000  loss: 4.1805 (3.9306)  class_acc: 0.2083 (0.2749)  loss_scale: 65536.0000 (63152.3506)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1408  max mem: 15572
Epoch: [17]  [1150/2809]  eta: 0:15:58  lr: 0.000034  min_lr: 0.000000  loss: 4.0784 (3.9304)  class_acc: 0.2917 (0.2749)  loss_scale: 65536.0000 (63173.0599)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1230  max mem: 15572
Epoch: [17]  [1160/2809]  eta: 0:15:51  lr: 0.000034  min_lr: 0.000000  loss: 4.0128 (3.9316)  class_acc: 0.2083 (0.2742)  loss_scale: 65536.0000 (63193.4126)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1004  max mem: 15572
Epoch: [17]  [1170/2809]  eta: 0:15:46  lr: 0.000034  min_lr: 0.000000  loss: 4.0128 (3.9328)  class_acc: 0.2083 (0.2743)  loss_scale: 65536.0000 (63213.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1392  max mem: 15572
Epoch: [17]  [1180/2809]  eta: 0:15:39  lr: 0.000034  min_lr: 0.000000  loss: 3.9999 (3.9343)  class_acc: 0.2917 (0.2742)  loss_scale: 65536.0000 (63233.0838)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1201  max mem: 15572
Epoch: [17]  [1190/2809]  eta: 0:15:32  lr: 0.000034  min_lr: 0.000000  loss: 4.0331 (3.9351)  class_acc: 0.2500 (0.2740)  loss_scale: 65536.0000 (63252.4198)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0308  max mem: 15572
Epoch: [17]  [1200/2809]  eta: 0:15:27  lr: 0.000034  min_lr: 0.000000  loss: 3.9680 (3.9333)  class_acc: 0.2917 (0.2748)  loss_scale: 65536.0000 (63271.4338)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0849  max mem: 15572
Epoch: [17]  [1210/2809]  eta: 0:15:22  lr: 0.000034  min_lr: 0.000000  loss: 3.9317 (3.9330)  class_acc: 0.3333 (0.2753)  loss_scale: 65536.0000 (63290.1338)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1817  max mem: 15572
[2025-01-15 22:44:24,596] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:44:24,596] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:44:25,873] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48969
[2025-01-15 22:44:25,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:44:25,874] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1220/2809]  eta: 0:15:16  lr: 0.000034  min_lr: 0.000000  loss: 3.8781 (3.9322)  class_acc: 0.3333 (0.2754)  loss_scale: 65536.0000 (63469.5495)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1856  max mem: 15572
Epoch: [17]  [1230/2809]  eta: 0:15:10  lr: 0.000034  min_lr: 0.000000  loss: 4.0246 (3.9351)  class_acc: 0.2083 (0.2749)  loss_scale: 65536.0000 (63486.3363)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1138  max mem: 15572
Epoch: [17]  [1240/2809]  eta: 0:15:04  lr: 0.000034  min_lr: 0.000000  loss: 4.0028 (3.9334)  class_acc: 0.2500 (0.2755)  loss_scale: 65536.0000 (63502.8525)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0754  max mem: 15572
[2025-01-15 22:44:43,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=329, lr=[3.267739872028712e-07, 3.267739872028712e-07, 4.6681998171838747e-07, 4.6681998171838747e-07, 6.66885688169125e-07, 6.66885688169125e-07, 9.526938402416073e-07, 9.526938402416073e-07, 1.3609912003451532e-06, 1.3609912003451532e-06, 1.944273143350219e-06, 1.944273143350219e-06, 2.7775330619288843e-06, 2.7775330619288843e-06, 3.967904374184121e-06, 3.967904374184121e-06, 5.66843482026303e-06, 5.66843482026303e-06, 8.097764028947187e-06, 8.097764028947187e-06, 1.1568234327067409e-05, 1.1568234327067409e-05, 1.652604903866773e-05, 1.652604903866773e-05, 2.3608641483811045e-05, 2.3608641483811045e-05, 3.3726630691158636e-05, 3.3726630691158636e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:44:43,207] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=28.47598700310081, CurrSamplesPerSec=29.60662536110018, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [1250/2809]  eta: 0:14:57  lr: 0.000034  min_lr: 0.000000  loss: 3.7052 (3.9328)  class_acc: 0.3333 (0.2756)  loss_scale: 65536.0000 (63519.1047)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0744  max mem: 15572
Epoch: [17]  [1260/2809]  eta: 0:14:52  lr: 0.000034  min_lr: 0.000000  loss: 4.0786 (3.9351)  class_acc: 0.2500 (0.2753)  loss_scale: 65536.0000 (63535.0991)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1250  max mem: 15572
Epoch: [17]  [1270/2809]  eta: 0:14:47  lr: 0.000034  min_lr: 0.000000  loss: 4.0915 (3.9350)  class_acc: 0.2500 (0.2755)  loss_scale: 65536.0000 (63550.8419)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1886  max mem: 15572
Epoch: [17]  [1280/2809]  eta: 0:14:41  lr: 0.000034  min_lr: 0.000000  loss: 3.9239 (3.9337)  class_acc: 0.2917 (0.2761)  loss_scale: 65536.0000 (63566.3388)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1445  max mem: 15572
Epoch: [17]  [1290/2809]  eta: 0:14:35  lr: 0.000034  min_lr: 0.000000  loss: 3.7412 (3.9328)  class_acc: 0.2917 (0.2762)  loss_scale: 65536.0000 (63581.5957)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1409  max mem: 15572
Epoch: [17]  [1300/2809]  eta: 0:14:29  lr: 0.000034  min_lr: 0.000000  loss: 4.1362 (3.9343)  class_acc: 0.2500 (0.2755)  loss_scale: 65536.0000 (63596.6180)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1385  max mem: 15572
Epoch: [17]  [1310/2809]  eta: 0:14:23  lr: 0.000034  min_lr: 0.000000  loss: 4.0924 (3.9344)  class_acc: 0.2083 (0.2758)  loss_scale: 65536.0000 (63611.4111)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1198  max mem: 15572
Epoch: [17]  [1320/2809]  eta: 0:14:17  lr: 0.000034  min_lr: 0.000000  loss: 3.8950 (3.9336)  class_acc: 0.2917 (0.2760)  loss_scale: 65536.0000 (63625.9803)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1435  max mem: 15572
Epoch: [17]  [1330/2809]  eta: 0:14:13  lr: 0.000034  min_lr: 0.000000  loss: 3.8700 (3.9335)  class_acc: 0.2500 (0.2763)  loss_scale: 65536.0000 (63640.3306)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1640  max mem: 15572
Epoch: [17]  [1340/2809]  eta: 0:14:06  lr: 0.000034  min_lr: 0.000000  loss: 3.9332 (3.9335)  class_acc: 0.2500 (0.2763)  loss_scale: 65536.0000 (63654.4668)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1396  max mem: 15572
[2025-01-15 22:45:40,900] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:45:40,901] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:45:41,458] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49099
[2025-01-15 22:45:41,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:45:41,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1350/2809]  eta: 0:14:00  lr: 0.000034  min_lr: 0.000000  loss: 4.0212 (3.9337)  class_acc: 0.2500 (0.2762)  loss_scale: 65536.0000 (63716.9030)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0895  max mem: 15572
Epoch: [17]  [1360/2809]  eta: 0:13:54  lr: 0.000034  min_lr: 0.000000  loss: 4.0940 (3.9353)  class_acc: 0.2083 (0.2759)  loss_scale: 65536.0000 (63730.2689)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0974  max mem: 15572
Epoch: [17]  [1370/2809]  eta: 0:13:48  lr: 0.000034  min_lr: 0.000000  loss: 3.9003 (3.9328)  class_acc: 0.2917 (0.2765)  loss_scale: 65536.0000 (63743.4398)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1286  max mem: 15572
Epoch: [17]  [1380/2809]  eta: 0:13:41  lr: 0.000034  min_lr: 0.000000  loss: 3.7786 (3.9328)  class_acc: 0.3333 (0.2766)  loss_scale: 65536.0000 (63756.4200)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0783  max mem: 15572
Epoch: [17]  [1390/2809]  eta: 0:13:36  lr: 0.000034  min_lr: 0.000000  loss: 3.9572 (3.9329)  class_acc: 0.2917 (0.2767)  loss_scale: 65536.0000 (63769.2135)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0973  max mem: 15572
Epoch: [17]  [1400/2809]  eta: 0:13:30  lr: 0.000034  min_lr: 0.000000  loss: 4.0075 (3.9319)  class_acc: 0.2917 (0.2771)  loss_scale: 65536.0000 (63781.8244)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1141  max mem: 15572
Epoch: [17]  [1410/2809]  eta: 0:13:24  lr: 0.000034  min_lr: 0.000000  loss: 4.1275 (3.9337)  class_acc: 0.2500 (0.2767)  loss_scale: 65536.0000 (63794.2566)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0932  max mem: 15572
Epoch: [17]  [1420/2809]  eta: 0:13:18  lr: 0.000034  min_lr: 0.000000  loss: 4.0875 (3.9339)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (63806.5137)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1068  max mem: 15572
Epoch: [17]  [1430/2809]  eta: 0:13:12  lr: 0.000034  min_lr: 0.000000  loss: 3.9315 (3.9336)  class_acc: 0.2917 (0.2769)  loss_scale: 65536.0000 (63818.5996)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0969  max mem: 15572
Epoch: [17]  [1440/2809]  eta: 0:13:05  lr: 0.000034  min_lr: 0.000000  loss: 4.0088 (3.9349)  class_acc: 0.2500 (0.2765)  loss_scale: 65536.0000 (63830.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0859  max mem: 15572
Epoch: [17]  [1450/2809]  eta: 0:13:00  lr: 0.000034  min_lr: 0.000000  loss: 3.9805 (3.9345)  class_acc: 0.2083 (0.2764)  loss_scale: 65536.0000 (63842.2715)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1035  max mem: 15572
Epoch: [17]  [1460/2809]  eta: 0:12:54  lr: 0.000034  min_lr: 0.000000  loss: 4.0248 (3.9363)  class_acc: 0.2083 (0.2760)  loss_scale: 65536.0000 (63853.8645)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1263  max mem: 15572
Epoch: [17]  [1470/2809]  eta: 0:12:49  lr: 0.000034  min_lr: 0.000000  loss: 4.0192 (3.9357)  class_acc: 0.2083 (0.2763)  loss_scale: 65536.0000 (63865.2998)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1394  max mem: 15572
[2025-01-15 22:46:53,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:46:53,515] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:46:53,909] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49229
[2025-01-15 22:46:53,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:46:53,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1480/2809]  eta: 0:12:43  lr: 0.000034  min_lr: 0.000000  loss: 3.8760 (3.9351)  class_acc: 0.2917 (0.2763)  loss_scale: 65536.0000 (63920.8319)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1481  max mem: 15572
Epoch: [17]  [1490/2809]  eta: 0:12:38  lr: 0.000034  min_lr: 0.000000  loss: 3.8232 (3.9342)  class_acc: 0.2917 (0.2766)  loss_scale: 65536.0000 (63931.6647)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1398  max mem: 15572
Epoch: [17]  [1500/2809]  eta: 0:12:32  lr: 0.000034  min_lr: 0.000000  loss: 3.8386 (3.9331)  class_acc: 0.2917 (0.2769)  loss_scale: 65536.0000 (63942.3531)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1615  max mem: 15572
Epoch: [17]  [1510/2809]  eta: 0:12:27  lr: 0.000034  min_lr: 0.000000  loss: 3.8743 (3.9338)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (63952.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1692  max mem: 15572
Epoch: [17]  [1520/2809]  eta: 0:12:21  lr: 0.000034  min_lr: 0.000000  loss: 4.1376 (3.9347)  class_acc: 0.2083 (0.2770)  loss_scale: 65536.0000 (63963.3083)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1626  max mem: 15572
Epoch: [17]  [1530/2809]  eta: 0:12:15  lr: 0.000034  min_lr: 0.000000  loss: 4.0905 (3.9345)  class_acc: 0.2083 (0.2770)  loss_scale: 65536.0000 (63973.5807)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1029  max mem: 15572
Epoch: [17]  [1540/2809]  eta: 0:12:08  lr: 0.000034  min_lr: 0.000000  loss: 3.9299 (3.9343)  class_acc: 0.2917 (0.2771)  loss_scale: 65536.0000 (63983.7197)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0634  max mem: 15572
Epoch: [17]  [1550/2809]  eta: 0:12:02  lr: 0.000034  min_lr: 0.000000  loss: 3.9069 (3.9346)  class_acc: 0.3333 (0.2773)  loss_scale: 65536.0000 (63993.7279)  weight_decay: 0.0500 (0.0500)  time: 0.4994  data: 0.0534  max mem: 15572
Epoch: [17]  [1560/2809]  eta: 0:11:56  lr: 0.000034  min_lr: 0.000000  loss: 3.9069 (3.9342)  class_acc: 0.2500 (0.2773)  loss_scale: 65536.0000 (64003.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1017  max mem: 15572
Epoch: [17]  [1570/2809]  eta: 0:11:50  lr: 0.000034  min_lr: 0.000000  loss: 3.7683 (3.9335)  class_acc: 0.2917 (0.2776)  loss_scale: 65536.0000 (64013.3622)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1181  max mem: 15572
Epoch: [17]  [1580/2809]  eta: 0:11:45  lr: 0.000034  min_lr: 0.000000  loss: 3.8229 (3.9329)  class_acc: 0.3333 (0.2779)  loss_scale: 65536.0000 (64022.9930)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1092  max mem: 15572
Epoch: [17]  [1590/2809]  eta: 0:11:38  lr: 0.000033  min_lr: 0.000000  loss: 3.8672 (3.9327)  class_acc: 0.2500 (0.2780)  loss_scale: 65536.0000 (64032.5028)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0959  max mem: 15572
Epoch: [17]  [1600/2809]  eta: 0:11:32  lr: 0.000033  min_lr: 0.000000  loss: 3.8672 (3.9325)  class_acc: 0.2917 (0.2782)  loss_scale: 65536.0000 (64041.8938)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0484  max mem: 15572
[2025-01-15 22:48:06,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:48:06,051] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [1610/2809]  eta: 0:11:27  lr: 0.000033  min_lr: 0.000000  loss: 3.9356 (3.9337)  class_acc: 0.2917 (0.2779)  loss_scale: 65536.0000 (64295.2502)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1057  max mem: 15572
[2025-01-15 22:48:11,332] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49366
[2025-01-15 22:48:11,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:48:11,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1620/2809]  eta: 0:11:21  lr: 0.000033  min_lr: 0.000000  loss: 4.0452 (3.9357)  class_acc: 0.2083 (0.2775)  loss_scale: 65536.0000 (64383.7631)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1642  max mem: 15572
Epoch: [17]  [1630/2809]  eta: 0:11:15  lr: 0.000033  min_lr: 0.000000  loss: 4.1335 (3.9361)  class_acc: 0.2083 (0.2772)  loss_scale: 65536.0000 (64390.8277)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1480  max mem: 15572
Epoch: [17]  [1640/2809]  eta: 0:11:10  lr: 0.000033  min_lr: 0.000000  loss: 4.0885 (3.9364)  class_acc: 0.2083 (0.2770)  loss_scale: 65536.0000 (64397.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1459  max mem: 15572
Epoch: [17]  [1650/2809]  eta: 0:11:04  lr: 0.000033  min_lr: 0.000000  loss: 3.9539 (3.9372)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (64404.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1553  max mem: 15572
Epoch: [17]  [1660/2809]  eta: 0:10:59  lr: 0.000033  min_lr: 0.000000  loss: 4.0108 (3.9374)  class_acc: 0.2500 (0.2767)  loss_scale: 65536.0000 (64411.5111)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1610  max mem: 15572
Epoch: [17]  [1670/2809]  eta: 0:10:53  lr: 0.000033  min_lr: 0.000000  loss: 4.0108 (3.9373)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (64418.2406)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1224  max mem: 15572
Epoch: [17]  [1680/2809]  eta: 0:10:47  lr: 0.000033  min_lr: 0.000000  loss: 3.7721 (3.9358)  class_acc: 0.3333 (0.2776)  loss_scale: 65536.0000 (64424.8899)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.1146  max mem: 15572
Epoch: [17]  [1690/2809]  eta: 0:10:40  lr: 0.000033  min_lr: 0.000000  loss: 3.7721 (3.9364)  class_acc: 0.2500 (0.2773)  loss_scale: 65536.0000 (64431.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0734  max mem: 15572
Epoch: [17]  [1700/2809]  eta: 0:10:34  lr: 0.000033  min_lr: 0.000000  loss: 4.1831 (3.9374)  class_acc: 0.2083 (0.2768)  loss_scale: 65536.0000 (64437.9541)  weight_decay: 0.0500 (0.0500)  time: 0.4542  data: 0.0122  max mem: 15572
Epoch: [17]  [1710/2809]  eta: 0:10:29  lr: 0.000033  min_lr: 0.000000  loss: 3.9563 (3.9377)  class_acc: 0.2083 (0.2768)  loss_scale: 65536.0000 (64444.3717)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0969  max mem: 15572
Epoch: [17]  [1720/2809]  eta: 0:10:23  lr: 0.000033  min_lr: 0.000000  loss: 3.8458 (3.9365)  class_acc: 0.2917 (0.2770)  loss_scale: 65536.0000 (64450.7147)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1257  max mem: 15572
Epoch: [17]  [1730/2809]  eta: 0:10:18  lr: 0.000033  min_lr: 0.000000  loss: 3.8861 (3.9370)  class_acc: 0.2500 (0.2767)  loss_scale: 65536.0000 (64456.9844)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1538  max mem: 15572
Epoch: [17]  [1740/2809]  eta: 0:10:12  lr: 0.000033  min_lr: 0.000000  loss: 3.9866 (3.9364)  class_acc: 0.2083 (0.2769)  loss_scale: 65536.0000 (64463.1821)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1659  max mem: 15572
[2025-01-15 22:49:24,747] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:49:24,747] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:49:25,610] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49497
[2025-01-15 22:49:25,610] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:49:25,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1750/2809]  eta: 0:10:06  lr: 0.000033  min_lr: 0.000000  loss: 3.9884 (3.9364)  class_acc: 0.2500 (0.2769)  loss_scale: 65536.0000 (64544.1645)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1256  max mem: 15572
Epoch: [17]  [1760/2809]  eta: 0:10:01  lr: 0.000033  min_lr: 0.000000  loss: 4.1754 (3.9377)  class_acc: 0.2083 (0.2766)  loss_scale: 65536.0000 (64549.7967)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1452  max mem: 15572
Epoch: [17]  [1770/2809]  eta: 0:09:54  lr: 0.000033  min_lr: 0.000000  loss: 4.0282 (3.9378)  class_acc: 0.2083 (0.2762)  loss_scale: 65536.0000 (64555.3653)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0990  max mem: 15572
Epoch: [17]  [1780/2809]  eta: 0:09:49  lr: 0.000033  min_lr: 0.000000  loss: 3.8548 (3.9363)  class_acc: 0.2083 (0.2765)  loss_scale: 65536.0000 (64560.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.0991  max mem: 15572
Epoch: [17]  [1790/2809]  eta: 0:09:44  lr: 0.000033  min_lr: 0.000000  loss: 3.7873 (3.9357)  class_acc: 0.3333 (0.2767)  loss_scale: 65536.0000 (64566.3160)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1610  max mem: 15572
Epoch: [17]  [1800/2809]  eta: 0:09:38  lr: 0.000033  min_lr: 0.000000  loss: 3.9261 (3.9359)  class_acc: 0.2500 (0.2765)  loss_scale: 65536.0000 (64571.7002)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1678  max mem: 15572
Epoch: [17]  [1810/2809]  eta: 0:09:32  lr: 0.000033  min_lr: 0.000000  loss: 3.6599 (3.9336)  class_acc: 0.3333 (0.2770)  loss_scale: 65536.0000 (64577.0248)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1186  max mem: 15572
Epoch: [17]  [1820/2809]  eta: 0:09:26  lr: 0.000033  min_lr: 0.000000  loss: 3.5739 (3.9329)  class_acc: 0.3333 (0.2772)  loss_scale: 65536.0000 (64582.2910)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0889  max mem: 15572
Epoch: [17]  [1830/2809]  eta: 0:09:20  lr: 0.000033  min_lr: 0.000000  loss: 3.9488 (3.9334)  class_acc: 0.2500 (0.2770)  loss_scale: 65536.0000 (64587.4997)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0606  max mem: 15572
Epoch: [17]  [1840/2809]  eta: 0:09:14  lr: 0.000033  min_lr: 0.000000  loss: 3.9488 (3.9335)  class_acc: 0.2917 (0.2772)  loss_scale: 65536.0000 (64592.6518)  weight_decay: 0.0500 (0.0500)  time: 0.4678  data: 0.0197  max mem: 15572
Epoch: [17]  [1850/2809]  eta: 0:09:08  lr: 0.000033  min_lr: 0.000000  loss: 3.8731 (3.9336)  class_acc: 0.2917 (0.2770)  loss_scale: 65536.0000 (64597.7482)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0930  max mem: 15572
Epoch: [17]  [1860/2809]  eta: 0:09:02  lr: 0.000033  min_lr: 0.000000  loss: 3.9715 (3.9343)  class_acc: 0.2083 (0.2768)  loss_scale: 65536.0000 (64602.7899)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1359  max mem: 15572
Epoch: [17]  [1870/2809]  eta: 0:08:56  lr: 0.000033  min_lr: 0.000000  loss: 3.9474 (3.9331)  class_acc: 0.2500 (0.2771)  loss_scale: 65536.0000 (64607.7777)  weight_decay: 0.0500 (0.0500)  time: 0.4938  data: 0.0685  max mem: 15572
[2025-01-15 22:50:36,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:50:36,627] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:50:37,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49627
[2025-01-15 22:50:37,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:50:37,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1880/2809]  eta: 0:08:50  lr: 0.000033  min_lr: 0.000000  loss: 3.7747 (3.9320)  class_acc: 0.3750 (0.2774)  loss_scale: 65536.0000 (64647.5534)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0626  max mem: 15572
Epoch: [17]  [1890/2809]  eta: 0:08:44  lr: 0.000033  min_lr: 0.000000  loss: 3.8576 (3.9316)  class_acc: 0.3333 (0.2775)  loss_scale: 65536.0000 (64652.2517)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0730  max mem: 15572
Epoch: [17]  [1900/2809]  eta: 0:08:39  lr: 0.000033  min_lr: 0.000000  loss: 3.9589 (3.9321)  class_acc: 0.2500 (0.2773)  loss_scale: 65536.0000 (64656.9006)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1044  max mem: 15572
Epoch: [17]  [1910/2809]  eta: 0:08:33  lr: 0.000033  min_lr: 0.000000  loss: 3.7922 (3.9312)  class_acc: 0.2500 (0.2774)  loss_scale: 65536.0000 (64661.5008)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1098  max mem: 15572
Epoch: [17]  [1920/2809]  eta: 0:08:27  lr: 0.000033  min_lr: 0.000000  loss: 3.8265 (3.9314)  class_acc: 0.2917 (0.2776)  loss_scale: 65536.0000 (64666.0531)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0843  max mem: 15572
Epoch: [17]  [1930/2809]  eta: 0:08:21  lr: 0.000033  min_lr: 0.000000  loss: 3.9590 (3.9312)  class_acc: 0.2917 (0.2778)  loss_scale: 65536.0000 (64670.5583)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1062  max mem: 15572
Epoch: [17]  [1940/2809]  eta: 0:08:16  lr: 0.000033  min_lr: 0.000000  loss: 3.9590 (3.9312)  class_acc: 0.2500 (0.2776)  loss_scale: 65536.0000 (64675.0170)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1628  max mem: 15572
Epoch: [17]  [1950/2809]  eta: 0:08:10  lr: 0.000033  min_lr: 0.000000  loss: 3.9546 (3.9307)  class_acc: 0.2500 (0.2777)  loss_scale: 65536.0000 (64679.4300)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1522  max mem: 15572
Epoch: [17]  [1960/2809]  eta: 0:08:05  lr: 0.000033  min_lr: 0.000000  loss: 3.6204 (3.9289)  class_acc: 0.3750 (0.2784)  loss_scale: 65536.0000 (64683.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1237  max mem: 15572
Epoch: [17]  [1970/2809]  eta: 0:07:59  lr: 0.000033  min_lr: 0.000000  loss: 3.6204 (3.9282)  class_acc: 0.3750 (0.2786)  loss_scale: 65536.0000 (64688.1218)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1137  max mem: 15572
Epoch: [17]  [1980/2809]  eta: 0:07:53  lr: 0.000033  min_lr: 0.000000  loss: 3.7735 (3.9285)  class_acc: 0.2917 (0.2785)  loss_scale: 65536.0000 (64692.4018)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1112  max mem: 15572
Epoch: [17]  [1990/2809]  eta: 0:07:47  lr: 0.000033  min_lr: 0.000000  loss: 3.8788 (3.9284)  class_acc: 0.2917 (0.2784)  loss_scale: 65536.0000 (64696.6389)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1038  max mem: 15572
Epoch: [17]  [2000/2809]  eta: 0:07:41  lr: 0.000033  min_lr: 0.000000  loss: 3.8533 (3.9279)  class_acc: 0.2917 (0.2785)  loss_scale: 65536.0000 (64700.8336)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0808  max mem: 15572
[2025-01-15 22:51:49,801] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:51:49,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:51:52,179] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49759
[2025-01-15 22:51:52,179] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:51:52,180] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2010/2809]  eta: 0:07:36  lr: 0.000033  min_lr: 0.000000  loss: 3.6600 (3.9268)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (64802.7529)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1565  max mem: 15572
Epoch: [17]  [2020/2809]  eta: 0:07:30  lr: 0.000033  min_lr: 0.000000  loss: 3.7112 (3.9271)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (64806.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1747  max mem: 15572
Epoch: [17]  [2030/2809]  eta: 0:07:25  lr: 0.000033  min_lr: 0.000000  loss: 3.8754 (3.9272)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (64809.9734)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1755  max mem: 15572
Epoch: [17]  [2040/2809]  eta: 0:07:19  lr: 0.000033  min_lr: 0.000000  loss: 3.8677 (3.9276)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (64813.5306)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1480  max mem: 15572
Epoch: [17]  [2050/2809]  eta: 0:07:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8025 (3.9261)  class_acc: 0.3333 (0.2791)  loss_scale: 65536.0000 (64817.0531)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1086  max mem: 15572
Epoch: [17]  [2060/2809]  eta: 0:07:07  lr: 0.000033  min_lr: 0.000000  loss: 3.8025 (3.9262)  class_acc: 0.2917 (0.2790)  loss_scale: 65536.0000 (64820.5415)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1081  max mem: 15572
Epoch: [17]  [2070/2809]  eta: 0:07:01  lr: 0.000033  min_lr: 0.000000  loss: 3.9798 (3.9268)  class_acc: 0.2083 (0.2789)  loss_scale: 65536.0000 (64823.9961)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0661  max mem: 15572
Epoch: [17]  [2080/2809]  eta: 0:06:56  lr: 0.000033  min_lr: 0.000000  loss: 3.9358 (3.9263)  class_acc: 0.2500 (0.2790)  loss_scale: 65536.0000 (64827.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0827  max mem: 15572
Epoch: [17]  [2090/2809]  eta: 0:06:50  lr: 0.000033  min_lr: 0.000000  loss: 3.8985 (3.9263)  class_acc: 0.2500 (0.2791)  loss_scale: 65536.0000 (64830.8063)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1364  max mem: 15572
Epoch: [17]  [2100/2809]  eta: 0:06:44  lr: 0.000033  min_lr: 0.000000  loss: 3.9263 (3.9259)  class_acc: 0.2917 (0.2793)  loss_scale: 65536.0000 (64834.1628)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1497  max mem: 15572
Epoch: [17]  [2110/2809]  eta: 0:06:39  lr: 0.000033  min_lr: 0.000000  loss: 3.9892 (3.9257)  class_acc: 0.2917 (0.2794)  loss_scale: 65536.0000 (64837.4874)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1113  max mem: 15572
Epoch: [17]  [2120/2809]  eta: 0:06:33  lr: 0.000033  min_lr: 0.000000  loss: 3.9740 (3.9260)  class_acc: 0.2500 (0.2792)  loss_scale: 65536.0000 (64840.7808)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1196  max mem: 15572
Epoch: [17]  [2130/2809]  eta: 0:06:27  lr: 0.000033  min_lr: 0.000000  loss: 3.9070 (3.9264)  class_acc: 0.2500 (0.2791)  loss_scale: 65536.0000 (64844.0432)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1767  max mem: 15572
[2025-01-15 22:53:05,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:53:05,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:53:06,063] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49889
[2025-01-15 22:53:06,063] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:53:06,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2140/2809]  eta: 0:06:22  lr: 0.000033  min_lr: 0.000000  loss: 4.0653 (3.9275)  class_acc: 0.2083 (0.2789)  loss_scale: 65536.0000 (64877.8851)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1229  max mem: 15572
Epoch: [17]  [2150/2809]  eta: 0:06:16  lr: 0.000033  min_lr: 0.000000  loss: 4.1246 (3.9284)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (64880.9447)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0758  max mem: 15572
Epoch: [17]  [2160/2809]  eta: 0:06:10  lr: 0.000033  min_lr: 0.000000  loss: 4.1066 (3.9284)  class_acc: 0.2917 (0.2788)  loss_scale: 65536.0000 (64883.9759)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0603  max mem: 15572
Epoch: [17]  [2170/2809]  eta: 0:06:04  lr: 0.000033  min_lr: 0.000000  loss: 4.1368 (3.9296)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (64886.9793)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1172  max mem: 15572
Epoch: [17]  [2180/2809]  eta: 0:05:59  lr: 0.000033  min_lr: 0.000000  loss: 4.2239 (3.9315)  class_acc: 0.2083 (0.2782)  loss_scale: 65536.0000 (64889.9551)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1819  max mem: 15572
Epoch: [17]  [2190/2809]  eta: 0:05:53  lr: 0.000033  min_lr: 0.000000  loss: 4.1887 (3.9323)  class_acc: 0.2083 (0.2780)  loss_scale: 65536.0000 (64892.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1421  max mem: 15572
Epoch: [17]  [2200/2809]  eta: 0:05:47  lr: 0.000033  min_lr: 0.000000  loss: 3.9996 (3.9320)  class_acc: 0.2500 (0.2780)  loss_scale: 65536.0000 (64895.8255)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1398  max mem: 15572
Epoch: [17]  [2210/2809]  eta: 0:05:41  lr: 0.000033  min_lr: 0.000000  loss: 4.0021 (3.9323)  class_acc: 0.2500 (0.2780)  loss_scale: 65536.0000 (64898.7209)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1411  max mem: 15572
Epoch: [17]  [2220/2809]  eta: 0:05:36  lr: 0.000033  min_lr: 0.000000  loss: 3.7471 (3.9308)  class_acc: 0.2917 (0.2783)  loss_scale: 65536.0000 (64901.5903)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1209  max mem: 15572
Epoch: [17]  [2230/2809]  eta: 0:05:30  lr: 0.000033  min_lr: 0.000000  loss: 3.8354 (3.9316)  class_acc: 0.2917 (0.2784)  loss_scale: 65536.0000 (64904.4339)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1159  max mem: 15572
Epoch: [17]  [2240/2809]  eta: 0:05:24  lr: 0.000033  min_lr: 0.000000  loss: 4.0641 (3.9314)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (64907.2521)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0835  max mem: 15572
[2025-01-15 22:54:06,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=336, lr=[3.202091611303319e-07, 3.202091611303319e-07, 4.5744165875761706e-07, 4.5744165875761706e-07, 6.534880839394531e-07, 6.534880839394531e-07, 9.3355440562779e-07, 9.3355440562779e-07, 1.333649150896843e-06, 1.333649150896843e-06, 1.9052130727097759e-06, 1.9052130727097759e-06, 2.7217329610139656e-06, 2.7217329610139656e-06, 3.888189944305666e-06, 3.888189944305666e-06, 5.554557063293808e-06, 5.554557063293808e-06, 7.935081518991155e-06, 7.935081518991155e-06, 1.1335830741415936e-05, 1.1335830741415936e-05, 1.619404391630848e-05, 1.619404391630848e-05, 2.313434845186926e-05, 2.313434845186926e-05, 3.304906921695609e-05, 3.304906921695609e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 22:54:06,937] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=28.47684006965503, CurrSamplesPerSec=26.26857894407215, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [17]  [2250/2809]  eta: 0:05:18  lr: 0.000033  min_lr: 0.000000  loss: 3.8132 (3.9301)  class_acc: 0.3333 (0.2788)  loss_scale: 65536.0000 (64910.0453)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0572  max mem: 15572
Epoch: [17]  [2260/2809]  eta: 0:05:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8759 (3.9300)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (64912.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1118  max mem: 15572
[2025-01-15 22:54:17,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:54:17,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [2270/2809]  eta: 0:05:07  lr: 0.000033  min_lr: 0.000000  loss: 3.9425 (3.9295)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (65088.7045)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1357  max mem: 15572
[2025-01-15 22:54:21,708] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50025
[2025-01-15 22:54:21,708] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:54:21,708] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2280/2809]  eta: 0:05:01  lr: 0.000033  min_lr: 0.000000  loss: 3.8585 (3.9288)  class_acc: 0.2917 (0.2790)  loss_scale: 65536.0000 (65119.3968)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1043  max mem: 15572
Epoch: [17]  [2290/2809]  eta: 0:04:56  lr: 0.000033  min_lr: 0.000000  loss: 3.6053 (3.9279)  class_acc: 0.2917 (0.2791)  loss_scale: 65536.0000 (65121.2152)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1385  max mem: 15572
Epoch: [17]  [2300/2809]  eta: 0:04:50  lr: 0.000033  min_lr: 0.000000  loss: 3.9409 (3.9288)  class_acc: 0.2500 (0.2789)  loss_scale: 65536.0000 (65123.0178)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.1394  max mem: 15572
Epoch: [17]  [2310/2809]  eta: 0:04:44  lr: 0.000033  min_lr: 0.000000  loss: 4.1235 (3.9287)  class_acc: 0.2500 (0.2790)  loss_scale: 65536.0000 (65124.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1360  max mem: 15572
Epoch: [17]  [2320/2809]  eta: 0:04:39  lr: 0.000033  min_lr: 0.000000  loss: 4.1823 (3.9301)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (65126.5765)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1383  max mem: 15572
Epoch: [17]  [2330/2809]  eta: 0:04:33  lr: 0.000033  min_lr: 0.000000  loss: 3.9878 (3.9299)  class_acc: 0.2083 (0.2786)  loss_scale: 65536.0000 (65128.3329)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1126  max mem: 15572
Epoch: [17]  [2340/2809]  eta: 0:04:27  lr: 0.000033  min_lr: 0.000000  loss: 3.7221 (3.9291)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65130.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1285  max mem: 15572
Epoch: [17]  [2350/2809]  eta: 0:04:21  lr: 0.000033  min_lr: 0.000000  loss: 3.7603 (3.9289)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65131.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1270  max mem: 15572
Epoch: [17]  [2360/2809]  eta: 0:04:16  lr: 0.000033  min_lr: 0.000000  loss: 3.8024 (3.9290)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65133.5129)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1253  max mem: 15572
Epoch: [17]  [2370/2809]  eta: 0:04:10  lr: 0.000033  min_lr: 0.000000  loss: 3.8864 (3.9291)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65135.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0897  max mem: 15572
Epoch: [17]  [2380/2809]  eta: 0:04:04  lr: 0.000033  min_lr: 0.000000  loss: 3.8864 (3.9286)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65136.8937)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0606  max mem: 15572
Epoch: [17]  [2390/2809]  eta: 0:03:58  lr: 0.000033  min_lr: 0.000000  loss: 4.0413 (3.9286)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (65138.5629)  weight_decay: 0.0500 (0.0500)  time: 0.5090  data: 0.0666  max mem: 15572
Epoch: [17]  [2400/2809]  eta: 0:03:53  lr: 0.000033  min_lr: 0.000000  loss: 4.0673 (3.9289)  class_acc: 0.2083 (0.2784)  loss_scale: 65536.0000 (65140.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0863  max mem: 15572
[2025-01-15 22:55:34,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:55:34,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:55:35,719] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50157
[2025-01-15 22:55:35,719] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:55:35,719] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2410/2809]  eta: 0:03:47  lr: 0.000033  min_lr: 0.000000  loss: 3.9049 (3.9281)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (65223.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0949  max mem: 15572
Epoch: [17]  [2420/2809]  eta: 0:03:41  lr: 0.000033  min_lr: 0.000000  loss: 3.9093 (3.9285)  class_acc: 0.2083 (0.2783)  loss_scale: 65536.0000 (65224.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1258  max mem: 15572
Epoch: [17]  [2430/2809]  eta: 0:03:35  lr: 0.000033  min_lr: 0.000000  loss: 3.9174 (3.9280)  class_acc: 0.2083 (0.2783)  loss_scale: 65536.0000 (65225.9778)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1533  max mem: 15572
Epoch: [17]  [2440/2809]  eta: 0:03:30  lr: 0.000033  min_lr: 0.000000  loss: 3.9174 (3.9283)  class_acc: 0.2917 (0.2784)  loss_scale: 65536.0000 (65227.2478)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1486  max mem: 15572
Epoch: [17]  [2450/2809]  eta: 0:03:24  lr: 0.000033  min_lr: 0.000000  loss: 4.1238 (3.9281)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (65228.5075)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1647  max mem: 15572
Epoch: [17]  [2460/2809]  eta: 0:03:18  lr: 0.000033  min_lr: 0.000000  loss: 4.1238 (3.9286)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (65229.7570)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0904  max mem: 15572
Epoch: [17]  [2470/2809]  eta: 0:03:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8547 (3.9281)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (65230.9964)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0682  max mem: 15572
Epoch: [17]  [2480/2809]  eta: 0:03:07  lr: 0.000033  min_lr: 0.000000  loss: 3.7965 (3.9277)  class_acc: 0.2917 (0.2786)  loss_scale: 65536.0000 (65232.2257)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1006  max mem: 15572
Epoch: [17]  [2490/2809]  eta: 0:03:01  lr: 0.000033  min_lr: 0.000000  loss: 3.7965 (3.9275)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65233.4452)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0943  max mem: 15572
Epoch: [17]  [2500/2809]  eta: 0:02:55  lr: 0.000033  min_lr: 0.000000  loss: 3.8880 (3.9274)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65234.6549)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0866  max mem: 15572
Epoch: [17]  [2510/2809]  eta: 0:02:50  lr: 0.000033  min_lr: 0.000000  loss: 3.8982 (3.9276)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65235.8550)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0789  max mem: 15572
Epoch: [17]  [2520/2809]  eta: 0:02:44  lr: 0.000033  min_lr: 0.000000  loss: 3.9068 (3.9279)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (65237.0456)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1706  max mem: 15572
Epoch: [17]  [2530/2809]  eta: 0:02:38  lr: 0.000033  min_lr: 0.000000  loss: 3.8935 (3.9272)  class_acc: 0.2917 (0.2788)  loss_scale: 65536.0000 (65238.2268)  weight_decay: 0.0500 (0.0500)  time: 0.6847  data: 0.2314  max mem: 15572
[2025-01-15 22:56:49,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:56:49,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:56:50,981] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50289
[2025-01-15 22:56:50,981] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:56:50,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2540/2809]  eta: 0:02:33  lr: 0.000033  min_lr: 0.000000  loss: 3.9835 (3.9279)  class_acc: 0.2917 (0.2787)  loss_scale: 65536.0000 (65316.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1414  max mem: 15572
Epoch: [17]  [2550/2809]  eta: 0:02:27  lr: 0.000033  min_lr: 0.000000  loss: 3.8875 (3.9267)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (65317.6323)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1024  max mem: 15572
Epoch: [17]  [2560/2809]  eta: 0:02:21  lr: 0.000033  min_lr: 0.000000  loss: 3.6562 (3.9263)  class_acc: 0.2917 (0.2788)  loss_scale: 65536.0000 (65318.4850)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1564  max mem: 15572
Epoch: [17]  [2570/2809]  eta: 0:02:16  lr: 0.000033  min_lr: 0.000000  loss: 3.6562 (3.9258)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65319.3310)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1840  max mem: 15572
Epoch: [17]  [2580/2809]  eta: 0:02:10  lr: 0.000033  min_lr: 0.000000  loss: 3.8084 (3.9264)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65320.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1212  max mem: 15572
Epoch: [17]  [2590/2809]  eta: 0:02:04  lr: 0.000033  min_lr: 0.000000  loss: 4.0443 (3.9272)  class_acc: 0.2500 (0.2786)  loss_scale: 65536.0000 (65321.0035)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0479  max mem: 15572
Epoch: [17]  [2600/2809]  eta: 0:01:59  lr: 0.000033  min_lr: 0.000000  loss: 4.0076 (3.9266)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65321.8301)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0444  max mem: 15572
Epoch: [17]  [2610/2809]  eta: 0:01:53  lr: 0.000033  min_lr: 0.000000  loss: 3.8565 (3.9257)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65322.6503)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0774  max mem: 15572
Epoch: [17]  [2620/2809]  eta: 0:01:47  lr: 0.000033  min_lr: 0.000000  loss: 3.8458 (3.9255)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65323.4643)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1104  max mem: 15572
Epoch: [17]  [2630/2809]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000000  loss: 3.8991 (3.9257)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65324.2721)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1204  max mem: 15572
Epoch: [17]  [2640/2809]  eta: 0:01:36  lr: 0.000033  min_lr: 0.000000  loss: 3.9946 (3.9259)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65325.0738)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1169  max mem: 15572
Epoch: [17]  [2650/2809]  eta: 0:01:30  lr: 0.000033  min_lr: 0.000000  loss: 4.1081 (3.9269)  class_acc: 0.2083 (0.2786)  loss_scale: 65536.0000 (65325.8695)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.0799  max mem: 15572
Epoch: [17]  [2660/2809]  eta: 0:01:24  lr: 0.000033  min_lr: 0.000000  loss: 4.1433 (3.9272)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (65326.6592)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0753  max mem: 15572
[2025-01-15 22:58:04,365] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:58:04,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 22:58:05,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50421
[2025-01-15 22:58:05,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:58:05,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2670/2809]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000000  loss: 4.0785 (3.9276)  class_acc: 0.2500 (0.2784)  loss_scale: 65536.0000 (65401.0513)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1311  max mem: 15572
Epoch: [17]  [2680/2809]  eta: 0:01:13  lr: 0.000033  min_lr: 0.000000  loss: 3.9172 (3.9275)  class_acc: 0.2083 (0.2784)  loss_scale: 65536.0000 (65401.5546)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1286  max mem: 15572
Epoch: [17]  [2690/2809]  eta: 0:01:07  lr: 0.000033  min_lr: 0.000000  loss: 3.9071 (3.9277)  class_acc: 0.2083 (0.2783)  loss_scale: 65536.0000 (65402.0543)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1046  max mem: 15572
Epoch: [17]  [2700/2809]  eta: 0:01:02  lr: 0.000033  min_lr: 0.000000  loss: 3.9071 (3.9274)  class_acc: 0.2917 (0.2785)  loss_scale: 65536.0000 (65402.5502)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.1685  max mem: 15572
Epoch: [17]  [2710/2809]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000000  loss: 3.8870 (3.9274)  class_acc: 0.2917 (0.2785)  loss_scale: 65536.0000 (65403.0424)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1352  max mem: 15572
Epoch: [17]  [2720/2809]  eta: 0:00:50  lr: 0.000033  min_lr: 0.000000  loss: 4.0380 (3.9282)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (65403.5311)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1299  max mem: 15572
Epoch: [17]  [2730/2809]  eta: 0:00:45  lr: 0.000033  min_lr: 0.000000  loss: 3.9807 (3.9280)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (65404.0161)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1300  max mem: 15572
Epoch: [17]  [2740/2809]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000000  loss: 3.9368 (3.9279)  class_acc: 0.2500 (0.2785)  loss_scale: 65536.0000 (65404.4976)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.0852  max mem: 15572
Epoch: [17]  [2750/2809]  eta: 0:00:33  lr: 0.000033  min_lr: 0.000000  loss: 3.9847 (3.9279)  class_acc: 0.2500 (0.2787)  loss_scale: 65536.0000 (65404.9756)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0706  max mem: 15572
Epoch: [17]  [2760/2809]  eta: 0:00:27  lr: 0.000033  min_lr: 0.000000  loss: 4.0439 (3.9277)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65405.4502)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0613  max mem: 15572
Epoch: [17]  [2770/2809]  eta: 0:00:22  lr: 0.000033  min_lr: 0.000000  loss: 3.6840 (3.9269)  class_acc: 0.2500 (0.2789)  loss_scale: 65536.0000 (65405.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5041  data: 0.0799  max mem: 15572
Epoch: [17]  [2780/2809]  eta: 0:00:16  lr: 0.000033  min_lr: 0.000000  loss: 3.6992 (3.9272)  class_acc: 0.2500 (0.2789)  loss_scale: 65536.0000 (65406.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0843  max mem: 15572
Epoch: [17]  [2790/2809]  eta: 0:00:10  lr: 0.000033  min_lr: 0.000000  loss: 3.8997 (3.9274)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65406.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1375  max mem: 15572
[2025-01-15 22:59:18,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 22:59:18,121] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [2800/2809]  eta: 0:00:05  lr: 0.000033  min_lr: 0.000000  loss: 3.9007 (3.9271)  class_acc: 0.2917 (0.2789)  loss_scale: 65536.0000 (65500.9040)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1152  max mem: 15572
[2025-01-15 22:59:19,715] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50554
[2025-01-15 22:59:19,715] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 22:59:19,715] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [2808/2809]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000000  loss: 3.9315 (3.9273)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65501.0039)  weight_decay: 0.0500 (0.0500)  time: 0.4267  data: 0.0294  max mem: 15572
Epoch: [17] Total time: 0:26:38 (0.5689 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000000  loss: 3.9315 (3.9273)  class_acc: 0.2500 (0.2788)  loss_scale: 65536.0000 (65501.0039)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:27:14  loss: 0.4245 (0.4245)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.0098  data: 5.8335  max mem: 15572
Val:  [ 10/272]  eta: 0:03:19  loss: 2.6719 (2.5726)  acc1: 38.8889 (37.8788)  acc5: 66.6667 (67.1717)  time: 0.7607  data: 0.5812  max mem: 15572
Val:  [ 20/272]  eta: 0:02:05  loss: 2.6719 (2.6035)  acc1: 38.8889 (39.6825)  acc5: 66.6667 (69.5767)  time: 0.2218  data: 0.0397  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 2.8362 (2.7230)  acc1: 38.8889 (34.7670)  acc5: 66.6667 (68.1004)  time: 0.2678  data: 0.0710  max mem: 15572
Val:  [ 40/272]  eta: 0:01:35  loss: 2.7630 (2.7270)  acc1: 27.7778 (33.6043)  acc5: 66.6667 (68.6992)  time: 0.3236  data: 0.1259  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.6925 (2.6474)  acc1: 33.3333 (35.9477)  acc5: 77.7778 (70.4793)  time: 0.3170  data: 0.1294  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.6960 (2.5202)  acc1: 55.5556 (39.7086)  acc5: 77.7778 (71.9490)  time: 0.3035  data: 0.1063  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.7082 (2.4416)  acc1: 61.1111 (42.4100)  acc5: 83.3333 (73.0829)  time: 0.2949  data: 0.1021  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 2.1434 (2.4428)  acc1: 55.5556 (42.3182)  acc5: 77.7778 (72.9081)  time: 0.3023  data: 0.1119  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.3599 (2.4544)  acc1: 44.4444 (42.6129)  acc5: 83.3333 (73.6264)  time: 0.3233  data: 0.1210  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.4006 (2.4899)  acc1: 44.4444 (41.9142)  acc5: 83.3333 (73.1023)  time: 0.3071  data: 0.1093  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.8437 (2.5753)  acc1: 16.6667 (39.5395)  acc5: 61.1111 (71.3714)  time: 0.3120  data: 0.1262  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.2265 (2.6206)  acc1: 11.1111 (38.4757)  acc5: 61.1111 (70.5234)  time: 0.2811  data: 0.0933  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.6515 (2.5879)  acc1: 38.8889 (39.6947)  acc5: 72.2222 (70.9075)  time: 0.2682  data: 0.0765  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.8011 (2.5785)  acc1: 55.5556 (40.2679)  acc5: 77.7778 (70.8826)  time: 0.3164  data: 0.1185  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.5995 (2.5786)  acc1: 33.3333 (39.8455)  acc5: 72.2222 (71.2288)  time: 0.3100  data: 0.1128  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.4648 (2.5529)  acc1: 50.0000 (41.0628)  acc5: 77.7778 (71.8772)  time: 0.3022  data: 0.1070  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.4648 (2.5752)  acc1: 44.4444 (40.1884)  acc5: 72.2222 (71.4100)  time: 0.3053  data: 0.1059  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.5687 (2.5647)  acc1: 33.3333 (39.9325)  acc5: 72.2222 (71.8846)  time: 0.2854  data: 0.0995  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.6510 (2.6078)  acc1: 27.7778 (38.8016)  acc5: 72.2222 (70.7970)  time: 0.2526  data: 0.0799  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.7288 (2.6177)  acc1: 22.2222 (38.3637)  acc5: 61.1111 (70.5362)  time: 0.2129  data: 0.0415  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.3994 (2.6214)  acc1: 33.3333 (38.5466)  acc5: 77.7778 (70.4845)  time: 0.1907  data: 0.0077  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.4760 (2.6122)  acc1: 50.0000 (38.7883)  acc5: 72.2222 (70.6385)  time: 0.2477  data: 0.0513  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9464 (2.5767)  acc1: 55.5556 (39.9471)  acc5: 72.2222 (71.0438)  time: 0.3250  data: 0.1228  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 1.8302 (2.5576)  acc1: 55.5556 (40.1798)  acc5: 77.7778 (71.4154)  time: 0.3483  data: 0.1367  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.4979 (2.5716)  acc1: 33.3333 (39.5308)  acc5: 72.2222 (71.2483)  time: 0.3238  data: 0.1150  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.3849 (2.5023)  acc1: 72.2222 (41.4645)  acc5: 88.8889 (72.1158)  time: 0.3392  data: 0.1471  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5265 (2.5049)  acc1: 61.1111 (41.2669)  acc5: 83.3333 (72.0992)  time: 0.3344  data: 0.1595  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5733 (2.5099)  acc1: 55.5556 (41.2247)  acc5: 83.3333 (72.0868)  time: 0.2431  data: 0.0742  max mem: 15572
Val: Total time: 0:01:25 (0.3139 s / it)
* Acc@1 41.225 Acc@5 72.087 loss 2.510
Accuracy of the network on the 4883 val videos: 41.2%
[2025-01-15 23:00:47,970] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 23:00:47,973] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 23:00:47,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 23:00:51,356] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 23:00:51,357] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.22%
Epoch: [18]  [   0/2809]  eta: 8:24:06  lr: 0.000033  min_lr: 0.000000  loss: 4.2710 (4.2710)  class_acc: 0.2500 (0.2500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.7679  data: 10.0536  max mem: 15572
Epoch: [18]  [  10/2809]  eta: 1:10:07  lr: 0.000033  min_lr: 0.000000  loss: 4.0523 (4.0507)  class_acc: 0.2500 (0.2576)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5031  data: 1.0141  max mem: 15572
Epoch: [18]  [  20/2809]  eta: 0:51:20  lr: 0.000033  min_lr: 0.000000  loss: 4.0523 (4.0354)  class_acc: 0.2500 (0.2639)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1542  max mem: 15572
Epoch: [18]  [  30/2809]  eta: 0:46:26  lr: 0.000033  min_lr: 0.000000  loss: 4.0563 (4.0219)  class_acc: 0.2917 (0.2715)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7274  data: 0.2652  max mem: 15572
Epoch: [18]  [  40/2809]  eta: 0:42:13  lr: 0.000033  min_lr: 0.000000  loss: 3.8290 (3.8947)  class_acc: 0.3333 (0.2927)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.2567  max mem: 15572
Epoch: [18]  [  50/2809]  eta: 0:40:43  lr: 0.000033  min_lr: 0.000000  loss: 3.6438 (3.8773)  class_acc: 0.3333 (0.3023)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.2500  max mem: 15572
Epoch: [18]  [  60/2809]  eta: 0:38:42  lr: 0.000033  min_lr: 0.000000  loss: 3.8458 (3.8658)  class_acc: 0.3333 (0.3005)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7008  data: 0.2410  max mem: 15572
Epoch: [18]  [  70/2809]  eta: 0:36:09  lr: 0.000033  min_lr: 0.000000  loss: 3.5631 (3.8272)  class_acc: 0.2917 (0.3034)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1001  max mem: 15572
Epoch: [18]  [  80/2809]  eta: 0:33:55  lr: 0.000033  min_lr: 0.000000  loss: 3.7372 (3.8427)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4444  data: 0.0187  max mem: 15572
Epoch: [18]  [  90/2809]  eta: 0:32:12  lr: 0.000033  min_lr: 0.000000  loss: 3.7679 (3.8458)  class_acc: 0.2917 (0.2940)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4216  data: 0.0006  max mem: 15572
Epoch: [18]  [ 100/2809]  eta: 0:31:27  lr: 0.000033  min_lr: 0.000000  loss: 3.8594 (3.8420)  class_acc: 0.2500 (0.2941)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0625  max mem: 15572
Epoch: [18]  [ 110/2809]  eta: 0:30:33  lr: 0.000033  min_lr: 0.000000  loss: 3.8745 (3.8540)  class_acc: 0.2500 (0.2928)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0871  max mem: 15572
Epoch: [18]  [ 120/2809]  eta: 0:30:21  lr: 0.000033  min_lr: 0.000000  loss: 4.0550 (3.8674)  class_acc: 0.2500 (0.2882)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1431  max mem: 15572
[2025-01-15 23:02:14,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:02:14,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:02:15,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50685
[2025-01-15 23:02:15,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:02:15,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 130/2809]  eta: 0:29:41  lr: 0.000033  min_lr: 0.000000  loss: 4.1322 (3.8914)  class_acc: 0.2083 (0.2824)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1622  max mem: 15572
Epoch: [18]  [ 140/2809]  eta: 0:29:00  lr: 0.000033  min_lr: 0.000000  loss: 4.0598 (3.8886)  class_acc: 0.2917 (0.2840)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0748  max mem: 15572
Epoch: [18]  [ 150/2809]  eta: 0:28:39  lr: 0.000033  min_lr: 0.000000  loss: 3.9311 (3.8910)  class_acc: 0.2917 (0.2828)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0829  max mem: 15572
Epoch: [18]  [ 160/2809]  eta: 0:28:31  lr: 0.000033  min_lr: 0.000000  loss: 4.0247 (3.8966)  class_acc: 0.2500 (0.2813)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.1526  max mem: 15572
Epoch: [18]  [ 170/2809]  eta: 0:28:13  lr: 0.000033  min_lr: 0.000000  loss: 4.1532 (3.9160)  class_acc: 0.2500 (0.2795)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1731  max mem: 15572
Epoch: [18]  [ 180/2809]  eta: 0:27:51  lr: 0.000033  min_lr: 0.000000  loss: 4.1108 (3.9157)  class_acc: 0.2500 (0.2781)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0991  max mem: 15572
Epoch: [18]  [ 190/2809]  eta: 0:27:40  lr: 0.000033  min_lr: 0.000000  loss: 3.6812 (3.9032)  class_acc: 0.2917 (0.2812)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1127  max mem: 15572
Epoch: [18]  [ 200/2809]  eta: 0:27:24  lr: 0.000033  min_lr: 0.000000  loss: 3.4633 (3.8980)  class_acc: 0.3333 (0.2828)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1382  max mem: 15572
Epoch: [18]  [ 210/2809]  eta: 0:27:04  lr: 0.000033  min_lr: 0.000000  loss: 3.7188 (3.8989)  class_acc: 0.2917 (0.2834)  loss_scale: 65536.0000 (66157.1943)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0976  max mem: 15572
Epoch: [18]  [ 220/2809]  eta: 0:26:55  lr: 0.000033  min_lr: 0.000000  loss: 3.7338 (3.8876)  class_acc: 0.3333 (0.2870)  loss_scale: 65536.0000 (66129.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1231  max mem: 15572
Epoch: [18]  [ 230/2809]  eta: 0:26:43  lr: 0.000033  min_lr: 0.000000  loss: 3.5912 (3.8814)  class_acc: 0.3333 (0.2884)  loss_scale: 65536.0000 (66103.4113)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1487  max mem: 15572
Epoch: [18]  [ 240/2809]  eta: 0:26:34  lr: 0.000032  min_lr: 0.000000  loss: 3.5912 (3.8716)  class_acc: 0.2917 (0.2906)  loss_scale: 65536.0000 (66079.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1448  max mem: 15572
Epoch: [18]  [ 250/2809]  eta: 0:26:26  lr: 0.000032  min_lr: 0.000000  loss: 3.8936 (3.8794)  class_acc: 0.2917 (0.2887)  loss_scale: 65536.0000 (66058.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1474  max mem: 15572
[2025-01-15 23:03:28,799] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:03:28,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:03:29,583] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50816
[2025-01-15 23:03:29,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:03:29,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 260/2809]  eta: 0:26:11  lr: 0.000032  min_lr: 0.000000  loss: 3.9187 (3.8810)  class_acc: 0.2500 (0.2850)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1217  max mem: 15572
Epoch: [18]  [ 270/2809]  eta: 0:26:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9397 (3.8839)  class_acc: 0.2500 (0.2860)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1118  max mem: 15572
Epoch: [18]  [ 280/2809]  eta: 0:25:46  lr: 0.000032  min_lr: 0.000000  loss: 3.9397 (3.8852)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0999  max mem: 15572
Epoch: [18]  [ 290/2809]  eta: 0:25:36  lr: 0.000032  min_lr: 0.000000  loss: 3.9700 (3.8894)  class_acc: 0.2917 (0.2855)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.1053  max mem: 15572
Epoch: [18]  [ 300/2809]  eta: 0:25:33  lr: 0.000032  min_lr: 0.000000  loss: 4.0859 (3.8946)  class_acc: 0.2500 (0.2845)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1753  max mem: 15572
Epoch: [18]  [ 310/2809]  eta: 0:25:24  lr: 0.000032  min_lr: 0.000000  loss: 4.1060 (3.8989)  class_acc: 0.2500 (0.2834)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1910  max mem: 15572
Epoch: [18]  [ 320/2809]  eta: 0:25:13  lr: 0.000032  min_lr: 0.000000  loss: 4.0000 (3.9039)  class_acc: 0.2083 (0.2825)  loss_scale: 65536.0000 (66352.6480)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1323  max mem: 15572
Epoch: [18]  [ 330/2809]  eta: 0:25:03  lr: 0.000032  min_lr: 0.000000  loss: 4.0000 (3.9071)  class_acc: 0.2500 (0.2815)  loss_scale: 65536.0000 (66327.9758)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1048  max mem: 15572
Epoch: [18]  [ 340/2809]  eta: 0:24:48  lr: 0.000032  min_lr: 0.000000  loss: 3.9565 (3.9087)  class_acc: 0.2500 (0.2814)  loss_scale: 65536.0000 (66304.7507)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0834  max mem: 15572
Epoch: [18]  [ 350/2809]  eta: 0:24:43  lr: 0.000032  min_lr: 0.000000  loss: 3.8647 (3.9075)  class_acc: 0.2917 (0.2822)  loss_scale: 65536.0000 (66282.8490)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1234  max mem: 15572
Epoch: [18]  [ 360/2809]  eta: 0:24:40  lr: 0.000032  min_lr: 0.000000  loss: 3.8058 (3.9086)  class_acc: 0.2917 (0.2832)  loss_scale: 65536.0000 (66262.1607)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1862  max mem: 15572
Epoch: [18]  [ 370/2809]  eta: 0:24:32  lr: 0.000032  min_lr: 0.000000  loss: 3.7713 (3.9076)  class_acc: 0.2917 (0.2840)  loss_scale: 65536.0000 (66242.5876)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1598  max mem: 15572
Epoch: [18]  [ 380/2809]  eta: 0:24:23  lr: 0.000032  min_lr: 0.000000  loss: 3.8371 (3.9055)  class_acc: 0.3333 (0.2861)  loss_scale: 65536.0000 (66224.0420)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1234  max mem: 15572
[2025-01-15 23:04:43,052] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:04:43,052] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:04:44,044] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50947
[2025-01-15 23:04:44,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:04:44,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 390/2809]  eta: 0:24:12  lr: 0.000032  min_lr: 0.000000  loss: 3.8521 (3.9033)  class_acc: 0.3333 (0.2861)  loss_scale: 65536.0000 (66541.6675)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0869  max mem: 15572
Epoch: [18]  [ 400/2809]  eta: 0:24:07  lr: 0.000032  min_lr: 0.000000  loss: 3.9026 (3.9067)  class_acc: 0.2500 (0.2857)  loss_scale: 65536.0000 (66516.5885)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1341  max mem: 15572
Epoch: [18]  [ 410/2809]  eta: 0:24:00  lr: 0.000032  min_lr: 0.000000  loss: 4.0359 (3.9102)  class_acc: 0.2500 (0.2859)  loss_scale: 65536.0000 (66492.7299)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1631  max mem: 15572
Epoch: [18]  [ 420/2809]  eta: 0:23:53  lr: 0.000032  min_lr: 0.000000  loss: 3.9580 (3.9074)  class_acc: 0.2917 (0.2869)  loss_scale: 65536.0000 (66470.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1345  max mem: 15572
Epoch: [18]  [ 430/2809]  eta: 0:23:47  lr: 0.000032  min_lr: 0.000000  loss: 3.7982 (3.9037)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (66448.3341)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1571  max mem: 15572
[2025-01-15 23:05:14,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=344, lr=[3.1354948628075986e-07, 3.1354948628075986e-07, 4.4792783754394275e-07, 4.4792783754394275e-07, 6.398969107770612e-07, 6.398969107770612e-07, 9.141384439672303e-07, 9.141384439672303e-07, 1.3059120628103289e-06, 1.3059120628103289e-06, 1.865588661157613e-06, 1.865588661157613e-06, 2.66512665879659e-06, 2.66512665879659e-06, 3.807323798280843e-06, 3.807323798280843e-06, 5.4390339975440615e-06, 5.4390339975440615e-06, 7.770048567920089e-06, 7.770048567920089e-06, 1.1100069382742983e-05, 1.1100069382742983e-05, 1.5857241975347122e-05, 1.5857241975347122e-05, 2.2653202821924462e-05, 2.2653202821924462e-05, 3.236171831703495e-05, 3.236171831703495e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:05:14,132] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=28.476628981944817, CurrSamplesPerSec=32.76057768161901, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [ 440/2809]  eta: 0:23:38  lr: 0.000032  min_lr: 0.000000  loss: 3.7982 (3.9025)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (66427.6463)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1247  max mem: 15572
Epoch: [18]  [ 450/2809]  eta: 0:23:28  lr: 0.000032  min_lr: 0.000000  loss: 3.8054 (3.8994)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (66407.8758)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0956  max mem: 15572
Epoch: [18]  [ 460/2809]  eta: 0:23:21  lr: 0.000032  min_lr: 0.000000  loss: 3.8734 (3.9026)  class_acc: 0.2500 (0.2853)  loss_scale: 65536.0000 (66388.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.1232  max mem: 15572
Epoch: [18]  [ 470/2809]  eta: 0:23:11  lr: 0.000032  min_lr: 0.000000  loss: 4.0948 (3.9060)  class_acc: 0.2500 (0.2841)  loss_scale: 65536.0000 (66370.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1139  max mem: 15572
Epoch: [18]  [ 480/2809]  eta: 0:23:04  lr: 0.000032  min_lr: 0.000000  loss: 4.0231 (3.9063)  class_acc: 0.2500 (0.2842)  loss_scale: 65536.0000 (66353.4969)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1337  max mem: 15572
Epoch: [18]  [ 490/2809]  eta: 0:23:00  lr: 0.000032  min_lr: 0.000000  loss: 4.1501 (3.9085)  class_acc: 0.2083 (0.2830)  loss_scale: 65536.0000 (66336.8473)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1894  max mem: 15572
Epoch: [18]  [ 500/2809]  eta: 0:22:54  lr: 0.000032  min_lr: 0.000000  loss: 4.0377 (3.9089)  class_acc: 0.2083 (0.2832)  loss_scale: 65536.0000 (66320.8623)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1811  max mem: 15572
Epoch: [18]  [ 510/2809]  eta: 0:22:47  lr: 0.000032  min_lr: 0.000000  loss: 3.8944 (3.9045)  class_acc: 0.2917 (0.2843)  loss_scale: 65536.0000 (66305.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1495  max mem: 15572
[2025-01-15 23:05:58,933] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51076
[2025-01-15 23:05:58,933] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:05:58,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 520/2809]  eta: 0:22:42  lr: 0.000032  min_lr: 0.000000  loss: 3.9280 (3.9060)  class_acc: 0.2500 (0.2839)  loss_scale: 65536.0000 (65850.4722)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1582  max mem: 15572
Epoch: [18]  [ 530/2809]  eta: 0:22:34  lr: 0.000032  min_lr: 0.000000  loss: 4.1205 (3.9070)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (65227.4501)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1338  max mem: 15572
Epoch: [18]  [ 540/2809]  eta: 0:22:26  lr: 0.000032  min_lr: 0.000000  loss: 3.9850 (3.9082)  class_acc: 0.2917 (0.2837)  loss_scale: 32768.0000 (64627.4603)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1051  max mem: 15572
Epoch: [18]  [ 550/2809]  eta: 0:22:23  lr: 0.000032  min_lr: 0.000000  loss: 4.1213 (3.9107)  class_acc: 0.2500 (0.2835)  loss_scale: 32768.0000 (64049.2486)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1562  max mem: 15572
Epoch: [18]  [ 560/2809]  eta: 0:22:15  lr: 0.000032  min_lr: 0.000000  loss: 3.9065 (3.9097)  class_acc: 0.2500 (0.2833)  loss_scale: 32768.0000 (63491.6506)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1440  max mem: 15572
Epoch: [18]  [ 570/2809]  eta: 0:22:05  lr: 0.000032  min_lr: 0.000000  loss: 3.7478 (3.9086)  class_acc: 0.2500 (0.2831)  loss_scale: 32768.0000 (62953.5832)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0708  max mem: 15572
Epoch: [18]  [ 580/2809]  eta: 0:22:01  lr: 0.000032  min_lr: 0.000000  loss: 3.9447 (3.9086)  class_acc: 0.2500 (0.2836)  loss_scale: 32768.0000 (62434.0379)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1203  max mem: 15572
Epoch: [18]  [ 590/2809]  eta: 0:21:54  lr: 0.000032  min_lr: 0.000000  loss: 3.7930 (3.9040)  class_acc: 0.3333 (0.2850)  loss_scale: 32768.0000 (61932.0745)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1453  max mem: 15572
Epoch: [18]  [ 600/2809]  eta: 0:21:45  lr: 0.000032  min_lr: 0.000000  loss: 4.0964 (3.9091)  class_acc: 0.2917 (0.2844)  loss_scale: 32768.0000 (61446.8153)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0934  max mem: 15572
Epoch: [18]  [ 610/2809]  eta: 0:21:39  lr: 0.000032  min_lr: 0.000000  loss: 4.0739 (3.9091)  class_acc: 0.2917 (0.2852)  loss_scale: 32768.0000 (60977.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1085  max mem: 15572
Epoch: [18]  [ 620/2809]  eta: 0:21:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9371 (3.9078)  class_acc: 0.3333 (0.2850)  loss_scale: 32768.0000 (60523.1820)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1291  max mem: 15572
Epoch: [18]  [ 630/2809]  eta: 0:21:26  lr: 0.000032  min_lr: 0.000000  loss: 3.9371 (3.9071)  class_acc: 0.2500 (0.2852)  loss_scale: 32768.0000 (60083.3217)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1375  max mem: 15572
Epoch: [18]  [ 640/2809]  eta: 0:21:21  lr: 0.000032  min_lr: 0.000000  loss: 3.8473 (3.9059)  class_acc: 0.2917 (0.2854)  loss_scale: 32768.0000 (59657.1856)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1608  max mem: 15572
[2025-01-15 23:07:12,486] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:07:12,487] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 650/2809]  eta: 0:21:12  lr: 0.000032  min_lr: 0.000000  loss: 3.9363 (3.9066)  class_acc: 0.2500 (0.2858)  loss_scale: 32768.0000 (59646.8203)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1367  max mem: 15572
Epoch: [18]  [ 660/2809]  eta: 0:21:06  lr: 0.000032  min_lr: 0.000000  loss: 3.9016 (3.9052)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (59735.9153)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1220  max mem: 15572
Epoch: [18]  [ 670/2809]  eta: 0:21:03  lr: 0.000032  min_lr: 0.000000  loss: 3.8278 (3.9048)  class_acc: 0.2917 (0.2855)  loss_scale: 65536.0000 (59822.3547)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1882  max mem: 15572
Epoch: [18]  [ 680/2809]  eta: 0:20:52  lr: 0.000032  min_lr: 0.000000  loss: 3.8736 (3.9029)  class_acc: 0.2500 (0.2854)  loss_scale: 65536.0000 (59906.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1166  max mem: 15572
Epoch: [18]  [ 690/2809]  eta: 0:20:45  lr: 0.000032  min_lr: 0.000000  loss: 3.7760 (3.8985)  class_acc: 0.2917 (0.2865)  loss_scale: 65536.0000 (59987.7279)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0577  max mem: 15572
Epoch: [18]  [ 700/2809]  eta: 0:20:39  lr: 0.000032  min_lr: 0.000000  loss: 3.7760 (3.8973)  class_acc: 0.2917 (0.2869)  loss_scale: 65536.0000 (60066.8759)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1337  max mem: 15572
Epoch: [18]  [ 710/2809]  eta: 0:20:32  lr: 0.000032  min_lr: 0.000000  loss: 4.0317 (3.8996)  class_acc: 0.2917 (0.2864)  loss_scale: 65536.0000 (60143.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1367  max mem: 15572
Epoch: [18]  [ 720/2809]  eta: 0:20:26  lr: 0.000032  min_lr: 0.000000  loss: 3.7812 (3.8975)  class_acc: 0.2500 (0.2865)  loss_scale: 65536.0000 (60218.5853)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1350  max mem: 15572
Epoch: [18]  [ 730/2809]  eta: 0:20:16  lr: 0.000032  min_lr: 0.000000  loss: 3.7353 (3.8993)  class_acc: 0.2083 (0.2859)  loss_scale: 65536.0000 (60291.3269)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0746  max mem: 15572
Epoch: [18]  [ 740/2809]  eta: 0:20:07  lr: 0.000032  min_lr: 0.000000  loss: 4.0315 (3.8992)  class_acc: 0.2500 (0.2855)  loss_scale: 65536.0000 (60362.1053)  weight_decay: 0.0500 (0.0500)  time: 0.4456  data: 0.0108  max mem: 15572
Epoch: [18]  [ 750/2809]  eta: 0:19:59  lr: 0.000032  min_lr: 0.000000  loss: 4.0315 (3.8996)  class_acc: 0.2500 (0.2856)  loss_scale: 65536.0000 (60430.9987)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0566  max mem: 15572
Epoch: [18]  [ 760/2809]  eta: 0:19:54  lr: 0.000032  min_lr: 0.000000  loss: 3.7198 (3.8947)  class_acc: 0.3333 (0.2866)  loss_scale: 65536.0000 (60498.0815)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1458  max mem: 15572
Epoch: [18]  [ 770/2809]  eta: 0:19:49  lr: 0.000032  min_lr: 0.000000  loss: 3.7471 (3.8960)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (60563.4241)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1931  max mem: 15572
[2025-01-15 23:08:23,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:08:23,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:08:24,053] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51335
[2025-01-15 23:08:24,053] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:08:24,053] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [ 780/2809]  eta: 0:19:43  lr: 0.000032  min_lr: 0.000000  loss: 3.8456 (3.8952)  class_acc: 0.2917 (0.2861)  loss_scale: 65536.0000 (60794.9193)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1542  max mem: 15572
Epoch: [18]  [ 790/2809]  eta: 0:19:36  lr: 0.000032  min_lr: 0.000000  loss: 3.9160 (3.8942)  class_acc: 0.2917 (0.2863)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1035  max mem: 15572
Epoch: [18]  [ 800/2809]  eta: 0:19:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9491 (3.8955)  class_acc: 0.3333 (0.2866)  loss_scale: 65536.0000 (60913.2984)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1436  max mem: 15572
Epoch: [18]  [ 810/2809]  eta: 0:19:28  lr: 0.000032  min_lr: 0.000000  loss: 4.0292 (3.8972)  class_acc: 0.2917 (0.2863)  loss_scale: 65536.0000 (60970.2984)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.2050  max mem: 15572
Epoch: [18]  [ 820/2809]  eta: 0:19:20  lr: 0.000032  min_lr: 0.000000  loss: 3.8726 (3.8941)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (61025.9099)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1423  max mem: 15572
Epoch: [18]  [ 830/2809]  eta: 0:19:13  lr: 0.000032  min_lr: 0.000000  loss: 3.8063 (3.8931)  class_acc: 0.3333 (0.2878)  loss_scale: 65536.0000 (61080.1829)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0840  max mem: 15572
Epoch: [18]  [ 840/2809]  eta: 0:19:06  lr: 0.000032  min_lr: 0.000000  loss: 3.8611 (3.8929)  class_acc: 0.3333 (0.2880)  loss_scale: 65536.0000 (61133.1653)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0922  max mem: 15572
Epoch: [18]  [ 850/2809]  eta: 0:18:59  lr: 0.000032  min_lr: 0.000000  loss: 3.9551 (3.8930)  class_acc: 0.2917 (0.2885)  loss_scale: 65536.0000 (61184.9025)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.1010  max mem: 15572
Epoch: [18]  [ 860/2809]  eta: 0:18:55  lr: 0.000032  min_lr: 0.000000  loss: 4.0129 (3.8947)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (61235.4379)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1542  max mem: 15572
[2025-01-15 23:09:18,492] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51432
[2025-01-15 23:09:18,493] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:09:18,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 870/2809]  eta: 0:18:48  lr: 0.000032  min_lr: 0.000000  loss: 4.0129 (3.8965)  class_acc: 0.2083 (0.2875)  loss_scale: 65536.0000 (61247.1917)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1438  max mem: 15572
Epoch: [18]  [ 880/2809]  eta: 0:18:43  lr: 0.000032  min_lr: 0.000000  loss: 4.1118 (3.8984)  class_acc: 0.2083 (0.2870)  loss_scale: 32768.0000 (60923.9319)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1448  max mem: 15572
Epoch: [18]  [ 890/2809]  eta: 0:18:37  lr: 0.000032  min_lr: 0.000000  loss: 4.0892 (3.9000)  class_acc: 0.2917 (0.2867)  loss_scale: 32768.0000 (60607.9282)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1622  max mem: 15572
Epoch: [18]  [ 900/2809]  eta: 0:18:29  lr: 0.000032  min_lr: 0.000000  loss: 3.8200 (3.8994)  class_acc: 0.2917 (0.2868)  loss_scale: 32768.0000 (60298.9390)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0845  max mem: 15572
Epoch: [18]  [ 910/2809]  eta: 0:18:22  lr: 0.000032  min_lr: 0.000000  loss: 3.8200 (3.9003)  class_acc: 0.3333 (0.2868)  loss_scale: 32768.0000 (59996.7333)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0726  max mem: 15572
Epoch: [18]  [ 920/2809]  eta: 0:18:16  lr: 0.000032  min_lr: 0.000000  loss: 3.8275 (3.8960)  class_acc: 0.2917 (0.2873)  loss_scale: 32768.0000 (59701.0901)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1182  max mem: 15572
Epoch: [18]  [ 930/2809]  eta: 0:18:11  lr: 0.000032  min_lr: 0.000000  loss: 3.8041 (3.8959)  class_acc: 0.2500 (0.2873)  loss_scale: 32768.0000 (59411.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1321  max mem: 15572
Epoch: [18]  [ 940/2809]  eta: 0:18:05  lr: 0.000032  min_lr: 0.000000  loss: 4.0379 (3.8973)  class_acc: 0.2083 (0.2870)  loss_scale: 32768.0000 (59128.6546)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1487  max mem: 15572
Epoch: [18]  [ 950/2809]  eta: 0:18:02  lr: 0.000032  min_lr: 0.000000  loss: 4.0065 (3.8961)  class_acc: 0.2500 (0.2872)  loss_scale: 32768.0000 (58851.4658)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.2191  max mem: 15572
Epoch: [18]  [ 960/2809]  eta: 0:17:54  lr: 0.000032  min_lr: 0.000000  loss: 3.8487 (3.8949)  class_acc: 0.2917 (0.2875)  loss_scale: 32768.0000 (58580.0458)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1343  max mem: 15572
Epoch: [18]  [ 970/2809]  eta: 0:17:49  lr: 0.000032  min_lr: 0.000000  loss: 3.8116 (3.8953)  class_acc: 0.2917 (0.2876)  loss_scale: 32768.0000 (58314.2163)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.0951  max mem: 15572
Epoch: [18]  [ 980/2809]  eta: 0:17:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9179 (3.8953)  class_acc: 0.2500 (0.2878)  loss_scale: 32768.0000 (58053.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1613  max mem: 15572
Epoch: [18]  [ 990/2809]  eta: 0:17:37  lr: 0.000032  min_lr: 0.000000  loss: 3.9179 (3.8949)  class_acc: 0.2500 (0.2874)  loss_scale: 32768.0000 (57798.6519)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1276  max mem: 15572
[2025-01-15 23:10:31,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:10:31,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1000/2809]  eta: 0:17:30  lr: 0.000032  min_lr: 0.000000  loss: 3.8585 (3.8966)  class_acc: 0.2917 (0.2869)  loss_scale: 32768.0000 (57614.0659)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1115  max mem: 15572
Epoch: [18]  [1010/2809]  eta: 0:17:24  lr: 0.000032  min_lr: 0.000000  loss: 3.7802 (3.8942)  class_acc: 0.2917 (0.2872)  loss_scale: 65536.0000 (57692.4233)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1190  max mem: 15572
Epoch: [18]  [1020/2809]  eta: 0:17:18  lr: 0.000032  min_lr: 0.000000  loss: 3.7291 (3.8943)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (57769.2458)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1205  max mem: 15572
Epoch: [18]  [1030/2809]  eta: 0:17:11  lr: 0.000032  min_lr: 0.000000  loss: 3.9403 (3.8954)  class_acc: 0.2500 (0.2872)  loss_scale: 65536.0000 (57844.5781)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0924  max mem: 15572
Epoch: [18]  [1040/2809]  eta: 0:17:06  lr: 0.000032  min_lr: 0.000000  loss: 4.0490 (3.8965)  class_acc: 0.2500 (0.2868)  loss_scale: 65536.0000 (57918.4630)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1443  max mem: 15572
Epoch: [18]  [1050/2809]  eta: 0:17:01  lr: 0.000032  min_lr: 0.000000  loss: 4.0056 (3.8976)  class_acc: 0.2083 (0.2864)  loss_scale: 65536.0000 (57990.9420)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1933  max mem: 15572
Epoch: [18]  [1060/2809]  eta: 0:16:55  lr: 0.000032  min_lr: 0.000000  loss: 3.9115 (3.8974)  class_acc: 0.2500 (0.2865)  loss_scale: 65536.0000 (58062.0547)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1722  max mem: 15572
Epoch: [18]  [1070/2809]  eta: 0:16:49  lr: 0.000032  min_lr: 0.000000  loss: 3.8401 (3.8959)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (58131.8394)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1515  max mem: 15572
Epoch: [18]  [1080/2809]  eta: 0:16:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9101 (3.8971)  class_acc: 0.2500 (0.2864)  loss_scale: 65536.0000 (58200.3330)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1008  max mem: 15572
Epoch: [18]  [1090/2809]  eta: 0:16:38  lr: 0.000032  min_lr: 0.000000  loss: 3.8680 (3.8954)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (58267.5710)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1392  max mem: 15572
Epoch: [18]  [1100/2809]  eta: 0:16:31  lr: 0.000032  min_lr: 0.000000  loss: 3.8408 (3.8959)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (58333.5876)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1218  max mem: 15572
Epoch: [18]  [1110/2809]  eta: 0:16:24  lr: 0.000032  min_lr: 0.000000  loss: 3.9171 (3.8969)  class_acc: 0.2917 (0.2870)  loss_scale: 65536.0000 (58398.4158)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0406  max mem: 15572
Epoch: [18]  [1120/2809]  eta: 0:16:18  lr: 0.000032  min_lr: 0.000000  loss: 4.0865 (3.8965)  class_acc: 0.2083 (0.2870)  loss_scale: 65536.0000 (58462.0874)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0650  max mem: 15572
[2025-01-15 23:11:44,505] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:11:44,506] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:11:45,429] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51691
[2025-01-15 23:11:45,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:11:45,430] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1130/2809]  eta: 0:16:13  lr: 0.000032  min_lr: 0.000000  loss: 3.9100 (3.8968)  class_acc: 0.2083 (0.2865)  loss_scale: 65536.0000 (58640.5234)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1093  max mem: 15572
Epoch: [18]  [1140/2809]  eta: 0:16:07  lr: 0.000032  min_lr: 0.000000  loss: 3.7843 (3.8941)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (58700.9571)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1483  max mem: 15572
[2025-01-15 23:11:58,992] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51712
[2025-01-15 23:11:58,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:11:58,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1150/2809]  eta: 0:16:01  lr: 0.000032  min_lr: 0.000000  loss: 3.6982 (3.8926)  class_acc: 0.2917 (0.2874)  loss_scale: 65536.0000 (58731.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1480  max mem: 15572
Epoch: [18]  [1160/2809]  eta: 0:15:55  lr: 0.000032  min_lr: 0.000000  loss: 3.7899 (3.8931)  class_acc: 0.2917 (0.2876)  loss_scale: 32768.0000 (58508.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1447  max mem: 15572
Epoch: [18]  [1170/2809]  eta: 0:15:50  lr: 0.000032  min_lr: 0.000000  loss: 3.8971 (3.8935)  class_acc: 0.2917 (0.2878)  loss_scale: 32768.0000 (58288.4236)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1824  max mem: 15572
Epoch: [18]  [1180/2809]  eta: 0:15:43  lr: 0.000032  min_lr: 0.000000  loss: 3.9605 (3.8939)  class_acc: 0.2917 (0.2878)  loss_scale: 32768.0000 (58072.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1251  max mem: 15572
Epoch: [18]  [1190/2809]  eta: 0:15:37  lr: 0.000032  min_lr: 0.000000  loss: 3.9605 (3.8952)  class_acc: 0.2500 (0.2872)  loss_scale: 32768.0000 (57859.8690)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0923  max mem: 15572
Epoch: [18]  [1200/2809]  eta: 0:15:32  lr: 0.000032  min_lr: 0.000000  loss: 3.8661 (3.8939)  class_acc: 0.2500 (0.2873)  loss_scale: 32768.0000 (57650.9442)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1789  max mem: 15572
Epoch: [18]  [1210/2809]  eta: 0:15:26  lr: 0.000032  min_lr: 0.000000  loss: 3.6036 (3.8922)  class_acc: 0.2500 (0.2874)  loss_scale: 32768.0000 (57445.4699)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1847  max mem: 15572
Epoch: [18]  [1220/2809]  eta: 0:15:19  lr: 0.000032  min_lr: 0.000000  loss: 3.8268 (3.8922)  class_acc: 0.2500 (0.2876)  loss_scale: 32768.0000 (57243.3612)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0934  max mem: 15572
Epoch: [18]  [1230/2809]  eta: 0:15:13  lr: 0.000032  min_lr: 0.000000  loss: 3.8268 (3.8915)  class_acc: 0.2917 (0.2879)  loss_scale: 32768.0000 (57044.5361)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0780  max mem: 15572
Epoch: [18]  [1240/2809]  eta: 0:15:08  lr: 0.000032  min_lr: 0.000000  loss: 3.8087 (3.8906)  class_acc: 0.3333 (0.2883)  loss_scale: 32768.0000 (56848.9154)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1589  max mem: 15572
Epoch: [18]  [1250/2809]  eta: 0:15:02  lr: 0.000032  min_lr: 0.000000  loss: 3.8484 (3.8904)  class_acc: 0.3333 (0.2886)  loss_scale: 32768.0000 (56656.4221)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1779  max mem: 15572
Epoch: [18]  [1260/2809]  eta: 0:14:56  lr: 0.000032  min_lr: 0.000000  loss: 3.8484 (3.8909)  class_acc: 0.2500 (0.2880)  loss_scale: 32768.0000 (56466.9818)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1210  max mem: 15572
Epoch: [18]  [1270/2809]  eta: 0:14:49  lr: 0.000032  min_lr: 0.000000  loss: 3.8881 (3.8908)  class_acc: 0.2500 (0.2881)  loss_scale: 32768.0000 (56280.5224)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0574  max mem: 15572
[2025-01-15 23:13:10,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:13:10,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1280/2809]  eta: 0:14:43  lr: 0.000032  min_lr: 0.000000  loss: 3.8881 (3.8915)  class_acc: 0.2917 (0.2882)  loss_scale: 32768.0000 (56148.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0879  max mem: 15572
Epoch: [18]  [1290/2809]  eta: 0:14:36  lr: 0.000032  min_lr: 0.000000  loss: 3.8659 (3.8914)  class_acc: 0.3333 (0.2881)  loss_scale: 65536.0000 (56220.8521)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0678  max mem: 15572
Epoch: [18]  [1300/2809]  eta: 0:14:29  lr: 0.000032  min_lr: 0.000000  loss: 3.8461 (3.8911)  class_acc: 0.3333 (0.2884)  loss_scale: 65536.0000 (56292.4520)  weight_decay: 0.0500 (0.0500)  time: 0.4643  data: 0.0010  max mem: 15572
Epoch: [18]  [1310/2809]  eta: 0:14:23  lr: 0.000032  min_lr: 0.000000  loss: 3.8317 (3.8904)  class_acc: 0.3333 (0.2887)  loss_scale: 65536.0000 (56362.9596)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0814  max mem: 15572
Epoch: [18]  [1320/2809]  eta: 0:14:17  lr: 0.000032  min_lr: 0.000000  loss: 3.7494 (3.8899)  class_acc: 0.2917 (0.2886)  loss_scale: 65536.0000 (56432.3997)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1486  max mem: 15572
Epoch: [18]  [1330/2809]  eta: 0:14:12  lr: 0.000032  min_lr: 0.000000  loss: 4.0432 (3.8916)  class_acc: 0.2500 (0.2882)  loss_scale: 65536.0000 (56500.7964)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1555  max mem: 15572
Epoch: [18]  [1340/2809]  eta: 0:14:07  lr: 0.000032  min_lr: 0.000000  loss: 4.0720 (3.8918)  class_acc: 0.2500 (0.2880)  loss_scale: 65536.0000 (56568.1730)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1897  max mem: 15572
Epoch: [18]  [1350/2809]  eta: 0:14:01  lr: 0.000032  min_lr: 0.000000  loss: 3.9389 (3.8920)  class_acc: 0.2500 (0.2879)  loss_scale: 65536.0000 (56634.5522)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1859  max mem: 15572
Epoch: [18]  [1360/2809]  eta: 0:13:56  lr: 0.000032  min_lr: 0.000000  loss: 3.9455 (3.8925)  class_acc: 0.2500 (0.2877)  loss_scale: 65536.0000 (56699.9559)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1981  max mem: 15572
Epoch: [18]  [1370/2809]  eta: 0:13:50  lr: 0.000032  min_lr: 0.000000  loss: 4.0268 (3.8940)  class_acc: 0.2083 (0.2873)  loss_scale: 65536.0000 (56764.4055)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1693  max mem: 15572
Epoch: [18]  [1380/2809]  eta: 0:13:45  lr: 0.000032  min_lr: 0.000000  loss: 3.9829 (3.8948)  class_acc: 0.2083 (0.2872)  loss_scale: 65536.0000 (56827.9218)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1855  max mem: 15572
Epoch: [18]  [1390/2809]  eta: 0:13:39  lr: 0.000032  min_lr: 0.000000  loss: 3.9447 (3.8945)  class_acc: 0.2500 (0.2874)  loss_scale: 65536.0000 (56890.5248)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1549  max mem: 15572
Epoch: [18]  [1400/2809]  eta: 0:13:32  lr: 0.000032  min_lr: 0.000000  loss: 3.9447 (3.8948)  class_acc: 0.2500 (0.2873)  loss_scale: 65536.0000 (56952.2341)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0653  max mem: 15572
[2025-01-15 23:14:24,402] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:14:24,402] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:14:25,687] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51972
[2025-01-15 23:14:25,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:14:25,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1410/2809]  eta: 0:13:26  lr: 0.000032  min_lr: 0.000000  loss: 3.9518 (3.8955)  class_acc: 0.2500 (0.2871)  loss_scale: 65536.0000 (57152.4082)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.0901  max mem: 15572
Epoch: [18]  [1420/2809]  eta: 0:13:20  lr: 0.000032  min_lr: 0.000000  loss: 3.9465 (3.8957)  class_acc: 0.2500 (0.2872)  loss_scale: 65536.0000 (57211.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0634  max mem: 15572
Epoch: [18]  [1430/2809]  eta: 0:13:14  lr: 0.000032  min_lr: 0.000000  loss: 3.9079 (3.8955)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (57269.5793)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0741  max mem: 15572
[2025-01-15 23:14:40,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=350, lr=[3.068017621427496e-07, 3.068017621427496e-07, 4.382882316324995e-07, 4.382882316324995e-07, 6.26126045189285e-07, 6.26126045189285e-07, 8.944657788418358e-07, 8.944657788418358e-07, 1.277808255488337e-06, 1.277808255488337e-06, 1.8254403649833386e-06, 1.8254403649833386e-06, 2.607771949976198e-06, 2.607771949976198e-06, 3.7253884999659977e-06, 3.7253884999659977e-06, 5.321983571379997e-06, 5.321983571379997e-06, 7.602833673399996e-06, 7.602833673399996e-06, 1.0861190961999994e-05, 1.0861190961999994e-05, 1.5515987088571423e-05, 1.5515987088571423e-05, 2.2165695840816318e-05, 2.2165695840816318e-05, 3.166527977259474e-05, 3.166527977259474e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:14:40,788] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=28.48644922347241, CurrSamplesPerSec=32.55341450400194, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [1440/2809]  eta: 0:13:08  lr: 0.000032  min_lr: 0.000000  loss: 3.9556 (3.8963)  class_acc: 0.2917 (0.2872)  loss_scale: 65536.0000 (57326.9452)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1425  max mem: 15572
Epoch: [18]  [1450/2809]  eta: 0:13:02  lr: 0.000032  min_lr: 0.000000  loss: 3.9555 (3.8947)  class_acc: 0.2917 (0.2876)  loss_scale: 65536.0000 (57383.5203)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1587  max mem: 15572
Epoch: [18]  [1460/2809]  eta: 0:12:56  lr: 0.000032  min_lr: 0.000000  loss: 3.6641 (3.8938)  class_acc: 0.3333 (0.2877)  loss_scale: 65536.0000 (57439.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1330  max mem: 15572
Epoch: [18]  [1470/2809]  eta: 0:12:51  lr: 0.000032  min_lr: 0.000000  loss: 3.7955 (3.8933)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (57494.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1473  max mem: 15572
Epoch: [18]  [1480/2809]  eta: 0:12:44  lr: 0.000032  min_lr: 0.000000  loss: 3.9916 (3.8947)  class_acc: 0.2500 (0.2875)  loss_scale: 65536.0000 (57548.6617)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.0916  max mem: 15572
Epoch: [18]  [1490/2809]  eta: 0:12:38  lr: 0.000032  min_lr: 0.000000  loss: 3.8071 (3.8920)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (57602.2321)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0455  max mem: 15572
Epoch: [18]  [1500/2809]  eta: 0:12:33  lr: 0.000032  min_lr: 0.000000  loss: 3.7911 (3.8929)  class_acc: 0.2917 (0.2876)  loss_scale: 65536.0000 (57655.0886)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1351  max mem: 15572
Epoch: [18]  [1510/2809]  eta: 0:12:26  lr: 0.000032  min_lr: 0.000000  loss: 3.9272 (3.8929)  class_acc: 0.2500 (0.2876)  loss_scale: 65536.0000 (57707.2455)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1114  max mem: 15572
Epoch: [18]  [1520/2809]  eta: 0:12:21  lr: 0.000032  min_lr: 0.000000  loss: 3.8656 (3.8941)  class_acc: 0.2500 (0.2872)  loss_scale: 65536.0000 (57758.7166)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1014  max mem: 15572
Epoch: [18]  [1530/2809]  eta: 0:12:15  lr: 0.000032  min_lr: 0.000000  loss: 3.8577 (3.8932)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (57809.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1315  max mem: 15572
[2025-01-15 23:15:37,853] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:15:37,854] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [1540/2809]  eta: 0:12:09  lr: 0.000032  min_lr: 0.000000  loss: 3.8577 (3.8930)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57944.7112)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1460  max mem: 15572
[2025-01-15 23:15:42,790] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52109
[2025-01-15 23:15:42,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:15:42,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1550/2809]  eta: 0:12:04  lr: 0.000032  min_lr: 0.000000  loss: 3.8828 (3.8930)  class_acc: 0.2500 (0.2873)  loss_scale: 65536.0000 (58247.1799)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1604  max mem: 15572
Epoch: [18]  [1560/2809]  eta: 0:11:58  lr: 0.000032  min_lr: 0.000000  loss: 3.9723 (3.8941)  class_acc: 0.2500 (0.2871)  loss_scale: 65536.0000 (58293.8732)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1478  max mem: 15572
Epoch: [18]  [1570/2809]  eta: 0:11:52  lr: 0.000032  min_lr: 0.000000  loss: 4.2025 (3.8965)  class_acc: 0.1667 (0.2863)  loss_scale: 65536.0000 (58339.9720)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1266  max mem: 15572
Epoch: [18]  [1580/2809]  eta: 0:11:46  lr: 0.000032  min_lr: 0.000000  loss: 4.1071 (3.8970)  class_acc: 0.1667 (0.2862)  loss_scale: 65536.0000 (58385.4877)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1171  max mem: 15572
Epoch: [18]  [1590/2809]  eta: 0:11:40  lr: 0.000032  min_lr: 0.000000  loss: 3.9391 (3.8973)  class_acc: 0.2917 (0.2862)  loss_scale: 65536.0000 (58430.4312)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1123  max mem: 15572
Epoch: [18]  [1600/2809]  eta: 0:11:35  lr: 0.000032  min_lr: 0.000000  loss: 4.0897 (3.8980)  class_acc: 0.2500 (0.2859)  loss_scale: 65536.0000 (58474.8132)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1686  max mem: 15572
Epoch: [18]  [1610/2809]  eta: 0:11:29  lr: 0.000032  min_lr: 0.000000  loss: 4.0118 (3.8986)  class_acc: 0.2500 (0.2858)  loss_scale: 65536.0000 (58518.6443)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1658  max mem: 15572
Epoch: [18]  [1620/2809]  eta: 0:11:23  lr: 0.000032  min_lr: 0.000000  loss: 3.9972 (3.8986)  class_acc: 0.2500 (0.2858)  loss_scale: 65536.0000 (58561.9346)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1042  max mem: 15572
Epoch: [18]  [1630/2809]  eta: 0:11:17  lr: 0.000032  min_lr: 0.000000  loss: 3.9250 (3.8986)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (58604.6941)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1081  max mem: 15572
Epoch: [18]  [1640/2809]  eta: 0:11:11  lr: 0.000032  min_lr: 0.000000  loss: 3.8766 (3.8980)  class_acc: 0.2917 (0.2861)  loss_scale: 65536.0000 (58646.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0852  max mem: 15572
Epoch: [18]  [1650/2809]  eta: 0:11:05  lr: 0.000032  min_lr: 0.000000  loss: 3.9348 (3.8991)  class_acc: 0.2500 (0.2859)  loss_scale: 65536.0000 (58688.6590)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.0896  max mem: 15572
Epoch: [18]  [1660/2809]  eta: 0:10:59  lr: 0.000032  min_lr: 0.000000  loss: 4.1572 (3.8999)  class_acc: 0.2083 (0.2858)  loss_scale: 65536.0000 (58729.8832)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1119  max mem: 15572
Epoch: [18]  [1670/2809]  eta: 0:10:54  lr: 0.000032  min_lr: 0.000000  loss: 4.0448 (3.8995)  class_acc: 0.2917 (0.2859)  loss_scale: 65536.0000 (58770.6140)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1351  max mem: 15572
[2025-01-15 23:16:54,087] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:16:54,088] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:16:55,386] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52240
[2025-01-15 23:16:55,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:16:55,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [1680/2809]  eta: 0:10:48  lr: 0.000031  min_lr: 0.000000  loss: 3.9321 (3.8990)  class_acc: 0.2917 (0.2862)  loss_scale: 65536.0000 (58888.8328)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1265  max mem: 15572
Epoch: [18]  [1690/2809]  eta: 0:10:43  lr: 0.000031  min_lr: 0.000000  loss: 3.8063 (3.8996)  class_acc: 0.3333 (0.2863)  loss_scale: 65536.0000 (58928.1419)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.2099  max mem: 15572
Epoch: [18]  [1700/2809]  eta: 0:10:37  lr: 0.000031  min_lr: 0.000000  loss: 3.9360 (3.8981)  class_acc: 0.3750 (0.2870)  loss_scale: 65536.0000 (58966.9888)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1938  max mem: 15572
Epoch: [18]  [1710/2809]  eta: 0:10:31  lr: 0.000031  min_lr: 0.000000  loss: 3.8731 (3.8984)  class_acc: 0.3333 (0.2869)  loss_scale: 65536.0000 (59005.3816)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1360  max mem: 15572
Epoch: [18]  [1720/2809]  eta: 0:10:26  lr: 0.000031  min_lr: 0.000000  loss: 3.8731 (3.8987)  class_acc: 0.2917 (0.2869)  loss_scale: 65536.0000 (59043.3283)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1808  max mem: 15572
Epoch: [18]  [1730/2809]  eta: 0:10:21  lr: 0.000031  min_lr: 0.000000  loss: 3.8419 (3.8980)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (59080.8365)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.2202  max mem: 15572
Epoch: [18]  [1740/2809]  eta: 0:10:14  lr: 0.000031  min_lr: 0.000000  loss: 3.8688 (3.8986)  class_acc: 0.3333 (0.2873)  loss_scale: 65536.0000 (59117.9138)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1337  max mem: 15572
Epoch: [18]  [1750/2809]  eta: 0:10:08  lr: 0.000031  min_lr: 0.000000  loss: 4.0203 (3.8992)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (59154.5677)  weight_decay: 0.0500 (0.0500)  time: 0.4548  data: 0.0008  max mem: 15572
Epoch: [18]  [1760/2809]  eta: 0:10:01  lr: 0.000031  min_lr: 0.000000  loss: 4.0070 (3.8990)  class_acc: 0.3333 (0.2876)  loss_scale: 65536.0000 (59190.8052)  weight_decay: 0.0500 (0.0500)  time: 0.4722  data: 0.0099  max mem: 15572
Epoch: [18]  [1770/2809]  eta: 0:09:56  lr: 0.000031  min_lr: 0.000000  loss: 4.0589 (3.8999)  class_acc: 0.3333 (0.2874)  loss_scale: 65536.0000 (59226.6335)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1136  max mem: 15572
[2025-01-15 23:17:50,306] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52335
[2025-01-15 23:17:50,307] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:17:50,307] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [1780/2809]  eta: 0:09:51  lr: 0.000031  min_lr: 0.000000  loss: 3.9997 (3.8985)  class_acc: 0.2500 (0.2875)  loss_scale: 65536.0000 (59114.8703)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1916  max mem: 15572
Epoch: [18]  [1790/2809]  eta: 0:09:45  lr: 0.000031  min_lr: 0.000000  loss: 3.4576 (3.8968)  class_acc: 0.2917 (0.2877)  loss_scale: 32768.0000 (58967.7633)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1349  max mem: 15572
Epoch: [18]  [1800/2809]  eta: 0:09:39  lr: 0.000031  min_lr: 0.000000  loss: 3.6369 (3.8965)  class_acc: 0.2917 (0.2878)  loss_scale: 32768.0000 (58822.2898)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1471  max mem: 15572
Epoch: [18]  [1810/2809]  eta: 0:09:33  lr: 0.000031  min_lr: 0.000000  loss: 3.9217 (3.8981)  class_acc: 0.2083 (0.2874)  loss_scale: 32768.0000 (58678.4230)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1552  max mem: 15572
Epoch: [18]  [1820/2809]  eta: 0:09:27  lr: 0.000031  min_lr: 0.000000  loss: 3.9734 (3.8980)  class_acc: 0.2500 (0.2875)  loss_scale: 32768.0000 (58536.1362)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0821  max mem: 15572
Epoch: [18]  [1830/2809]  eta: 0:09:21  lr: 0.000031  min_lr: 0.000000  loss: 3.8407 (3.8981)  class_acc: 0.2917 (0.2876)  loss_scale: 32768.0000 (58395.4036)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0902  max mem: 15572
Epoch: [18]  [1840/2809]  eta: 0:09:15  lr: 0.000031  min_lr: 0.000000  loss: 3.8376 (3.8982)  class_acc: 0.2917 (0.2875)  loss_scale: 32768.0000 (58256.1999)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1056  max mem: 15572
Epoch: [18]  [1850/2809]  eta: 0:09:10  lr: 0.000031  min_lr: 0.000000  loss: 3.9471 (3.8989)  class_acc: 0.2500 (0.2875)  loss_scale: 32768.0000 (58118.5003)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1562  max mem: 15572
Epoch: [18]  [1860/2809]  eta: 0:09:04  lr: 0.000031  min_lr: 0.000000  loss: 4.0955 (3.9000)  class_acc: 0.2500 (0.2872)  loss_scale: 32768.0000 (57982.2805)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.2105  max mem: 15572
Epoch: [18]  [1870/2809]  eta: 0:08:59  lr: 0.000031  min_lr: 0.000000  loss: 4.1038 (3.9000)  class_acc: 0.2083 (0.2869)  loss_scale: 32768.0000 (57847.5168)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1690  max mem: 15572
Epoch: [18]  [1880/2809]  eta: 0:08:53  lr: 0.000031  min_lr: 0.000000  loss: 3.8785 (3.8990)  class_acc: 0.2917 (0.2873)  loss_scale: 32768.0000 (57714.1861)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1671  max mem: 15572
Epoch: [18]  [1890/2809]  eta: 0:08:48  lr: 0.000031  min_lr: 0.000000  loss: 3.8902 (3.8993)  class_acc: 0.2917 (0.2873)  loss_scale: 32768.0000 (57582.2655)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1477  max mem: 15572
Epoch: [18]  [1900/2809]  eta: 0:08:42  lr: 0.000031  min_lr: 0.000000  loss: 3.9766 (3.9001)  class_acc: 0.2083 (0.2867)  loss_scale: 32768.0000 (57451.7328)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1279  max mem: 15572
[2025-01-15 23:19:05,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:19:05,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [1910/2809]  eta: 0:08:36  lr: 0.000031  min_lr: 0.000000  loss: 4.0664 (3.9012)  class_acc: 0.1667 (0.2864)  loss_scale: 32768.0000 (57476.8896)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0994  max mem: 15572
Epoch: [18]  [1920/2809]  eta: 0:08:30  lr: 0.000031  min_lr: 0.000000  loss: 3.9298 (3.9010)  class_acc: 0.2083 (0.2864)  loss_scale: 65536.0000 (57518.8423)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1147  max mem: 15572
Epoch: [18]  [1930/2809]  eta: 0:08:24  lr: 0.000031  min_lr: 0.000000  loss: 3.8157 (3.9001)  class_acc: 0.2500 (0.2867)  loss_scale: 65536.0000 (57560.3604)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1192  max mem: 15572
Epoch: [18]  [1940/2809]  eta: 0:08:19  lr: 0.000031  min_lr: 0.000000  loss: 3.6528 (3.8996)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (57601.4508)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1249  max mem: 15572
Epoch: [18]  [1950/2809]  eta: 0:08:13  lr: 0.000031  min_lr: 0.000000  loss: 4.0019 (3.9002)  class_acc: 0.2500 (0.2866)  loss_scale: 65536.0000 (57642.1199)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1469  max mem: 15572
Epoch: [18]  [1960/2809]  eta: 0:08:07  lr: 0.000031  min_lr: 0.000000  loss: 4.0295 (3.8998)  class_acc: 0.2500 (0.2867)  loss_scale: 65536.0000 (57682.3743)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0892  max mem: 15572
Epoch: [18]  [1970/2809]  eta: 0:08:01  lr: 0.000031  min_lr: 0.000000  loss: 4.0108 (3.9002)  class_acc: 0.2500 (0.2866)  loss_scale: 65536.0000 (57722.2202)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0726  max mem: 15572
Epoch: [18]  [1980/2809]  eta: 0:07:55  lr: 0.000031  min_lr: 0.000000  loss: 4.0108 (3.9003)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (57761.6638)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0698  max mem: 15572
Epoch: [18]  [1990/2809]  eta: 0:07:50  lr: 0.000031  min_lr: 0.000000  loss: 4.0368 (3.9011)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (57800.7112)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1616  max mem: 15572
Epoch: [18]  [2000/2809]  eta: 0:07:44  lr: 0.000031  min_lr: 0.000000  loss: 4.1676 (3.9024)  class_acc: 0.2500 (0.2862)  loss_scale: 65536.0000 (57839.3683)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.2144  max mem: 15572
Epoch: [18]  [2010/2809]  eta: 0:07:38  lr: 0.000031  min_lr: 0.000000  loss: 4.2128 (3.9030)  class_acc: 0.2500 (0.2862)  loss_scale: 65536.0000 (57877.6410)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1207  max mem: 15572
Epoch: [18]  [2020/2809]  eta: 0:07:32  lr: 0.000031  min_lr: 0.000000  loss: 4.0584 (3.9038)  class_acc: 0.2500 (0.2861)  loss_scale: 65536.0000 (57915.5349)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0925  max mem: 15572
[2025-01-15 23:20:17,084] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:20:17,084] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [2030/2809]  eta: 0:07:26  lr: 0.000031  min_lr: 0.000000  loss: 4.0112 (3.9038)  class_acc: 0.2500 (0.2861)  loss_scale: 65536.0000 (57985.3235)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1154  max mem: 15572
[2025-01-15 23:20:19,442] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52595
[2025-01-15 23:20:19,443] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:20:19,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2040/2809]  eta: 0:07:20  lr: 0.000031  min_lr: 0.000000  loss: 4.0225 (3.9040)  class_acc: 0.2500 (0.2857)  loss_scale: 65536.0000 (58086.5380)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.1066  max mem: 15572
Epoch: [18]  [2050/2809]  eta: 0:07:14  lr: 0.000031  min_lr: 0.000000  loss: 3.9610 (3.9035)  class_acc: 0.2500 (0.2860)  loss_scale: 65536.0000 (58122.8591)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0738  max mem: 15572
Epoch: [18]  [2060/2809]  eta: 0:07:09  lr: 0.000031  min_lr: 0.000000  loss: 3.9410 (3.9039)  class_acc: 0.2917 (0.2860)  loss_scale: 65536.0000 (58158.8278)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0934  max mem: 15572
Epoch: [18]  [2070/2809]  eta: 0:07:03  lr: 0.000031  min_lr: 0.000000  loss: 3.7818 (3.9031)  class_acc: 0.3333 (0.2863)  loss_scale: 65536.0000 (58194.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1380  max mem: 15572
Epoch: [18]  [2080/2809]  eta: 0:06:57  lr: 0.000031  min_lr: 0.000000  loss: 3.7752 (3.9032)  class_acc: 0.3333 (0.2863)  loss_scale: 65536.0000 (58229.7280)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1514  max mem: 15572
[2025-01-15 23:20:51,165] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52652
[2025-01-15 23:20:51,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:20:51,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2090/2809]  eta: 0:06:52  lr: 0.000031  min_lr: 0.000000  loss: 3.9473 (3.9027)  class_acc: 0.2917 (0.2864)  loss_scale: 65536.0000 (58248.9986)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1693  max mem: 15572
Epoch: [18]  [2100/2809]  eta: 0:06:46  lr: 0.000031  min_lr: 0.000000  loss: 3.6812 (3.9012)  class_acc: 0.2917 (0.2867)  loss_scale: 32768.0000 (58127.7182)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1592  max mem: 15572
Epoch: [18]  [2110/2809]  eta: 0:06:40  lr: 0.000031  min_lr: 0.000000  loss: 3.7088 (3.9013)  class_acc: 0.2917 (0.2867)  loss_scale: 32768.0000 (58007.5869)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1495  max mem: 15572
Epoch: [18]  [2120/2809]  eta: 0:06:35  lr: 0.000031  min_lr: 0.000000  loss: 4.0195 (3.9018)  class_acc: 0.2500 (0.2865)  loss_scale: 32768.0000 (57888.5884)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1652  max mem: 15572
Epoch: [18]  [2130/2809]  eta: 0:06:29  lr: 0.000031  min_lr: 0.000000  loss: 4.0047 (3.9019)  class_acc: 0.2500 (0.2865)  loss_scale: 32768.0000 (57770.7067)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1558  max mem: 15572
Epoch: [18]  [2140/2809]  eta: 0:06:23  lr: 0.000031  min_lr: 0.000000  loss: 4.0047 (3.9020)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (57653.9262)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1248  max mem: 15572
Epoch: [18]  [2150/2809]  eta: 0:06:17  lr: 0.000031  min_lr: 0.000000  loss: 4.0723 (3.9028)  class_acc: 0.2500 (0.2863)  loss_scale: 32768.0000 (57538.2315)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0877  max mem: 15572
Epoch: [18]  [2160/2809]  eta: 0:06:12  lr: 0.000031  min_lr: 0.000000  loss: 4.0504 (3.9034)  class_acc: 0.2500 (0.2861)  loss_scale: 32768.0000 (57423.6076)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0979  max mem: 15572
Epoch: [18]  [2170/2809]  eta: 0:06:06  lr: 0.000031  min_lr: 0.000000  loss: 3.8505 (3.9032)  class_acc: 0.2917 (0.2862)  loss_scale: 32768.0000 (57310.0396)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1004  max mem: 15572
Epoch: [18]  [2180/2809]  eta: 0:06:00  lr: 0.000031  min_lr: 0.000000  loss: 3.7667 (3.9022)  class_acc: 0.3333 (0.2865)  loss_scale: 32768.0000 (57197.5131)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1126  max mem: 15572
Epoch: [18]  [2190/2809]  eta: 0:05:55  lr: 0.000031  min_lr: 0.000000  loss: 3.8125 (3.9022)  class_acc: 0.3333 (0.2866)  loss_scale: 32768.0000 (57086.0137)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1816  max mem: 15572
Epoch: [18]  [2200/2809]  eta: 0:05:49  lr: 0.000031  min_lr: 0.000000  loss: 3.8767 (3.9022)  class_acc: 0.2500 (0.2866)  loss_scale: 32768.0000 (56975.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1867  max mem: 15572
Epoch: [18]  [2210/2809]  eta: 0:05:43  lr: 0.000031  min_lr: 0.000000  loss: 3.8395 (3.9011)  class_acc: 0.2917 (0.2869)  loss_scale: 32768.0000 (56866.0407)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1292  max mem: 15572
[2025-01-15 23:22:05,572] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:22:05,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [2220/2809]  eta: 0:05:37  lr: 0.000031  min_lr: 0.000000  loss: 3.7270 (3.9008)  class_acc: 0.2917 (0.2869)  loss_scale: 32768.0000 (56787.0473)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1052  max mem: 15572
Epoch: [18]  [2230/2809]  eta: 0:05:32  lr: 0.000031  min_lr: 0.000000  loss: 3.8219 (3.9015)  class_acc: 0.2083 (0.2867)  loss_scale: 65536.0000 (56826.2627)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1364  max mem: 15572
Epoch: [18]  [2240/2809]  eta: 0:05:26  lr: 0.000031  min_lr: 0.000000  loss: 3.9775 (3.9006)  class_acc: 0.2500 (0.2869)  loss_scale: 65536.0000 (56865.1281)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1107  max mem: 15572
Epoch: [18]  [2250/2809]  eta: 0:05:20  lr: 0.000031  min_lr: 0.000000  loss: 3.9152 (3.9015)  class_acc: 0.2500 (0.2866)  loss_scale: 65536.0000 (56903.6482)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1234  max mem: 15572
Epoch: [18]  [2260/2809]  eta: 0:05:14  lr: 0.000031  min_lr: 0.000000  loss: 3.9152 (3.9006)  class_acc: 0.2500 (0.2867)  loss_scale: 65536.0000 (56941.8275)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1507  max mem: 15572
Epoch: [18]  [2270/2809]  eta: 0:05:09  lr: 0.000031  min_lr: 0.000000  loss: 3.7627 (3.9004)  class_acc: 0.2500 (0.2868)  loss_scale: 65536.0000 (56979.6706)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.1117  max mem: 15572
Epoch: [18]  [2280/2809]  eta: 0:05:03  lr: 0.000031  min_lr: 0.000000  loss: 3.7122 (3.8996)  class_acc: 0.2500 (0.2869)  loss_scale: 65536.0000 (57017.1819)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1460  max mem: 15572
Epoch: [18]  [2290/2809]  eta: 0:04:57  lr: 0.000031  min_lr: 0.000000  loss: 3.6991 (3.8993)  class_acc: 0.3333 (0.2870)  loss_scale: 65536.0000 (57054.3658)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1415  max mem: 15572
Epoch: [18]  [2300/2809]  eta: 0:04:51  lr: 0.000031  min_lr: 0.000000  loss: 4.0808 (3.8995)  class_acc: 0.2917 (0.2870)  loss_scale: 65536.0000 (57091.2264)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0936  max mem: 15572
Epoch: [18]  [2310/2809]  eta: 0:04:46  lr: 0.000031  min_lr: 0.000000  loss: 3.8418 (3.8990)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57127.7681)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1110  max mem: 15572
Epoch: [18]  [2320/2809]  eta: 0:04:40  lr: 0.000031  min_lr: 0.000000  loss: 3.7959 (3.8992)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57163.9948)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0865  max mem: 15572
Epoch: [18]  [2330/2809]  eta: 0:04:34  lr: 0.000031  min_lr: 0.000000  loss: 3.8694 (3.8987)  class_acc: 0.2500 (0.2871)  loss_scale: 65536.0000 (57199.9108)  weight_decay: 0.0500 (0.0500)  time: 0.4856  data: 0.0257  max mem: 15572
Epoch: [18]  [2340/2809]  eta: 0:04:28  lr: 0.000031  min_lr: 0.000000  loss: 3.7481 (3.8978)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57235.5199)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0618  max mem: 15572
[2025-01-15 23:23:17,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:23:17,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:23:18,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 52912
[2025-01-15 23:23:18,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:23:18,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2350/2809]  eta: 0:04:22  lr: 0.000031  min_lr: 0.000000  loss: 3.6892 (3.8969)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (57354.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1203  max mem: 15572
Epoch: [18]  [2360/2809]  eta: 0:04:17  lr: 0.000031  min_lr: 0.000000  loss: 3.7491 (3.8970)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57389.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1068  max mem: 15572
Epoch: [18]  [2370/2809]  eta: 0:04:11  lr: 0.000031  min_lr: 0.000000  loss: 4.0419 (3.8978)  class_acc: 0.2083 (0.2869)  loss_scale: 65536.0000 (57423.4669)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1043  max mem: 15572
Epoch: [18]  [2380/2809]  eta: 0:04:05  lr: 0.000031  min_lr: 0.000000  loss: 4.1006 (3.8986)  class_acc: 0.2083 (0.2869)  loss_scale: 65536.0000 (57457.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0987  max mem: 15572
Epoch: [18]  [2390/2809]  eta: 0:04:00  lr: 0.000031  min_lr: 0.000000  loss: 4.0465 (3.8983)  class_acc: 0.2917 (0.2870)  loss_scale: 65536.0000 (57491.3258)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1369  max mem: 15572
Epoch: [18]  [2400/2809]  eta: 0:03:54  lr: 0.000031  min_lr: 0.000000  loss: 3.9514 (3.8981)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (57524.8313)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1563  max mem: 15572
Epoch: [18]  [2410/2809]  eta: 0:03:48  lr: 0.000031  min_lr: 0.000000  loss: 3.8222 (3.8973)  class_acc: 0.2917 (0.2869)  loss_scale: 65536.0000 (57558.0589)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1406  max mem: 15572
Epoch: [18]  [2420/2809]  eta: 0:03:42  lr: 0.000031  min_lr: 0.000000  loss: 4.0141 (3.8981)  class_acc: 0.2917 (0.2867)  loss_scale: 65536.0000 (57591.0120)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1548  max mem: 15572
Epoch: [18]  [2430/2809]  eta: 0:03:37  lr: 0.000031  min_lr: 0.000000  loss: 4.0759 (3.8985)  class_acc: 0.2083 (0.2866)  loss_scale: 65536.0000 (57623.6940)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1433  max mem: 15572
[2025-01-15 23:24:09,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=53000, skipped=356, lr=[2.9997287810269747e-07, 2.9997287810269747e-07, 4.2853268300385355e-07, 4.2853268300385355e-07, 6.121895471483623e-07, 6.121895471483623e-07, 8.745564959262318e-07, 8.745564959262318e-07, 1.2493664227517598e-06, 1.2493664227517598e-06, 1.784809175359657e-06, 1.784809175359657e-06, 2.5497273933709387e-06, 2.5497273933709387e-06, 3.642467704815627e-06, 3.642467704815627e-06, 5.203525292593753e-06, 5.203525292593753e-06, 7.433607560848219e-06, 7.433607560848219e-06, 1.0619439372640312e-05, 1.0619439372640312e-05, 1.517062767520045e-05, 1.517062767520045e-05, 2.1672325250286358e-05, 2.1672325250286358e-05, 3.096046464326623e-05, 3.096046464326623e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:24:09,645] [INFO] [timer.py:260:stop] epoch=0/micro_step=53000/global_step=53000, RunningAvgSamplesPerSec=28.49188619051902, CurrSamplesPerSec=29.698357645498305, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [18]  [2440/2809]  eta: 0:03:31  lr: 0.000031  min_lr: 0.000000  loss: 4.0883 (3.8991)  class_acc: 0.2500 (0.2864)  loss_scale: 65536.0000 (57656.1082)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1114  max mem: 15572
Epoch: [18]  [2450/2809]  eta: 0:03:25  lr: 0.000031  min_lr: 0.000000  loss: 3.8039 (3.8988)  class_acc: 0.2917 (0.2866)  loss_scale: 65536.0000 (57688.2579)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1600  max mem: 15572
Epoch: [18]  [2460/2809]  eta: 0:03:20  lr: 0.000031  min_lr: 0.000000  loss: 3.8039 (3.8989)  class_acc: 0.3333 (0.2868)  loss_scale: 65536.0000 (57720.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1472  max mem: 15572
Epoch: [18]  [2470/2809]  eta: 0:03:14  lr: 0.000031  min_lr: 0.000000  loss: 4.0918 (3.8997)  class_acc: 0.2500 (0.2867)  loss_scale: 65536.0000 (57751.7766)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0848  max mem: 15572
[2025-01-15 23:24:33,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:24:33,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:24:33,492] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53042
[2025-01-15 23:24:33,492] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:24:33,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2480/2809]  eta: 0:03:08  lr: 0.000031  min_lr: 0.000000  loss: 3.9362 (3.8994)  class_acc: 0.2500 (0.2867)  loss_scale: 65536.0000 (57809.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0913  max mem: 15572
Epoch: [18]  [2490/2809]  eta: 0:03:02  lr: 0.000031  min_lr: 0.000000  loss: 3.7909 (3.8992)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (57840.5845)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0829  max mem: 15572
Epoch: [18]  [2500/2809]  eta: 0:02:56  lr: 0.000031  min_lr: 0.000000  loss: 3.7843 (3.8993)  class_acc: 0.2917 (0.2868)  loss_scale: 65536.0000 (57871.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1184  max mem: 15572
Epoch: [18]  [2510/2809]  eta: 0:02:51  lr: 0.000031  min_lr: 0.000000  loss: 3.7654 (3.8984)  class_acc: 0.3333 (0.2870)  loss_scale: 65536.0000 (57901.8781)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1817  max mem: 15572
Epoch: [18]  [2520/2809]  eta: 0:02:45  lr: 0.000031  min_lr: 0.000000  loss: 3.6786 (3.8977)  class_acc: 0.3750 (0.2873)  loss_scale: 65536.0000 (57932.1603)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1476  max mem: 15572
Epoch: [18]  [2530/2809]  eta: 0:02:39  lr: 0.000031  min_lr: 0.000000  loss: 3.7844 (3.8977)  class_acc: 0.2917 (0.2871)  loss_scale: 65536.0000 (57962.2031)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1014  max mem: 15572
Epoch: [18]  [2540/2809]  eta: 0:02:34  lr: 0.000031  min_lr: 0.000000  loss: 4.0605 (3.8981)  class_acc: 0.2500 (0.2872)  loss_scale: 65536.0000 (57992.0094)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0837  max mem: 15572
Epoch: [18]  [2550/2809]  eta: 0:02:28  lr: 0.000031  min_lr: 0.000000  loss: 3.7842 (3.8977)  class_acc: 0.2917 (0.2874)  loss_scale: 65536.0000 (58021.5821)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1093  max mem: 15572
Epoch: [18]  [2560/2809]  eta: 0:02:22  lr: 0.000031  min_lr: 0.000000  loss: 3.9954 (3.8988)  class_acc: 0.3333 (0.2872)  loss_scale: 65536.0000 (58050.9239)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1089  max mem: 15572
Epoch: [18]  [2570/2809]  eta: 0:02:16  lr: 0.000031  min_lr: 0.000000  loss: 3.9954 (3.8987)  class_acc: 0.2083 (0.2872)  loss_scale: 65536.0000 (58080.0373)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1084  max mem: 15572
Epoch: [18]  [2580/2809]  eta: 0:02:11  lr: 0.000031  min_lr: 0.000000  loss: 3.7634 (3.8981)  class_acc: 0.3333 (0.2873)  loss_scale: 65536.0000 (58108.9252)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1220  max mem: 15572
Epoch: [18]  [2590/2809]  eta: 0:02:05  lr: 0.000031  min_lr: 0.000000  loss: 3.7634 (3.8981)  class_acc: 0.3333 (0.2873)  loss_scale: 65536.0000 (58137.5901)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1497  max mem: 15572
Epoch: [18]  [2600/2809]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000000  loss: 3.9950 (3.8985)  class_acc: 0.2917 (0.2873)  loss_scale: 65536.0000 (58166.0346)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1921  max mem: 15572
[2025-01-15 23:25:47,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:25:47,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:25:47,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53172
[2025-01-15 23:25:47,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:25:47,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2610/2809]  eta: 0:01:53  lr: 0.000031  min_lr: 0.000000  loss: 3.7672 (3.8973)  class_acc: 0.3333 (0.2875)  loss_scale: 65536.0000 (58219.3612)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1232  max mem: 15572
Epoch: [18]  [2620/2809]  eta: 0:01:48  lr: 0.000031  min_lr: 0.000000  loss: 3.6633 (3.8980)  class_acc: 0.2917 (0.2874)  loss_scale: 65536.0000 (58247.2766)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0791  max mem: 15572
Epoch: [18]  [2630/2809]  eta: 0:01:42  lr: 0.000031  min_lr: 0.000000  loss: 4.0978 (3.8985)  class_acc: 0.2500 (0.2874)  loss_scale: 65536.0000 (58274.9799)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0756  max mem: 15572
Epoch: [18]  [2640/2809]  eta: 0:01:36  lr: 0.000031  min_lr: 0.000000  loss: 3.8326 (3.8977)  class_acc: 0.3333 (0.2878)  loss_scale: 65536.0000 (58302.4733)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0764  max mem: 15572
Epoch: [18]  [2650/2809]  eta: 0:01:30  lr: 0.000031  min_lr: 0.000000  loss: 3.8326 (3.8975)  class_acc: 0.3333 (0.2877)  loss_scale: 65536.0000 (58329.7593)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1079  max mem: 15572
Epoch: [18]  [2660/2809]  eta: 0:01:25  lr: 0.000031  min_lr: 0.000000  loss: 3.8952 (3.8977)  class_acc: 0.2500 (0.2875)  loss_scale: 65536.0000 (58356.8403)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1379  max mem: 15572
Epoch: [18]  [2670/2809]  eta: 0:01:19  lr: 0.000031  min_lr: 0.000000  loss: 4.0072 (3.8979)  class_acc: 0.2500 (0.2875)  loss_scale: 65536.0000 (58383.7185)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0937  max mem: 15572
Epoch: [18]  [2680/2809]  eta: 0:01:13  lr: 0.000031  min_lr: 0.000000  loss: 3.8531 (3.8972)  class_acc: 0.2917 (0.2876)  loss_scale: 65536.0000 (58410.3961)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0749  max mem: 15572
Epoch: [18]  [2690/2809]  eta: 0:01:08  lr: 0.000031  min_lr: 0.000000  loss: 3.7131 (3.8970)  class_acc: 0.2917 (0.2876)  loss_scale: 65536.0000 (58436.8755)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0963  max mem: 15572
Epoch: [18]  [2700/2809]  eta: 0:01:02  lr: 0.000031  min_lr: 0.000000  loss: 3.7470 (3.8961)  class_acc: 0.2917 (0.2878)  loss_scale: 65536.0000 (58463.1588)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1052  max mem: 15572
Epoch: [18]  [2710/2809]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000000  loss: 3.6391 (3.8961)  class_acc: 0.2917 (0.2877)  loss_scale: 65536.0000 (58489.2482)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.0726  max mem: 15572
Epoch: [18]  [2720/2809]  eta: 0:00:50  lr: 0.000031  min_lr: 0.000000  loss: 3.9204 (3.8964)  class_acc: 0.2500 (0.2876)  loss_scale: 65536.0000 (58515.1459)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1118  max mem: 15572
Epoch: [18]  [2730/2809]  eta: 0:00:45  lr: 0.000031  min_lr: 0.000000  loss: 4.0568 (3.8965)  class_acc: 0.2500 (0.2876)  loss_scale: 65536.0000 (58540.8539)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1743  max mem: 15572
[2025-01-15 23:27:00,390] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:27:00,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [2740/2809]  eta: 0:00:39  lr: 0.000031  min_lr: 0.000000  loss: 3.9324 (3.8963)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (58614.1934)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1345  max mem: 15572
[2025-01-15 23:27:05,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53307
[2025-01-15 23:27:05,223] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:27:05,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [18]  [2750/2809]  eta: 0:00:33  lr: 0.000031  min_lr: 0.000000  loss: 3.9086 (3.8963)  class_acc: 0.2917 (0.2878)  loss_scale: 65536.0000 (58734.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1567  max mem: 15572
Epoch: [18]  [2760/2809]  eta: 0:00:28  lr: 0.000031  min_lr: 0.000000  loss: 3.8887 (3.8957)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (58759.2785)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1343  max mem: 15572
Epoch: [18]  [2770/2809]  eta: 0:00:22  lr: 0.000031  min_lr: 0.000000  loss: 3.9152 (3.8958)  class_acc: 0.2917 (0.2879)  loss_scale: 65536.0000 (58783.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0347  max mem: 15572
Epoch: [18]  [2780/2809]  eta: 0:00:16  lr: 0.000031  min_lr: 0.000000  loss: 3.9501 (3.8956)  class_acc: 0.2083 (0.2879)  loss_scale: 65536.0000 (58808.0144)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.0926  max mem: 15572
[2025-01-15 23:27:28,607] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53351
[2025-01-15 23:27:28,607] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:27:28,607] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [2790/2809]  eta: 0:00:10  lr: 0.000031  min_lr: 0.000000  loss: 3.9501 (3.8960)  class_acc: 0.2083 (0.2878)  loss_scale: 65536.0000 (58808.6392)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1298  max mem: 15572
Epoch: [18]  [2800/2809]  eta: 0:00:05  lr: 0.000031  min_lr: 0.000000  loss: 3.9585 (3.8959)  class_acc: 0.2917 (0.2878)  loss_scale: 32768.0000 (58715.6701)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0791  max mem: 15572
Epoch: [18]  [2808/2809]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000000  loss: 3.8282 (3.8957)  class_acc: 0.2917 (0.2879)  loss_scale: 32768.0000 (58641.7714)  weight_decay: 0.0500 (0.0500)  time: 0.4450  data: 0.0379  max mem: 15572
Epoch: [18] Total time: 0:26:45 (0.5717 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000000  loss: 3.8282 (3.8957)  class_acc: 0.2917 (0.2879)  loss_scale: 32768.0000 (58641.7714)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:16:15  loss: 0.3135 (0.3135)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.5873  data: 3.3957  max mem: 15572
Val:  [ 10/272]  eta: 0:03:03  loss: 2.7544 (2.5540)  acc1: 33.3333 (35.3535)  acc5: 66.6667 (64.6465)  time: 0.7002  data: 0.5020  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 2.6864 (2.5974)  acc1: 38.8889 (38.8889)  acc5: 66.6667 (66.9312)  time: 0.3752  data: 0.1824  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 2.6813 (2.6898)  acc1: 38.8889 (35.3047)  acc5: 66.6667 (67.2043)  time: 0.3098  data: 0.1210  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 2.7177 (2.7017)  acc1: 27.7778 (34.6883)  acc5: 72.2222 (68.5637)  time: 0.3034  data: 0.1083  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.6073 (2.6423)  acc1: 33.3333 (35.5120)  acc5: 72.2222 (70.5882)  time: 0.3111  data: 0.1135  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 1.7784 (2.5030)  acc1: 50.0000 (39.9818)  acc5: 83.3333 (72.1311)  time: 0.3103  data: 0.1164  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.5705 (2.4156)  acc1: 66.6667 (42.7230)  acc5: 88.8889 (73.5524)  time: 0.3277  data: 0.1246  max mem: 15572
Val:  [ 80/272]  eta: 0:01:10  loss: 2.0827 (2.4170)  acc1: 50.0000 (42.7298)  acc5: 77.7778 (73.1824)  time: 0.3211  data: 0.1227  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.5288 (2.4482)  acc1: 38.8889 (42.3077)  acc5: 72.2222 (73.1380)  time: 0.2886  data: 0.1084  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 2.5288 (2.4828)  acc1: 38.8889 (41.4741)  acc5: 77.7778 (72.4422)  time: 0.2228  data: 0.0584  max mem: 15572
Val:  [110/272]  eta: 0:00:52  loss: 2.7618 (2.5463)  acc1: 16.6667 (39.5896)  acc5: 66.6667 (71.4715)  time: 0.1765  data: 0.0146  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 2.9376 (2.5781)  acc1: 16.6667 (38.8889)  acc5: 66.6667 (70.8907)  time: 0.2196  data: 0.0242  max mem: 15572
Val:  [130/272]  eta: 0:00:45  loss: 2.4533 (2.5410)  acc1: 38.8889 (40.0339)  acc5: 72.2222 (71.5437)  time: 0.2803  data: 0.0702  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 1.9917 (2.5370)  acc1: 44.4444 (40.4255)  acc5: 77.7778 (71.3160)  time: 0.3524  data: 0.1460  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.5071 (2.5381)  acc1: 38.8889 (39.9559)  acc5: 72.2222 (71.7439)  time: 0.4151  data: 0.2132  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.4597 (2.5154)  acc1: 44.4444 (40.9248)  acc5: 77.7778 (72.3257)  time: 0.3794  data: 0.1939  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.6204 (2.5397)  acc1: 44.4444 (40.0585)  acc5: 72.2222 (71.9298)  time: 0.3693  data: 0.1713  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.5607 (2.5288)  acc1: 27.7778 (40.0859)  acc5: 72.2222 (72.4064)  time: 0.3927  data: 0.1868  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.5607 (2.5776)  acc1: 22.2222 (38.7144)  acc5: 72.2222 (71.0878)  time: 0.3333  data: 0.1350  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.7216 (2.5852)  acc1: 22.2222 (38.6954)  acc5: 66.6667 (70.9508)  time: 0.2935  data: 0.0989  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3042 (2.5903)  acc1: 38.8889 (38.8626)  acc5: 72.2222 (70.8531)  time: 0.3292  data: 0.1342  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.5076 (2.5771)  acc1: 38.8889 (38.9894)  acc5: 72.2222 (70.9904)  time: 0.4057  data: 0.1998  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.8304 (2.5381)  acc1: 55.5556 (40.2597)  acc5: 83.3333 (71.6450)  time: 0.4221  data: 0.2141  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7545 (2.5192)  acc1: 61.1111 (40.7331)  acc5: 88.8889 (72.0609)  time: 0.3541  data: 0.1334  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5025 (2.5312)  acc1: 33.3333 (40.1948)  acc5: 72.2222 (72.0009)  time: 0.3490  data: 0.1155  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.4013 (2.4634)  acc1: 72.2222 (42.1030)  acc5: 88.8889 (72.8395)  time: 0.3574  data: 0.1474  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2465 (2.4628)  acc1: 72.2222 (41.8614)  acc5: 88.8889 (72.8372)  time: 0.2468  data: 0.0768  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2465 (2.4670)  acc1: 72.2222 (41.8390)  acc5: 88.8889 (72.8036)  time: 0.2304  data: 0.0668  max mem: 15572
Val: Total time: 0:01:31 (0.3358 s / it)
* Acc@1 41.839 Acc@5 72.804 loss 2.467
Accuracy of the network on the 4883 val videos: 41.8%
[2025-01-15 23:29:08,703] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 23:29:08,707] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 23:29:08,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 23:29:12,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 23:29:12,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.84%
Epoch: [19]  [   0/2809]  eta: 6:31:28  lr: 0.000031  min_lr: 0.000000  loss: 4.3882 (4.3882)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3617  data: 7.9221  max mem: 15572
Epoch: [19]  [  10/2809]  eta: 1:06:33  lr: 0.000031  min_lr: 0.000000  loss: 3.8908 (3.7901)  class_acc: 0.2500 (0.2727)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4266  data: 0.9611  max mem: 15572
Epoch: [19]  [  20/2809]  eta: 0:50:48  lr: 0.000031  min_lr: 0.000000  loss: 3.7579 (3.7600)  class_acc: 0.2500 (0.2837)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7295  data: 0.2638  max mem: 15572
Epoch: [19]  [  30/2809]  eta: 0:41:32  lr: 0.000031  min_lr: 0.000000  loss: 3.8189 (3.7849)  class_acc: 0.2917 (0.2930)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1793  max mem: 15572
Epoch: [19]  [  40/2809]  eta: 0:36:19  lr: 0.000031  min_lr: 0.000000  loss: 3.8189 (3.7462)  class_acc: 0.3333 (0.3130)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4663  data: 0.0481  max mem: 15572
Epoch: [19]  [  50/2809]  eta: 0:33:29  lr: 0.000031  min_lr: 0.000000  loss: 3.9152 (3.7898)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4671  data: 0.0008  max mem: 15572
Epoch: [19]  [  60/2809]  eta: 0:31:12  lr: 0.000031  min_lr: 0.000000  loss: 3.7197 (3.7618)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4640  data: 0.0050  max mem: 15572
Epoch: [19]  [  70/2809]  eta: 0:29:34  lr: 0.000031  min_lr: 0.000000  loss: 3.6483 (3.7616)  class_acc: 0.3750 (0.3187)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4429  data: 0.0048  max mem: 15572
Epoch: [19]  [  80/2809]  eta: 0:28:46  lr: 0.000031  min_lr: 0.000000  loss: 3.7745 (3.7923)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4847  data: 0.0472  max mem: 15572
Epoch: [19]  [  90/2809]  eta: 0:28:39  lr: 0.000031  min_lr: 0.000000  loss: 3.9137 (3.7843)  class_acc: 0.2500 (0.3095)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1405  max mem: 15572
Epoch: [19]  [ 100/2809]  eta: 0:28:08  lr: 0.000031  min_lr: 0.000000  loss: 3.8951 (3.7867)  class_acc: 0.2500 (0.3073)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1492  max mem: 15572
[2025-01-15 23:30:20,414] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:30:20,414] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 110/2809]  eta: 0:27:47  lr: 0.000031  min_lr: 0.000000  loss: 3.8376 (3.7895)  class_acc: 0.2500 (0.3056)  loss_scale: 32768.0000 (33358.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1199  max mem: 15572
Epoch: [19]  [ 120/2809]  eta: 0:27:51  lr: 0.000031  min_lr: 0.000000  loss: 4.0023 (3.8095)  class_acc: 0.2500 (0.3044)  loss_scale: 65536.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.1892  max mem: 15572
Epoch: [19]  [ 130/2809]  eta: 0:27:25  lr: 0.000031  min_lr: 0.000000  loss: 4.0833 (3.8195)  class_acc: 0.2500 (0.3031)  loss_scale: 65536.0000 (38271.0229)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1721  max mem: 15572
Epoch: [19]  [ 140/2809]  eta: 0:27:06  lr: 0.000031  min_lr: 0.000000  loss: 3.9514 (3.8324)  class_acc: 0.2500 (0.3002)  loss_scale: 65536.0000 (40204.7092)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.1086  max mem: 15572
Epoch: [19]  [ 150/2809]  eta: 0:27:00  lr: 0.000031  min_lr: 0.000000  loss: 3.8600 (3.8338)  class_acc: 0.2917 (0.3016)  loss_scale: 65536.0000 (41882.2781)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1373  max mem: 15572
Epoch: [19]  [ 160/2809]  eta: 0:26:58  lr: 0.000031  min_lr: 0.000000  loss: 3.8573 (3.8410)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (43351.4534)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.1803  max mem: 15572
Epoch: [19]  [ 170/2809]  eta: 0:26:47  lr: 0.000031  min_lr: 0.000000  loss: 3.9374 (3.8477)  class_acc: 0.2500 (0.2995)  loss_scale: 65536.0000 (44648.7953)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1699  max mem: 15572
Epoch: [19]  [ 180/2809]  eta: 0:26:30  lr: 0.000031  min_lr: 0.000000  loss: 3.9214 (3.8509)  class_acc: 0.2500 (0.3002)  loss_scale: 65536.0000 (45802.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1121  max mem: 15572
Epoch: [19]  [ 190/2809]  eta: 0:26:14  lr: 0.000031  min_lr: 0.000000  loss: 3.9044 (3.8576)  class_acc: 0.2500 (0.2993)  loss_scale: 65536.0000 (46835.9372)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0951  max mem: 15572
Epoch: [19]  [ 200/2809]  eta: 0:25:54  lr: 0.000031  min_lr: 0.000000  loss: 4.0734 (3.8609)  class_acc: 0.2500 (0.2991)  loss_scale: 65536.0000 (47766.2886)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0773  max mem: 15572
Epoch: [19]  [ 210/2809]  eta: 0:25:52  lr: 0.000031  min_lr: 0.000000  loss: 4.0734 (3.8665)  class_acc: 0.2500 (0.2982)  loss_scale: 65536.0000 (48608.4550)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1102  max mem: 15572
Epoch: [19]  [ 220/2809]  eta: 0:25:43  lr: 0.000031  min_lr: 0.000000  loss: 4.0625 (3.8768)  class_acc: 0.2500 (0.2968)  loss_scale: 65536.0000 (49374.4072)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1531  max mem: 15572
Epoch: [19]  [ 230/2809]  eta: 0:25:29  lr: 0.000031  min_lr: 0.000000  loss: 4.0148 (3.8722)  class_acc: 0.2917 (0.2987)  loss_scale: 65536.0000 (50074.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1205  max mem: 15572
[2025-01-15 23:31:32,557] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:31:32,557] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:31:33,004] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53609
[2025-01-15 23:31:33,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:31:33,005] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 240/2809]  eta: 0:25:09  lr: 0.000031  min_lr: 0.000000  loss: 3.8914 (3.8699)  class_acc: 0.3333 (0.2988)  loss_scale: 65536.0000 (50987.5519)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0663  max mem: 15572
Epoch: [19]  [ 250/2809]  eta: 0:25:04  lr: 0.000031  min_lr: 0.000000  loss: 3.8560 (3.8631)  class_acc: 0.3333 (0.3008)  loss_scale: 65536.0000 (51567.1713)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0936  max mem: 15572
Epoch: [19]  [ 260/2809]  eta: 0:24:59  lr: 0.000031  min_lr: 0.000000  loss: 3.8724 (3.8689)  class_acc: 0.2917 (0.2979)  loss_scale: 65536.0000 (52102.3755)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.1665  max mem: 15572
Epoch: [19]  [ 270/2809]  eta: 0:24:56  lr: 0.000031  min_lr: 0.000000  loss: 3.8673 (3.8615)  class_acc: 0.2917 (0.2998)  loss_scale: 65536.0000 (52598.0812)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1815  max mem: 15572
Epoch: [19]  [ 280/2809]  eta: 0:24:46  lr: 0.000030  min_lr: 0.000000  loss: 3.8107 (3.8680)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (53058.5053)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1365  max mem: 15572
Epoch: [19]  [ 290/2809]  eta: 0:24:30  lr: 0.000030  min_lr: 0.000000  loss: 3.8806 (3.8608)  class_acc: 0.2917 (0.3015)  loss_scale: 65536.0000 (53487.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0700  max mem: 15572
Epoch: [19]  [ 300/2809]  eta: 0:24:21  lr: 0.000030  min_lr: 0.000000  loss: 3.7582 (3.8634)  class_acc: 0.2917 (0.3027)  loss_scale: 65536.0000 (53887.5748)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0738  max mem: 15572
Epoch: [19]  [ 310/2809]  eta: 0:24:19  lr: 0.000030  min_lr: 0.000000  loss: 3.6892 (3.8620)  class_acc: 0.2917 (0.3036)  loss_scale: 65536.0000 (54262.1222)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1551  max mem: 15572
Epoch: [19]  [ 320/2809]  eta: 0:24:07  lr: 0.000030  min_lr: 0.000000  loss: 4.0345 (3.8681)  class_acc: 0.2917 (0.3033)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1485  max mem: 15572
Epoch: [19]  [ 330/2809]  eta: 0:23:59  lr: 0.000030  min_lr: 0.000000  loss: 4.0506 (3.8750)  class_acc: 0.2917 (0.3027)  loss_scale: 65536.0000 (54943.3233)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.1115  max mem: 15572
Epoch: [19]  [ 340/2809]  eta: 0:23:57  lr: 0.000030  min_lr: 0.000000  loss: 3.9703 (3.8745)  class_acc: 0.2500 (0.3028)  loss_scale: 65536.0000 (55253.9589)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1660  max mem: 15572
Epoch: [19]  [ 350/2809]  eta: 0:23:54  lr: 0.000030  min_lr: 0.000000  loss: 3.8642 (3.8761)  class_acc: 0.2917 (0.3033)  loss_scale: 65536.0000 (55546.8946)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1871  max mem: 15572
Epoch: [19]  [ 360/2809]  eta: 0:23:44  lr: 0.000030  min_lr: 0.000000  loss: 3.8249 (3.8748)  class_acc: 0.3333 (0.3033)  loss_scale: 65536.0000 (55823.6011)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1327  max mem: 15572
[2025-01-15 23:32:46,090] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:32:46,090] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:32:47,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53740
[2025-01-15 23:32:47,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:32:47,062] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 370/2809]  eta: 0:23:34  lr: 0.000030  min_lr: 0.000000  loss: 3.9630 (3.8774)  class_acc: 0.2500 (0.3015)  loss_scale: 65536.0000 (56438.6846)  weight_decay: 0.0500 (0.0500)  time: 0.5184  data: 0.0941  max mem: 15572
Epoch: [19]  [ 380/2809]  eta: 0:23:29  lr: 0.000030  min_lr: 0.000000  loss: 3.9708 (3.8816)  class_acc: 0.2083 (0.2994)  loss_scale: 65536.0000 (56677.4593)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1268  max mem: 15572
Epoch: [19]  [ 390/2809]  eta: 0:23:31  lr: 0.000030  min_lr: 0.000000  loss: 4.0305 (3.8854)  class_acc: 0.2083 (0.2978)  loss_scale: 65536.0000 (56904.0205)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.2090  max mem: 15572
Epoch: [19]  [ 400/2809]  eta: 0:23:26  lr: 0.000030  min_lr: 0.000000  loss: 3.9378 (3.8815)  class_acc: 0.2500 (0.2989)  loss_scale: 65536.0000 (57119.2818)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.1761  max mem: 15572
Epoch: [19]  [ 410/2809]  eta: 0:23:13  lr: 0.000030  min_lr: 0.000000  loss: 3.8202 (3.8807)  class_acc: 0.3333 (0.2992)  loss_scale: 65536.0000 (57324.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.0425  max mem: 15572
Epoch: [19]  [ 420/2809]  eta: 0:23:06  lr: 0.000030  min_lr: 0.000000  loss: 3.8396 (3.8767)  class_acc: 0.3333 (0.2997)  loss_scale: 65536.0000 (57519.1259)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0642  max mem: 15572
Epoch: [19]  [ 430/2809]  eta: 0:22:57  lr: 0.000030  min_lr: 0.000000  loss: 3.8396 (3.8769)  class_acc: 0.2917 (0.2990)  loss_scale: 65536.0000 (57705.1323)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.1146  max mem: 15572
Epoch: [19]  [ 440/2809]  eta: 0:22:57  lr: 0.000030  min_lr: 0.000000  loss: 3.9330 (3.8757)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (57882.7029)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1581  max mem: 15572
Epoch: [19]  [ 450/2809]  eta: 0:22:48  lr: 0.000030  min_lr: 0.000000  loss: 3.9365 (3.8812)  class_acc: 0.2917 (0.2979)  loss_scale: 65536.0000 (58052.3991)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1464  max mem: 15572
Epoch: [19]  [ 460/2809]  eta: 0:22:41  lr: 0.000030  min_lr: 0.000000  loss: 4.0932 (3.8874)  class_acc: 0.2083 (0.2960)  loss_scale: 65536.0000 (58214.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1015  max mem: 15572
Epoch: [19]  [ 470/2809]  eta: 0:22:35  lr: 0.000030  min_lr: 0.000000  loss: 3.9249 (3.8843)  class_acc: 0.2917 (0.2970)  loss_scale: 65536.0000 (58370.1741)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1291  max mem: 15572
Epoch: [19]  [ 480/2809]  eta: 0:22:26  lr: 0.000030  min_lr: 0.000000  loss: 3.7862 (3.8857)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (58519.1518)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1127  max mem: 15572
Epoch: [19]  [ 490/2809]  eta: 0:22:20  lr: 0.000030  min_lr: 0.000000  loss: 3.9520 (3.8853)  class_acc: 0.2500 (0.2968)  loss_scale: 65536.0000 (58662.0611)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1157  max mem: 15572
[2025-01-15 23:34:00,400] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:34:00,400] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:34:01,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53871
[2025-01-15 23:34:01,269] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:34:01,269] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 500/2809]  eta: 0:22:11  lr: 0.000030  min_lr: 0.000000  loss: 3.9971 (3.8864)  class_acc: 0.2917 (0.2973)  loss_scale: 65536.0000 (59060.8862)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.1080  max mem: 15572
Epoch: [19]  [ 510/2809]  eta: 0:22:03  lr: 0.000030  min_lr: 0.000000  loss: 4.0176 (3.8885)  class_acc: 0.3333 (0.2976)  loss_scale: 65536.0000 (59187.6008)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0880  max mem: 15572
Epoch: [19]  [ 520/2809]  eta: 0:21:59  lr: 0.000030  min_lr: 0.000000  loss: 4.0176 (3.8886)  class_acc: 0.2917 (0.2984)  loss_scale: 65536.0000 (59309.4511)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1193  max mem: 15572
Epoch: [19]  [ 530/2809]  eta: 0:21:50  lr: 0.000030  min_lr: 0.000000  loss: 4.0200 (3.8918)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (59426.7119)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0849  max mem: 15572
Epoch: [19]  [ 540/2809]  eta: 0:21:39  lr: 0.000030  min_lr: 0.000000  loss: 4.1693 (3.8953)  class_acc: 0.2500 (0.2966)  loss_scale: 65536.0000 (59539.6377)  weight_decay: 0.0500 (0.0500)  time: 0.4773  data: 0.0154  max mem: 15572
[2025-01-15 23:34:25,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 53914
[2025-01-15 23:34:25,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:34:25,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 550/2809]  eta: 0:21:35  lr: 0.000030  min_lr: 0.000000  loss: 3.9301 (3.8895)  class_acc: 0.2917 (0.2970)  loss_scale: 65536.0000 (59172.7042)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.0989  max mem: 15572
Epoch: [19]  [ 560/2809]  eta: 0:21:32  lr: 0.000030  min_lr: 0.000000  loss: 3.9304 (3.8917)  class_acc: 0.2917 (0.2954)  loss_scale: 32768.0000 (58702.0321)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1954  max mem: 15572
Epoch: [19]  [ 570/2809]  eta: 0:21:27  lr: 0.000030  min_lr: 0.000000  loss: 4.1240 (3.8948)  class_acc: 0.2083 (0.2947)  loss_scale: 32768.0000 (58247.8459)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1787  max mem: 15572
Epoch: [19]  [ 580/2809]  eta: 0:21:17  lr: 0.000030  min_lr: 0.000000  loss: 3.9336 (3.8962)  class_acc: 0.2917 (0.2945)  loss_scale: 32768.0000 (57809.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0824  max mem: 15572
Epoch: [19]  [ 590/2809]  eta: 0:21:11  lr: 0.000030  min_lr: 0.000000  loss: 3.8458 (3.8947)  class_acc: 0.2917 (0.2948)  loss_scale: 32768.0000 (57385.5838)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0718  max mem: 15572
Epoch: [19]  [ 600/2809]  eta: 0:21:05  lr: 0.000030  min_lr: 0.000000  loss: 3.8515 (3.8942)  class_acc: 0.2917 (0.2949)  loss_scale: 32768.0000 (56975.9734)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1324  max mem: 15572
Epoch: [19]  [ 610/2809]  eta: 0:21:01  lr: 0.000030  min_lr: 0.000000  loss: 3.8974 (3.8916)  class_acc: 0.2917 (0.2945)  loss_scale: 32768.0000 (56579.7709)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1635  max mem: 15572
Epoch: [19]  [ 620/2809]  eta: 0:20:53  lr: 0.000030  min_lr: 0.000000  loss: 3.8367 (3.8917)  class_acc: 0.2500 (0.2938)  loss_scale: 32768.0000 (56196.3285)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1335  max mem: 15572
[2025-01-15 23:35:12,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=364, lr=[2.930698064107794e-07, 2.930698064107794e-07, 4.186711520153992e-07, 4.186711520153992e-07, 5.981016457362846e-07, 5.981016457362846e-07, 8.544309224804067e-07, 8.544309224804067e-07, 1.220615603543438e-06, 1.220615603543438e-06, 1.7437365764906259e-06, 1.7437365764906259e-06, 2.4910522521294658e-06, 2.4910522521294658e-06, 3.558646074470666e-06, 3.558646074470666e-06, 5.0837801063866654e-06, 5.0837801063866654e-06, 7.262543009123808e-06, 7.262543009123808e-06, 1.037506144160544e-05, 1.037506144160544e-05, 1.4821516345150631e-05, 1.4821516345150631e-05, 2.1173594778786615e-05, 2.1173594778786615e-05, 3.024799254112374e-05, 3.024799254112374e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:35:12,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=28.49598610812734, CurrSamplesPerSec=31.84910359700212, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [ 630/2809]  eta: 0:20:49  lr: 0.000030  min_lr: 0.000000  loss: 3.8654 (3.8911)  class_acc: 0.2917 (0.2940)  loss_scale: 32768.0000 (55825.0396)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1115  max mem: 15572
Epoch: [19]  [ 640/2809]  eta: 0:20:39  lr: 0.000030  min_lr: 0.000000  loss: 3.7042 (3.8881)  class_acc: 0.2917 (0.2947)  loss_scale: 32768.0000 (55465.3354)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0944  max mem: 15572
Epoch: [19]  [ 650/2809]  eta: 0:20:31  lr: 0.000030  min_lr: 0.000000  loss: 3.7042 (3.8875)  class_acc: 0.2500 (0.2950)  loss_scale: 32768.0000 (55116.6820)  weight_decay: 0.0500 (0.0500)  time: 0.4876  data: 0.0514  max mem: 15572
Epoch: [19]  [ 660/2809]  eta: 0:20:27  lr: 0.000030  min_lr: 0.000000  loss: 3.9139 (3.8886)  class_acc: 0.2500 (0.2945)  loss_scale: 32768.0000 (54778.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1177  max mem: 15572
Epoch: [19]  [ 670/2809]  eta: 0:20:21  lr: 0.000030  min_lr: 0.000000  loss: 4.0986 (3.8920)  class_acc: 0.2500 (0.2938)  loss_scale: 32768.0000 (54450.5514)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1407  max mem: 15572
[2025-01-15 23:35:37,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:35:37,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 680/2809]  eta: 0:20:15  lr: 0.000030  min_lr: 0.000000  loss: 4.0527 (3.8914)  class_acc: 0.2500 (0.2938)  loss_scale: 32768.0000 (54565.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1128  max mem: 15572
Epoch: [19]  [ 690/2809]  eta: 0:20:10  lr: 0.000030  min_lr: 0.000000  loss: 4.0527 (3.8935)  class_acc: 0.2500 (0.2930)  loss_scale: 65536.0000 (54723.9826)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1206  max mem: 15572
Epoch: [19]  [ 700/2809]  eta: 0:20:04  lr: 0.000030  min_lr: 0.000000  loss: 4.0323 (3.8934)  class_acc: 0.2500 (0.2927)  loss_scale: 65536.0000 (54878.2197)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1323  max mem: 15572
Epoch: [19]  [ 710/2809]  eta: 0:20:00  lr: 0.000030  min_lr: 0.000000  loss: 4.0435 (3.8963)  class_acc: 0.2917 (0.2927)  loss_scale: 65536.0000 (55028.1181)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1460  max mem: 15572
Epoch: [19]  [ 720/2809]  eta: 0:19:56  lr: 0.000030  min_lr: 0.000000  loss: 4.1194 (3.9012)  class_acc: 0.2500 (0.2914)  loss_scale: 65536.0000 (55173.8585)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1849  max mem: 15572
Epoch: [19]  [ 730/2809]  eta: 0:19:51  lr: 0.000030  min_lr: 0.000000  loss: 3.9394 (3.8969)  class_acc: 0.2500 (0.2914)  loss_scale: 65536.0000 (55315.6115)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1505  max mem: 15572
Epoch: [19]  [ 740/2809]  eta: 0:19:44  lr: 0.000030  min_lr: 0.000000  loss: 3.5569 (3.8960)  class_acc: 0.3333 (0.2918)  loss_scale: 65536.0000 (55453.5385)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0783  max mem: 15572
Epoch: [19]  [ 750/2809]  eta: 0:19:38  lr: 0.000030  min_lr: 0.000000  loss: 3.8575 (3.8945)  class_acc: 0.3333 (0.2924)  loss_scale: 65536.0000 (55587.7923)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0901  max mem: 15572
Epoch: [19]  [ 760/2809]  eta: 0:19:31  lr: 0.000030  min_lr: 0.000000  loss: 3.7017 (3.8911)  class_acc: 0.3750 (0.2936)  loss_scale: 65536.0000 (55718.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1092  max mem: 15572
Epoch: [19]  [ 770/2809]  eta: 0:19:28  lr: 0.000030  min_lr: 0.000000  loss: 3.7705 (3.8920)  class_acc: 0.3333 (0.2933)  loss_scale: 65536.0000 (55845.8521)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1499  max mem: 15572
Epoch: [19]  [ 780/2809]  eta: 0:19:20  lr: 0.000030  min_lr: 0.000000  loss: 3.8823 (3.8885)  class_acc: 0.3333 (0.2941)  loss_scale: 65536.0000 (55969.9257)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1064  max mem: 15572
Epoch: [19]  [ 790/2809]  eta: 0:19:14  lr: 0.000030  min_lr: 0.000000  loss: 3.6247 (3.8887)  class_acc: 0.3750 (0.2944)  loss_scale: 65536.0000 (56090.8622)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0582  max mem: 15572
[2025-01-15 23:36:49,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:36:49,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 800/2809]  eta: 0:19:06  lr: 0.000030  min_lr: 0.000000  loss: 3.7130 (3.8879)  class_acc: 0.2917 (0.2944)  loss_scale: 65536.0000 (56290.5968)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0735  max mem: 15572
[2025-01-15 23:36:51,550] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54175
[2025-01-15 23:36:51,550] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:36:51,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 810/2809]  eta: 0:19:01  lr: 0.000030  min_lr: 0.000000  loss: 3.7393 (3.8876)  class_acc: 0.2917 (0.2945)  loss_scale: 65536.0000 (56647.0234)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0911  max mem: 15572
Epoch: [19]  [ 820/2809]  eta: 0:18:54  lr: 0.000030  min_lr: 0.000000  loss: 3.9643 (3.8901)  class_acc: 0.2500 (0.2942)  loss_scale: 65536.0000 (56755.2935)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1089  max mem: 15572
Epoch: [19]  [ 830/2809]  eta: 0:18:48  lr: 0.000030  min_lr: 0.000000  loss: 4.0154 (3.8913)  class_acc: 0.2500 (0.2941)  loss_scale: 65536.0000 (56860.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0939  max mem: 15572
Epoch: [19]  [ 840/2809]  eta: 0:18:42  lr: 0.000030  min_lr: 0.000000  loss: 3.8492 (3.8907)  class_acc: 0.2500 (0.2939)  loss_scale: 65536.0000 (56964.1094)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1185  max mem: 15572
Epoch: [19]  [ 850/2809]  eta: 0:18:36  lr: 0.000030  min_lr: 0.000000  loss: 3.8675 (3.8910)  class_acc: 0.2500 (0.2934)  loss_scale: 65536.0000 (57064.8367)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1081  max mem: 15572
Epoch: [19]  [ 860/2809]  eta: 0:18:29  lr: 0.000030  min_lr: 0.000000  loss: 3.8675 (3.8896)  class_acc: 0.2500 (0.2935)  loss_scale: 65536.0000 (57163.2242)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0996  max mem: 15572
Epoch: [19]  [ 870/2809]  eta: 0:18:23  lr: 0.000030  min_lr: 0.000000  loss: 3.7715 (3.8883)  class_acc: 0.2500 (0.2936)  loss_scale: 65536.0000 (57259.3525)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1211  max mem: 15572
Epoch: [19]  [ 880/2809]  eta: 0:18:19  lr: 0.000030  min_lr: 0.000000  loss: 4.0025 (3.8905)  class_acc: 0.2500 (0.2928)  loss_scale: 65536.0000 (57353.2985)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1589  max mem: 15572
Epoch: [19]  [ 890/2809]  eta: 0:18:13  lr: 0.000030  min_lr: 0.000000  loss: 4.0203 (3.8911)  class_acc: 0.2500 (0.2928)  loss_scale: 65536.0000 (57445.1358)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1547  max mem: 15572
Epoch: [19]  [ 900/2809]  eta: 0:18:06  lr: 0.000030  min_lr: 0.000000  loss: 4.0300 (3.8936)  class_acc: 0.2500 (0.2925)  loss_scale: 65536.0000 (57534.9345)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0894  max mem: 15572
Epoch: [19]  [ 910/2809]  eta: 0:18:02  lr: 0.000030  min_lr: 0.000000  loss: 4.0783 (3.8962)  class_acc: 0.2500 (0.2921)  loss_scale: 65536.0000 (57622.7618)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1241  max mem: 15572
Epoch: [19]  [ 920/2809]  eta: 0:17:56  lr: 0.000030  min_lr: 0.000000  loss: 3.9120 (3.8950)  class_acc: 0.2917 (0.2925)  loss_scale: 65536.0000 (57708.6819)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1389  max mem: 15572
Epoch: [19]  [ 930/2809]  eta: 0:17:51  lr: 0.000030  min_lr: 0.000000  loss: 3.8219 (3.8948)  class_acc: 0.3750 (0.2932)  loss_scale: 65536.0000 (57792.7562)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1422  max mem: 15572
[2025-01-15 23:38:05,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:38:05,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:38:06,051] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54305
[2025-01-15 23:38:06,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:38:06,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 940/2809]  eta: 0:17:44  lr: 0.000030  min_lr: 0.000000  loss: 3.9466 (3.8944)  class_acc: 0.3750 (0.2934)  loss_scale: 65536.0000 (57944.6886)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1281  max mem: 15572
Epoch: [19]  [ 950/2809]  eta: 0:17:37  lr: 0.000030  min_lr: 0.000000  loss: 3.8595 (3.8940)  class_acc: 0.2917 (0.2931)  loss_scale: 65536.0000 (58024.5131)  weight_decay: 0.0500 (0.0500)  time: 0.4944  data: 0.0575  max mem: 15572
Epoch: [19]  [ 960/2809]  eta: 0:17:33  lr: 0.000030  min_lr: 0.000000  loss: 3.8595 (3.8938)  class_acc: 0.2500 (0.2927)  loss_scale: 65536.0000 (58102.6764)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1248  max mem: 15572
Epoch: [19]  [ 970/2809]  eta: 0:17:25  lr: 0.000030  min_lr: 0.000000  loss: 3.9375 (3.8948)  class_acc: 0.2083 (0.2924)  loss_scale: 65536.0000 (58179.2297)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0964  max mem: 15572
Epoch: [19]  [ 980/2809]  eta: 0:17:19  lr: 0.000030  min_lr: 0.000000  loss: 3.9365 (3.8947)  class_acc: 0.2917 (0.2926)  loss_scale: 65536.0000 (58254.2222)  weight_decay: 0.0500 (0.0500)  time: 0.4866  data: 0.0487  max mem: 15572
Epoch: [19]  [ 990/2809]  eta: 0:17:15  lr: 0.000030  min_lr: 0.000000  loss: 3.8220 (3.8944)  class_acc: 0.3333 (0.2928)  loss_scale: 65536.0000 (58327.7013)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1725  max mem: 15572
Epoch: [19]  [1000/2809]  eta: 0:17:09  lr: 0.000030  min_lr: 0.000000  loss: 3.5866 (3.8911)  class_acc: 0.3333 (0.2933)  loss_scale: 65536.0000 (58399.7123)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1728  max mem: 15572
Epoch: [19]  [1010/2809]  eta: 0:17:02  lr: 0.000030  min_lr: 0.000000  loss: 3.6741 (3.8925)  class_acc: 0.3333 (0.2930)  loss_scale: 65536.0000 (58470.2987)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0880  max mem: 15572
Epoch: [19]  [1020/2809]  eta: 0:16:56  lr: 0.000030  min_lr: 0.000000  loss: 4.0584 (3.8941)  class_acc: 0.2500 (0.2927)  loss_scale: 65536.0000 (58539.5024)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0926  max mem: 15572
Epoch: [19]  [1030/2809]  eta: 0:16:50  lr: 0.000030  min_lr: 0.000000  loss: 3.9287 (3.8938)  class_acc: 0.2500 (0.2926)  loss_scale: 65536.0000 (58607.3637)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1141  max mem: 15572
Epoch: [19]  [1040/2809]  eta: 0:16:46  lr: 0.000030  min_lr: 0.000000  loss: 3.7583 (3.8922)  class_acc: 0.2917 (0.2927)  loss_scale: 65536.0000 (58673.9212)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1740  max mem: 15572
Epoch: [19]  [1050/2809]  eta: 0:16:40  lr: 0.000030  min_lr: 0.000000  loss: 3.8168 (3.8913)  class_acc: 0.2917 (0.2929)  loss_scale: 65536.0000 (58739.2122)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1636  max mem: 15572
Epoch: [19]  [1060/2809]  eta: 0:16:35  lr: 0.000030  min_lr: 0.000000  loss: 3.8415 (3.8917)  class_acc: 0.2917 (0.2926)  loss_scale: 65536.0000 (58803.2724)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1249  max mem: 15572
[2025-01-15 23:39:18,434] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:39:18,435] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:39:19,644] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54436
[2025-01-15 23:39:19,644] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:39:19,645] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1070/2809]  eta: 0:16:29  lr: 0.000030  min_lr: 0.000000  loss: 3.8421 (3.8912)  class_acc: 0.2917 (0.2927)  loss_scale: 65536.0000 (58988.5191)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1399  max mem: 15572
Epoch: [19]  [1080/2809]  eta: 0:16:23  lr: 0.000030  min_lr: 0.000000  loss: 3.7196 (3.8886)  class_acc: 0.3333 (0.2932)  loss_scale: 65536.0000 (59049.0879)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1338  max mem: 15572
Epoch: [19]  [1090/2809]  eta: 0:16:17  lr: 0.000030  min_lr: 0.000000  loss: 3.7345 (3.8891)  class_acc: 0.2917 (0.2930)  loss_scale: 65536.0000 (59108.5463)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1444  max mem: 15572
Epoch: [19]  [1100/2809]  eta: 0:16:12  lr: 0.000030  min_lr: 0.000000  loss: 3.9586 (3.8897)  class_acc: 0.2917 (0.2931)  loss_scale: 65536.0000 (59166.9246)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1610  max mem: 15572
Epoch: [19]  [1110/2809]  eta: 0:16:07  lr: 0.000030  min_lr: 0.000000  loss: 4.0594 (3.8903)  class_acc: 0.2917 (0.2932)  loss_scale: 65536.0000 (59224.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1732  max mem: 15572
Epoch: [19]  [1120/2809]  eta: 0:16:01  lr: 0.000030  min_lr: 0.000000  loss: 3.8896 (3.8882)  class_acc: 0.3333 (0.2940)  loss_scale: 65536.0000 (59280.5566)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1467  max mem: 15572
Epoch: [19]  [1130/2809]  eta: 0:15:56  lr: 0.000030  min_lr: 0.000000  loss: 3.8896 (3.8898)  class_acc: 0.2917 (0.2935)  loss_scale: 65536.0000 (59335.8656)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1725  max mem: 15572
Epoch: [19]  [1140/2809]  eta: 0:15:51  lr: 0.000030  min_lr: 0.000000  loss: 3.9383 (3.8892)  class_acc: 0.2500 (0.2939)  loss_scale: 65536.0000 (59390.2051)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.2044  max mem: 15572
Epoch: [19]  [1150/2809]  eta: 0:15:47  lr: 0.000030  min_lr: 0.000000  loss: 3.7182 (3.8883)  class_acc: 0.3333 (0.2944)  loss_scale: 65536.0000 (59443.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.2103  max mem: 15572
Epoch: [19]  [1160/2809]  eta: 0:15:40  lr: 0.000030  min_lr: 0.000000  loss: 3.8406 (3.8881)  class_acc: 0.2917 (0.2943)  loss_scale: 65536.0000 (59496.0758)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1354  max mem: 15572
Epoch: [19]  [1170/2809]  eta: 0:15:35  lr: 0.000030  min_lr: 0.000000  loss: 3.8427 (3.8870)  class_acc: 0.2917 (0.2947)  loss_scale: 65536.0000 (59547.6550)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1124  max mem: 15572
Epoch: [19]  [1180/2809]  eta: 0:15:30  lr: 0.000030  min_lr: 0.000000  loss: 3.8786 (3.8888)  class_acc: 0.2500 (0.2940)  loss_scale: 65536.0000 (59598.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1824  max mem: 15572
Epoch: [19]  [1190/2809]  eta: 0:15:25  lr: 0.000030  min_lr: 0.000000  loss: 4.1544 (3.8892)  class_acc: 0.2083 (0.2938)  loss_scale: 65536.0000 (59648.2149)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1976  max mem: 15572
[2025-01-15 23:40:35,121] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:40:35,121] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:40:36,803] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54569
[2025-01-15 23:40:36,803] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:40:36,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1200/2809]  eta: 0:15:17  lr: 0.000030  min_lr: 0.000000  loss: 3.9651 (3.8893)  class_acc: 0.2500 (0.2937)  loss_scale: 65536.0000 (59915.5104)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.1126  max mem: 15572
Epoch: [19]  [1210/2809]  eta: 0:15:11  lr: 0.000030  min_lr: 0.000000  loss: 3.9185 (3.8882)  class_acc: 0.2500 (0.2938)  loss_scale: 65536.0000 (59961.9224)  weight_decay: 0.0500 (0.0500)  time: 0.4857  data: 0.0363  max mem: 15572
Epoch: [19]  [1220/2809]  eta: 0:15:05  lr: 0.000030  min_lr: 0.000000  loss: 3.9185 (3.8899)  class_acc: 0.2917 (0.2935)  loss_scale: 65536.0000 (60007.5741)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0892  max mem: 15572
Epoch: [19]  [1230/2809]  eta: 0:14:59  lr: 0.000030  min_lr: 0.000000  loss: 3.9915 (3.8899)  class_acc: 0.2917 (0.2935)  loss_scale: 65536.0000 (60052.4842)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0965  max mem: 15572
Epoch: [19]  [1240/2809]  eta: 0:14:53  lr: 0.000030  min_lr: 0.000000  loss: 3.9166 (3.8884)  class_acc: 0.2917 (0.2937)  loss_scale: 65536.0000 (60096.6704)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1056  max mem: 15572
Epoch: [19]  [1250/2809]  eta: 0:14:47  lr: 0.000030  min_lr: 0.000000  loss: 3.9142 (3.8886)  class_acc: 0.2917 (0.2938)  loss_scale: 65536.0000 (60140.1503)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1273  max mem: 15572
Epoch: [19]  [1260/2809]  eta: 0:14:41  lr: 0.000030  min_lr: 0.000000  loss: 3.9809 (3.8891)  class_acc: 0.2500 (0.2936)  loss_scale: 65536.0000 (60182.9405)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1312  max mem: 15572
Epoch: [19]  [1270/2809]  eta: 0:14:37  lr: 0.000030  min_lr: 0.000000  loss: 3.7916 (3.8898)  class_acc: 0.2500 (0.2936)  loss_scale: 65536.0000 (60225.0574)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1727  max mem: 15572
Epoch: [19]  [1280/2809]  eta: 0:14:31  lr: 0.000030  min_lr: 0.000000  loss: 3.7222 (3.8881)  class_acc: 0.3333 (0.2941)  loss_scale: 65536.0000 (60266.5168)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1564  max mem: 15572
Epoch: [19]  [1290/2809]  eta: 0:14:25  lr: 0.000030  min_lr: 0.000000  loss: 3.5126 (3.8858)  class_acc: 0.3333 (0.2950)  loss_scale: 65536.0000 (60307.3338)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1196  max mem: 15572
Epoch: [19]  [1300/2809]  eta: 0:14:19  lr: 0.000030  min_lr: 0.000000  loss: 3.5126 (3.8846)  class_acc: 0.4167 (0.2954)  loss_scale: 65536.0000 (60347.5234)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0874  max mem: 15572
Epoch: [19]  [1310/2809]  eta: 0:14:13  lr: 0.000030  min_lr: 0.000000  loss: 3.8488 (3.8861)  class_acc: 0.2917 (0.2951)  loss_scale: 65536.0000 (60387.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0774  max mem: 15572
Epoch: [19]  [1320/2809]  eta: 0:14:07  lr: 0.000030  min_lr: 0.000000  loss: 3.8977 (3.8850)  class_acc: 0.2500 (0.2951)  loss_scale: 65536.0000 (60426.0772)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0905  max mem: 15572
[2025-01-15 23:41:48,077] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:41:48,077] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:41:48,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54700
[2025-01-15 23:41:48,899] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:41:48,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1330/2809]  eta: 0:14:00  lr: 0.000030  min_lr: 0.000000  loss: 3.6416 (3.8831)  class_acc: 0.2500 (0.2953)  loss_scale: 65536.0000 (60562.9452)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0542  max mem: 15572
Epoch: [19]  [1340/2809]  eta: 0:13:55  lr: 0.000030  min_lr: 0.000000  loss: 3.6987 (3.8832)  class_acc: 0.2500 (0.2951)  loss_scale: 65536.0000 (60600.0298)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0856  max mem: 15572
Epoch: [19]  [1350/2809]  eta: 0:13:49  lr: 0.000030  min_lr: 0.000000  loss: 3.7857 (3.8825)  class_acc: 0.2500 (0.2952)  loss_scale: 65536.0000 (60636.5655)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1095  max mem: 15572
Epoch: [19]  [1360/2809]  eta: 0:13:44  lr: 0.000030  min_lr: 0.000000  loss: 3.7067 (3.8812)  class_acc: 0.2917 (0.2952)  loss_scale: 65536.0000 (60672.5643)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1630  max mem: 15572
Epoch: [19]  [1370/2809]  eta: 0:13:38  lr: 0.000030  min_lr: 0.000000  loss: 3.7067 (3.8808)  class_acc: 0.2917 (0.2950)  loss_scale: 65536.0000 (60708.0379)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1512  max mem: 15572
Epoch: [19]  [1380/2809]  eta: 0:13:32  lr: 0.000030  min_lr: 0.000000  loss: 3.9631 (3.8807)  class_acc: 0.2500 (0.2951)  loss_scale: 65536.0000 (60742.9978)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0985  max mem: 15572
Epoch: [19]  [1390/2809]  eta: 0:13:27  lr: 0.000030  min_lr: 0.000000  loss: 3.6943 (3.8791)  class_acc: 0.3750 (0.2957)  loss_scale: 65536.0000 (60777.4551)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1155  max mem: 15572
Epoch: [19]  [1400/2809]  eta: 0:13:21  lr: 0.000030  min_lr: 0.000000  loss: 3.6079 (3.8779)  class_acc: 0.3750 (0.2959)  loss_scale: 65536.0000 (60811.4204)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1351  max mem: 15572
Epoch: [19]  [1410/2809]  eta: 0:13:15  lr: 0.000030  min_lr: 0.000000  loss: 3.7401 (3.8778)  class_acc: 0.3333 (0.2959)  loss_scale: 65536.0000 (60844.9043)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1263  max mem: 15572
Epoch: [19]  [1420/2809]  eta: 0:13:10  lr: 0.000030  min_lr: 0.000000  loss: 3.8456 (3.8782)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (60877.9170)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1291  max mem: 15572
Epoch: [19]  [1430/2809]  eta: 0:13:04  lr: 0.000030  min_lr: 0.000000  loss: 3.8723 (3.8781)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (60910.4682)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1356  max mem: 15572
Epoch: [19]  [1440/2809]  eta: 0:12:57  lr: 0.000030  min_lr: 0.000000  loss: 3.8723 (3.8782)  class_acc: 0.2917 (0.2957)  loss_scale: 65536.0000 (60942.5677)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0738  max mem: 15572
Epoch: [19]  [1450/2809]  eta: 0:12:51  lr: 0.000030  min_lr: 0.000000  loss: 3.7767 (3.8781)  class_acc: 0.2500 (0.2957)  loss_scale: 65536.0000 (60974.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5178  data: 0.0831  max mem: 15572
[2025-01-15 23:43:01,240] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:43:01,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1460/2809]  eta: 0:12:45  lr: 0.000030  min_lr: 0.000000  loss: 3.8581 (3.8790)  class_acc: 0.2500 (0.2952)  loss_scale: 65536.0000 (61140.0192)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1061  max mem: 15572
[2025-01-15 23:43:04,353] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54836
[2025-01-15 23:43:04,354] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:43:04,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1470/2809]  eta: 0:12:39  lr: 0.000030  min_lr: 0.000000  loss: 3.8434 (3.8771)  class_acc: 0.2917 (0.2956)  loss_scale: 65536.0000 (61348.1115)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1016  max mem: 15572
Epoch: [19]  [1480/2809]  eta: 0:12:33  lr: 0.000030  min_lr: 0.000000  loss: 3.8242 (3.8771)  class_acc: 0.3333 (0.2958)  loss_scale: 65536.0000 (61376.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0873  max mem: 15572
Epoch: [19]  [1490/2809]  eta: 0:12:28  lr: 0.000030  min_lr: 0.000000  loss: 3.9803 (3.8772)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (61404.2871)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1154  max mem: 15572
Epoch: [19]  [1500/2809]  eta: 0:12:22  lr: 0.000030  min_lr: 0.000000  loss: 3.8774 (3.8770)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (61431.8135)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1185  max mem: 15572
Epoch: [19]  [1510/2809]  eta: 0:12:16  lr: 0.000030  min_lr: 0.000000  loss: 4.0488 (3.8789)  class_acc: 0.2917 (0.2955)  loss_scale: 65536.0000 (61458.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0949  max mem: 15572
Epoch: [19]  [1520/2809]  eta: 0:12:10  lr: 0.000030  min_lr: 0.000000  loss: 4.0488 (3.8784)  class_acc: 0.2917 (0.2955)  loss_scale: 65536.0000 (61485.7804)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1051  max mem: 15572
Epoch: [19]  [1530/2809]  eta: 0:12:05  lr: 0.000030  min_lr: 0.000000  loss: 4.0192 (3.8800)  class_acc: 0.2500 (0.2950)  loss_scale: 65536.0000 (61512.2351)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1327  max mem: 15572
Epoch: [19]  [1540/2809]  eta: 0:11:59  lr: 0.000030  min_lr: 0.000000  loss: 4.0192 (3.8798)  class_acc: 0.2500 (0.2952)  loss_scale: 65536.0000 (61538.3465)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1372  max mem: 15572
Epoch: [19]  [1550/2809]  eta: 0:11:54  lr: 0.000030  min_lr: 0.000000  loss: 3.8916 (3.8798)  class_acc: 0.2917 (0.2952)  loss_scale: 65536.0000 (61564.1212)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1519  max mem: 15572
Epoch: [19]  [1560/2809]  eta: 0:11:48  lr: 0.000030  min_lr: 0.000000  loss: 3.8915 (3.8799)  class_acc: 0.2917 (0.2949)  loss_scale: 65536.0000 (61589.5657)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1510  max mem: 15572
Epoch: [19]  [1570/2809]  eta: 0:11:42  lr: 0.000030  min_lr: 0.000000  loss: 4.1460 (3.8813)  class_acc: 0.2083 (0.2945)  loss_scale: 65536.0000 (61614.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.0794  max mem: 15572
Epoch: [19]  [1580/2809]  eta: 0:11:36  lr: 0.000030  min_lr: 0.000000  loss: 4.0024 (3.8801)  class_acc: 0.2500 (0.2950)  loss_scale: 65536.0000 (61639.4889)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0791  max mem: 15572
Epoch: [19]  [1590/2809]  eta: 0:11:30  lr: 0.000030  min_lr: 0.000000  loss: 3.8860 (3.8797)  class_acc: 0.2917 (0.2949)  loss_scale: 65536.0000 (61663.9799)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0615  max mem: 15572
[2025-01-15 23:44:15,714] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:44:15,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:44:19,954] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54970
[2025-01-15 23:44:19,954] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:44:19,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1600/2809]  eta: 0:11:25  lr: 0.000030  min_lr: 0.000000  loss: 3.8860 (3.8785)  class_acc: 0.2500 (0.2950)  loss_scale: 65536.0000 (61892.8370)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1216  max mem: 15572
[2025-01-15 23:44:21,453] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 54973
[2025-01-15 23:44:21,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:44:21,453] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [1610/2809]  eta: 0:11:19  lr: 0.000030  min_lr: 0.000000  loss: 3.7728 (3.8790)  class_acc: 0.2917 (0.2948)  loss_scale: 65536.0000 (61732.3898)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1624  max mem: 15572
Epoch: [19]  [1620/2809]  eta: 0:11:14  lr: 0.000030  min_lr: 0.000000  loss: 3.9604 (3.8797)  class_acc: 0.2500 (0.2949)  loss_scale: 32768.0000 (61553.7076)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1340  max mem: 15572
[2025-01-15 23:44:37,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=55000, skipped=372, lr=[2.860995950623248e-07, 2.860995950623248e-07, 4.087137072318926e-07, 4.087137072318926e-07, 5.838767246169895e-07, 5.838767246169895e-07, 8.341096065956993e-07, 8.341096065956993e-07, 1.1915851522795705e-06, 1.1915851522795705e-06, 1.7022645032565293e-06, 1.7022645032565293e-06, 2.4318064332236134e-06, 2.4318064332236134e-06, 3.474009190319448e-06, 3.474009190319448e-06, 4.962870271884926e-06, 4.962870271884926e-06, 7.089814674121324e-06, 7.089814674121324e-06, 1.0128306677316177e-05, 1.0128306677316177e-05, 1.446900953902311e-05, 1.446900953902311e-05, 2.0670013627175875e-05, 2.0670013627175875e-05, 2.9528590895965536e-05, 2.9528590895965536e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:44:37,156] [INFO] [timer.py:260:stop] epoch=0/micro_step=55000/global_step=55000, RunningAvgSamplesPerSec=28.50041733327195, CurrSamplesPerSec=26.277013195504914, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [1630/2809]  eta: 0:11:08  lr: 0.000030  min_lr: 0.000000  loss: 3.8072 (3.8789)  class_acc: 0.3333 (0.2953)  loss_scale: 32768.0000 (61377.2164)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1662  max mem: 15572
Epoch: [19]  [1640/2809]  eta: 0:11:03  lr: 0.000030  min_lr: 0.000000  loss: 3.7533 (3.8787)  class_acc: 0.3333 (0.2955)  loss_scale: 32768.0000 (61202.8763)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1702  max mem: 15572
Epoch: [19]  [1650/2809]  eta: 0:10:58  lr: 0.000030  min_lr: 0.000000  loss: 3.9064 (3.8787)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (61030.6481)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1740  max mem: 15572
Epoch: [19]  [1660/2809]  eta: 0:10:52  lr: 0.000030  min_lr: 0.000000  loss: 4.0283 (3.8791)  class_acc: 0.2500 (0.2951)  loss_scale: 32768.0000 (60860.4937)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1260  max mem: 15572
Epoch: [19]  [1670/2809]  eta: 0:10:46  lr: 0.000029  min_lr: 0.000000  loss: 3.8916 (3.8780)  class_acc: 0.2500 (0.2952)  loss_scale: 32768.0000 (60692.3758)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0798  max mem: 15572
Epoch: [19]  [1680/2809]  eta: 0:10:41  lr: 0.000029  min_lr: 0.000000  loss: 3.7562 (3.8784)  class_acc: 0.2500 (0.2951)  loss_scale: 32768.0000 (60526.2582)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1473  max mem: 15572
Epoch: [19]  [1690/2809]  eta: 0:10:35  lr: 0.000029  min_lr: 0.000000  loss: 3.9756 (3.8781)  class_acc: 0.2917 (0.2954)  loss_scale: 32768.0000 (60362.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1422  max mem: 15572
Epoch: [19]  [1700/2809]  eta: 0:10:29  lr: 0.000029  min_lr: 0.000000  loss: 3.9697 (3.8783)  class_acc: 0.3333 (0.2954)  loss_scale: 32768.0000 (60199.8824)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0946  max mem: 15572
Epoch: [19]  [1710/2809]  eta: 0:10:24  lr: 0.000029  min_lr: 0.000000  loss: 3.8877 (3.8775)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (60039.5558)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1321  max mem: 15572
Epoch: [19]  [1720/2809]  eta: 0:10:18  lr: 0.000029  min_lr: 0.000000  loss: 3.7606 (3.8762)  class_acc: 0.3333 (0.2960)  loss_scale: 32768.0000 (59881.0924)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1228  max mem: 15572
Epoch: [19]  [1730/2809]  eta: 0:10:12  lr: 0.000029  min_lr: 0.000000  loss: 3.7606 (3.8766)  class_acc: 0.3333 (0.2957)  loss_scale: 32768.0000 (59724.4598)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1006  max mem: 15572
[2025-01-15 23:45:36,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:45:36,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [1740/2809]  eta: 0:10:06  lr: 0.000029  min_lr: 0.000000  loss: 4.0766 (3.8781)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (59757.8403)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0538  max mem: 15572
Epoch: [19]  [1750/2809]  eta: 0:10:00  lr: 0.000029  min_lr: 0.000000  loss: 4.0334 (3.8773)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (59790.8395)  weight_decay: 0.0500 (0.0500)  time: 0.4890  data: 0.0366  max mem: 15572
Epoch: [19]  [1760/2809]  eta: 0:09:54  lr: 0.000029  min_lr: 0.000000  loss: 3.8288 (3.8771)  class_acc: 0.2917 (0.2961)  loss_scale: 65536.0000 (59823.4639)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0771  max mem: 15572
Epoch: [19]  [1770/2809]  eta: 0:09:48  lr: 0.000029  min_lr: 0.000000  loss: 3.8288 (3.8777)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (59855.7199)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0982  max mem: 15572
Epoch: [19]  [1780/2809]  eta: 0:09:43  lr: 0.000029  min_lr: 0.000000  loss: 3.8203 (3.8775)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (59887.6137)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0952  max mem: 15572
Epoch: [19]  [1790/2809]  eta: 0:09:37  lr: 0.000029  min_lr: 0.000000  loss: 3.8431 (3.8779)  class_acc: 0.2917 (0.2957)  loss_scale: 65536.0000 (59919.1513)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1210  max mem: 15572
Epoch: [19]  [1800/2809]  eta: 0:09:31  lr: 0.000029  min_lr: 0.000000  loss: 3.8612 (3.8775)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (59950.3387)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1182  max mem: 15572
Epoch: [19]  [1810/2809]  eta: 0:09:25  lr: 0.000029  min_lr: 0.000000  loss: 3.8586 (3.8777)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (59981.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0650  max mem: 15572
Epoch: [19]  [1820/2809]  eta: 0:09:19  lr: 0.000029  min_lr: 0.000000  loss: 4.0339 (3.8783)  class_acc: 0.2917 (0.2957)  loss_scale: 65536.0000 (60011.6859)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0705  max mem: 15572
Epoch: [19]  [1830/2809]  eta: 0:09:14  lr: 0.000029  min_lr: 0.000000  loss: 4.0339 (3.8786)  class_acc: 0.2500 (0.2953)  loss_scale: 65536.0000 (60041.8569)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0980  max mem: 15572
Epoch: [19]  [1840/2809]  eta: 0:09:08  lr: 0.000029  min_lr: 0.000000  loss: 3.9915 (3.8792)  class_acc: 0.2500 (0.2953)  loss_scale: 65536.0000 (60071.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1325  max mem: 15572
Epoch: [19]  [1850/2809]  eta: 0:09:03  lr: 0.000029  min_lr: 0.000000  loss: 3.8706 (3.8793)  class_acc: 0.2917 (0.2953)  loss_scale: 65536.0000 (60101.2210)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1324  max mem: 15572
[2025-01-15 23:46:46,716] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:46:46,716] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1860/2809]  eta: 0:08:57  lr: 0.000029  min_lr: 0.000000  loss: 3.8328 (3.8792)  class_acc: 0.2917 (0.2954)  loss_scale: 65536.0000 (60200.8555)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1267  max mem: 15572
[2025-01-15 23:46:48,087] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55233
[2025-01-15 23:46:48,087] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:46:48,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1870/2809]  eta: 0:08:52  lr: 0.000029  min_lr: 0.000000  loss: 3.7230 (3.8787)  class_acc: 0.2917 (0.2953)  loss_scale: 65536.0000 (60264.3976)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1650  max mem: 15572
Epoch: [19]  [1880/2809]  eta: 0:08:46  lr: 0.000029  min_lr: 0.000000  loss: 3.7032 (3.8781)  class_acc: 0.2917 (0.2955)  loss_scale: 65536.0000 (60292.4232)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1634  max mem: 15572
Epoch: [19]  [1890/2809]  eta: 0:08:40  lr: 0.000029  min_lr: 0.000000  loss: 3.7873 (3.8778)  class_acc: 0.3333 (0.2957)  loss_scale: 65536.0000 (60320.1523)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1269  max mem: 15572
Epoch: [19]  [1900/2809]  eta: 0:08:35  lr: 0.000029  min_lr: 0.000000  loss: 3.9175 (3.8775)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (60347.5897)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1197  max mem: 15572
Epoch: [19]  [1910/2809]  eta: 0:08:29  lr: 0.000029  min_lr: 0.000000  loss: 3.9139 (3.8773)  class_acc: 0.2500 (0.2957)  loss_scale: 65536.0000 (60374.7399)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1654  max mem: 15572
Epoch: [19]  [1920/2809]  eta: 0:08:23  lr: 0.000029  min_lr: 0.000000  loss: 3.9139 (3.8770)  class_acc: 0.2917 (0.2957)  loss_scale: 65536.0000 (60401.6075)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1127  max mem: 15572
Epoch: [19]  [1930/2809]  eta: 0:08:17  lr: 0.000029  min_lr: 0.000000  loss: 3.7804 (3.8763)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (60428.1968)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0692  max mem: 15572
Epoch: [19]  [1940/2809]  eta: 0:08:12  lr: 0.000029  min_lr: 0.000000  loss: 3.7804 (3.8766)  class_acc: 0.2917 (0.2960)  loss_scale: 65536.0000 (60454.5121)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1657  max mem: 15572
Epoch: [19]  [1950/2809]  eta: 0:08:07  lr: 0.000029  min_lr: 0.000000  loss: 3.8512 (3.8766)  class_acc: 0.2500 (0.2959)  loss_scale: 65536.0000 (60480.5577)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1652  max mem: 15572
Epoch: [19]  [1960/2809]  eta: 0:08:01  lr: 0.000029  min_lr: 0.000000  loss: 4.0294 (3.8776)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (60506.3376)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0958  max mem: 15572
Epoch: [19]  [1970/2809]  eta: 0:07:55  lr: 0.000029  min_lr: 0.000000  loss: 4.0294 (3.8780)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (60531.8559)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1057  max mem: 15572
Epoch: [19]  [1980/2809]  eta: 0:07:49  lr: 0.000029  min_lr: 0.000000  loss: 4.0018 (3.8785)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (60557.1166)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1084  max mem: 15572
Epoch: [19]  [1990/2809]  eta: 0:07:44  lr: 0.000029  min_lr: 0.000000  loss: 3.9949 (3.8781)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (60582.1236)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1024  max mem: 15572
[2025-01-15 23:48:01,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:48:01,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:48:02,875] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55364
[2025-01-15 23:48:02,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:48:02,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2000/2809]  eta: 0:07:38  lr: 0.000029  min_lr: 0.000000  loss: 3.9351 (3.8788)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (60672.3838)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0698  max mem: 15572
Epoch: [19]  [2010/2809]  eta: 0:07:32  lr: 0.000029  min_lr: 0.000000  loss: 4.0489 (3.8797)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (60696.5689)  weight_decay: 0.0500 (0.0500)  time: 0.4934  data: 0.0545  max mem: 15572
Epoch: [19]  [2020/2809]  eta: 0:07:26  lr: 0.000029  min_lr: 0.000000  loss: 3.8318 (3.8797)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (60720.5146)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1298  max mem: 15572
Epoch: [19]  [2030/2809]  eta: 0:07:21  lr: 0.000029  min_lr: 0.000000  loss: 3.7821 (3.8801)  class_acc: 0.2917 (0.2960)  loss_scale: 65536.0000 (60744.2245)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1396  max mem: 15572
Epoch: [19]  [2040/2809]  eta: 0:07:15  lr: 0.000029  min_lr: 0.000000  loss: 3.8981 (3.8809)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (60767.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0947  max mem: 15572
Epoch: [19]  [2050/2809]  eta: 0:07:09  lr: 0.000029  min_lr: 0.000000  loss: 4.1264 (3.8816)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (60790.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0545  max mem: 15572
Epoch: [19]  [2060/2809]  eta: 0:07:03  lr: 0.000029  min_lr: 0.000000  loss: 4.1086 (3.8821)  class_acc: 0.2500 (0.2955)  loss_scale: 65536.0000 (60813.9738)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1293  max mem: 15572
Epoch: [19]  [2070/2809]  eta: 0:06:58  lr: 0.000029  min_lr: 0.000000  loss: 4.0012 (3.8824)  class_acc: 0.2500 (0.2953)  loss_scale: 65536.0000 (60836.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.2138  max mem: 15572
Epoch: [19]  [2080/2809]  eta: 0:06:52  lr: 0.000029  min_lr: 0.000000  loss: 3.9368 (3.8818)  class_acc: 0.2500 (0.2955)  loss_scale: 65536.0000 (60859.3561)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1574  max mem: 15572
Epoch: [19]  [2090/2809]  eta: 0:06:47  lr: 0.000029  min_lr: 0.000000  loss: 3.6639 (3.8815)  class_acc: 0.2917 (0.2956)  loss_scale: 65536.0000 (60881.7217)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0983  max mem: 15572
Epoch: [19]  [2100/2809]  eta: 0:06:41  lr: 0.000029  min_lr: 0.000000  loss: 3.6339 (3.8808)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (60903.8743)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1471  max mem: 15572
Epoch: [19]  [2110/2809]  eta: 0:06:35  lr: 0.000029  min_lr: 0.000000  loss: 3.9429 (3.8820)  class_acc: 0.2917 (0.2955)  loss_scale: 65536.0000 (60925.8171)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1246  max mem: 15572
Epoch: [19]  [2120/2809]  eta: 0:06:30  lr: 0.000029  min_lr: 0.000000  loss: 4.0797 (3.8825)  class_acc: 0.2083 (0.2953)  loss_scale: 65536.0000 (60947.5530)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0390  max mem: 15572
[2025-01-15 23:49:14,539] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:49:14,539] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:49:15,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55494
[2025-01-15 23:49:15,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:49:15,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2130/2809]  eta: 0:06:24  lr: 0.000029  min_lr: 0.000000  loss: 3.9073 (3.8814)  class_acc: 0.3333 (0.2956)  loss_scale: 65536.0000 (60999.8386)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0562  max mem: 15572
Epoch: [19]  [2140/2809]  eta: 0:06:18  lr: 0.000029  min_lr: 0.000000  loss: 3.4906 (3.8801)  class_acc: 0.3750 (0.2959)  loss_scale: 65536.0000 (61021.0257)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.0836  max mem: 15572
[2025-01-15 23:49:25,662] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55513
[2025-01-15 23:49:25,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:49:25,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2150/2809]  eta: 0:06:12  lr: 0.000029  min_lr: 0.000000  loss: 3.9237 (3.8803)  class_acc: 0.3750 (0.2958)  loss_scale: 65536.0000 (60904.9112)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0827  max mem: 15572
Epoch: [19]  [2160/2809]  eta: 0:06:07  lr: 0.000029  min_lr: 0.000000  loss: 4.0648 (3.8816)  class_acc: 0.2083 (0.2953)  loss_scale: 32768.0000 (60774.7080)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1131  max mem: 15572
Epoch: [19]  [2170/2809]  eta: 0:06:01  lr: 0.000029  min_lr: 0.000000  loss: 3.9952 (3.8812)  class_acc: 0.2083 (0.2953)  loss_scale: 32768.0000 (60645.7043)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1477  max mem: 15572
Epoch: [19]  [2180/2809]  eta: 0:05:56  lr: 0.000029  min_lr: 0.000000  loss: 3.6968 (3.8810)  class_acc: 0.2917 (0.2955)  loss_scale: 32768.0000 (60517.8835)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1435  max mem: 15572
Epoch: [19]  [2190/2809]  eta: 0:05:50  lr: 0.000029  min_lr: 0.000000  loss: 4.0509 (3.8827)  class_acc: 0.2500 (0.2951)  loss_scale: 32768.0000 (60391.2296)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1012  max mem: 15572
Epoch: [19]  [2200/2809]  eta: 0:05:44  lr: 0.000029  min_lr: 0.000000  loss: 4.0132 (3.8824)  class_acc: 0.2500 (0.2950)  loss_scale: 32768.0000 (60265.7265)  weight_decay: 0.0500 (0.0500)  time: 0.5668  data: 0.1092  max mem: 15572
Epoch: [19]  [2210/2809]  eta: 0:05:38  lr: 0.000029  min_lr: 0.000000  loss: 3.8322 (3.8820)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (60141.3587)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0794  max mem: 15572
Epoch: [19]  [2220/2809]  eta: 0:05:33  lr: 0.000029  min_lr: 0.000000  loss: 3.9928 (3.8821)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (60018.1108)  weight_decay: 0.0500 (0.0500)  time: 0.4896  data: 0.0528  max mem: 15572
Epoch: [19]  [2230/2809]  eta: 0:05:27  lr: 0.000029  min_lr: 0.000000  loss: 3.9164 (3.8823)  class_acc: 0.2917 (0.2952)  loss_scale: 32768.0000 (59895.9677)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1219  max mem: 15572
Epoch: [19]  [2240/2809]  eta: 0:05:21  lr: 0.000029  min_lr: 0.000000  loss: 3.9115 (3.8823)  class_acc: 0.2917 (0.2953)  loss_scale: 32768.0000 (59774.9148)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1442  max mem: 15572
Epoch: [19]  [2250/2809]  eta: 0:05:16  lr: 0.000029  min_lr: 0.000000  loss: 3.8895 (3.8822)  class_acc: 0.2917 (0.2955)  loss_scale: 32768.0000 (59654.9374)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1077  max mem: 15572
Epoch: [19]  [2260/2809]  eta: 0:05:10  lr: 0.000029  min_lr: 0.000000  loss: 3.9574 (3.8826)  class_acc: 0.2917 (0.2954)  loss_scale: 32768.0000 (59536.0212)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0853  max mem: 15572
Epoch: [19]  [2270/2809]  eta: 0:05:04  lr: 0.000029  min_lr: 0.000000  loss: 3.9570 (3.8821)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (59418.1524)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1223  max mem: 15572
[2025-01-15 23:50:37,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:50:37,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2280/2809]  eta: 0:04:59  lr: 0.000029  min_lr: 0.000000  loss: 3.8609 (3.8817)  class_acc: 0.2500 (0.2955)  loss_scale: 32768.0000 (59444.9733)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1385  max mem: 15572
Epoch: [19]  [2290/2809]  eta: 0:04:53  lr: 0.000029  min_lr: 0.000000  loss: 3.6071 (3.8796)  class_acc: 0.3333 (0.2958)  loss_scale: 65536.0000 (59471.5600)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1134  max mem: 15572
Epoch: [19]  [2300/2809]  eta: 0:04:47  lr: 0.000029  min_lr: 0.000000  loss: 3.5543 (3.8794)  class_acc: 0.3333 (0.2958)  loss_scale: 65536.0000 (59497.9157)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1365  max mem: 15572
Epoch: [19]  [2310/2809]  eta: 0:04:42  lr: 0.000029  min_lr: 0.000000  loss: 3.8387 (3.8790)  class_acc: 0.2500 (0.2957)  loss_scale: 65536.0000 (59524.0433)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1870  max mem: 15572
Epoch: [19]  [2320/2809]  eta: 0:04:36  lr: 0.000029  min_lr: 0.000000  loss: 3.9563 (3.8793)  class_acc: 0.2917 (0.2956)  loss_scale: 65536.0000 (59549.9457)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.1888  max mem: 15572
Epoch: [19]  [2330/2809]  eta: 0:04:31  lr: 0.000029  min_lr: 0.000000  loss: 4.0046 (3.8798)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (59575.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1817  max mem: 15572
Epoch: [19]  [2340/2809]  eta: 0:04:25  lr: 0.000029  min_lr: 0.000000  loss: 4.0380 (3.8798)  class_acc: 0.2500 (0.2956)  loss_scale: 65536.0000 (59601.0867)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1304  max mem: 15572
Epoch: [19]  [2350/2809]  eta: 0:04:19  lr: 0.000029  min_lr: 0.000000  loss: 3.8457 (3.8791)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (59626.3309)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0422  max mem: 15572
Epoch: [19]  [2360/2809]  eta: 0:04:13  lr: 0.000029  min_lr: 0.000000  loss: 3.7702 (3.8787)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (59651.3613)  weight_decay: 0.0500 (0.0500)  time: 0.4795  data: 0.0228  max mem: 15572
Epoch: [19]  [2370/2809]  eta: 0:04:08  lr: 0.000029  min_lr: 0.000000  loss: 3.8474 (3.8792)  class_acc: 0.2917 (0.2958)  loss_scale: 65536.0000 (59676.1805)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1103  max mem: 15572
Epoch: [19]  [2380/2809]  eta: 0:04:02  lr: 0.000029  min_lr: 0.000000  loss: 3.9622 (3.8798)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (59700.7913)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1152  max mem: 15572
Epoch: [19]  [2390/2809]  eta: 0:03:57  lr: 0.000029  min_lr: 0.000000  loss: 3.9373 (3.8791)  class_acc: 0.3333 (0.2960)  loss_scale: 65536.0000 (59725.1962)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1608  max mem: 15572
[2025-01-15 23:51:50,617] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:51:50,617] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:51:51,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55771
[2025-01-15 23:51:51,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:51:51,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [2400/2809]  eta: 0:03:51  lr: 0.000029  min_lr: 0.000000  loss: 3.5848 (3.8777)  class_acc: 0.3750 (0.2963)  loss_scale: 65536.0000 (59776.6930)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1700  max mem: 15572
Epoch: [19]  [2410/2809]  eta: 0:03:45  lr: 0.000029  min_lr: 0.000000  loss: 3.9116 (3.8782)  class_acc: 0.2917 (0.2960)  loss_scale: 65536.0000 (59800.5807)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0999  max mem: 15572
Epoch: [19]  [2420/2809]  eta: 0:03:39  lr: 0.000029  min_lr: 0.000000  loss: 3.9713 (3.8786)  class_acc: 0.2083 (0.2958)  loss_scale: 65536.0000 (59824.2710)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1239  max mem: 15572
Epoch: [19]  [2430/2809]  eta: 0:03:34  lr: 0.000029  min_lr: 0.000000  loss: 3.8280 (3.8776)  class_acc: 0.3333 (0.2960)  loss_scale: 65536.0000 (59847.7664)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1219  max mem: 15572
Epoch: [19]  [2440/2809]  eta: 0:03:28  lr: 0.000029  min_lr: 0.000000  loss: 3.7926 (3.8776)  class_acc: 0.3333 (0.2960)  loss_scale: 65536.0000 (59871.0692)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1399  max mem: 15572
Epoch: [19]  [2450/2809]  eta: 0:03:23  lr: 0.000029  min_lr: 0.000000  loss: 3.9081 (3.8773)  class_acc: 0.2917 (0.2960)  loss_scale: 65536.0000 (59894.1820)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1329  max mem: 15572
Epoch: [19]  [2460/2809]  eta: 0:03:17  lr: 0.000029  min_lr: 0.000000  loss: 3.6129 (3.8765)  class_acc: 0.2917 (0.2961)  loss_scale: 65536.0000 (59917.1069)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1315  max mem: 15572
Epoch: [19]  [2470/2809]  eta: 0:03:11  lr: 0.000029  min_lr: 0.000000  loss: 3.8226 (3.8762)  class_acc: 0.2917 (0.2962)  loss_scale: 65536.0000 (59939.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1204  max mem: 15572
Epoch: [19]  [2480/2809]  eta: 0:03:05  lr: 0.000029  min_lr: 0.000000  loss: 3.8226 (3.8758)  class_acc: 0.3333 (0.2962)  loss_scale: 65536.0000 (59962.4023)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0702  max mem: 15572
Epoch: [19]  [2490/2809]  eta: 0:03:00  lr: 0.000029  min_lr: 0.000000  loss: 3.9529 (3.8773)  class_acc: 0.2917 (0.2960)  loss_scale: 65536.0000 (59984.7772)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.0975  max mem: 15572
Epoch: [19]  [2500/2809]  eta: 0:02:54  lr: 0.000029  min_lr: 0.000000  loss: 3.9529 (3.8767)  class_acc: 0.2917 (0.2962)  loss_scale: 65536.0000 (60006.9732)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1195  max mem: 15572
[2025-01-15 23:52:51,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 55880
[2025-01-15 23:52:51,367] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:52:51,367] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2510/2809]  eta: 0:02:48  lr: 0.000029  min_lr: 0.000000  loss: 3.8385 (3.8770)  class_acc: 0.2917 (0.2961)  loss_scale: 65536.0000 (60002.8929)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.1043  max mem: 15572
Epoch: [19]  [2520/2809]  eta: 0:02:43  lr: 0.000029  min_lr: 0.000000  loss: 3.8572 (3.8767)  class_acc: 0.2917 (0.2961)  loss_scale: 32768.0000 (59894.8608)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1794  max mem: 15572
Epoch: [19]  [2530/2809]  eta: 0:02:37  lr: 0.000029  min_lr: 0.000000  loss: 3.7311 (3.8764)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (59787.6823)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1777  max mem: 15572
Epoch: [19]  [2540/2809]  eta: 0:02:32  lr: 0.000029  min_lr: 0.000000  loss: 3.9638 (3.8764)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (59681.3475)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1304  max mem: 15572
Epoch: [19]  [2550/2809]  eta: 0:02:26  lr: 0.000029  min_lr: 0.000000  loss: 3.9772 (3.8771)  class_acc: 0.2917 (0.2960)  loss_scale: 32768.0000 (59575.8463)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1155  max mem: 15572
Epoch: [19]  [2560/2809]  eta: 0:02:20  lr: 0.000029  min_lr: 0.000000  loss: 3.9865 (3.8775)  class_acc: 0.2083 (0.2959)  loss_scale: 32768.0000 (59471.1691)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1280  max mem: 15572
Epoch: [19]  [2570/2809]  eta: 0:02:15  lr: 0.000029  min_lr: 0.000000  loss: 3.7551 (3.8764)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (59367.3061)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1287  max mem: 15572
Epoch: [19]  [2580/2809]  eta: 0:02:09  lr: 0.000029  min_lr: 0.000000  loss: 3.7211 (3.8767)  class_acc: 0.2917 (0.2960)  loss_scale: 32768.0000 (59264.2480)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0959  max mem: 15572
Epoch: [19]  [2590/2809]  eta: 0:02:03  lr: 0.000029  min_lr: 0.000000  loss: 4.0332 (3.8773)  class_acc: 0.2917 (0.2960)  loss_scale: 32768.0000 (59161.9853)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1145  max mem: 15572
Epoch: [19]  [2600/2809]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000000  loss: 3.9457 (3.8768)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (59060.5090)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1497  max mem: 15572
Epoch: [19]  [2610/2809]  eta: 0:01:52  lr: 0.000029  min_lr: 0.000000  loss: 4.0915 (3.8780)  class_acc: 0.2500 (0.2960)  loss_scale: 32768.0000 (58959.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0919  max mem: 15572
Epoch: [19]  [2620/2809]  eta: 0:01:46  lr: 0.000029  min_lr: 0.000000  loss: 4.0174 (3.8775)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (58859.8794)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0536  max mem: 15572
[2025-01-15 23:53:59,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=378, lr=[2.790693606018548e-07, 2.790693606018548e-07, 3.9867051514550694e-07, 3.9867051514550694e-07, 5.695293073507243e-07, 5.695293073507243e-07, 8.136132962153204e-07, 8.136132962153204e-07, 1.1623047088790292e-06, 1.1623047088790292e-06, 1.6604352983986132e-06, 1.6604352983986132e-06, 2.3720504262837332e-06, 2.3720504262837332e-06, 3.3886434661196192e-06, 3.3886434661196192e-06, 4.840919237313742e-06, 4.840919237313742e-06, 6.915598910448203e-06, 6.915598910448203e-06, 9.879427014926004e-06, 9.879427014926004e-06, 1.4113467164180008e-05, 1.4113467164180008e-05, 2.0162095948828585e-05, 2.0162095948828585e-05, 2.8802994212612265e-05, 2.8802994212612265e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-15 23:53:59,550] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=28.500047100586354, CurrSamplesPerSec=30.713213999348287, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [19]  [2630/2809]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000000  loss: 3.8736 (3.8776)  class_acc: 0.2917 (0.2959)  loss_scale: 32768.0000 (58760.7085)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0801  max mem: 15572
[2025-01-15 23:54:04,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:54:04,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [2640/2809]  eta: 0:01:35  lr: 0.000029  min_lr: 0.000000  loss: 3.9134 (3.8773)  class_acc: 0.2917 (0.2961)  loss_scale: 32768.0000 (58699.5108)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0576  max mem: 15572
Epoch: [19]  [2650/2809]  eta: 0:01:29  lr: 0.000029  min_lr: 0.000000  loss: 3.9659 (3.8781)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (58725.2991)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0640  max mem: 15572
Epoch: [19]  [2660/2809]  eta: 0:01:24  lr: 0.000029  min_lr: 0.000000  loss: 3.8444 (3.8780)  class_acc: 0.2500 (0.2960)  loss_scale: 65536.0000 (58750.8936)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0894  max mem: 15572
Epoch: [19]  [2670/2809]  eta: 0:01:18  lr: 0.000029  min_lr: 0.000000  loss: 3.7695 (3.8776)  class_acc: 0.2917 (0.2961)  loss_scale: 65536.0000 (58776.2965)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1108  max mem: 15572
Epoch: [19]  [2680/2809]  eta: 0:01:12  lr: 0.000029  min_lr: 0.000000  loss: 3.9719 (3.8778)  class_acc: 0.2917 (0.2959)  loss_scale: 65536.0000 (58801.5099)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0894  max mem: 15572
Epoch: [19]  [2690/2809]  eta: 0:01:07  lr: 0.000029  min_lr: 0.000000  loss: 4.0031 (3.8779)  class_acc: 0.2917 (0.2961)  loss_scale: 65536.0000 (58826.5359)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0802  max mem: 15572
Epoch: [19]  [2700/2809]  eta: 0:01:01  lr: 0.000029  min_lr: 0.000000  loss: 3.8789 (3.8777)  class_acc: 0.3333 (0.2962)  loss_scale: 65536.0000 (58851.3765)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1317  max mem: 15572
Epoch: [19]  [2710/2809]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000000  loss: 3.8305 (3.8773)  class_acc: 0.2917 (0.2962)  loss_scale: 65536.0000 (58876.0339)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1699  max mem: 15572
[2025-01-15 23:54:47,998] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56087
[2025-01-15 23:54:47,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:54:47,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [2720/2809]  eta: 0:00:50  lr: 0.000029  min_lr: 0.000000  loss: 3.8275 (3.8773)  class_acc: 0.2083 (0.2961)  loss_scale: 65536.0000 (58840.2969)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1724  max mem: 15572
Epoch: [19]  [2730/2809]  eta: 0:00:44  lr: 0.000029  min_lr: 0.000000  loss: 3.8275 (3.8768)  class_acc: 0.2500 (0.2962)  loss_scale: 32768.0000 (58744.8290)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1608  max mem: 15572
Epoch: [19]  [2740/2809]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000000  loss: 3.9499 (3.8771)  class_acc: 0.2500 (0.2963)  loss_scale: 32768.0000 (58650.0576)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0870  max mem: 15572
Epoch: [19]  [2750/2809]  eta: 0:00:33  lr: 0.000029  min_lr: 0.000000  loss: 3.9407 (3.8766)  class_acc: 0.3333 (0.2963)  loss_scale: 32768.0000 (58555.9753)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1071  max mem: 15572
Epoch: [19]  [2760/2809]  eta: 0:00:27  lr: 0.000029  min_lr: 0.000000  loss: 3.6752 (3.8762)  class_acc: 0.3333 (0.2964)  loss_scale: 32768.0000 (58462.5744)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1390  max mem: 15572
Epoch: [19]  [2770/2809]  eta: 0:00:22  lr: 0.000029  min_lr: 0.000000  loss: 3.7271 (3.8752)  class_acc: 0.2917 (0.2967)  loss_scale: 32768.0000 (58369.8477)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0868  max mem: 15572
Epoch: [19]  [2780/2809]  eta: 0:00:16  lr: 0.000029  min_lr: 0.000000  loss: 3.7088 (3.8749)  class_acc: 0.2917 (0.2968)  loss_scale: 32768.0000 (58277.7878)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0990  max mem: 15572
Epoch: [19]  [2790/2809]  eta: 0:00:10  lr: 0.000029  min_lr: 0.000000  loss: 3.7098 (3.8745)  class_acc: 0.2917 (0.2969)  loss_scale: 32768.0000 (58186.3877)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1418  max mem: 15572
Epoch: [19]  [2800/2809]  eta: 0:00:05  lr: 0.000029  min_lr: 0.000000  loss: 3.9319 (3.8747)  class_acc: 0.2917 (0.2969)  loss_scale: 32768.0000 (58095.6401)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0825  max mem: 15572
Epoch: [19]  [2808/2809]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000000  loss: 3.7519 (3.8741)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (58023.5073)  weight_decay: 0.0500 (0.0500)  time: 0.3925  data: 0.0010  max mem: 15572
Epoch: [19] Total time: 0:26:27 (0.5650 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000000  loss: 3.7519 (3.8741)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (58023.5073)  weight_decay: 0.0500 (0.0500)
[2025-01-15 23:55:39,375] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-01-15 23:55:39,378] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-19/mp_rank_00_model_states.pt
[2025-01-15 23:55:39,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-19/mp_rank_00_model_states.pt...
[2025-01-15 23:55:39,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-19/mp_rank_00_model_states.pt.
[2025-01-15 23:55:39,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/272]  eta: 0:16:43  loss: 0.5930 (0.5930)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.6899  data: 3.5039  max mem: 15572
Val:  [ 10/272]  eta: 0:02:51  loss: 2.5842 (2.4424)  acc1: 44.4444 (38.8889)  acc5: 77.7778 (68.6869)  time: 0.6542  data: 0.4784  max mem: 15572
Val:  [ 20/272]  eta: 0:02:05  loss: 2.5757 (2.4903)  acc1: 44.4444 (42.0635)  acc5: 72.2222 (70.1058)  time: 0.3384  data: 0.1600  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 2.4834 (2.5922)  acc1: 38.8889 (37.9928)  acc5: 72.2222 (68.8172)  time: 0.3153  data: 0.1363  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 2.6920 (2.6148)  acc1: 27.7778 (36.3144)  acc5: 77.7778 (69.7832)  time: 0.3096  data: 0.1214  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 2.6689 (2.5344)  acc1: 27.7778 (38.0174)  acc5: 77.7778 (72.1133)  time: 0.3197  data: 0.1174  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.5652 (2.4128)  acc1: 61.1111 (41.7122)  acc5: 88.8889 (73.2240)  time: 0.2959  data: 0.0993  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.5678 (2.3264)  acc1: 66.6667 (44.6009)  acc5: 88.8889 (74.7261)  time: 0.2891  data: 0.0998  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 1.9965 (2.3346)  acc1: 55.5556 (44.6502)  acc5: 77.7778 (74.8285)  time: 0.3004  data: 0.1178  max mem: 15572
Val:  [ 90/272]  eta: 0:01:01  loss: 2.4203 (2.3606)  acc1: 44.4444 (44.0171)  acc5: 77.7778 (74.9695)  time: 0.2582  data: 0.0909  max mem: 15572
Val:  [100/272]  eta: 0:00:55  loss: 2.4203 (2.3916)  acc1: 38.8889 (43.3993)  acc5: 77.7778 (74.3124)  time: 0.2104  data: 0.0538  max mem: 15572
Val:  [110/272]  eta: 0:00:50  loss: 2.6752 (2.4699)  acc1: 16.6667 (41.1411)  acc5: 66.6667 (73.0731)  time: 0.1930  data: 0.0303  max mem: 15572
Val:  [120/272]  eta: 0:00:45  loss: 3.0384 (2.4984)  acc1: 22.2222 (40.6336)  acc5: 66.6667 (72.5436)  time: 0.1839  data: 0.0107  max mem: 15572
Val:  [130/272]  eta: 0:00:43  loss: 2.1989 (2.4684)  acc1: 44.4444 (41.8999)  acc5: 72.2222 (72.9856)  time: 0.2624  data: 0.0832  max mem: 15572
Val:  [140/272]  eta: 0:00:40  loss: 1.8722 (2.4617)  acc1: 50.0000 (42.2774)  acc5: 77.7778 (72.8920)  time: 0.3370  data: 0.1565  max mem: 15572
Val:  [150/272]  eta: 0:00:37  loss: 2.4028 (2.4651)  acc1: 38.8889 (41.6483)  acc5: 72.2222 (73.0684)  time: 0.3269  data: 0.1450  max mem: 15572
Val:  [160/272]  eta: 0:00:34  loss: 2.3960 (2.4470)  acc1: 44.4444 (42.6501)  acc5: 77.7778 (73.6715)  time: 0.3350  data: 0.1360  max mem: 15572
Val:  [170/272]  eta: 0:00:31  loss: 2.5310 (2.4716)  acc1: 38.8889 (41.8129)  acc5: 72.2222 (73.1644)  time: 0.3491  data: 0.1378  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.5310 (2.4649)  acc1: 27.7778 (41.6820)  acc5: 66.6667 (73.5421)  time: 0.3400  data: 0.1383  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.5441 (2.5146)  acc1: 27.7778 (40.5468)  acc5: 66.6667 (72.1059)  time: 0.3590  data: 0.1567  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.7615 (2.5235)  acc1: 27.7778 (40.4091)  acc5: 66.6667 (71.9458)  time: 0.3703  data: 0.1668  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.2138 (2.5191)  acc1: 44.4444 (40.9426)  acc5: 77.7778 (72.0906)  time: 0.3160  data: 0.1122  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.0573 (2.5060)  acc1: 55.5556 (41.3022)  acc5: 77.7778 (72.2976)  time: 0.3413  data: 0.1281  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9451 (2.4779)  acc1: 55.5556 (42.2799)  acc5: 83.3333 (72.6792)  time: 0.3954  data: 0.1862  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7384 (2.4601)  acc1: 61.1111 (42.6464)  acc5: 83.3333 (73.0752)  time: 0.3825  data: 0.1820  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4885 (2.4742)  acc1: 38.8889 (42.0540)  acc5: 77.7778 (72.9305)  time: 0.3754  data: 0.1741  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.4552 (2.4131)  acc1: 72.2222 (43.8059)  acc5: 83.3333 (73.6909)  time: 0.3354  data: 0.1336  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4968 (2.4128)  acc1: 66.6667 (43.6039)  acc5: 88.8889 (73.7802)  time: 0.2475  data: 0.0695  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4968 (2.4191)  acc1: 61.1111 (43.5798)  acc5: 88.8889 (73.7456)  time: 0.2409  data: 0.0694  max mem: 15572
Val: Total time: 0:01:27 (0.3232 s / it)
* Acc@1 43.580 Acc@5 73.746 loss 2.419
Accuracy of the network on the 4883 val videos: 43.6%
[2025-01-15 23:57:07,787] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-15 23:57:07,790] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-15 23:57:07,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-15 23:57:11,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-15 23:57:11,143] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.58%
Epoch: [20]  [   0/2809]  eta: 7:24:52  lr: 0.000029  min_lr: 0.000000  loss: 3.8506 (3.8506)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.5026  data: 9.0419  max mem: 15572
Epoch: [20]  [  10/2809]  eta: 1:13:47  lr: 0.000029  min_lr: 0.000000  loss: 3.9951 (4.0283)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5819  data: 1.1436  max mem: 15572
Epoch: [20]  [  20/2809]  eta: 0:54:57  lr: 0.000029  min_lr: 0.000000  loss: 3.9727 (3.9511)  class_acc: 0.2500 (0.2718)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7663  data: 0.3304  max mem: 15572
Epoch: [20]  [  30/2809]  eta: 0:44:34  lr: 0.000029  min_lr: 0.000000  loss: 3.8444 (3.8699)  class_acc: 0.2500 (0.2890)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1707  max mem: 15572
[2025-01-15 23:57:43,279] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:57:43,279] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [  40/2809]  eta: 0:37:51  lr: 0.000029  min_lr: 0.000000  loss: 4.0131 (3.9016)  class_acc: 0.2917 (0.2876)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  time: 0.4406  data: 0.0174  max mem: 15572
Epoch: [20]  [  50/2809]  eta: 0:34:08  lr: 0.000029  min_lr: 0.000000  loss: 3.8137 (3.8645)  class_acc: 0.2917 (0.2949)  loss_scale: 65536.0000 (42405.6471)  weight_decay: 0.0500 (0.0500)  time: 0.4011  data: 0.0005  max mem: 15572
Epoch: [20]  [  60/2809]  eta: 0:31:43  lr: 0.000029  min_lr: 0.000000  loss: 3.8929 (3.8906)  class_acc: 0.2500 (0.2896)  loss_scale: 65536.0000 (46197.5082)  weight_decay: 0.0500 (0.0500)  time: 0.4297  data: 0.0005  max mem: 15572
Epoch: [20]  [  70/2809]  eta: 0:30:02  lr: 0.000029  min_lr: 0.000000  loss: 3.9885 (3.9087)  class_acc: 0.2500 (0.2881)  loss_scale: 65536.0000 (48921.2394)  weight_decay: 0.0500 (0.0500)  time: 0.4434  data: 0.0005  max mem: 15572
Epoch: [20]  [  80/2809]  eta: 0:28:47  lr: 0.000029  min_lr: 0.000000  loss: 3.8443 (3.8729)  class_acc: 0.2917 (0.2968)  loss_scale: 65536.0000 (50972.4444)  weight_decay: 0.0500 (0.0500)  time: 0.4516  data: 0.0006  max mem: 15572
Epoch: [20]  [  90/2809]  eta: 0:28:17  lr: 0.000029  min_lr: 0.000000  loss: 3.8199 (3.8753)  class_acc: 0.3333 (0.2962)  loss_scale: 65536.0000 (52572.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5051  data: 0.0543  max mem: 15572
Epoch: [20]  [ 100/2809]  eta: 0:27:44  lr: 0.000029  min_lr: 0.000000  loss: 3.9439 (3.8749)  class_acc: 0.3333 (0.2970)  loss_scale: 65536.0000 (53856.3168)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0819  max mem: 15572
Epoch: [20]  [ 110/2809]  eta: 0:27:44  lr: 0.000029  min_lr: 0.000000  loss: 3.8301 (3.8800)  class_acc: 0.2917 (0.2924)  loss_scale: 65536.0000 (54908.5405)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1252  max mem: 15572
Epoch: [20]  [ 120/2809]  eta: 0:27:20  lr: 0.000029  min_lr: 0.000000  loss: 3.7965 (3.8774)  class_acc: 0.2917 (0.2951)  loss_scale: 65536.0000 (55786.8430)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1415  max mem: 15572
Epoch: [20]  [ 130/2809]  eta: 0:27:05  lr: 0.000029  min_lr: 0.000000  loss: 3.8429 (3.8811)  class_acc: 0.2917 (0.2939)  loss_scale: 65536.0000 (56531.0534)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.0946  max mem: 15572
Epoch: [20]  [ 140/2809]  eta: 0:26:50  lr: 0.000029  min_lr: 0.000000  loss: 4.0618 (3.9039)  class_acc: 0.2083 (0.2893)  loss_scale: 65536.0000 (57169.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1091  max mem: 15572
Epoch: [20]  [ 150/2809]  eta: 0:26:38  lr: 0.000029  min_lr: 0.000000  loss: 4.1278 (3.9142)  class_acc: 0.2083 (0.2870)  loss_scale: 65536.0000 (57723.7616)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1234  max mem: 15572
Epoch: [20]  [ 160/2809]  eta: 0:26:24  lr: 0.000029  min_lr: 0.000000  loss: 4.0742 (3.9134)  class_acc: 0.2500 (0.2842)  loss_scale: 65536.0000 (58208.9938)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1092  max mem: 15572
[2025-01-15 23:58:49,244] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-15 23:58:49,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-15 23:58:52,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56347
[2025-01-15 23:58:52,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-15 23:58:52,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 170/2809]  eta: 0:26:20  lr: 0.000029  min_lr: 0.000000  loss: 3.9645 (3.9094)  class_acc: 0.2500 (0.2841)  loss_scale: 65536.0000 (59787.2281)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1354  max mem: 15572
Epoch: [20]  [ 180/2809]  eta: 0:26:22  lr: 0.000029  min_lr: 0.000000  loss: 4.0656 (3.9194)  class_acc: 0.2500 (0.2848)  loss_scale: 65536.0000 (60104.8398)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1995  max mem: 15572
Epoch: [20]  [ 190/2809]  eta: 0:26:07  lr: 0.000029  min_lr: 0.000000  loss: 4.0832 (3.9213)  class_acc: 0.2917 (0.2858)  loss_scale: 65536.0000 (60389.1937)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1547  max mem: 15572
Epoch: [20]  [ 200/2809]  eta: 0:26:07  lr: 0.000029  min_lr: 0.000000  loss: 3.9516 (3.9235)  class_acc: 0.2917 (0.2844)  loss_scale: 65536.0000 (60645.2537)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1455  max mem: 15572
Epoch: [20]  [ 210/2809]  eta: 0:25:47  lr: 0.000029  min_lr: 0.000000  loss: 4.0443 (3.9204)  class_acc: 0.2917 (0.2861)  loss_scale: 65536.0000 (60877.0427)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1165  max mem: 15572
Epoch: [20]  [ 220/2809]  eta: 0:25:32  lr: 0.000029  min_lr: 0.000000  loss: 3.9603 (3.9187)  class_acc: 0.2917 (0.2856)  loss_scale: 65536.0000 (61087.8552)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0710  max mem: 15572
Epoch: [20]  [ 230/2809]  eta: 0:25:19  lr: 0.000029  min_lr: 0.000000  loss: 3.9603 (3.9234)  class_acc: 0.2500 (0.2839)  loss_scale: 65536.0000 (61280.4156)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0912  max mem: 15572
Epoch: [20]  [ 240/2809]  eta: 0:25:12  lr: 0.000028  min_lr: 0.000000  loss: 3.9723 (3.9193)  class_acc: 0.2500 (0.2841)  loss_scale: 65536.0000 (61456.9959)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1016  max mem: 15572
Epoch: [20]  [ 250/2809]  eta: 0:24:59  lr: 0.000028  min_lr: 0.000000  loss: 3.8404 (3.9111)  class_acc: 0.2917 (0.2870)  loss_scale: 65536.0000 (61619.5060)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0948  max mem: 15572
Epoch: [20]  [ 260/2809]  eta: 0:24:48  lr: 0.000028  min_lr: 0.000000  loss: 3.6381 (3.8996)  class_acc: 0.3333 (0.2898)  loss_scale: 65536.0000 (61769.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0819  max mem: 15572
Epoch: [20]  [ 270/2809]  eta: 0:24:46  lr: 0.000028  min_lr: 0.000000  loss: 3.8598 (3.9014)  class_acc: 0.3333 (0.2906)  loss_scale: 65536.0000 (61908.5461)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1354  max mem: 15572
[2025-01-15 23:59:52,461] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56456
[2025-01-15 23:59:52,462] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-15 23:59:52,462] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 280/2809]  eta: 0:24:39  lr: 0.000028  min_lr: 0.000000  loss: 3.7073 (3.8893)  class_acc: 0.3333 (0.2943)  loss_scale: 65536.0000 (61454.5765)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1533  max mem: 15572
Epoch: [20]  [ 290/2809]  eta: 0:24:27  lr: 0.000028  min_lr: 0.000000  loss: 3.7682 (3.8894)  class_acc: 0.3333 (0.2952)  loss_scale: 32768.0000 (60468.7835)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1077  max mem: 15572
Epoch: [20]  [ 300/2809]  eta: 0:24:16  lr: 0.000028  min_lr: 0.000000  loss: 3.7941 (3.8809)  class_acc: 0.3333 (0.2967)  loss_scale: 32768.0000 (59548.4917)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0829  max mem: 15572
Epoch: [20]  [ 310/2809]  eta: 0:24:12  lr: 0.000028  min_lr: 0.000000  loss: 3.8978 (3.8850)  class_acc: 0.3333 (0.2974)  loss_scale: 32768.0000 (58687.3826)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1352  max mem: 15572
Epoch: [20]  [ 320/2809]  eta: 0:24:04  lr: 0.000028  min_lr: 0.000000  loss: 3.9001 (3.8812)  class_acc: 0.3333 (0.2983)  loss_scale: 32768.0000 (57879.9252)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1566  max mem: 15572
Epoch: [20]  [ 330/2809]  eta: 0:23:52  lr: 0.000028  min_lr: 0.000000  loss: 3.6978 (3.8784)  class_acc: 0.3333 (0.2995)  loss_scale: 32768.0000 (57121.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.1055  max mem: 15572
Epoch: [20]  [ 340/2809]  eta: 0:23:47  lr: 0.000028  min_lr: 0.000000  loss: 3.8393 (3.8786)  class_acc: 0.3333 (0.3001)  loss_scale: 32768.0000 (56407.0850)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1286  max mem: 15572
Epoch: [20]  [ 350/2809]  eta: 0:23:43  lr: 0.000028  min_lr: 0.000000  loss: 3.9195 (3.8754)  class_acc: 0.2917 (0.2996)  loss_scale: 32768.0000 (55733.6068)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1797  max mem: 15572
Epoch: [20]  [ 360/2809]  eta: 0:23:37  lr: 0.000028  min_lr: 0.000000  loss: 3.8329 (3.8726)  class_acc: 0.2500 (0.2988)  loss_scale: 32768.0000 (55097.4404)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1600  max mem: 15572
Epoch: [20]  [ 370/2809]  eta: 0:23:35  lr: 0.000028  min_lr: 0.000000  loss: 3.6896 (3.8676)  class_acc: 0.3333 (0.3017)  loss_scale: 32768.0000 (54495.5687)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1810  max mem: 15572
Epoch: [20]  [ 380/2809]  eta: 0:23:27  lr: 0.000028  min_lr: 0.000000  loss: 3.8484 (3.8684)  class_acc: 0.3750 (0.3015)  loss_scale: 32768.0000 (53925.2913)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1699  max mem: 15572
Epoch: [20]  [ 390/2809]  eta: 0:23:28  lr: 0.000028  min_lr: 0.000000  loss: 3.8475 (3.8622)  class_acc: 0.2917 (0.3023)  loss_scale: 32768.0000 (53384.1841)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1760  max mem: 15572
Epoch: [20]  [ 400/2809]  eta: 0:23:14  lr: 0.000028  min_lr: 0.000000  loss: 3.7791 (3.8630)  class_acc: 0.2917 (0.3026)  loss_scale: 32768.0000 (52870.0648)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1181  max mem: 15572
[2025-01-16 00:01:06,574] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:01:06,574] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 410/2809]  eta: 0:23:05  lr: 0.000028  min_lr: 0.000000  loss: 4.0598 (3.8666)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (52859.3285)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0539  max mem: 15572
Epoch: [20]  [ 420/2809]  eta: 0:23:04  lr: 0.000028  min_lr: 0.000000  loss: 3.9706 (3.8696)  class_acc: 0.2500 (0.3014)  loss_scale: 65536.0000 (53160.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1469  max mem: 15572
Epoch: [20]  [ 430/2809]  eta: 0:22:55  lr: 0.000028  min_lr: 0.000000  loss: 3.8478 (3.8671)  class_acc: 0.2500 (0.3013)  loss_scale: 65536.0000 (53447.5731)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1366  max mem: 15572
Epoch: [20]  [ 440/2809]  eta: 0:22:54  lr: 0.000028  min_lr: 0.000000  loss: 3.9794 (3.8727)  class_acc: 0.2500 (0.2997)  loss_scale: 65536.0000 (53721.6871)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1695  max mem: 15572
Epoch: [20]  [ 450/2809]  eta: 0:22:41  lr: 0.000028  min_lr: 0.000000  loss: 3.9800 (3.8719)  class_acc: 0.2500 (0.3000)  loss_scale: 65536.0000 (53983.6452)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1449  max mem: 15572
Epoch: [20]  [ 460/2809]  eta: 0:22:38  lr: 0.000028  min_lr: 0.000000  loss: 3.9194 (3.8738)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (54234.2386)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.1051  max mem: 15572
Epoch: [20]  [ 470/2809]  eta: 0:22:30  lr: 0.000028  min_lr: 0.000000  loss: 3.9275 (3.8728)  class_acc: 0.2917 (0.3009)  loss_scale: 65536.0000 (54474.1911)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1487  max mem: 15572
Epoch: [20]  [ 480/2809]  eta: 0:22:25  lr: 0.000028  min_lr: 0.000000  loss: 3.7894 (3.8723)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (54704.1663)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1217  max mem: 15572
Epoch: [20]  [ 490/2809]  eta: 0:22:15  lr: 0.000028  min_lr: 0.000000  loss: 4.0035 (3.8739)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (54924.7739)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0905  max mem: 15572
Epoch: [20]  [ 500/2809]  eta: 0:22:17  lr: 0.000028  min_lr: 0.000000  loss: 4.0115 (3.8742)  class_acc: 0.2083 (0.2995)  loss_scale: 65536.0000 (55136.5749)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1806  max mem: 15572
Epoch: [20]  [ 510/2809]  eta: 0:22:11  lr: 0.000028  min_lr: 0.000000  loss: 4.0115 (3.8726)  class_acc: 0.2500 (0.3000)  loss_scale: 65536.0000 (55340.0861)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.2208  max mem: 15572
Epoch: [20]  [ 520/2809]  eta: 0:22:01  lr: 0.000028  min_lr: 0.000000  loss: 3.9978 (3.8731)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (55535.7850)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0756  max mem: 15572
Epoch: [20]  [ 530/2809]  eta: 0:21:58  lr: 0.000028  min_lr: 0.000000  loss: 3.9894 (3.8696)  class_acc: 0.3333 (0.3016)  loss_scale: 65536.0000 (55724.1130)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1205  max mem: 15572
[2025-01-16 00:02:20,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:02:20,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:02:20,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56714
[2025-01-16 00:02:20,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:02:20,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 540/2809]  eta: 0:21:52  lr: 0.000028  min_lr: 0.000000  loss: 3.9969 (3.8721)  class_acc: 0.3333 (0.3011)  loss_scale: 65536.0000 (56026.6174)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1643  max mem: 15572
Epoch: [20]  [ 550/2809]  eta: 0:21:42  lr: 0.000028  min_lr: 0.000000  loss: 3.9969 (3.8692)  class_acc: 0.2917 (0.3015)  loss_scale: 65536.0000 (56199.2015)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0701  max mem: 15572
Epoch: [20]  [ 560/2809]  eta: 0:21:34  lr: 0.000028  min_lr: 0.000000  loss: 3.7864 (3.8697)  class_acc: 0.3333 (0.3016)  loss_scale: 65536.0000 (56365.6328)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0501  max mem: 15572
Epoch: [20]  [ 570/2809]  eta: 0:21:29  lr: 0.000028  min_lr: 0.000000  loss: 3.8386 (3.8693)  class_acc: 0.2500 (0.3006)  loss_scale: 65536.0000 (56526.2347)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1034  max mem: 15572
Epoch: [20]  [ 580/2809]  eta: 0:21:18  lr: 0.000028  min_lr: 0.000000  loss: 3.8386 (3.8701)  class_acc: 0.2500 (0.2998)  loss_scale: 65536.0000 (56681.3081)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0807  max mem: 15572
Epoch: [20]  [ 590/2809]  eta: 0:21:16  lr: 0.000028  min_lr: 0.000000  loss: 3.7758 (3.8696)  class_acc: 0.2500 (0.3002)  loss_scale: 65536.0000 (56831.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1126  max mem: 15572
Epoch: [20]  [ 600/2809]  eta: 0:21:09  lr: 0.000028  min_lr: 0.000000  loss: 3.8774 (3.8701)  class_acc: 0.2500 (0.2994)  loss_scale: 65536.0000 (56975.9734)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1587  max mem: 15572
Epoch: [20]  [ 610/2809]  eta: 0:21:08  lr: 0.000028  min_lr: 0.000000  loss: 3.9724 (3.8702)  class_acc: 0.2500 (0.2991)  loss_scale: 65536.0000 (57116.0720)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1649  max mem: 15572
Epoch: [20]  [ 620/2809]  eta: 0:20:56  lr: 0.000028  min_lr: 0.000000  loss: 3.9369 (3.8690)  class_acc: 0.2917 (0.2995)  loss_scale: 65536.0000 (57251.6586)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1135  max mem: 15572
Epoch: [20]  [ 630/2809]  eta: 0:20:56  lr: 0.000028  min_lr: 0.000000  loss: 3.9639 (3.8710)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (57382.9477)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1393  max mem: 15572
Epoch: [20]  [ 640/2809]  eta: 0:20:49  lr: 0.000028  min_lr: 0.000000  loss: 3.8162 (3.8685)  class_acc: 0.3333 (0.3003)  loss_scale: 65536.0000 (57510.1404)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1715  max mem: 15572
Epoch: [20]  [ 650/2809]  eta: 0:20:42  lr: 0.000028  min_lr: 0.000000  loss: 3.8162 (3.8697)  class_acc: 0.3333 (0.3002)  loss_scale: 65536.0000 (57633.4255)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0989  max mem: 15572
Epoch: [20]  [ 660/2809]  eta: 0:20:35  lr: 0.000028  min_lr: 0.000000  loss: 4.0469 (3.8743)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (57752.9803)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1057  max mem: 15572
[2025-01-16 00:03:32,928] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:03:32,928] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:03:33,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56844
[2025-01-16 00:03:33,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:03:33,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 670/2809]  eta: 0:20:30  lr: 0.000028  min_lr: 0.000000  loss: 4.0855 (3.8778)  class_acc: 0.2500 (0.2985)  loss_scale: 65536.0000 (57966.6408)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1060  max mem: 15572
Epoch: [20]  [ 680/2809]  eta: 0:20:24  lr: 0.000028  min_lr: 0.000000  loss: 4.0115 (3.8767)  class_acc: 0.2917 (0.2990)  loss_scale: 65536.0000 (58077.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1338  max mem: 15572
Epoch: [20]  [ 690/2809]  eta: 0:20:17  lr: 0.000028  min_lr: 0.000000  loss: 3.8239 (3.8761)  class_acc: 0.2917 (0.2993)  loss_scale: 65536.0000 (58185.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1258  max mem: 15572
Epoch: [20]  [ 700/2809]  eta: 0:20:10  lr: 0.000028  min_lr: 0.000000  loss: 3.8653 (3.8769)  class_acc: 0.2500 (0.2986)  loss_scale: 65536.0000 (58290.5792)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.1007  max mem: 15572
Epoch: [20]  [ 710/2809]  eta: 0:20:05  lr: 0.000028  min_lr: 0.000000  loss: 3.9403 (3.8765)  class_acc: 0.2500 (0.2989)  loss_scale: 65536.0000 (58392.4838)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1124  max mem: 15572
Epoch: [20]  [ 720/2809]  eta: 0:19:58  lr: 0.000028  min_lr: 0.000000  loss: 3.9274 (3.8755)  class_acc: 0.2500 (0.2987)  loss_scale: 65536.0000 (58491.5617)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1201  max mem: 15572
Epoch: [20]  [ 730/2809]  eta: 0:19:52  lr: 0.000028  min_lr: 0.000000  loss: 4.0725 (3.8762)  class_acc: 0.2500 (0.2986)  loss_scale: 65536.0000 (58587.9289)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1261  max mem: 15572
[2025-01-16 00:04:14,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 56919
[2025-01-16 00:04:14,556] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:04:14,556] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 740/2809]  eta: 0:19:42  lr: 0.000028  min_lr: 0.000000  loss: 3.8727 (3.8764)  class_acc: 0.2500 (0.2977)  loss_scale: 65536.0000 (58593.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0769  max mem: 15572
Epoch: [20]  [ 750/2809]  eta: 0:19:37  lr: 0.000028  min_lr: 0.000000  loss: 3.8937 (3.8775)  class_acc: 0.2083 (0.2970)  loss_scale: 32768.0000 (58249.3742)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0700  max mem: 15572
Epoch: [20]  [ 760/2809]  eta: 0:19:31  lr: 0.000028  min_lr: 0.000000  loss: 3.9214 (3.8762)  class_acc: 0.2500 (0.2969)  loss_scale: 32768.0000 (57914.5335)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1239  max mem: 15572
Epoch: [20]  [ 770/2809]  eta: 0:19:25  lr: 0.000028  min_lr: 0.000000  loss: 3.7943 (3.8739)  class_acc: 0.2917 (0.2971)  loss_scale: 32768.0000 (57588.3787)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1217  max mem: 15572
Epoch: [20]  [ 780/2809]  eta: 0:19:17  lr: 0.000028  min_lr: 0.000000  loss: 3.7645 (3.8739)  class_acc: 0.2917 (0.2968)  loss_scale: 32768.0000 (57270.5762)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.1039  max mem: 15572
Epoch: [20]  [ 790/2809]  eta: 0:19:13  lr: 0.000028  min_lr: 0.000000  loss: 4.1086 (3.8753)  class_acc: 0.2500 (0.2961)  loss_scale: 32768.0000 (56960.8091)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1443  max mem: 15572
Epoch: [20]  [ 800/2809]  eta: 0:19:04  lr: 0.000028  min_lr: 0.000000  loss: 4.1610 (3.8789)  class_acc: 0.2083 (0.2952)  loss_scale: 32768.0000 (56658.7765)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1082  max mem: 15572
Epoch: [20]  [ 810/2809]  eta: 0:18:58  lr: 0.000028  min_lr: 0.000000  loss: 4.1610 (3.8787)  class_acc: 0.2500 (0.2958)  loss_scale: 32768.0000 (56364.1924)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0723  max mem: 15572
[2025-01-16 00:04:58,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=57000, skipped=384, lr=[2.7198628085713264e-07, 2.7198628085713264e-07, 3.885518297959038e-07, 3.885518297959038e-07, 5.550740425655769e-07, 5.550740425655769e-07, 7.929629179508242e-07, 7.929629179508242e-07, 1.1328041685011774e-06, 1.1328041685011774e-06, 1.6182916692873964e-06, 1.6182916692873964e-06, 2.311845241839138e-06, 2.311845241839138e-06, 3.3026360597701973e-06, 3.3026360597701973e-06, 4.718051513957425e-06, 4.718051513957425e-06, 6.740073591367751e-06, 6.740073591367751e-06, 9.628676559096786e-06, 9.628676559096786e-06, 1.3755252227281124e-05, 1.3755252227281124e-05, 1.9650360324687324e-05, 1.9650360324687324e-05, 2.807194332098189e-05, 2.807194332098189e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:04:58,092] [INFO] [timer.py:260:stop] epoch=0/micro_step=57000/global_step=57000, RunningAvgSamplesPerSec=28.505941180372986, CurrSamplesPerSec=31.623739458575074, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [ 820/2809]  eta: 0:18:51  lr: 0.000028  min_lr: 0.000000  loss: 3.7239 (3.8781)  class_acc: 0.2917 (0.2961)  loss_scale: 32768.0000 (56076.7844)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.1091  max mem: 15572
Epoch: [20]  [ 830/2809]  eta: 0:18:48  lr: 0.000028  min_lr: 0.000000  loss: 4.1237 (3.8789)  class_acc: 0.2083 (0.2956)  loss_scale: 32768.0000 (55796.2936)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1434  max mem: 15572
Epoch: [20]  [ 840/2809]  eta: 0:18:43  lr: 0.000028  min_lr: 0.000000  loss: 3.8430 (3.8781)  class_acc: 0.2917 (0.2958)  loss_scale: 32768.0000 (55522.4732)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.1951  max mem: 15572
Epoch: [20]  [ 850/2809]  eta: 0:18:35  lr: 0.000028  min_lr: 0.000000  loss: 3.6614 (3.8747)  class_acc: 0.3333 (0.2964)  loss_scale: 32768.0000 (55255.0881)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0990  max mem: 15572
Epoch: [20]  [ 860/2809]  eta: 0:18:32  lr: 0.000028  min_lr: 0.000000  loss: 3.5138 (3.8732)  class_acc: 0.3333 (0.2967)  loss_scale: 32768.0000 (54993.9141)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1132  max mem: 15572
[2025-01-16 00:05:26,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:05:26,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 870/2809]  eta: 0:18:27  lr: 0.000028  min_lr: 0.000000  loss: 3.9192 (3.8727)  class_acc: 0.2917 (0.2968)  loss_scale: 32768.0000 (54851.6005)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1948  max mem: 15572
Epoch: [20]  [ 880/2809]  eta: 0:18:22  lr: 0.000028  min_lr: 0.000000  loss: 4.0577 (3.8734)  class_acc: 0.2917 (0.2969)  loss_scale: 65536.0000 (54972.8763)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1581  max mem: 15572
Epoch: [20]  [ 890/2809]  eta: 0:18:17  lr: 0.000028  min_lr: 0.000000  loss: 3.9626 (3.8719)  class_acc: 0.2917 (0.2976)  loss_scale: 65536.0000 (55091.4299)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1265  max mem: 15572
Epoch: [20]  [ 900/2809]  eta: 0:18:12  lr: 0.000028  min_lr: 0.000000  loss: 3.7246 (3.8698)  class_acc: 0.2917 (0.2976)  loss_scale: 65536.0000 (55207.3518)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1445  max mem: 15572
Epoch: [20]  [ 910/2809]  eta: 0:18:07  lr: 0.000028  min_lr: 0.000000  loss: 3.7246 (3.8704)  class_acc: 0.2917 (0.2975)  loss_scale: 65536.0000 (55320.7289)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1639  max mem: 15572
Epoch: [20]  [ 920/2809]  eta: 0:18:01  lr: 0.000028  min_lr: 0.000000  loss: 4.0058 (3.8723)  class_acc: 0.2500 (0.2973)  loss_scale: 65536.0000 (55431.6439)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1393  max mem: 15572
Epoch: [20]  [ 930/2809]  eta: 0:17:53  lr: 0.000028  min_lr: 0.000000  loss: 3.8860 (3.8725)  class_acc: 0.2500 (0.2970)  loss_scale: 65536.0000 (55540.1762)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0791  max mem: 15572
Epoch: [20]  [ 940/2809]  eta: 0:17:47  lr: 0.000028  min_lr: 0.000000  loss: 3.8334 (3.8725)  class_acc: 0.2917 (0.2973)  loss_scale: 65536.0000 (55646.4017)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0835  max mem: 15572
Epoch: [20]  [ 950/2809]  eta: 0:17:42  lr: 0.000028  min_lr: 0.000000  loss: 3.6354 (3.8730)  class_acc: 0.2917 (0.2969)  loss_scale: 65536.0000 (55750.3933)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1360  max mem: 15572
Epoch: [20]  [ 960/2809]  eta: 0:17:35  lr: 0.000028  min_lr: 0.000000  loss: 3.7461 (3.8738)  class_acc: 0.2500 (0.2964)  loss_scale: 65536.0000 (55852.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0948  max mem: 15572
Epoch: [20]  [ 970/2809]  eta: 0:17:29  lr: 0.000028  min_lr: 0.000000  loss: 3.8512 (3.8745)  class_acc: 0.2917 (0.2966)  loss_scale: 65536.0000 (55951.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0676  max mem: 15572
Epoch: [20]  [ 980/2809]  eta: 0:17:24  lr: 0.000028  min_lr: 0.000000  loss: 3.8385 (3.8725)  class_acc: 0.3333 (0.2973)  loss_scale: 65536.0000 (56049.6473)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0930  max mem: 15572
Epoch: [20]  [ 990/2809]  eta: 0:17:18  lr: 0.000028  min_lr: 0.000000  loss: 3.6225 (3.8722)  class_acc: 0.3750 (0.2978)  loss_scale: 65536.0000 (56145.3724)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1164  max mem: 15572
[2025-01-16 00:06:39,824] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:06:39,824] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:06:42,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57179
[2025-01-16 00:06:42,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:06:42,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1000/2809]  eta: 0:17:12  lr: 0.000028  min_lr: 0.000000  loss: 3.6225 (3.8713)  class_acc: 0.3333 (0.2977)  loss_scale: 65536.0000 (56435.5964)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1336  max mem: 15572
Epoch: [20]  [1010/2809]  eta: 0:17:06  lr: 0.000028  min_lr: 0.000000  loss: 3.6920 (3.8710)  class_acc: 0.2917 (0.2978)  loss_scale: 65536.0000 (56525.6103)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0940  max mem: 15572
Epoch: [20]  [1020/2809]  eta: 0:17:01  lr: 0.000028  min_lr: 0.000000  loss: 3.7847 (3.8721)  class_acc: 0.2500 (0.2974)  loss_scale: 65536.0000 (56613.8609)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1080  max mem: 15572
Epoch: [20]  [1030/2809]  eta: 0:16:55  lr: 0.000028  min_lr: 0.000000  loss: 3.8427 (3.8713)  class_acc: 0.2500 (0.2971)  loss_scale: 65536.0000 (56700.3996)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1495  max mem: 15572
Epoch: [20]  [1040/2809]  eta: 0:16:49  lr: 0.000028  min_lr: 0.000000  loss: 3.8178 (3.8712)  class_acc: 0.2500 (0.2972)  loss_scale: 65536.0000 (56785.2757)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0996  max mem: 15572
Epoch: [20]  [1050/2809]  eta: 0:16:42  lr: 0.000028  min_lr: 0.000000  loss: 3.8178 (3.8703)  class_acc: 0.2917 (0.2975)  loss_scale: 65536.0000 (56868.5366)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0493  max mem: 15572
[2025-01-16 00:07:13,940] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57238
[2025-01-16 00:07:13,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:07:13,940] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1060/2809]  eta: 0:16:37  lr: 0.000028  min_lr: 0.000000  loss: 3.7116 (3.8667)  class_acc: 0.3333 (0.2980)  loss_scale: 65536.0000 (56857.5759)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1069  max mem: 15572
Epoch: [20]  [1070/2809]  eta: 0:16:31  lr: 0.000028  min_lr: 0.000000  loss: 3.7698 (3.8659)  class_acc: 0.2500 (0.2976)  loss_scale: 32768.0000 (56632.6499)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1495  max mem: 15572
Epoch: [20]  [1080/2809]  eta: 0:16:26  lr: 0.000028  min_lr: 0.000000  loss: 4.0018 (3.8680)  class_acc: 0.2083 (0.2973)  loss_scale: 32768.0000 (56411.8853)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1348  max mem: 15572
Epoch: [20]  [1090/2809]  eta: 0:16:21  lr: 0.000028  min_lr: 0.000000  loss: 4.0760 (3.8671)  class_acc: 0.2500 (0.2972)  loss_scale: 32768.0000 (56195.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1676  max mem: 15572
Epoch: [20]  [1100/2809]  eta: 0:16:15  lr: 0.000028  min_lr: 0.000000  loss: 3.6990 (3.8658)  class_acc: 0.3333 (0.2978)  loss_scale: 32768.0000 (55982.3869)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1256  max mem: 15572
Epoch: [20]  [1110/2809]  eta: 0:16:09  lr: 0.000028  min_lr: 0.000000  loss: 3.7320 (3.8652)  class_acc: 0.3333 (0.2977)  loss_scale: 32768.0000 (55773.4365)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0817  max mem: 15572
Epoch: [20]  [1120/2809]  eta: 0:16:02  lr: 0.000028  min_lr: 0.000000  loss: 3.7537 (3.8649)  class_acc: 0.2917 (0.2977)  loss_scale: 32768.0000 (55568.2141)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1065  max mem: 15572
Epoch: [20]  [1130/2809]  eta: 0:15:56  lr: 0.000028  min_lr: 0.000000  loss: 3.7537 (3.8653)  class_acc: 0.2917 (0.2978)  loss_scale: 32768.0000 (55366.6207)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1069  max mem: 15572
Epoch: [20]  [1140/2809]  eta: 0:15:51  lr: 0.000028  min_lr: 0.000000  loss: 3.9051 (3.8669)  class_acc: 0.2917 (0.2975)  loss_scale: 32768.0000 (55168.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1182  max mem: 15572
Epoch: [20]  [1150/2809]  eta: 0:15:44  lr: 0.000028  min_lr: 0.000000  loss: 4.0245 (3.8663)  class_acc: 0.2917 (0.2976)  loss_scale: 32768.0000 (54973.9427)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0960  max mem: 15572
Epoch: [20]  [1160/2809]  eta: 0:15:40  lr: 0.000028  min_lr: 0.000000  loss: 4.0496 (3.8684)  class_acc: 0.2917 (0.2973)  loss_scale: 32768.0000 (54782.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1235  max mem: 15572
Epoch: [20]  [1170/2809]  eta: 0:15:33  lr: 0.000028  min_lr: 0.000000  loss: 3.9617 (3.8683)  class_acc: 0.2500 (0.2970)  loss_scale: 32768.0000 (54594.6781)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1407  max mem: 15572
Epoch: [20]  [1180/2809]  eta: 0:15:27  lr: 0.000028  min_lr: 0.000000  loss: 3.7273 (3.8668)  class_acc: 0.2917 (0.2975)  loss_scale: 32768.0000 (54409.8628)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0767  max mem: 15572
[2025-01-16 00:08:26,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:08:26,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1190/2809]  eta: 0:15:21  lr: 0.000028  min_lr: 0.000000  loss: 3.5778 (3.8660)  class_acc: 0.3333 (0.2975)  loss_scale: 32768.0000 (54338.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1197  max mem: 15572
Epoch: [20]  [1200/2809]  eta: 0:15:15  lr: 0.000028  min_lr: 0.000000  loss: 3.9277 (3.8663)  class_acc: 0.2500 (0.2970)  loss_scale: 65536.0000 (54431.4405)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1522  max mem: 15572
Epoch: [20]  [1210/2809]  eta: 0:15:11  lr: 0.000028  min_lr: 0.000000  loss: 3.9976 (3.8666)  class_acc: 0.2500 (0.2971)  loss_scale: 65536.0000 (54523.1379)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1689  max mem: 15572
Epoch: [20]  [1220/2809]  eta: 0:15:05  lr: 0.000028  min_lr: 0.000000  loss: 3.7311 (3.8642)  class_acc: 0.3750 (0.2976)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1703  max mem: 15572
Epoch: [20]  [1230/2809]  eta: 0:15:00  lr: 0.000028  min_lr: 0.000000  loss: 3.7523 (3.8644)  class_acc: 0.3333 (0.2974)  loss_scale: 65536.0000 (54702.0634)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1588  max mem: 15572
Epoch: [20]  [1240/2809]  eta: 0:14:53  lr: 0.000028  min_lr: 0.000000  loss: 3.9039 (3.8644)  class_acc: 0.2500 (0.2976)  loss_scale: 65536.0000 (54789.3634)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1354  max mem: 15572
Epoch: [20]  [1250/2809]  eta: 0:14:48  lr: 0.000028  min_lr: 0.000000  loss: 3.8774 (3.8646)  class_acc: 0.3333 (0.2978)  loss_scale: 65536.0000 (54875.2678)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0978  max mem: 15572
Epoch: [20]  [1260/2809]  eta: 0:14:42  lr: 0.000028  min_lr: 0.000000  loss: 3.7215 (3.8634)  class_acc: 0.3333 (0.2984)  loss_scale: 65536.0000 (54959.8097)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1148  max mem: 15572
Epoch: [20]  [1270/2809]  eta: 0:14:35  lr: 0.000028  min_lr: 0.000000  loss: 3.9188 (3.8644)  class_acc: 0.2917 (0.2980)  loss_scale: 65536.0000 (55043.0212)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1034  max mem: 15572
Epoch: [20]  [1280/2809]  eta: 0:14:30  lr: 0.000028  min_lr: 0.000000  loss: 3.7903 (3.8632)  class_acc: 0.2917 (0.2983)  loss_scale: 65536.0000 (55124.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1311  max mem: 15572
Epoch: [20]  [1290/2809]  eta: 0:14:25  lr: 0.000028  min_lr: 0.000000  loss: 3.5730 (3.8611)  class_acc: 0.3750 (0.2986)  loss_scale: 65536.0000 (55205.5771)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1731  max mem: 15572
Epoch: [20]  [1300/2809]  eta: 0:14:19  lr: 0.000028  min_lr: 0.000000  loss: 3.7204 (3.8610)  class_acc: 0.3333 (0.2986)  loss_scale: 65536.0000 (55284.9808)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1480  max mem: 15572
Epoch: [20]  [1310/2809]  eta: 0:14:12  lr: 0.000028  min_lr: 0.000000  loss: 3.9018 (3.8602)  class_acc: 0.3333 (0.2991)  loss_scale: 65536.0000 (55363.1732)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0734  max mem: 15572
[2025-01-16 00:09:39,827] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:09:39,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:09:41,134] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57498
[2025-01-16 00:09:41,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:09:41,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1320/2809]  eta: 0:14:07  lr: 0.000028  min_lr: 0.000000  loss: 3.7610 (3.8592)  class_acc: 0.3750 (0.2992)  loss_scale: 65536.0000 (55589.0144)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0953  max mem: 15572
Epoch: [20]  [1330/2809]  eta: 0:14:00  lr: 0.000028  min_lr: 0.000000  loss: 3.8348 (3.8598)  class_acc: 0.2917 (0.2991)  loss_scale: 65536.0000 (55663.7476)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1064  max mem: 15572
Epoch: [20]  [1340/2809]  eta: 0:13:55  lr: 0.000028  min_lr: 0.000000  loss: 4.0228 (3.8610)  class_acc: 0.2917 (0.2990)  loss_scale: 65536.0000 (55737.3661)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0757  max mem: 15572
Epoch: [20]  [1350/2809]  eta: 0:13:49  lr: 0.000028  min_lr: 0.000000  loss: 3.9575 (3.8605)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (55809.8949)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.0879  max mem: 15572
Epoch: [20]  [1360/2809]  eta: 0:13:43  lr: 0.000028  min_lr: 0.000000  loss: 3.7958 (3.8606)  class_acc: 0.2500 (0.2988)  loss_scale: 65536.0000 (55881.3578)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1154  max mem: 15572
Epoch: [20]  [1370/2809]  eta: 0:13:37  lr: 0.000028  min_lr: 0.000000  loss: 3.8306 (3.8598)  class_acc: 0.2500 (0.2991)  loss_scale: 65536.0000 (55951.7783)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1131  max mem: 15572
Epoch: [20]  [1380/2809]  eta: 0:13:31  lr: 0.000028  min_lr: 0.000000  loss: 4.0027 (3.8610)  class_acc: 0.2500 (0.2988)  loss_scale: 65536.0000 (56021.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.1021  max mem: 15572
Epoch: [20]  [1390/2809]  eta: 0:13:26  lr: 0.000028  min_lr: 0.000000  loss: 4.0027 (3.8610)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (56089.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1658  max mem: 15572
Epoch: [20]  [1400/2809]  eta: 0:13:21  lr: 0.000028  min_lr: 0.000000  loss: 3.8034 (3.8607)  class_acc: 0.2917 (0.2991)  loss_scale: 65536.0000 (56157.0079)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.1906  max mem: 15572
Epoch: [20]  [1410/2809]  eta: 0:13:15  lr: 0.000028  min_lr: 0.000000  loss: 3.9483 (3.8620)  class_acc: 0.2917 (0.2989)  loss_scale: 65536.0000 (56223.4784)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1264  max mem: 15572
Epoch: [20]  [1420/2809]  eta: 0:13:10  lr: 0.000028  min_lr: 0.000000  loss: 4.0590 (3.8631)  class_acc: 0.2500 (0.2985)  loss_scale: 65536.0000 (56289.0134)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1323  max mem: 15572
Epoch: [20]  [1430/2809]  eta: 0:13:03  lr: 0.000028  min_lr: 0.000000  loss: 3.8572 (3.8623)  class_acc: 0.2500 (0.2989)  loss_scale: 65536.0000 (56353.6324)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1229  max mem: 15572
Epoch: [20]  [1440/2809]  eta: 0:12:58  lr: 0.000028  min_lr: 0.000000  loss: 3.7377 (3.8626)  class_acc: 0.3333 (0.2991)  loss_scale: 65536.0000 (56417.3546)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1224  max mem: 15572
[2025-01-16 00:10:55,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:10:55,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1450/2809]  eta: 0:12:52  lr: 0.000028  min_lr: 0.000000  loss: 3.8150 (3.8626)  class_acc: 0.3333 (0.2990)  loss_scale: 65536.0000 (56660.8629)  weight_decay: 0.0500 (0.0500)  time: 0.5902  data: 0.1385  max mem: 15572
[2025-01-16 00:10:57,356] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57631
[2025-01-16 00:10:57,357] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:10:57,357] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1460/2809]  eta: 0:12:47  lr: 0.000028  min_lr: 0.000000  loss: 3.8579 (3.8630)  class_acc: 0.2500 (0.2990)  loss_scale: 65536.0000 (56721.6099)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1181  max mem: 15572
Epoch: [20]  [1470/2809]  eta: 0:12:41  lr: 0.000028  min_lr: 0.000000  loss: 3.8579 (3.8633)  class_acc: 0.2500 (0.2990)  loss_scale: 65536.0000 (56781.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1695  max mem: 15572
Epoch: [20]  [1480/2809]  eta: 0:12:35  lr: 0.000028  min_lr: 0.000000  loss: 3.8962 (3.8631)  class_acc: 0.2917 (0.2987)  loss_scale: 65536.0000 (56840.6428)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1438  max mem: 15572
Epoch: [20]  [1490/2809]  eta: 0:12:30  lr: 0.000028  min_lr: 0.000000  loss: 3.8402 (3.8635)  class_acc: 0.2917 (0.2986)  loss_scale: 65536.0000 (56898.9618)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1108  max mem: 15572
Epoch: [20]  [1500/2809]  eta: 0:12:23  lr: 0.000028  min_lr: 0.000000  loss: 3.8044 (3.8641)  class_acc: 0.2917 (0.2984)  loss_scale: 65536.0000 (56956.5037)  weight_decay: 0.0500 (0.0500)  time: 0.5306  data: 0.1021  max mem: 15572
Epoch: [20]  [1510/2809]  eta: 0:12:18  lr: 0.000028  min_lr: 0.000000  loss: 4.0213 (3.8653)  class_acc: 0.2500 (0.2980)  loss_scale: 65536.0000 (57013.2839)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1293  max mem: 15572
Epoch: [20]  [1520/2809]  eta: 0:12:12  lr: 0.000028  min_lr: 0.000000  loss: 4.0213 (3.8658)  class_acc: 0.2917 (0.2982)  loss_scale: 65536.0000 (57069.3176)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1222  max mem: 15572
Epoch: [20]  [1530/2809]  eta: 0:12:07  lr: 0.000028  min_lr: 0.000000  loss: 3.6154 (3.8635)  class_acc: 0.3750 (0.2989)  loss_scale: 65536.0000 (57124.6192)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1337  max mem: 15572
Epoch: [20]  [1540/2809]  eta: 0:12:01  lr: 0.000028  min_lr: 0.000000  loss: 3.6154 (3.8626)  class_acc: 0.3333 (0.2992)  loss_scale: 65536.0000 (57179.2031)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1575  max mem: 15572
Epoch: [20]  [1550/2809]  eta: 0:11:55  lr: 0.000028  min_lr: 0.000000  loss: 3.8794 (3.8622)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (57233.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0997  max mem: 15572
Epoch: [20]  [1560/2809]  eta: 0:11:49  lr: 0.000028  min_lr: 0.000000  loss: 3.8593 (3.8605)  class_acc: 0.2917 (0.2996)  loss_scale: 65536.0000 (57286.2729)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1004  max mem: 15572
Epoch: [20]  [1570/2809]  eta: 0:11:44  lr: 0.000028  min_lr: 0.000000  loss: 3.7977 (3.8607)  class_acc: 0.3333 (0.2998)  loss_scale: 65536.0000 (57338.7855)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0962  max mem: 15572
[2025-01-16 00:12:10,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:12:10,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1580/2809]  eta: 0:11:38  lr: 0.000028  min_lr: 0.000000  loss: 3.8240 (3.8601)  class_acc: 0.3333 (0.3001)  loss_scale: 65536.0000 (57432.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1134  max mem: 15572
[2025-01-16 00:12:11,327] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57763
[2025-01-16 00:12:11,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:12:11,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1590/2809]  eta: 0:11:32  lr: 0.000028  min_lr: 0.000000  loss: 3.8177 (3.8608)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (57565.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.1096  max mem: 15572
Epoch: [20]  [1600/2809]  eta: 0:11:26  lr: 0.000027  min_lr: 0.000000  loss: 3.7005 (3.8599)  class_acc: 0.3333 (0.3004)  loss_scale: 65536.0000 (57615.1905)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0711  max mem: 15572
Epoch: [20]  [1610/2809]  eta: 0:11:20  lr: 0.000027  min_lr: 0.000000  loss: 3.8764 (3.8616)  class_acc: 0.2917 (0.3001)  loss_scale: 65536.0000 (57664.3575)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1287  max mem: 15572
Epoch: [20]  [1620/2809]  eta: 0:11:15  lr: 0.000027  min_lr: 0.000000  loss: 3.9810 (3.8619)  class_acc: 0.2500 (0.3001)  loss_scale: 65536.0000 (57712.9180)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1793  max mem: 15572
Epoch: [20]  [1630/2809]  eta: 0:11:09  lr: 0.000027  min_lr: 0.000000  loss: 3.9086 (3.8616)  class_acc: 0.2500 (0.3001)  loss_scale: 65536.0000 (57760.8829)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1429  max mem: 15572
[2025-01-16 00:12:38,942] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 57812
[2025-01-16 00:12:38,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:12:38,943] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1640/2809]  eta: 0:11:03  lr: 0.000027  min_lr: 0.000000  loss: 3.8443 (3.8607)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (57628.5484)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1240  max mem: 15572
Epoch: [20]  [1650/2809]  eta: 0:10:58  lr: 0.000027  min_lr: 0.000000  loss: 3.6883 (3.8606)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (57477.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1632  max mem: 15572
Epoch: [20]  [1660/2809]  eta: 0:10:53  lr: 0.000027  min_lr: 0.000000  loss: 3.7570 (3.8602)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (57329.2041)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1790  max mem: 15572
Epoch: [20]  [1670/2809]  eta: 0:10:46  lr: 0.000027  min_lr: 0.000000  loss: 3.7570 (3.8609)  class_acc: 0.3333 (0.3000)  loss_scale: 32768.0000 (57182.2190)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1085  max mem: 15572
Epoch: [20]  [1680/2809]  eta: 0:10:41  lr: 0.000027  min_lr: 0.000000  loss: 3.7543 (3.8596)  class_acc: 0.3333 (0.3004)  loss_scale: 32768.0000 (57036.9827)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0850  max mem: 15572
Epoch: [20]  [1690/2809]  eta: 0:10:35  lr: 0.000027  min_lr: 0.000000  loss: 3.5429 (3.8583)  class_acc: 0.3750 (0.3006)  loss_scale: 32768.0000 (56893.4642)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0944  max mem: 15572
Epoch: [20]  [1700/2809]  eta: 0:10:29  lr: 0.000027  min_lr: 0.000000  loss: 3.5429 (3.8559)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (56751.6332)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1197  max mem: 15572
Epoch: [20]  [1710/2809]  eta: 0:10:24  lr: 0.000027  min_lr: 0.000000  loss: 3.7097 (3.8556)  class_acc: 0.3333 (0.3012)  loss_scale: 32768.0000 (56611.4600)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1757  max mem: 15572
Epoch: [20]  [1720/2809]  eta: 0:10:17  lr: 0.000027  min_lr: 0.000000  loss: 3.8186 (3.8551)  class_acc: 0.3333 (0.3013)  loss_scale: 32768.0000 (56472.9157)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0997  max mem: 15572
Epoch: [20]  [1730/2809]  eta: 0:10:12  lr: 0.000027  min_lr: 0.000000  loss: 3.9078 (3.8552)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (56335.9723)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.1018  max mem: 15572
Epoch: [20]  [1740/2809]  eta: 0:10:06  lr: 0.000027  min_lr: 0.000000  loss: 3.8733 (3.8548)  class_acc: 0.3333 (0.3015)  loss_scale: 32768.0000 (56200.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1383  max mem: 15572
Epoch: [20]  [1750/2809]  eta: 0:10:01  lr: 0.000027  min_lr: 0.000000  loss: 3.8288 (3.8543)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (56066.7778)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1072  max mem: 15572
Epoch: [20]  [1760/2809]  eta: 0:09:55  lr: 0.000027  min_lr: 0.000000  loss: 3.7517 (3.8539)  class_acc: 0.2500 (0.3017)  loss_scale: 32768.0000 (55934.4736)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1385  max mem: 15572
[2025-01-16 00:13:51,914] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:13:51,914] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [1770/2809]  eta: 0:09:49  lr: 0.000027  min_lr: 0.000000  loss: 4.0157 (3.8549)  class_acc: 0.2500 (0.3014)  loss_scale: 32768.0000 (55988.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1259  max mem: 15572
Epoch: [20]  [1780/2809]  eta: 0:09:44  lr: 0.000027  min_lr: 0.000000  loss: 4.0547 (3.8557)  class_acc: 0.2500 (0.3011)  loss_scale: 65536.0000 (56042.2953)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1186  max mem: 15572
Epoch: [20]  [1790/2809]  eta: 0:09:38  lr: 0.000027  min_lr: 0.000000  loss: 4.0840 (3.8571)  class_acc: 0.2500 (0.3006)  loss_scale: 65536.0000 (56095.3032)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1285  max mem: 15572
Epoch: [20]  [1800/2809]  eta: 0:09:32  lr: 0.000027  min_lr: 0.000000  loss: 4.1486 (3.8579)  class_acc: 0.2083 (0.3003)  loss_scale: 65536.0000 (56147.7224)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1064  max mem: 15572
Epoch: [20]  [1810/2809]  eta: 0:09:27  lr: 0.000027  min_lr: 0.000000  loss: 4.0545 (3.8576)  class_acc: 0.2500 (0.3004)  loss_scale: 65536.0000 (56199.5627)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.1616  max mem: 15572
[2025-01-16 00:14:25,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=58000, skipped=390, lr=[2.648575876106438e-07, 2.648575876106438e-07, 3.783679823009198e-07, 3.783679823009198e-07, 5.40525689001314e-07, 5.40525689001314e-07, 7.721795557161629e-07, 7.721795557161629e-07, 1.1031136510230898e-06, 1.1031136510230898e-06, 1.5758766443187e-06, 1.5758766443187e-06, 2.2512523490267144e-06, 2.2512523490267144e-06, 3.216074784323878e-06, 3.216074784323878e-06, 4.594392549034111e-06, 4.594392549034111e-06, 6.563417927191589e-06, 6.563417927191589e-06, 9.376311324559412e-06, 9.376311324559412e-06, 1.3394730463656304e-05, 1.3394730463656304e-05, 1.9135329233794723e-05, 1.9135329233794723e-05, 2.7336184619706747e-05, 2.7336184619706747e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:14:25,488] [INFO] [timer.py:260:stop] epoch=0/micro_step=58000/global_step=58000, RunningAvgSamplesPerSec=28.509387616336625, CurrSamplesPerSec=29.63301597469295, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [20]  [1820/2809]  eta: 0:09:21  lr: 0.000027  min_lr: 0.000000  loss: 3.8686 (3.8566)  class_acc: 0.3333 (0.3005)  loss_scale: 65536.0000 (56250.8336)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1488  max mem: 15572
Epoch: [20]  [1830/2809]  eta: 0:09:15  lr: 0.000027  min_lr: 0.000000  loss: 3.8686 (3.8568)  class_acc: 0.2500 (0.3004)  loss_scale: 65536.0000 (56301.5445)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0826  max mem: 15572
Epoch: [20]  [1840/2809]  eta: 0:09:10  lr: 0.000027  min_lr: 0.000000  loss: 3.8708 (3.8571)  class_acc: 0.2500 (0.3004)  loss_scale: 65536.0000 (56351.7045)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1325  max mem: 15572
Epoch: [20]  [1850/2809]  eta: 0:09:04  lr: 0.000027  min_lr: 0.000000  loss: 3.7775 (3.8565)  class_acc: 0.3333 (0.3008)  loss_scale: 65536.0000 (56401.3225)  weight_decay: 0.0500 (0.0500)  time: 0.6193  data: 0.1603  max mem: 15572
Epoch: [20]  [1860/2809]  eta: 0:08:59  lr: 0.000027  min_lr: 0.000000  loss: 3.7821 (3.8570)  class_acc: 0.3333 (0.3007)  loss_scale: 65536.0000 (56450.4073)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1748  max mem: 15572
Epoch: [20]  [1870/2809]  eta: 0:08:53  lr: 0.000027  min_lr: 0.000000  loss: 3.9100 (3.8579)  class_acc: 0.2500 (0.3004)  loss_scale: 65536.0000 (56498.9674)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1178  max mem: 15572
Epoch: [20]  [1880/2809]  eta: 0:08:48  lr: 0.000027  min_lr: 0.000000  loss: 3.8860 (3.8581)  class_acc: 0.2500 (0.3004)  loss_scale: 65536.0000 (56547.0112)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1472  max mem: 15572
[2025-01-16 00:15:06,089] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:15:06,089] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1890/2809]  eta: 0:08:42  lr: 0.000027  min_lr: 0.000000  loss: 3.7363 (3.8588)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (56663.8604)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1324  max mem: 15572
[2025-01-16 00:15:10,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58078
[2025-01-16 00:15:10,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:15:10,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1900/2809]  eta: 0:08:36  lr: 0.000027  min_lr: 0.000000  loss: 3.7640 (3.8579)  class_acc: 0.3333 (0.3006)  loss_scale: 65536.0000 (56951.8527)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0447  max mem: 15572
Epoch: [20]  [1910/2809]  eta: 0:08:30  lr: 0.000027  min_lr: 0.000000  loss: 3.7114 (3.8573)  class_acc: 0.2917 (0.3004)  loss_scale: 65536.0000 (56996.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1234  max mem: 15572
Epoch: [20]  [1920/2809]  eta: 0:08:25  lr: 0.000027  min_lr: 0.000000  loss: 4.1135 (3.8588)  class_acc: 0.2083 (0.3001)  loss_scale: 65536.0000 (57041.2244)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1468  max mem: 15572
Epoch: [20]  [1930/2809]  eta: 0:08:19  lr: 0.000027  min_lr: 0.000000  loss: 4.1878 (3.8604)  class_acc: 0.2083 (0.2998)  loss_scale: 65536.0000 (57085.2160)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1568  max mem: 15572
Epoch: [20]  [1940/2809]  eta: 0:08:13  lr: 0.000027  min_lr: 0.000000  loss: 4.0954 (3.8606)  class_acc: 0.2083 (0.2997)  loss_scale: 65536.0000 (57128.7543)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1436  max mem: 15572
Epoch: [20]  [1950/2809]  eta: 0:08:08  lr: 0.000027  min_lr: 0.000000  loss: 3.9161 (3.8614)  class_acc: 0.2500 (0.2994)  loss_scale: 65536.0000 (57171.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1489  max mem: 15572
Epoch: [20]  [1960/2809]  eta: 0:08:02  lr: 0.000027  min_lr: 0.000000  loss: 3.8461 (3.8607)  class_acc: 0.2500 (0.2995)  loss_scale: 65536.0000 (57214.4987)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1340  max mem: 15572
Epoch: [20]  [1970/2809]  eta: 0:07:56  lr: 0.000027  min_lr: 0.000000  loss: 3.5516 (3.8598)  class_acc: 0.2917 (0.2995)  loss_scale: 65536.0000 (57256.7184)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1200  max mem: 15572
[2025-01-16 00:15:54,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58156
[2025-01-16 00:15:54,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:15:54,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [1980/2809]  eta: 0:07:50  lr: 0.000027  min_lr: 0.000000  loss: 3.7883 (3.8603)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (57215.8062)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1119  max mem: 15572
Epoch: [20]  [1990/2809]  eta: 0:07:45  lr: 0.000027  min_lr: 0.000000  loss: 3.8429 (3.8600)  class_acc: 0.2500 (0.2992)  loss_scale: 32768.0000 (57093.0146)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1281  max mem: 15572
Epoch: [20]  [2000/2809]  eta: 0:07:39  lr: 0.000027  min_lr: 0.000000  loss: 3.8234 (3.8592)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (56971.4503)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.1663  max mem: 15572
Epoch: [20]  [2010/2809]  eta: 0:07:34  lr: 0.000027  min_lr: 0.000000  loss: 3.9092 (3.8601)  class_acc: 0.2500 (0.2990)  loss_scale: 32768.0000 (56851.0950)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1612  max mem: 15572
Epoch: [20]  [2020/2809]  eta: 0:07:28  lr: 0.000027  min_lr: 0.000000  loss: 4.1094 (3.8608)  class_acc: 0.2500 (0.2990)  loss_scale: 32768.0000 (56731.9307)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1006  max mem: 15572
Epoch: [20]  [2030/2809]  eta: 0:07:22  lr: 0.000027  min_lr: 0.000000  loss: 3.8071 (3.8603)  class_acc: 0.2917 (0.2991)  loss_scale: 32768.0000 (56613.9399)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0956  max mem: 15572
Epoch: [20]  [2040/2809]  eta: 0:07:17  lr: 0.000027  min_lr: 0.000000  loss: 3.6806 (3.8596)  class_acc: 0.3333 (0.2994)  loss_scale: 32768.0000 (56497.1053)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1339  max mem: 15572
Epoch: [20]  [2050/2809]  eta: 0:07:11  lr: 0.000027  min_lr: 0.000000  loss: 3.8601 (3.8595)  class_acc: 0.2917 (0.2993)  loss_scale: 32768.0000 (56381.4100)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1310  max mem: 15572
Epoch: [20]  [2060/2809]  eta: 0:07:05  lr: 0.000027  min_lr: 0.000000  loss: 3.8781 (3.8594)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (56266.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1189  max mem: 15572
Epoch: [20]  [2070/2809]  eta: 0:06:59  lr: 0.000027  min_lr: 0.000000  loss: 3.7491 (3.8585)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (56153.3713)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1044  max mem: 15572
Epoch: [20]  [2080/2809]  eta: 0:06:54  lr: 0.000027  min_lr: 0.000000  loss: 3.7591 (3.8584)  class_acc: 0.2917 (0.2994)  loss_scale: 32768.0000 (56040.9957)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1539  max mem: 15572
Epoch: [20]  [2090/2809]  eta: 0:06:48  lr: 0.000027  min_lr: 0.000000  loss: 3.8368 (3.8580)  class_acc: 0.2917 (0.2995)  loss_scale: 32768.0000 (55929.6949)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1282  max mem: 15572
Epoch: [20]  [2100/2809]  eta: 0:06:42  lr: 0.000027  min_lr: 0.000000  loss: 3.8703 (3.8586)  class_acc: 0.2500 (0.2993)  loss_scale: 32768.0000 (55819.4536)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1007  max mem: 15572
[2025-01-16 00:17:08,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:17:08,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2110/2809]  eta: 0:06:37  lr: 0.000027  min_lr: 0.000000  loss: 3.6525 (3.8568)  class_acc: 0.2917 (0.2998)  loss_scale: 32768.0000 (55803.3918)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1147  max mem: 15572
Epoch: [20]  [2120/2809]  eta: 0:06:31  lr: 0.000027  min_lr: 0.000000  loss: 3.6690 (3.8575)  class_acc: 0.3333 (0.2996)  loss_scale: 65536.0000 (55849.2786)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1008  max mem: 15572
Epoch: [20]  [2130/2809]  eta: 0:06:25  lr: 0.000027  min_lr: 0.000000  loss: 3.9238 (3.8574)  class_acc: 0.2500 (0.2994)  loss_scale: 65536.0000 (55894.7349)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0684  max mem: 15572
Epoch: [20]  [2140/2809]  eta: 0:06:19  lr: 0.000027  min_lr: 0.000000  loss: 3.9238 (3.8585)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (55939.7665)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0681  max mem: 15572
Epoch: [20]  [2150/2809]  eta: 0:06:14  lr: 0.000027  min_lr: 0.000000  loss: 3.8905 (3.8581)  class_acc: 0.2917 (0.2993)  loss_scale: 65536.0000 (55984.3794)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1071  max mem: 15572
Epoch: [20]  [2160/2809]  eta: 0:06:08  lr: 0.000027  min_lr: 0.000000  loss: 3.7484 (3.8579)  class_acc: 0.3333 (0.2995)  loss_scale: 65536.0000 (56028.5794)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1605  max mem: 15572
Epoch: [20]  [2170/2809]  eta: 0:06:02  lr: 0.000027  min_lr: 0.000000  loss: 3.8823 (3.8580)  class_acc: 0.2917 (0.2994)  loss_scale: 65536.0000 (56072.3722)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1627  max mem: 15572
Epoch: [20]  [2180/2809]  eta: 0:05:57  lr: 0.000027  min_lr: 0.000000  loss: 3.8043 (3.8569)  class_acc: 0.3333 (0.2998)  loss_scale: 65536.0000 (56115.7634)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0992  max mem: 15572
Epoch: [20]  [2190/2809]  eta: 0:05:51  lr: 0.000027  min_lr: 0.000000  loss: 3.6941 (3.8568)  class_acc: 0.3333 (0.2998)  loss_scale: 65536.0000 (56158.7586)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0942  max mem: 15572
Epoch: [20]  [2200/2809]  eta: 0:05:45  lr: 0.000027  min_lr: 0.000000  loss: 3.7801 (3.8568)  class_acc: 0.2917 (0.2998)  loss_scale: 65536.0000 (56201.3630)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1193  max mem: 15572
Epoch: [20]  [2210/2809]  eta: 0:05:40  lr: 0.000027  min_lr: 0.000000  loss: 3.7986 (3.8564)  class_acc: 0.2500 (0.2998)  loss_scale: 65536.0000 (56243.5821)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1481  max mem: 15572
Epoch: [20]  [2220/2809]  eta: 0:05:34  lr: 0.000027  min_lr: 0.000000  loss: 3.8125 (3.8557)  class_acc: 0.2500 (0.2998)  loss_scale: 65536.0000 (56285.4210)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1153  max mem: 15572
Epoch: [20]  [2230/2809]  eta: 0:05:28  lr: 0.000027  min_lr: 0.000000  loss: 3.8125 (3.8551)  class_acc: 0.2917 (0.3001)  loss_scale: 65536.0000 (56326.8848)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0824  max mem: 15572
[2025-01-16 00:18:18,969] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:18:18,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:18:19,374] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58414
[2025-01-16 00:18:19,375] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:18:19,375] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2240/2809]  eta: 0:05:22  lr: 0.000027  min_lr: 0.000000  loss: 3.8055 (3.8547)  class_acc: 0.3333 (0.3002)  loss_scale: 65536.0000 (56397.2227)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.1147  max mem: 15572
Epoch: [20]  [2250/2809]  eta: 0:05:17  lr: 0.000027  min_lr: 0.000000  loss: 3.8805 (3.8550)  class_acc: 0.2917 (0.3003)  loss_scale: 65536.0000 (56437.8214)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1247  max mem: 15572
Epoch: [20]  [2260/2809]  eta: 0:05:11  lr: 0.000027  min_lr: 0.000000  loss: 3.9755 (3.8548)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (56478.0610)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1557  max mem: 15572
Epoch: [20]  [2270/2809]  eta: 0:05:05  lr: 0.000027  min_lr: 0.000000  loss: 3.9021 (3.8545)  class_acc: 0.2917 (0.3003)  loss_scale: 65536.0000 (56517.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1582  max mem: 15572
Epoch: [20]  [2280/2809]  eta: 0:05:00  lr: 0.000027  min_lr: 0.000000  loss: 3.8645 (3.8544)  class_acc: 0.2917 (0.3004)  loss_scale: 65536.0000 (56557.4818)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1326  max mem: 15572
Epoch: [20]  [2290/2809]  eta: 0:04:54  lr: 0.000027  min_lr: 0.000000  loss: 3.8764 (3.8548)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (56596.6722)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1015  max mem: 15572
Epoch: [20]  [2300/2809]  eta: 0:04:48  lr: 0.000027  min_lr: 0.000000  loss: 3.7932 (3.8542)  class_acc: 0.3333 (0.3005)  loss_scale: 65536.0000 (56635.5219)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0744  max mem: 15572
Epoch: [20]  [2310/2809]  eta: 0:04:43  lr: 0.000027  min_lr: 0.000000  loss: 3.7907 (3.8549)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (56674.0355)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1255  max mem: 15572
Epoch: [20]  [2320/2809]  eta: 0:04:37  lr: 0.000027  min_lr: 0.000000  loss: 3.8540 (3.8545)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (56712.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1227  max mem: 15572
Epoch: [20]  [2330/2809]  eta: 0:04:31  lr: 0.000027  min_lr: 0.000000  loss: 3.8163 (3.8547)  class_acc: 0.2917 (0.3004)  loss_scale: 65536.0000 (56750.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0968  max mem: 15572
Epoch: [20]  [2340/2809]  eta: 0:04:25  lr: 0.000027  min_lr: 0.000000  loss: 3.6723 (3.8536)  class_acc: 0.3333 (0.3008)  loss_scale: 65536.0000 (56787.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1103  max mem: 15572
Epoch: [20]  [2350/2809]  eta: 0:04:20  lr: 0.000027  min_lr: 0.000000  loss: 3.6723 (3.8531)  class_acc: 0.3333 (0.3010)  loss_scale: 65536.0000 (56824.8133)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1127  max mem: 15572
Epoch: [20]  [2360/2809]  eta: 0:04:14  lr: 0.000027  min_lr: 0.000000  loss: 3.8543 (3.8527)  class_acc: 0.2917 (0.3010)  loss_scale: 65536.0000 (56861.7094)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1211  max mem: 15572
[2025-01-16 00:19:31,410] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:19:31,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:19:32,241] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58545
[2025-01-16 00:19:32,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:19:32,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2370/2809]  eta: 0:04:08  lr: 0.000027  min_lr: 0.000000  loss: 3.8669 (3.8527)  class_acc: 0.2917 (0.3010)  loss_scale: 65536.0000 (56953.5757)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1552  max mem: 15572
Epoch: [20]  [2380/2809]  eta: 0:04:03  lr: 0.000027  min_lr: 0.000000  loss: 3.8474 (3.8527)  class_acc: 0.2917 (0.3009)  loss_scale: 65536.0000 (56989.6212)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1593  max mem: 15572
Epoch: [20]  [2390/2809]  eta: 0:03:57  lr: 0.000027  min_lr: 0.000000  loss: 3.8411 (3.8528)  class_acc: 0.2500 (0.3009)  loss_scale: 65536.0000 (57025.3651)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1325  max mem: 15572
Epoch: [20]  [2400/2809]  eta: 0:03:51  lr: 0.000027  min_lr: 0.000000  loss: 3.8573 (3.8527)  class_acc: 0.2917 (0.3008)  loss_scale: 65536.0000 (57060.8113)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1582  max mem: 15572
Epoch: [20]  [2410/2809]  eta: 0:03:46  lr: 0.000027  min_lr: 0.000000  loss: 3.8873 (3.8524)  class_acc: 0.2917 (0.3008)  loss_scale: 65536.0000 (57095.9635)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.2252  max mem: 15572
Epoch: [20]  [2420/2809]  eta: 0:03:40  lr: 0.000027  min_lr: 0.000000  loss: 3.8873 (3.8518)  class_acc: 0.2917 (0.3009)  loss_scale: 65536.0000 (57130.8253)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1738  max mem: 15572
Epoch: [20]  [2430/2809]  eta: 0:03:35  lr: 0.000027  min_lr: 0.000000  loss: 3.6930 (3.8519)  class_acc: 0.2917 (0.3008)  loss_scale: 65536.0000 (57165.4002)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1199  max mem: 15572
Epoch: [20]  [2440/2809]  eta: 0:03:29  lr: 0.000027  min_lr: 0.000000  loss: 3.7832 (3.8520)  class_acc: 0.2500 (0.3007)  loss_scale: 65536.0000 (57199.6919)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1766  max mem: 15572
Epoch: [20]  [2450/2809]  eta: 0:03:23  lr: 0.000027  min_lr: 0.000000  loss: 3.7614 (3.8514)  class_acc: 0.2917 (0.3009)  loss_scale: 65536.0000 (57233.7038)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1162  max mem: 15572
Epoch: [20]  [2460/2809]  eta: 0:03:17  lr: 0.000027  min_lr: 0.000000  loss: 3.7614 (3.8510)  class_acc: 0.3750 (0.3011)  loss_scale: 65536.0000 (57267.4393)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0674  max mem: 15572
Epoch: [20]  [2470/2809]  eta: 0:03:12  lr: 0.000027  min_lr: 0.000000  loss: 3.8102 (3.8508)  class_acc: 0.2917 (0.3010)  loss_scale: 65536.0000 (57300.9017)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1036  max mem: 15572
Epoch: [20]  [2480/2809]  eta: 0:03:06  lr: 0.000027  min_lr: 0.000000  loss: 3.8186 (3.8512)  class_acc: 0.2500 (0.3010)  loss_scale: 65536.0000 (57334.0943)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1229  max mem: 15572
Epoch: [20]  [2490/2809]  eta: 0:03:00  lr: 0.000027  min_lr: 0.000000  loss: 4.0565 (3.8514)  class_acc: 0.2917 (0.3010)  loss_scale: 65536.0000 (57367.0205)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1081  max mem: 15572
[2025-01-16 00:20:47,030] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:20:47,030] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:20:48,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58677
[2025-01-16 00:20:48,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:20:48,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [2500/2809]  eta: 0:02:55  lr: 0.000027  min_lr: 0.000000  loss: 4.0122 (3.8523)  class_acc: 0.2500 (0.3007)  loss_scale: 65536.0000 (57478.2951)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0619  max mem: 15572
Epoch: [20]  [2510/2809]  eta: 0:02:49  lr: 0.000027  min_lr: 0.000000  loss: 4.0715 (3.8532)  class_acc: 0.2083 (0.3004)  loss_scale: 65536.0000 (57510.3847)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1027  max mem: 15572
Epoch: [20]  [2520/2809]  eta: 0:02:43  lr: 0.000027  min_lr: 0.000000  loss: 4.0350 (3.8532)  class_acc: 0.2500 (0.3005)  loss_scale: 65536.0000 (57542.2198)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1460  max mem: 15572
Epoch: [20]  [2530/2809]  eta: 0:02:38  lr: 0.000027  min_lr: 0.000000  loss: 3.9246 (3.8533)  class_acc: 0.2500 (0.3005)  loss_scale: 65536.0000 (57573.8032)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1519  max mem: 15572
[2025-01-16 00:21:13,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58720
[2025-01-16 00:21:13,784] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:21:13,784] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2540/2809]  eta: 0:02:32  lr: 0.000027  min_lr: 0.000000  loss: 3.8469 (3.8525)  class_acc: 0.2917 (0.3007)  loss_scale: 65536.0000 (57592.2424)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1757  max mem: 15572
Epoch: [20]  [2550/2809]  eta: 0:02:26  lr: 0.000027  min_lr: 0.000000  loss: 3.8164 (3.8522)  class_acc: 0.3333 (0.3007)  loss_scale: 32768.0000 (57494.9306)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1519  max mem: 15572
Epoch: [20]  [2560/2809]  eta: 0:02:21  lr: 0.000027  min_lr: 0.000000  loss: 3.8358 (3.8523)  class_acc: 0.2500 (0.3006)  loss_scale: 32768.0000 (57398.3788)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0894  max mem: 15572
Epoch: [20]  [2570/2809]  eta: 0:02:15  lr: 0.000027  min_lr: 0.000000  loss: 4.0824 (3.8523)  class_acc: 0.2917 (0.3008)  loss_scale: 32768.0000 (57302.5780)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0740  max mem: 15572
Epoch: [20]  [2580/2809]  eta: 0:02:09  lr: 0.000027  min_lr: 0.000000  loss: 4.0013 (3.8527)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (57207.5196)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1123  max mem: 15572
Epoch: [20]  [2590/2809]  eta: 0:02:04  lr: 0.000027  min_lr: 0.000000  loss: 4.0013 (3.8533)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (57113.1949)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1195  max mem: 15572
Epoch: [20]  [2600/2809]  eta: 0:01:58  lr: 0.000027  min_lr: 0.000000  loss: 3.9697 (3.8538)  class_acc: 0.2917 (0.3008)  loss_scale: 32768.0000 (57019.5955)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1466  max mem: 15572
Epoch: [20]  [2610/2809]  eta: 0:01:52  lr: 0.000027  min_lr: 0.000000  loss: 3.8837 (3.8535)  class_acc: 0.2917 (0.3008)  loss_scale: 32768.0000 (56926.7131)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1617  max mem: 15572
Epoch: [20]  [2620/2809]  eta: 0:01:47  lr: 0.000027  min_lr: 0.000000  loss: 3.8891 (3.8540)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (56834.5395)  weight_decay: 0.0500 (0.0500)  time: 0.6434  data: 0.1652  max mem: 15572
Epoch: [20]  [2630/2809]  eta: 0:01:41  lr: 0.000027  min_lr: 0.000000  loss: 3.8918 (3.8536)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (56743.0665)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0883  max mem: 15572
Epoch: [20]  [2640/2809]  eta: 0:01:35  lr: 0.000027  min_lr: 0.000000  loss: 3.7608 (3.8534)  class_acc: 0.2917 (0.3009)  loss_scale: 32768.0000 (56652.2863)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0965  max mem: 15572
Epoch: [20]  [2650/2809]  eta: 0:01:30  lr: 0.000027  min_lr: 0.000000  loss: 3.9135 (3.8541)  class_acc: 0.2500 (0.3006)  loss_scale: 32768.0000 (56562.1909)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1133  max mem: 15572
Epoch: [20]  [2660/2809]  eta: 0:01:24  lr: 0.000027  min_lr: 0.000000  loss: 3.8161 (3.8538)  class_acc: 0.2083 (0.3007)  loss_scale: 32768.0000 (56472.7726)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0658  max mem: 15572
[2025-01-16 00:22:28,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:22:28,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [2670/2809]  eta: 0:01:18  lr: 0.000027  min_lr: 0.000000  loss: 3.7216 (3.8527)  class_acc: 0.3333 (0.3011)  loss_scale: 32768.0000 (56408.5601)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1677  max mem: 15572
Epoch: [20]  [2680/2809]  eta: 0:01:13  lr: 0.000027  min_lr: 0.000000  loss: 3.8162 (3.8532)  class_acc: 0.3333 (0.3011)  loss_scale: 65536.0000 (56442.6050)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1683  max mem: 15572
Epoch: [20]  [2690/2809]  eta: 0:01:07  lr: 0.000027  min_lr: 0.000000  loss: 3.9977 (3.8537)  class_acc: 0.2500 (0.3010)  loss_scale: 65536.0000 (56476.3969)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0934  max mem: 15572
Epoch: [20]  [2700/2809]  eta: 0:01:01  lr: 0.000027  min_lr: 0.000000  loss: 4.0270 (3.8543)  class_acc: 0.2083 (0.3008)  loss_scale: 65536.0000 (56509.9385)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1214  max mem: 15572
Epoch: [20]  [2710/2809]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000000  loss: 4.0061 (3.8549)  class_acc: 0.2917 (0.3007)  loss_scale: 65536.0000 (56543.2328)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1086  max mem: 15572
Epoch: [20]  [2720/2809]  eta: 0:00:50  lr: 0.000027  min_lr: 0.000000  loss: 3.7701 (3.8537)  class_acc: 0.3333 (0.3010)  loss_scale: 65536.0000 (56576.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5100  data: 0.0486  max mem: 15572
Epoch: [20]  [2730/2809]  eta: 0:00:44  lr: 0.000027  min_lr: 0.000000  loss: 3.4640 (3.8536)  class_acc: 0.3750 (0.3010)  loss_scale: 65536.0000 (56609.0897)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1088  max mem: 15572
Epoch: [20]  [2740/2809]  eta: 0:00:39  lr: 0.000027  min_lr: 0.000000  loss: 3.6399 (3.8530)  class_acc: 0.3333 (0.3013)  loss_scale: 65536.0000 (56641.6578)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1491  max mem: 15572
Epoch: [20]  [2750/2809]  eta: 0:00:33  lr: 0.000027  min_lr: 0.000000  loss: 3.9407 (3.8533)  class_acc: 0.3333 (0.3015)  loss_scale: 65536.0000 (56673.9891)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0581  max mem: 15572
Epoch: [20]  [2760/2809]  eta: 0:00:27  lr: 0.000027  min_lr: 0.000000  loss: 3.7871 (3.8526)  class_acc: 0.3333 (0.3016)  loss_scale: 65536.0000 (56706.0862)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0454  max mem: 15572
Epoch: [20]  [2770/2809]  eta: 0:00:22  lr: 0.000027  min_lr: 0.000000  loss: 3.7572 (3.8521)  class_acc: 0.3750 (0.3018)  loss_scale: 65536.0000 (56737.9516)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0453  max mem: 15572
Epoch: [20]  [2780/2809]  eta: 0:00:16  lr: 0.000027  min_lr: 0.000000  loss: 3.8919 (3.8528)  class_acc: 0.3333 (0.3016)  loss_scale: 65536.0000 (56769.5879)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0572  max mem: 15572
[2025-01-16 00:23:34,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 58968
[2025-01-16 00:23:34,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:23:34,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [2790/2809]  eta: 0:00:10  lr: 0.000027  min_lr: 0.000000  loss: 3.9844 (3.8535)  class_acc: 0.2083 (0.3015)  loss_scale: 65536.0000 (56765.7757)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1323  max mem: 15572
Epoch: [20]  [2800/2809]  eta: 0:00:05  lr: 0.000027  min_lr: 0.000000  loss: 3.9267 (3.8541)  class_acc: 0.2083 (0.3011)  loss_scale: 32768.0000 (56680.1000)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0756  max mem: 15572
Epoch: [20]  [2808/2809]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000000  loss: 3.9267 (3.8546)  class_acc: 0.2083 (0.3009)  loss_scale: 32768.0000 (56611.9986)  weight_decay: 0.0500 (0.0500)  time: 0.4787  data: 0.0755  max mem: 15572
Epoch: [20] Total time: 0:26:32 (0.5670 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000000  loss: 3.9267 (3.8546)  class_acc: 0.2083 (0.3009)  loss_scale: 32768.0000 (56611.9986)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:04  loss: 0.4270 (0.4270)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.9859  data: 3.8134  max mem: 15572
Val:  [ 10/272]  eta: 0:03:21  loss: 2.6913 (2.4609)  acc1: 33.3333 (38.3838)  acc5: 66.6667 (66.6667)  time: 0.7709  data: 0.5814  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 2.6913 (2.5196)  acc1: 38.8889 (39.1534)  acc5: 66.6667 (67.9894)  time: 0.3574  data: 0.1615  max mem: 15572
Val:  [ 30/272]  eta: 0:01:41  loss: 2.5512 (2.5838)  acc1: 38.8889 (36.2007)  acc5: 66.6667 (67.9211)  time: 0.2271  data: 0.0328  max mem: 15572
Val:  [ 40/272]  eta: 0:01:30  loss: 2.6664 (2.6058)  acc1: 33.3333 (35.5014)  acc5: 72.2222 (68.9702)  time: 0.2406  data: 0.0445  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 2.6664 (2.5471)  acc1: 38.8889 (36.6013)  acc5: 77.7778 (71.0240)  time: 0.3177  data: 0.1265  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 1.6049 (2.4159)  acc1: 61.1111 (40.8015)  acc5: 88.8889 (72.5865)  time: 0.3046  data: 0.1235  max mem: 15572
Val:  [ 70/272]  eta: 0:01:08  loss: 1.5252 (2.3177)  acc1: 66.6667 (44.0532)  acc5: 88.8889 (74.1784)  time: 0.2371  data: 0.0594  max mem: 15572
Val:  [ 80/272]  eta: 0:01:02  loss: 1.9223 (2.3120)  acc1: 55.5556 (44.1015)  acc5: 77.7778 (74.2798)  time: 0.2186  data: 0.0499  max mem: 15572
Val:  [ 90/272]  eta: 0:00:56  loss: 2.2731 (2.3226)  acc1: 50.0000 (44.3834)  acc5: 77.7778 (74.8474)  time: 0.2093  data: 0.0409  max mem: 15572
Val:  [100/272]  eta: 0:00:53  loss: 2.2810 (2.3527)  acc1: 50.0000 (43.8944)  acc5: 77.7778 (74.7525)  time: 0.2436  data: 0.0603  max mem: 15572
Val:  [110/272]  eta: 0:00:49  loss: 2.7069 (2.4220)  acc1: 22.2222 (41.8919)  acc5: 72.2222 (73.4735)  time: 0.2919  data: 0.0914  max mem: 15572
Val:  [120/272]  eta: 0:00:47  loss: 2.9077 (2.4703)  acc1: 22.2222 (40.9550)  acc5: 66.6667 (72.4059)  time: 0.3310  data: 0.1290  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 2.3812 (2.4433)  acc1: 38.8889 (41.8575)  acc5: 72.2222 (72.8159)  time: 0.3589  data: 0.1375  max mem: 15572
Val:  [140/272]  eta: 0:00:42  loss: 2.0794 (2.4357)  acc1: 50.0000 (42.4350)  acc5: 77.7778 (72.7344)  time: 0.3560  data: 0.1253  max mem: 15572
Val:  [150/272]  eta: 0:00:39  loss: 2.5427 (2.4428)  acc1: 38.8889 (41.7954)  acc5: 72.2222 (73.0316)  time: 0.3478  data: 0.1468  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.4938 (2.4342)  acc1: 44.4444 (42.3395)  acc5: 72.2222 (73.2919)  time: 0.3552  data: 0.1639  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.5508 (2.4570)  acc1: 44.4444 (41.6829)  acc5: 72.2222 (72.9370)  time: 0.3805  data: 0.1806  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.5122 (2.4485)  acc1: 33.3333 (41.5285)  acc5: 72.2222 (73.3272)  time: 0.3590  data: 0.1521  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.5329 (2.4987)  acc1: 27.7778 (40.3723)  acc5: 72.2222 (72.0477)  time: 0.3290  data: 0.1235  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6532 (2.5093)  acc1: 27.7778 (40.0498)  acc5: 61.1111 (71.7523)  time: 0.3482  data: 0.1489  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.2498 (2.5078)  acc1: 38.8889 (40.4687)  acc5: 77.7778 (71.7483)  time: 0.3494  data: 0.1392  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.0927 (2.5013)  acc1: 55.5556 (40.7240)  acc5: 77.7778 (71.8200)  time: 0.3658  data: 0.1409  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.9234 (2.4717)  acc1: 61.1111 (41.6546)  acc5: 83.3333 (72.3184)  time: 0.4140  data: 0.1940  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6677 (2.4551)  acc1: 61.1111 (42.0240)  acc5: 83.3333 (72.6372)  time: 0.3725  data: 0.1613  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.5267 (2.4654)  acc1: 33.3333 (41.6556)  acc5: 72.2222 (72.5985)  time: 0.3347  data: 0.1343  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.2685 (2.4053)  acc1: 66.6667 (43.4440)  acc5: 88.8889 (73.3504)  time: 0.3368  data: 0.1416  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.5208 (2.4058)  acc1: 61.1111 (43.1734)  acc5: 88.8889 (73.4112)  time: 0.2709  data: 0.0825  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.5208 (2.4115)  acc1: 61.1111 (43.1497)  acc5: 88.8889 (73.3770)  time: 0.2653  data: 0.0825  max mem: 15572
Val: Total time: 0:01:30 (0.3322 s / it)
* Acc@1 43.150 Acc@5 73.377 loss 2.411
Accuracy of the network on the 4883 val videos: 43.1%
Max accuracy: 43.58%
Epoch: [21]  [   0/2809]  eta: 6:31:16  lr: 0.000027  min_lr: 0.000000  loss: 4.1026 (4.1026)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 8.3575  data: 7.8598  max mem: 15572
[2025-01-16 00:25:29,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=59000, skipped=397, lr=[2.57690559215989e-07, 2.57690559215989e-07, 3.681293703085558e-07, 3.681293703085558e-07, 5.25899100440794e-07, 5.25899100440794e-07, 7.512844292011343e-07, 7.512844292011343e-07, 1.0732634702873348e-06, 1.0732634702873348e-06, 1.533233528981907e-06, 1.533233528981907e-06, 2.1903336128312955e-06, 2.1903336128312955e-06, 3.129048018330423e-06, 3.129048018330423e-06, 4.470068597614889e-06, 4.470068597614889e-06, 6.385812282306986e-06, 6.385812282306986e-06, 9.122588974724265e-06, 9.122588974724265e-06, 1.3032269963891809e-05, 1.3032269963891809e-05, 1.8617528519845444e-05, 1.8617528519845444e-05, 2.659646931406492e-05, 2.659646931406492e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:25:29,954] [INFO] [timer.py:260:stop] epoch=0/micro_step=59000/global_step=59000, RunningAvgSamplesPerSec=28.510563607329466, CurrSamplesPerSec=31.66892740043302, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [  10/2809]  eta: 1:06:15  lr: 0.000027  min_lr: 0.000000  loss: 3.7318 (3.6788)  class_acc: 0.3333 (0.3106)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4205  data: 0.9700  max mem: 15572
Epoch: [21]  [  20/2809]  eta: 0:49:41  lr: 0.000027  min_lr: 0.000000  loss: 3.7318 (3.8356)  class_acc: 0.2917 (0.2976)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7046  data: 0.2552  max mem: 15572
Epoch: [21]  [  30/2809]  eta: 0:40:35  lr: 0.000027  min_lr: 0.000000  loss: 3.6862 (3.7869)  class_acc: 0.2917 (0.3065)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1298  max mem: 15572
Epoch: [21]  [  40/2809]  eta: 0:34:57  lr: 0.000027  min_lr: 0.000000  loss: 3.7215 (3.8008)  class_acc: 0.2917 (0.3089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4301  data: 0.0152  max mem: 15572
Epoch: [21]  [  50/2809]  eta: 0:31:42  lr: 0.000027  min_lr: 0.000000  loss: 3.9734 (3.8643)  class_acc: 0.2500 (0.2925)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4005  data: 0.0004  max mem: 15572
Epoch: [21]  [  60/2809]  eta: 0:29:42  lr: 0.000027  min_lr: 0.000000  loss: 3.9734 (3.8513)  class_acc: 0.2917 (0.2958)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4252  data: 0.0081  max mem: 15572
Epoch: [21]  [  70/2809]  eta: 0:28:56  lr: 0.000027  min_lr: 0.000000  loss: 3.8417 (3.8474)  class_acc: 0.2917 (0.2946)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4916  data: 0.0683  max mem: 15572
Epoch: [21]  [  80/2809]  eta: 0:28:52  lr: 0.000027  min_lr: 0.000000  loss: 3.9599 (3.8553)  class_acc: 0.2500 (0.2953)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1515  max mem: 15572
Epoch: [21]  [  90/2809]  eta: 0:27:55  lr: 0.000027  min_lr: 0.000000  loss: 3.9234 (3.8561)  class_acc: 0.2917 (0.2962)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1106  max mem: 15572
Epoch: [21]  [ 100/2809]  eta: 0:27:35  lr: 0.000027  min_lr: 0.000000  loss: 3.8475 (3.8472)  class_acc: 0.2917 (0.2966)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0918  max mem: 15572
[2025-01-16 00:26:21,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:26:21,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 110/2809]  eta: 0:27:41  lr: 0.000027  min_lr: 0.000000  loss: 3.8004 (3.8349)  class_acc: 0.3333 (0.3029)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1944  max mem: 15572
Epoch: [21]  [ 120/2809]  eta: 0:27:10  lr: 0.000027  min_lr: 0.000000  loss: 3.7280 (3.8242)  class_acc: 0.3333 (0.3054)  loss_scale: 65536.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1652  max mem: 15572
Epoch: [21]  [ 130/2809]  eta: 0:26:59  lr: 0.000027  min_lr: 0.000000  loss: 3.7663 (3.8283)  class_acc: 0.2917 (0.3041)  loss_scale: 65536.0000 (38521.1603)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1053  max mem: 15572
Epoch: [21]  [ 140/2809]  eta: 0:26:32  lr: 0.000027  min_lr: 0.000000  loss: 3.8053 (3.8391)  class_acc: 0.2917 (0.3026)  loss_scale: 65536.0000 (40437.1064)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0934  max mem: 15572
Epoch: [21]  [ 150/2809]  eta: 0:26:35  lr: 0.000026  min_lr: 0.000000  loss: 3.9931 (3.8405)  class_acc: 0.2500 (0.3035)  loss_scale: 65536.0000 (42099.2848)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1397  max mem: 15572
Epoch: [21]  [ 160/2809]  eta: 0:26:30  lr: 0.000026  min_lr: 0.000000  loss: 3.8684 (3.8330)  class_acc: 0.2917 (0.3062)  loss_scale: 65536.0000 (43554.9814)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1985  max mem: 15572
Epoch: [21]  [ 170/2809]  eta: 0:26:18  lr: 0.000026  min_lr: 0.000000  loss: 3.8508 (3.8433)  class_acc: 0.2917 (0.3048)  loss_scale: 65536.0000 (44840.4211)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1552  max mem: 15572
Epoch: [21]  [ 180/2809]  eta: 0:26:00  lr: 0.000026  min_lr: 0.000000  loss: 3.8169 (3.8371)  class_acc: 0.2917 (0.3073)  loss_scale: 65536.0000 (45983.8232)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1111  max mem: 15572
Epoch: [21]  [ 190/2809]  eta: 0:25:43  lr: 0.000026  min_lr: 0.000000  loss: 3.8650 (3.8373)  class_acc: 0.2917 (0.3065)  loss_scale: 65536.0000 (47007.4974)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0788  max mem: 15572
Epoch: [21]  [ 200/2809]  eta: 0:25:42  lr: 0.000026  min_lr: 0.000000  loss: 3.8650 (3.8340)  class_acc: 0.2917 (0.3074)  loss_scale: 65536.0000 (47929.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1255  max mem: 15572
Epoch: [21]  [ 210/2809]  eta: 0:25:39  lr: 0.000026  min_lr: 0.000000  loss: 3.9495 (3.8449)  class_acc: 0.2917 (0.3053)  loss_scale: 65536.0000 (48763.7536)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.1732  max mem: 15572
Epoch: [21]  [ 220/2809]  eta: 0:25:35  lr: 0.000026  min_lr: 0.000000  loss: 4.1080 (3.8505)  class_acc: 0.2083 (0.3028)  loss_scale: 65536.0000 (49522.6787)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1621  max mem: 15572
Epoch: [21]  [ 230/2809]  eta: 0:25:22  lr: 0.000026  min_lr: 0.000000  loss: 3.8998 (3.8518)  class_acc: 0.2500 (0.3027)  loss_scale: 65536.0000 (50215.8961)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1322  max mem: 15572
[2025-01-16 00:27:33,507] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:27:33,508] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:27:36,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59229
[2025-01-16 00:27:36,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:27:36,123] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 240/2809]  eta: 0:25:10  lr: 0.000026  min_lr: 0.000000  loss: 3.8779 (3.8472)  class_acc: 0.2917 (0.3036)  loss_scale: 65536.0000 (51939.3195)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0968  max mem: 15572
Epoch: [21]  [ 250/2809]  eta: 0:25:01  lr: 0.000026  min_lr: 0.000000  loss: 3.7087 (3.8455)  class_acc: 0.3333 (0.3048)  loss_scale: 65536.0000 (52481.0199)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1108  max mem: 15572
Epoch: [21]  [ 260/2809]  eta: 0:25:00  lr: 0.000026  min_lr: 0.000000  loss: 3.6626 (3.8350)  class_acc: 0.3333 (0.3064)  loss_scale: 65536.0000 (52981.2107)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1593  max mem: 15572
Epoch: [21]  [ 270/2809]  eta: 0:25:06  lr: 0.000026  min_lr: 0.000000  loss: 3.6645 (3.8322)  class_acc: 0.3333 (0.3072)  loss_scale: 65536.0000 (53444.4871)  weight_decay: 0.0500 (0.0500)  time: 0.6717  data: 0.2307  max mem: 15572
[2025-01-16 00:27:58,890] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59268
[2025-01-16 00:27:58,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:27:58,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 280/2809]  eta: 0:24:52  lr: 0.000026  min_lr: 0.000000  loss: 3.9437 (3.8362)  class_acc: 0.2917 (0.3072)  loss_scale: 65536.0000 (53641.5658)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1800  max mem: 15572
Epoch: [21]  [ 290/2809]  eta: 0:24:47  lr: 0.000026  min_lr: 0.000000  loss: 3.9157 (3.8357)  class_acc: 0.2917 (0.3071)  loss_scale: 32768.0000 (52924.2612)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1403  max mem: 15572
Epoch: [21]  [ 300/2809]  eta: 0:24:40  lr: 0.000026  min_lr: 0.000000  loss: 3.7195 (3.8348)  class_acc: 0.3333 (0.3079)  loss_scale: 32768.0000 (52254.6179)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1496  max mem: 15572
Epoch: [21]  [ 310/2809]  eta: 0:24:33  lr: 0.000026  min_lr: 0.000000  loss: 3.6170 (3.8262)  class_acc: 0.3333 (0.3099)  loss_scale: 32768.0000 (51628.0386)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1086  max mem: 15572
Epoch: [21]  [ 320/2809]  eta: 0:24:21  lr: 0.000026  min_lr: 0.000000  loss: 3.9155 (3.8351)  class_acc: 0.2917 (0.3082)  loss_scale: 32768.0000 (51040.4984)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0858  max mem: 15572
Epoch: [21]  [ 330/2809]  eta: 0:24:17  lr: 0.000026  min_lr: 0.000000  loss: 4.1241 (3.8409)  class_acc: 0.2500 (0.3063)  loss_scale: 32768.0000 (50488.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1045  max mem: 15572
Epoch: [21]  [ 340/2809]  eta: 0:24:07  lr: 0.000026  min_lr: 0.000000  loss: 4.0925 (3.8414)  class_acc: 0.2500 (0.3056)  loss_scale: 32768.0000 (49968.7977)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1192  max mem: 15572
Epoch: [21]  [ 350/2809]  eta: 0:24:01  lr: 0.000026  min_lr: 0.000000  loss: 4.0194 (3.8464)  class_acc: 0.2500 (0.3035)  loss_scale: 32768.0000 (49478.7464)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1315  max mem: 15572
Epoch: [21]  [ 360/2809]  eta: 0:23:55  lr: 0.000026  min_lr: 0.000000  loss: 4.1376 (3.8570)  class_acc: 0.2083 (0.3012)  loss_scale: 32768.0000 (49015.8449)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1626  max mem: 15572
Epoch: [21]  [ 370/2809]  eta: 0:23:46  lr: 0.000026  min_lr: 0.000000  loss: 4.1376 (3.8620)  class_acc: 0.2083 (0.2999)  loss_scale: 32768.0000 (48577.8976)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1303  max mem: 15572
Epoch: [21]  [ 380/2809]  eta: 0:23:36  lr: 0.000026  min_lr: 0.000000  loss: 3.9611 (3.8615)  class_acc: 0.2500 (0.3006)  loss_scale: 32768.0000 (48162.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0813  max mem: 15572
Epoch: [21]  [ 390/2809]  eta: 0:23:26  lr: 0.000026  min_lr: 0.000000  loss: 3.7204 (3.8551)  class_acc: 0.3333 (0.3018)  loss_scale: 32768.0000 (47769.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0739  max mem: 15572
Epoch: [21]  [ 400/2809]  eta: 0:23:14  lr: 0.000026  min_lr: 0.000000  loss: 3.7340 (3.8607)  class_acc: 0.2917 (0.3007)  loss_scale: 32768.0000 (47395.1122)  weight_decay: 0.0500 (0.0500)  time: 0.4987  data: 0.0687  max mem: 15572
[2025-01-16 00:29:10,177] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:29:10,177] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 410/2809]  eta: 0:23:06  lr: 0.000026  min_lr: 0.000000  loss: 3.9555 (3.8619)  class_acc: 0.2917 (0.3004)  loss_scale: 32768.0000 (47278.4039)  weight_decay: 0.0500 (0.0500)  time: 0.5104  data: 0.0764  max mem: 15572
Epoch: [21]  [ 420/2809]  eta: 0:23:03  lr: 0.000026  min_lr: 0.000000  loss: 3.9555 (3.8642)  class_acc: 0.2500 (0.2993)  loss_scale: 65536.0000 (47712.0760)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1452  max mem: 15572
Epoch: [21]  [ 430/2809]  eta: 0:22:55  lr: 0.000026  min_lr: 0.000000  loss: 3.8333 (3.8615)  class_acc: 0.2500 (0.3000)  loss_scale: 65536.0000 (48125.6241)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1330  max mem: 15572
Epoch: [21]  [ 440/2809]  eta: 0:22:43  lr: 0.000026  min_lr: 0.000000  loss: 3.7729 (3.8616)  class_acc: 0.2917 (0.3001)  loss_scale: 65536.0000 (48520.4172)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0424  max mem: 15572
Epoch: [21]  [ 450/2809]  eta: 0:22:36  lr: 0.000026  min_lr: 0.000000  loss: 3.9051 (3.8620)  class_acc: 0.2917 (0.3008)  loss_scale: 65536.0000 (48897.7029)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0660  max mem: 15572
Epoch: [21]  [ 460/2809]  eta: 0:22:26  lr: 0.000026  min_lr: 0.000000  loss: 3.9670 (3.8634)  class_acc: 0.2917 (0.3003)  loss_scale: 65536.0000 (49258.6204)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0994  max mem: 15572
Epoch: [21]  [ 470/2809]  eta: 0:22:20  lr: 0.000026  min_lr: 0.000000  loss: 3.8397 (3.8551)  class_acc: 0.3333 (0.3024)  loss_scale: 65536.0000 (49604.2123)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0907  max mem: 15572
Epoch: [21]  [ 480/2809]  eta: 0:22:13  lr: 0.000026  min_lr: 0.000000  loss: 3.7113 (3.8539)  class_acc: 0.3333 (0.3034)  loss_scale: 65536.0000 (49935.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1040  max mem: 15572
Epoch: [21]  [ 490/2809]  eta: 0:22:08  lr: 0.000026  min_lr: 0.000000  loss: 3.8110 (3.8528)  class_acc: 0.2917 (0.3035)  loss_scale: 65536.0000 (50253.1650)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1330  max mem: 15572
Epoch: [21]  [ 500/2809]  eta: 0:21:59  lr: 0.000026  min_lr: 0.000000  loss: 3.9336 (3.8563)  class_acc: 0.2917 (0.3033)  loss_scale: 65536.0000 (50558.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1185  max mem: 15572
Epoch: [21]  [ 510/2809]  eta: 0:21:54  lr: 0.000026  min_lr: 0.000000  loss: 3.9264 (3.8549)  class_acc: 0.3333 (0.3045)  loss_scale: 65536.0000 (50851.3190)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.1037  max mem: 15572
Epoch: [21]  [ 520/2809]  eta: 0:21:45  lr: 0.000026  min_lr: 0.000000  loss: 3.8641 (3.8563)  class_acc: 0.3333 (0.3037)  loss_scale: 65536.0000 (51133.1747)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1149  max mem: 15572
Epoch: [21]  [ 530/2809]  eta: 0:21:45  lr: 0.000026  min_lr: 0.000000  loss: 3.9622 (3.8590)  class_acc: 0.3333 (0.3038)  loss_scale: 65536.0000 (51404.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1820  max mem: 15572
[2025-01-16 00:30:22,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:30:22,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:30:23,944] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59529
[2025-01-16 00:30:23,945] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:30:23,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 540/2809]  eta: 0:21:37  lr: 0.000026  min_lr: 0.000000  loss: 3.9622 (3.8585)  class_acc: 0.3333 (0.3042)  loss_scale: 65536.0000 (52150.1811)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1806  max mem: 15572
Epoch: [21]  [ 550/2809]  eta: 0:21:36  lr: 0.000026  min_lr: 0.000000  loss: 3.8930 (3.8579)  class_acc: 0.3333 (0.3048)  loss_scale: 65536.0000 (52393.1180)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1653  max mem: 15572
Epoch: [21]  [ 560/2809]  eta: 0:21:36  lr: 0.000026  min_lr: 0.000000  loss: 3.7282 (3.8556)  class_acc: 0.3333 (0.3056)  loss_scale: 65536.0000 (52627.3939)  weight_decay: 0.0500 (0.0500)  time: 0.6998  data: 0.2551  max mem: 15572
Epoch: [21]  [ 570/2809]  eta: 0:21:29  lr: 0.000026  min_lr: 0.000000  loss: 3.9887 (3.8603)  class_acc: 0.2917 (0.3050)  loss_scale: 65536.0000 (52853.4641)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1866  max mem: 15572
Epoch: [21]  [ 580/2809]  eta: 0:21:20  lr: 0.000026  min_lr: 0.000000  loss: 3.9026 (3.8582)  class_acc: 0.2917 (0.3054)  loss_scale: 65536.0000 (53071.7522)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0979  max mem: 15572
Epoch: [21]  [ 590/2809]  eta: 0:21:12  lr: 0.000026  min_lr: 0.000000  loss: 3.8383 (3.8584)  class_acc: 0.2917 (0.3056)  loss_scale: 65536.0000 (53282.6531)  weight_decay: 0.0500 (0.0500)  time: 0.5127  data: 0.0984  max mem: 15572
Epoch: [21]  [ 600/2809]  eta: 0:21:08  lr: 0.000026  min_lr: 0.000000  loss: 3.9581 (3.8587)  class_acc: 0.2917 (0.3058)  loss_scale: 65536.0000 (53486.5358)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1303  max mem: 15572
Epoch: [21]  [ 610/2809]  eta: 0:21:02  lr: 0.000026  min_lr: 0.000000  loss: 4.0345 (3.8622)  class_acc: 0.2500 (0.3046)  loss_scale: 65536.0000 (53683.7447)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1296  max mem: 15572
Epoch: [21]  [ 620/2809]  eta: 0:20:54  lr: 0.000026  min_lr: 0.000000  loss: 3.8714 (3.8602)  class_acc: 0.2500 (0.3054)  loss_scale: 65536.0000 (53874.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0886  max mem: 15572
Epoch: [21]  [ 630/2809]  eta: 0:20:47  lr: 0.000026  min_lr: 0.000000  loss: 3.7635 (3.8583)  class_acc: 0.3750 (0.3057)  loss_scale: 65536.0000 (54059.4105)  weight_decay: 0.0500 (0.0500)  time: 0.5255  data: 0.0955  max mem: 15572
Epoch: [21]  [ 640/2809]  eta: 0:20:44  lr: 0.000026  min_lr: 0.000000  loss: 3.9785 (3.8611)  class_acc: 0.2917 (0.3053)  loss_scale: 65536.0000 (54238.4524)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1777  max mem: 15572
[2025-01-16 00:31:22,835] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59630
[2025-01-16 00:31:22,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:31:22,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 650/2809]  eta: 0:20:39  lr: 0.000026  min_lr: 0.000000  loss: 3.8818 (3.8573)  class_acc: 0.2917 (0.3063)  loss_scale: 32768.0000 (53908.6452)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1975  max mem: 15572
Epoch: [21]  [ 660/2809]  eta: 0:20:32  lr: 0.000026  min_lr: 0.000000  loss: 3.7404 (3.8566)  class_acc: 0.3333 (0.3063)  loss_scale: 32768.0000 (53588.8169)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1240  max mem: 15572
Epoch: [21]  [ 670/2809]  eta: 0:20:26  lr: 0.000026  min_lr: 0.000000  loss: 3.8743 (3.8559)  class_acc: 0.2500 (0.3056)  loss_scale: 32768.0000 (53278.5216)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0985  max mem: 15572
Epoch: [21]  [ 680/2809]  eta: 0:20:20  lr: 0.000026  min_lr: 0.000000  loss: 3.9239 (3.8582)  class_acc: 0.2500 (0.3051)  loss_scale: 32768.0000 (52977.3392)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1280  max mem: 15572
Epoch: [21]  [ 690/2809]  eta: 0:20:15  lr: 0.000026  min_lr: 0.000000  loss: 3.9726 (3.8577)  class_acc: 0.2083 (0.3044)  loss_scale: 32768.0000 (52684.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1440  max mem: 15572
Epoch: [21]  [ 700/2809]  eta: 0:20:05  lr: 0.000026  min_lr: 0.000000  loss: 3.6517 (3.8529)  class_acc: 0.3333 (0.3057)  loss_scale: 32768.0000 (52400.7532)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0757  max mem: 15572
Epoch: [21]  [ 710/2809]  eta: 0:20:00  lr: 0.000026  min_lr: 0.000000  loss: 3.6517 (3.8519)  class_acc: 0.2917 (0.3050)  loss_scale: 32768.0000 (52124.6245)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0787  max mem: 15572
Epoch: [21]  [ 720/2809]  eta: 0:19:55  lr: 0.000026  min_lr: 0.000000  loss: 3.8489 (3.8518)  class_acc: 0.2500 (0.3044)  loss_scale: 32768.0000 (51856.1553)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1405  max mem: 15572
Epoch: [21]  [ 730/2809]  eta: 0:19:50  lr: 0.000026  min_lr: 0.000000  loss: 3.8645 (3.8518)  class_acc: 0.3333 (0.3047)  loss_scale: 32768.0000 (51595.0315)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1623  max mem: 15572
Epoch: [21]  [ 740/2809]  eta: 0:19:43  lr: 0.000026  min_lr: 0.000000  loss: 3.9173 (3.8517)  class_acc: 0.2917 (0.3048)  loss_scale: 32768.0000 (51340.9555)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1352  max mem: 15572
Epoch: [21]  [ 750/2809]  eta: 0:19:34  lr: 0.000026  min_lr: 0.000000  loss: 3.6133 (3.8478)  class_acc: 0.3333 (0.3058)  loss_scale: 32768.0000 (51093.6458)  weight_decay: 0.0500 (0.0500)  time: 0.4908  data: 0.0513  max mem: 15572
Epoch: [21]  [ 760/2809]  eta: 0:19:29  lr: 0.000026  min_lr: 0.000000  loss: 3.9028 (3.8489)  class_acc: 0.3333 (0.3055)  loss_scale: 32768.0000 (50852.8357)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0921  max mem: 15572
[2025-01-16 00:32:35,254] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:32:35,255] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 770/2809]  eta: 0:19:25  lr: 0.000026  min_lr: 0.000000  loss: 3.9533 (3.8483)  class_acc: 0.2917 (0.3058)  loss_scale: 32768.0000 (50660.7730)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1627  max mem: 15572
[2025-01-16 00:32:39,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 59768
[2025-01-16 00:32:39,350] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:32:39,351] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 780/2809]  eta: 0:19:19  lr: 0.000026  min_lr: 0.000000  loss: 3.8457 (3.8458)  class_acc: 0.2917 (0.3065)  loss_scale: 32768.0000 (50767.3239)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1455  max mem: 15572
Epoch: [21]  [ 790/2809]  eta: 0:19:16  lr: 0.000026  min_lr: 0.000000  loss: 3.6694 (3.8445)  class_acc: 0.2917 (0.3066)  loss_scale: 32768.0000 (50539.7724)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1634  max mem: 15572
Epoch: [21]  [ 800/2809]  eta: 0:19:13  lr: 0.000026  min_lr: 0.000000  loss: 3.6529 (3.8439)  class_acc: 0.2917 (0.3064)  loss_scale: 32768.0000 (50317.9026)  weight_decay: 0.0500 (0.0500)  time: 0.6683  data: 0.2231  max mem: 15572
Epoch: [21]  [ 810/2809]  eta: 0:19:08  lr: 0.000026  min_lr: 0.000000  loss: 3.8541 (3.8448)  class_acc: 0.2917 (0.3062)  loss_scale: 32768.0000 (50101.5043)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.2331  max mem: 15572
Epoch: [21]  [ 820/2809]  eta: 0:19:00  lr: 0.000026  min_lr: 0.000000  loss: 4.0747 (3.8493)  class_acc: 0.2500 (0.3049)  loss_scale: 32768.0000 (49890.3776)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1257  max mem: 15572
Epoch: [21]  [ 830/2809]  eta: 0:18:54  lr: 0.000026  min_lr: 0.000000  loss: 3.9912 (3.8504)  class_acc: 0.2083 (0.3045)  loss_scale: 32768.0000 (49684.3321)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0585  max mem: 15572
Epoch: [21]  [ 840/2809]  eta: 0:18:47  lr: 0.000026  min_lr: 0.000000  loss: 3.8409 (3.8507)  class_acc: 0.2500 (0.3040)  loss_scale: 32768.0000 (49483.1867)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.1031  max mem: 15572
Epoch: [21]  [ 850/2809]  eta: 0:18:40  lr: 0.000026  min_lr: 0.000000  loss: 3.9006 (3.8503)  class_acc: 0.2917 (0.3049)  loss_scale: 32768.0000 (49286.7685)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.1072  max mem: 15572
Epoch: [21]  [ 860/2809]  eta: 0:18:36  lr: 0.000026  min_lr: 0.000000  loss: 3.8477 (3.8502)  class_acc: 0.2917 (0.3047)  loss_scale: 32768.0000 (49094.9129)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1502  max mem: 15572
Epoch: [21]  [ 870/2809]  eta: 0:18:29  lr: 0.000026  min_lr: 0.000000  loss: 3.8225 (3.8503)  class_acc: 0.2917 (0.3050)  loss_scale: 32768.0000 (48907.4627)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1443  max mem: 15572
Epoch: [21]  [ 880/2809]  eta: 0:18:22  lr: 0.000026  min_lr: 0.000000  loss: 3.9617 (3.8524)  class_acc: 0.2917 (0.3046)  loss_scale: 32768.0000 (48724.2679)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0786  max mem: 15572
Epoch: [21]  [ 890/2809]  eta: 0:18:15  lr: 0.000026  min_lr: 0.000000  loss: 3.7946 (3.8503)  class_acc: 0.2500 (0.3046)  loss_scale: 32768.0000 (48545.1852)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0827  max mem: 15572
Epoch: [21]  [ 900/2809]  eta: 0:18:10  lr: 0.000026  min_lr: 0.000000  loss: 3.6856 (3.8477)  class_acc: 0.2917 (0.3053)  loss_scale: 32768.0000 (48370.0777)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0852  max mem: 15572
[2025-01-16 00:33:52,656] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:33:52,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [ 910/2809]  eta: 0:18:01  lr: 0.000026  min_lr: 0.000000  loss: 3.7774 (3.8477)  class_acc: 0.3333 (0.3057)  loss_scale: 32768.0000 (48306.7223)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0391  max mem: 15572
Epoch: [21]  [ 920/2809]  eta: 0:17:54  lr: 0.000026  min_lr: 0.000000  loss: 3.7769 (3.8473)  class_acc: 0.3333 (0.3062)  loss_scale: 65536.0000 (48493.7937)  weight_decay: 0.0500 (0.0500)  time: 0.4635  data: 0.0191  max mem: 15572
Epoch: [21]  [ 930/2809]  eta: 0:17:47  lr: 0.000026  min_lr: 0.000000  loss: 3.7130 (3.8462)  class_acc: 0.3333 (0.3064)  loss_scale: 65536.0000 (48676.8464)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0562  max mem: 15572
Epoch: [21]  [ 940/2809]  eta: 0:17:42  lr: 0.000026  min_lr: 0.000000  loss: 3.7319 (3.8452)  class_acc: 0.2917 (0.3068)  loss_scale: 65536.0000 (48856.0085)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1129  max mem: 15572
Epoch: [21]  [ 950/2809]  eta: 0:17:35  lr: 0.000026  min_lr: 0.000000  loss: 3.6832 (3.8440)  class_acc: 0.2917 (0.3067)  loss_scale: 65536.0000 (49031.4027)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1091  max mem: 15572
Epoch: [21]  [ 960/2809]  eta: 0:17:30  lr: 0.000026  min_lr: 0.000000  loss: 3.6910 (3.8444)  class_acc: 0.2500 (0.3064)  loss_scale: 65536.0000 (49203.1467)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.0986  max mem: 15572
Epoch: [21]  [ 970/2809]  eta: 0:17:24  lr: 0.000026  min_lr: 0.000000  loss: 3.8269 (3.8433)  class_acc: 0.2917 (0.3067)  loss_scale: 65536.0000 (49371.3532)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1418  max mem: 15572
Epoch: [21]  [ 980/2809]  eta: 0:17:19  lr: 0.000026  min_lr: 0.000000  loss: 3.9261 (3.8450)  class_acc: 0.2917 (0.3061)  loss_scale: 65536.0000 (49536.1305)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1758  max mem: 15572
Epoch: [21]  [ 990/2809]  eta: 0:17:14  lr: 0.000026  min_lr: 0.000000  loss: 4.1835 (3.8474)  class_acc: 0.2500 (0.3053)  loss_scale: 65536.0000 (49697.5822)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1811  max mem: 15572
Epoch: [21]  [1000/2809]  eta: 0:17:09  lr: 0.000026  min_lr: 0.000000  loss: 3.9924 (3.8470)  class_acc: 0.2500 (0.3053)  loss_scale: 65536.0000 (49855.8082)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1379  max mem: 15572
[2025-01-16 00:34:49,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=60000, skipped=402, lr=[2.5049251316672856e-07, 2.5049251316672856e-07, 3.5784644738104086e-07, 3.5784644738104086e-07, 5.112092105443442e-07, 5.112092105443442e-07, 7.30298872206206e-07, 7.30298872206206e-07, 1.0432841031517227e-06, 1.0432841031517227e-06, 1.4904058616453183e-06, 1.4904058616453183e-06, 2.1291512309218833e-06, 2.1291512309218833e-06, 3.041644615602691e-06, 3.041644615602691e-06, 4.34520659371813e-06, 4.34520659371813e-06, 6.207437991025901e-06, 6.207437991025901e-06, 8.86776855860843e-06, 8.86776855860843e-06, 1.2668240798012043e-05, 1.2668240798012043e-05, 1.8097486854302922e-05, 1.8097486854302922e-05, 2.5853552649004174e-05, 2.5853552649004174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:34:49,175] [INFO] [timer.py:260:stop] epoch=0/micro_step=60000/global_step=60000, RunningAvgSamplesPerSec=28.520143903122023, CurrSamplesPerSec=31.696669093327355, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [1010/2809]  eta: 0:17:02  lr: 0.000026  min_lr: 0.000000  loss: 4.0506 (3.8490)  class_acc: 0.2083 (0.3047)  loss_scale: 65536.0000 (50010.9041)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0877  max mem: 15572
Epoch: [21]  [1020/2809]  eta: 0:16:56  lr: 0.000026  min_lr: 0.000000  loss: 4.0506 (3.8489)  class_acc: 0.2083 (0.3046)  loss_scale: 65536.0000 (50162.9618)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1052  max mem: 15572
Epoch: [21]  [1030/2809]  eta: 0:16:51  lr: 0.000026  min_lr: 0.000000  loss: 3.9730 (3.8503)  class_acc: 0.3333 (0.3045)  loss_scale: 65536.0000 (50312.0698)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1301  max mem: 15572
[2025-01-16 00:35:03,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:35:03,322] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:35:05,237] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60029
[2025-01-16 00:35:05,237] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:35:05,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [1040/2809]  eta: 0:16:43  lr: 0.000026  min_lr: 0.000000  loss: 3.9730 (3.8514)  class_acc: 0.3333 (0.3044)  loss_scale: 65536.0000 (50710.1326)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0570  max mem: 15572
Epoch: [21]  [1050/2809]  eta: 0:16:38  lr: 0.000026  min_lr: 0.000000  loss: 4.0741 (3.8527)  class_acc: 0.2500 (0.3042)  loss_scale: 65536.0000 (50851.1970)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0589  max mem: 15572
Epoch: [21]  [1060/2809]  eta: 0:16:32  lr: 0.000026  min_lr: 0.000000  loss: 4.0082 (3.8533)  class_acc: 0.2500 (0.3038)  loss_scale: 65536.0000 (50989.6023)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1153  max mem: 15572
Epoch: [21]  [1070/2809]  eta: 0:16:26  lr: 0.000026  min_lr: 0.000000  loss: 3.8908 (3.8540)  class_acc: 0.2500 (0.3040)  loss_scale: 65536.0000 (51125.4230)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1340  max mem: 15572
[2025-01-16 00:35:27,411] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60067
[2025-01-16 00:35:27,411] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:35:27,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1080/2809]  eta: 0:16:21  lr: 0.000026  min_lr: 0.000000  loss: 3.7532 (3.8525)  class_acc: 0.2917 (0.3040)  loss_scale: 65536.0000 (51167.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1554  max mem: 15572
Epoch: [21]  [1090/2809]  eta: 0:16:15  lr: 0.000026  min_lr: 0.000000  loss: 3.7532 (3.8524)  class_acc: 0.2500 (0.3035)  loss_scale: 32768.0000 (50999.1421)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1272  max mem: 15572
Epoch: [21]  [1100/2809]  eta: 0:16:09  lr: 0.000026  min_lr: 0.000000  loss: 3.8925 (3.8527)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (50833.5550)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0907  max mem: 15572
Epoch: [21]  [1110/2809]  eta: 0:16:04  lr: 0.000026  min_lr: 0.000000  loss: 3.8546 (3.8525)  class_acc: 0.2917 (0.3039)  loss_scale: 32768.0000 (50670.9487)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1413  max mem: 15572
Epoch: [21]  [1120/2809]  eta: 0:15:58  lr: 0.000026  min_lr: 0.000000  loss: 3.8882 (3.8531)  class_acc: 0.2500 (0.3037)  loss_scale: 32768.0000 (50511.2435)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1404  max mem: 15572
Epoch: [21]  [1130/2809]  eta: 0:15:51  lr: 0.000026  min_lr: 0.000000  loss: 3.8306 (3.8533)  class_acc: 0.2917 (0.3035)  loss_scale: 32768.0000 (50354.3625)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0655  max mem: 15572
Epoch: [21]  [1140/2809]  eta: 0:15:46  lr: 0.000026  min_lr: 0.000000  loss: 3.8161 (3.8538)  class_acc: 0.2500 (0.3032)  loss_scale: 32768.0000 (50200.2314)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0935  max mem: 15572
Epoch: [21]  [1150/2809]  eta: 0:15:40  lr: 0.000026  min_lr: 0.000000  loss: 3.9634 (3.8550)  class_acc: 0.2500 (0.3033)  loss_scale: 32768.0000 (50048.7785)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1309  max mem: 15572
Epoch: [21]  [1160/2809]  eta: 0:15:34  lr: 0.000026  min_lr: 0.000000  loss: 3.9693 (3.8546)  class_acc: 0.2500 (0.3032)  loss_scale: 32768.0000 (49899.9345)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1171  max mem: 15572
Epoch: [21]  [1170/2809]  eta: 0:15:28  lr: 0.000026  min_lr: 0.000000  loss: 3.8630 (3.8549)  class_acc: 0.2500 (0.3031)  loss_scale: 32768.0000 (49753.6328)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1249  max mem: 15572
Epoch: [21]  [1180/2809]  eta: 0:15:22  lr: 0.000026  min_lr: 0.000000  loss: 3.8630 (3.8547)  class_acc: 0.2917 (0.3031)  loss_scale: 32768.0000 (49609.8086)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1234  max mem: 15572
Epoch: [21]  [1190/2809]  eta: 0:15:16  lr: 0.000026  min_lr: 0.000000  loss: 3.9842 (3.8552)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (49468.3997)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0989  max mem: 15572
Epoch: [21]  [1200/2809]  eta: 0:15:10  lr: 0.000026  min_lr: 0.000000  loss: 3.8876 (3.8556)  class_acc: 0.2500 (0.3027)  loss_scale: 32768.0000 (49329.3455)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1120  max mem: 15572
[2025-01-16 00:36:39,385] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:36:39,385] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1210/2809]  eta: 0:15:05  lr: 0.000026  min_lr: 0.000000  loss: 3.7571 (3.8527)  class_acc: 0.2500 (0.3030)  loss_scale: 32768.0000 (49300.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1444  max mem: 15572
Epoch: [21]  [1220/2809]  eta: 0:14:59  lr: 0.000026  min_lr: 0.000000  loss: 3.8901 (3.8543)  class_acc: 0.2500 (0.3027)  loss_scale: 65536.0000 (49433.7887)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1156  max mem: 15572
Epoch: [21]  [1230/2809]  eta: 0:14:52  lr: 0.000026  min_lr: 0.000000  loss: 3.9696 (3.8559)  class_acc: 0.2500 (0.3024)  loss_scale: 65536.0000 (49564.5946)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0778  max mem: 15572
Epoch: [21]  [1240/2809]  eta: 0:14:47  lr: 0.000026  min_lr: 0.000000  loss: 3.9337 (3.8561)  class_acc: 0.2500 (0.3021)  loss_scale: 65536.0000 (49693.2925)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1282  max mem: 15572
Epoch: [21]  [1250/2809]  eta: 0:14:41  lr: 0.000026  min_lr: 0.000000  loss: 3.8780 (3.8555)  class_acc: 0.2917 (0.3025)  loss_scale: 65536.0000 (49819.9329)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1364  max mem: 15572
Epoch: [21]  [1260/2809]  eta: 0:14:38  lr: 0.000026  min_lr: 0.000000  loss: 3.8408 (3.8558)  class_acc: 0.2917 (0.3023)  loss_scale: 65536.0000 (49944.5646)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1980  max mem: 15572
Epoch: [21]  [1270/2809]  eta: 0:14:32  lr: 0.000026  min_lr: 0.000000  loss: 3.7005 (3.8541)  class_acc: 0.2917 (0.3026)  loss_scale: 65536.0000 (50067.2352)  weight_decay: 0.0500 (0.0500)  time: 0.6649  data: 0.2304  max mem: 15572
Epoch: [21]  [1280/2809]  eta: 0:14:25  lr: 0.000026  min_lr: 0.000000  loss: 3.7005 (3.8539)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (50187.9906)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0873  max mem: 15572
Epoch: [21]  [1290/2809]  eta: 0:14:20  lr: 0.000026  min_lr: 0.000000  loss: 3.9244 (3.8535)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (50306.8753)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0612  max mem: 15572
Epoch: [21]  [1300/2809]  eta: 0:14:13  lr: 0.000026  min_lr: 0.000000  loss: 3.7260 (3.8539)  class_acc: 0.3333 (0.3026)  loss_scale: 65536.0000 (50423.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0724  max mem: 15572
Epoch: [21]  [1310/2809]  eta: 0:14:07  lr: 0.000026  min_lr: 0.000000  loss: 3.9352 (3.8549)  class_acc: 0.2500 (0.3023)  loss_scale: 65536.0000 (50539.2037)  weight_decay: 0.0500 (0.0500)  time: 0.5103  data: 0.0769  max mem: 15572
Epoch: [21]  [1320/2809]  eta: 0:14:02  lr: 0.000026  min_lr: 0.000000  loss: 3.8604 (3.8536)  class_acc: 0.2500 (0.3025)  loss_scale: 65536.0000 (50652.7298)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1771  max mem: 15572
[2025-01-16 00:37:47,142] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60318
[2025-01-16 00:37:47,142] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:37:47,142] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1330/2809]  eta: 0:13:56  lr: 0.000026  min_lr: 0.000000  loss: 3.7380 (3.8527)  class_acc: 0.2917 (0.3027)  loss_scale: 65536.0000 (50715.3118)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1460  max mem: 15572
Epoch: [21]  [1340/2809]  eta: 0:13:51  lr: 0.000026  min_lr: 0.000000  loss: 3.8037 (3.8523)  class_acc: 0.2917 (0.3028)  loss_scale: 32768.0000 (50581.4765)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1094  max mem: 15572
Epoch: [21]  [1350/2809]  eta: 0:13:45  lr: 0.000026  min_lr: 0.000000  loss: 3.8037 (3.8513)  class_acc: 0.2917 (0.3029)  loss_scale: 32768.0000 (50449.6225)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1120  max mem: 15572
Epoch: [21]  [1360/2809]  eta: 0:13:39  lr: 0.000026  min_lr: 0.000000  loss: 3.9296 (3.8518)  class_acc: 0.2500 (0.3028)  loss_scale: 32768.0000 (50319.7061)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0983  max mem: 15572
Epoch: [21]  [1370/2809]  eta: 0:13:32  lr: 0.000026  min_lr: 0.000000  loss: 3.9969 (3.8514)  class_acc: 0.2500 (0.3029)  loss_scale: 32768.0000 (50191.6849)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0867  max mem: 15572
Epoch: [21]  [1380/2809]  eta: 0:13:27  lr: 0.000026  min_lr: 0.000000  loss: 3.9575 (3.8529)  class_acc: 0.2917 (0.3027)  loss_scale: 32768.0000 (50065.5177)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0972  max mem: 15572
Epoch: [21]  [1390/2809]  eta: 0:13:22  lr: 0.000026  min_lr: 0.000000  loss: 3.9575 (3.8528)  class_acc: 0.2500 (0.3026)  loss_scale: 32768.0000 (49941.1646)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1343  max mem: 15572
Epoch: [21]  [1400/2809]  eta: 0:13:16  lr: 0.000026  min_lr: 0.000000  loss: 3.6199 (3.8528)  class_acc: 0.2500 (0.3026)  loss_scale: 32768.0000 (49818.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1003  max mem: 15572
Epoch: [21]  [1410/2809]  eta: 0:13:10  lr: 0.000026  min_lr: 0.000000  loss: 3.8084 (3.8523)  class_acc: 0.2917 (0.3028)  loss_scale: 32768.0000 (49697.7463)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1070  max mem: 15572
Epoch: [21]  [1420/2809]  eta: 0:13:04  lr: 0.000026  min_lr: 0.000000  loss: 3.8174 (3.8527)  class_acc: 0.2500 (0.3026)  loss_scale: 32768.0000 (49578.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1049  max mem: 15572
Epoch: [21]  [1430/2809]  eta: 0:12:59  lr: 0.000026  min_lr: 0.000000  loss: 3.8053 (3.8517)  class_acc: 0.2917 (0.3027)  loss_scale: 32768.0000 (49461.1321)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1259  max mem: 15572
Epoch: [21]  [1440/2809]  eta: 0:12:53  lr: 0.000026  min_lr: 0.000000  loss: 3.8053 (3.8519)  class_acc: 0.2917 (0.3027)  loss_scale: 32768.0000 (49345.2880)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1374  max mem: 15572
Epoch: [21]  [1450/2809]  eta: 0:12:47  lr: 0.000026  min_lr: 0.000000  loss: 4.0883 (3.8529)  class_acc: 0.2917 (0.3025)  loss_scale: 32768.0000 (49231.0407)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0989  max mem: 15572
[2025-01-16 00:38:59,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:38:59,130] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1460/2809]  eta: 0:12:41  lr: 0.000026  min_lr: 0.000000  loss: 4.0696 (3.8524)  class_acc: 0.3333 (0.3027)  loss_scale: 32768.0000 (49185.6427)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1094  max mem: 15572
Epoch: [21]  [1470/2809]  eta: 0:12:36  lr: 0.000026  min_lr: 0.000000  loss: 3.8288 (3.8522)  class_acc: 0.3333 (0.3028)  loss_scale: 65536.0000 (49296.7940)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1754  max mem: 15572
Epoch: [21]  [1480/2809]  eta: 0:12:31  lr: 0.000026  min_lr: 0.000000  loss: 3.9684 (3.8520)  class_acc: 0.2917 (0.3029)  loss_scale: 65536.0000 (49406.4443)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1622  max mem: 15572
Epoch: [21]  [1490/2809]  eta: 0:12:26  lr: 0.000025  min_lr: 0.000000  loss: 3.7904 (3.8521)  class_acc: 0.2917 (0.3028)  loss_scale: 65536.0000 (49514.6237)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1951  max mem: 15572
Epoch: [21]  [1500/2809]  eta: 0:12:21  lr: 0.000025  min_lr: 0.000000  loss: 3.7369 (3.8515)  class_acc: 0.2917 (0.3029)  loss_scale: 65536.0000 (49621.3618)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.1950  max mem: 15572
Epoch: [21]  [1510/2809]  eta: 0:12:15  lr: 0.000025  min_lr: 0.000000  loss: 3.6794 (3.8509)  class_acc: 0.2917 (0.3031)  loss_scale: 65536.0000 (49726.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0878  max mem: 15572
Epoch: [21]  [1520/2809]  eta: 0:12:09  lr: 0.000025  min_lr: 0.000000  loss: 3.8989 (3.8515)  class_acc: 0.2500 (0.3028)  loss_scale: 65536.0000 (49830.6272)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.0810  max mem: 15572
Epoch: [21]  [1530/2809]  eta: 0:12:03  lr: 0.000025  min_lr: 0.000000  loss: 3.9573 (3.8515)  class_acc: 0.2917 (0.3030)  loss_scale: 65536.0000 (49933.2097)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1029  max mem: 15572
Epoch: [21]  [1540/2809]  eta: 0:11:57  lr: 0.000025  min_lr: 0.000000  loss: 4.0214 (3.8535)  class_acc: 0.2917 (0.3025)  loss_scale: 65536.0000 (50034.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.0759  max mem: 15572
Epoch: [21]  [1550/2809]  eta: 0:11:51  lr: 0.000025  min_lr: 0.000000  loss: 4.1622 (3.8548)  class_acc: 0.2500 (0.3022)  loss_scale: 65536.0000 (50134.4062)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0719  max mem: 15572
Epoch: [21]  [1560/2809]  eta: 0:11:46  lr: 0.000025  min_lr: 0.000000  loss: 4.1172 (3.8560)  class_acc: 0.2500 (0.3017)  loss_scale: 65536.0000 (50233.0711)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1119  max mem: 15572
Epoch: [21]  [1570/2809]  eta: 0:11:40  lr: 0.000025  min_lr: 0.000000  loss: 3.9323 (3.8557)  class_acc: 0.2500 (0.3018)  loss_scale: 65536.0000 (50330.4799)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1190  max mem: 15572
[2025-01-16 00:40:07,582] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60566
[2025-01-16 00:40:07,583] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:40:07,584] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1580/2809]  eta: 0:11:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8299 (3.8544)  class_acc: 0.2917 (0.3017)  loss_scale: 65536.0000 (50343.7521)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1209  max mem: 15572
Epoch: [21]  [1590/2809]  eta: 0:11:29  lr: 0.000025  min_lr: 0.000000  loss: 3.6688 (3.8535)  class_acc: 0.2917 (0.3022)  loss_scale: 32768.0000 (50233.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1000  max mem: 15572
Epoch: [21]  [1600/2809]  eta: 0:11:24  lr: 0.000025  min_lr: 0.000000  loss: 3.8558 (3.8544)  class_acc: 0.2500 (0.3018)  loss_scale: 32768.0000 (50124.1924)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1897  max mem: 15572
Epoch: [21]  [1610/2809]  eta: 0:11:17  lr: 0.000025  min_lr: 0.000000  loss: 3.8844 (3.8545)  class_acc: 0.2083 (0.3015)  loss_scale: 32768.0000 (50016.4569)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1490  max mem: 15572
Epoch: [21]  [1620/2809]  eta: 0:11:12  lr: 0.000025  min_lr: 0.000000  loss: 3.8757 (3.8541)  class_acc: 0.2500 (0.3017)  loss_scale: 32768.0000 (49910.0506)  weight_decay: 0.0500 (0.0500)  time: 0.4957  data: 0.0535  max mem: 15572
Epoch: [21]  [1630/2809]  eta: 0:11:06  lr: 0.000025  min_lr: 0.000000  loss: 3.9195 (3.8542)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49804.9491)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0938  max mem: 15572
Epoch: [21]  [1640/2809]  eta: 0:11:00  lr: 0.000025  min_lr: 0.000000  loss: 3.7814 (3.8527)  class_acc: 0.3333 (0.3024)  loss_scale: 32768.0000 (49701.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0954  max mem: 15572
Epoch: [21]  [1650/2809]  eta: 0:10:54  lr: 0.000025  min_lr: 0.000000  loss: 3.8342 (3.8542)  class_acc: 0.2917 (0.3021)  loss_scale: 32768.0000 (49598.5657)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0903  max mem: 15572
Epoch: [21]  [1660/2809]  eta: 0:10:48  lr: 0.000025  min_lr: 0.000000  loss: 3.9925 (3.8546)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49497.2378)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0724  max mem: 15572
Epoch: [21]  [1670/2809]  eta: 0:10:42  lr: 0.000025  min_lr: 0.000000  loss: 4.0179 (3.8559)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (49397.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0828  max mem: 15572
Epoch: [21]  [1680/2809]  eta: 0:10:37  lr: 0.000025  min_lr: 0.000000  loss: 4.0105 (3.8563)  class_acc: 0.2500 (0.3010)  loss_scale: 32768.0000 (49298.1987)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1160  max mem: 15572
Epoch: [21]  [1690/2809]  eta: 0:10:31  lr: 0.000025  min_lr: 0.000000  loss: 4.0060 (3.8570)  class_acc: 0.1667 (0.3007)  loss_scale: 32768.0000 (49200.4447)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1004  max mem: 15572
Epoch: [21]  [1700/2809]  eta: 0:10:25  lr: 0.000025  min_lr: 0.000000  loss: 4.0060 (3.8578)  class_acc: 0.2917 (0.3006)  loss_scale: 32768.0000 (49103.8401)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1134  max mem: 15572
[2025-01-16 00:41:17,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:41:17,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [1710/2809]  eta: 0:10:20  lr: 0.000025  min_lr: 0.000000  loss: 3.8514 (3.8570)  class_acc: 0.2917 (0.3004)  loss_scale: 32768.0000 (49104.1216)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1429  max mem: 15572
Epoch: [21]  [1720/2809]  eta: 0:10:14  lr: 0.000025  min_lr: 0.000000  loss: 3.9745 (3.8578)  class_acc: 0.2083 (0.3001)  loss_scale: 65536.0000 (49199.6002)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1514  max mem: 15572
Epoch: [21]  [1730/2809]  eta: 0:10:09  lr: 0.000025  min_lr: 0.000000  loss: 4.0009 (3.8579)  class_acc: 0.2500 (0.2999)  loss_scale: 65536.0000 (49293.9757)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1363  max mem: 15572
Epoch: [21]  [1740/2809]  eta: 0:10:03  lr: 0.000025  min_lr: 0.000000  loss: 3.9337 (3.8573)  class_acc: 0.2917 (0.3001)  loss_scale: 65536.0000 (49387.2671)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1248  max mem: 15572
Epoch: [21]  [1750/2809]  eta: 0:09:58  lr: 0.000025  min_lr: 0.000000  loss: 3.9110 (3.8587)  class_acc: 0.2500 (0.2998)  loss_scale: 65536.0000 (49479.4929)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1357  max mem: 15572
Epoch: [21]  [1760/2809]  eta: 0:09:52  lr: 0.000025  min_lr: 0.000000  loss: 4.0837 (3.8589)  class_acc: 0.2500 (0.2997)  loss_scale: 65536.0000 (49570.6712)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1292  max mem: 15572
Epoch: [21]  [1770/2809]  eta: 0:09:46  lr: 0.000025  min_lr: 0.000000  loss: 4.0837 (3.8593)  class_acc: 0.2500 (0.2996)  loss_scale: 65536.0000 (49660.8199)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0958  max mem: 15572
Epoch: [21]  [1780/2809]  eta: 0:09:41  lr: 0.000025  min_lr: 0.000000  loss: 3.7352 (3.8581)  class_acc: 0.2917 (0.2999)  loss_scale: 65536.0000 (49749.9562)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0926  max mem: 15572
Epoch: [21]  [1790/2809]  eta: 0:09:35  lr: 0.000025  min_lr: 0.000000  loss: 3.6079 (3.8573)  class_acc: 0.3333 (0.3001)  loss_scale: 65536.0000 (49838.0972)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1221  max mem: 15572
Epoch: [21]  [1800/2809]  eta: 0:09:29  lr: 0.000025  min_lr: 0.000000  loss: 3.6079 (3.8565)  class_acc: 0.3333 (0.3004)  loss_scale: 65536.0000 (49925.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0892  max mem: 15572
Epoch: [21]  [1810/2809]  eta: 0:09:23  lr: 0.000025  min_lr: 0.000000  loss: 3.9494 (3.8575)  class_acc: 0.2500 (0.3002)  loss_scale: 65536.0000 (50011.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0892  max mem: 15572
Epoch: [21]  [1820/2809]  eta: 0:09:17  lr: 0.000025  min_lr: 0.000000  loss: 3.9786 (3.8577)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (50096.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1066  max mem: 15572
Epoch: [21]  [1830/2809]  eta: 0:09:12  lr: 0.000025  min_lr: 0.000000  loss: 3.8060 (3.8580)  class_acc: 0.2917 (0.3003)  loss_scale: 65536.0000 (50181.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1185  max mem: 15572
[2025-01-16 00:42:30,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:42:30,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:42:32,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60828
[2025-01-16 00:42:32,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:42:32,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [1840/2809]  eta: 0:09:06  lr: 0.000025  min_lr: 0.000000  loss: 3.8705 (3.8583)  class_acc: 0.2917 (0.3005)  loss_scale: 65536.0000 (50442.4291)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1519  max mem: 15572
Epoch: [21]  [1850/2809]  eta: 0:09:01  lr: 0.000025  min_lr: 0.000000  loss: 3.9205 (3.8597)  class_acc: 0.2917 (0.3002)  loss_scale: 65536.0000 (50523.9719)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1903  max mem: 15572
Epoch: [21]  [1860/2809]  eta: 0:08:56  lr: 0.000025  min_lr: 0.000000  loss: 3.8910 (3.8589)  class_acc: 0.2500 (0.3003)  loss_scale: 65536.0000 (50604.6384)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.2207  max mem: 15572
Epoch: [21]  [1870/2809]  eta: 0:08:50  lr: 0.000025  min_lr: 0.000000  loss: 3.6169 (3.8569)  class_acc: 0.3333 (0.3006)  loss_scale: 65536.0000 (50684.4425)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.1915  max mem: 15572
Epoch: [21]  [1880/2809]  eta: 0:08:45  lr: 0.000025  min_lr: 0.000000  loss: 3.5270 (3.8568)  class_acc: 0.3750 (0.3008)  loss_scale: 65536.0000 (50763.3982)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1393  max mem: 15572
[2025-01-16 00:42:59,514] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 60872
[2025-01-16 00:42:59,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:42:59,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [1890/2809]  eta: 0:08:39  lr: 0.000025  min_lr: 0.000000  loss: 3.7393 (3.8558)  class_acc: 0.3750 (0.3011)  loss_scale: 65536.0000 (50702.8916)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1021  max mem: 15572
Epoch: [21]  [1900/2809]  eta: 0:08:33  lr: 0.000025  min_lr: 0.000000  loss: 3.7566 (3.8557)  class_acc: 0.3750 (0.3013)  loss_scale: 32768.0000 (50608.5471)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0748  max mem: 15572
Epoch: [21]  [1910/2809]  eta: 0:08:28  lr: 0.000025  min_lr: 0.000000  loss: 3.8291 (3.8557)  class_acc: 0.2917 (0.3013)  loss_scale: 32768.0000 (50515.1900)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1550  max mem: 15572
Epoch: [21]  [1920/2809]  eta: 0:08:22  lr: 0.000025  min_lr: 0.000000  loss: 3.9996 (3.8564)  class_acc: 0.2500 (0.3010)  loss_scale: 32768.0000 (50422.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1162  max mem: 15572
Epoch: [21]  [1930/2809]  eta: 0:08:16  lr: 0.000025  min_lr: 0.000000  loss: 3.8771 (3.8561)  class_acc: 0.2500 (0.3009)  loss_scale: 32768.0000 (50331.3765)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0824  max mem: 15572
Epoch: [21]  [1940/2809]  eta: 0:08:10  lr: 0.000025  min_lr: 0.000000  loss: 3.7768 (3.8561)  class_acc: 0.2500 (0.3009)  loss_scale: 32768.0000 (50240.8903)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1213  max mem: 15572
Epoch: [21]  [1950/2809]  eta: 0:08:04  lr: 0.000025  min_lr: 0.000000  loss: 3.6315 (3.8555)  class_acc: 0.2917 (0.3010)  loss_scale: 32768.0000 (50151.3316)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0598  max mem: 15572
Epoch: [21]  [1960/2809]  eta: 0:07:59  lr: 0.000025  min_lr: 0.000000  loss: 3.7250 (3.8552)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (50062.6864)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0860  max mem: 15572
Epoch: [21]  [1970/2809]  eta: 0:07:53  lr: 0.000025  min_lr: 0.000000  loss: 3.7263 (3.8549)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (49974.9406)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1048  max mem: 15572
Epoch: [21]  [1980/2809]  eta: 0:07:47  lr: 0.000025  min_lr: 0.000000  loss: 3.8680 (3.8553)  class_acc: 0.2500 (0.3008)  loss_scale: 32768.0000 (49888.0808)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0798  max mem: 15572
Epoch: [21]  [1990/2809]  eta: 0:07:41  lr: 0.000025  min_lr: 0.000000  loss: 3.8680 (3.8559)  class_acc: 0.2500 (0.3006)  loss_scale: 32768.0000 (49802.0934)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1028  max mem: 15572
Epoch: [21]  [2000/2809]  eta: 0:07:36  lr: 0.000025  min_lr: 0.000000  loss: 3.7894 (3.8551)  class_acc: 0.3333 (0.3010)  loss_scale: 32768.0000 (49716.9655)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1297  max mem: 15572
[2025-01-16 00:44:09,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=61000, skipped=408, lr=[2.432707986252642e-07, 2.432707986252642e-07, 3.4752971232180603e-07, 3.4752971232180603e-07, 4.964710176025801e-07, 4.964710176025801e-07, 7.092443108608287e-07, 7.092443108608287e-07, 1.0132061583726125e-06, 1.0132061583726125e-06, 1.4474373691037321e-06, 1.4474373691037321e-06, 2.067767670148189e-06, 2.067767670148189e-06, 2.9539538144974133e-06, 2.9539538144974133e-06, 4.2199340207105905e-06, 4.2199340207105905e-06, 6.028477172443701e-06, 6.028477172443701e-06, 8.612110246348144e-06, 8.612110246348144e-06, 1.2303014637640208e-05, 1.2303014637640208e-05, 1.7575735196628867e-05, 1.7575735196628867e-05, 2.5108193138041243e-05, 2.5108193138041243e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:44:09,235] [INFO] [timer.py:260:stop] epoch=0/micro_step=61000/global_step=61000, RunningAvgSamplesPerSec=28.523778553351804, CurrSamplesPerSec=21.83794028063416, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [21]  [2010/2809]  eta: 0:07:30  lr: 0.000025  min_lr: 0.000000  loss: 3.7894 (3.8547)  class_acc: 0.3333 (0.3013)  loss_scale: 32768.0000 (49632.6842)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1034  max mem: 15572
[2025-01-16 00:44:10,023] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:44:10,023] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2020/2809]  eta: 0:07:25  lr: 0.000025  min_lr: 0.000000  loss: 3.9027 (3.8551)  class_acc: 0.2917 (0.3011)  loss_scale: 32768.0000 (49695.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1246  max mem: 15572
Epoch: [21]  [2030/2809]  eta: 0:07:19  lr: 0.000025  min_lr: 0.000000  loss: 3.9027 (3.8549)  class_acc: 0.2500 (0.3013)  loss_scale: 65536.0000 (49773.1561)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1767  max mem: 15572
Epoch: [21]  [2040/2809]  eta: 0:07:13  lr: 0.000025  min_lr: 0.000000  loss: 3.8045 (3.8549)  class_acc: 0.2917 (0.3013)  loss_scale: 65536.0000 (49850.3871)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1178  max mem: 15572
Epoch: [21]  [2050/2809]  eta: 0:07:08  lr: 0.000025  min_lr: 0.000000  loss: 4.0073 (3.8564)  class_acc: 0.2500 (0.3011)  loss_scale: 65536.0000 (49926.8649)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1090  max mem: 15572
[2025-01-16 00:44:38,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61049
[2025-01-16 00:44:38,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:44:38,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2060/2809]  eta: 0:07:02  lr: 0.000025  min_lr: 0.000000  loss: 3.9912 (3.8553)  class_acc: 0.2917 (0.3016)  loss_scale: 65536.0000 (49986.7016)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1613  max mem: 15572
Epoch: [21]  [2070/2809]  eta: 0:06:56  lr: 0.000025  min_lr: 0.000000  loss: 3.9057 (3.8558)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (49903.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1011  max mem: 15572
Epoch: [21]  [2080/2809]  eta: 0:06:51  lr: 0.000025  min_lr: 0.000000  loss: 3.9057 (3.8550)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (49821.2167)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0948  max mem: 15572
Epoch: [21]  [2090/2809]  eta: 0:06:45  lr: 0.000025  min_lr: 0.000000  loss: 3.7335 (3.8541)  class_acc: 0.3333 (0.3017)  loss_scale: 32768.0000 (49739.6614)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1492  max mem: 15572
Epoch: [21]  [2100/2809]  eta: 0:06:40  lr: 0.000025  min_lr: 0.000000  loss: 3.7689 (3.8540)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49658.8824)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1031  max mem: 15572
Epoch: [21]  [2110/2809]  eta: 0:06:34  lr: 0.000025  min_lr: 0.000000  loss: 3.7103 (3.8537)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49578.8688)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1174  max mem: 15572
Epoch: [21]  [2120/2809]  eta: 0:06:28  lr: 0.000025  min_lr: 0.000000  loss: 3.8058 (3.8541)  class_acc: 0.2917 (0.3016)  loss_scale: 32768.0000 (49499.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1421  max mem: 15572
Epoch: [21]  [2130/2809]  eta: 0:06:22  lr: 0.000025  min_lr: 0.000000  loss: 3.9419 (3.8541)  class_acc: 0.2917 (0.3015)  loss_scale: 32768.0000 (49421.0943)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0902  max mem: 15572
Epoch: [21]  [2140/2809]  eta: 0:06:17  lr: 0.000025  min_lr: 0.000000  loss: 3.9306 (3.8537)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (49343.3125)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1071  max mem: 15572
Epoch: [21]  [2150/2809]  eta: 0:06:11  lr: 0.000025  min_lr: 0.000000  loss: 3.9306 (3.8532)  class_acc: 0.2917 (0.3017)  loss_scale: 32768.0000 (49266.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1024  max mem: 15572
Epoch: [21]  [2160/2809]  eta: 0:06:06  lr: 0.000025  min_lr: 0.000000  loss: 3.9081 (3.8531)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (49189.9084)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1375  max mem: 15572
Epoch: [21]  [2170/2809]  eta: 0:06:00  lr: 0.000025  min_lr: 0.000000  loss: 4.0021 (3.8541)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (49114.2662)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1647  max mem: 15572
Epoch: [21]  [2180/2809]  eta: 0:05:54  lr: 0.000025  min_lr: 0.000000  loss: 3.8369 (3.8523)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49039.3177)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1332  max mem: 15572
[2025-01-16 00:45:50,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:45:50,980] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2190/2809]  eta: 0:05:49  lr: 0.000025  min_lr: 0.000000  loss: 3.6681 (3.8531)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (48994.9649)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1301  max mem: 15572
Epoch: [21]  [2200/2809]  eta: 0:05:43  lr: 0.000025  min_lr: 0.000000  loss: 3.8343 (3.8529)  class_acc: 0.2917 (0.3015)  loss_scale: 65536.0000 (49070.1172)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1302  max mem: 15572
Epoch: [21]  [2210/2809]  eta: 0:05:38  lr: 0.000025  min_lr: 0.000000  loss: 3.8210 (3.8528)  class_acc: 0.3333 (0.3016)  loss_scale: 65536.0000 (49144.5898)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1517  max mem: 15572
Epoch: [21]  [2220/2809]  eta: 0:05:32  lr: 0.000025  min_lr: 0.000000  loss: 3.7977 (3.8528)  class_acc: 0.2917 (0.3017)  loss_scale: 65536.0000 (49218.3917)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1238  max mem: 15572
Epoch: [21]  [2230/2809]  eta: 0:05:26  lr: 0.000025  min_lr: 0.000000  loss: 3.9748 (3.8536)  class_acc: 0.2500 (0.3014)  loss_scale: 65536.0000 (49291.5320)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1103  max mem: 15572
Epoch: [21]  [2240/2809]  eta: 0:05:20  lr: 0.000025  min_lr: 0.000000  loss: 4.0075 (3.8537)  class_acc: 0.3333 (0.3015)  loss_scale: 65536.0000 (49364.0196)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0907  max mem: 15572
Epoch: [21]  [2250/2809]  eta: 0:05:15  lr: 0.000025  min_lr: 0.000000  loss: 3.9213 (3.8543)  class_acc: 0.3333 (0.3015)  loss_scale: 65536.0000 (49435.8632)  weight_decay: 0.0500 (0.0500)  time: 0.4775  data: 0.0405  max mem: 15572
Epoch: [21]  [2260/2809]  eta: 0:05:09  lr: 0.000025  min_lr: 0.000000  loss: 3.9213 (3.8543)  class_acc: 0.2500 (0.3013)  loss_scale: 65536.0000 (49507.0712)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0763  max mem: 15572
Epoch: [21]  [2270/2809]  eta: 0:05:03  lr: 0.000025  min_lr: 0.000000  loss: 3.9762 (3.8550)  class_acc: 0.2500 (0.3012)  loss_scale: 65536.0000 (49577.6521)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1475  max mem: 15572
Epoch: [21]  [2280/2809]  eta: 0:04:58  lr: 0.000025  min_lr: 0.000000  loss: 3.8498 (3.8545)  class_acc: 0.2917 (0.3012)  loss_scale: 65536.0000 (49647.6142)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1042  max mem: 15572
Epoch: [21]  [2290/2809]  eta: 0:04:52  lr: 0.000025  min_lr: 0.000000  loss: 3.9075 (3.8555)  class_acc: 0.2500 (0.3010)  loss_scale: 65536.0000 (49716.9655)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1347  max mem: 15572
Epoch: [21]  [2300/2809]  eta: 0:04:47  lr: 0.000025  min_lr: 0.000000  loss: 3.8743 (3.8543)  class_acc: 0.2500 (0.3011)  loss_scale: 65536.0000 (49785.7140)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1601  max mem: 15572
Epoch: [21]  [2310/2809]  eta: 0:04:41  lr: 0.000025  min_lr: 0.000000  loss: 3.8380 (3.8544)  class_acc: 0.2917 (0.3011)  loss_scale: 65536.0000 (49853.8676)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0846  max mem: 15572
[2025-01-16 00:47:02,750] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:47:02,751] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [2320/2809]  eta: 0:04:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8177 (3.8542)  class_acc: 0.2917 (0.3013)  loss_scale: 65536.0000 (50034.3783)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.1263  max mem: 15572
[2025-01-16 00:47:04,458] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61310
[2025-01-16 00:47:04,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:47:04,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2330/2809]  eta: 0:04:30  lr: 0.000025  min_lr: 0.000000  loss: 3.7938 (3.8541)  class_acc: 0.2917 (0.3014)  loss_scale: 65536.0000 (50100.8803)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1707  max mem: 15572
Epoch: [21]  [2340/2809]  eta: 0:04:24  lr: 0.000025  min_lr: 0.000000  loss: 3.8702 (3.8541)  class_acc: 0.2500 (0.3014)  loss_scale: 65536.0000 (50166.8142)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1580  max mem: 15572
[2025-01-16 00:47:17,195] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61332
[2025-01-16 00:47:17,195] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:47:17,195] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2350/2809]  eta: 0:04:19  lr: 0.000025  min_lr: 0.000000  loss: 3.8451 (3.8536)  class_acc: 0.2500 (0.3015)  loss_scale: 65536.0000 (50120.6840)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1610  max mem: 15572
Epoch: [21]  [2360/2809]  eta: 0:04:13  lr: 0.000025  min_lr: 0.000000  loss: 3.8451 (3.8541)  class_acc: 0.2083 (0.3012)  loss_scale: 32768.0000 (50047.1868)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.1794  max mem: 15572
Epoch: [21]  [2370/2809]  eta: 0:04:07  lr: 0.000025  min_lr: 0.000000  loss: 3.8299 (3.8535)  class_acc: 0.2500 (0.3012)  loss_scale: 32768.0000 (49974.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1430  max mem: 15572
Epoch: [21]  [2380/2809]  eta: 0:04:02  lr: 0.000025  min_lr: 0.000000  loss: 3.8853 (3.8538)  class_acc: 0.2917 (0.3012)  loss_scale: 32768.0000 (49902.0445)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1110  max mem: 15572
Epoch: [21]  [2390/2809]  eta: 0:03:56  lr: 0.000025  min_lr: 0.000000  loss: 3.8853 (3.8530)  class_acc: 0.3333 (0.3013)  loss_scale: 32768.0000 (49830.3839)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1295  max mem: 15572
Epoch: [21]  [2400/2809]  eta: 0:03:50  lr: 0.000025  min_lr: 0.000000  loss: 3.8036 (3.8534)  class_acc: 0.3333 (0.3015)  loss_scale: 32768.0000 (49759.3203)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1661  max mem: 15572
Epoch: [21]  [2410/2809]  eta: 0:03:45  lr: 0.000025  min_lr: 0.000000  loss: 3.8056 (3.8532)  class_acc: 0.3333 (0.3014)  loss_scale: 32768.0000 (49688.8461)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1641  max mem: 15572
Epoch: [21]  [2420/2809]  eta: 0:03:39  lr: 0.000025  min_lr: 0.000000  loss: 3.8041 (3.8533)  class_acc: 0.2917 (0.3014)  loss_scale: 32768.0000 (49618.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1304  max mem: 15572
Epoch: [21]  [2430/2809]  eta: 0:03:34  lr: 0.000025  min_lr: 0.000000  loss: 3.6624 (3.8523)  class_acc: 0.3333 (0.3018)  loss_scale: 32768.0000 (49549.6372)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1435  max mem: 15572
Epoch: [21]  [2440/2809]  eta: 0:03:28  lr: 0.000025  min_lr: 0.000000  loss: 3.7389 (3.8529)  class_acc: 0.3333 (0.3016)  loss_scale: 32768.0000 (49480.8882)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1608  max mem: 15572
Epoch: [21]  [2450/2809]  eta: 0:03:23  lr: 0.000025  min_lr: 0.000000  loss: 4.0025 (3.8536)  class_acc: 0.2500 (0.3015)  loss_scale: 32768.0000 (49412.7001)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1945  max mem: 15572
Epoch: [21]  [2460/2809]  eta: 0:03:17  lr: 0.000025  min_lr: 0.000000  loss: 3.9887 (3.8532)  class_acc: 0.2500 (0.3017)  loss_scale: 32768.0000 (49345.0662)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1617  max mem: 15572
Epoch: [21]  [2470/2809]  eta: 0:03:11  lr: 0.000025  min_lr: 0.000000  loss: 3.8292 (3.8530)  class_acc: 0.2917 (0.3018)  loss_scale: 32768.0000 (49277.9798)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0910  max mem: 15572
[2025-01-16 00:48:32,999] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:48:32,999] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [2480/2809]  eta: 0:03:06  lr: 0.000025  min_lr: 0.000000  loss: 3.6982 (3.8520)  class_acc: 0.2917 (0.3019)  loss_scale: 32768.0000 (49330.3023)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1247  max mem: 15572
Epoch: [21]  [2490/2809]  eta: 0:03:00  lr: 0.000025  min_lr: 0.000000  loss: 3.7695 (3.8523)  class_acc: 0.2917 (0.3018)  loss_scale: 65536.0000 (49395.3593)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.1811  max mem: 15572
Epoch: [21]  [2500/2809]  eta: 0:02:54  lr: 0.000025  min_lr: 0.000000  loss: 3.8843 (3.8517)  class_acc: 0.2917 (0.3018)  loss_scale: 65536.0000 (49459.8960)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.1587  max mem: 15572
Epoch: [21]  [2510/2809]  eta: 0:02:49  lr: 0.000025  min_lr: 0.000000  loss: 3.7289 (3.8509)  class_acc: 0.2917 (0.3019)  loss_scale: 65536.0000 (49523.9188)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1375  max mem: 15572
Epoch: [21]  [2520/2809]  eta: 0:02:43  lr: 0.000025  min_lr: 0.000000  loss: 3.6221 (3.8499)  class_acc: 0.3333 (0.3021)  loss_scale: 65536.0000 (49587.4336)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1211  max mem: 15572
Epoch: [21]  [2530/2809]  eta: 0:02:37  lr: 0.000025  min_lr: 0.000000  loss: 3.8137 (3.8503)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (49650.4465)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1167  max mem: 15572
Epoch: [21]  [2540/2809]  eta: 0:02:32  lr: 0.000025  min_lr: 0.000000  loss: 3.9302 (3.8504)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (49712.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1238  max mem: 15572
Epoch: [21]  [2550/2809]  eta: 0:02:26  lr: 0.000025  min_lr: 0.000000  loss: 3.7998 (3.8505)  class_acc: 0.3333 (0.3022)  loss_scale: 65536.0000 (49774.9902)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0914  max mem: 15572
Epoch: [21]  [2560/2809]  eta: 0:02:20  lr: 0.000025  min_lr: 0.000000  loss: 3.9837 (3.8513)  class_acc: 0.2917 (0.3019)  loss_scale: 65536.0000 (49836.5326)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1347  max mem: 15572
Epoch: [21]  [2570/2809]  eta: 0:02:15  lr: 0.000025  min_lr: 0.000000  loss: 3.9467 (3.8509)  class_acc: 0.2500 (0.3020)  loss_scale: 65536.0000 (49897.5963)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.1750  max mem: 15572
Epoch: [21]  [2580/2809]  eta: 0:02:09  lr: 0.000025  min_lr: 0.000000  loss: 3.9467 (3.8516)  class_acc: 0.2917 (0.3019)  loss_scale: 65536.0000 (49958.1867)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0821  max mem: 15572
Epoch: [21]  [2590/2809]  eta: 0:02:03  lr: 0.000025  min_lr: 0.000000  loss: 4.0876 (3.8517)  class_acc: 0.2500 (0.3019)  loss_scale: 65536.0000 (50018.3095)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0598  max mem: 15572
[2025-01-16 00:49:46,923] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:49:46,923] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [2600/2809]  eta: 0:01:58  lr: 0.000025  min_lr: 0.000000  loss: 3.8147 (3.8516)  class_acc: 0.2500 (0.3019)  loss_scale: 65536.0000 (50103.1665)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0959  max mem: 15572
[2025-01-16 00:49:47,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61591
[2025-01-16 00:49:47,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:49:47,951] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2610/2809]  eta: 0:01:52  lr: 0.000025  min_lr: 0.000000  loss: 3.7923 (3.8512)  class_acc: 0.2500 (0.3019)  loss_scale: 65536.0000 (50187.3734)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1098  max mem: 15572
Epoch: [21]  [2620/2809]  eta: 0:01:46  lr: 0.000025  min_lr: 0.000000  loss: 3.6939 (3.8511)  class_acc: 0.3333 (0.3021)  loss_scale: 65536.0000 (50245.9336)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1083  max mem: 15572
Epoch: [21]  [2630/2809]  eta: 0:01:41  lr: 0.000025  min_lr: 0.000000  loss: 3.8013 (3.8510)  class_acc: 0.3333 (0.3022)  loss_scale: 65536.0000 (50304.0487)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0884  max mem: 15572
Epoch: [21]  [2640/2809]  eta: 0:01:35  lr: 0.000025  min_lr: 0.000000  loss: 3.8161 (3.8513)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (50361.7236)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1306  max mem: 15572
Epoch: [21]  [2650/2809]  eta: 0:01:29  lr: 0.000025  min_lr: 0.000000  loss: 3.9363 (3.8516)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (50418.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1293  max mem: 15572
Epoch: [21]  [2660/2809]  eta: 0:01:24  lr: 0.000025  min_lr: 0.000000  loss: 3.9363 (3.8517)  class_acc: 0.2917 (0.3021)  loss_scale: 65536.0000 (50475.7730)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1320  max mem: 15572
Epoch: [21]  [2670/2809]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000000  loss: 4.0814 (3.8522)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (50532.1572)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1266  max mem: 15572
Epoch: [21]  [2680/2809]  eta: 0:01:12  lr: 0.000025  min_lr: 0.000000  loss: 4.1059 (3.8524)  class_acc: 0.2917 (0.3020)  loss_scale: 65536.0000 (50588.1209)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0938  max mem: 15572
Epoch: [21]  [2690/2809]  eta: 0:01:07  lr: 0.000025  min_lr: 0.000000  loss: 3.9475 (3.8522)  class_acc: 0.2500 (0.3017)  loss_scale: 65536.0000 (50643.6685)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0886  max mem: 15572
Epoch: [21]  [2700/2809]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000000  loss: 3.6626 (3.8510)  class_acc: 0.3333 (0.3022)  loss_scale: 65536.0000 (50698.8049)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0908  max mem: 15572
Epoch: [21]  [2710/2809]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000000  loss: 3.7902 (3.8516)  class_acc: 0.3750 (0.3022)  loss_scale: 65536.0000 (50753.5345)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1245  max mem: 15572
Epoch: [21]  [2720/2809]  eta: 0:00:50  lr: 0.000025  min_lr: 0.000000  loss: 3.9417 (3.8515)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (50807.8618)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.0852  max mem: 15572
Epoch: [21]  [2730/2809]  eta: 0:00:44  lr: 0.000025  min_lr: 0.000000  loss: 3.7299 (3.8510)  class_acc: 0.2917 (0.3024)  loss_scale: 65536.0000 (50861.7913)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0108  max mem: 15572
[2025-01-16 00:50:59,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:50:59,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:51:00,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61722
[2025-01-16 00:51:00,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:51:00,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [2740/2809]  eta: 0:00:38  lr: 0.000025  min_lr: 0.000000  loss: 3.7994 (3.8513)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (50963.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0384  max mem: 15572
Epoch: [21]  [2750/2809]  eta: 0:00:33  lr: 0.000025  min_lr: 0.000000  loss: 3.8936 (3.8511)  class_acc: 0.2500 (0.3023)  loss_scale: 65536.0000 (51016.1192)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1083  max mem: 15572
Epoch: [21]  [2760/2809]  eta: 0:00:27  lr: 0.000025  min_lr: 0.000000  loss: 3.8995 (3.8517)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (51068.7084)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1178  max mem: 15572
Epoch: [21]  [2770/2809]  eta: 0:00:22  lr: 0.000025  min_lr: 0.000000  loss: 3.8852 (3.8517)  class_acc: 0.2917 (0.3022)  loss_scale: 65536.0000 (51120.9181)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1735  max mem: 15572
[2025-01-16 00:51:24,455] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 61764
[2025-01-16 00:51:24,456] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:51:24,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [2780/2809]  eta: 0:00:16  lr: 0.000025  min_lr: 0.000000  loss: 3.6247 (3.8502)  class_acc: 0.3333 (0.3025)  loss_scale: 65536.0000 (51102.0554)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.1665  max mem: 15572
Epoch: [21]  [2790/2809]  eta: 0:00:10  lr: 0.000025  min_lr: 0.000000  loss: 3.6515 (3.8500)  class_acc: 0.3333 (0.3025)  loss_scale: 32768.0000 (51036.3655)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0750  max mem: 15572
Epoch: [21]  [2800/2809]  eta: 0:00:05  lr: 0.000025  min_lr: 0.000000  loss: 3.8619 (3.8505)  class_acc: 0.2500 (0.3023)  loss_scale: 32768.0000 (50971.1446)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0797  max mem: 15572
Epoch: [21]  [2808/2809]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000000  loss: 3.9194 (3.8505)  class_acc: 0.2500 (0.3023)  loss_scale: 32768.0000 (50919.3022)  weight_decay: 0.0500 (0.0500)  time: 0.4883  data: 0.0796  max mem: 15572
Epoch: [21] Total time: 0:26:26 (0.5649 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000000  loss: 3.9194 (3.8505)  class_acc: 0.2500 (0.3023)  loss_scale: 32768.0000 (50919.3022)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:57  loss: 0.3532 (0.3532)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.6218  data: 4.4257  max mem: 15572
Val:  [ 10/272]  eta: 0:03:09  loss: 2.6252 (2.4696)  acc1: 38.8889 (39.3939)  acc5: 66.6667 (67.1717)  time: 0.7228  data: 0.5282  max mem: 15572
Val:  [ 20/272]  eta: 0:02:08  loss: 2.5807 (2.4863)  acc1: 44.4444 (43.9153)  acc5: 66.6667 (69.8413)  time: 0.3033  data: 0.1022  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 2.4462 (2.5433)  acc1: 44.4444 (40.5018)  acc5: 72.2222 (69.5341)  time: 0.2884  data: 0.0881  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 2.5061 (2.5681)  acc1: 27.7778 (38.0759)  acc5: 72.2222 (70.5962)  time: 0.3325  data: 0.1365  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.4784 (2.5032)  acc1: 33.3333 (39.3246)  acc5: 77.7778 (72.7669)  time: 0.3626  data: 0.1638  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 1.7083 (2.3799)  acc1: 61.1111 (42.9872)  acc5: 88.8889 (74.1348)  time: 0.3364  data: 0.1383  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.5280 (2.2915)  acc1: 66.6667 (45.5399)  acc5: 83.3333 (75.3521)  time: 0.2793  data: 0.0768  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.8714 (2.3086)  acc1: 55.5556 (45.1989)  acc5: 77.7778 (74.6228)  time: 0.2718  data: 0.0696  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.3472 (2.3332)  acc1: 44.4444 (44.6276)  acc5: 77.7778 (74.6032)  time: 0.2776  data: 0.0851  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.3373 (2.3605)  acc1: 44.4444 (44.0044)  acc5: 77.7778 (74.3124)  time: 0.2625  data: 0.0846  max mem: 15572
Val:  [110/272]  eta: 0:00:53  loss: 2.5252 (2.4306)  acc1: 33.3333 (42.4925)  acc5: 77.7778 (73.3734)  time: 0.2274  data: 0.0610  max mem: 15572
Val:  [120/272]  eta: 0:00:48  loss: 3.0712 (2.4689)  acc1: 27.7778 (41.7815)  acc5: 66.6667 (72.6814)  time: 0.1824  data: 0.0131  max mem: 15572
Val:  [130/272]  eta: 0:00:44  loss: 2.3934 (2.4255)  acc1: 50.0000 (43.0450)  acc5: 77.7778 (73.5793)  time: 0.2007  data: 0.0007  max mem: 15572
Val:  [140/272]  eta: 0:00:40  loss: 1.6675 (2.4147)  acc1: 55.5556 (43.6170)  acc5: 83.3333 (73.6407)  time: 0.2670  data: 0.0545  max mem: 15572
Val:  [150/272]  eta: 0:00:37  loss: 2.3815 (2.4180)  acc1: 38.8889 (42.9728)  acc5: 77.7778 (74.1354)  time: 0.3156  data: 0.1130  max mem: 15572
Val:  [160/272]  eta: 0:00:34  loss: 2.3815 (2.4055)  acc1: 38.8889 (43.6163)  acc5: 77.7778 (74.4997)  time: 0.3177  data: 0.0985  max mem: 15572
Val:  [170/272]  eta: 0:00:31  loss: 2.4770 (2.4269)  acc1: 44.4444 (42.9175)  acc5: 72.2222 (74.0741)  time: 0.3199  data: 0.1066  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.3909 (2.4139)  acc1: 33.3333 (42.8484)  acc5: 72.2222 (74.5242)  time: 0.3620  data: 0.1577  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.4558 (2.4662)  acc1: 27.7778 (41.4194)  acc5: 72.2222 (73.0948)  time: 0.3689  data: 0.1531  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.6424 (2.4715)  acc1: 27.7778 (41.3212)  acc5: 61.1111 (72.9685)  time: 0.3205  data: 0.1036  max mem: 15572
Val:  [210/272]  eta: 0:00:19  loss: 2.0368 (2.4702)  acc1: 50.0000 (41.8641)  acc5: 77.7778 (72.8541)  time: 0.3376  data: 0.1240  max mem: 15572
Val:  [220/272]  eta: 0:00:16  loss: 2.2781 (2.4588)  acc1: 55.5556 (42.3580)  acc5: 77.7778 (73.0266)  time: 0.3907  data: 0.1935  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.8830 (2.4280)  acc1: 61.1111 (43.4584)  acc5: 77.7778 (73.4247)  time: 0.3778  data: 0.1945  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7982 (2.4146)  acc1: 55.5556 (43.5915)  acc5: 83.3333 (73.7437)  time: 0.3460  data: 0.1573  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.4471 (2.4274)  acc1: 33.3333 (42.8287)  acc5: 77.7778 (73.6830)  time: 0.3339  data: 0.1338  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.3111 (2.3665)  acc1: 72.2222 (44.6147)  acc5: 88.8889 (74.4785)  time: 0.3454  data: 0.1531  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4049 (2.3622)  acc1: 61.1111 (44.3829)  acc5: 88.8889 (74.7847)  time: 0.3385  data: 0.1682  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4049 (2.3669)  acc1: 61.1111 (44.3580)  acc5: 88.8889 (74.7696)  time: 0.2694  data: 0.1050  max mem: 15572
Val: Total time: 0:01:29 (0.3273 s / it)
* Acc@1 44.358 Acc@5 74.770 loss 2.367
Accuracy of the network on the 4883 val videos: 44.4%
[2025-01-16 00:53:10,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 00:53:10,154] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 00:53:10,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 00:53:12,467] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 00:53:12,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.36%
Epoch: [22]  [   0/2809]  eta: 9:03:12  lr: 0.000025  min_lr: 0.000000  loss: 3.7619 (3.7619)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 11.6029  data: 11.0244  max mem: 15572
Epoch: [22]  [  10/2809]  eta: 1:14:20  lr: 0.000025  min_lr: 0.000000  loss: 3.8713 (3.8797)  class_acc: 0.2917 (0.2841)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5937  data: 1.1270  max mem: 15572
Epoch: [22]  [  20/2809]  eta: 0:52:19  lr: 0.000024  min_lr: 0.000000  loss: 3.7155 (3.7953)  class_acc: 0.3333 (0.3175)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1648  max mem: 15572
Epoch: [22]  [  30/2809]  eta: 0:45:35  lr: 0.000024  min_lr: 0.000000  loss: 3.7155 (3.7707)  class_acc: 0.3333 (0.3266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.1933  max mem: 15572
Epoch: [22]  [  40/2809]  eta: 0:39:01  lr: 0.000024  min_lr: 0.000000  loss: 3.7562 (3.7498)  class_acc: 0.2917 (0.3384)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0974  max mem: 15572
Epoch: [22]  [  50/2809]  eta: 0:34:46  lr: 0.000024  min_lr: 0.000000  loss: 3.7457 (3.7271)  class_acc: 0.3750 (0.3513)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4031  data: 0.0006  max mem: 15572
Epoch: [22]  [  60/2809]  eta: 0:32:12  lr: 0.000024  min_lr: 0.000000  loss: 3.7400 (3.7388)  class_acc: 0.3333 (0.3429)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4105  data: 0.0006  max mem: 15572
Epoch: [22]  [  70/2809]  eta: 0:30:26  lr: 0.000024  min_lr: 0.000000  loss: 3.9143 (3.7892)  class_acc: 0.2500 (0.3263)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4387  data: 0.0006  max mem: 15572
Epoch: [22]  [  80/2809]  eta: 0:29:07  lr: 0.000024  min_lr: 0.000000  loss: 4.0212 (3.7949)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4491  data: 0.0007  max mem: 15572
Epoch: [22]  [  90/2809]  eta: 0:28:29  lr: 0.000024  min_lr: 0.000000  loss: 3.7969 (3.8056)  class_acc: 0.2917 (0.3173)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0486  max mem: 15572
[2025-01-16 00:54:14,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:54:14,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 100/2809]  eta: 0:28:29  lr: 0.000024  min_lr: 0.000000  loss: 3.9997 (3.8249)  class_acc: 0.2917 (0.3139)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1688  max mem: 15572
Epoch: [22]  [ 110/2809]  eta: 0:28:12  lr: 0.000024  min_lr: 0.000000  loss: 3.8696 (3.8325)  class_acc: 0.2917 (0.3123)  loss_scale: 65536.0000 (37491.3153)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1781  max mem: 15572
Epoch: [22]  [ 120/2809]  eta: 0:27:42  lr: 0.000024  min_lr: 0.000000  loss: 3.8433 (3.8454)  class_acc: 0.2917 (0.3092)  loss_scale: 65536.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.0942  max mem: 15572
Epoch: [22]  [ 130/2809]  eta: 0:27:19  lr: 0.000024  min_lr: 0.000000  loss: 3.8638 (3.8525)  class_acc: 0.2500 (0.3066)  loss_scale: 65536.0000 (41772.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0898  max mem: 15572
Epoch: [22]  [ 140/2809]  eta: 0:27:04  lr: 0.000024  min_lr: 0.000000  loss: 4.0179 (3.8635)  class_acc: 0.2500 (0.3056)  loss_scale: 65536.0000 (43458.2695)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1055  max mem: 15572
Epoch: [22]  [ 150/2809]  eta: 0:26:50  lr: 0.000024  min_lr: 0.000000  loss: 4.0304 (3.8794)  class_acc: 0.2917 (0.3044)  loss_scale: 65536.0000 (44920.3709)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1178  max mem: 15572
Epoch: [22]  [ 160/2809]  eta: 0:26:39  lr: 0.000024  min_lr: 0.000000  loss: 3.8428 (3.8795)  class_acc: 0.2917 (0.3028)  loss_scale: 65536.0000 (46200.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1456  max mem: 15572
Epoch: [22]  [ 170/2809]  eta: 0:26:55  lr: 0.000024  min_lr: 0.000000  loss: 3.8354 (3.8807)  class_acc: 0.2917 (0.3041)  loss_scale: 65536.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.2355  max mem: 15572
Epoch: [22]  [ 180/2809]  eta: 0:26:36  lr: 0.000024  min_lr: 0.000000  loss: 3.9175 (3.8731)  class_acc: 0.2917 (0.3069)  loss_scale: 65536.0000 (48337.3260)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.2038  max mem: 15572
Epoch: [22]  [ 190/2809]  eta: 0:26:30  lr: 0.000024  min_lr: 0.000000  loss: 3.7037 (3.8614)  class_acc: 0.3750 (0.3104)  loss_scale: 65536.0000 (49237.7801)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1446  max mem: 15572
Epoch: [22]  [ 200/2809]  eta: 0:26:13  lr: 0.000024  min_lr: 0.000000  loss: 3.7037 (3.8604)  class_acc: 0.3333 (0.3085)  loss_scale: 65536.0000 (50048.6368)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1340  max mem: 15572
[2025-01-16 00:55:15,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=62000, skipped=414, lr=[2.360327889193882e-07, 2.360327889193882e-07, 3.371896984562689e-07, 3.371896984562689e-07, 4.816995692232414e-07, 4.816995692232414e-07, 6.881422417474877e-07, 6.881422417474877e-07, 9.830603453535539e-07, 9.830603453535539e-07, 1.4043719219336484e-06, 1.4043719219336484e-06, 2.006245602762355e-06, 2.006245602762355e-06, 2.8660651468033645e-06, 2.8660651468033645e-06, 4.094378781147663e-06, 4.094378781147663e-06, 5.849112544496663e-06, 5.849112544496663e-06, 8.35587506356666e-06, 8.35587506356666e-06, 1.1936964376523803e-05, 1.1936964376523803e-05, 1.705280625217686e-05, 1.705280625217686e-05, 2.436115178882409e-05, 2.436115178882409e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 00:55:15,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=62000/global_step=62000, RunningAvgSamplesPerSec=28.527607020999, CurrSamplesPerSec=30.145552793118917, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [ 210/2809]  eta: 0:26:03  lr: 0.000024  min_lr: 0.000000  loss: 3.9339 (3.8622)  class_acc: 0.2500 (0.3075)  loss_scale: 65536.0000 (50782.6351)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0874  max mem: 15572
Epoch: [22]  [ 220/2809]  eta: 0:25:50  lr: 0.000024  min_lr: 0.000000  loss: 3.9558 (3.8583)  class_acc: 0.2917 (0.3090)  loss_scale: 65536.0000 (51450.2081)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1048  max mem: 15572
[2025-01-16 00:55:26,801] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:55:26,801] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:55:27,196] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62022
[2025-01-16 00:55:27,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:55:27,197] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 230/2809]  eta: 0:25:41  lr: 0.000024  min_lr: 0.000000  loss: 3.6730 (3.8514)  class_acc: 0.3333 (0.3093)  loss_scale: 65536.0000 (52343.6883)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1378  max mem: 15572
Epoch: [22]  [ 240/2809]  eta: 0:25:18  lr: 0.000024  min_lr: 0.000000  loss: 3.8042 (3.8531)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (52891.0871)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0830  max mem: 15572
Epoch: [22]  [ 250/2809]  eta: 0:25:10  lr: 0.000024  min_lr: 0.000000  loss: 3.8747 (3.8523)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (53394.8685)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0708  max mem: 15572
Epoch: [22]  [ 260/2809]  eta: 0:24:59  lr: 0.000024  min_lr: 0.000000  loss: 3.9398 (3.8607)  class_acc: 0.2917 (0.3067)  loss_scale: 65536.0000 (53860.0460)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1123  max mem: 15572
Epoch: [22]  [ 270/2809]  eta: 0:24:43  lr: 0.000024  min_lr: 0.000000  loss: 3.9398 (3.8588)  class_acc: 0.2917 (0.3069)  loss_scale: 65536.0000 (54290.8930)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0593  max mem: 15572
Epoch: [22]  [ 280/2809]  eta: 0:24:37  lr: 0.000024  min_lr: 0.000000  loss: 3.7676 (3.8594)  class_acc: 0.2917 (0.3077)  loss_scale: 65536.0000 (54691.0747)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0750  max mem: 15572
Epoch: [22]  [ 290/2809]  eta: 0:24:28  lr: 0.000024  min_lr: 0.000000  loss: 4.1007 (3.8633)  class_acc: 0.2917 (0.3067)  loss_scale: 65536.0000 (55063.7526)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1184  max mem: 15572
Epoch: [22]  [ 300/2809]  eta: 0:24:22  lr: 0.000024  min_lr: 0.000000  loss: 4.0730 (3.8602)  class_acc: 0.2917 (0.3083)  loss_scale: 65536.0000 (55411.6678)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1403  max mem: 15572
Epoch: [22]  [ 310/2809]  eta: 0:24:04  lr: 0.000024  min_lr: 0.000000  loss: 3.6456 (3.8522)  class_acc: 0.3750 (0.3103)  loss_scale: 65536.0000 (55737.2090)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0834  max mem: 15572
Epoch: [22]  [ 320/2809]  eta: 0:23:59  lr: 0.000024  min_lr: 0.000000  loss: 3.6731 (3.8558)  class_acc: 0.3333 (0.3087)  loss_scale: 65536.0000 (56042.4673)  weight_decay: 0.0500 (0.0500)  time: 0.5078  data: 0.0840  max mem: 15572
Epoch: [22]  [ 330/2809]  eta: 0:23:52  lr: 0.000024  min_lr: 0.000000  loss: 3.7932 (3.8550)  class_acc: 0.2500 (0.3092)  loss_scale: 65536.0000 (56329.2810)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1381  max mem: 15572
Epoch: [22]  [ 340/2809]  eta: 0:23:50  lr: 0.000024  min_lr: 0.000000  loss: 3.7932 (3.8545)  class_acc: 0.3333 (0.3105)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1608  max mem: 15572
Epoch: [22]  [ 350/2809]  eta: 0:23:40  lr: 0.000024  min_lr: 0.000000  loss: 3.9026 (3.8550)  class_acc: 0.3333 (0.3099)  loss_scale: 65536.0000 (56853.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1294  max mem: 15572
[2025-01-16 00:56:38,319] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:56:38,319] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:56:40,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62157
[2025-01-16 00:56:40,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:56:40,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 360/2809]  eta: 0:23:34  lr: 0.000024  min_lr: 0.000000  loss: 3.6102 (3.8524)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (58183.6233)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1094  max mem: 15572
Epoch: [22]  [ 370/2809]  eta: 0:23:35  lr: 0.000024  min_lr: 0.000000  loss: 3.6769 (3.8502)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (58381.8005)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1998  max mem: 15572
Epoch: [22]  [ 380/2809]  eta: 0:23:31  lr: 0.000024  min_lr: 0.000000  loss: 3.7424 (3.8482)  class_acc: 0.2917 (0.3105)  loss_scale: 65536.0000 (58569.5748)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.2004  max mem: 15572
Epoch: [22]  [ 390/2809]  eta: 0:23:20  lr: 0.000024  min_lr: 0.000000  loss: 3.8229 (3.8484)  class_acc: 0.2917 (0.3104)  loss_scale: 65536.0000 (58747.7442)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1136  max mem: 15572
Epoch: [22]  [ 400/2809]  eta: 0:23:15  lr: 0.000024  min_lr: 0.000000  loss: 3.5683 (3.8418)  class_acc: 0.3333 (0.3121)  loss_scale: 65536.0000 (58917.0274)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0951  max mem: 15572
Epoch: [22]  [ 410/2809]  eta: 0:23:07  lr: 0.000024  min_lr: 0.000000  loss: 3.7458 (3.8408)  class_acc: 0.3333 (0.3120)  loss_scale: 65536.0000 (59078.0730)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1152  max mem: 15572
Epoch: [22]  [ 420/2809]  eta: 0:23:11  lr: 0.000024  min_lr: 0.000000  loss: 3.7932 (3.8358)  class_acc: 0.2917 (0.3127)  loss_scale: 65536.0000 (59231.4679)  weight_decay: 0.0500 (0.0500)  time: 0.6507  data: 0.2088  max mem: 15572
Epoch: [22]  [ 430/2809]  eta: 0:22:58  lr: 0.000024  min_lr: 0.000000  loss: 3.8388 (3.8361)  class_acc: 0.2917 (0.3119)  loss_scale: 65536.0000 (59377.7448)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1672  max mem: 15572
Epoch: [22]  [ 440/2809]  eta: 0:22:52  lr: 0.000024  min_lr: 0.000000  loss: 3.8482 (3.8345)  class_acc: 0.2500 (0.3118)  loss_scale: 65536.0000 (59517.3878)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0648  max mem: 15572
Epoch: [22]  [ 450/2809]  eta: 0:22:42  lr: 0.000024  min_lr: 0.000000  loss: 3.9144 (3.8366)  class_acc: 0.2917 (0.3104)  loss_scale: 65536.0000 (59650.8381)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0831  max mem: 15572
Epoch: [22]  [ 460/2809]  eta: 0:22:40  lr: 0.000024  min_lr: 0.000000  loss: 3.8046 (3.8319)  class_acc: 0.3333 (0.3118)  loss_scale: 65536.0000 (59778.4989)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1225  max mem: 15572
Epoch: [22]  [ 470/2809]  eta: 0:22:29  lr: 0.000024  min_lr: 0.000000  loss: 3.8339 (3.8350)  class_acc: 0.2917 (0.3108)  loss_scale: 65536.0000 (59900.7389)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1123  max mem: 15572
Epoch: [22]  [ 480/2809]  eta: 0:22:20  lr: 0.000024  min_lr: 0.000000  loss: 3.6826 (3.8277)  class_acc: 0.3333 (0.3128)  loss_scale: 65536.0000 (60017.8960)  weight_decay: 0.0500 (0.0500)  time: 0.4922  data: 0.0480  max mem: 15572
[2025-01-16 00:57:53,691] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:57:53,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 00:57:54,097] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62287
[2025-01-16 00:57:54,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:57:54,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 490/2809]  eta: 0:22:10  lr: 0.000024  min_lr: 0.000000  loss: 3.5938 (3.8293)  class_acc: 0.3333 (0.3121)  loss_scale: 65536.0000 (60263.7556)  weight_decay: 0.0500 (0.0500)  time: 0.4966  data: 0.0620  max mem: 15572
Epoch: [22]  [ 500/2809]  eta: 0:22:05  lr: 0.000024  min_lr: 0.000000  loss: 3.9660 (3.8291)  class_acc: 0.2917 (0.3122)  loss_scale: 65536.0000 (60368.9900)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0820  max mem: 15572
Epoch: [22]  [ 510/2809]  eta: 0:22:01  lr: 0.000024  min_lr: 0.000000  loss: 3.7869 (3.8245)  class_acc: 0.3750 (0.3144)  loss_scale: 65536.0000 (60470.1057)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1278  max mem: 15572
Epoch: [22]  [ 520/2809]  eta: 0:22:00  lr: 0.000024  min_lr: 0.000000  loss: 3.7795 (3.8222)  class_acc: 0.3750 (0.3143)  loss_scale: 65536.0000 (60567.3397)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.1848  max mem: 15572
Epoch: [22]  [ 530/2809]  eta: 0:21:51  lr: 0.000024  min_lr: 0.000000  loss: 3.9431 (3.8241)  class_acc: 0.2500 (0.3129)  loss_scale: 65536.0000 (60660.9115)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1307  max mem: 15572
Epoch: [22]  [ 540/2809]  eta: 0:21:42  lr: 0.000024  min_lr: 0.000000  loss: 3.9954 (3.8258)  class_acc: 0.2500 (0.3117)  loss_scale: 65536.0000 (60751.0240)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0601  max mem: 15572
Epoch: [22]  [ 550/2809]  eta: 0:21:36  lr: 0.000024  min_lr: 0.000000  loss: 3.9921 (3.8236)  class_acc: 0.2500 (0.3114)  loss_scale: 65536.0000 (60837.8657)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1052  max mem: 15572
Epoch: [22]  [ 560/2809]  eta: 0:21:29  lr: 0.000024  min_lr: 0.000000  loss: 3.9060 (3.8224)  class_acc: 0.2500 (0.3112)  loss_scale: 65536.0000 (60921.6114)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1227  max mem: 15572
Epoch: [22]  [ 570/2809]  eta: 0:21:23  lr: 0.000024  min_lr: 0.000000  loss: 3.8868 (3.8253)  class_acc: 0.2500 (0.3112)  loss_scale: 65536.0000 (61002.4238)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1099  max mem: 15572
Epoch: [22]  [ 580/2809]  eta: 0:21:16  lr: 0.000024  min_lr: 0.000000  loss: 3.7106 (3.8199)  class_acc: 0.3333 (0.3129)  loss_scale: 65536.0000 (61080.4544)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1053  max mem: 15572
Epoch: [22]  [ 590/2809]  eta: 0:21:08  lr: 0.000024  min_lr: 0.000000  loss: 3.6985 (3.8185)  class_acc: 0.3333 (0.3131)  loss_scale: 65536.0000 (61155.8443)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.1011  max mem: 15572
Epoch: [22]  [ 600/2809]  eta: 0:21:05  lr: 0.000024  min_lr: 0.000000  loss: 3.8009 (3.8198)  class_acc: 0.2917 (0.3128)  loss_scale: 65536.0000 (61228.7255)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1419  max mem: 15572
Epoch: [22]  [ 610/2809]  eta: 0:20:57  lr: 0.000024  min_lr: 0.000000  loss: 3.8137 (3.8196)  class_acc: 0.2500 (0.3123)  loss_scale: 65536.0000 (61299.2209)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1194  max mem: 15572
[2025-01-16 00:59:06,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 00:59:06,709] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [ 620/2809]  eta: 0:20:50  lr: 0.000024  min_lr: 0.000000  loss: 3.8137 (3.8185)  class_acc: 0.2917 (0.3125)  loss_scale: 65536.0000 (61684.0451)  weight_decay: 0.0500 (0.0500)  time: 0.5208  data: 0.0749  max mem: 15572
[2025-01-16 00:59:09,554] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62422
[2025-01-16 00:59:09,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 00:59:09,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 630/2809]  eta: 0:20:45  lr: 0.000024  min_lr: 0.000000  loss: 3.9674 (3.8192)  class_acc: 0.3333 (0.3134)  loss_scale: 65536.0000 (62056.6719)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1177  max mem: 15572
Epoch: [22]  [ 640/2809]  eta: 0:20:36  lr: 0.000024  min_lr: 0.000000  loss: 3.8725 (3.8215)  class_acc: 0.2917 (0.3132)  loss_scale: 65536.0000 (62110.9516)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1000  max mem: 15572
Epoch: [22]  [ 650/2809]  eta: 0:20:30  lr: 0.000024  min_lr: 0.000000  loss: 3.8725 (3.8225)  class_acc: 0.2917 (0.3129)  loss_scale: 65536.0000 (62163.5637)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0927  max mem: 15572
Epoch: [22]  [ 660/2809]  eta: 0:20:27  lr: 0.000024  min_lr: 0.000000  loss: 4.1844 (3.8284)  class_acc: 0.2917 (0.3116)  loss_scale: 65536.0000 (62214.5840)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1793  max mem: 15572
Epoch: [22]  [ 670/2809]  eta: 0:20:20  lr: 0.000024  min_lr: 0.000000  loss: 4.0226 (3.8263)  class_acc: 0.2500 (0.3118)  loss_scale: 65536.0000 (62264.0835)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1647  max mem: 15572
Epoch: [22]  [ 680/2809]  eta: 0:20:14  lr: 0.000024  min_lr: 0.000000  loss: 3.8042 (3.8261)  class_acc: 0.2917 (0.3119)  loss_scale: 65536.0000 (62312.1292)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0981  max mem: 15572
Epoch: [22]  [ 690/2809]  eta: 0:20:10  lr: 0.000024  min_lr: 0.000000  loss: 3.8099 (3.8249)  class_acc: 0.3333 (0.3126)  loss_scale: 65536.0000 (62358.7844)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1376  max mem: 15572
[2025-01-16 00:59:48,431] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62490
[2025-01-16 00:59:48,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 00:59:48,432] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 700/2809]  eta: 0:20:03  lr: 0.000024  min_lr: 0.000000  loss: 3.8099 (3.8239)  class_acc: 0.3333 (0.3127)  loss_scale: 65536.0000 (61983.4066)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1373  max mem: 15572
Epoch: [22]  [ 710/2809]  eta: 0:19:56  lr: 0.000024  min_lr: 0.000000  loss: 3.9197 (3.8258)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (61572.5007)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0928  max mem: 15572
Epoch: [22]  [ 720/2809]  eta: 0:19:51  lr: 0.000024  min_lr: 0.000000  loss: 3.8583 (3.8249)  class_acc: 0.2917 (0.3118)  loss_scale: 32768.0000 (61172.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0973  max mem: 15572
Epoch: [22]  [ 730/2809]  eta: 0:19:43  lr: 0.000024  min_lr: 0.000000  loss: 3.8047 (3.8257)  class_acc: 0.2500 (0.3113)  loss_scale: 32768.0000 (60784.4159)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0785  max mem: 15572
Epoch: [22]  [ 740/2809]  eta: 0:19:35  lr: 0.000024  min_lr: 0.000000  loss: 3.8388 (3.8275)  class_acc: 0.2500 (0.3111)  loss_scale: 32768.0000 (60406.3266)  weight_decay: 0.0500 (0.0500)  time: 0.4950  data: 0.0501  max mem: 15572
Epoch: [22]  [ 750/2809]  eta: 0:19:33  lr: 0.000024  min_lr: 0.000000  loss: 3.9242 (3.8284)  class_acc: 0.2917 (0.3110)  loss_scale: 32768.0000 (60038.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1603  max mem: 15572
Epoch: [22]  [ 760/2809]  eta: 0:19:27  lr: 0.000024  min_lr: 0.000000  loss: 3.8470 (3.8259)  class_acc: 0.3333 (0.3118)  loss_scale: 32768.0000 (59679.9580)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.2089  max mem: 15572
Epoch: [22]  [ 770/2809]  eta: 0:19:20  lr: 0.000024  min_lr: 0.000000  loss: 3.6120 (3.8248)  class_acc: 0.3750 (0.3121)  loss_scale: 32768.0000 (59330.9053)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1119  max mem: 15572
Epoch: [22]  [ 780/2809]  eta: 0:19:14  lr: 0.000024  min_lr: 0.000000  loss: 3.6653 (3.8244)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (58990.7913)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0938  max mem: 15572
Epoch: [22]  [ 790/2809]  eta: 0:19:07  lr: 0.000024  min_lr: 0.000000  loss: 3.6653 (3.8220)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (58659.2769)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0907  max mem: 15572
Epoch: [22]  [ 800/2809]  eta: 0:19:03  lr: 0.000024  min_lr: 0.000000  loss: 3.7017 (3.8214)  class_acc: 0.3333 (0.3133)  loss_scale: 32768.0000 (58336.0400)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1369  max mem: 15572
Epoch: [22]  [ 810/2809]  eta: 0:18:59  lr: 0.000024  min_lr: 0.000000  loss: 3.8169 (3.8213)  class_acc: 0.3333 (0.3135)  loss_scale: 32768.0000 (58020.7744)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.2132  max mem: 15572
Epoch: [22]  [ 820/2809]  eta: 0:18:51  lr: 0.000024  min_lr: 0.000000  loss: 3.8499 (3.8197)  class_acc: 0.3333 (0.3137)  loss_scale: 32768.0000 (57713.1888)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1225  max mem: 15572
[2025-01-16 01:01:00,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:01:00,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 830/2809]  eta: 0:18:47  lr: 0.000024  min_lr: 0.000000  loss: 3.5125 (3.8173)  class_acc: 0.3333 (0.3142)  loss_scale: 32768.0000 (57807.3261)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1001  max mem: 15572
Epoch: [22]  [ 840/2809]  eta: 0:18:40  lr: 0.000024  min_lr: 0.000000  loss: 3.8035 (3.8189)  class_acc: 0.2917 (0.3135)  loss_scale: 65536.0000 (57899.2247)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1258  max mem: 15572
[2025-01-16 01:01:14,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62643
[2025-01-16 01:01:14,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:01:14,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 850/2809]  eta: 0:18:33  lr: 0.000024  min_lr: 0.000000  loss: 3.8035 (3.8168)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (57757.9318)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0672  max mem: 15572
Epoch: [22]  [ 860/2809]  eta: 0:18:28  lr: 0.000024  min_lr: 0.000000  loss: 3.8168 (3.8186)  class_acc: 0.2500 (0.3133)  loss_scale: 32768.0000 (57467.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1051  max mem: 15572
Epoch: [22]  [ 870/2809]  eta: 0:18:20  lr: 0.000024  min_lr: 0.000000  loss: 3.9798 (3.8217)  class_acc: 0.2500 (0.3128)  loss_scale: 32768.0000 (57184.1102)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.0960  max mem: 15572
Epoch: [22]  [ 880/2809]  eta: 0:18:16  lr: 0.000024  min_lr: 0.000000  loss: 4.0039 (3.8230)  class_acc: 0.2500 (0.3123)  loss_scale: 32768.0000 (56906.9694)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1054  max mem: 15572
Epoch: [22]  [ 890/2809]  eta: 0:18:08  lr: 0.000024  min_lr: 0.000000  loss: 4.0039 (3.8235)  class_acc: 0.2500 (0.3124)  loss_scale: 32768.0000 (56636.0494)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1066  max mem: 15572
Epoch: [22]  [ 900/2809]  eta: 0:18:00  lr: 0.000024  min_lr: 0.000000  loss: 3.8532 (3.8236)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (56371.1432)  weight_decay: 0.0500 (0.0500)  time: 0.4746  data: 0.0538  max mem: 15572
Epoch: [22]  [ 910/2809]  eta: 0:17:54  lr: 0.000024  min_lr: 0.000000  loss: 3.8065 (3.8225)  class_acc: 0.3333 (0.3127)  loss_scale: 32768.0000 (56112.0527)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0681  max mem: 15572
Epoch: [22]  [ 920/2809]  eta: 0:17:49  lr: 0.000024  min_lr: 0.000000  loss: 3.8366 (3.8250)  class_acc: 0.2500 (0.3123)  loss_scale: 32768.0000 (55858.5885)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1161  max mem: 15572
Epoch: [22]  [ 930/2809]  eta: 0:17:44  lr: 0.000024  min_lr: 0.000000  loss: 3.9082 (3.8240)  class_acc: 0.3333 (0.3125)  loss_scale: 32768.0000 (55610.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1539  max mem: 15572
Epoch: [22]  [ 940/2809]  eta: 0:17:37  lr: 0.000024  min_lr: 0.000000  loss: 3.7196 (3.8233)  class_acc: 0.3333 (0.3128)  loss_scale: 32768.0000 (55367.8215)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1084  max mem: 15572
Epoch: [22]  [ 950/2809]  eta: 0:17:32  lr: 0.000024  min_lr: 0.000000  loss: 3.7056 (3.8226)  class_acc: 0.3333 (0.3136)  loss_scale: 32768.0000 (55130.1788)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1179  max mem: 15572
Epoch: [22]  [ 960/2809]  eta: 0:17:28  lr: 0.000024  min_lr: 0.000000  loss: 3.6236 (3.8217)  class_acc: 0.3333 (0.3134)  loss_scale: 32768.0000 (54897.4818)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1790  max mem: 15572
Epoch: [22]  [ 970/2809]  eta: 0:17:23  lr: 0.000024  min_lr: 0.000000  loss: 3.9175 (3.8223)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (54669.5778)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1724  max mem: 15572
[2025-01-16 01:02:25,783] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:02:25,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [ 980/2809]  eta: 0:17:16  lr: 0.000024  min_lr: 0.000000  loss: 4.0185 (3.8241)  class_acc: 0.2500 (0.3126)  loss_scale: 32768.0000 (54680.1386)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0781  max mem: 15572
Epoch: [22]  [ 990/2809]  eta: 0:17:10  lr: 0.000024  min_lr: 0.000000  loss: 4.0984 (3.8273)  class_acc: 0.2500 (0.3117)  loss_scale: 65536.0000 (54789.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0563  max mem: 15572
Epoch: [22]  [1000/2809]  eta: 0:17:04  lr: 0.000024  min_lr: 0.000000  loss: 4.0054 (3.8265)  class_acc: 0.2500 (0.3116)  loss_scale: 65536.0000 (54897.0390)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1147  max mem: 15572
Epoch: [22]  [1010/2809]  eta: 0:16:59  lr: 0.000024  min_lr: 0.000000  loss: 3.7944 (3.8260)  class_acc: 0.2500 (0.3117)  loss_scale: 65536.0000 (55002.2710)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1239  max mem: 15572
Epoch: [22]  [1020/2809]  eta: 0:16:52  lr: 0.000024  min_lr: 0.000000  loss: 3.9108 (3.8257)  class_acc: 0.2500 (0.3119)  loss_scale: 65536.0000 (55105.4417)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1152  max mem: 15572
Epoch: [22]  [1030/2809]  eta: 0:16:48  lr: 0.000024  min_lr: 0.000000  loss: 3.9108 (3.8275)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (55206.6111)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1591  max mem: 15572
Epoch: [22]  [1040/2809]  eta: 0:16:45  lr: 0.000024  min_lr: 0.000000  loss: 3.9720 (3.8284)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (55305.8367)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.2426  max mem: 15572
Epoch: [22]  [1050/2809]  eta: 0:16:37  lr: 0.000024  min_lr: 0.000000  loss: 3.8439 (3.8267)  class_acc: 0.2917 (0.3115)  loss_scale: 65536.0000 (55403.1741)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1438  max mem: 15572
Epoch: [22]  [1060/2809]  eta: 0:16:32  lr: 0.000024  min_lr: 0.000000  loss: 3.8331 (3.8272)  class_acc: 0.3333 (0.3116)  loss_scale: 65536.0000 (55498.6767)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0844  max mem: 15572
Epoch: [22]  [1070/2809]  eta: 0:16:26  lr: 0.000024  min_lr: 0.000000  loss: 3.9597 (3.8272)  class_acc: 0.2500 (0.3117)  loss_scale: 65536.0000 (55592.3959)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1138  max mem: 15572
Epoch: [22]  [1080/2809]  eta: 0:16:18  lr: 0.000024  min_lr: 0.000000  loss: 4.0467 (3.8287)  class_acc: 0.2500 (0.3115)  loss_scale: 65536.0000 (55684.3811)  weight_decay: 0.0500 (0.0500)  time: 0.4916  data: 0.0465  max mem: 15572
Epoch: [22]  [1090/2809]  eta: 0:16:14  lr: 0.000024  min_lr: 0.000000  loss: 3.8057 (3.8269)  class_acc: 0.2917 (0.3120)  loss_scale: 65536.0000 (55774.6801)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1174  max mem: 15572
Epoch: [22]  [1100/2809]  eta: 0:16:09  lr: 0.000024  min_lr: 0.000000  loss: 3.9431 (3.8294)  class_acc: 0.2917 (0.3115)  loss_scale: 65536.0000 (55863.3388)  weight_decay: 0.0500 (0.0500)  time: 0.6399  data: 0.2061  max mem: 15572
[2025-01-16 01:03:38,681] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:03:38,682] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:03:39,567] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62902
[2025-01-16 01:03:39,568] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:03:39,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1110/2809]  eta: 0:16:03  lr: 0.000024  min_lr: 0.000000  loss: 4.0037 (3.8302)  class_acc: 0.2083 (0.3114)  loss_scale: 65536.0000 (56068.3780)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1208  max mem: 15572
Epoch: [22]  [1120/2809]  eta: 0:15:57  lr: 0.000024  min_lr: 0.000000  loss: 3.8638 (3.8290)  class_acc: 0.2917 (0.3113)  loss_scale: 65536.0000 (56152.8350)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1022  max mem: 15572
[2025-01-16 01:03:52,628] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 62927
[2025-01-16 01:03:52,629] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:03:52,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1130/2809]  eta: 0:15:52  lr: 0.000024  min_lr: 0.000000  loss: 4.0377 (3.8323)  class_acc: 0.2500 (0.3103)  loss_scale: 65536.0000 (56177.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1649  max mem: 15572
Epoch: [22]  [1140/2809]  eta: 0:15:45  lr: 0.000024  min_lr: 0.000000  loss: 4.0377 (3.8326)  class_acc: 0.2500 (0.3102)  loss_scale: 32768.0000 (55972.6836)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0882  max mem: 15572
Epoch: [22]  [1150/2809]  eta: 0:15:40  lr: 0.000024  min_lr: 0.000000  loss: 3.8805 (3.8327)  class_acc: 0.2917 (0.3102)  loss_scale: 32768.0000 (55771.0791)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0963  max mem: 15572
Epoch: [22]  [1160/2809]  eta: 0:15:33  lr: 0.000024  min_lr: 0.000000  loss: 3.7252 (3.8329)  class_acc: 0.2917 (0.3103)  loss_scale: 32768.0000 (55572.9475)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1012  max mem: 15572
Epoch: [22]  [1170/2809]  eta: 0:15:28  lr: 0.000024  min_lr: 0.000000  loss: 3.8663 (3.8325)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (55378.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1058  max mem: 15572
Epoch: [22]  [1180/2809]  eta: 0:15:21  lr: 0.000024  min_lr: 0.000000  loss: 3.8211 (3.8320)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (55186.7502)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1155  max mem: 15572
Epoch: [22]  [1190/2809]  eta: 0:15:16  lr: 0.000024  min_lr: 0.000000  loss: 3.8211 (3.8312)  class_acc: 0.2500 (0.3103)  loss_scale: 32768.0000 (54998.5155)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1062  max mem: 15572
Epoch: [22]  [1200/2809]  eta: 0:15:10  lr: 0.000024  min_lr: 0.000000  loss: 3.8246 (3.8306)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (54813.4155)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1424  max mem: 15572
[2025-01-16 01:04:32,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=63000, skipped=422, lr=[2.2878587401415915e-07, 2.2878587401415915e-07, 3.2683696287737025e-07, 3.2683696287737025e-07, 4.6690994696767183e-07, 4.6690994696767183e-07, 6.670142099538169e-07, 6.670142099538169e-07, 9.528774427911671e-07, 9.528774427911671e-07, 1.3612534897016674e-06, 1.3612534897016674e-06, 1.9446478424309535e-06, 1.9446478424309535e-06, 2.778068346329934e-06, 2.778068346329934e-06, 3.96866906618562e-06, 3.96866906618562e-06, 5.6695272374080294e-06, 5.6695272374080294e-06, 8.099324624868612e-06, 8.099324624868612e-06, 1.1570463749812306e-05, 1.1570463749812306e-05, 1.6529233928303293e-05, 1.6529233928303293e-05, 2.3613191326147565e-05, 2.3613191326147565e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:04:32,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=63000/global_step=63000, RunningAvgSamplesPerSec=28.532441160391457, CurrSamplesPerSec=32.31730405587186, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [1210/2809]  eta: 0:15:03  lr: 0.000024  min_lr: 0.000000  loss: 3.5980 (3.8298)  class_acc: 0.3750 (0.3113)  loss_scale: 32768.0000 (54631.3724)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0964  max mem: 15572
Epoch: [22]  [1220/2809]  eta: 0:14:57  lr: 0.000024  min_lr: 0.000000  loss: 3.8957 (3.8304)  class_acc: 0.3333 (0.3110)  loss_scale: 32768.0000 (54452.3112)  weight_decay: 0.0500 (0.0500)  time: 0.5018  data: 0.0814  max mem: 15572
Epoch: [22]  [1230/2809]  eta: 0:14:51  lr: 0.000024  min_lr: 0.000000  loss: 3.9649 (3.8314)  class_acc: 0.2500 (0.3108)  loss_scale: 32768.0000 (54276.1592)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1052  max mem: 15572
Epoch: [22]  [1240/2809]  eta: 0:14:46  lr: 0.000024  min_lr: 0.000000  loss: 3.9513 (3.8309)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (54102.8461)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1256  max mem: 15572
Epoch: [22]  [1250/2809]  eta: 0:14:41  lr: 0.000024  min_lr: 0.000000  loss: 3.8769 (3.8314)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (53932.3038)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1539  max mem: 15572
[2025-01-16 01:05:03,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:05:03,870] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1260/2809]  eta: 0:14:34  lr: 0.000024  min_lr: 0.000000  loss: 4.0064 (3.8317)  class_acc: 0.2500 (0.3106)  loss_scale: 32768.0000 (53842.4235)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0976  max mem: 15572
Epoch: [22]  [1270/2809]  eta: 0:14:28  lr: 0.000024  min_lr: 0.000000  loss: 3.8651 (3.8293)  class_acc: 0.3750 (0.3112)  loss_scale: 65536.0000 (53934.4264)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0515  max mem: 15572
Epoch: [22]  [1280/2809]  eta: 0:14:24  lr: 0.000024  min_lr: 0.000000  loss: 3.8651 (3.8310)  class_acc: 0.3333 (0.3111)  loss_scale: 65536.0000 (54024.9930)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1900  max mem: 15572
Epoch: [22]  [1290/2809]  eta: 0:14:19  lr: 0.000024  min_lr: 0.000000  loss: 4.0475 (3.8321)  class_acc: 0.3333 (0.3113)  loss_scale: 65536.0000 (54114.1565)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.2088  max mem: 15572
Epoch: [22]  [1300/2809]  eta: 0:14:12  lr: 0.000024  min_lr: 0.000000  loss: 3.9782 (3.8340)  class_acc: 0.2917 (0.3108)  loss_scale: 65536.0000 (54201.9493)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0704  max mem: 15572
Epoch: [22]  [1310/2809]  eta: 0:14:06  lr: 0.000024  min_lr: 0.000000  loss: 3.8191 (3.8328)  class_acc: 0.3333 (0.3113)  loss_scale: 65536.0000 (54288.4027)  weight_decay: 0.0500 (0.0500)  time: 0.4884  data: 0.0454  max mem: 15572
Epoch: [22]  [1320/2809]  eta: 0:14:00  lr: 0.000024  min_lr: 0.000000  loss: 3.7512 (3.8330)  class_acc: 0.3333 (0.3109)  loss_scale: 65536.0000 (54373.5473)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1080  max mem: 15572
[2025-01-16 01:05:39,826] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63120
[2025-01-16 01:05:39,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:05:39,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1330/2809]  eta: 0:13:54  lr: 0.000024  min_lr: 0.000000  loss: 4.0198 (3.8337)  class_acc: 0.2083 (0.3104)  loss_scale: 65536.0000 (54235.8407)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0794  max mem: 15572
Epoch: [22]  [1340/2809]  eta: 0:13:48  lr: 0.000024  min_lr: 0.000000  loss: 3.8438 (3.8336)  class_acc: 0.2500 (0.3105)  loss_scale: 32768.0000 (54075.7524)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0522  max mem: 15572
Epoch: [22]  [1350/2809]  eta: 0:13:42  lr: 0.000024  min_lr: 0.000000  loss: 3.6214 (3.8310)  class_acc: 0.3333 (0.3112)  loss_scale: 32768.0000 (53918.0340)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1080  max mem: 15572
Epoch: [22]  [1360/2809]  eta: 0:13:36  lr: 0.000023  min_lr: 0.000000  loss: 3.5568 (3.8300)  class_acc: 0.2917 (0.3111)  loss_scale: 32768.0000 (53762.6334)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.1121  max mem: 15572
Epoch: [22]  [1370/2809]  eta: 0:13:30  lr: 0.000023  min_lr: 0.000000  loss: 3.9635 (3.8309)  class_acc: 0.2500 (0.3107)  loss_scale: 32768.0000 (53609.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0927  max mem: 15572
Epoch: [22]  [1380/2809]  eta: 0:13:25  lr: 0.000023  min_lr: 0.000000  loss: 3.9636 (3.8312)  class_acc: 0.2500 (0.3105)  loss_scale: 32768.0000 (53458.5836)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1346  max mem: 15572
Epoch: [22]  [1390/2809]  eta: 0:13:20  lr: 0.000023  min_lr: 0.000000  loss: 3.9555 (3.8318)  class_acc: 0.2500 (0.3101)  loss_scale: 32768.0000 (53309.8375)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1665  max mem: 15572
Epoch: [22]  [1400/2809]  eta: 0:13:14  lr: 0.000023  min_lr: 0.000000  loss: 3.9462 (3.8323)  class_acc: 0.2500 (0.3101)  loss_scale: 32768.0000 (53163.2148)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1357  max mem: 15572
Epoch: [22]  [1410/2809]  eta: 0:13:08  lr: 0.000023  min_lr: 0.000000  loss: 3.7919 (3.8312)  class_acc: 0.3333 (0.3107)  loss_scale: 32768.0000 (53018.6704)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1113  max mem: 15572
Epoch: [22]  [1420/2809]  eta: 0:13:03  lr: 0.000023  min_lr: 0.000000  loss: 3.7753 (3.8317)  class_acc: 0.3333 (0.3109)  loss_scale: 32768.0000 (52876.1605)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1518  max mem: 15572
Epoch: [22]  [1430/2809]  eta: 0:12:57  lr: 0.000023  min_lr: 0.000000  loss: 3.9474 (3.8318)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (52735.6422)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1395  max mem: 15572
Epoch: [22]  [1440/2809]  eta: 0:12:52  lr: 0.000023  min_lr: 0.000000  loss: 3.8848 (3.8324)  class_acc: 0.2917 (0.3108)  loss_scale: 32768.0000 (52597.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1345  max mem: 15572
Epoch: [22]  [1450/2809]  eta: 0:12:47  lr: 0.000023  min_lr: 0.000000  loss: 3.9214 (3.8334)  class_acc: 0.2917 (0.3107)  loss_scale: 32768.0000 (52460.4163)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1775  max mem: 15572
[2025-01-16 01:06:53,012] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:06:53,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1460/2809]  eta: 0:12:40  lr: 0.000023  min_lr: 0.000000  loss: 4.0045 (3.8339)  class_acc: 0.2917 (0.3105)  loss_scale: 32768.0000 (52549.9138)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0916  max mem: 15572
Epoch: [22]  [1470/2809]  eta: 0:12:34  lr: 0.000023  min_lr: 0.000000  loss: 4.0043 (3.8353)  class_acc: 0.2500 (0.3102)  loss_scale: 65536.0000 (52638.1944)  weight_decay: 0.0500 (0.0500)  time: 0.4622  data: 0.0262  max mem: 15572
Epoch: [22]  [1480/2809]  eta: 0:12:29  lr: 0.000023  min_lr: 0.000000  loss: 4.0043 (3.8344)  class_acc: 0.2083 (0.3099)  loss_scale: 65536.0000 (52725.2829)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1338  max mem: 15572
[2025-01-16 01:07:10,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63283
[2025-01-16 01:07:10,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:07:10,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1490/2809]  eta: 0:12:23  lr: 0.000023  min_lr: 0.000000  loss: 3.8516 (3.8338)  class_acc: 0.2917 (0.3101)  loss_scale: 65536.0000 (52679.3400)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1690  max mem: 15572
Epoch: [22]  [1500/2809]  eta: 0:12:18  lr: 0.000023  min_lr: 0.000000  loss: 3.8480 (3.8339)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (52546.6862)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1163  max mem: 15572
Epoch: [22]  [1510/2809]  eta: 0:12:12  lr: 0.000023  min_lr: 0.000000  loss: 3.9824 (3.8356)  class_acc: 0.2917 (0.3099)  loss_scale: 32768.0000 (52415.7882)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1134  max mem: 15572
Epoch: [22]  [1520/2809]  eta: 0:12:06  lr: 0.000023  min_lr: 0.000000  loss: 3.8182 (3.8347)  class_acc: 0.2917 (0.3101)  loss_scale: 32768.0000 (52286.6114)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0839  max mem: 15572
Epoch: [22]  [1530/2809]  eta: 0:12:00  lr: 0.000023  min_lr: 0.000000  loss: 3.8223 (3.8354)  class_acc: 0.2917 (0.3102)  loss_scale: 32768.0000 (52159.1221)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0602  max mem: 15572
Epoch: [22]  [1540/2809]  eta: 0:11:55  lr: 0.000023  min_lr: 0.000000  loss: 4.0561 (3.8369)  class_acc: 0.2917 (0.3099)  loss_scale: 32768.0000 (52033.2875)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1142  max mem: 15572
Epoch: [22]  [1550/2809]  eta: 0:11:49  lr: 0.000023  min_lr: 0.000000  loss: 3.9039 (3.8375)  class_acc: 0.2917 (0.3097)  loss_scale: 32768.0000 (51909.0754)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1104  max mem: 15572
Epoch: [22]  [1560/2809]  eta: 0:11:44  lr: 0.000023  min_lr: 0.000000  loss: 3.8303 (3.8358)  class_acc: 0.3333 (0.3101)  loss_scale: 32768.0000 (51786.4548)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1164  max mem: 15572
Epoch: [22]  [1570/2809]  eta: 0:11:37  lr: 0.000023  min_lr: 0.000000  loss: 3.5308 (3.8355)  class_acc: 0.3333 (0.3103)  loss_scale: 32768.0000 (51665.3953)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1128  max mem: 15572
Epoch: [22]  [1580/2809]  eta: 0:11:31  lr: 0.000023  min_lr: 0.000000  loss: 3.9857 (3.8364)  class_acc: 0.2917 (0.3099)  loss_scale: 32768.0000 (51545.8672)  weight_decay: 0.0500 (0.0500)  time: 0.5125  data: 0.0697  max mem: 15572
Epoch: [22]  [1590/2809]  eta: 0:11:26  lr: 0.000023  min_lr: 0.000000  loss: 3.9567 (3.8366)  class_acc: 0.2500 (0.3096)  loss_scale: 32768.0000 (51427.8416)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1002  max mem: 15572
Epoch: [22]  [1600/2809]  eta: 0:11:20  lr: 0.000023  min_lr: 0.000000  loss: 3.9567 (3.8359)  class_acc: 0.2917 (0.3095)  loss_scale: 32768.0000 (51311.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1239  max mem: 15572
Epoch: [22]  [1610/2809]  eta: 0:11:14  lr: 0.000023  min_lr: 0.000000  loss: 3.9920 (3.8364)  class_acc: 0.2500 (0.3095)  loss_scale: 32768.0000 (51196.1862)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1156  max mem: 15572
[2025-01-16 01:08:22,529] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:08:22,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [1620/2809]  eta: 0:11:09  lr: 0.000023  min_lr: 0.000000  loss: 3.8264 (3.8357)  class_acc: 0.3333 (0.3098)  loss_scale: 32768.0000 (51224.0049)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1236  max mem: 15572
Epoch: [22]  [1630/2809]  eta: 0:11:03  lr: 0.000023  min_lr: 0.000000  loss: 3.8264 (3.8365)  class_acc: 0.2917 (0.3094)  loss_scale: 65536.0000 (51311.7548)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1157  max mem: 15572
Epoch: [22]  [1640/2809]  eta: 0:10:59  lr: 0.000023  min_lr: 0.000000  loss: 3.9197 (3.8360)  class_acc: 0.2500 (0.3096)  loss_scale: 65536.0000 (51398.4351)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1643  max mem: 15572
Epoch: [22]  [1650/2809]  eta: 0:10:54  lr: 0.000023  min_lr: 0.000000  loss: 3.9197 (3.8370)  class_acc: 0.3333 (0.3096)  loss_scale: 65536.0000 (51484.0654)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.2400  max mem: 15572
Epoch: [22]  [1660/2809]  eta: 0:10:49  lr: 0.000023  min_lr: 0.000000  loss: 3.9589 (3.8373)  class_acc: 0.2500 (0.3093)  loss_scale: 65536.0000 (51568.6647)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.2318  max mem: 15572
Epoch: [22]  [1670/2809]  eta: 0:10:43  lr: 0.000023  min_lr: 0.000000  loss: 3.7720 (3.8363)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (51652.2513)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1368  max mem: 15572
Epoch: [22]  [1680/2809]  eta: 0:10:38  lr: 0.000023  min_lr: 0.000000  loss: 3.6839 (3.8367)  class_acc: 0.3333 (0.3094)  loss_scale: 65536.0000 (51734.8435)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1003  max mem: 15572
Epoch: [22]  [1690/2809]  eta: 0:10:32  lr: 0.000023  min_lr: 0.000000  loss: 3.6839 (3.8363)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (51816.4589)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1398  max mem: 15572
Epoch: [22]  [1700/2809]  eta: 0:10:27  lr: 0.000023  min_lr: 0.000000  loss: 3.9269 (3.8361)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (51897.1146)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1325  max mem: 15572
Epoch: [22]  [1710/2809]  eta: 0:10:21  lr: 0.000023  min_lr: 0.000000  loss: 3.9617 (3.8372)  class_acc: 0.2500 (0.3092)  loss_scale: 65536.0000 (51976.8276)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0713  max mem: 15572
Epoch: [22]  [1720/2809]  eta: 0:10:15  lr: 0.000023  min_lr: 0.000000  loss: 3.8688 (3.8368)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (52055.6142)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0371  max mem: 15572
Epoch: [22]  [1730/2809]  eta: 0:10:08  lr: 0.000023  min_lr: 0.000000  loss: 3.7827 (3.8365)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (52133.4905)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0526  max mem: 15572
Epoch: [22]  [1740/2809]  eta: 0:10:03  lr: 0.000023  min_lr: 0.000000  loss: 3.8781 (3.8369)  class_acc: 0.2500 (0.3090)  loss_scale: 65536.0000 (52210.4721)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0868  max mem: 15572
[2025-01-16 01:09:36,373] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:09:36,373] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:09:38,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63544
[2025-01-16 01:09:38,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:09:38,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1750/2809]  eta: 0:09:57  lr: 0.000023  min_lr: 0.000000  loss: 3.8951 (3.8368)  class_acc: 0.2500 (0.3089)  loss_scale: 65536.0000 (52436.2856)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.1082  max mem: 15572
Epoch: [22]  [1760/2809]  eta: 0:09:51  lr: 0.000023  min_lr: 0.000000  loss: 3.9262 (3.8375)  class_acc: 0.2500 (0.3086)  loss_scale: 65536.0000 (52510.6735)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.1024  max mem: 15572
Epoch: [22]  [1770/2809]  eta: 0:09:45  lr: 0.000023  min_lr: 0.000000  loss: 3.9523 (3.8381)  class_acc: 0.2500 (0.3084)  loss_scale: 65536.0000 (52584.2213)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1038  max mem: 15572
Epoch: [22]  [1780/2809]  eta: 0:09:39  lr: 0.000023  min_lr: 0.000000  loss: 3.9425 (3.8386)  class_acc: 0.2917 (0.3083)  loss_scale: 65536.0000 (52656.9433)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0857  max mem: 15572
Epoch: [22]  [1790/2809]  eta: 0:09:34  lr: 0.000023  min_lr: 0.000000  loss: 3.8585 (3.8386)  class_acc: 0.2917 (0.3084)  loss_scale: 65536.0000 (52728.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.1083  max mem: 15572
Epoch: [22]  [1800/2809]  eta: 0:09:28  lr: 0.000023  min_lr: 0.000000  loss: 3.7453 (3.8378)  class_acc: 0.3750 (0.3088)  loss_scale: 65536.0000 (52799.9645)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.0953  max mem: 15572
Epoch: [22]  [1810/2809]  eta: 0:09:22  lr: 0.000023  min_lr: 0.000000  loss: 3.7116 (3.8372)  class_acc: 0.3333 (0.3088)  loss_scale: 65536.0000 (52870.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0697  max mem: 15572
Epoch: [22]  [1820/2809]  eta: 0:09:16  lr: 0.000023  min_lr: 0.000000  loss: 3.8709 (3.8383)  class_acc: 0.2500 (0.3085)  loss_scale: 65536.0000 (52939.8440)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0751  max mem: 15572
Epoch: [22]  [1830/2809]  eta: 0:09:11  lr: 0.000023  min_lr: 0.000000  loss: 3.9191 (3.8379)  class_acc: 0.2500 (0.3087)  loss_scale: 65536.0000 (53008.6379)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1334  max mem: 15572
Epoch: [22]  [1840/2809]  eta: 0:09:06  lr: 0.000023  min_lr: 0.000000  loss: 3.9320 (3.8379)  class_acc: 0.2917 (0.3089)  loss_scale: 65536.0000 (53076.6844)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.2042  max mem: 15572
Epoch: [22]  [1850/2809]  eta: 0:09:00  lr: 0.000023  min_lr: 0.000000  loss: 3.7665 (3.8376)  class_acc: 0.3333 (0.3088)  loss_scale: 65536.0000 (53143.9957)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.1474  max mem: 15572
Epoch: [22]  [1860/2809]  eta: 0:08:54  lr: 0.000023  min_lr: 0.000000  loss: 3.7570 (3.8375)  class_acc: 0.3333 (0.3090)  loss_scale: 65536.0000 (53210.5836)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0537  max mem: 15572
Epoch: [22]  [1870/2809]  eta: 0:08:48  lr: 0.000023  min_lr: 0.000000  loss: 4.0962 (3.8394)  class_acc: 0.2917 (0.3086)  loss_scale: 65536.0000 (53276.4596)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0863  max mem: 15572
[2025-01-16 01:10:48,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:10:48,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:10:51,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63676
[2025-01-16 01:10:51,874] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:10:51,874] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [1880/2809]  eta: 0:08:43  lr: 0.000023  min_lr: 0.000000  loss: 4.0962 (3.8398)  class_acc: 0.2500 (0.3084)  loss_scale: 65536.0000 (53446.1584)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1557  max mem: 15572
Epoch: [22]  [1890/2809]  eta: 0:08:37  lr: 0.000023  min_lr: 0.000000  loss: 3.9061 (3.8396)  class_acc: 0.2917 (0.3084)  loss_scale: 65536.0000 (53510.0920)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1136  max mem: 15572
Epoch: [22]  [1900/2809]  eta: 0:08:31  lr: 0.000023  min_lr: 0.000000  loss: 3.8029 (3.8394)  class_acc: 0.3750 (0.3085)  loss_scale: 65536.0000 (53573.3530)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1222  max mem: 15572
Epoch: [22]  [1910/2809]  eta: 0:08:26  lr: 0.000023  min_lr: 0.000000  loss: 3.8188 (3.8394)  class_acc: 0.3333 (0.3084)  loss_scale: 65536.0000 (53635.9519)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1651  max mem: 15572
Epoch: [22]  [1920/2809]  eta: 0:08:20  lr: 0.000023  min_lr: 0.000000  loss: 3.9892 (3.8404)  class_acc: 0.2500 (0.3082)  loss_scale: 65536.0000 (53697.8990)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1533  max mem: 15572
Epoch: [22]  [1930/2809]  eta: 0:08:14  lr: 0.000023  min_lr: 0.000000  loss: 4.0270 (3.8406)  class_acc: 0.2500 (0.3082)  loss_scale: 65536.0000 (53759.2046)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1082  max mem: 15572
[2025-01-16 01:11:21,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63730
[2025-01-16 01:11:21,485] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:11:21,485] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [1940/2809]  eta: 0:08:09  lr: 0.000023  min_lr: 0.000000  loss: 4.0270 (3.8413)  class_acc: 0.2917 (0.3082)  loss_scale: 65536.0000 (53667.9402)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1021  max mem: 15572
Epoch: [22]  [1950/2809]  eta: 0:08:03  lr: 0.000023  min_lr: 0.000000  loss: 3.9647 (3.8412)  class_acc: 0.2917 (0.3082)  loss_scale: 32768.0000 (53560.8160)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1231  max mem: 15572
Epoch: [22]  [1960/2809]  eta: 0:07:57  lr: 0.000023  min_lr: 0.000000  loss: 3.9100 (3.8416)  class_acc: 0.2500 (0.3081)  loss_scale: 32768.0000 (53454.7843)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0991  max mem: 15572
Epoch: [22]  [1970/2809]  eta: 0:07:52  lr: 0.000023  min_lr: 0.000000  loss: 3.8467 (3.8417)  class_acc: 0.2500 (0.3081)  loss_scale: 32768.0000 (53349.8285)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.1033  max mem: 15572
Epoch: [22]  [1980/2809]  eta: 0:07:46  lr: 0.000023  min_lr: 0.000000  loss: 3.6768 (3.8416)  class_acc: 0.2500 (0.3082)  loss_scale: 32768.0000 (53245.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1444  max mem: 15572
Epoch: [22]  [1990/2809]  eta: 0:07:40  lr: 0.000023  min_lr: 0.000000  loss: 3.9503 (3.8421)  class_acc: 0.2500 (0.3080)  loss_scale: 32768.0000 (53143.0799)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1011  max mem: 15572
Epoch: [22]  [2000/2809]  eta: 0:07:35  lr: 0.000023  min_lr: 0.000000  loss: 3.9415 (3.8417)  class_acc: 0.2500 (0.3078)  loss_scale: 32768.0000 (53041.2554)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0892  max mem: 15572
Epoch: [22]  [2010/2809]  eta: 0:07:29  lr: 0.000023  min_lr: 0.000000  loss: 3.9222 (3.8416)  class_acc: 0.2917 (0.3081)  loss_scale: 32768.0000 (52940.4436)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1125  max mem: 15572
Epoch: [22]  [2020/2809]  eta: 0:07:24  lr: 0.000023  min_lr: 0.000000  loss: 3.9222 (3.8420)  class_acc: 0.2917 (0.3079)  loss_scale: 32768.0000 (52840.6294)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1545  max mem: 15572
Epoch: [22]  [2030/2809]  eta: 0:07:18  lr: 0.000023  min_lr: 0.000000  loss: 3.7228 (3.8416)  class_acc: 0.2917 (0.3080)  loss_scale: 32768.0000 (52741.7981)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1384  max mem: 15572
Epoch: [22]  [2040/2809]  eta: 0:07:12  lr: 0.000023  min_lr: 0.000000  loss: 3.6629 (3.8408)  class_acc: 0.3333 (0.3081)  loss_scale: 32768.0000 (52643.9353)  weight_decay: 0.0500 (0.0500)  time: 0.5449  data: 0.1056  max mem: 15572
Epoch: [22]  [2050/2809]  eta: 0:07:07  lr: 0.000023  min_lr: 0.000000  loss: 3.8342 (3.8418)  class_acc: 0.3333 (0.3079)  loss_scale: 32768.0000 (52547.0268)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1830  max mem: 15572
Epoch: [22]  [2060/2809]  eta: 0:07:01  lr: 0.000023  min_lr: 0.000000  loss: 3.8980 (3.8403)  class_acc: 0.2917 (0.3082)  loss_scale: 32768.0000 (52451.0587)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0922  max mem: 15572
[2025-01-16 01:12:32,422] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:12:32,422] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2070/2809]  eta: 0:06:55  lr: 0.000023  min_lr: 0.000000  loss: 3.9133 (3.8422)  class_acc: 0.2500 (0.3077)  loss_scale: 32768.0000 (52514.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0713  max mem: 15572
Epoch: [22]  [2080/2809]  eta: 0:06:49  lr: 0.000023  min_lr: 0.000000  loss: 4.0257 (3.8430)  class_acc: 0.2500 (0.3078)  loss_scale: 65536.0000 (52576.8150)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1293  max mem: 15572
Epoch: [22]  [2090/2809]  eta: 0:06:44  lr: 0.000023  min_lr: 0.000000  loss: 4.0045 (3.8433)  class_acc: 0.2500 (0.3079)  loss_scale: 65536.0000 (52638.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1117  max mem: 15572
Epoch: [22]  [2100/2809]  eta: 0:06:38  lr: 0.000023  min_lr: 0.000000  loss: 3.9365 (3.8444)  class_acc: 0.2500 (0.3078)  loss_scale: 65536.0000 (52700.1771)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0640  max mem: 15572
[2025-01-16 01:12:58,424] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 63905
[2025-01-16 01:12:58,424] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:12:58,425] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2110/2809]  eta: 0:06:32  lr: 0.000023  min_lr: 0.000000  loss: 3.8652 (3.8442)  class_acc: 0.2917 (0.3079)  loss_scale: 65536.0000 (52698.8915)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0872  max mem: 15572
Epoch: [22]  [2120/2809]  eta: 0:06:27  lr: 0.000023  min_lr: 0.000000  loss: 3.8162 (3.8442)  class_acc: 0.2917 (0.3078)  loss_scale: 32768.0000 (52604.9222)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1209  max mem: 15572
Epoch: [22]  [2130/2809]  eta: 0:06:21  lr: 0.000023  min_lr: 0.000000  loss: 3.8482 (3.8442)  class_acc: 0.2917 (0.3077)  loss_scale: 32768.0000 (52511.8348)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1317  max mem: 15572
Epoch: [22]  [2140/2809]  eta: 0:06:16  lr: 0.000023  min_lr: 0.000000  loss: 3.9238 (3.8443)  class_acc: 0.2917 (0.3076)  loss_scale: 32768.0000 (52419.6170)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1403  max mem: 15572
Epoch: [22]  [2150/2809]  eta: 0:06:10  lr: 0.000023  min_lr: 0.000000  loss: 3.8105 (3.8440)  class_acc: 0.2917 (0.3077)  loss_scale: 32768.0000 (52328.2566)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1018  max mem: 15572
Epoch: [22]  [2160/2809]  eta: 0:06:04  lr: 0.000023  min_lr: 0.000000  loss: 3.9873 (3.8444)  class_acc: 0.2917 (0.3078)  loss_scale: 32768.0000 (52237.7418)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1361  max mem: 15572
Epoch: [22]  [2170/2809]  eta: 0:05:59  lr: 0.000023  min_lr: 0.000000  loss: 3.9873 (3.8445)  class_acc: 0.2917 (0.3078)  loss_scale: 32768.0000 (52148.0608)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1161  max mem: 15572
Epoch: [22]  [2180/2809]  eta: 0:05:53  lr: 0.000023  min_lr: 0.000000  loss: 3.8469 (3.8439)  class_acc: 0.2917 (0.3080)  loss_scale: 32768.0000 (52059.2022)  weight_decay: 0.0500 (0.0500)  time: 0.5014  data: 0.0507  max mem: 15572
Epoch: [22]  [2190/2809]  eta: 0:05:48  lr: 0.000023  min_lr: 0.000000  loss: 3.7449 (3.8433)  class_acc: 0.3333 (0.3082)  loss_scale: 32768.0000 (51971.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1351  max mem: 15572
Epoch: [22]  [2200/2809]  eta: 0:05:42  lr: 0.000023  min_lr: 0.000000  loss: 3.8670 (3.8432)  class_acc: 0.2917 (0.3082)  loss_scale: 32768.0000 (51883.9073)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1439  max mem: 15572
[2025-01-16 01:13:50,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=64000, skipped=428, lr=[2.2153745296679163e-07, 2.2153745296679163e-07, 3.164820756668452e-07, 3.164820756668452e-07, 4.5211725095263607e-07, 4.5211725095263607e-07, 6.458817870751944e-07, 6.458817870751944e-07, 9.226882672502778e-07, 9.226882672502778e-07, 1.3181260960718254e-06, 1.3181260960718254e-06, 1.8830372801026078e-06, 1.8830372801026078e-06, 2.69005325728944e-06, 2.69005325728944e-06, 3.8429332246992e-06, 3.8429332246992e-06, 5.489904606713144e-06, 5.489904606713144e-06, 7.842720866733062e-06, 7.842720866733062e-06, 1.1203886952475804e-05, 1.1203886952475804e-05, 1.600555278925115e-05, 1.600555278925115e-05, 2.286507541321593e-05, 2.286507541321593e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:13:50,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=64000/global_step=64000, RunningAvgSamplesPerSec=28.53528367691529, CurrSamplesPerSec=31.540922584077652, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [22]  [2210/2809]  eta: 0:05:36  lr: 0.000023  min_lr: 0.000000  loss: 3.8753 (3.8439)  class_acc: 0.2917 (0.3080)  loss_scale: 32768.0000 (51797.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1056  max mem: 15572
Epoch: [22]  [2220/2809]  eta: 0:05:30  lr: 0.000023  min_lr: 0.000000  loss: 3.8983 (3.8447)  class_acc: 0.2500 (0.3078)  loss_scale: 32768.0000 (51711.7695)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1023  max mem: 15572
Epoch: [22]  [2230/2809]  eta: 0:05:25  lr: 0.000023  min_lr: 0.000000  loss: 3.8867 (3.8451)  class_acc: 0.2917 (0.3077)  loss_scale: 32768.0000 (51626.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1172  max mem: 15572
[2025-01-16 01:14:11,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:14:11,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2240/2809]  eta: 0:05:19  lr: 0.000023  min_lr: 0.000000  loss: 3.7751 (3.8426)  class_acc: 0.3333 (0.3083)  loss_scale: 32768.0000 (51615.8144)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1400  max mem: 15572
Epoch: [22]  [2250/2809]  eta: 0:05:14  lr: 0.000023  min_lr: 0.000000  loss: 3.7806 (3.8430)  class_acc: 0.3333 (0.3083)  loss_scale: 65536.0000 (51677.6544)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1222  max mem: 15572
Epoch: [22]  [2260/2809]  eta: 0:05:08  lr: 0.000023  min_lr: 0.000000  loss: 3.9294 (3.8432)  class_acc: 0.2500 (0.3082)  loss_scale: 65536.0000 (51738.9474)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1114  max mem: 15572
Epoch: [22]  [2270/2809]  eta: 0:05:02  lr: 0.000023  min_lr: 0.000000  loss: 3.7949 (3.8432)  class_acc: 0.2500 (0.3080)  loss_scale: 65536.0000 (51799.7006)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1100  max mem: 15572
Epoch: [22]  [2280/2809]  eta: 0:04:57  lr: 0.000023  min_lr: 0.000000  loss: 3.8152 (3.8437)  class_acc: 0.2917 (0.3077)  loss_scale: 65536.0000 (51859.9211)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1074  max mem: 15572
Epoch: [22]  [2290/2809]  eta: 0:04:51  lr: 0.000023  min_lr: 0.000000  loss: 3.8399 (3.8435)  class_acc: 0.2500 (0.3076)  loss_scale: 65536.0000 (51919.6159)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0369  max mem: 15572
Epoch: [22]  [2300/2809]  eta: 0:04:45  lr: 0.000023  min_lr: 0.000000  loss: 3.8846 (3.8434)  class_acc: 0.2500 (0.3077)  loss_scale: 65536.0000 (51978.7918)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0478  max mem: 15572
Epoch: [22]  [2310/2809]  eta: 0:04:40  lr: 0.000023  min_lr: 0.000000  loss: 3.8160 (3.8428)  class_acc: 0.3333 (0.3080)  loss_scale: 65536.0000 (52037.4556)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1422  max mem: 15572
Epoch: [22]  [2320/2809]  eta: 0:04:34  lr: 0.000023  min_lr: 0.000000  loss: 3.7202 (3.8425)  class_acc: 0.3333 (0.3080)  loss_scale: 65536.0000 (52095.6140)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1639  max mem: 15572
Epoch: [22]  [2330/2809]  eta: 0:04:29  lr: 0.000023  min_lr: 0.000000  loss: 3.5489 (3.8416)  class_acc: 0.3333 (0.3082)  loss_scale: 65536.0000 (52153.2733)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.1655  max mem: 15572
Epoch: [22]  [2340/2809]  eta: 0:04:23  lr: 0.000023  min_lr: 0.000000  loss: 3.5984 (3.8413)  class_acc: 0.3333 (0.3082)  loss_scale: 65536.0000 (52210.4400)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1444  max mem: 15572
Epoch: [22]  [2350/2809]  eta: 0:04:17  lr: 0.000023  min_lr: 0.000000  loss: 3.8004 (3.8410)  class_acc: 0.2917 (0.3082)  loss_scale: 65536.0000 (52267.1204)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0698  max mem: 15572
Epoch: [22]  [2360/2809]  eta: 0:04:12  lr: 0.000023  min_lr: 0.000000  loss: 3.8004 (3.8401)  class_acc: 0.3333 (0.3084)  loss_scale: 65536.0000 (52323.3206)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1469  max mem: 15572
[2025-01-16 01:15:22,679] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:15:22,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:15:24,196] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64164
[2025-01-16 01:15:24,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:15:24,196] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [2370/2809]  eta: 0:04:06  lr: 0.000023  min_lr: 0.000000  loss: 3.8034 (3.8401)  class_acc: 0.3333 (0.3084)  loss_scale: 65536.0000 (52434.3281)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1595  max mem: 15572
Epoch: [22]  [2380/2809]  eta: 0:04:01  lr: 0.000023  min_lr: 0.000000  loss: 4.1288 (3.8415)  class_acc: 0.2083 (0.3079)  loss_scale: 65536.0000 (52489.3541)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0824  max mem: 15572
Epoch: [22]  [2390/2809]  eta: 0:03:55  lr: 0.000023  min_lr: 0.000000  loss: 4.1523 (3.8417)  class_acc: 0.2083 (0.3078)  loss_scale: 65536.0000 (52543.9197)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1236  max mem: 15572
Epoch: [22]  [2400/2809]  eta: 0:03:49  lr: 0.000023  min_lr: 0.000000  loss: 3.7462 (3.8405)  class_acc: 0.3333 (0.3082)  loss_scale: 65536.0000 (52598.0308)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1187  max mem: 15572
Epoch: [22]  [2410/2809]  eta: 0:03:44  lr: 0.000023  min_lr: 0.000000  loss: 3.7520 (3.8404)  class_acc: 0.3750 (0.3083)  loss_scale: 65536.0000 (52651.6931)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1327  max mem: 15572
Epoch: [22]  [2420/2809]  eta: 0:03:38  lr: 0.000023  min_lr: 0.000000  loss: 3.7994 (3.8405)  class_acc: 0.2917 (0.3083)  loss_scale: 65536.0000 (52704.9120)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1379  max mem: 15572
Epoch: [22]  [2430/2809]  eta: 0:03:32  lr: 0.000023  min_lr: 0.000000  loss: 3.7714 (3.8400)  class_acc: 0.3333 (0.3084)  loss_scale: 65536.0000 (52757.6931)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1068  max mem: 15572
Epoch: [22]  [2440/2809]  eta: 0:03:27  lr: 0.000023  min_lr: 0.000000  loss: 3.7714 (3.8400)  class_acc: 0.3333 (0.3084)  loss_scale: 65536.0000 (52810.0418)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1067  max mem: 15572
Epoch: [22]  [2450/2809]  eta: 0:03:21  lr: 0.000023  min_lr: 0.000000  loss: 3.8238 (3.8398)  class_acc: 0.2917 (0.3083)  loss_scale: 65536.0000 (52861.9633)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0765  max mem: 15572
Epoch: [22]  [2460/2809]  eta: 0:03:15  lr: 0.000023  min_lr: 0.000000  loss: 3.6023 (3.8388)  class_acc: 0.3333 (0.3086)  loss_scale: 65536.0000 (52913.4628)  weight_decay: 0.0500 (0.0500)  time: 0.4926  data: 0.0542  max mem: 15572
Epoch: [22]  [2470/2809]  eta: 0:03:10  lr: 0.000023  min_lr: 0.000000  loss: 3.7982 (3.8391)  class_acc: 0.3333 (0.3085)  loss_scale: 65536.0000 (52964.5455)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1070  max mem: 15572
Epoch: [22]  [2480/2809]  eta: 0:03:04  lr: 0.000023  min_lr: 0.000000  loss: 3.8739 (3.8388)  class_acc: 0.2917 (0.3086)  loss_scale: 65536.0000 (53015.2164)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1530  max mem: 15572
[2025-01-16 01:16:29,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64280
[2025-01-16 01:16:29,587] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:16:29,587] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [2490/2809]  eta: 0:02:59  lr: 0.000023  min_lr: 0.000000  loss: 3.7641 (3.8386)  class_acc: 0.3333 (0.3086)  loss_scale: 65536.0000 (52947.0895)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1592  max mem: 15572
Epoch: [22]  [2500/2809]  eta: 0:02:53  lr: 0.000023  min_lr: 0.000000  loss: 3.8860 (3.8389)  class_acc: 0.3333 (0.3086)  loss_scale: 32768.0000 (52866.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1200  max mem: 15572
Epoch: [22]  [2510/2809]  eta: 0:02:47  lr: 0.000023  min_lr: 0.000000  loss: 3.9893 (3.8391)  class_acc: 0.2917 (0.3088)  loss_scale: 32768.0000 (52786.3640)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0634  max mem: 15572
Epoch: [22]  [2520/2809]  eta: 0:02:42  lr: 0.000023  min_lr: 0.000000  loss: 4.0213 (3.8395)  class_acc: 0.2917 (0.3087)  loss_scale: 32768.0000 (52706.9576)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.1126  max mem: 15572
Epoch: [22]  [2530/2809]  eta: 0:02:36  lr: 0.000023  min_lr: 0.000000  loss: 3.6982 (3.8389)  class_acc: 0.2917 (0.3090)  loss_scale: 32768.0000 (52628.1786)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1592  max mem: 15572
Epoch: [22]  [2540/2809]  eta: 0:02:31  lr: 0.000023  min_lr: 0.000000  loss: 3.7177 (3.8389)  class_acc: 0.3333 (0.3091)  loss_scale: 32768.0000 (52550.0197)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1394  max mem: 15572
Epoch: [22]  [2550/2809]  eta: 0:02:25  lr: 0.000023  min_lr: 0.000000  loss: 3.6876 (3.8377)  class_acc: 0.3750 (0.3094)  loss_scale: 32768.0000 (52472.4735)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0830  max mem: 15572
Epoch: [22]  [2560/2809]  eta: 0:02:19  lr: 0.000023  min_lr: 0.000000  loss: 3.7089 (3.8380)  class_acc: 0.2917 (0.3094)  loss_scale: 32768.0000 (52395.5330)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1196  max mem: 15572
Epoch: [22]  [2570/2809]  eta: 0:02:14  lr: 0.000023  min_lr: 0.000000  loss: 3.8295 (3.8379)  class_acc: 0.2917 (0.3093)  loss_scale: 32768.0000 (52319.1910)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1716  max mem: 15572
Epoch: [22]  [2580/2809]  eta: 0:02:08  lr: 0.000023  min_lr: 0.000000  loss: 3.8295 (3.8381)  class_acc: 0.2917 (0.3092)  loss_scale: 32768.0000 (52243.4405)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1224  max mem: 15572
Epoch: [22]  [2590/2809]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000000  loss: 3.7588 (3.8376)  class_acc: 0.2917 (0.3094)  loss_scale: 32768.0000 (52168.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1398  max mem: 15572
Epoch: [22]  [2600/2809]  eta: 0:01:57  lr: 0.000023  min_lr: 0.000000  loss: 3.7376 (3.8377)  class_acc: 0.2917 (0.3091)  loss_scale: 32768.0000 (52093.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1041  max mem: 15572
Epoch: [22]  [2610/2809]  eta: 0:01:51  lr: 0.000023  min_lr: 0.000000  loss: 3.9037 (3.8376)  class_acc: 0.2500 (0.3092)  loss_scale: 32768.0000 (52019.6706)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0879  max mem: 15572
[2025-01-16 01:17:42,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:17:42,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [2620/2809]  eta: 0:01:46  lr: 0.000023  min_lr: 0.000000  loss: 4.0107 (3.8387)  class_acc: 0.2917 (0.3091)  loss_scale: 32768.0000 (52071.2400)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1825  max mem: 15572
Epoch: [22]  [2630/2809]  eta: 0:01:40  lr: 0.000023  min_lr: 0.000000  loss: 3.9208 (3.8381)  class_acc: 0.2917 (0.3091)  loss_scale: 65536.0000 (52122.4173)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1205  max mem: 15572
Epoch: [22]  [2640/2809]  eta: 0:01:34  lr: 0.000023  min_lr: 0.000000  loss: 3.6054 (3.8376)  class_acc: 0.3333 (0.3092)  loss_scale: 65536.0000 (52173.2071)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0466  max mem: 15572
Epoch: [22]  [2650/2809]  eta: 0:01:29  lr: 0.000023  min_lr: 0.000000  loss: 3.6440 (3.8368)  class_acc: 0.3750 (0.3095)  loss_scale: 65536.0000 (52223.6137)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1145  max mem: 15572
Epoch: [22]  [2660/2809]  eta: 0:01:23  lr: 0.000023  min_lr: 0.000000  loss: 3.8315 (3.8372)  class_acc: 0.2917 (0.3093)  loss_scale: 65536.0000 (52273.6415)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1070  max mem: 15572
Epoch: [22]  [2670/2809]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000000  loss: 3.8866 (3.8370)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (52323.2946)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.0920  max mem: 15572
Epoch: [22]  [2680/2809]  eta: 0:01:12  lr: 0.000023  min_lr: 0.000000  loss: 3.7084 (3.8368)  class_acc: 0.3750 (0.3097)  loss_scale: 65536.0000 (52372.5774)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0976  max mem: 15572
Epoch: [22]  [2690/2809]  eta: 0:01:06  lr: 0.000022  min_lr: 0.000000  loss: 3.8131 (3.8375)  class_acc: 0.3750 (0.3095)  loss_scale: 65536.0000 (52421.4939)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1118  max mem: 15572
Epoch: [22]  [2700/2809]  eta: 0:01:01  lr: 0.000022  min_lr: 0.000000  loss: 3.7233 (3.8367)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (52470.0481)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0911  max mem: 15572
Epoch: [22]  [2710/2809]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000000  loss: 3.7193 (3.8363)  class_acc: 0.3333 (0.3096)  loss_scale: 65536.0000 (52518.2442)  weight_decay: 0.0500 (0.0500)  time: 0.4689  data: 0.0122  max mem: 15572
Epoch: [22]  [2720/2809]  eta: 0:00:49  lr: 0.000022  min_lr: 0.000000  loss: 3.7906 (3.8366)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (52566.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1001  max mem: 15572
Epoch: [22]  [2730/2809]  eta: 0:00:44  lr: 0.000022  min_lr: 0.000000  loss: 3.7906 (3.8358)  class_acc: 0.3333 (0.3097)  loss_scale: 65536.0000 (52613.5774)  weight_decay: 0.0500 (0.0500)  time: 0.6261  data: 0.1841  max mem: 15572
[2025-01-16 01:18:51,745] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:18:51,745] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [2740/2809]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000000  loss: 3.7568 (3.8356)  class_acc: 0.3333 (0.3096)  loss_scale: 65536.0000 (52708.5414)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1335  max mem: 15572
[2025-01-16 01:18:53,158] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64539
[2025-01-16 01:18:53,158] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:18:53,158] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [2750/2809]  eta: 0:00:33  lr: 0.000022  min_lr: 0.000000  loss: 3.7865 (3.8354)  class_acc: 0.2917 (0.3096)  loss_scale: 65536.0000 (52755.1698)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.1764  max mem: 15572
Epoch: [22]  [2760/2809]  eta: 0:00:27  lr: 0.000022  min_lr: 0.000000  loss: 3.7865 (3.8353)  class_acc: 0.2917 (0.3096)  loss_scale: 65536.0000 (52801.4603)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1427  max mem: 15572
Epoch: [22]  [2770/2809]  eta: 0:00:21  lr: 0.000022  min_lr: 0.000000  loss: 3.8408 (3.8353)  class_acc: 0.2917 (0.3096)  loss_scale: 65536.0000 (52847.4168)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0897  max mem: 15572
Epoch: [22]  [2780/2809]  eta: 0:00:16  lr: 0.000022  min_lr: 0.000000  loss: 4.0713 (3.8361)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (52893.0428)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1305  max mem: 15572
Epoch: [22]  [2790/2809]  eta: 0:00:10  lr: 0.000022  min_lr: 0.000000  loss: 4.0891 (3.8364)  class_acc: 0.2917 (0.3096)  loss_scale: 65536.0000 (52938.3418)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.1618  max mem: 15572
Epoch: [22]  [2800/2809]  eta: 0:00:05  lr: 0.000022  min_lr: 0.000000  loss: 3.8611 (3.8369)  class_acc: 0.2500 (0.3094)  loss_scale: 65536.0000 (52983.3174)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1438  max mem: 15572
Epoch: [22]  [2808/2809]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000000  loss: 3.9103 (3.8373)  class_acc: 0.2083 (0.3092)  loss_scale: 65536.0000 (53019.0673)  weight_decay: 0.0500 (0.0500)  time: 0.4485  data: 0.0386  max mem: 15572
Epoch: [22] Total time: 0:26:18 (0.5620 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000000  loss: 3.9103 (3.8373)  class_acc: 0.2083 (0.3092)  loss_scale: 65536.0000 (53019.0673)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:59  loss: 0.3566 (0.3566)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.6291  data: 4.4507  max mem: 15572
Val:  [ 10/272]  eta: 0:03:20  loss: 2.4917 (2.4261)  acc1: 38.8889 (42.9293)  acc5: 72.2222 (68.6869)  time: 0.7658  data: 0.5711  max mem: 15572
Val:  [ 20/272]  eta: 0:02:11  loss: 2.4796 (2.4223)  acc1: 50.0000 (44.9735)  acc5: 72.2222 (71.6931)  time: 0.3162  data: 0.1260  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.4625 (2.5095)  acc1: 50.0000 (40.5018)  acc5: 72.2222 (70.9677)  time: 0.2590  data: 0.0707  max mem: 15572
Val:  [ 40/272]  eta: 0:01:34  loss: 2.5647 (2.5407)  acc1: 27.7778 (38.4824)  acc5: 72.2222 (71.4092)  time: 0.2843  data: 0.0949  max mem: 15572
Val:  [ 50/272]  eta: 0:01:27  loss: 2.4765 (2.4646)  acc1: 27.7778 (39.8693)  acc5: 77.7778 (73.5294)  time: 0.3281  data: 0.1347  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 1.5679 (2.3572)  acc1: 55.5556 (43.0783)  acc5: 88.8889 (74.7723)  time: 0.3363  data: 0.1437  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.5950 (2.2672)  acc1: 61.1111 (45.6182)  acc5: 88.8889 (76.1346)  time: 0.3254  data: 0.1250  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.9568 (2.2904)  acc1: 55.5556 (45.3361)  acc5: 83.3333 (75.7888)  time: 0.2973  data: 0.0956  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 2.3136 (2.3030)  acc1: 50.0000 (45.9096)  acc5: 77.7778 (76.1905)  time: 0.3185  data: 0.1192  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.3136 (2.3241)  acc1: 50.0000 (45.2145)  acc5: 83.3333 (75.9626)  time: 0.3290  data: 0.1143  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.5693 (2.4001)  acc1: 16.6667 (43.0430)  acc5: 66.6667 (74.9750)  time: 0.2846  data: 0.0764  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.0371 (2.4424)  acc1: 16.6667 (42.0569)  acc5: 66.6667 (74.2424)  time: 0.3063  data: 0.1115  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.2779 (2.4043)  acc1: 44.4444 (43.1298)  acc5: 77.7778 (74.9788)  time: 0.3031  data: 0.1152  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.7630 (2.3960)  acc1: 55.5556 (43.6170)  acc5: 83.3333 (74.6257)  time: 0.2827  data: 0.0938  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.3661 (2.3963)  acc1: 33.3333 (42.9360)  acc5: 77.7778 (74.9816)  time: 0.3192  data: 0.1200  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.3700 (2.3904)  acc1: 44.4444 (43.6163)  acc5: 77.7778 (75.1553)  time: 0.3524  data: 0.1587  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.5197 (2.4154)  acc1: 38.8889 (42.7225)  acc5: 72.2222 (74.5289)  time: 0.2918  data: 0.1155  max mem: 15572
Val:  [180/272]  eta: 0:00:29  loss: 2.4639 (2.4072)  acc1: 27.7778 (42.2652)  acc5: 72.2222 (74.8619)  time: 0.1941  data: 0.0299  max mem: 15572
Val:  [190/272]  eta: 0:00:25  loss: 2.4738 (2.4600)  acc1: 22.2222 (40.9540)  acc5: 77.7778 (73.5311)  time: 0.1599  data: 0.0003  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.5926 (2.4630)  acc1: 22.2222 (40.8513)  acc5: 66.6667 (73.4107)  time: 0.1786  data: 0.0006  max mem: 15572
Val:  [210/272]  eta: 0:00:18  loss: 2.0844 (2.4570)  acc1: 50.0000 (41.2849)  acc5: 77.7778 (73.5387)  time: 0.2038  data: 0.0117  max mem: 15572
Val:  [220/272]  eta: 0:00:15  loss: 2.1251 (2.4478)  acc1: 50.0000 (41.6290)  acc5: 77.7778 (73.7054)  time: 0.2659  data: 0.0784  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.8707 (2.4184)  acc1: 61.1111 (42.7609)  acc5: 77.7778 (74.0260)  time: 0.3630  data: 0.1733  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 1.6390 (2.4013)  acc1: 61.1111 (43.1996)  acc5: 83.3333 (74.3661)  time: 0.3878  data: 0.1974  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.2060 (2.4099)  acc1: 44.4444 (42.6737)  acc5: 72.2222 (74.2585)  time: 0.3565  data: 0.1613  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.0850 (2.3445)  acc1: 72.2222 (44.5509)  acc5: 88.8889 (75.0532)  time: 0.3431  data: 0.1487  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3713 (2.3410)  acc1: 72.2222 (44.5674)  acc5: 88.8889 (75.2358)  time: 0.3070  data: 0.1274  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3713 (2.3461)  acc1: 66.6667 (44.5423)  acc5: 88.8889 (75.1997)  time: 0.3006  data: 0.1274  max mem: 15572
Val: Total time: 0:01:25 (0.3139 s / it)
* Acc@1 44.542 Acc@5 75.200 loss 2.346
Accuracy of the network on the 4883 val videos: 44.5%
[2025-01-16 01:20:56,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 01:20:56,469] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 01:20:56,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 01:20:58,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 01:20:58,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.54%
Epoch: [23]  [   0/2809]  eta: 7:48:41  lr: 0.000022  min_lr: 0.000000  loss: 3.7736 (3.7736)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 10.0113  data: 9.5612  max mem: 15572
Epoch: [23]  [  10/2809]  eta: 1:14:18  lr: 0.000022  min_lr: 0.000000  loss: 3.8770 (3.8763)  class_acc: 0.2917 (0.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5930  data: 1.1305  max mem: 15572
Epoch: [23]  [  20/2809]  eta: 0:55:34  lr: 0.000022  min_lr: 0.000000  loss: 3.8783 (3.9160)  class_acc: 0.2917 (0.3095)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7550  data: 0.2973  max mem: 15572
Epoch: [23]  [  30/2809]  eta: 0:45:23  lr: 0.000022  min_lr: 0.000000  loss: 3.8414 (3.7926)  class_acc: 0.3333 (0.3239)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.1858  max mem: 15572
Epoch: [23]  [  40/2809]  eta: 0:41:38  lr: 0.000022  min_lr: 0.000000  loss: 3.5941 (3.7872)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1295  max mem: 15572
Epoch: [23]  [  50/2809]  eta: 0:38:59  lr: 0.000022  min_lr: 0.000000  loss: 3.6990 (3.7664)  class_acc: 0.3333 (0.3325)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.1755  max mem: 15572
Epoch: [23]  [  60/2809]  eta: 0:37:18  lr: 0.000022  min_lr: 0.000000  loss: 3.6990 (3.7241)  class_acc: 0.3750 (0.3381)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1747  max mem: 15572
[2025-01-16 01:21:49,337] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:21:49,338] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:21:49,743] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64669
[2025-01-16 01:21:49,743] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:21:49,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [  70/2809]  eta: 0:35:06  lr: 0.000022  min_lr: 0.000000  loss: 3.6400 (3.7336)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1200  max mem: 15572
Epoch: [23]  [  80/2809]  eta: 0:33:06  lr: 0.000022  min_lr: 0.000000  loss: 3.8106 (3.7602)  class_acc: 0.2917 (0.3246)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.4638  data: 0.0452  max mem: 15572
Epoch: [23]  [  90/2809]  eta: 0:31:32  lr: 0.000022  min_lr: 0.000000  loss: 3.8638 (3.7607)  class_acc: 0.3333 (0.3260)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.4371  data: 0.0221  max mem: 15572
Epoch: [23]  [ 100/2809]  eta: 0:30:18  lr: 0.000022  min_lr: 0.000000  loss: 3.8756 (3.7750)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.4413  data: 0.0036  max mem: 15572
Epoch: [23]  [ 110/2809]  eta: 0:29:26  lr: 0.000022  min_lr: 0.000000  loss: 3.8756 (3.7689)  class_acc: 0.2917 (0.3191)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.4664  data: 0.0259  max mem: 15572
Epoch: [23]  [ 120/2809]  eta: 0:29:32  lr: 0.000022  min_lr: 0.000000  loss: 3.7012 (3.7694)  class_acc: 0.2083 (0.3158)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1530  max mem: 15572
Epoch: [23]  [ 130/2809]  eta: 0:28:56  lr: 0.000022  min_lr: 0.000000  loss: 3.6272 (3.7715)  class_acc: 0.2500 (0.3155)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1636  max mem: 15572
Epoch: [23]  [ 140/2809]  eta: 0:28:50  lr: 0.000022  min_lr: 0.000000  loss: 3.9526 (3.7806)  class_acc: 0.3333 (0.3168)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1279  max mem: 15572
Epoch: [23]  [ 150/2809]  eta: 0:28:49  lr: 0.000022  min_lr: 0.000000  loss: 3.8378 (3.7744)  class_acc: 0.3333 (0.3209)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2159  max mem: 15572
Epoch: [23]  [ 160/2809]  eta: 0:28:29  lr: 0.000022  min_lr: 0.000000  loss: 3.7085 (3.7843)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1884  max mem: 15572
Epoch: [23]  [ 170/2809]  eta: 0:28:01  lr: 0.000022  min_lr: 0.000000  loss: 3.6738 (3.7749)  class_acc: 0.3333 (0.3209)  loss_scale: 65536.0000 (65919.2515)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.1084  max mem: 15572
Epoch: [23]  [ 180/2809]  eta: 0:27:38  lr: 0.000022  min_lr: 0.000000  loss: 3.7297 (3.7811)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (65898.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5151  data: 0.0850  max mem: 15572
Epoch: [23]  [ 190/2809]  eta: 0:27:28  lr: 0.000022  min_lr: 0.000000  loss: 3.5938 (3.7700)  class_acc: 0.3333 (0.3209)  loss_scale: 65536.0000 (65879.1204)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1256  max mem: 15572
[2025-01-16 01:22:59,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:22:59,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:23:00,800] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64801
[2025-01-16 01:23:00,801] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:23:00,801] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 200/2809]  eta: 0:27:12  lr: 0.000022  min_lr: 0.000000  loss: 3.5938 (3.7643)  class_acc: 0.3333 (0.3221)  loss_scale: 65536.0000 (66840.1990)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1470  max mem: 15572
Epoch: [23]  [ 210/2809]  eta: 0:26:52  lr: 0.000022  min_lr: 0.000000  loss: 3.6370 (3.7595)  class_acc: 0.3333 (0.3227)  loss_scale: 65536.0000 (66778.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.1044  max mem: 15572
Epoch: [23]  [ 220/2809]  eta: 0:26:44  lr: 0.000022  min_lr: 0.000000  loss: 3.6351 (3.7478)  class_acc: 0.3333 (0.3233)  loss_scale: 65536.0000 (66722.1719)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1212  max mem: 15572
Epoch: [23]  [ 230/2809]  eta: 0:26:15  lr: 0.000022  min_lr: 0.000000  loss: 3.7243 (3.7489)  class_acc: 0.3333 (0.3238)  loss_scale: 65536.0000 (66670.8225)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0822  max mem: 15572
Epoch: [23]  [ 240/2809]  eta: 0:26:03  lr: 0.000022  min_lr: 0.000000  loss: 3.8595 (3.7516)  class_acc: 0.2917 (0.3240)  loss_scale: 65536.0000 (66623.7344)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0369  max mem: 15572
Epoch: [23]  [ 250/2809]  eta: 0:25:58  lr: 0.000022  min_lr: 0.000000  loss: 3.8880 (3.7602)  class_acc: 0.3333 (0.3239)  loss_scale: 65536.0000 (66580.3984)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1261  max mem: 15572
Epoch: [23]  [ 260/2809]  eta: 0:25:54  lr: 0.000022  min_lr: 0.000000  loss: 3.8880 (3.7656)  class_acc: 0.3333 (0.3234)  loss_scale: 65536.0000 (66540.3831)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1799  max mem: 15572
Epoch: [23]  [ 270/2809]  eta: 0:25:36  lr: 0.000022  min_lr: 0.000000  loss: 3.8489 (3.7679)  class_acc: 0.3333 (0.3240)  loss_scale: 65536.0000 (66503.3210)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1062  max mem: 15572
Epoch: [23]  [ 280/2809]  eta: 0:25:22  lr: 0.000022  min_lr: 0.000000  loss: 3.7857 (3.7688)  class_acc: 0.3750 (0.3240)  loss_scale: 65536.0000 (66468.8968)  weight_decay: 0.0500 (0.0500)  time: 0.5024  data: 0.0532  max mem: 15572
Epoch: [23]  [ 290/2809]  eta: 0:25:14  lr: 0.000022  min_lr: 0.000000  loss: 3.7840 (3.7747)  class_acc: 0.2917 (0.3225)  loss_scale: 65536.0000 (66436.8385)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0879  max mem: 15572
Epoch: [23]  [ 300/2809]  eta: 0:24:57  lr: 0.000022  min_lr: 0.000000  loss: 3.7526 (3.7729)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (66406.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0563  max mem: 15572
Epoch: [23]  [ 310/2809]  eta: 0:24:46  lr: 0.000022  min_lr: 0.000000  loss: 3.7425 (3.7711)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (66378.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0648  max mem: 15572
Epoch: [23]  [ 320/2809]  eta: 0:24:40  lr: 0.000022  min_lr: 0.000000  loss: 3.8753 (3.7731)  class_acc: 0.2917 (0.3236)  loss_scale: 65536.0000 (66352.6480)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1487  max mem: 15572
[2025-01-16 01:24:11,337] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:24:11,337] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:24:11,733] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 64931
[2025-01-16 01:24:11,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:24:11,734] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 330/2809]  eta: 0:24:32  lr: 0.000022  min_lr: 0.000000  loss: 4.1541 (3.7805)  class_acc: 0.2500 (0.3211)  loss_scale: 65536.0000 (66525.9698)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1607  max mem: 15572
Epoch: [23]  [ 340/2809]  eta: 0:24:26  lr: 0.000022  min_lr: 0.000000  loss: 3.9468 (3.7811)  class_acc: 0.2083 (0.3195)  loss_scale: 65536.0000 (66496.9384)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1503  max mem: 15572
Epoch: [23]  [ 350/2809]  eta: 0:24:19  lr: 0.000022  min_lr: 0.000000  loss: 3.8130 (3.7824)  class_acc: 0.2083 (0.3187)  loss_scale: 65536.0000 (66469.5613)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1420  max mem: 15572
Epoch: [23]  [ 360/2809]  eta: 0:24:10  lr: 0.000022  min_lr: 0.000000  loss: 3.7921 (3.7855)  class_acc: 0.2917 (0.3189)  loss_scale: 65536.0000 (66443.7008)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1243  max mem: 15572
Epoch: [23]  [ 370/2809]  eta: 0:24:03  lr: 0.000022  min_lr: 0.000000  loss: 3.7835 (3.7850)  class_acc: 0.3333 (0.3200)  loss_scale: 65536.0000 (66419.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1221  max mem: 15572
Epoch: [23]  [ 380/2809]  eta: 0:23:57  lr: 0.000022  min_lr: 0.000000  loss: 3.6929 (3.7833)  class_acc: 0.3750 (0.3211)  loss_scale: 65536.0000 (66396.0525)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1287  max mem: 15572
Epoch: [23]  [ 390/2809]  eta: 0:23:48  lr: 0.000022  min_lr: 0.000000  loss: 3.8138 (3.7832)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (66374.0563)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1341  max mem: 15572
[2025-01-16 01:24:50,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=65000, skipped=434, lr=[2.1429492637226224e-07, 2.1429492637226224e-07, 3.061356091032318e-07, 3.061356091032318e-07, 4.3733658443318834e-07, 4.3733658443318834e-07, 6.24766549190269e-07, 6.24766549190269e-07, 8.925236417003845e-07, 8.925236417003845e-07, 1.275033773857692e-06, 1.275033773857692e-06, 1.821476819796703e-06, 1.821476819796703e-06, 2.602109742566719e-06, 2.602109742566719e-06, 3.71729963223817e-06, 3.71729963223817e-06, 5.310428046054529e-06, 5.310428046054529e-06, 7.586325780077899e-06, 7.586325780077899e-06, 1.0837608257254143e-05, 1.0837608257254143e-05, 1.548229751036306e-05, 1.548229751036306e-05, 2.211756787194723e-05, 2.211756787194723e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:24:50,842] [INFO] [timer.py:260:stop] epoch=0/micro_step=65000/global_step=65000, RunningAvgSamplesPerSec=28.53758469081866, CurrSamplesPerSec=31.85757037172067, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [ 400/2809]  eta: 0:23:44  lr: 0.000022  min_lr: 0.000000  loss: 3.9648 (3.7856)  class_acc: 0.2500 (0.3193)  loss_scale: 65536.0000 (66353.1571)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1553  max mem: 15572
Epoch: [23]  [ 410/2809]  eta: 0:23:34  lr: 0.000022  min_lr: 0.000000  loss: 3.9556 (3.7841)  class_acc: 0.2500 (0.3204)  loss_scale: 65536.0000 (66333.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1282  max mem: 15572
Epoch: [23]  [ 420/2809]  eta: 0:23:27  lr: 0.000022  min_lr: 0.000000  loss: 3.9556 (3.7907)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (66314.3373)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1155  max mem: 15572
Epoch: [23]  [ 430/2809]  eta: 0:23:23  lr: 0.000022  min_lr: 0.000000  loss: 3.8542 (3.7877)  class_acc: 0.2500 (0.3186)  loss_scale: 65536.0000 (66296.2784)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1767  max mem: 15572
Epoch: [23]  [ 440/2809]  eta: 0:23:16  lr: 0.000022  min_lr: 0.000000  loss: 3.7214 (3.7915)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (66279.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1571  max mem: 15572
Epoch: [23]  [ 450/2809]  eta: 0:23:06  lr: 0.000022  min_lr: 0.000000  loss: 3.9406 (3.7927)  class_acc: 0.2500 (0.3166)  loss_scale: 65536.0000 (66262.5632)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0895  max mem: 15572
[2025-01-16 01:25:25,469] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:25:25,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:25:25,903] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65061
[2025-01-16 01:25:25,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:25:25,903] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 460/2809]  eta: 0:22:59  lr: 0.000022  min_lr: 0.000000  loss: 3.8491 (3.7932)  class_acc: 0.2917 (0.3167)  loss_scale: 65536.0000 (66388.9631)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0941  max mem: 15572
Epoch: [23]  [ 470/2809]  eta: 0:22:50  lr: 0.000022  min_lr: 0.000000  loss: 3.7376 (3.7904)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (66370.8535)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1225  max mem: 15572
Epoch: [23]  [ 480/2809]  eta: 0:22:42  lr: 0.000022  min_lr: 0.000000  loss: 3.7768 (3.7920)  class_acc: 0.2917 (0.3170)  loss_scale: 65536.0000 (66353.4969)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1035  max mem: 15572
Epoch: [23]  [ 490/2809]  eta: 0:22:36  lr: 0.000022  min_lr: 0.000000  loss: 3.8306 (3.7930)  class_acc: 0.2917 (0.3163)  loss_scale: 65536.0000 (66336.8473)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0842  max mem: 15572
Epoch: [23]  [ 500/2809]  eta: 0:22:32  lr: 0.000022  min_lr: 0.000000  loss: 3.8717 (3.7965)  class_acc: 0.2083 (0.3149)  loss_scale: 65536.0000 (66320.8623)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.1513  max mem: 15572
Epoch: [23]  [ 510/2809]  eta: 0:22:22  lr: 0.000022  min_lr: 0.000000  loss: 3.8932 (3.7981)  class_acc: 0.2500 (0.3145)  loss_scale: 65536.0000 (66305.5029)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1288  max mem: 15572
Epoch: [23]  [ 520/2809]  eta: 0:22:13  lr: 0.000022  min_lr: 0.000000  loss: 3.8932 (3.7976)  class_acc: 0.2917 (0.3148)  loss_scale: 65536.0000 (66290.7332)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0561  max mem: 15572
Epoch: [23]  [ 530/2809]  eta: 0:22:03  lr: 0.000022  min_lr: 0.000000  loss: 3.4873 (3.7924)  class_acc: 0.3333 (0.3164)  loss_scale: 65536.0000 (66276.5198)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0376  max mem: 15572
Epoch: [23]  [ 540/2809]  eta: 0:21:57  lr: 0.000022  min_lr: 0.000000  loss: 3.5687 (3.7912)  class_acc: 0.3333 (0.3173)  loss_scale: 65536.0000 (66262.8318)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0678  max mem: 15572
Epoch: [23]  [ 550/2809]  eta: 0:21:52  lr: 0.000022  min_lr: 0.000000  loss: 3.7154 (3.7905)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (66249.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1556  max mem: 15572
Epoch: [23]  [ 560/2809]  eta: 0:21:46  lr: 0.000022  min_lr: 0.000000  loss: 3.9913 (3.7904)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (66236.9198)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1589  max mem: 15572
Epoch: [23]  [ 570/2809]  eta: 0:21:38  lr: 0.000022  min_lr: 0.000000  loss: 4.0168 (3.7947)  class_acc: 0.2500 (0.3171)  loss_scale: 65536.0000 (66224.6445)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1024  max mem: 15572
Epoch: [23]  [ 580/2809]  eta: 0:21:32  lr: 0.000022  min_lr: 0.000000  loss: 4.0168 (3.7983)  class_acc: 0.2500 (0.3165)  loss_scale: 65536.0000 (66212.7917)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1121  max mem: 15572
[2025-01-16 01:26:37,342] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:26:37,343] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:26:41,317] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65196
[2025-01-16 01:26:41,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:26:41,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 590/2809]  eta: 0:21:26  lr: 0.000022  min_lr: 0.000000  loss: 4.2166 (3.8025)  class_acc: 0.2500 (0.3158)  loss_scale: 65536.0000 (66866.6802)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1505  max mem: 15572
Epoch: [23]  [ 600/2809]  eta: 0:21:19  lr: 0.000022  min_lr: 0.000000  loss: 4.2014 (3.8083)  class_acc: 0.2500 (0.3149)  loss_scale: 65536.0000 (66844.5391)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1243  max mem: 15572
Epoch: [23]  [ 610/2809]  eta: 0:21:15  lr: 0.000022  min_lr: 0.000000  loss: 3.9663 (3.8079)  class_acc: 0.2917 (0.3148)  loss_scale: 65536.0000 (66823.1227)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1316  max mem: 15572
Epoch: [23]  [ 620/2809]  eta: 0:21:09  lr: 0.000022  min_lr: 0.000000  loss: 3.9178 (3.8141)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (66802.3961)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1601  max mem: 15572
Epoch: [23]  [ 630/2809]  eta: 0:21:04  lr: 0.000022  min_lr: 0.000000  loss: 3.9491 (3.8090)  class_acc: 0.3333 (0.3160)  loss_scale: 65536.0000 (66782.3265)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1638  max mem: 15572
Epoch: [23]  [ 640/2809]  eta: 0:20:53  lr: 0.000022  min_lr: 0.000000  loss: 3.7384 (3.8099)  class_acc: 0.3750 (0.3157)  loss_scale: 65536.0000 (66762.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0952  max mem: 15572
[2025-01-16 01:27:12,670] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65252
[2025-01-16 01:27:12,670] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:27:12,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 650/2809]  eta: 0:20:46  lr: 0.000022  min_lr: 0.000000  loss: 3.7981 (3.8092)  class_acc: 0.3750 (0.3159)  loss_scale: 65536.0000 (66442.0276)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0544  max mem: 15572
Epoch: [23]  [ 660/2809]  eta: 0:20:40  lr: 0.000022  min_lr: 0.000000  loss: 3.8469 (3.8087)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (65932.5870)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.0908  max mem: 15572
Epoch: [23]  [ 670/2809]  eta: 0:20:38  lr: 0.000022  min_lr: 0.000000  loss: 3.9038 (3.8098)  class_acc: 0.2500 (0.3151)  loss_scale: 32768.0000 (65438.3308)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1826  max mem: 15572
Epoch: [23]  [ 680/2809]  eta: 0:20:32  lr: 0.000022  min_lr: 0.000000  loss: 4.0511 (3.8112)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (64958.5903)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.2103  max mem: 15572
Epoch: [23]  [ 690/2809]  eta: 0:20:30  lr: 0.000022  min_lr: 0.000000  loss: 3.8051 (3.8118)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (64492.7352)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.2179  max mem: 15572
Epoch: [23]  [ 700/2809]  eta: 0:20:21  lr: 0.000022  min_lr: 0.000000  loss: 3.7051 (3.8085)  class_acc: 0.3333 (0.3156)  loss_scale: 32768.0000 (64040.1712)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1515  max mem: 15572
Epoch: [23]  [ 710/2809]  eta: 0:20:12  lr: 0.000022  min_lr: 0.000000  loss: 3.7060 (3.8110)  class_acc: 0.3333 (0.3155)  loss_scale: 32768.0000 (63600.3376)  weight_decay: 0.0500 (0.0500)  time: 0.4741  data: 0.0467  max mem: 15572
Epoch: [23]  [ 720/2809]  eta: 0:20:07  lr: 0.000022  min_lr: 0.000000  loss: 3.8854 (3.8114)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (63172.7046)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.1082  max mem: 15572
Epoch: [23]  [ 730/2809]  eta: 0:20:02  lr: 0.000022  min_lr: 0.000000  loss: 3.7003 (3.8102)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (62756.7715)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1630  max mem: 15572
Epoch: [23]  [ 740/2809]  eta: 0:19:54  lr: 0.000022  min_lr: 0.000000  loss: 3.8469 (3.8112)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (62352.0648)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1277  max mem: 15572
Epoch: [23]  [ 750/2809]  eta: 0:19:49  lr: 0.000022  min_lr: 0.000000  loss: 3.8921 (3.8126)  class_acc: 0.2917 (0.3149)  loss_scale: 32768.0000 (61958.1358)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1115  max mem: 15572
Epoch: [23]  [ 760/2809]  eta: 0:19:44  lr: 0.000022  min_lr: 0.000000  loss: 3.5898 (3.8069)  class_acc: 0.3750 (0.3165)  loss_scale: 32768.0000 (61574.5598)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1663  max mem: 15572
Epoch: [23]  [ 770/2809]  eta: 0:19:39  lr: 0.000022  min_lr: 0.000000  loss: 3.5898 (3.8096)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (61200.9339)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1775  max mem: 15572
[2025-01-16 01:28:26,759] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:28:26,759] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [ 780/2809]  eta: 0:19:32  lr: 0.000022  min_lr: 0.000000  loss: 4.0528 (3.8096)  class_acc: 0.2500 (0.3159)  loss_scale: 32768.0000 (61130.5711)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1170  max mem: 15572
Epoch: [23]  [ 790/2809]  eta: 0:19:27  lr: 0.000022  min_lr: 0.000000  loss: 3.8914 (3.8100)  class_acc: 0.3333 (0.3160)  loss_scale: 65536.0000 (61186.2655)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1352  max mem: 15572
Epoch: [23]  [ 800/2809]  eta: 0:19:20  lr: 0.000022  min_lr: 0.000000  loss: 3.8486 (3.8116)  class_acc: 0.3333 (0.3166)  loss_scale: 65536.0000 (61240.5693)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1252  max mem: 15572
Epoch: [23]  [ 810/2809]  eta: 0:19:13  lr: 0.000022  min_lr: 0.000000  loss: 3.8486 (3.8110)  class_acc: 0.3333 (0.3166)  loss_scale: 65536.0000 (61293.5339)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0474  max mem: 15572
Epoch: [23]  [ 820/2809]  eta: 0:19:05  lr: 0.000022  min_lr: 0.000000  loss: 3.8738 (3.8142)  class_acc: 0.2917 (0.3157)  loss_scale: 65536.0000 (61345.2083)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0580  max mem: 15572
Epoch: [23]  [ 830/2809]  eta: 0:18:59  lr: 0.000022  min_lr: 0.000000  loss: 4.1412 (3.8165)  class_acc: 0.2917 (0.3158)  loss_scale: 65536.0000 (61395.6390)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0554  max mem: 15572
Epoch: [23]  [ 840/2809]  eta: 0:18:51  lr: 0.000022  min_lr: 0.000000  loss: 3.8480 (3.8168)  class_acc: 0.3333 (0.3158)  loss_scale: 65536.0000 (61444.8704)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0504  max mem: 15572
Epoch: [23]  [ 850/2809]  eta: 0:18:46  lr: 0.000022  min_lr: 0.000000  loss: 3.7820 (3.8176)  class_acc: 0.2917 (0.3156)  loss_scale: 65536.0000 (61492.9448)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1146  max mem: 15572
Epoch: [23]  [ 860/2809]  eta: 0:18:39  lr: 0.000022  min_lr: 0.000000  loss: 3.8135 (3.8186)  class_acc: 0.2917 (0.3156)  loss_scale: 65536.0000 (61539.9024)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1377  max mem: 15572
Epoch: [23]  [ 870/2809]  eta: 0:18:32  lr: 0.000022  min_lr: 0.000000  loss: 3.7798 (3.8181)  class_acc: 0.2917 (0.3154)  loss_scale: 65536.0000 (61585.7819)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0858  max mem: 15572
Epoch: [23]  [ 880/2809]  eta: 0:18:26  lr: 0.000022  min_lr: 0.000000  loss: 3.7920 (3.8180)  class_acc: 0.2917 (0.3156)  loss_scale: 65536.0000 (61630.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0966  max mem: 15572
Epoch: [23]  [ 890/2809]  eta: 0:18:21  lr: 0.000022  min_lr: 0.000000  loss: 3.9025 (3.8193)  class_acc: 0.2917 (0.3157)  loss_scale: 65536.0000 (61674.4512)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1339  max mem: 15572
Epoch: [23]  [ 900/2809]  eta: 0:18:12  lr: 0.000022  min_lr: 0.000000  loss: 3.9616 (3.8206)  class_acc: 0.2917 (0.3153)  loss_scale: 65536.0000 (61717.3097)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0723  max mem: 15572
[2025-01-16 01:29:35,554] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:29:35,554] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:29:39,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65515
[2025-01-16 01:29:39,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:29:39,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 910/2809]  eta: 0:18:05  lr: 0.000022  min_lr: 0.000000  loss: 4.0866 (3.8229)  class_acc: 0.2500 (0.3145)  loss_scale: 65536.0000 (62190.8584)  weight_decay: 0.0500 (0.0500)  time: 0.4765  data: 0.0490  max mem: 15572
Epoch: [23]  [ 920/2809]  eta: 0:17:59  lr: 0.000022  min_lr: 0.000000  loss: 3.9856 (3.8217)  class_acc: 0.2500 (0.3147)  loss_scale: 65536.0000 (62227.1792)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1038  max mem: 15572
Epoch: [23]  [ 930/2809]  eta: 0:17:53  lr: 0.000022  min_lr: 0.000000  loss: 4.0422 (3.8260)  class_acc: 0.2500 (0.3138)  loss_scale: 65536.0000 (62262.7197)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1138  max mem: 15572
Epoch: [23]  [ 940/2809]  eta: 0:17:47  lr: 0.000022  min_lr: 0.000000  loss: 4.0422 (3.8248)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (62297.5048)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1395  max mem: 15572
Epoch: [23]  [ 950/2809]  eta: 0:17:43  lr: 0.000022  min_lr: 0.000000  loss: 3.8512 (3.8261)  class_acc: 0.2917 (0.3134)  loss_scale: 65536.0000 (62331.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1731  max mem: 15572
Epoch: [23]  [ 960/2809]  eta: 0:17:38  lr: 0.000022  min_lr: 0.000000  loss: 3.9796 (3.8271)  class_acc: 0.2917 (0.3130)  loss_scale: 65536.0000 (62364.9032)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1983  max mem: 15572
Epoch: [23]  [ 970/2809]  eta: 0:17:30  lr: 0.000022  min_lr: 0.000000  loss: 3.9691 (3.8271)  class_acc: 0.2917 (0.3130)  loss_scale: 65536.0000 (62397.5613)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1062  max mem: 15572
Epoch: [23]  [ 980/2809]  eta: 0:17:25  lr: 0.000022  min_lr: 0.000000  loss: 3.8441 (3.8257)  class_acc: 0.3333 (0.3136)  loss_scale: 65536.0000 (62429.5535)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0707  max mem: 15572
Epoch: [23]  [ 990/2809]  eta: 0:17:20  lr: 0.000022  min_lr: 0.000000  loss: 3.6182 (3.8238)  class_acc: 0.3333 (0.3139)  loss_scale: 65536.0000 (62460.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1700  max mem: 15572
Epoch: [23]  [1000/2809]  eta: 0:17:13  lr: 0.000022  min_lr: 0.000000  loss: 3.8352 (3.8260)  class_acc: 0.3333 (0.3137)  loss_scale: 65536.0000 (62491.6204)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1299  max mem: 15572
Epoch: [23]  [1010/2809]  eta: 0:17:08  lr: 0.000022  min_lr: 0.000000  loss: 3.8352 (3.8233)  class_acc: 0.3750 (0.3143)  loss_scale: 65536.0000 (62521.7329)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.0950  max mem: 15572
Epoch: [23]  [1020/2809]  eta: 0:17:02  lr: 0.000022  min_lr: 0.000000  loss: 3.7154 (3.8236)  class_acc: 0.3333 (0.3138)  loss_scale: 65536.0000 (62551.2556)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1211  max mem: 15572
Epoch: [23]  [1030/2809]  eta: 0:16:55  lr: 0.000022  min_lr: 0.000000  loss: 3.9756 (3.8251)  class_acc: 0.2083 (0.3131)  loss_scale: 65536.0000 (62580.2056)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0657  max mem: 15572
[2025-01-16 01:30:50,819] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:30:50,820] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:30:52,704] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65646
[2025-01-16 01:30:52,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:30:52,705] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1040/2809]  eta: 0:16:49  lr: 0.000022  min_lr: 0.000000  loss: 3.9593 (3.8253)  class_acc: 0.2917 (0.3131)  loss_scale: 65536.0000 (62734.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0677  max mem: 15572
Epoch: [23]  [1050/2809]  eta: 0:16:43  lr: 0.000022  min_lr: 0.000000  loss: 3.8178 (3.8237)  class_acc: 0.3333 (0.3132)  loss_scale: 65536.0000 (62761.1646)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1213  max mem: 15572
Epoch: [23]  [1060/2809]  eta: 0:16:39  lr: 0.000022  min_lr: 0.000000  loss: 3.8536 (3.8260)  class_acc: 0.2500 (0.3124)  loss_scale: 65536.0000 (62787.3176)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1819  max mem: 15572
Epoch: [23]  [1070/2809]  eta: 0:16:33  lr: 0.000022  min_lr: 0.000000  loss: 4.0938 (3.8268)  class_acc: 0.2500 (0.3124)  loss_scale: 65536.0000 (62812.9823)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1779  max mem: 15572
Epoch: [23]  [1080/2809]  eta: 0:16:27  lr: 0.000022  min_lr: 0.000000  loss: 3.7377 (3.8255)  class_acc: 0.2917 (0.3128)  loss_scale: 65536.0000 (62838.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1216  max mem: 15572
Epoch: [23]  [1090/2809]  eta: 0:16:21  lr: 0.000022  min_lr: 0.000000  loss: 3.8702 (3.8265)  class_acc: 0.2917 (0.3124)  loss_scale: 65536.0000 (62862.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1268  max mem: 15572
Epoch: [23]  [1100/2809]  eta: 0:16:16  lr: 0.000022  min_lr: 0.000000  loss: 3.9969 (3.8258)  class_acc: 0.2500 (0.3124)  loss_scale: 65536.0000 (62887.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1354  max mem: 15572
Epoch: [23]  [1110/2809]  eta: 0:16:10  lr: 0.000022  min_lr: 0.000000  loss: 3.7019 (3.8251)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (62911.0207)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1510  max mem: 15572
Epoch: [23]  [1120/2809]  eta: 0:16:04  lr: 0.000022  min_lr: 0.000000  loss: 3.8754 (3.8269)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (62934.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1401  max mem: 15572
Epoch: [23]  [1130/2809]  eta: 0:15:58  lr: 0.000022  min_lr: 0.000000  loss: 3.8139 (3.8245)  class_acc: 0.3333 (0.3134)  loss_scale: 65536.0000 (62957.4394)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0990  max mem: 15572
Epoch: [23]  [1140/2809]  eta: 0:15:51  lr: 0.000022  min_lr: 0.000000  loss: 3.6836 (3.8245)  class_acc: 0.3333 (0.3131)  loss_scale: 65536.0000 (62980.0386)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0699  max mem: 15572
Epoch: [23]  [1150/2809]  eta: 0:15:45  lr: 0.000022  min_lr: 0.000000  loss: 3.9169 (3.8250)  class_acc: 0.2917 (0.3130)  loss_scale: 65536.0000 (63002.2450)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0877  max mem: 15572
Epoch: [23]  [1160/2809]  eta: 0:15:40  lr: 0.000022  min_lr: 0.000000  loss: 3.9275 (3.8265)  class_acc: 0.2917 (0.3124)  loss_scale: 65536.0000 (63024.0689)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1381  max mem: 15572
[2025-01-16 01:32:05,281] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:32:05,282] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:32:06,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65776
[2025-01-16 01:32:06,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:32:06,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1170/2809]  eta: 0:15:34  lr: 0.000022  min_lr: 0.000000  loss: 3.6945 (3.8228)  class_acc: 0.2917 (0.3128)  loss_scale: 65536.0000 (63101.4859)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1446  max mem: 15572
Epoch: [23]  [1180/2809]  eta: 0:15:29  lr: 0.000022  min_lr: 0.000000  loss: 3.6935 (3.8228)  class_acc: 0.3333 (0.3131)  loss_scale: 65536.0000 (63122.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1277  max mem: 15572
Epoch: [23]  [1190/2809]  eta: 0:15:22  lr: 0.000022  min_lr: 0.000000  loss: 4.1092 (3.8249)  class_acc: 0.2917 (0.3128)  loss_scale: 65536.0000 (63142.3678)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0980  max mem: 15572
Epoch: [23]  [1200/2809]  eta: 0:15:16  lr: 0.000022  min_lr: 0.000000  loss: 4.0617 (3.8261)  class_acc: 0.2500 (0.3125)  loss_scale: 65536.0000 (63162.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0894  max mem: 15572
Epoch: [23]  [1210/2809]  eta: 0:15:09  lr: 0.000022  min_lr: 0.000000  loss: 3.6089 (3.8240)  class_acc: 0.3333 (0.3130)  loss_scale: 65536.0000 (63181.8993)  weight_decay: 0.0500 (0.0500)  time: 0.5108  data: 0.0568  max mem: 15572
Epoch: [23]  [1220/2809]  eta: 0:15:04  lr: 0.000021  min_lr: 0.000000  loss: 3.5349 (3.8215)  class_acc: 0.3750 (0.3133)  loss_scale: 65536.0000 (63201.1794)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0587  max mem: 15572
Epoch: [23]  [1230/2809]  eta: 0:14:57  lr: 0.000021  min_lr: 0.000000  loss: 3.6232 (3.8194)  class_acc: 0.3333 (0.3138)  loss_scale: 65536.0000 (63220.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.1003  max mem: 15572
Epoch: [23]  [1240/2809]  eta: 0:14:52  lr: 0.000021  min_lr: 0.000000  loss: 3.6747 (3.8197)  class_acc: 0.2917 (0.3135)  loss_scale: 65536.0000 (63238.8074)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1285  max mem: 15572
Epoch: [23]  [1250/2809]  eta: 0:14:47  lr: 0.000021  min_lr: 0.000000  loss: 3.8782 (3.8207)  class_acc: 0.2917 (0.3134)  loss_scale: 65536.0000 (63257.1703)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1776  max mem: 15572
Epoch: [23]  [1260/2809]  eta: 0:14:40  lr: 0.000021  min_lr: 0.000000  loss: 4.0118 (3.8219)  class_acc: 0.2917 (0.3135)  loss_scale: 65536.0000 (63275.2419)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1160  max mem: 15572
Epoch: [23]  [1270/2809]  eta: 0:14:35  lr: 0.000021  min_lr: 0.000000  loss: 4.0664 (3.8226)  class_acc: 0.3333 (0.3135)  loss_scale: 65536.0000 (63293.0291)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1295  max mem: 15572
[2025-01-16 01:33:08,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65886
[2025-01-16 01:33:08,702] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:33:08,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1280/2809]  eta: 0:14:31  lr: 0.000021  min_lr: 0.000000  loss: 4.1173 (3.8236)  class_acc: 0.3333 (0.3136)  loss_scale: 65536.0000 (63259.3786)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.2106  max mem: 15572
Epoch: [23]  [1290/2809]  eta: 0:14:24  lr: 0.000021  min_lr: 0.000000  loss: 3.8083 (3.8224)  class_acc: 0.2917 (0.3135)  loss_scale: 32768.0000 (63023.1944)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1559  max mem: 15572
Epoch: [23]  [1300/2809]  eta: 0:14:19  lr: 0.000021  min_lr: 0.000000  loss: 3.8387 (3.8240)  class_acc: 0.3333 (0.3133)  loss_scale: 32768.0000 (62790.6410)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1069  max mem: 15572
Epoch: [23]  [1310/2809]  eta: 0:14:12  lr: 0.000021  min_lr: 0.000000  loss: 4.0780 (3.8246)  class_acc: 0.2917 (0.3133)  loss_scale: 32768.0000 (62561.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0846  max mem: 15572
Epoch: [23]  [1320/2809]  eta: 0:14:06  lr: 0.000021  min_lr: 0.000000  loss: 4.0581 (3.8247)  class_acc: 0.2917 (0.3132)  loss_scale: 32768.0000 (62336.0969)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0632  max mem: 15572
Epoch: [23]  [1330/2809]  eta: 0:14:01  lr: 0.000021  min_lr: 0.000000  loss: 3.8444 (3.8250)  class_acc: 0.2500 (0.3130)  loss_scale: 32768.0000 (62113.9474)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1110  max mem: 15572
Epoch: [23]  [1340/2809]  eta: 0:13:55  lr: 0.000021  min_lr: 0.000000  loss: 3.8177 (3.8250)  class_acc: 0.2917 (0.3131)  loss_scale: 32768.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1407  max mem: 15572
Epoch: [23]  [1350/2809]  eta: 0:13:49  lr: 0.000021  min_lr: 0.000000  loss: 3.7267 (3.8243)  class_acc: 0.2917 (0.3130)  loss_scale: 32768.0000 (61679.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1234  max mem: 15572
Epoch: [23]  [1360/2809]  eta: 0:13:43  lr: 0.000021  min_lr: 0.000000  loss: 3.5558 (3.8227)  class_acc: 0.2917 (0.3133)  loss_scale: 32768.0000 (61467.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0942  max mem: 15572
[2025-01-16 01:33:54,593] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 65971
[2025-01-16 01:33:54,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 01:33:54,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [1370/2809]  eta: 0:13:37  lr: 0.000021  min_lr: 0.000000  loss: 3.4808 (3.8189)  class_acc: 0.3750 (0.3142)  loss_scale: 32768.0000 (61174.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0782  max mem: 15572
Epoch: [23]  [1380/2809]  eta: 0:13:31  lr: 0.000021  min_lr: 0.000000  loss: 3.6871 (3.8193)  class_acc: 0.3333 (0.3141)  loss_scale: 16384.0000 (60849.7726)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1161  max mem: 15572
Epoch: [23]  [1390/2809]  eta: 0:13:25  lr: 0.000021  min_lr: 0.000000  loss: 3.9165 (3.8203)  class_acc: 0.2917 (0.3136)  loss_scale: 16384.0000 (60530.1050)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1281  max mem: 15572
[2025-01-16 01:34:10,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=66000, skipped=442, lr=[2.0706568880734621e-07, 2.0706568880734621e-07, 2.958081268676375e-07, 2.958081268676375e-07, 4.2258303838233926e-07, 4.2258303838233926e-07, 6.036900548319133e-07, 6.036900548319133e-07, 8.624143640455905e-07, 8.624143640455905e-07, 1.2320205200651293e-06, 1.2320205200651293e-06, 1.7600293143787562e-06, 1.7600293143787562e-06, 2.514327591969652e-06, 2.514327591969652e-06, 3.5918965599566456e-06, 3.5918965599566456e-06, 5.131280799938066e-06, 5.131280799938066e-06, 7.330401142768665e-06, 7.330401142768665e-06, 1.0472001632526666e-05, 1.0472001632526666e-05, 1.4960002332180953e-05, 1.4960002332180953e-05, 2.1371431903115647e-05, 2.1371431903115647e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:34:10,662] [INFO] [timer.py:260:stop] epoch=0/micro_step=66000/global_step=66000, RunningAvgSamplesPerSec=28.542927352576935, CurrSamplesPerSec=24.479361504996145, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [1400/2809]  eta: 0:13:20  lr: 0.000021  min_lr: 0.000000  loss: 3.8840 (3.8199)  class_acc: 0.2917 (0.3136)  loss_scale: 16384.0000 (60215.0007)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1391  max mem: 15572
Epoch: [23]  [1410/2809]  eta: 0:13:15  lr: 0.000021  min_lr: 0.000000  loss: 3.7420 (3.8197)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (59904.3629)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1548  max mem: 15572
Epoch: [23]  [1420/2809]  eta: 0:13:09  lr: 0.000021  min_lr: 0.000000  loss: 3.6451 (3.8188)  class_acc: 0.2917 (0.3135)  loss_scale: 16384.0000 (59598.0971)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1190  max mem: 15572
Epoch: [23]  [1430/2809]  eta: 0:13:03  lr: 0.000021  min_lr: 0.000000  loss: 3.6731 (3.8175)  class_acc: 0.3333 (0.3139)  loss_scale: 16384.0000 (59296.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1234  max mem: 15572
Epoch: [23]  [1440/2809]  eta: 0:12:56  lr: 0.000021  min_lr: 0.000000  loss: 3.8747 (3.8175)  class_acc: 0.3333 (0.3140)  loss_scale: 16384.0000 (58998.3178)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0833  max mem: 15572
Epoch: [23]  [1450/2809]  eta: 0:12:50  lr: 0.000021  min_lr: 0.000000  loss: 3.8747 (3.8178)  class_acc: 0.3333 (0.3140)  loss_scale: 16384.0000 (58704.6285)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0572  max mem: 15572
Epoch: [23]  [1460/2809]  eta: 0:12:45  lr: 0.000021  min_lr: 0.000000  loss: 4.1343 (3.8190)  class_acc: 0.2500 (0.3137)  loss_scale: 16384.0000 (58414.9596)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1210  max mem: 15572
Epoch: [23]  [1470/2809]  eta: 0:12:39  lr: 0.000021  min_lr: 0.000000  loss: 4.1593 (3.8205)  class_acc: 0.2500 (0.3135)  loss_scale: 16384.0000 (58129.2291)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1110  max mem: 15572
Epoch: [23]  [1480/2809]  eta: 0:12:33  lr: 0.000021  min_lr: 0.000000  loss: 4.0918 (3.8219)  class_acc: 0.2500 (0.3132)  loss_scale: 16384.0000 (57847.3572)  weight_decay: 0.0500 (0.0500)  time: 0.5416  data: 0.0886  max mem: 15572
Epoch: [23]  [1490/2809]  eta: 0:12:28  lr: 0.000021  min_lr: 0.000000  loss: 3.8449 (3.8217)  class_acc: 0.2500 (0.3131)  loss_scale: 16384.0000 (57569.2663)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1596  max mem: 15572
[2025-01-16 01:35:09,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:35:09,009] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [1500/2809]  eta: 0:12:23  lr: 0.000021  min_lr: 0.000000  loss: 4.0225 (3.8236)  class_acc: 0.2500 (0.3126)  loss_scale: 16384.0000 (57382.2039)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.1827  max mem: 15572
Epoch: [23]  [1510/2809]  eta: 0:12:18  lr: 0.000021  min_lr: 0.000000  loss: 3.9807 (3.8235)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (57219.3038)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.1800  max mem: 15572
Epoch: [23]  [1520/2809]  eta: 0:12:12  lr: 0.000021  min_lr: 0.000000  loss: 3.8022 (3.8239)  class_acc: 0.2917 (0.3122)  loss_scale: 32768.0000 (57058.5457)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1589  max mem: 15572
Epoch: [23]  [1530/2809]  eta: 0:12:07  lr: 0.000021  min_lr: 0.000000  loss: 3.8237 (3.8240)  class_acc: 0.2917 (0.3121)  loss_scale: 32768.0000 (56899.8877)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1197  max mem: 15572
Epoch: [23]  [1540/2809]  eta: 0:12:01  lr: 0.000021  min_lr: 0.000000  loss: 3.7686 (3.8235)  class_acc: 0.2917 (0.3126)  loss_scale: 32768.0000 (56743.2888)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1190  max mem: 15572
Epoch: [23]  [1550/2809]  eta: 0:11:55  lr: 0.000021  min_lr: 0.000000  loss: 3.8120 (3.8239)  class_acc: 0.2917 (0.3125)  loss_scale: 32768.0000 (56588.7092)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1209  max mem: 15572
Epoch: [23]  [1560/2809]  eta: 0:11:50  lr: 0.000021  min_lr: 0.000000  loss: 3.7454 (3.8229)  class_acc: 0.3750 (0.3129)  loss_scale: 32768.0000 (56436.1102)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1293  max mem: 15572
Epoch: [23]  [1570/2809]  eta: 0:11:44  lr: 0.000021  min_lr: 0.000000  loss: 3.7073 (3.8222)  class_acc: 0.3750 (0.3134)  loss_scale: 32768.0000 (56285.4539)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0855  max mem: 15572
Epoch: [23]  [1580/2809]  eta: 0:11:37  lr: 0.000021  min_lr: 0.000000  loss: 3.7127 (3.8211)  class_acc: 0.3333 (0.3136)  loss_scale: 32768.0000 (56136.7034)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0425  max mem: 15572
Epoch: [23]  [1590/2809]  eta: 0:11:31  lr: 0.000021  min_lr: 0.000000  loss: 3.7127 (3.8196)  class_acc: 0.3333 (0.3141)  loss_scale: 32768.0000 (55989.8228)  weight_decay: 0.0500 (0.0500)  time: 0.4887  data: 0.0543  max mem: 15572
Epoch: [23]  [1600/2809]  eta: 0:11:26  lr: 0.000021  min_lr: 0.000000  loss: 3.8202 (3.8201)  class_acc: 0.2917 (0.3142)  loss_scale: 32768.0000 (55844.7770)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1250  max mem: 15572
Epoch: [23]  [1610/2809]  eta: 0:11:20  lr: 0.000021  min_lr: 0.000000  loss: 3.7879 (3.8193)  class_acc: 0.2917 (0.3144)  loss_scale: 32768.0000 (55701.5320)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1347  max mem: 15572
Epoch: [23]  [1620/2809]  eta: 0:11:14  lr: 0.000021  min_lr: 0.000000  loss: 3.7158 (3.8187)  class_acc: 0.2917 (0.3143)  loss_scale: 32768.0000 (55560.0543)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0751  max mem: 15572
[2025-01-16 01:36:20,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:36:20,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1630/2809]  eta: 0:11:08  lr: 0.000021  min_lr: 0.000000  loss: 3.9521 (3.8190)  class_acc: 0.2917 (0.3144)  loss_scale: 32768.0000 (55621.2189)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0686  max mem: 15572
Epoch: [23]  [1640/2809]  eta: 0:11:02  lr: 0.000021  min_lr: 0.000000  loss: 3.9454 (3.8184)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (55681.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.0986  max mem: 15572
Epoch: [23]  [1650/2809]  eta: 0:10:57  lr: 0.000021  min_lr: 0.000000  loss: 3.8830 (3.8186)  class_acc: 0.3333 (0.3147)  loss_scale: 65536.0000 (55741.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1415  max mem: 15572
Epoch: [23]  [1660/2809]  eta: 0:10:51  lr: 0.000021  min_lr: 0.000000  loss: 3.8626 (3.8179)  class_acc: 0.3333 (0.3146)  loss_scale: 65536.0000 (55800.2938)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1592  max mem: 15572
Epoch: [23]  [1670/2809]  eta: 0:10:46  lr: 0.000021  min_lr: 0.000000  loss: 3.6880 (3.8171)  class_acc: 0.3333 (0.3148)  loss_scale: 65536.0000 (55858.5566)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1229  max mem: 15572
Epoch: [23]  [1680/2809]  eta: 0:10:41  lr: 0.000021  min_lr: 0.000000  loss: 3.6187 (3.8181)  class_acc: 0.3333 (0.3145)  loss_scale: 65536.0000 (55916.1261)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1484  max mem: 15572
Epoch: [23]  [1690/2809]  eta: 0:10:35  lr: 0.000021  min_lr: 0.000000  loss: 3.8184 (3.8181)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (55973.0148)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1360  max mem: 15572
Epoch: [23]  [1700/2809]  eta: 0:10:28  lr: 0.000021  min_lr: 0.000000  loss: 3.9613 (3.8182)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (56029.2346)  weight_decay: 0.0500 (0.0500)  time: 0.4915  data: 0.0348  max mem: 15572
Epoch: [23]  [1710/2809]  eta: 0:10:22  lr: 0.000021  min_lr: 0.000000  loss: 3.7584 (3.8166)  class_acc: 0.3333 (0.3146)  loss_scale: 65536.0000 (56084.7972)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0413  max mem: 15572
Epoch: [23]  [1720/2809]  eta: 0:10:17  lr: 0.000021  min_lr: 0.000000  loss: 3.5489 (3.8157)  class_acc: 0.3333 (0.3148)  loss_scale: 65536.0000 (56139.7141)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1582  max mem: 15572
Epoch: [23]  [1730/2809]  eta: 0:10:11  lr: 0.000021  min_lr: 0.000000  loss: 3.6682 (3.8157)  class_acc: 0.3333 (0.3148)  loss_scale: 65536.0000 (56193.9965)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1175  max mem: 15572
Epoch: [23]  [1740/2809]  eta: 0:10:05  lr: 0.000021  min_lr: 0.000000  loss: 3.8352 (3.8158)  class_acc: 0.2917 (0.3147)  loss_scale: 65536.0000 (56247.6554)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0382  max mem: 15572
[2025-01-16 01:37:31,125] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:37:31,125] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [1750/2809]  eta: 0:09:59  lr: 0.000021  min_lr: 0.000000  loss: 3.8935 (3.8168)  class_acc: 0.2917 (0.3147)  loss_scale: 65536.0000 (56375.5568)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0844  max mem: 15572
[2025-01-16 01:37:31,937] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66358
[2025-01-16 01:37:31,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:37:31,937] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1760/2809]  eta: 0:09:54  lr: 0.000021  min_lr: 0.000000  loss: 3.9451 (3.8179)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (56427.5752)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1051  max mem: 15572
Epoch: [23]  [1770/2809]  eta: 0:09:48  lr: 0.000021  min_lr: 0.000000  loss: 4.0768 (3.8180)  class_acc: 0.2500 (0.3145)  loss_scale: 65536.0000 (56479.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0928  max mem: 15572
Epoch: [23]  [1780/2809]  eta: 0:09:42  lr: 0.000021  min_lr: 0.000000  loss: 4.0201 (3.8187)  class_acc: 0.2500 (0.3142)  loss_scale: 65536.0000 (56529.8596)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.0972  max mem: 15572
Epoch: [23]  [1790/2809]  eta: 0:09:37  lr: 0.000021  min_lr: 0.000000  loss: 3.9629 (3.8195)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (56580.1452)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1612  max mem: 15572
Epoch: [23]  [1800/2809]  eta: 0:09:31  lr: 0.000021  min_lr: 0.000000  loss: 3.9440 (3.8194)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (56629.8723)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.0982  max mem: 15572
Epoch: [23]  [1810/2809]  eta: 0:09:26  lr: 0.000021  min_lr: 0.000000  loss: 3.7798 (3.8191)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (56679.0502)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0673  max mem: 15572
Epoch: [23]  [1820/2809]  eta: 0:09:20  lr: 0.000021  min_lr: 0.000000  loss: 3.8306 (3.8191)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (56727.6881)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1228  max mem: 15572
Epoch: [23]  [1830/2809]  eta: 0:09:14  lr: 0.000021  min_lr: 0.000000  loss: 3.9344 (3.8193)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (56775.7946)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1420  max mem: 15572
Epoch: [23]  [1840/2809]  eta: 0:09:08  lr: 0.000021  min_lr: 0.000000  loss: 3.7525 (3.8181)  class_acc: 0.2500 (0.3142)  loss_scale: 65536.0000 (56823.3786)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0865  max mem: 15572
Epoch: [23]  [1850/2809]  eta: 0:09:02  lr: 0.000021  min_lr: 0.000000  loss: 3.8218 (3.8183)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (56870.4484)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0824  max mem: 15572
Epoch: [23]  [1860/2809]  eta: 0:08:57  lr: 0.000021  min_lr: 0.000000  loss: 3.8931 (3.8184)  class_acc: 0.3333 (0.3146)  loss_scale: 65536.0000 (56917.0124)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1938  max mem: 15572
[2025-01-16 01:38:34,682] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66468
[2025-01-16 01:38:34,683] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:38:34,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1870/2809]  eta: 0:08:52  lr: 0.000021  min_lr: 0.000000  loss: 3.5347 (3.8172)  class_acc: 0.3750 (0.3148)  loss_scale: 32768.0000 (56787.9423)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1557  max mem: 15572
Epoch: [23]  [1880/2809]  eta: 0:08:46  lr: 0.000021  min_lr: 0.000000  loss: 3.9197 (3.8185)  class_acc: 0.3333 (0.3147)  loss_scale: 32768.0000 (56660.2446)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1134  max mem: 15572
Epoch: [23]  [1890/2809]  eta: 0:08:40  lr: 0.000021  min_lr: 0.000000  loss: 3.8163 (3.8166)  class_acc: 0.3333 (0.3152)  loss_scale: 32768.0000 (56533.8974)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0859  max mem: 15572
Epoch: [23]  [1900/2809]  eta: 0:08:35  lr: 0.000021  min_lr: 0.000000  loss: 3.7499 (3.8168)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (56408.8795)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1476  max mem: 15572
Epoch: [23]  [1910/2809]  eta: 0:08:29  lr: 0.000021  min_lr: 0.000000  loss: 3.8028 (3.8167)  class_acc: 0.2917 (0.3149)  loss_scale: 32768.0000 (56285.1701)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1536  max mem: 15572
Epoch: [23]  [1920/2809]  eta: 0:08:23  lr: 0.000021  min_lr: 0.000000  loss: 3.7887 (3.8168)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (56162.7486)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0920  max mem: 15572
Epoch: [23]  [1930/2809]  eta: 0:08:18  lr: 0.000021  min_lr: 0.000000  loss: 3.7613 (3.8171)  class_acc: 0.3333 (0.3149)  loss_scale: 32768.0000 (56041.5950)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.1601  max mem: 15572
Epoch: [23]  [1940/2809]  eta: 0:08:12  lr: 0.000021  min_lr: 0.000000  loss: 3.8263 (3.8163)  class_acc: 0.3333 (0.3151)  loss_scale: 32768.0000 (55921.6899)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1504  max mem: 15572
Epoch: [23]  [1950/2809]  eta: 0:08:07  lr: 0.000021  min_lr: 0.000000  loss: 3.8263 (3.8170)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (55803.0138)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1353  max mem: 15572
Epoch: [23]  [1960/2809]  eta: 0:08:01  lr: 0.000021  min_lr: 0.000000  loss: 4.0515 (3.8179)  class_acc: 0.2500 (0.3146)  loss_scale: 32768.0000 (55685.5482)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1887  max mem: 15572
Epoch: [23]  [1970/2809]  eta: 0:07:56  lr: 0.000021  min_lr: 0.000000  loss: 4.0515 (3.8191)  class_acc: 0.2500 (0.3142)  loss_scale: 32768.0000 (55569.2745)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1575  max mem: 15572
Epoch: [23]  [1980/2809]  eta: 0:07:50  lr: 0.000021  min_lr: 0.000000  loss: 3.9048 (3.8186)  class_acc: 0.2500 (0.3142)  loss_scale: 32768.0000 (55454.1747)  weight_decay: 0.0500 (0.0500)  time: 0.5189  data: 0.0751  max mem: 15572
[2025-01-16 01:39:48,869] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:39:48,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1990/2809]  eta: 0:07:44  lr: 0.000021  min_lr: 0.000000  loss: 3.8107 (3.8187)  class_acc: 0.2917 (0.3140)  loss_scale: 32768.0000 (55356.6891)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0908  max mem: 15572
Epoch: [23]  [2000/2809]  eta: 0:07:38  lr: 0.000021  min_lr: 0.000000  loss: 3.7870 (3.8184)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (55407.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0971  max mem: 15572
Epoch: [23]  [2010/2809]  eta: 0:07:32  lr: 0.000021  min_lr: 0.000000  loss: 3.7870 (3.8186)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (55457.9254)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0755  max mem: 15572
Epoch: [23]  [2020/2809]  eta: 0:07:26  lr: 0.000021  min_lr: 0.000000  loss: 3.7209 (3.8171)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (55507.7922)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0687  max mem: 15572
Epoch: [23]  [2030/2809]  eta: 0:07:21  lr: 0.000021  min_lr: 0.000000  loss: 3.7111 (3.8170)  class_acc: 0.3333 (0.3145)  loss_scale: 65536.0000 (55557.1679)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1221  max mem: 15572
Epoch: [23]  [2040/2809]  eta: 0:07:16  lr: 0.000021  min_lr: 0.000000  loss: 3.9433 (3.8170)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (55606.0598)  weight_decay: 0.0500 (0.0500)  time: 0.6321  data: 0.1885  max mem: 15572
Epoch: [23]  [2050/2809]  eta: 0:07:10  lr: 0.000021  min_lr: 0.000000  loss: 3.9387 (3.8178)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (55654.4749)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1272  max mem: 15572
Epoch: [23]  [2060/2809]  eta: 0:07:04  lr: 0.000021  min_lr: 0.000000  loss: 3.8895 (3.8174)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (55702.4202)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0644  max mem: 15572
Epoch: [23]  [2070/2809]  eta: 0:06:58  lr: 0.000021  min_lr: 0.000000  loss: 3.8475 (3.8177)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (55749.9025)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0835  max mem: 15572
Epoch: [23]  [2080/2809]  eta: 0:06:52  lr: 0.000021  min_lr: 0.000000  loss: 3.8459 (3.8184)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (55796.9284)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1218  max mem: 15572
Epoch: [23]  [2090/2809]  eta: 0:06:47  lr: 0.000021  min_lr: 0.000000  loss: 3.7135 (3.8171)  class_acc: 0.3333 (0.3146)  loss_scale: 65536.0000 (55843.5045)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1501  max mem: 15572
Epoch: [23]  [2100/2809]  eta: 0:06:41  lr: 0.000021  min_lr: 0.000000  loss: 3.6907 (3.8176)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (55889.6373)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1585  max mem: 15572
Epoch: [23]  [2110/2809]  eta: 0:06:36  lr: 0.000021  min_lr: 0.000000  loss: 3.8306 (3.8175)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (55935.3330)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1391  max mem: 15572
[2025-01-16 01:41:01,607] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:41:01,607] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2120/2809]  eta: 0:06:30  lr: 0.000021  min_lr: 0.000000  loss: 3.8352 (3.8178)  class_acc: 0.2500 (0.3143)  loss_scale: 65536.0000 (56073.2937)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1497  max mem: 15572
[2025-01-16 01:41:02,832] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66728
[2025-01-16 01:41:02,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:41:02,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2130/2809]  eta: 0:06:24  lr: 0.000021  min_lr: 0.000000  loss: 3.6912 (3.8175)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (56117.6987)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1364  max mem: 15572
Epoch: [23]  [2140/2809]  eta: 0:06:19  lr: 0.000021  min_lr: 0.000000  loss: 3.6467 (3.8177)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (56161.6889)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1232  max mem: 15572
Epoch: [23]  [2150/2809]  eta: 0:06:13  lr: 0.000021  min_lr: 0.000000  loss: 3.8427 (3.8179)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (56205.2701)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.1798  max mem: 15572
Epoch: [23]  [2160/2809]  eta: 0:06:08  lr: 0.000021  min_lr: 0.000000  loss: 4.0348 (3.8188)  class_acc: 0.2500 (0.3140)  loss_scale: 65536.0000 (56248.4479)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1165  max mem: 15572
Epoch: [23]  [2170/2809]  eta: 0:06:02  lr: 0.000021  min_lr: 0.000000  loss: 4.1038 (3.8191)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (56291.2280)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1000  max mem: 15572
Epoch: [23]  [2180/2809]  eta: 0:05:56  lr: 0.000021  min_lr: 0.000000  loss: 4.0334 (3.8199)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (56333.6158)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1553  max mem: 15572
Epoch: [23]  [2190/2809]  eta: 0:05:51  lr: 0.000021  min_lr: 0.000000  loss: 4.0060 (3.8201)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (56375.6166)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1550  max mem: 15572
Epoch: [23]  [2200/2809]  eta: 0:05:45  lr: 0.000021  min_lr: 0.000000  loss: 3.8276 (3.8201)  class_acc: 0.3333 (0.3142)  loss_scale: 65536.0000 (56417.2358)  weight_decay: 0.0500 (0.0500)  time: 0.6078  data: 0.1633  max mem: 15572
Epoch: [23]  [2210/2809]  eta: 0:05:39  lr: 0.000021  min_lr: 0.000000  loss: 3.8276 (3.8202)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (56458.4785)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1216  max mem: 15572
Epoch: [23]  [2220/2809]  eta: 0:05:34  lr: 0.000021  min_lr: 0.000000  loss: 3.9463 (3.8207)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (56499.3498)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.0944  max mem: 15572
Epoch: [23]  [2230/2809]  eta: 0:05:28  lr: 0.000021  min_lr: 0.000000  loss: 3.9463 (3.8215)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (56539.8548)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1544  max mem: 15572
Epoch: [23]  [2240/2809]  eta: 0:05:22  lr: 0.000021  min_lr: 0.000000  loss: 4.0460 (3.8219)  class_acc: 0.2500 (0.3139)  loss_scale: 65536.0000 (56579.9982)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1140  max mem: 15572
[2025-01-16 01:42:16,790] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:42:16,790] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2250/2809]  eta: 0:05:17  lr: 0.000021  min_lr: 0.000000  loss: 4.0460 (3.8222)  class_acc: 0.2500 (0.3138)  loss_scale: 65536.0000 (56648.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5070  data: 0.0654  max mem: 15572
[2025-01-16 01:42:17,709] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66859
[2025-01-16 01:42:17,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:42:17,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2260/2809]  eta: 0:05:11  lr: 0.000021  min_lr: 0.000000  loss: 3.8364 (3.8225)  class_acc: 0.2917 (0.3139)  loss_scale: 65536.0000 (56717.1906)  weight_decay: 0.0500 (0.0500)  time: 0.5186  data: 0.0911  max mem: 15572
Epoch: [23]  [2270/2809]  eta: 0:05:05  lr: 0.000021  min_lr: 0.000000  loss: 3.8352 (3.8222)  class_acc: 0.3333 (0.3139)  loss_scale: 65536.0000 (56756.0229)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1095  max mem: 15572
Epoch: [23]  [2280/2809]  eta: 0:04:59  lr: 0.000021  min_lr: 0.000000  loss: 3.8243 (3.8214)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (56794.5147)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.0997  max mem: 15572
Epoch: [23]  [2290/2809]  eta: 0:04:54  lr: 0.000021  min_lr: 0.000000  loss: 3.8323 (3.8213)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (56832.6704)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0724  max mem: 15572
Epoch: [23]  [2300/2809]  eta: 0:04:48  lr: 0.000021  min_lr: 0.000000  loss: 3.8048 (3.8208)  class_acc: 0.3750 (0.3144)  loss_scale: 65536.0000 (56870.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1199  max mem: 15572
Epoch: [23]  [2310/2809]  eta: 0:04:42  lr: 0.000021  min_lr: 0.000000  loss: 3.8101 (3.8208)  class_acc: 0.3750 (0.3142)  loss_scale: 65536.0000 (56907.9913)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1238  max mem: 15572
Epoch: [23]  [2320/2809]  eta: 0:04:37  lr: 0.000021  min_lr: 0.000000  loss: 3.9019 (3.8215)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (56945.1650)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1357  max mem: 15572
Epoch: [23]  [2330/2809]  eta: 0:04:31  lr: 0.000021  min_lr: 0.000000  loss: 3.8918 (3.8207)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (56982.0197)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0989  max mem: 15572
Epoch: [23]  [2340/2809]  eta: 0:04:25  lr: 0.000021  min_lr: 0.000000  loss: 3.8210 (3.8211)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (57018.5596)  weight_decay: 0.0500 (0.0500)  time: 0.5124  data: 0.0670  max mem: 15572
Epoch: [23]  [2350/2809]  eta: 0:04:19  lr: 0.000021  min_lr: 0.000000  loss: 3.8459 (3.8213)  class_acc: 0.2917 (0.3139)  loss_scale: 65536.0000 (57054.7886)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0828  max mem: 15572
Epoch: [23]  [2360/2809]  eta: 0:04:14  lr: 0.000021  min_lr: 0.000000  loss: 3.9099 (3.8218)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (57090.7107)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.1118  max mem: 15572
Epoch: [23]  [2370/2809]  eta: 0:04:08  lr: 0.000021  min_lr: 0.000000  loss: 3.9194 (3.8216)  class_acc: 0.2917 (0.3139)  loss_scale: 65536.0000 (57126.3298)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1892  max mem: 15572
Epoch: [23]  [2380/2809]  eta: 0:04:03  lr: 0.000021  min_lr: 0.000000  loss: 3.8653 (3.8220)  class_acc: 0.2500 (0.3139)  loss_scale: 65536.0000 (57161.6497)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1387  max mem: 15572
[2025-01-16 01:43:29,137] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:43:29,138] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:43:33,706] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 66995
[2025-01-16 01:43:33,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:43:33,706] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2390/2809]  eta: 0:03:57  lr: 0.000021  min_lr: 0.000000  loss: 3.7662 (3.8210)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (57388.5404)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1048  max mem: 15572
[2025-01-16 01:43:35,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=67000, skipped=447, lr=[1.998571212807982e-07, 1.998571212807982e-07, 2.855101732582832e-07, 2.855101732582832e-07, 4.0787167608326175e-07, 4.0787167608326175e-07, 5.826738229760882e-07, 5.826738229760882e-07, 8.323911756801261e-07, 8.323911756801261e-07, 1.1891302509716086e-06, 1.1891302509716086e-06, 1.6987575013880125e-06, 1.6987575013880125e-06, 2.426796430554304e-06, 2.426796430554304e-06, 3.4668520436490055e-06, 3.4668520436490055e-06, 4.952645776641437e-06, 4.952645776641437e-06, 7.07520825234491e-06, 7.07520825234491e-06, 1.010744036049273e-05, 1.010744036049273e-05, 1.4439200514989614e-05, 1.4439200514989614e-05, 2.0627429307128022e-05, 2.0627429307128022e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:43:35,438] [INFO] [timer.py:260:stop] epoch=0/micro_step=67000/global_step=67000, RunningAvgSamplesPerSec=28.54137463213941, CurrSamplesPerSec=22.080891594839215, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [23]  [2400/2809]  eta: 0:03:51  lr: 0.000021  min_lr: 0.000000  loss: 3.6811 (3.8202)  class_acc: 0.3333 (0.3142)  loss_scale: 65536.0000 (57422.4740)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1261  max mem: 15572
Epoch: [23]  [2410/2809]  eta: 0:03:46  lr: 0.000021  min_lr: 0.000000  loss: 3.7682 (3.8205)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (57456.1261)  weight_decay: 0.0500 (0.0500)  time: 0.5644  data: 0.1165  max mem: 15572
Epoch: [23]  [2420/2809]  eta: 0:03:40  lr: 0.000021  min_lr: 0.000000  loss: 3.7682 (3.8195)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (57489.5002)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0728  max mem: 15572
Epoch: [23]  [2430/2809]  eta: 0:03:34  lr: 0.000021  min_lr: 0.000000  loss: 3.7141 (3.8195)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (57522.5998)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1464  max mem: 15572
Epoch: [23]  [2440/2809]  eta: 0:03:29  lr: 0.000021  min_lr: 0.000000  loss: 3.7206 (3.8198)  class_acc: 0.2917 (0.3141)  loss_scale: 65536.0000 (57555.4281)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.1694  max mem: 15572
Epoch: [23]  [2450/2809]  eta: 0:03:23  lr: 0.000021  min_lr: 0.000000  loss: 3.6992 (3.8195)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (57587.9886)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0738  max mem: 15572
Epoch: [23]  [2460/2809]  eta: 0:03:17  lr: 0.000021  min_lr: 0.000000  loss: 3.8987 (3.8202)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (57620.2844)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0644  max mem: 15572
Epoch: [23]  [2470/2809]  eta: 0:03:11  lr: 0.000021  min_lr: 0.000000  loss: 3.9550 (3.8210)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (57652.3189)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1060  max mem: 15572
Epoch: [23]  [2480/2809]  eta: 0:03:06  lr: 0.000021  min_lr: 0.000000  loss: 3.7027 (3.8195)  class_acc: 0.3333 (0.3143)  loss_scale: 65536.0000 (57684.0951)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1779  max mem: 15572
Epoch: [23]  [2490/2809]  eta: 0:03:00  lr: 0.000021  min_lr: 0.000000  loss: 3.5958 (3.8192)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (57715.6162)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1705  max mem: 15572
Epoch: [23]  [2500/2809]  eta: 0:02:55  lr: 0.000021  min_lr: 0.000000  loss: 3.6541 (3.8190)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (57746.8852)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1523  max mem: 15572
Epoch: [23]  [2510/2809]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000000  loss: 3.6695 (3.8181)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (57777.9052)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1429  max mem: 15572
[2025-01-16 01:44:46,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:44:46,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [2520/2809]  eta: 0:02:43  lr: 0.000021  min_lr: 0.000000  loss: 3.7554 (3.8188)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (57912.6632)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1427  max mem: 15572
[2025-01-16 01:44:50,464] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67130
[2025-01-16 01:44:50,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:44:50,464] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2530/2809]  eta: 0:02:38  lr: 0.000021  min_lr: 0.000000  loss: 3.9863 (3.8192)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (57994.5697)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.1065  max mem: 15572
Epoch: [23]  [2540/2809]  eta: 0:02:32  lr: 0.000021  min_lr: 0.000000  loss: 3.8091 (3.8191)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (58024.2487)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0901  max mem: 15572
Epoch: [23]  [2550/2809]  eta: 0:02:26  lr: 0.000021  min_lr: 0.000000  loss: 3.5612 (3.8182)  class_acc: 0.3333 (0.3145)  loss_scale: 65536.0000 (58053.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5007  data: 0.0616  max mem: 15572
Epoch: [23]  [2560/2809]  eta: 0:02:21  lr: 0.000021  min_lr: 0.000000  loss: 3.5813 (3.8183)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (58082.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0968  max mem: 15572
Epoch: [23]  [2570/2809]  eta: 0:02:15  lr: 0.000020  min_lr: 0.000000  loss: 3.8206 (3.8188)  class_acc: 0.2917 (0.3144)  loss_scale: 65536.0000 (58111.9004)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1516  max mem: 15572
Epoch: [23]  [2580/2809]  eta: 0:02:09  lr: 0.000020  min_lr: 0.000000  loss: 3.8296 (3.8195)  class_acc: 0.2917 (0.3143)  loss_scale: 65536.0000 (58140.6649)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1141  max mem: 15572
Epoch: [23]  [2590/2809]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 3.8296 (3.8188)  class_acc: 0.2500 (0.3143)  loss_scale: 65536.0000 (58169.2073)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0751  max mem: 15572
Epoch: [23]  [2600/2809]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 3.6458 (3.8185)  class_acc: 0.3333 (0.3144)  loss_scale: 65536.0000 (58197.5302)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0673  max mem: 15572
Epoch: [23]  [2610/2809]  eta: 0:01:52  lr: 0.000020  min_lr: 0.000000  loss: 3.8497 (3.8193)  class_acc: 0.2917 (0.3140)  loss_scale: 65536.0000 (58225.6362)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1173  max mem: 15572
Epoch: [23]  [2620/2809]  eta: 0:01:46  lr: 0.000020  min_lr: 0.000000  loss: 3.8485 (3.8185)  class_acc: 0.2500 (0.3141)  loss_scale: 65536.0000 (58253.5277)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1126  max mem: 15572
Epoch: [23]  [2630/2809]  eta: 0:01:41  lr: 0.000020  min_lr: 0.000000  loss: 3.6728 (3.8194)  class_acc: 0.2500 (0.3137)  loss_scale: 65536.0000 (58281.2071)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1433  max mem: 15572
Epoch: [23]  [2640/2809]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 3.6728 (3.8186)  class_acc: 0.2500 (0.3137)  loss_scale: 65536.0000 (58308.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1294  max mem: 15572
Epoch: [23]  [2650/2809]  eta: 0:01:30  lr: 0.000020  min_lr: 0.000000  loss: 3.6386 (3.8181)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (58335.9396)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1197  max mem: 15572
[2025-01-16 01:46:04,069] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:46:04,069] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:46:04,470] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67260
[2025-01-16 01:46:04,471] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:46:04,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2660/2809]  eta: 0:01:24  lr: 0.000020  min_lr: 0.000000  loss: 3.6723 (3.8171)  class_acc: 0.3333 (0.3139)  loss_scale: 65536.0000 (58387.6257)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1426  max mem: 15572
Epoch: [23]  [2670/2809]  eta: 0:01:18  lr: 0.000020  min_lr: 0.000000  loss: 3.5223 (3.8165)  class_acc: 0.3333 (0.3141)  loss_scale: 65536.0000 (58414.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0763  max mem: 15572
Epoch: [23]  [2680/2809]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 3.7981 (3.8170)  class_acc: 0.2917 (0.3139)  loss_scale: 65536.0000 (58440.9519)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0722  max mem: 15572
Epoch: [23]  [2690/2809]  eta: 0:01:07  lr: 0.000020  min_lr: 0.000000  loss: 3.8977 (3.8174)  class_acc: 0.2500 (0.3137)  loss_scale: 65536.0000 (58467.3177)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0953  max mem: 15572
Epoch: [23]  [2700/2809]  eta: 0:01:01  lr: 0.000020  min_lr: 0.000000  loss: 3.8928 (3.8177)  class_acc: 0.2500 (0.3136)  loss_scale: 65536.0000 (58493.4883)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0930  max mem: 15572
Epoch: [23]  [2710/2809]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 3.6893 (3.8171)  class_acc: 0.3333 (0.3137)  loss_scale: 65536.0000 (58519.4659)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1323  max mem: 15572
Epoch: [23]  [2720/2809]  eta: 0:00:50  lr: 0.000020  min_lr: 0.000000  loss: 3.5737 (3.8160)  class_acc: 0.3333 (0.3139)  loss_scale: 65536.0000 (58545.2525)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1301  max mem: 15572
Epoch: [23]  [2730/2809]  eta: 0:00:44  lr: 0.000020  min_lr: 0.000000  loss: 3.7447 (3.8158)  class_acc: 0.3750 (0.3140)  loss_scale: 65536.0000 (58570.8502)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1168  max mem: 15572
Epoch: [23]  [2740/2809]  eta: 0:00:39  lr: 0.000020  min_lr: 0.000000  loss: 3.8891 (3.8167)  class_acc: 0.2500 (0.3139)  loss_scale: 65536.0000 (58596.2612)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1765  max mem: 15572
Epoch: [23]  [2750/2809]  eta: 0:00:33  lr: 0.000020  min_lr: 0.000000  loss: 3.9771 (3.8172)  class_acc: 0.2500 (0.3138)  loss_scale: 65536.0000 (58621.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1861  max mem: 15572
Epoch: [23]  [2760/2809]  eta: 0:00:27  lr: 0.000020  min_lr: 0.000000  loss: 3.9611 (3.8178)  class_acc: 0.2917 (0.3138)  loss_scale: 65536.0000 (58646.5310)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.1892  max mem: 15572
Epoch: [23]  [2770/2809]  eta: 0:00:22  lr: 0.000020  min_lr: 0.000000  loss: 3.9376 (3.8178)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (58671.3937)  weight_decay: 0.0500 (0.0500)  time: 0.6525  data: 0.1575  max mem: 15572
Epoch: [23]  [2780/2809]  eta: 0:00:16  lr: 0.000020  min_lr: 0.000000  loss: 3.7455 (3.8173)  class_acc: 0.2917 (0.3138)  loss_scale: 65536.0000 (58696.0777)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.1367  max mem: 15572
[2025-01-16 01:47:20,412] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:47:20,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:47:21,270] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67391
[2025-01-16 01:47:21,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:47:21,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [2790/2809]  eta: 0:00:10  lr: 0.000020  min_lr: 0.000000  loss: 3.7430 (3.8175)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (58767.5471)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0888  max mem: 15572
Epoch: [23]  [2800/2809]  eta: 0:00:05  lr: 0.000020  min_lr: 0.000000  loss: 3.8676 (3.8174)  class_acc: 0.2917 (0.3138)  loss_scale: 65536.0000 (58791.7115)  weight_decay: 0.0500 (0.0500)  time: 0.4670  data: 0.0227  max mem: 15572
Epoch: [23]  [2808/2809]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 3.8676 (3.8174)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (58810.9192)  weight_decay: 0.0500 (0.0500)  time: 0.4314  data: 0.0226  max mem: 15572
Epoch: [23] Total time: 0:26:33 (0.5671 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 3.8676 (3.8174)  class_acc: 0.2917 (0.3137)  loss_scale: 65536.0000 (58810.9192)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:23:26  loss: 0.4937 (0.4937)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.1698  data: 4.9816  max mem: 15572
Val:  [ 10/272]  eta: 0:03:05  loss: 2.6291 (2.4105)  acc1: 44.4444 (41.4141)  acc5: 72.2222 (68.6869)  time: 0.7094  data: 0.5199  max mem: 15572
Val:  [ 20/272]  eta: 0:02:10  loss: 2.4224 (2.4117)  acc1: 50.0000 (45.5026)  acc5: 72.2222 (70.6349)  time: 0.2845  data: 0.0958  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 2.4224 (2.4905)  acc1: 50.0000 (41.9355)  acc5: 72.2222 (70.6093)  time: 0.3265  data: 0.1214  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 2.5927 (2.5259)  acc1: 22.2222 (38.4824)  acc5: 72.2222 (71.1382)  time: 0.3119  data: 0.0980  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 2.4022 (2.4454)  acc1: 33.3333 (40.6318)  acc5: 77.7778 (73.4205)  time: 0.3448  data: 0.1516  max mem: 15572
Val:  [ 60/272]  eta: 0:01:28  loss: 1.6148 (2.3313)  acc1: 66.6667 (43.9891)  acc5: 88.8889 (74.6812)  time: 0.4145  data: 0.2308  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 1.5491 (2.2539)  acc1: 66.6667 (46.6354)  acc5: 88.8889 (75.6651)  time: 0.3355  data: 0.1269  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.9997 (2.2836)  acc1: 50.0000 (46.2277)  acc5: 77.7778 (75.2401)  time: 0.2433  data: 0.0277  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 2.4267 (2.3075)  acc1: 44.4444 (46.3980)  acc5: 77.7778 (75.0305)  time: 0.2769  data: 0.0796  max mem: 15572
Val:  [100/272]  eta: 0:01:02  loss: 2.3333 (2.3308)  acc1: 44.4444 (45.9296)  acc5: 77.7778 (75.1375)  time: 0.3204  data: 0.1238  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 2.3800 (2.4063)  acc1: 27.7778 (43.7938)  acc5: 77.7778 (73.9239)  time: 0.2801  data: 0.0896  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 2.9516 (2.4453)  acc1: 16.6667 (43.0211)  acc5: 61.1111 (73.3242)  time: 0.2859  data: 0.1061  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.3031 (2.4082)  acc1: 44.4444 (43.8507)  acc5: 77.7778 (74.0034)  time: 0.3501  data: 0.1638  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 1.9269 (2.3967)  acc1: 50.0000 (44.3262)  acc5: 77.7778 (73.8771)  time: 0.3427  data: 0.1446  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.2901 (2.3972)  acc1: 38.8889 (43.7454)  acc5: 72.2222 (74.1722)  time: 0.3159  data: 0.1121  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.2901 (2.3822)  acc1: 44.4444 (44.2374)  acc5: 77.7778 (74.6032)  time: 0.2715  data: 0.0756  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4521 (2.3996)  acc1: 44.4444 (43.5997)  acc5: 77.7778 (74.2365)  time: 0.2424  data: 0.0562  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.2501 (2.3845)  acc1: 33.3333 (43.5236)  acc5: 77.7778 (74.7084)  time: 0.2281  data: 0.0464  max mem: 15572
Val:  [190/272]  eta: 0:00:26  loss: 2.3037 (2.4312)  acc1: 38.8889 (42.3502)  acc5: 77.7778 (73.5020)  time: 0.1775  data: 0.0110  max mem: 15572
Val:  [200/272]  eta: 0:00:22  loss: 2.5677 (2.4373)  acc1: 33.3333 (42.1504)  acc5: 66.6667 (73.3831)  time: 0.1817  data: 0.0200  max mem: 15572
Val:  [210/272]  eta: 0:00:18  loss: 2.1881 (2.4391)  acc1: 50.0000 (42.4961)  acc5: 77.7778 (73.3807)  time: 0.1932  data: 0.0202  max mem: 15572
Val:  [220/272]  eta: 0:00:15  loss: 2.2697 (2.4276)  acc1: 55.5556 (42.9110)  acc5: 77.7778 (73.6551)  time: 0.2567  data: 0.0697  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.9267 (2.4021)  acc1: 61.1111 (43.8672)  acc5: 83.3333 (73.9298)  time: 0.3465  data: 0.1492  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7458 (2.3875)  acc1: 55.5556 (44.1217)  acc5: 83.3333 (74.2969)  time: 0.3685  data: 0.1637  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.3091 (2.3983)  acc1: 38.8889 (43.4706)  acc5: 83.3333 (74.2143)  time: 0.3573  data: 0.1534  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.2812 (2.3362)  acc1: 77.7778 (45.2959)  acc5: 88.8889 (74.9894)  time: 0.3143  data: 0.1233  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3916 (2.3301)  acc1: 77.7778 (45.4490)  acc5: 83.3333 (75.2153)  time: 0.2559  data: 0.0772  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3916 (2.3350)  acc1: 61.1111 (45.4229)  acc5: 83.3333 (75.1792)  time: 0.2486  data: 0.0772  max mem: 15572
Val: Total time: 0:01:24 (0.3104 s / it)
* Acc@1 45.423 Acc@5 75.179 loss 2.335
Accuracy of the network on the 4883 val videos: 45.4%
[2025-01-16 01:48:56,330] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 01:48:56,335] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 01:48:56,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 01:48:59,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 01:48:59,045] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.42%
Epoch: [24]  [   0/2809]  eta: 6:55:28  lr: 0.000020  min_lr: 0.000000  loss: 3.5263 (3.5263)  class_acc: 0.4167 (0.4167)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.8744  data: 8.4454  max mem: 15572
Epoch: [24]  [  10/2809]  eta: 1:06:14  lr: 0.000020  min_lr: 0.000000  loss: 3.8892 (3.7813)  class_acc: 0.2917 (0.3220)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4199  data: 0.9673  max mem: 15572
Epoch: [24]  [  20/2809]  eta: 0:52:26  lr: 0.000020  min_lr: 0.000000  loss: 3.8078 (3.7509)  class_acc: 0.2917 (0.3274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7411  data: 0.3047  max mem: 15572
Epoch: [24]  [  30/2809]  eta: 0:45:29  lr: 0.000020  min_lr: 0.000000  loss: 3.8161 (3.8466)  class_acc: 0.2500 (0.3065)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7413  data: 0.2912  max mem: 15572
Epoch: [24]  [  40/2809]  eta: 0:42:15  lr: 0.000020  min_lr: 0.000000  loss: 4.0558 (3.8169)  class_acc: 0.2083 (0.3140)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6921  data: 0.2241  max mem: 15572
Epoch: [24]  [  50/2809]  eta: 0:39:46  lr: 0.000020  min_lr: 0.000000  loss: 3.6894 (3.8034)  class_acc: 0.2500 (0.3113)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6835  data: 0.2259  max mem: 15572
Epoch: [24]  [  60/2809]  eta: 0:38:17  lr: 0.000020  min_lr: 0.000000  loss: 3.6967 (3.7764)  class_acc: 0.3333 (0.3251)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6725  data: 0.2183  max mem: 15572
Epoch: [24]  [  70/2809]  eta: 0:36:53  lr: 0.000020  min_lr: 0.000000  loss: 3.7502 (3.7849)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.2129  max mem: 15572
Epoch: [24]  [  80/2809]  eta: 0:34:24  lr: 0.000020  min_lr: 0.000000  loss: 3.5597 (3.7620)  class_acc: 0.3333 (0.3318)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0928  max mem: 15572
Epoch: [24]  [  90/2809]  eta: 0:32:42  lr: 0.000020  min_lr: 0.000000  loss: 3.6800 (3.7741)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4152  data: 0.0004  max mem: 15572
Epoch: [24]  [ 100/2809]  eta: 0:31:17  lr: 0.000020  min_lr: 0.000000  loss: 3.8027 (3.7635)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4364  data: 0.0005  max mem: 15572
[2025-01-16 01:50:10,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:50:10,852] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:50:12,062] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67523
[2025-01-16 01:50:12,062] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:50:12,062] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 110/2809]  eta: 0:30:29  lr: 0.000020  min_lr: 0.000000  loss: 3.7643 (3.7615)  class_acc: 0.3333 (0.3326)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.4768  data: 0.0473  max mem: 15572
Epoch: [24]  [ 120/2809]  eta: 0:29:46  lr: 0.000020  min_lr: 0.000000  loss: 3.6799 (3.7548)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0677  max mem: 15572
Epoch: [24]  [ 130/2809]  eta: 0:29:44  lr: 0.000020  min_lr: 0.000000  loss: 3.6956 (3.7558)  class_acc: 0.2500 (0.3302)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1476  max mem: 15572
Epoch: [24]  [ 140/2809]  eta: 0:29:04  lr: 0.000020  min_lr: 0.000000  loss: 3.5621 (3.7362)  class_acc: 0.3750 (0.3348)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1562  max mem: 15572
Epoch: [24]  [ 150/2809]  eta: 0:29:01  lr: 0.000020  min_lr: 0.000000  loss: 3.6689 (3.7506)  class_acc: 0.3750 (0.3317)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1469  max mem: 15572
Epoch: [24]  [ 160/2809]  eta: 0:28:44  lr: 0.000020  min_lr: 0.000000  loss: 3.8484 (3.7510)  class_acc: 0.2917 (0.3320)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.1932  max mem: 15572
Epoch: [24]  [ 170/2809]  eta: 0:28:25  lr: 0.000020  min_lr: 0.000000  loss: 3.8942 (3.7636)  class_acc: 0.3750 (0.3331)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1371  max mem: 15572
Epoch: [24]  [ 180/2809]  eta: 0:28:19  lr: 0.000020  min_lr: 0.000000  loss: 3.8942 (3.7622)  class_acc: 0.3333 (0.3336)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1760  max mem: 15572
Epoch: [24]  [ 190/2809]  eta: 0:27:52  lr: 0.000020  min_lr: 0.000000  loss: 3.8099 (3.7649)  class_acc: 0.2917 (0.3301)  loss_scale: 65536.0000 (66565.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1429  max mem: 15572
Epoch: [24]  [ 200/2809]  eta: 0:27:34  lr: 0.000020  min_lr: 0.000000  loss: 3.8099 (3.7655)  class_acc: 0.2917 (0.3306)  loss_scale: 65536.0000 (66514.1493)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0948  max mem: 15572
Epoch: [24]  [ 210/2809]  eta: 0:27:20  lr: 0.000020  min_lr: 0.000000  loss: 3.8362 (3.7599)  class_acc: 0.3333 (0.3318)  loss_scale: 65536.0000 (66467.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1259  max mem: 15572
Epoch: [24]  [ 220/2809]  eta: 0:27:06  lr: 0.000020  min_lr: 0.000000  loss: 3.9078 (3.7642)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1377  max mem: 15572
Epoch: [24]  [ 230/2809]  eta: 0:26:47  lr: 0.000020  min_lr: 0.000000  loss: 3.8858 (3.7674)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (66387.1169)  weight_decay: 0.0500 (0.0500)  time: 0.5433  data: 0.1163  max mem: 15572
[2025-01-16 01:51:26,207] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:51:26,207] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:51:27,591] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67655
[2025-01-16 01:51:27,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:51:27,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 240/2809]  eta: 0:26:47  lr: 0.000020  min_lr: 0.000000  loss: 3.8601 (3.7681)  class_acc: 0.2917 (0.3288)  loss_scale: 65536.0000 (67167.6017)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1585  max mem: 15572
Epoch: [24]  [ 250/2809]  eta: 0:26:52  lr: 0.000020  min_lr: 0.000000  loss: 3.9328 (3.7767)  class_acc: 0.2917 (0.3282)  loss_scale: 65536.0000 (67102.5976)  weight_decay: 0.0500 (0.0500)  time: 0.7078  data: 0.2718  max mem: 15572
Epoch: [24]  [ 260/2809]  eta: 0:26:39  lr: 0.000020  min_lr: 0.000000  loss: 3.9017 (3.7776)  class_acc: 0.2500 (0.3266)  loss_scale: 65536.0000 (67042.5747)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.2195  max mem: 15572
Epoch: [24]  [ 270/2809]  eta: 0:26:20  lr: 0.000020  min_lr: 0.000000  loss: 3.8766 (3.7841)  class_acc: 0.2500 (0.3235)  loss_scale: 65536.0000 (66986.9815)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0831  max mem: 15572
Epoch: [24]  [ 280/2809]  eta: 0:26:12  lr: 0.000020  min_lr: 0.000000  loss: 4.0297 (3.7865)  class_acc: 0.2083 (0.3224)  loss_scale: 65536.0000 (66935.3452)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1034  max mem: 15572
Epoch: [24]  [ 290/2809]  eta: 0:25:59  lr: 0.000020  min_lr: 0.000000  loss: 4.0297 (3.7906)  class_acc: 0.2500 (0.3222)  loss_scale: 65536.0000 (66887.2577)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1405  max mem: 15572
[2025-01-16 01:52:01,361] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67711
[2025-01-16 01:52:01,361] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:52:01,361] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 300/2809]  eta: 0:25:58  lr: 0.000020  min_lr: 0.000000  loss: 3.9307 (3.7889)  class_acc: 0.2917 (0.3224)  loss_scale: 65536.0000 (66189.1827)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1802  max mem: 15572
Epoch: [24]  [ 310/2809]  eta: 0:25:40  lr: 0.000020  min_lr: 0.000000  loss: 3.7109 (3.7885)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (65114.5466)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1612  max mem: 15572
Epoch: [24]  [ 320/2809]  eta: 0:25:27  lr: 0.000020  min_lr: 0.000000  loss: 3.8080 (3.7914)  class_acc: 0.3333 (0.3229)  loss_scale: 32768.0000 (64106.8660)  weight_decay: 0.0500 (0.0500)  time: 0.4963  data: 0.0819  max mem: 15572
Epoch: [24]  [ 330/2809]  eta: 0:25:11  lr: 0.000020  min_lr: 0.000000  loss: 3.6406 (3.7926)  class_acc: 0.2917 (0.3219)  loss_scale: 32768.0000 (63160.0725)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0736  max mem: 15572
Epoch: [24]  [ 340/2809]  eta: 0:25:06  lr: 0.000020  min_lr: 0.000000  loss: 3.5695 (3.7873)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (62268.8094)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1263  max mem: 15572
Epoch: [24]  [ 350/2809]  eta: 0:24:51  lr: 0.000020  min_lr: 0.000000  loss: 3.6871 (3.7878)  class_acc: 0.3750 (0.3232)  loss_scale: 32768.0000 (61428.3305)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1261  max mem: 15572
Epoch: [24]  [ 360/2809]  eta: 0:24:44  lr: 0.000020  min_lr: 0.000000  loss: 3.7386 (3.7874)  class_acc: 0.3333 (0.3234)  loss_scale: 32768.0000 (60634.4155)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.1044  max mem: 15572
Epoch: [24]  [ 370/2809]  eta: 0:24:35  lr: 0.000020  min_lr: 0.000000  loss: 3.9027 (3.7890)  class_acc: 0.3333 (0.3230)  loss_scale: 32768.0000 (59883.2992)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1473  max mem: 15572
Epoch: [24]  [ 380/2809]  eta: 0:24:27  lr: 0.000020  min_lr: 0.000000  loss: 3.9502 (3.7948)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (59171.6115)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1401  max mem: 15572
Epoch: [24]  [ 390/2809]  eta: 0:24:23  lr: 0.000020  min_lr: 0.000000  loss: 3.9502 (3.7961)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (58496.3274)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1762  max mem: 15572
Epoch: [24]  [ 400/2809]  eta: 0:24:13  lr: 0.000020  min_lr: 0.000000  loss: 3.7852 (3.7967)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (57854.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1508  max mem: 15572
Epoch: [24]  [ 410/2809]  eta: 0:24:03  lr: 0.000020  min_lr: 0.000000  loss: 3.7301 (3.7961)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (57244.3406)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.1021  max mem: 15572
Epoch: [24]  [ 420/2809]  eta: 0:23:50  lr: 0.000020  min_lr: 0.000000  loss: 3.6511 (3.7943)  class_acc: 0.3333 (0.3218)  loss_scale: 32768.0000 (56662.9549)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0789  max mem: 15572
[2025-01-16 01:53:13,354] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:53:13,354] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 430/2809]  eta: 0:23:38  lr: 0.000020  min_lr: 0.000000  loss: 3.6562 (3.7923)  class_acc: 0.2917 (0.3210)  loss_scale: 32768.0000 (56640.7425)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0350  max mem: 15572
Epoch: [24]  [ 440/2809]  eta: 0:23:29  lr: 0.000020  min_lr: 0.000000  loss: 3.6110 (3.7859)  class_acc: 0.3750 (0.3230)  loss_scale: 65536.0000 (56842.4490)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0714  max mem: 15572
Epoch: [24]  [ 450/2809]  eta: 0:23:27  lr: 0.000020  min_lr: 0.000000  loss: 3.4913 (3.7839)  class_acc: 0.3750 (0.3235)  loss_scale: 65536.0000 (57035.2106)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1513  max mem: 15572
Epoch: [24]  [ 460/2809]  eta: 0:23:20  lr: 0.000020  min_lr: 0.000000  loss: 3.8451 (3.7858)  class_acc: 0.2917 (0.3228)  loss_scale: 65536.0000 (57219.6095)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1702  max mem: 15572
Epoch: [24]  [ 470/2809]  eta: 0:23:17  lr: 0.000020  min_lr: 0.000000  loss: 3.8517 (3.7874)  class_acc: 0.2917 (0.3218)  loss_scale: 65536.0000 (57396.1783)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1848  max mem: 15572
Epoch: [24]  [ 480/2809]  eta: 0:23:07  lr: 0.000020  min_lr: 0.000000  loss: 3.7820 (3.7857)  class_acc: 0.2917 (0.3225)  loss_scale: 65536.0000 (57565.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1464  max mem: 15572
Epoch: [24]  [ 490/2809]  eta: 0:23:01  lr: 0.000020  min_lr: 0.000000  loss: 3.8635 (3.7892)  class_acc: 0.2917 (0.3220)  loss_scale: 65536.0000 (57727.7393)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1186  max mem: 15572
Epoch: [24]  [ 500/2809]  eta: 0:22:53  lr: 0.000020  min_lr: 0.000000  loss: 3.9751 (3.7938)  class_acc: 0.2917 (0.3213)  loss_scale: 65536.0000 (57883.5928)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1394  max mem: 15572
Epoch: [24]  [ 510/2809]  eta: 0:22:48  lr: 0.000020  min_lr: 0.000000  loss: 3.9751 (3.7962)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (58033.3464)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1652  max mem: 15572
Epoch: [24]  [ 520/2809]  eta: 0:22:40  lr: 0.000020  min_lr: 0.000000  loss: 3.9167 (3.7962)  class_acc: 0.2917 (0.3193)  loss_scale: 65536.0000 (58177.3512)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1576  max mem: 15572
Epoch: [24]  [ 530/2809]  eta: 0:22:33  lr: 0.000020  min_lr: 0.000000  loss: 3.7452 (3.7929)  class_acc: 0.2917 (0.3202)  loss_scale: 65536.0000 (58315.9322)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1019  max mem: 15572
Epoch: [24]  [ 540/2809]  eta: 0:22:28  lr: 0.000020  min_lr: 0.000000  loss: 3.7452 (3.7937)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (58449.3900)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1392  max mem: 15572
Epoch: [24]  [ 550/2809]  eta: 0:22:19  lr: 0.000020  min_lr: 0.000000  loss: 3.8871 (3.7926)  class_acc: 0.2500 (0.3189)  loss_scale: 65536.0000 (58578.0036)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1209  max mem: 15572
[2025-01-16 01:54:26,703] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:54:26,704] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:54:27,179] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67969
[2025-01-16 01:54:27,180] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:54:27,180] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 560/2809]  eta: 0:22:12  lr: 0.000020  min_lr: 0.000000  loss: 3.7616 (3.7924)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (58818.8520)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0981  max mem: 15572
Epoch: [24]  [ 570/2809]  eta: 0:22:03  lr: 0.000020  min_lr: 0.000000  loss: 3.8287 (3.7934)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (58936.4904)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0903  max mem: 15572
[2025-01-16 01:54:39,734] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 67993
[2025-01-16 01:54:39,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:54:39,735] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 580/2809]  eta: 0:21:54  lr: 0.000020  min_lr: 0.000000  loss: 3.8287 (3.7905)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (58824.4819)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0610  max mem: 15572
[2025-01-16 01:54:43,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=68000, skipped=455, lr=[1.926765836973862e-07, 1.926765836973862e-07, 2.7525226242483745e-07, 2.7525226242483745e-07, 3.932175177497678e-07, 3.932175177497678e-07, 5.617393110710969e-07, 5.617393110710969e-07, 8.024847301015671e-07, 8.024847301015671e-07, 1.146406757287953e-06, 1.146406757287953e-06, 1.6377239389827901e-06, 1.6377239389827901e-06, 2.339605627118272e-06, 2.339605627118272e-06, 3.3422937530261024e-06, 3.3422937530261024e-06, 4.774705361465862e-06, 4.774705361465862e-06, 6.821007659236944e-06, 6.821007659236944e-06, 9.74429665605278e-06, 9.74429665605278e-06, 1.3920423794361114e-05, 1.3920423794361114e-05, 1.9886319706230164e-05, 1.9886319706230164e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 01:54:43,142] [INFO] [timer.py:260:stop] epoch=0/micro_step=68000/global_step=68000, RunningAvgSamplesPerSec=28.543561965653044, CurrSamplesPerSec=28.213850960853932, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [ 590/2809]  eta: 0:21:51  lr: 0.000020  min_lr: 0.000000  loss: 3.7439 (3.7914)  class_acc: 0.3333 (0.3179)  loss_scale: 32768.0000 (58383.5939)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.1618  max mem: 15572
Epoch: [24]  [ 600/2809]  eta: 0:21:39  lr: 0.000020  min_lr: 0.000000  loss: 3.7382 (3.7885)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (57957.3777)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1226  max mem: 15572
Epoch: [24]  [ 610/2809]  eta: 0:21:35  lr: 0.000020  min_lr: 0.000000  loss: 3.6442 (3.7908)  class_acc: 0.3333 (0.3181)  loss_scale: 32768.0000 (57545.1129)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0980  max mem: 15572
Epoch: [24]  [ 620/2809]  eta: 0:21:28  lr: 0.000020  min_lr: 0.000000  loss: 3.8831 (3.7935)  class_acc: 0.2500 (0.3174)  loss_scale: 32768.0000 (57146.1256)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1626  max mem: 15572
Epoch: [24]  [ 630/2809]  eta: 0:21:20  lr: 0.000020  min_lr: 0.000000  loss: 3.6613 (3.7895)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (56759.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.1037  max mem: 15572
Epoch: [24]  [ 640/2809]  eta: 0:21:14  lr: 0.000020  min_lr: 0.000000  loss: 3.5741 (3.7884)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (56385.4977)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0984  max mem: 15572
Epoch: [24]  [ 650/2809]  eta: 0:21:04  lr: 0.000020  min_lr: 0.000000  loss: 3.7066 (3.7877)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (56022.7097)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0822  max mem: 15572
Epoch: [24]  [ 660/2809]  eta: 0:20:57  lr: 0.000020  min_lr: 0.000000  loss: 3.7529 (3.7859)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (55670.8986)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0557  max mem: 15572
Epoch: [24]  [ 670/2809]  eta: 0:20:50  lr: 0.000020  min_lr: 0.000000  loss: 3.7583 (3.7867)  class_acc: 0.2917 (0.3194)  loss_scale: 32768.0000 (55329.5738)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.0977  max mem: 15572
Epoch: [24]  [ 680/2809]  eta: 0:20:44  lr: 0.000020  min_lr: 0.000000  loss: 3.7665 (3.7833)  class_acc: 0.3333 (0.3205)  loss_scale: 32768.0000 (54998.2731)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1234  max mem: 15572
Epoch: [24]  [ 690/2809]  eta: 0:20:36  lr: 0.000020  min_lr: 0.000000  loss: 3.7629 (3.7841)  class_acc: 0.3333 (0.3207)  loss_scale: 32768.0000 (54676.5615)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0842  max mem: 15572
Epoch: [24]  [ 700/2809]  eta: 0:20:30  lr: 0.000020  min_lr: 0.000000  loss: 3.7629 (3.7838)  class_acc: 0.3750 (0.3214)  loss_scale: 32768.0000 (54364.0285)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0813  max mem: 15572
[2025-01-16 01:55:51,186] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:55:51,187] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [ 710/2809]  eta: 0:20:24  lr: 0.000020  min_lr: 0.000000  loss: 3.6552 (3.7807)  class_acc: 0.3750 (0.3224)  loss_scale: 32768.0000 (54290.7229)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1061  max mem: 15572
Epoch: [24]  [ 720/2809]  eta: 0:20:15  lr: 0.000020  min_lr: 0.000000  loss: 3.7186 (3.7834)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (54446.6907)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0569  max mem: 15572
Epoch: [24]  [ 730/2809]  eta: 0:20:09  lr: 0.000020  min_lr: 0.000000  loss: 3.7769 (3.7805)  class_acc: 0.2917 (0.3227)  loss_scale: 65536.0000 (54598.3912)  weight_decay: 0.0500 (0.0500)  time: 0.5141  data: 0.0589  max mem: 15572
Epoch: [24]  [ 740/2809]  eta: 0:20:01  lr: 0.000020  min_lr: 0.000000  loss: 3.6783 (3.7798)  class_acc: 0.3750 (0.3234)  loss_scale: 65536.0000 (54745.9973)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1021  max mem: 15572
Epoch: [24]  [ 750/2809]  eta: 0:19:53  lr: 0.000020  min_lr: 0.000000  loss: 3.6783 (3.7787)  class_acc: 0.3333 (0.3232)  loss_scale: 65536.0000 (54889.6724)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0773  max mem: 15572
Epoch: [24]  [ 760/2809]  eta: 0:19:48  lr: 0.000020  min_lr: 0.000000  loss: 3.8618 (3.7809)  class_acc: 0.2917 (0.3227)  loss_scale: 65536.0000 (55029.5716)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0924  max mem: 15572
Epoch: [24]  [ 770/2809]  eta: 0:19:43  lr: 0.000020  min_lr: 0.000000  loss: 4.0251 (3.7841)  class_acc: 0.2917 (0.3221)  loss_scale: 65536.0000 (55165.8418)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1515  max mem: 15572
Epoch: [24]  [ 780/2809]  eta: 0:19:39  lr: 0.000020  min_lr: 0.000000  loss: 4.0577 (3.7874)  class_acc: 0.2917 (0.3219)  loss_scale: 65536.0000 (55298.6223)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.2141  max mem: 15572
Epoch: [24]  [ 790/2809]  eta: 0:19:33  lr: 0.000020  min_lr: 0.000000  loss: 3.9104 (3.7872)  class_acc: 0.2917 (0.3221)  loss_scale: 65536.0000 (55428.0455)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.2062  max mem: 15572
Epoch: [24]  [ 800/2809]  eta: 0:19:27  lr: 0.000020  min_lr: 0.000000  loss: 3.7519 (3.7867)  class_acc: 0.3333 (0.3225)  loss_scale: 65536.0000 (55554.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1371  max mem: 15572
Epoch: [24]  [ 810/2809]  eta: 0:19:24  lr: 0.000020  min_lr: 0.000000  loss: 3.7371 (3.7880)  class_acc: 0.2917 (0.3223)  loss_scale: 65536.0000 (55677.3169)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1806  max mem: 15572
Epoch: [24]  [ 820/2809]  eta: 0:19:15  lr: 0.000020  min_lr: 0.000000  loss: 3.8887 (3.7911)  class_acc: 0.2917 (0.3213)  loss_scale: 65536.0000 (55797.3983)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1227  max mem: 15572
Epoch: [24]  [ 830/2809]  eta: 0:19:08  lr: 0.000020  min_lr: 0.000000  loss: 3.9872 (3.7929)  class_acc: 0.2500 (0.3210)  loss_scale: 65536.0000 (55914.5897)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0568  max mem: 15572
[2025-01-16 01:57:03,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:57:03,521] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 01:57:03,917] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68251
[2025-01-16 01:57:03,918] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 01:57:03,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 840/2809]  eta: 0:19:00  lr: 0.000020  min_lr: 0.000000  loss: 3.9872 (3.7951)  class_acc: 0.2917 (0.3209)  loss_scale: 65536.0000 (56106.9203)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0888  max mem: 15572
Epoch: [24]  [ 850/2809]  eta: 0:18:55  lr: 0.000020  min_lr: 0.000000  loss: 3.9477 (3.7964)  class_acc: 0.2500 (0.3202)  loss_scale: 65536.0000 (56217.7203)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1132  max mem: 15572
Epoch: [24]  [ 860/2809]  eta: 0:18:50  lr: 0.000020  min_lr: 0.000000  loss: 3.9558 (3.7982)  class_acc: 0.2500 (0.3198)  loss_scale: 65536.0000 (56325.9466)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1610  max mem: 15572
Epoch: [24]  [ 870/2809]  eta: 0:18:42  lr: 0.000020  min_lr: 0.000000  loss: 3.7579 (3.7965)  class_acc: 0.3333 (0.3200)  loss_scale: 65536.0000 (56431.6877)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1093  max mem: 15572
Epoch: [24]  [ 880/2809]  eta: 0:18:36  lr: 0.000020  min_lr: 0.000000  loss: 3.6631 (3.7965)  class_acc: 0.3333 (0.3201)  loss_scale: 65536.0000 (56535.0284)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0989  max mem: 15572
Epoch: [24]  [ 890/2809]  eta: 0:18:30  lr: 0.000020  min_lr: 0.000000  loss: 3.8382 (3.7970)  class_acc: 0.2917 (0.3199)  loss_scale: 65536.0000 (56636.0494)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1459  max mem: 15572
Epoch: [24]  [ 900/2809]  eta: 0:18:25  lr: 0.000020  min_lr: 0.000000  loss: 3.9938 (3.7979)  class_acc: 0.2500 (0.3196)  loss_scale: 65536.0000 (56734.8280)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1526  max mem: 15572
Epoch: [24]  [ 910/2809]  eta: 0:18:20  lr: 0.000020  min_lr: 0.000000  loss: 3.8336 (3.7979)  class_acc: 0.2500 (0.3194)  loss_scale: 65536.0000 (56831.4380)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1806  max mem: 15572
Epoch: [24]  [ 920/2809]  eta: 0:18:13  lr: 0.000020  min_lr: 0.000000  loss: 3.7685 (3.7987)  class_acc: 0.2917 (0.3195)  loss_scale: 65536.0000 (56925.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1545  max mem: 15572
Epoch: [24]  [ 930/2809]  eta: 0:18:07  lr: 0.000020  min_lr: 0.000000  loss: 3.9878 (3.8016)  class_acc: 0.2917 (0.3186)  loss_scale: 65536.0000 (57018.4318)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.1029  max mem: 15572
Epoch: [24]  [ 940/2809]  eta: 0:18:00  lr: 0.000020  min_lr: 0.000000  loss: 3.9677 (3.8031)  class_acc: 0.2500 (0.3184)  loss_scale: 65536.0000 (57108.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0836  max mem: 15572
Epoch: [24]  [ 950/2809]  eta: 0:17:55  lr: 0.000020  min_lr: 0.000000  loss: 3.9677 (3.8049)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (57197.5605)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1371  max mem: 15572
[2025-01-16 01:58:14,720] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68374
[2025-01-16 01:58:14,720] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 01:58:14,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [ 960/2809]  eta: 0:17:50  lr: 0.000020  min_lr: 0.000000  loss: 4.0649 (3.8070)  class_acc: 0.1667 (0.3169)  loss_scale: 65536.0000 (57182.0354)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.1829  max mem: 15572
Epoch: [24]  [ 970/2809]  eta: 0:17:44  lr: 0.000020  min_lr: 0.000000  loss: 3.9108 (3.8078)  class_acc: 0.2500 (0.3169)  loss_scale: 32768.0000 (56930.6035)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1418  max mem: 15572
Epoch: [24]  [ 980/2809]  eta: 0:17:37  lr: 0.000020  min_lr: 0.000000  loss: 3.8174 (3.8080)  class_acc: 0.2917 (0.3169)  loss_scale: 32768.0000 (56684.2977)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1163  max mem: 15572
Epoch: [24]  [ 990/2809]  eta: 0:17:31  lr: 0.000020  min_lr: 0.000000  loss: 3.9005 (3.8097)  class_acc: 0.2917 (0.3165)  loss_scale: 32768.0000 (56442.9627)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1192  max mem: 15572
Epoch: [24]  [1000/2809]  eta: 0:17:25  lr: 0.000020  min_lr: 0.000000  loss: 3.9872 (3.8104)  class_acc: 0.2500 (0.3163)  loss_scale: 32768.0000 (56206.4496)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1535  max mem: 15572
Epoch: [24]  [1010/2809]  eta: 0:17:21  lr: 0.000020  min_lr: 0.000000  loss: 3.9156 (3.8109)  class_acc: 0.3333 (0.3162)  loss_scale: 32768.0000 (55974.6152)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1814  max mem: 15572
Epoch: [24]  [1020/2809]  eta: 0:17:15  lr: 0.000020  min_lr: 0.000000  loss: 3.9875 (3.8140)  class_acc: 0.2917 (0.3157)  loss_scale: 32768.0000 (55747.3222)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1616  max mem: 15572
Epoch: [24]  [1030/2809]  eta: 0:17:09  lr: 0.000020  min_lr: 0.000000  loss: 3.9957 (3.8140)  class_acc: 0.2500 (0.3157)  loss_scale: 32768.0000 (55524.4384)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1469  max mem: 15572
Epoch: [24]  [1040/2809]  eta: 0:17:03  lr: 0.000020  min_lr: 0.000000  loss: 3.8654 (3.8129)  class_acc: 0.2917 (0.3161)  loss_scale: 32768.0000 (55305.8367)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1455  max mem: 15572
Epoch: [24]  [1050/2809]  eta: 0:16:57  lr: 0.000020  min_lr: 0.000000  loss: 3.8654 (3.8155)  class_acc: 0.2500 (0.3151)  loss_scale: 32768.0000 (55091.3949)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1209  max mem: 15572
Epoch: [24]  [1060/2809]  eta: 0:16:51  lr: 0.000020  min_lr: 0.000000  loss: 3.9220 (3.8153)  class_acc: 0.2917 (0.3152)  loss_scale: 32768.0000 (54880.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1063  max mem: 15572
Epoch: [24]  [1070/2809]  eta: 0:16:43  lr: 0.000020  min_lr: 0.000000  loss: 3.7030 (3.8132)  class_acc: 0.3750 (0.3157)  loss_scale: 32768.0000 (54674.5247)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0705  max mem: 15572
Epoch: [24]  [1080/2809]  eta: 0:16:38  lr: 0.000020  min_lr: 0.000000  loss: 3.5560 (3.8115)  class_acc: 0.3333 (0.3160)  loss_scale: 32768.0000 (54471.8742)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.1055  max mem: 15572
[2025-01-16 01:59:26,469] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 01:59:26,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [1090/2809]  eta: 0:16:32  lr: 0.000020  min_lr: 0.000000  loss: 3.5671 (3.8088)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (54393.0779)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1486  max mem: 15572
Epoch: [24]  [1100/2809]  eta: 0:16:27  lr: 0.000020  min_lr: 0.000000  loss: 3.5671 (3.8069)  class_acc: 0.3750 (0.3172)  loss_scale: 65536.0000 (54494.2852)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1705  max mem: 15572
Epoch: [24]  [1110/2809]  eta: 0:16:21  lr: 0.000019  min_lr: 0.000000  loss: 3.6466 (3.8084)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (54593.6706)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.1715  max mem: 15572
Epoch: [24]  [1120/2809]  eta: 0:16:16  lr: 0.000019  min_lr: 0.000000  loss: 3.7079 (3.8076)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (54691.2828)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1523  max mem: 15572
Epoch: [24]  [1130/2809]  eta: 0:16:08  lr: 0.000019  min_lr: 0.000000  loss: 3.7045 (3.8069)  class_acc: 0.3333 (0.3172)  loss_scale: 65536.0000 (54787.1689)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0900  max mem: 15572
Epoch: [24]  [1140/2809]  eta: 0:16:02  lr: 0.000019  min_lr: 0.000000  loss: 3.7774 (3.8078)  class_acc: 0.3333 (0.3172)  loss_scale: 65536.0000 (54881.3742)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0460  max mem: 15572
Epoch: [24]  [1150/2809]  eta: 0:15:56  lr: 0.000019  min_lr: 0.000000  loss: 4.0944 (3.8105)  class_acc: 0.2917 (0.3167)  loss_scale: 65536.0000 (54973.9427)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1011  max mem: 15572
Epoch: [24]  [1160/2809]  eta: 0:15:52  lr: 0.000019  min_lr: 0.000000  loss: 3.8795 (3.8083)  class_acc: 0.3333 (0.3173)  loss_scale: 65536.0000 (55064.9165)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1766  max mem: 15572
Epoch: [24]  [1170/2809]  eta: 0:15:46  lr: 0.000019  min_lr: 0.000000  loss: 3.5786 (3.8077)  class_acc: 0.3333 (0.3174)  loss_scale: 65536.0000 (55154.3365)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1951  max mem: 15572
Epoch: [24]  [1180/2809]  eta: 0:15:39  lr: 0.000019  min_lr: 0.000000  loss: 3.9261 (3.8092)  class_acc: 0.2917 (0.3172)  loss_scale: 65536.0000 (55242.2422)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1038  max mem: 15572
Epoch: [24]  [1190/2809]  eta: 0:15:34  lr: 0.000019  min_lr: 0.000000  loss: 3.9013 (3.8078)  class_acc: 0.3333 (0.3176)  loss_scale: 65536.0000 (55328.6717)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1011  max mem: 15572
Epoch: [24]  [1200/2809]  eta: 0:15:27  lr: 0.000019  min_lr: 0.000000  loss: 3.7901 (3.8077)  class_acc: 0.3750 (0.3175)  loss_scale: 65536.0000 (55413.6619)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1111  max mem: 15572
Epoch: [24]  [1210/2809]  eta: 0:15:21  lr: 0.000019  min_lr: 0.000000  loss: 3.8380 (3.8085)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (55497.2486)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0636  max mem: 15572
[2025-01-16 02:00:39,488] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:00:39,489] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:00:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68632
[2025-01-16 02:00:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:00:39,869] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1220/2809]  eta: 0:15:15  lr: 0.000019  min_lr: 0.000000  loss: 3.8119 (3.8066)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (55633.1400)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0927  max mem: 15572
Epoch: [24]  [1230/2809]  eta: 0:15:10  lr: 0.000019  min_lr: 0.000000  loss: 3.5851 (3.8086)  class_acc: 0.2500 (0.3171)  loss_scale: 65536.0000 (55713.5857)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1466  max mem: 15572
Epoch: [24]  [1240/2809]  eta: 0:15:05  lr: 0.000019  min_lr: 0.000000  loss: 3.9956 (3.8098)  class_acc: 0.2500 (0.3166)  loss_scale: 65536.0000 (55792.7349)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.1913  max mem: 15572
Epoch: [24]  [1250/2809]  eta: 0:14:58  lr: 0.000019  min_lr: 0.000000  loss: 3.9844 (3.8119)  class_acc: 0.2500 (0.3163)  loss_scale: 65536.0000 (55870.6187)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1331  max mem: 15572
Epoch: [24]  [1260/2809]  eta: 0:14:53  lr: 0.000019  min_lr: 0.000000  loss: 3.9844 (3.8116)  class_acc: 0.2917 (0.3167)  loss_scale: 65536.0000 (55947.2672)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0793  max mem: 15572
Epoch: [24]  [1270/2809]  eta: 0:14:46  lr: 0.000019  min_lr: 0.000000  loss: 3.7316 (3.8091)  class_acc: 0.3750 (0.3172)  loss_scale: 65536.0000 (56022.7097)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0835  max mem: 15572
Epoch: [24]  [1280/2809]  eta: 0:14:40  lr: 0.000019  min_lr: 0.000000  loss: 3.7265 (3.8088)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (56096.9742)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0529  max mem: 15572
Epoch: [24]  [1290/2809]  eta: 0:14:33  lr: 0.000019  min_lr: 0.000000  loss: 3.8737 (3.8085)  class_acc: 0.3333 (0.3171)  loss_scale: 65536.0000 (56170.0883)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0652  max mem: 15572
Epoch: [24]  [1300/2809]  eta: 0:14:26  lr: 0.000019  min_lr: 0.000000  loss: 3.8737 (3.8089)  class_acc: 0.3333 (0.3171)  loss_scale: 65536.0000 (56242.0784)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0477  max mem: 15572
Epoch: [24]  [1310/2809]  eta: 0:14:21  lr: 0.000019  min_lr: 0.000000  loss: 3.9587 (3.8105)  class_acc: 0.3333 (0.3171)  loss_scale: 65536.0000 (56312.9703)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0944  max mem: 15572
Epoch: [24]  [1320/2809]  eta: 0:14:15  lr: 0.000019  min_lr: 0.000000  loss: 3.9587 (3.8090)  class_acc: 0.3333 (0.3174)  loss_scale: 65536.0000 (56382.7888)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1436  max mem: 15572
Epoch: [24]  [1330/2809]  eta: 0:14:10  lr: 0.000019  min_lr: 0.000000  loss: 3.6408 (3.8082)  class_acc: 0.3750 (0.3175)  loss_scale: 65536.0000 (56451.5582)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1279  max mem: 15572
Epoch: [24]  [1340/2809]  eta: 0:14:04  lr: 0.000019  min_lr: 0.000000  loss: 3.7236 (3.8089)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (56519.3020)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1161  max mem: 15572
[2025-01-16 02:01:52,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:01:52,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:01:54,760] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68764
[2025-01-16 02:01:54,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:01:54,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1350/2809]  eta: 0:13:58  lr: 0.000019  min_lr: 0.000000  loss: 3.7307 (3.8078)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (56731.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0744  max mem: 15572
Epoch: [24]  [1360/2809]  eta: 0:13:53  lr: 0.000019  min_lr: 0.000000  loss: 3.7220 (3.8067)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (56796.2616)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1196  max mem: 15572
Epoch: [24]  [1370/2809]  eta: 0:13:47  lr: 0.000019  min_lr: 0.000000  loss: 3.7639 (3.8082)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (56860.0088)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1362  max mem: 15572
Epoch: [24]  [1380/2809]  eta: 0:13:42  lr: 0.000019  min_lr: 0.000000  loss: 3.9312 (3.8079)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (56922.8327)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.1456  max mem: 15572
Epoch: [24]  [1390/2809]  eta: 0:13:36  lr: 0.000019  min_lr: 0.000000  loss: 3.8582 (3.8086)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (56984.7534)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.1787  max mem: 15572
Epoch: [24]  [1400/2809]  eta: 0:13:30  lr: 0.000019  min_lr: 0.000000  loss: 3.9254 (3.8083)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (57045.7901)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1471  max mem: 15572
Epoch: [24]  [1410/2809]  eta: 0:13:24  lr: 0.000019  min_lr: 0.000000  loss: 3.9368 (3.8083)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (57105.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1206  max mem: 15572
Epoch: [24]  [1420/2809]  eta: 0:13:17  lr: 0.000019  min_lr: 0.000000  loss: 3.8697 (3.8093)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (57165.2864)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0509  max mem: 15572
Epoch: [24]  [1430/2809]  eta: 0:13:11  lr: 0.000019  min_lr: 0.000000  loss: 3.9066 (3.8093)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (57223.7820)  weight_decay: 0.0500 (0.0500)  time: 0.4572  data: 0.0235  max mem: 15572
Epoch: [24]  [1440/2809]  eta: 0:13:05  lr: 0.000019  min_lr: 0.000000  loss: 3.8523 (3.8086)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (57281.4656)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1203  max mem: 15572
Epoch: [24]  [1450/2809]  eta: 0:12:59  lr: 0.000019  min_lr: 0.000000  loss: 3.6277 (3.8074)  class_acc: 0.3750 (0.3183)  loss_scale: 65536.0000 (57338.3542)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1443  max mem: 15572
Epoch: [24]  [1460/2809]  eta: 0:12:54  lr: 0.000019  min_lr: 0.000000  loss: 3.7246 (3.8075)  class_acc: 0.3750 (0.3184)  loss_scale: 65536.0000 (57394.4641)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1328  max mem: 15572
Epoch: [24]  [1470/2809]  eta: 0:12:48  lr: 0.000019  min_lr: 0.000000  loss: 3.9570 (3.8087)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (57449.8110)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1737  max mem: 15572
[2025-01-16 02:03:07,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:03:07,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:03:07,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 68894
[2025-01-16 02:03:07,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:03:07,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1480/2809]  eta: 0:12:43  lr: 0.000019  min_lr: 0.000000  loss: 3.9192 (3.8088)  class_acc: 0.2500 (0.3175)  loss_scale: 65536.0000 (57548.6617)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1493  max mem: 15572
Epoch: [24]  [1490/2809]  eta: 0:12:37  lr: 0.000019  min_lr: 0.000000  loss: 3.8170 (3.8078)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (57602.2321)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1606  max mem: 15572
Epoch: [24]  [1500/2809]  eta: 0:12:32  lr: 0.000019  min_lr: 0.000000  loss: 3.7933 (3.8081)  class_acc: 0.2917 (0.3175)  loss_scale: 65536.0000 (57655.0886)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1905  max mem: 15572
Epoch: [24]  [1510/2809]  eta: 0:12:26  lr: 0.000019  min_lr: 0.000000  loss: 4.0495 (3.8089)  class_acc: 0.2500 (0.3175)  loss_scale: 65536.0000 (57707.2455)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1381  max mem: 15572
Epoch: [24]  [1520/2809]  eta: 0:12:21  lr: 0.000019  min_lr: 0.000000  loss: 3.7057 (3.8080)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (57758.7166)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1724  max mem: 15572
Epoch: [24]  [1530/2809]  eta: 0:12:15  lr: 0.000019  min_lr: 0.000000  loss: 3.6665 (3.8082)  class_acc: 0.3750 (0.3181)  loss_scale: 65536.0000 (57809.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1551  max mem: 15572
Epoch: [24]  [1540/2809]  eta: 0:12:08  lr: 0.000019  min_lr: 0.000000  loss: 3.8080 (3.8079)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (57859.6548)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0577  max mem: 15572
Epoch: [24]  [1550/2809]  eta: 0:12:03  lr: 0.000019  min_lr: 0.000000  loss: 3.9457 (3.8103)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (57909.1476)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1061  max mem: 15572
Epoch: [24]  [1560/2809]  eta: 0:11:56  lr: 0.000019  min_lr: 0.000000  loss: 4.0405 (3.8106)  class_acc: 0.2500 (0.3177)  loss_scale: 65536.0000 (57958.0064)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0857  max mem: 15572
Epoch: [24]  [1570/2809]  eta: 0:11:50  lr: 0.000019  min_lr: 0.000000  loss: 3.7070 (3.8093)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (58006.2432)  weight_decay: 0.0500 (0.0500)  time: 0.5169  data: 0.0761  max mem: 15572
Epoch: [24]  [1580/2809]  eta: 0:11:45  lr: 0.000019  min_lr: 0.000000  loss: 3.7070 (3.8098)  class_acc: 0.2500 (0.3175)  loss_scale: 65536.0000 (58053.8697)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1450  max mem: 15572
[2025-01-16 02:04:08,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=69000, skipped=460, lr=[1.8553140734347188e-07, 1.8553140734347188e-07, 2.650448676335313e-07, 2.650448676335313e-07, 3.7863552519075905e-07, 3.7863552519075905e-07, 5.409078931296558e-07, 5.409078931296558e-07, 7.72725561613794e-07, 7.72725561613794e-07, 1.1038936594482773e-06, 1.1038936594482773e-06, 1.5769909420689674e-06, 1.5769909420689674e-06, 2.252844202955668e-06, 2.252844202955668e-06, 3.21834886136524e-06, 3.21834886136524e-06, 4.597641230521772e-06, 4.597641230521772e-06, 6.568058900745388e-06, 6.568058900745388e-06, 9.382941286779127e-06, 9.382941286779127e-06, 1.3404201838255898e-05, 1.3404201838255898e-05, 1.9148859768936997e-05, 1.9148859768936997e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:04:08,535] [INFO] [timer.py:260:stop] epoch=0/micro_step=69000/global_step=69000, RunningAvgSamplesPerSec=28.54568416480583, CurrSamplesPerSec=26.232721916713704, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [1590/2809]  eta: 0:11:39  lr: 0.000019  min_lr: 0.000000  loss: 4.0123 (3.8125)  class_acc: 0.2500 (0.3170)  loss_scale: 65536.0000 (58100.8975)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.1494  max mem: 15572
Epoch: [24]  [1600/2809]  eta: 0:11:33  lr: 0.000019  min_lr: 0.000000  loss: 3.8658 (3.8118)  class_acc: 0.2500 (0.3170)  loss_scale: 65536.0000 (58147.3379)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1318  max mem: 15572
[2025-01-16 02:04:21,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:04:21,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:04:22,072] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69024
[2025-01-16 02:04:22,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:04:22,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1610/2809]  eta: 0:11:28  lr: 0.000019  min_lr: 0.000000  loss: 3.8411 (3.8127)  class_acc: 0.2500 (0.3167)  loss_scale: 65536.0000 (58233.8821)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1232  max mem: 15572
Epoch: [24]  [1620/2809]  eta: 0:11:22  lr: 0.000019  min_lr: 0.000000  loss: 3.8479 (3.8121)  class_acc: 0.2500 (0.3165)  loss_scale: 65536.0000 (58278.9291)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1650  max mem: 15572
Epoch: [24]  [1630/2809]  eta: 0:11:17  lr: 0.000019  min_lr: 0.000000  loss: 3.5868 (3.8103)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (58323.4237)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1824  max mem: 15572
Epoch: [24]  [1640/2809]  eta: 0:11:11  lr: 0.000019  min_lr: 0.000000  loss: 3.7014 (3.8113)  class_acc: 0.3333 (0.3167)  loss_scale: 65536.0000 (58367.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1079  max mem: 15572
Epoch: [24]  [1650/2809]  eta: 0:11:04  lr: 0.000019  min_lr: 0.000000  loss: 3.8941 (3.8121)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (58410.7959)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0643  max mem: 15572
Epoch: [24]  [1660/2809]  eta: 0:10:59  lr: 0.000019  min_lr: 0.000000  loss: 3.7860 (3.8109)  class_acc: 0.3333 (0.3169)  loss_scale: 65536.0000 (58453.6930)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1473  max mem: 15572
Epoch: [24]  [1670/2809]  eta: 0:10:53  lr: 0.000019  min_lr: 0.000000  loss: 3.5535 (3.8093)  class_acc: 0.3333 (0.3173)  loss_scale: 65536.0000 (58496.0766)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1556  max mem: 15572
Epoch: [24]  [1680/2809]  eta: 0:10:48  lr: 0.000019  min_lr: 0.000000  loss: 3.4826 (3.8076)  class_acc: 0.3750 (0.3178)  loss_scale: 65536.0000 (58537.9560)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1180  max mem: 15572
Epoch: [24]  [1690/2809]  eta: 0:10:42  lr: 0.000019  min_lr: 0.000000  loss: 3.7835 (3.8083)  class_acc: 0.3750 (0.3178)  loss_scale: 65536.0000 (58579.3400)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1283  max mem: 15572
Epoch: [24]  [1700/2809]  eta: 0:10:35  lr: 0.000019  min_lr: 0.000000  loss: 3.7936 (3.8085)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (58620.2375)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0844  max mem: 15572
Epoch: [24]  [1710/2809]  eta: 0:10:30  lr: 0.000019  min_lr: 0.000000  loss: 3.7166 (3.8072)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (58660.6569)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1129  max mem: 15572
Epoch: [24]  [1720/2809]  eta: 0:10:24  lr: 0.000019  min_lr: 0.000000  loss: 3.4133 (3.8058)  class_acc: 0.3750 (0.3183)  loss_scale: 65536.0000 (58700.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1562  max mem: 15572
Epoch: [24]  [1730/2809]  eta: 0:10:18  lr: 0.000019  min_lr: 0.000000  loss: 3.6772 (3.8051)  class_acc: 0.3750 (0.3187)  loss_scale: 65536.0000 (58740.0947)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.1105  max mem: 15572
[2025-01-16 02:05:34,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:05:34,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [1740/2809]  eta: 0:10:12  lr: 0.000019  min_lr: 0.000000  loss: 3.6838 (3.8055)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (58929.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0912  max mem: 15572
[2025-01-16 02:05:38,163] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69158
[2025-01-16 02:05:38,164] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:05:38,164] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1750/2809]  eta: 0:10:06  lr: 0.000019  min_lr: 0.000000  loss: 3.7879 (3.8053)  class_acc: 0.2917 (0.3185)  loss_scale: 65536.0000 (59004.8567)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1220  max mem: 15572
Epoch: [24]  [1760/2809]  eta: 0:10:00  lr: 0.000019  min_lr: 0.000000  loss: 3.8634 (3.8055)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (59041.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1041  max mem: 15572
Epoch: [24]  [1770/2809]  eta: 0:09:54  lr: 0.000019  min_lr: 0.000000  loss: 3.8191 (3.8049)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (59078.6132)  weight_decay: 0.0500 (0.0500)  time: 0.5236  data: 0.0807  max mem: 15572
Epoch: [24]  [1780/2809]  eta: 0:09:49  lr: 0.000019  min_lr: 0.000000  loss: 3.7552 (3.8048)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (59114.8703)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1219  max mem: 15572
Epoch: [24]  [1790/2809]  eta: 0:09:42  lr: 0.000019  min_lr: 0.000000  loss: 3.7274 (3.8044)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (59150.7225)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0985  max mem: 15572
Epoch: [24]  [1800/2809]  eta: 0:09:36  lr: 0.000019  min_lr: 0.000000  loss: 3.8320 (3.8045)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (59186.1766)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0489  max mem: 15572
Epoch: [24]  [1810/2809]  eta: 0:09:30  lr: 0.000019  min_lr: 0.000000  loss: 3.8835 (3.8047)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (59221.2391)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0654  max mem: 15572
Epoch: [24]  [1820/2809]  eta: 0:09:25  lr: 0.000019  min_lr: 0.000000  loss: 3.8796 (3.8041)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (59255.9165)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1389  max mem: 15572
Epoch: [24]  [1830/2809]  eta: 0:09:19  lr: 0.000019  min_lr: 0.000000  loss: 3.8234 (3.8042)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (59290.2152)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1359  max mem: 15572
Epoch: [24]  [1840/2809]  eta: 0:09:13  lr: 0.000019  min_lr: 0.000000  loss: 3.7787 (3.8035)  class_acc: 0.2500 (0.3183)  loss_scale: 65536.0000 (59324.1412)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1133  max mem: 15572
Epoch: [24]  [1850/2809]  eta: 0:09:08  lr: 0.000019  min_lr: 0.000000  loss: 3.6334 (3.8025)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (59357.7007)  weight_decay: 0.0500 (0.0500)  time: 0.6303  data: 0.1855  max mem: 15572
Epoch: [24]  [1860/2809]  eta: 0:09:02  lr: 0.000019  min_lr: 0.000000  loss: 3.7154 (3.8032)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (59390.8995)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1513  max mem: 15572
Epoch: [24]  [1870/2809]  eta: 0:08:57  lr: 0.000019  min_lr: 0.000000  loss: 3.8395 (3.8031)  class_acc: 0.2500 (0.3181)  loss_scale: 65536.0000 (59423.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1004  max mem: 15572
[2025-01-16 02:06:50,443] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:06:50,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:06:51,729] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69290
[2025-01-16 02:06:51,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:06:51,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [1880/2809]  eta: 0:08:51  lr: 0.000019  min_lr: 0.000000  loss: 3.8018 (3.8034)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (59560.7613)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1497  max mem: 15572
Epoch: [24]  [1890/2809]  eta: 0:08:45  lr: 0.000019  min_lr: 0.000000  loss: 3.7807 (3.8023)  class_acc: 0.3333 (0.3183)  loss_scale: 65536.0000 (59592.3596)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1438  max mem: 15572
Epoch: [24]  [1900/2809]  eta: 0:08:40  lr: 0.000019  min_lr: 0.000000  loss: 3.7378 (3.8024)  class_acc: 0.3333 (0.3181)  loss_scale: 65536.0000 (59623.6255)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1123  max mem: 15572
Epoch: [24]  [1910/2809]  eta: 0:08:34  lr: 0.000019  min_lr: 0.000000  loss: 3.8910 (3.8029)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (59654.5641)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1185  max mem: 15572
Epoch: [24]  [1920/2809]  eta: 0:08:28  lr: 0.000019  min_lr: 0.000000  loss: 3.9312 (3.8029)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (59685.1806)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0592  max mem: 15572
Epoch: [24]  [1930/2809]  eta: 0:08:21  lr: 0.000019  min_lr: 0.000000  loss: 3.6985 (3.8018)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (59715.4801)  weight_decay: 0.0500 (0.0500)  time: 0.4728  data: 0.0007  max mem: 15572
Epoch: [24]  [1940/2809]  eta: 0:08:16  lr: 0.000019  min_lr: 0.000000  loss: 3.8024 (3.8030)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (59745.4673)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0932  max mem: 15572
Epoch: [24]  [1950/2809]  eta: 0:08:10  lr: 0.000019  min_lr: 0.000000  loss: 4.0832 (3.8031)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (59775.1471)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1810  max mem: 15572
Epoch: [24]  [1960/2809]  eta: 0:08:05  lr: 0.000019  min_lr: 0.000000  loss: 3.8417 (3.8036)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (59804.5242)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1500  max mem: 15572
Epoch: [24]  [1970/2809]  eta: 0:07:59  lr: 0.000019  min_lr: 0.000000  loss: 3.8417 (3.8037)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (59833.6032)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1161  max mem: 15572
Epoch: [24]  [1980/2809]  eta: 0:07:53  lr: 0.000019  min_lr: 0.000000  loss: 3.7944 (3.8033)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (59862.3887)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1259  max mem: 15572
Epoch: [24]  [1990/2809]  eta: 0:07:47  lr: 0.000019  min_lr: 0.000000  loss: 3.5928 (3.8027)  class_acc: 0.3750 (0.3183)  loss_scale: 65536.0000 (59890.8850)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1095  max mem: 15572
Epoch: [24]  [2000/2809]  eta: 0:07:41  lr: 0.000019  min_lr: 0.000000  loss: 3.5804 (3.8024)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (59919.0965)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0698  max mem: 15572
[2025-01-16 02:08:03,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:08:03,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:08:08,498] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69426
[2025-01-16 02:08:08,498] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:08:08,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2010/2809]  eta: 0:07:36  lr: 0.000019  min_lr: 0.000000  loss: 3.9093 (3.8030)  class_acc: 0.2500 (0.3181)  loss_scale: 65536.0000 (60175.1487)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1156  max mem: 15572
Epoch: [24]  [2020/2809]  eta: 0:07:30  lr: 0.000019  min_lr: 0.000000  loss: 3.9699 (3.8036)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (60201.6744)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1166  max mem: 15572
Epoch: [24]  [2030/2809]  eta: 0:07:25  lr: 0.000019  min_lr: 0.000000  loss: 3.9498 (3.8038)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (60227.9389)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1316  max mem: 15572
Epoch: [24]  [2040/2809]  eta: 0:07:19  lr: 0.000019  min_lr: 0.000000  loss: 3.7792 (3.8042)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (60253.9461)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1257  max mem: 15572
Epoch: [24]  [2050/2809]  eta: 0:07:13  lr: 0.000019  min_lr: 0.000000  loss: 3.8958 (3.8045)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (60279.6997)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1096  max mem: 15572
[2025-01-16 02:08:34,239] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69472
[2025-01-16 02:08:34,240] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:08:34,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2060/2809]  eta: 0:07:07  lr: 0.000019  min_lr: 0.000000  loss: 3.8469 (3.8045)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (60225.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.1342  max mem: 15572
Epoch: [24]  [2070/2809]  eta: 0:07:02  lr: 0.000019  min_lr: 0.000000  loss: 3.8709 (3.8053)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (60093.1260)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1172  max mem: 15572
Epoch: [24]  [2080/2809]  eta: 0:06:56  lr: 0.000019  min_lr: 0.000000  loss: 3.8709 (3.8048)  class_acc: 0.3333 (0.3177)  loss_scale: 32768.0000 (59961.8184)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1451  max mem: 15572
Epoch: [24]  [2090/2809]  eta: 0:06:50  lr: 0.000019  min_lr: 0.000000  loss: 3.6165 (3.8047)  class_acc: 0.3333 (0.3178)  loss_scale: 32768.0000 (59831.7666)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1201  max mem: 15572
Epoch: [24]  [2100/2809]  eta: 0:06:45  lr: 0.000019  min_lr: 0.000000  loss: 3.7768 (3.8046)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (59702.9529)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0813  max mem: 15572
Epoch: [24]  [2110/2809]  eta: 0:06:39  lr: 0.000019  min_lr: 0.000000  loss: 3.7768 (3.8043)  class_acc: 0.2500 (0.3179)  loss_scale: 32768.0000 (59575.3595)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0786  max mem: 15572
Epoch: [24]  [2120/2809]  eta: 0:06:33  lr: 0.000019  min_lr: 0.000000  loss: 3.8296 (3.8051)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (59448.9694)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.1090  max mem: 15572
Epoch: [24]  [2130/2809]  eta: 0:06:27  lr: 0.000019  min_lr: 0.000000  loss: 3.9036 (3.8053)  class_acc: 0.2917 (0.3176)  loss_scale: 32768.0000 (59323.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1895  max mem: 15572
Epoch: [24]  [2140/2809]  eta: 0:06:22  lr: 0.000019  min_lr: 0.000000  loss: 3.8270 (3.8053)  class_acc: 0.2500 (0.3175)  loss_scale: 32768.0000 (59199.7310)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.2108  max mem: 15572
Epoch: [24]  [2150/2809]  eta: 0:06:16  lr: 0.000019  min_lr: 0.000000  loss: 3.7102 (3.8047)  class_acc: 0.2917 (0.3178)  loss_scale: 32768.0000 (59076.8498)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1692  max mem: 15572
Epoch: [24]  [2160/2809]  eta: 0:06:11  lr: 0.000019  min_lr: 0.000000  loss: 3.6609 (3.8035)  class_acc: 0.3750 (0.3181)  loss_scale: 32768.0000 (58955.1060)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1542  max mem: 15572
Epoch: [24]  [2170/2809]  eta: 0:06:05  lr: 0.000019  min_lr: 0.000000  loss: 3.7132 (3.8034)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (58834.4836)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1201  max mem: 15572
Epoch: [24]  [2180/2809]  eta: 0:05:59  lr: 0.000019  min_lr: 0.000000  loss: 3.9056 (3.8029)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (58714.9674)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1090  max mem: 15572
[2025-01-16 02:09:48,678] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:09:48,679] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2190/2809]  eta: 0:05:53  lr: 0.000019  min_lr: 0.000000  loss: 3.8862 (3.8026)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (58686.2766)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0857  max mem: 15572
Epoch: [24]  [2200/2809]  eta: 0:05:48  lr: 0.000019  min_lr: 0.000000  loss: 3.8862 (3.8032)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (58717.3975)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1336  max mem: 15572
Epoch: [24]  [2210/2809]  eta: 0:05:42  lr: 0.000019  min_lr: 0.000000  loss: 3.7221 (3.8018)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (58748.2370)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1735  max mem: 15572
Epoch: [24]  [2220/2809]  eta: 0:05:36  lr: 0.000019  min_lr: 0.000000  loss: 3.5004 (3.8005)  class_acc: 0.3750 (0.3189)  loss_scale: 65536.0000 (58778.7987)  weight_decay: 0.0500 (0.0500)  time: 0.5273  data: 0.0881  max mem: 15572
Epoch: [24]  [2230/2809]  eta: 0:05:30  lr: 0.000019  min_lr: 0.000000  loss: 3.5829 (3.8001)  class_acc: 0.3750 (0.3190)  loss_scale: 65536.0000 (58809.0865)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0492  max mem: 15572
Epoch: [24]  [2240/2809]  eta: 0:05:24  lr: 0.000019  min_lr: 0.000000  loss: 3.7463 (3.7997)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (58839.1040)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0432  max mem: 15572
Epoch: [24]  [2250/2809]  eta: 0:05:19  lr: 0.000019  min_lr: 0.000000  loss: 3.8208 (3.8002)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (58868.8547)  weight_decay: 0.0500 (0.0500)  time: 0.5386  data: 0.0988  max mem: 15572
Epoch: [24]  [2260/2809]  eta: 0:05:13  lr: 0.000019  min_lr: 0.000000  loss: 3.7901 (3.7997)  class_acc: 0.2917 (0.3188)  loss_scale: 65536.0000 (58898.3423)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1050  max mem: 15572
Epoch: [24]  [2270/2809]  eta: 0:05:07  lr: 0.000019  min_lr: 0.000000  loss: 3.9269 (3.8007)  class_acc: 0.2500 (0.3185)  loss_scale: 65536.0000 (58927.5702)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0773  max mem: 15572
[2025-01-16 02:10:36,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69688
[2025-01-16 02:10:36,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:10:36,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2280/2809]  eta: 0:05:01  lr: 0.000019  min_lr: 0.000000  loss: 3.9316 (3.8009)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (58827.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1040  max mem: 15572
Epoch: [24]  [2290/2809]  eta: 0:04:55  lr: 0.000019  min_lr: 0.000000  loss: 3.8292 (3.8014)  class_acc: 0.3333 (0.3185)  loss_scale: 32768.0000 (58713.5050)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0726  max mem: 15572
Epoch: [24]  [2300/2809]  eta: 0:04:50  lr: 0.000019  min_lr: 0.000000  loss: 3.8330 (3.8016)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (58600.7475)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0867  max mem: 15572
Epoch: [24]  [2310/2809]  eta: 0:04:44  lr: 0.000019  min_lr: 0.000000  loss: 3.8330 (3.8017)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (58488.9658)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.1820  max mem: 15572
Epoch: [24]  [2320/2809]  eta: 0:04:38  lr: 0.000019  min_lr: 0.000000  loss: 3.9602 (3.8022)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (58378.1474)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1123  max mem: 15572
Epoch: [24]  [2330/2809]  eta: 0:04:32  lr: 0.000019  min_lr: 0.000000  loss: 3.8842 (3.8026)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (58268.2797)  weight_decay: 0.0500 (0.0500)  time: 0.4861  data: 0.0455  max mem: 15572
Epoch: [24]  [2340/2809]  eta: 0:04:27  lr: 0.000019  min_lr: 0.000000  loss: 3.7850 (3.8024)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (58159.3507)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1147  max mem: 15572
Epoch: [24]  [2350/2809]  eta: 0:04:21  lr: 0.000019  min_lr: 0.000000  loss: 3.8139 (3.8031)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (58051.3484)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1095  max mem: 15572
Epoch: [24]  [2360/2809]  eta: 0:04:15  lr: 0.000019  min_lr: 0.000000  loss: 3.8719 (3.8034)  class_acc: 0.2500 (0.3182)  loss_scale: 32768.0000 (57944.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1453  max mem: 15572
Epoch: [24]  [2370/2809]  eta: 0:04:10  lr: 0.000019  min_lr: 0.000000  loss: 3.6326 (3.8022)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (57838.0768)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1697  max mem: 15572
Epoch: [24]  [2380/2809]  eta: 0:04:04  lr: 0.000019  min_lr: 0.000000  loss: 3.6326 (3.8019)  class_acc: 0.3333 (0.3185)  loss_scale: 32768.0000 (57732.7845)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1244  max mem: 15572
Epoch: [24]  [2390/2809]  eta: 0:03:58  lr: 0.000019  min_lr: 0.000000  loss: 3.8462 (3.8024)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (57628.3731)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0847  max mem: 15572
Epoch: [24]  [2400/2809]  eta: 0:03:52  lr: 0.000019  min_lr: 0.000000  loss: 3.8462 (3.8016)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (57524.8313)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0879  max mem: 15572
[2025-01-16 02:11:47,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:11:47,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2410/2809]  eta: 0:03:47  lr: 0.000019  min_lr: 0.000000  loss: 3.7421 (3.8022)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (57558.0589)  weight_decay: 0.0500 (0.0500)  time: 0.5235  data: 0.0638  max mem: 15572
Epoch: [24]  [2420/2809]  eta: 0:03:41  lr: 0.000019  min_lr: 0.000000  loss: 3.9015 (3.8028)  class_acc: 0.2500 (0.3180)  loss_scale: 65536.0000 (57591.0120)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.0902  max mem: 15572
Epoch: [24]  [2430/2809]  eta: 0:03:35  lr: 0.000019  min_lr: 0.000000  loss: 3.9110 (3.8035)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (57623.6940)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1081  max mem: 15572
Epoch: [24]  [2440/2809]  eta: 0:03:29  lr: 0.000019  min_lr: 0.000000  loss: 3.9760 (3.8030)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (57656.1082)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0887  max mem: 15572
Epoch: [24]  [2450/2809]  eta: 0:03:24  lr: 0.000019  min_lr: 0.000000  loss: 3.9793 (3.8039)  class_acc: 0.2500 (0.3179)  loss_scale: 65536.0000 (57688.2579)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1501  max mem: 15572
Epoch: [24]  [2460/2809]  eta: 0:03:18  lr: 0.000019  min_lr: 0.000000  loss: 3.8130 (3.8024)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (57720.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1476  max mem: 15572
[2025-01-16 02:12:26,787] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 69885
[2025-01-16 02:12:26,787] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:12:26,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [2470/2809]  eta: 0:03:13  lr: 0.000018  min_lr: 0.000000  loss: 3.6386 (3.8021)  class_acc: 0.3750 (0.3181)  loss_scale: 65536.0000 (57725.2546)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1811  max mem: 15572
Epoch: [24]  [2480/2809]  eta: 0:03:07  lr: 0.000018  min_lr: 0.000000  loss: 3.7538 (3.8023)  class_acc: 0.3333 (0.3181)  loss_scale: 32768.0000 (57624.6610)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1272  max mem: 15572
Epoch: [24]  [2490/2809]  eta: 0:03:01  lr: 0.000018  min_lr: 0.000000  loss: 3.7215 (3.8019)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (57524.8752)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0740  max mem: 15572
Epoch: [24]  [2500/2809]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000000  loss: 3.7313 (3.8015)  class_acc: 0.3333 (0.3184)  loss_scale: 32768.0000 (57425.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1198  max mem: 15572
Epoch: [24]  [2510/2809]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 3.9023 (3.8020)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (57327.6878)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1827  max mem: 15572
Epoch: [24]  [2520/2809]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000000  loss: 3.9023 (3.8029)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (57230.2674)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.2117  max mem: 15572
Epoch: [24]  [2530/2809]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 3.8857 (3.8031)  class_acc: 0.2917 (0.3181)  loss_scale: 32768.0000 (57133.6168)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1490  max mem: 15572
Epoch: [24]  [2540/2809]  eta: 0:02:33  lr: 0.000018  min_lr: 0.000000  loss: 3.7922 (3.8032)  class_acc: 0.2917 (0.3182)  loss_scale: 32768.0000 (57037.7269)  weight_decay: 0.0500 (0.0500)  time: 0.5287  data: 0.0784  max mem: 15572
Epoch: [24]  [2550/2809]  eta: 0:02:27  lr: 0.000018  min_lr: 0.000000  loss: 3.6687 (3.8027)  class_acc: 0.2500 (0.3180)  loss_scale: 32768.0000 (56942.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0765  max mem: 15572
Epoch: [24]  [2560/2809]  eta: 0:02:21  lr: 0.000018  min_lr: 0.000000  loss: 3.6921 (3.8020)  class_acc: 0.3750 (0.3184)  loss_scale: 32768.0000 (56848.1937)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1059  max mem: 15572
Epoch: [24]  [2570/2809]  eta: 0:02:16  lr: 0.000018  min_lr: 0.000000  loss: 3.6921 (3.8006)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (56754.5329)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.0974  max mem: 15572
Epoch: [24]  [2580/2809]  eta: 0:02:10  lr: 0.000018  min_lr: 0.000000  loss: 3.5900 (3.8000)  class_acc: 0.3750 (0.3188)  loss_scale: 32768.0000 (56661.5978)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1758  max mem: 15572
[2025-01-16 02:13:31,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=70000, skipped=467, lr=[1.7842888740181126e-07, 1.7842888740181126e-07, 2.548984105740161e-07, 2.548984105740161e-07, 3.6414058653430874e-07, 3.6414058653430874e-07, 5.202008379061554e-07, 5.202008379061554e-07, 7.431440541516506e-07, 7.431440541516506e-07, 1.0616343630737865e-06, 1.0616343630737865e-06, 1.516620518676838e-06, 1.516620518676838e-06, 2.1666007409669117e-06, 2.1666007409669117e-06, 3.095143915667017e-06, 3.095143915667017e-06, 4.421634165238596e-06, 4.421634165238596e-06, 6.316620236055137e-06, 6.316620236055137e-06, 9.023743194364482e-06, 9.023743194364482e-06, 1.2891061706234976e-05, 1.2891061706234976e-05, 1.8415802437478537e-05, 1.8415802437478537e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:13:31,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=70000/global_step=70000, RunningAvgSamplesPerSec=28.546679957709134, CurrSamplesPerSec=32.517660588244794, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [24]  [2590/2809]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 3.5900 (3.7996)  class_acc: 0.3333 (0.3188)  loss_scale: 32768.0000 (56569.3802)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1534  max mem: 15572
[2025-01-16 02:13:39,080] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:13:39,080] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [2600/2809]  eta: 0:01:59  lr: 0.000018  min_lr: 0.000000  loss: 3.7669 (3.7995)  class_acc: 0.2917 (0.3187)  loss_scale: 32768.0000 (56515.6663)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.1647  max mem: 15572
Epoch: [24]  [2610/2809]  eta: 0:01:53  lr: 0.000018  min_lr: 0.000000  loss: 3.7555 (3.7993)  class_acc: 0.3333 (0.3188)  loss_scale: 65536.0000 (56550.2137)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1588  max mem: 15572
Epoch: [24]  [2620/2809]  eta: 0:01:47  lr: 0.000018  min_lr: 0.000000  loss: 3.7057 (3.7986)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (56584.4975)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0936  max mem: 15572
Epoch: [24]  [2630/2809]  eta: 0:01:42  lr: 0.000018  min_lr: 0.000000  loss: 3.7484 (3.7992)  class_acc: 0.3333 (0.3186)  loss_scale: 65536.0000 (56618.5207)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1447  max mem: 15572
Epoch: [24]  [2640/2809]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 3.7820 (3.7992)  class_acc: 0.3333 (0.3187)  loss_scale: 65536.0000 (56652.2863)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.0837  max mem: 15572
Epoch: [24]  [2650/2809]  eta: 0:01:30  lr: 0.000018  min_lr: 0.000000  loss: 3.8235 (3.7987)  class_acc: 0.3333 (0.3188)  loss_scale: 65536.0000 (56685.7971)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0682  max mem: 15572
Epoch: [24]  [2660/2809]  eta: 0:01:24  lr: 0.000018  min_lr: 0.000000  loss: 3.8235 (3.7995)  class_acc: 0.3333 (0.3187)  loss_scale: 65536.0000 (56719.0560)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1450  max mem: 15572
Epoch: [24]  [2670/2809]  eta: 0:01:19  lr: 0.000018  min_lr: 0.000000  loss: 3.8344 (3.7990)  class_acc: 0.2917 (0.3189)  loss_scale: 65536.0000 (56752.0659)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1456  max mem: 15572
Epoch: [24]  [2680/2809]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 3.8344 (3.7987)  class_acc: 0.2917 (0.3189)  loss_scale: 65536.0000 (56784.8295)  weight_decay: 0.0500 (0.0500)  time: 0.6498  data: 0.1902  max mem: 15572
Epoch: [24]  [2690/2809]  eta: 0:01:07  lr: 0.000018  min_lr: 0.000000  loss: 3.6669 (3.7980)  class_acc: 0.3750 (0.3192)  loss_scale: 65536.0000 (56817.3497)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.1985  max mem: 15572
Epoch: [24]  [2700/2809]  eta: 0:01:02  lr: 0.000018  min_lr: 0.000000  loss: 3.7197 (3.7987)  class_acc: 0.3750 (0.3190)  loss_scale: 65536.0000 (56849.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1087  max mem: 15572
Epoch: [24]  [2710/2809]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 3.7819 (3.7981)  class_acc: 0.3333 (0.3193)  loss_scale: 65536.0000 (56881.6702)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1036  max mem: 15572
Epoch: [24]  [2720/2809]  eta: 0:00:50  lr: 0.000018  min_lr: 0.000000  loss: 3.7365 (3.7977)  class_acc: 0.3333 (0.3194)  loss_scale: 65536.0000 (56913.4759)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1172  max mem: 15572
[2025-01-16 02:14:54,708] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:14:54,708] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:14:55,139] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70143
[2025-01-16 02:14:55,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:14:55,141] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [2730/2809]  eta: 0:00:45  lr: 0.000018  min_lr: 0.000000  loss: 3.8100 (3.7980)  class_acc: 0.2917 (0.3193)  loss_scale: 65536.0000 (56969.0458)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1302  max mem: 15572
Epoch: [24]  [2740/2809]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 3.8935 (3.7984)  class_acc: 0.2083 (0.3191)  loss_scale: 65536.0000 (57000.3006)  weight_decay: 0.0500 (0.0500)  time: 0.5402  data: 0.0849  max mem: 15572
Epoch: [24]  [2750/2809]  eta: 0:00:33  lr: 0.000018  min_lr: 0.000000  loss: 3.7499 (3.7988)  class_acc: 0.2917 (0.3191)  loss_scale: 65536.0000 (57031.3282)  weight_decay: 0.0500 (0.0500)  time: 0.4895  data: 0.0345  max mem: 15572
Epoch: [24]  [2760/2809]  eta: 0:00:27  lr: 0.000018  min_lr: 0.000000  loss: 3.7451 (3.7980)  class_acc: 0.3333 (0.3192)  loss_scale: 65536.0000 (57062.1311)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0862  max mem: 15572
Epoch: [24]  [2770/2809]  eta: 0:00:22  lr: 0.000018  min_lr: 0.000000  loss: 3.6970 (3.7981)  class_acc: 0.3333 (0.3191)  loss_scale: 65536.0000 (57092.7117)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0754  max mem: 15572
Epoch: [24]  [2780/2809]  eta: 0:00:16  lr: 0.000018  min_lr: 0.000000  loss: 3.8789 (3.7989)  class_acc: 0.2500 (0.3189)  loss_scale: 65536.0000 (57123.0723)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0862  max mem: 15572
Epoch: [24]  [2790/2809]  eta: 0:00:10  lr: 0.000018  min_lr: 0.000000  loss: 4.0086 (3.7993)  class_acc: 0.2500 (0.3188)  loss_scale: 65536.0000 (57153.2153)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.1206  max mem: 15572
Epoch: [24]  [2800/2809]  eta: 0:00:05  lr: 0.000018  min_lr: 0.000000  loss: 3.7944 (3.7981)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (57183.1432)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0476  max mem: 15572
Epoch: [24]  [2808/2809]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 3.7708 (3.7980)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (57206.9320)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0473  max mem: 15572
Epoch: [24] Total time: 0:26:39 (0.5694 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 3.7708 (3.7980)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (57206.9320)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:44  loss: 0.3536 (0.3536)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.9146  data: 3.7017  max mem: 15572
Val:  [ 10/272]  eta: 0:03:08  loss: 2.5608 (2.4091)  acc1: 38.8889 (39.8990)  acc5: 72.2222 (71.2121)  time: 0.7188  data: 0.5241  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.5193 (2.4394)  acc1: 44.4444 (43.3862)  acc5: 66.6667 (70.1058)  time: 0.3299  data: 0.1402  max mem: 15572
Val:  [ 30/272]  eta: 0:01:38  loss: 2.5193 (2.5182)  acc1: 44.4444 (39.2473)  acc5: 66.6667 (70.2509)  time: 0.2380  data: 0.0428  max mem: 15572
Val:  [ 40/272]  eta: 0:01:28  loss: 2.5015 (2.5244)  acc1: 33.3333 (37.5339)  acc5: 77.7778 (71.1382)  time: 0.2530  data: 0.0597  max mem: 15572
Val:  [ 50/272]  eta: 0:01:23  loss: 2.3149 (2.4033)  acc1: 33.3333 (40.6318)  acc5: 77.7778 (73.9651)  time: 0.3290  data: 0.1290  max mem: 15572
Val:  [ 60/272]  eta: 0:01:16  loss: 1.5456 (2.2971)  acc1: 66.6667 (44.2623)  acc5: 83.3333 (74.9545)  time: 0.3205  data: 0.1163  max mem: 15572
Val:  [ 70/272]  eta: 0:01:11  loss: 1.7049 (2.2264)  acc1: 61.1111 (46.6354)  acc5: 83.3333 (76.0563)  time: 0.2868  data: 0.0977  max mem: 15572
Val:  [ 80/272]  eta: 0:01:07  loss: 1.9678 (2.2514)  acc1: 50.0000 (46.2277)  acc5: 77.7778 (75.5144)  time: 0.3192  data: 0.1243  max mem: 15572
Val:  [ 90/272]  eta: 0:01:02  loss: 2.2154 (2.2501)  acc1: 44.4444 (46.2149)  acc5: 77.7778 (76.3126)  time: 0.3248  data: 0.1275  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 2.2154 (2.2885)  acc1: 38.8889 (45.4345)  acc5: 83.3333 (75.7976)  time: 0.3111  data: 0.1195  max mem: 15572
Val:  [110/272]  eta: 0:00:54  loss: 2.5787 (2.3705)  acc1: 27.7778 (43.3433)  acc5: 61.1111 (74.3744)  time: 0.3061  data: 0.1031  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 3.0529 (2.4122)  acc1: 27.7778 (42.3324)  acc5: 61.1111 (73.4619)  time: 0.3044  data: 0.1045  max mem: 15572
Val:  [130/272]  eta: 0:00:47  loss: 2.2767 (2.3821)  acc1: 44.4444 (43.3418)  acc5: 83.3333 (74.1306)  time: 0.3338  data: 0.1388  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.9298 (2.3682)  acc1: 55.5556 (43.9322)  acc5: 83.3333 (74.1135)  time: 0.3458  data: 0.1427  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.4351 (2.3707)  acc1: 44.4444 (43.5982)  acc5: 77.7778 (74.2826)  time: 0.2888  data: 0.0973  max mem: 15572
Val:  [160/272]  eta: 0:00:36  loss: 2.3034 (2.3579)  acc1: 44.4444 (44.2029)  acc5: 77.7778 (74.5687)  time: 0.2276  data: 0.0528  max mem: 15572
Val:  [170/272]  eta: 0:00:32  loss: 2.4660 (2.3809)  acc1: 38.8889 (43.3723)  acc5: 72.2222 (74.1066)  time: 0.2067  data: 0.0376  max mem: 15572
Val:  [180/272]  eta: 0:00:28  loss: 2.3631 (2.3690)  acc1: 33.3333 (43.1246)  acc5: 72.2222 (74.6163)  time: 0.1950  data: 0.0278  max mem: 15572
Val:  [190/272]  eta: 0:00:24  loss: 2.3840 (2.4197)  acc1: 33.3333 (41.9721)  acc5: 77.7778 (73.2693)  time: 0.1900  data: 0.0213  max mem: 15572
Val:  [200/272]  eta: 0:00:21  loss: 2.5813 (2.4295)  acc1: 33.3333 (41.7634)  acc5: 66.6667 (73.0514)  time: 0.2022  data: 0.0110  max mem: 15572
Val:  [210/272]  eta: 0:00:18  loss: 2.2805 (2.4341)  acc1: 44.4444 (42.0484)  acc5: 77.7778 (72.9595)  time: 0.2432  data: 0.0428  max mem: 15572
Val:  [220/272]  eta: 0:00:15  loss: 2.3303 (2.4231)  acc1: 44.4444 (42.2323)  acc5: 77.7778 (73.1775)  time: 0.3468  data: 0.1456  max mem: 15572
Val:  [230/272]  eta: 0:00:12  loss: 1.9116 (2.3904)  acc1: 61.1111 (43.3862)  acc5: 83.3333 (73.5931)  time: 0.4127  data: 0.2079  max mem: 15572
Val:  [240/272]  eta: 0:00:09  loss: 1.6576 (2.3719)  acc1: 66.6667 (43.7759)  acc5: 83.3333 (73.9972)  time: 0.3931  data: 0.2008  max mem: 15572
Val:  [250/272]  eta: 0:00:06  loss: 2.1769 (2.3805)  acc1: 38.8889 (43.2271)  acc5: 77.7778 (74.1036)  time: 0.3347  data: 0.1462  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.2487 (2.3195)  acc1: 72.2222 (44.9553)  acc5: 88.8889 (74.8191)  time: 0.3016  data: 0.1066  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4026 (2.3156)  acc1: 72.2222 (45.0595)  acc5: 88.8889 (75.0513)  time: 0.2743  data: 0.0946  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4026 (2.3194)  acc1: 72.2222 (45.0338)  acc5: 88.8889 (75.0358)  time: 0.2620  data: 0.0945  max mem: 15572
Val: Total time: 0:01:23 (0.3079 s / it)
* Acc@1 45.034 Acc@5 75.036 loss 2.319
Accuracy of the network on the 4883 val videos: 45.0%
Max accuracy: 45.42%
Epoch: [25]  [   0/2809]  eta: 9:12:09  lr: 0.000018  min_lr: 0.000000  loss: 3.2993 (3.2993)  class_acc: 0.4167 (0.4167)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 11.7941  data: 11.3308  max mem: 15572
Epoch: [25]  [  10/2809]  eta: 1:10:37  lr: 0.000018  min_lr: 0.000000  loss: 3.9402 (3.8208)  class_acc: 0.2500 (0.2992)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5141  data: 1.0309  max mem: 15572
Epoch: [25]  [  20/2809]  eta: 0:49:18  lr: 0.000018  min_lr: 0.000000  loss: 3.8165 (3.8205)  class_acc: 0.2500 (0.2976)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0289  max mem: 15572
Epoch: [25]  [  30/2809]  eta: 0:44:00  lr: 0.000018  min_lr: 0.000000  loss: 3.7884 (3.8033)  class_acc: 0.2917 (0.2930)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.1595  max mem: 15572
Epoch: [25]  [  40/2809]  eta: 0:40:49  lr: 0.000018  min_lr: 0.000000  loss: 3.8457 (3.8100)  class_acc: 0.2917 (0.3028)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.2555  max mem: 15572
[2025-01-16 02:17:41,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:17:41,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:17:45,808] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70275
[2025-01-16 02:17:45,808] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:17:45,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [  50/2809]  eta: 0:39:11  lr: 0.000018  min_lr: 0.000000  loss: 3.8149 (3.7809)  class_acc: 0.2917 (0.3105)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.7007  data: 0.2582  max mem: 15572
Epoch: [25]  [  60/2809]  eta: 0:37:39  lr: 0.000018  min_lr: 0.000000  loss: 3.6814 (3.7703)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (68759.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.2439  max mem: 15572
Epoch: [25]  [  70/2809]  eta: 0:35:45  lr: 0.000018  min_lr: 0.000000  loss: 4.0065 (3.7971)  class_acc: 0.2917 (0.3058)  loss_scale: 65536.0000 (68305.1268)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.1648  max mem: 15572
Epoch: [25]  [  80/2809]  eta: 0:33:49  lr: 0.000018  min_lr: 0.000000  loss: 3.9064 (3.8023)  class_acc: 0.2917 (0.3061)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0843  max mem: 15572
Epoch: [25]  [  90/2809]  eta: 0:31:57  lr: 0.000018  min_lr: 0.000000  loss: 3.8389 (3.7961)  class_acc: 0.2917 (0.3109)  loss_scale: 65536.0000 (67696.5275)  weight_decay: 0.0500 (0.0500)  time: 0.4288  data: 0.0298  max mem: 15572
Epoch: [25]  [ 100/2809]  eta: 0:30:39  lr: 0.000018  min_lr: 0.000000  loss: 3.6389 (3.7858)  class_acc: 0.3333 (0.3139)  loss_scale: 65536.0000 (67482.6139)  weight_decay: 0.0500 (0.0500)  time: 0.4166  data: 0.0007  max mem: 15572
Epoch: [25]  [ 110/2809]  eta: 0:29:33  lr: 0.000018  min_lr: 0.000000  loss: 3.7465 (3.7896)  class_acc: 0.3333 (0.3131)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.4373  data: 0.0008  max mem: 15572
Epoch: [25]  [ 120/2809]  eta: 0:28:42  lr: 0.000018  min_lr: 0.000000  loss: 3.8257 (3.7936)  class_acc: 0.3333 (0.3165)  loss_scale: 65536.0000 (67160.8595)  weight_decay: 0.0500 (0.0500)  time: 0.4476  data: 0.0005  max mem: 15572
Epoch: [25]  [ 130/2809]  eta: 0:27:58  lr: 0.000018  min_lr: 0.000000  loss: 3.8274 (3.8012)  class_acc: 0.3333 (0.3197)  loss_scale: 65536.0000 (67036.8244)  weight_decay: 0.0500 (0.0500)  time: 0.4575  data: 0.0130  max mem: 15572
Epoch: [25]  [ 140/2809]  eta: 0:27:57  lr: 0.000018  min_lr: 0.000000  loss: 3.8979 (3.8102)  class_acc: 0.3333 (0.3218)  loss_scale: 65536.0000 (66930.3830)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1319  max mem: 15572
Epoch: [25]  [ 150/2809]  eta: 0:27:52  lr: 0.000018  min_lr: 0.000000  loss: 3.7832 (3.8021)  class_acc: 0.3333 (0.3228)  loss_scale: 65536.0000 (66838.0397)  weight_decay: 0.0500 (0.0500)  time: 0.6457  data: 0.2257  max mem: 15572
Epoch: [25]  [ 160/2809]  eta: 0:27:49  lr: 0.000018  min_lr: 0.000000  loss: 3.8285 (3.8138)  class_acc: 0.2917 (0.3199)  loss_scale: 65536.0000 (66757.1677)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.2114  max mem: 15572
Epoch: [25]  [ 170/2809]  eta: 0:27:26  lr: 0.000018  min_lr: 0.000000  loss: 3.9668 (3.8205)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1419  max mem: 15572
[2025-01-16 02:18:53,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:18:53,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 180/2809]  eta: 0:27:15  lr: 0.000018  min_lr: 0.000000  loss: 3.9899 (3.8222)  class_acc: 0.2917 (0.3204)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1147  max mem: 15572
[2025-01-16 02:18:56,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70409
[2025-01-16 02:18:56,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:18:56,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 190/2809]  eta: 0:26:53  lr: 0.000018  min_lr: 0.000000  loss: 3.8019 (3.8202)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (68280.9634)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1227  max mem: 15572
Epoch: [25]  [ 200/2809]  eta: 0:26:44  lr: 0.000018  min_lr: 0.000000  loss: 3.6608 (3.8114)  class_acc: 0.3333 (0.3232)  loss_scale: 65536.0000 (68144.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1331  max mem: 15572
Epoch: [25]  [ 210/2809]  eta: 0:26:18  lr: 0.000018  min_lr: 0.000000  loss: 3.7855 (3.8197)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (68020.7773)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.1041  max mem: 15572
Epoch: [25]  [ 220/2809]  eta: 0:26:08  lr: 0.000018  min_lr: 0.000000  loss: 4.0244 (3.8254)  class_acc: 0.2917 (0.3222)  loss_scale: 65536.0000 (67908.3439)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0708  max mem: 15572
Epoch: [25]  [ 230/2809]  eta: 0:25:56  lr: 0.000018  min_lr: 0.000000  loss: 3.9443 (3.8251)  class_acc: 0.3333 (0.3227)  loss_scale: 65536.0000 (67805.6450)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1149  max mem: 15572
Epoch: [25]  [ 240/2809]  eta: 0:25:49  lr: 0.000018  min_lr: 0.000000  loss: 3.7490 (3.8208)  class_acc: 0.3333 (0.3242)  loss_scale: 65536.0000 (67711.4689)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1453  max mem: 15572
Epoch: [25]  [ 250/2809]  eta: 0:25:42  lr: 0.000018  min_lr: 0.000000  loss: 3.7923 (3.8150)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (67624.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1674  max mem: 15572
Epoch: [25]  [ 260/2809]  eta: 0:25:36  lr: 0.000018  min_lr: 0.000000  loss: 3.9123 (3.8217)  class_acc: 0.2083 (0.3210)  loss_scale: 65536.0000 (67544.7663)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1427  max mem: 15572
Epoch: [25]  [ 270/2809]  eta: 0:25:23  lr: 0.000018  min_lr: 0.000000  loss: 3.5809 (3.8088)  class_acc: 0.3333 (0.3255)  loss_scale: 65536.0000 (67470.6421)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1062  max mem: 15572
Epoch: [25]  [ 280/2809]  eta: 0:25:13  lr: 0.000018  min_lr: 0.000000  loss: 3.7382 (3.8127)  class_acc: 0.3750 (0.3253)  loss_scale: 65536.0000 (67401.7936)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0919  max mem: 15572
Epoch: [25]  [ 290/2809]  eta: 0:25:07  lr: 0.000018  min_lr: 0.000000  loss: 3.8016 (3.8119)  class_acc: 0.2917 (0.3245)  loss_scale: 65536.0000 (67337.6770)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1272  max mem: 15572
Epoch: [25]  [ 300/2809]  eta: 0:24:54  lr: 0.000018  min_lr: 0.000000  loss: 3.7503 (3.8112)  class_acc: 0.3333 (0.3238)  loss_scale: 65536.0000 (67277.8206)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1191  max mem: 15572
Epoch: [25]  [ 310/2809]  eta: 0:24:56  lr: 0.000018  min_lr: 0.000000  loss: 3.8686 (3.8175)  class_acc: 0.2917 (0.3222)  loss_scale: 65536.0000 (67221.8135)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1609  max mem: 15572
[2025-01-16 02:20:10,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:20:10,071] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:20:10,488] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70539
[2025-01-16 02:20:10,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:20:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 02:20:12,494] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70544
[2025-01-16 02:20:12,495] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:20:12,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 320/2809]  eta: 0:24:42  lr: 0.000018  min_lr: 0.000000  loss: 3.8666 (3.8089)  class_acc: 0.2917 (0.3236)  loss_scale: 65536.0000 (67169.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1593  max mem: 15572
Epoch: [25]  [ 330/2809]  eta: 0:24:36  lr: 0.000018  min_lr: 0.000000  loss: 3.6670 (3.8066)  class_acc: 0.2917 (0.3228)  loss_scale: 32768.0000 (66129.9819)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1202  max mem: 15572
Epoch: [25]  [ 340/2809]  eta: 0:24:27  lr: 0.000018  min_lr: 0.000000  loss: 3.6354 (3.8015)  class_acc: 0.2500 (0.3237)  loss_scale: 32768.0000 (65151.6246)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1445  max mem: 15572
Epoch: [25]  [ 350/2809]  eta: 0:24:14  lr: 0.000018  min_lr: 0.000000  loss: 3.9586 (3.8063)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (64229.0142)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0699  max mem: 15572
Epoch: [25]  [ 360/2809]  eta: 0:24:10  lr: 0.000018  min_lr: 0.000000  loss: 3.9586 (3.8047)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (63357.5180)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.0970  max mem: 15572
Epoch: [25]  [ 370/2809]  eta: 0:23:59  lr: 0.000018  min_lr: 0.000000  loss: 3.8807 (3.8051)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (62533.0027)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1301  max mem: 15572
Epoch: [25]  [ 380/2809]  eta: 0:23:58  lr: 0.000018  min_lr: 0.000000  loss: 4.0421 (3.8111)  class_acc: 0.2500 (0.3185)  loss_scale: 32768.0000 (61751.7690)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1582  max mem: 15572
Epoch: [25]  [ 390/2809]  eta: 0:23:45  lr: 0.000018  min_lr: 0.000000  loss: 3.9904 (3.8101)  class_acc: 0.2917 (0.3191)  loss_scale: 32768.0000 (61010.4962)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1358  max mem: 15572
Epoch: [25]  [ 400/2809]  eta: 0:23:35  lr: 0.000018  min_lr: 0.000000  loss: 3.8036 (3.8097)  class_acc: 0.3750 (0.3199)  loss_scale: 32768.0000 (60306.1945)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0726  max mem: 15572
Epoch: [25]  [ 410/2809]  eta: 0:23:22  lr: 0.000018  min_lr: 0.000000  loss: 3.8036 (3.8103)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (59636.1655)  weight_decay: 0.0500 (0.0500)  time: 0.4962  data: 0.0646  max mem: 15572
Epoch: [25]  [ 420/2809]  eta: 0:23:27  lr: 0.000018  min_lr: 0.000000  loss: 3.7231 (3.8075)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (58997.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1831  max mem: 15572
Epoch: [25]  [ 430/2809]  eta: 0:23:13  lr: 0.000018  min_lr: 0.000000  loss: 3.7376 (3.8082)  class_acc: 0.3333 (0.3196)  loss_scale: 32768.0000 (58389.3828)  weight_decay: 0.0500 (0.0500)  time: 0.6113  data: 0.1715  max mem: 15572
Epoch: [25]  [ 440/2809]  eta: 0:23:03  lr: 0.000018  min_lr: 0.000000  loss: 3.8572 (3.8107)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (57808.3991)  weight_decay: 0.0500 (0.0500)  time: 0.4833  data: 0.0417  max mem: 15572
[2025-01-16 02:21:24,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:21:24,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 450/2809]  eta: 0:22:55  lr: 0.000018  min_lr: 0.000000  loss: 3.9335 (3.8126)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (57471.1486)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.1044  max mem: 15572
Epoch: [25]  [ 460/2809]  eta: 0:22:49  lr: 0.000018  min_lr: 0.000000  loss: 3.9817 (3.8147)  class_acc: 0.2500 (0.3192)  loss_scale: 65536.0000 (57646.0911)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1123  max mem: 15572
Epoch: [25]  [ 470/2809]  eta: 0:22:41  lr: 0.000018  min_lr: 0.000000  loss: 3.9495 (3.8140)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (57813.6051)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1061  max mem: 15572
Epoch: [25]  [ 480/2809]  eta: 0:22:37  lr: 0.000018  min_lr: 0.000000  loss: 3.9387 (3.8174)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (57974.1538)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1469  max mem: 15572
Epoch: [25]  [ 490/2809]  eta: 0:22:29  lr: 0.000018  min_lr: 0.000000  loss: 3.9387 (3.8187)  class_acc: 0.2917 (0.3185)  loss_scale: 65536.0000 (58128.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1516  max mem: 15572
Epoch: [25]  [ 500/2809]  eta: 0:22:27  lr: 0.000018  min_lr: 0.000000  loss: 3.9862 (3.8211)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (58276.0240)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1708  max mem: 15572
Epoch: [25]  [ 510/2809]  eta: 0:22:21  lr: 0.000018  min_lr: 0.000000  loss: 3.9862 (3.8224)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (58418.0978)  weight_decay: 0.0500 (0.0500)  time: 0.6190  data: 0.1861  max mem: 15572
Epoch: [25]  [ 520/2809]  eta: 0:22:11  lr: 0.000018  min_lr: 0.000000  loss: 3.5686 (3.8151)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (58554.7179)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0885  max mem: 15572
Epoch: [25]  [ 530/2809]  eta: 0:22:05  lr: 0.000018  min_lr: 0.000000  loss: 3.4763 (3.8084)  class_acc: 0.3750 (0.3198)  loss_scale: 65536.0000 (58686.1921)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0800  max mem: 15572
Epoch: [25]  [ 540/2809]  eta: 0:22:06  lr: 0.000018  min_lr: 0.000000  loss: 3.5436 (3.8079)  class_acc: 0.3333 (0.3195)  loss_scale: 65536.0000 (58812.8059)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.2096  max mem: 15572
Epoch: [25]  [ 550/2809]  eta: 0:21:56  lr: 0.000018  min_lr: 0.000000  loss: 3.9114 (3.8078)  class_acc: 0.2500 (0.3196)  loss_scale: 65536.0000 (58934.8240)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1760  max mem: 15572
Epoch: [25]  [ 560/2809]  eta: 0:21:48  lr: 0.000018  min_lr: 0.000000  loss: 3.8764 (3.8100)  class_acc: 0.2917 (0.3194)  loss_scale: 65536.0000 (59052.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5083  data: 0.0794  max mem: 15572
Epoch: [25]  [ 570/2809]  eta: 0:21:39  lr: 0.000018  min_lr: 0.000000  loss: 3.6016 (3.8014)  class_acc: 0.3750 (0.3207)  loss_scale: 65536.0000 (59166.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0794  max mem: 15572
[2025-01-16 02:22:38,627] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:22:38,628] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:22:39,514] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70803
[2025-01-16 02:22:39,515] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:22:39,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 580/2809]  eta: 0:21:35  lr: 0.000018  min_lr: 0.000000  loss: 3.5102 (3.8012)  class_acc: 0.3750 (0.3205)  loss_scale: 65536.0000 (59501.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1404  max mem: 15572
Epoch: [25]  [ 590/2809]  eta: 0:21:32  lr: 0.000018  min_lr: 0.000000  loss: 3.9606 (3.8031)  class_acc: 0.2083 (0.3195)  loss_scale: 65536.0000 (59603.3841)  weight_decay: 0.0500 (0.0500)  time: 0.6414  data: 0.2123  max mem: 15572
Epoch: [25]  [ 600/2809]  eta: 0:21:22  lr: 0.000018  min_lr: 0.000000  loss: 4.0618 (3.8041)  class_acc: 0.2083 (0.3191)  loss_scale: 65536.0000 (59702.0965)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1240  max mem: 15572
Epoch: [25]  [ 610/2809]  eta: 0:21:16  lr: 0.000018  min_lr: 0.000000  loss: 3.9474 (3.8060)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (59797.5777)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0814  max mem: 15572
Epoch: [25]  [ 620/2809]  eta: 0:21:09  lr: 0.000018  min_lr: 0.000000  loss: 3.9474 (3.8075)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (59889.9839)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1168  max mem: 15572
Epoch: [25]  [ 630/2809]  eta: 0:21:06  lr: 0.000018  min_lr: 0.000000  loss: 3.8955 (3.8081)  class_acc: 0.2917 (0.3174)  loss_scale: 65536.0000 (59979.4612)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1519  max mem: 15572
Epoch: [25]  [ 640/2809]  eta: 0:20:58  lr: 0.000018  min_lr: 0.000000  loss: 3.9103 (3.8132)  class_acc: 0.2500 (0.3160)  loss_scale: 65536.0000 (60066.1466)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1323  max mem: 15572
Epoch: [25]  [ 650/2809]  eta: 0:20:49  lr: 0.000018  min_lr: 0.000000  loss: 3.8613 (3.8110)  class_acc: 0.2500 (0.3166)  loss_scale: 65536.0000 (60150.1690)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0616  max mem: 15572
Epoch: [25]  [ 660/2809]  eta: 0:20:45  lr: 0.000018  min_lr: 0.000000  loss: 3.7515 (3.8108)  class_acc: 0.3750 (0.3171)  loss_scale: 65536.0000 (60231.6490)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0877  max mem: 15572
Epoch: [25]  [ 670/2809]  eta: 0:20:37  lr: 0.000018  min_lr: 0.000000  loss: 3.7515 (3.8104)  class_acc: 0.3333 (0.3170)  loss_scale: 65536.0000 (60310.7004)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1018  max mem: 15572
Epoch: [25]  [ 680/2809]  eta: 0:20:30  lr: 0.000018  min_lr: 0.000000  loss: 3.6744 (3.8091)  class_acc: 0.2500 (0.3172)  loss_scale: 65536.0000 (60387.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0915  max mem: 15572
Epoch: [25]  [ 690/2809]  eta: 0:20:24  lr: 0.000018  min_lr: 0.000000  loss: 3.9163 (3.8118)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (60461.9392)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1109  max mem: 15572
Epoch: [25]  [ 700/2809]  eta: 0:20:19  lr: 0.000018  min_lr: 0.000000  loss: 3.7909 (3.8063)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (60534.3224)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1411  max mem: 15572
[2025-01-16 02:23:51,619] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:23:51,619] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:23:52,874] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 70935
[2025-01-16 02:23:52,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:23:52,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 710/2809]  eta: 0:20:11  lr: 0.000018  min_lr: 0.000000  loss: 3.5407 (3.8038)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (60881.1927)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1156  max mem: 15572
Epoch: [25]  [ 720/2809]  eta: 0:20:04  lr: 0.000018  min_lr: 0.000000  loss: 3.9066 (3.8075)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (60945.7531)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0759  max mem: 15572
Epoch: [25]  [ 730/2809]  eta: 0:19:58  lr: 0.000018  min_lr: 0.000000  loss: 4.0847 (3.8113)  class_acc: 0.2500 (0.3165)  loss_scale: 65536.0000 (61008.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.0974  max mem: 15572
Epoch: [25]  [ 740/2809]  eta: 0:19:51  lr: 0.000018  min_lr: 0.000000  loss: 4.1208 (3.8158)  class_acc: 0.2500 (0.3156)  loss_scale: 65536.0000 (61069.6464)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0955  max mem: 15572
Epoch: [25]  [ 750/2809]  eta: 0:19:45  lr: 0.000018  min_lr: 0.000000  loss: 3.9734 (3.8162)  class_acc: 0.2917 (0.3161)  loss_scale: 65536.0000 (61129.1185)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0937  max mem: 15572
Epoch: [25]  [ 760/2809]  eta: 0:19:38  lr: 0.000018  min_lr: 0.000000  loss: 3.7672 (3.8140)  class_acc: 0.3750 (0.3170)  loss_scale: 65536.0000 (61187.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0888  max mem: 15572
Epoch: [25]  [ 770/2809]  eta: 0:19:31  lr: 0.000018  min_lr: 0.000000  loss: 3.7330 (3.8152)  class_acc: 0.3333 (0.3171)  loss_scale: 65536.0000 (61243.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0915  max mem: 15572
[2025-01-16 02:24:29,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=71000, skipped=474, lr=[1.7137627550321582e-07, 1.7137627550321582e-07, 2.4482325071887975e-07, 2.4482325071887975e-07, 3.497475010269711e-07, 3.497475010269711e-07, 4.996392871813873e-07, 4.996392871813873e-07, 7.137704102591248e-07, 7.137704102591248e-07, 1.0196720146558926e-06, 1.0196720146558926e-06, 1.4566743066512753e-06, 1.4566743066512753e-06, 2.0809632952161078e-06, 2.0809632952161078e-06, 2.9728047074515825e-06, 2.9728047074515825e-06, 4.2468638677879755e-06, 4.2468638677879755e-06, 6.06694838255425e-06, 6.06694838255425e-06, 8.667069117934644e-06, 8.667069117934644e-06, 1.2381527311335207e-05, 1.2381527311335207e-05, 1.7687896159050297e-05, 1.7687896159050297e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:24:29,067] [INFO] [timer.py:260:stop] epoch=0/micro_step=71000/global_step=71000, RunningAvgSamplesPerSec=28.54834656319951, CurrSamplesPerSec=31.74302675900623, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [ 780/2809]  eta: 0:19:27  lr: 0.000018  min_lr: 0.000000  loss: 3.7330 (3.8149)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (61298.3969)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1438  max mem: 15572
Epoch: [25]  [ 790/2809]  eta: 0:19:21  lr: 0.000018  min_lr: 0.000000  loss: 3.7151 (3.8148)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (61351.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1524  max mem: 15572
Epoch: [25]  [ 800/2809]  eta: 0:19:16  lr: 0.000018  min_lr: 0.000000  loss: 3.8087 (3.8165)  class_acc: 0.2917 (0.3165)  loss_scale: 65536.0000 (61404.2047)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1444  max mem: 15572
Epoch: [25]  [ 810/2809]  eta: 0:19:09  lr: 0.000018  min_lr: 0.000000  loss: 3.9096 (3.8171)  class_acc: 0.2917 (0.3163)  loss_scale: 65536.0000 (61455.1517)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1229  max mem: 15572
Epoch: [25]  [ 820/2809]  eta: 0:19:03  lr: 0.000018  min_lr: 0.000000  loss: 3.9483 (3.8181)  class_acc: 0.2917 (0.3163)  loss_scale: 65536.0000 (61504.8575)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1109  max mem: 15572
Epoch: [25]  [ 830/2809]  eta: 0:18:58  lr: 0.000018  min_lr: 0.000000  loss: 4.0963 (3.8227)  class_acc: 0.2500 (0.3149)  loss_scale: 65536.0000 (61553.3670)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1498  max mem: 15572
[2025-01-16 02:25:05,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:25:05,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 840/2809]  eta: 0:18:51  lr: 0.000018  min_lr: 0.000000  loss: 3.8180 (3.8204)  class_acc: 0.2917 (0.3158)  loss_scale: 65536.0000 (61756.5755)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1092  max mem: 15572
[2025-01-16 02:25:06,719] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71067
[2025-01-16 02:25:06,719] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:25:06,720] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 850/2809]  eta: 0:18:42  lr: 0.000018  min_lr: 0.000000  loss: 3.7729 (3.8193)  class_acc: 0.3333 (0.3157)  loss_scale: 65536.0000 (61877.9976)  weight_decay: 0.0500 (0.0500)  time: 0.4704  data: 0.0244  max mem: 15572
Epoch: [25]  [ 860/2809]  eta: 0:18:34  lr: 0.000018  min_lr: 0.000000  loss: 3.6312 (3.8168)  class_acc: 0.3333 (0.3162)  loss_scale: 65536.0000 (61920.4832)  weight_decay: 0.0500 (0.0500)  time: 0.4587  data: 0.0005  max mem: 15572
[2025-01-16 02:25:15,788] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71087
[2025-01-16 02:25:15,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:25:15,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [ 870/2809]  eta: 0:18:30  lr: 0.000018  min_lr: 0.000000  loss: 3.6312 (3.8176)  class_acc: 0.2917 (0.3158)  loss_scale: 65536.0000 (61623.4030)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0960  max mem: 15572
Epoch: [25]  [ 880/2809]  eta: 0:18:25  lr: 0.000018  min_lr: 0.000000  loss: 3.8042 (3.8173)  class_acc: 0.2500 (0.3155)  loss_scale: 32768.0000 (61295.8729)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.1869  max mem: 15572
Epoch: [25]  [ 890/2809]  eta: 0:18:19  lr: 0.000018  min_lr: 0.000000  loss: 3.7920 (3.8167)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (60975.6947)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.1373  max mem: 15572
Epoch: [25]  [ 900/2809]  eta: 0:18:14  lr: 0.000018  min_lr: 0.000000  loss: 3.8043 (3.8188)  class_acc: 0.2917 (0.3153)  loss_scale: 32768.0000 (60662.6238)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1333  max mem: 15572
Epoch: [25]  [ 910/2809]  eta: 0:18:08  lr: 0.000018  min_lr: 0.000000  loss: 3.9439 (3.8187)  class_acc: 0.2917 (0.3155)  loss_scale: 32768.0000 (60356.4259)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1443  max mem: 15572
Epoch: [25]  [ 920/2809]  eta: 0:18:03  lr: 0.000018  min_lr: 0.000000  loss: 3.9993 (3.8191)  class_acc: 0.2917 (0.3148)  loss_scale: 32768.0000 (60056.8773)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1412  max mem: 15572
Epoch: [25]  [ 930/2809]  eta: 0:17:58  lr: 0.000018  min_lr: 0.000000  loss: 3.8584 (3.8189)  class_acc: 0.3333 (0.3153)  loss_scale: 32768.0000 (59763.7637)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1594  max mem: 15572
Epoch: [25]  [ 940/2809]  eta: 0:17:51  lr: 0.000018  min_lr: 0.000000  loss: 3.7828 (3.8183)  class_acc: 0.3333 (0.3156)  loss_scale: 32768.0000 (59476.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1115  max mem: 15572
Epoch: [25]  [ 950/2809]  eta: 0:17:45  lr: 0.000018  min_lr: 0.000000  loss: 3.7713 (3.8184)  class_acc: 0.2917 (0.3154)  loss_scale: 32768.0000 (59196.0294)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1098  max mem: 15572
Epoch: [25]  [ 960/2809]  eta: 0:17:38  lr: 0.000018  min_lr: 0.000000  loss: 3.8168 (3.8192)  class_acc: 0.2500 (0.3151)  loss_scale: 32768.0000 (58921.0239)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0973  max mem: 15572
Epoch: [25]  [ 970/2809]  eta: 0:17:32  lr: 0.000018  min_lr: 0.000000  loss: 3.9044 (3.8196)  class_acc: 0.2917 (0.3151)  loss_scale: 32768.0000 (58651.6828)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0706  max mem: 15572
Epoch: [25]  [ 980/2809]  eta: 0:17:26  lr: 0.000018  min_lr: 0.000000  loss: 3.8821 (3.8180)  class_acc: 0.3333 (0.3154)  loss_scale: 32768.0000 (58387.8328)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.0928  max mem: 15572
Epoch: [25]  [ 990/2809]  eta: 0:17:19  lr: 0.000018  min_lr: 0.000000  loss: 3.6725 (3.8168)  class_acc: 0.2917 (0.3153)  loss_scale: 32768.0000 (58129.3078)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0937  max mem: 15572
[2025-01-16 02:26:29,567] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:26:29,567] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1000/2809]  eta: 0:17:13  lr: 0.000018  min_lr: 0.000000  loss: 3.8239 (3.8164)  class_acc: 0.2917 (0.3156)  loss_scale: 32768.0000 (58203.3007)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0916  max mem: 15572
Epoch: [25]  [1010/2809]  eta: 0:17:07  lr: 0.000018  min_lr: 0.000000  loss: 3.9781 (3.8174)  class_acc: 0.3333 (0.3156)  loss_scale: 65536.0000 (58275.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1078  max mem: 15572
Epoch: [25]  [1020/2809]  eta: 0:17:01  lr: 0.000018  min_lr: 0.000000  loss: 3.9074 (3.8179)  class_acc: 0.2917 (0.3154)  loss_scale: 65536.0000 (58346.9383)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1160  max mem: 15572
Epoch: [25]  [1030/2809]  eta: 0:16:57  lr: 0.000018  min_lr: 0.000000  loss: 3.8602 (3.8159)  class_acc: 0.2917 (0.3159)  loss_scale: 65536.0000 (58416.6673)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1398  max mem: 15572
Epoch: [25]  [1040/2809]  eta: 0:16:50  lr: 0.000017  min_lr: 0.000000  loss: 3.8094 (3.8160)  class_acc: 0.3333 (0.3158)  loss_scale: 65536.0000 (58485.0567)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1197  max mem: 15572
Epoch: [25]  [1050/2809]  eta: 0:16:44  lr: 0.000017  min_lr: 0.000000  loss: 3.9449 (3.8180)  class_acc: 0.2917 (0.3155)  loss_scale: 65536.0000 (58552.1446)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0755  max mem: 15572
Epoch: [25]  [1060/2809]  eta: 0:16:37  lr: 0.000017  min_lr: 0.000000  loss: 3.9542 (3.8165)  class_acc: 0.2500 (0.3152)  loss_scale: 65536.0000 (58617.9680)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0754  max mem: 15572
Epoch: [25]  [1070/2809]  eta: 0:16:31  lr: 0.000017  min_lr: 0.000000  loss: 3.7562 (3.8157)  class_acc: 0.2917 (0.3160)  loss_scale: 65536.0000 (58682.5621)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0871  max mem: 15572
Epoch: [25]  [1080/2809]  eta: 0:16:24  lr: 0.000017  min_lr: 0.000000  loss: 3.7850 (3.8160)  class_acc: 0.3333 (0.3158)  loss_scale: 65536.0000 (58745.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0886  max mem: 15572
Epoch: [25]  [1090/2809]  eta: 0:16:19  lr: 0.000017  min_lr: 0.000000  loss: 3.9190 (3.8179)  class_acc: 0.2500 (0.3150)  loss_scale: 65536.0000 (58808.1980)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0868  max mem: 15572
Epoch: [25]  [1100/2809]  eta: 0:16:13  lr: 0.000017  min_lr: 0.000000  loss: 3.6762 (3.8146)  class_acc: 0.2917 (0.3159)  loss_scale: 65536.0000 (58869.3043)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0956  max mem: 15572
Epoch: [25]  [1110/2809]  eta: 0:16:07  lr: 0.000017  min_lr: 0.000000  loss: 3.4480 (3.8124)  class_acc: 0.3333 (0.3163)  loss_scale: 65536.0000 (58929.3105)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1027  max mem: 15572
[2025-01-16 02:27:40,452] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:27:40,452] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [1120/2809]  eta: 0:16:01  lr: 0.000017  min_lr: 0.000000  loss: 3.5980 (3.8115)  class_acc: 0.3333 (0.3164)  loss_scale: 65536.0000 (59105.1704)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1282  max mem: 15572
[2025-01-16 02:27:43,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71350
[2025-01-16 02:27:43,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:27:43,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1130/2809]  eta: 0:15:56  lr: 0.000017  min_lr: 0.000000  loss: 3.5285 (3.8086)  class_acc: 0.3750 (0.3173)  loss_scale: 65536.0000 (59393.8108)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1617  max mem: 15572
Epoch: [25]  [1140/2809]  eta: 0:15:50  lr: 0.000017  min_lr: 0.000000  loss: 3.7741 (3.8090)  class_acc: 0.3750 (0.3177)  loss_scale: 65536.0000 (59447.6424)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1475  max mem: 15572
Epoch: [25]  [1150/2809]  eta: 0:15:44  lr: 0.000017  min_lr: 0.000000  loss: 3.9735 (3.8077)  class_acc: 0.2917 (0.3176)  loss_scale: 65536.0000 (59500.5387)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1033  max mem: 15572
Epoch: [25]  [1160/2809]  eta: 0:15:38  lr: 0.000017  min_lr: 0.000000  loss: 3.9413 (3.8091)  class_acc: 0.2917 (0.3173)  loss_scale: 65536.0000 (59552.5237)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1138  max mem: 15572
Epoch: [25]  [1170/2809]  eta: 0:15:32  lr: 0.000017  min_lr: 0.000000  loss: 4.0261 (3.8088)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (59603.6208)  weight_decay: 0.0500 (0.0500)  time: 0.5546  data: 0.1127  max mem: 15572
Epoch: [25]  [1180/2809]  eta: 0:15:27  lr: 0.000017  min_lr: 0.000000  loss: 4.0261 (3.8089)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (59653.8527)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1408  max mem: 15572
Epoch: [25]  [1190/2809]  eta: 0:15:20  lr: 0.000017  min_lr: 0.000000  loss: 3.9144 (3.8097)  class_acc: 0.2917 (0.3177)  loss_scale: 65536.0000 (59703.2410)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1210  max mem: 15572
Epoch: [25]  [1200/2809]  eta: 0:15:16  lr: 0.000017  min_lr: 0.000000  loss: 3.9039 (3.8070)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (59751.8068)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1303  max mem: 15572
Epoch: [25]  [1210/2809]  eta: 0:15:09  lr: 0.000017  min_lr: 0.000000  loss: 3.9039 (3.8076)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (59799.5706)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1342  max mem: 15572
Epoch: [25]  [1220/2809]  eta: 0:15:03  lr: 0.000017  min_lr: 0.000000  loss: 3.9124 (3.8080)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (59846.5520)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0626  max mem: 15572
Epoch: [25]  [1230/2809]  eta: 0:14:58  lr: 0.000017  min_lr: 0.000000  loss: 3.9118 (3.8086)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (59892.7701)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1241  max mem: 15572
Epoch: [25]  [1240/2809]  eta: 0:14:53  lr: 0.000017  min_lr: 0.000000  loss: 3.8040 (3.8076)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (59938.2434)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.1908  max mem: 15572
Epoch: [25]  [1250/2809]  eta: 0:14:47  lr: 0.000017  min_lr: 0.000000  loss: 3.6834 (3.8051)  class_acc: 0.3750 (0.3184)  loss_scale: 65536.0000 (59982.9896)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1417  max mem: 15572
[2025-01-16 02:28:56,955] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:28:56,955] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:28:57,938] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71481
[2025-01-16 02:28:57,938] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:28:57,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1260/2809]  eta: 0:14:42  lr: 0.000017  min_lr: 0.000000  loss: 3.7813 (3.8074)  class_acc: 0.2500 (0.3178)  loss_scale: 65536.0000 (60130.9691)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1192  max mem: 15572
Epoch: [25]  [1270/2809]  eta: 0:14:36  lr: 0.000017  min_lr: 0.000000  loss: 3.9678 (3.8071)  class_acc: 0.2500 (0.3176)  loss_scale: 65536.0000 (60173.4949)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1374  max mem: 15572
Epoch: [25]  [1280/2809]  eta: 0:14:30  lr: 0.000017  min_lr: 0.000000  loss: 3.9169 (3.8080)  class_acc: 0.3333 (0.3177)  loss_scale: 65536.0000 (60215.3568)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1123  max mem: 15572
Epoch: [25]  [1290/2809]  eta: 0:14:24  lr: 0.000017  min_lr: 0.000000  loss: 3.7704 (3.8062)  class_acc: 0.3333 (0.3179)  loss_scale: 65536.0000 (60256.5701)  weight_decay: 0.0500 (0.0500)  time: 0.5406  data: 0.1183  max mem: 15572
Epoch: [25]  [1300/2809]  eta: 0:14:18  lr: 0.000017  min_lr: 0.000000  loss: 3.7939 (3.8063)  class_acc: 0.2917 (0.3175)  loss_scale: 65536.0000 (60297.1499)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1062  max mem: 15572
Epoch: [25]  [1310/2809]  eta: 0:14:12  lr: 0.000017  min_lr: 0.000000  loss: 3.9046 (3.8060)  class_acc: 0.2917 (0.3175)  loss_scale: 65536.0000 (60337.1106)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1073  max mem: 15572
Epoch: [25]  [1320/2809]  eta: 0:14:08  lr: 0.000017  min_lr: 0.000000  loss: 3.7479 (3.8059)  class_acc: 0.3333 (0.3176)  loss_scale: 65536.0000 (60376.4663)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.1848  max mem: 15572
Epoch: [25]  [1330/2809]  eta: 0:14:02  lr: 0.000017  min_lr: 0.000000  loss: 3.7101 (3.8042)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (60415.2307)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1505  max mem: 15572
Epoch: [25]  [1340/2809]  eta: 0:13:56  lr: 0.000017  min_lr: 0.000000  loss: 3.6996 (3.8040)  class_acc: 0.3333 (0.3182)  loss_scale: 65536.0000 (60453.4169)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1223  max mem: 15572
Epoch: [25]  [1350/2809]  eta: 0:13:50  lr: 0.000017  min_lr: 0.000000  loss: 3.7370 (3.8043)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (60491.0377)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1112  max mem: 15572
Epoch: [25]  [1360/2809]  eta: 0:13:43  lr: 0.000017  min_lr: 0.000000  loss: 3.7684 (3.8038)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (60528.1058)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0496  max mem: 15572
Epoch: [25]  [1370/2809]  eta: 0:13:37  lr: 0.000017  min_lr: 0.000000  loss: 3.8052 (3.8058)  class_acc: 0.2500 (0.3177)  loss_scale: 65536.0000 (60564.6331)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0653  max mem: 15572
Epoch: [25]  [1380/2809]  eta: 0:13:31  lr: 0.000017  min_lr: 0.000000  loss: 3.9187 (3.8050)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (60600.6314)  weight_decay: 0.0500 (0.0500)  time: 0.5077  data: 0.0769  max mem: 15572
[2025-01-16 02:30:09,129] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:30:09,129] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:30:09,576] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71611
[2025-01-16 02:30:09,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:30:09,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1390/2809]  eta: 0:13:25  lr: 0.000017  min_lr: 0.000000  loss: 3.5211 (3.8045)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (60683.2265)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1094  max mem: 15572
Epoch: [25]  [1400/2809]  eta: 0:13:20  lr: 0.000017  min_lr: 0.000000  loss: 3.7562 (3.8043)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (60717.8644)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1410  max mem: 15572
Epoch: [25]  [1410/2809]  eta: 0:13:14  lr: 0.000017  min_lr: 0.000000  loss: 3.8559 (3.8048)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (60752.0113)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1193  max mem: 15572
[2025-01-16 02:30:25,774] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71639
[2025-01-16 02:30:25,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:30:25,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [1420/2809]  eta: 0:13:09  lr: 0.000017  min_lr: 0.000000  loss: 3.8530 (3.8039)  class_acc: 0.2917 (0.3180)  loss_scale: 65536.0000 (60624.2590)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1444  max mem: 15572
Epoch: [25]  [1430/2809]  eta: 0:13:03  lr: 0.000017  min_lr: 0.000000  loss: 3.5221 (3.8023)  class_acc: 0.3750 (0.3186)  loss_scale: 32768.0000 (60429.5961)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1265  max mem: 15572
Epoch: [25]  [1440/2809]  eta: 0:12:57  lr: 0.000017  min_lr: 0.000000  loss: 3.6957 (3.8022)  class_acc: 0.4167 (0.3190)  loss_scale: 32768.0000 (60237.6350)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0780  max mem: 15572
Epoch: [25]  [1450/2809]  eta: 0:12:50  lr: 0.000017  min_lr: 0.000000  loss: 3.9276 (3.8036)  class_acc: 0.3333 (0.3186)  loss_scale: 32768.0000 (60048.3198)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0857  max mem: 15572
Epoch: [25]  [1460/2809]  eta: 0:12:45  lr: 0.000017  min_lr: 0.000000  loss: 4.0010 (3.8033)  class_acc: 0.2917 (0.3186)  loss_scale: 32768.0000 (59861.5962)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1129  max mem: 15572
Epoch: [25]  [1470/2809]  eta: 0:12:39  lr: 0.000017  min_lr: 0.000000  loss: 3.8748 (3.8040)  class_acc: 0.2500 (0.3183)  loss_scale: 32768.0000 (59677.4113)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1506  max mem: 15572
Epoch: [25]  [1480/2809]  eta: 0:12:34  lr: 0.000017  min_lr: 0.000000  loss: 3.9198 (3.8048)  class_acc: 0.2500 (0.3181)  loss_scale: 32768.0000 (59495.7137)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1403  max mem: 15572
Epoch: [25]  [1490/2809]  eta: 0:12:28  lr: 0.000017  min_lr: 0.000000  loss: 3.7655 (3.8041)  class_acc: 0.2917 (0.3185)  loss_scale: 32768.0000 (59316.4534)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1036  max mem: 15572
Epoch: [25]  [1500/2809]  eta: 0:12:22  lr: 0.000017  min_lr: 0.000000  loss: 3.8027 (3.8047)  class_acc: 0.3333 (0.3182)  loss_scale: 32768.0000 (59139.5816)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1145  max mem: 15572
Epoch: [25]  [1510/2809]  eta: 0:12:16  lr: 0.000017  min_lr: 0.000000  loss: 3.9063 (3.8044)  class_acc: 0.3333 (0.3183)  loss_scale: 32768.0000 (58965.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1109  max mem: 15572
Epoch: [25]  [1520/2809]  eta: 0:12:10  lr: 0.000017  min_lr: 0.000000  loss: 3.7250 (3.8038)  class_acc: 0.2917 (0.3183)  loss_scale: 32768.0000 (58792.8153)  weight_decay: 0.0500 (0.0500)  time: 0.4940  data: 0.0492  max mem: 15572
Epoch: [25]  [1530/2809]  eta: 0:12:03  lr: 0.000017  min_lr: 0.000000  loss: 3.7500 (3.8045)  class_acc: 0.2917 (0.3179)  loss_scale: 32768.0000 (58622.8295)  weight_decay: 0.0500 (0.0500)  time: 0.4939  data: 0.0493  max mem: 15572
Epoch: [25]  [1540/2809]  eta: 0:11:58  lr: 0.000017  min_lr: 0.000000  loss: 3.8792 (3.8055)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (58455.0500)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0836  max mem: 15572
[2025-01-16 02:31:36,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:31:36,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [1550/2809]  eta: 0:11:53  lr: 0.000017  min_lr: 0.000000  loss: 3.8906 (3.8061)  class_acc: 0.2917 (0.3180)  loss_scale: 32768.0000 (58458.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1449  max mem: 15572
Epoch: [25]  [1560/2809]  eta: 0:11:46  lr: 0.000017  min_lr: 0.000000  loss: 3.8906 (3.8056)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (58503.7899)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1286  max mem: 15572
Epoch: [25]  [1570/2809]  eta: 0:11:42  lr: 0.000017  min_lr: 0.000000  loss: 3.6807 (3.8047)  class_acc: 0.2917 (0.3181)  loss_scale: 65536.0000 (58548.5525)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1496  max mem: 15572
Epoch: [25]  [1580/2809]  eta: 0:11:36  lr: 0.000017  min_lr: 0.000000  loss: 3.7397 (3.8045)  class_acc: 0.2917 (0.3179)  loss_scale: 65536.0000 (58592.7489)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1674  max mem: 15572
Epoch: [25]  [1590/2809]  eta: 0:11:30  lr: 0.000017  min_lr: 0.000000  loss: 3.9675 (3.8061)  class_acc: 0.2500 (0.3173)  loss_scale: 65536.0000 (58636.3897)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1114  max mem: 15572
Epoch: [25]  [1600/2809]  eta: 0:11:26  lr: 0.000017  min_lr: 0.000000  loss: 3.9639 (3.8060)  class_acc: 0.2500 (0.3176)  loss_scale: 65536.0000 (58679.4853)  weight_decay: 0.0500 (0.0500)  time: 0.6474  data: 0.2109  max mem: 15572
Epoch: [25]  [1610/2809]  eta: 0:11:20  lr: 0.000017  min_lr: 0.000000  loss: 3.9092 (3.8067)  class_acc: 0.2917 (0.3175)  loss_scale: 65536.0000 (58722.0459)  weight_decay: 0.0500 (0.0500)  time: 0.6554  data: 0.2080  max mem: 15572
Epoch: [25]  [1620/2809]  eta: 0:11:14  lr: 0.000017  min_lr: 0.000000  loss: 3.7517 (3.8057)  class_acc: 0.3333 (0.3180)  loss_scale: 65536.0000 (58764.0814)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1047  max mem: 15572
Epoch: [25]  [1630/2809]  eta: 0:11:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6801 (3.8059)  class_acc: 0.3750 (0.3180)  loss_scale: 65536.0000 (58805.6015)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0634  max mem: 15572
Epoch: [25]  [1640/2809]  eta: 0:11:03  lr: 0.000017  min_lr: 0.000000  loss: 3.8573 (3.8063)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (58846.6155)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1012  max mem: 15572
Epoch: [25]  [1650/2809]  eta: 0:10:57  lr: 0.000017  min_lr: 0.000000  loss: 3.7810 (3.8048)  class_acc: 0.2917 (0.3183)  loss_scale: 65536.0000 (58887.1326)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.1614  max mem: 15572
Epoch: [25]  [1660/2809]  eta: 0:10:51  lr: 0.000017  min_lr: 0.000000  loss: 3.7810 (3.8046)  class_acc: 0.2917 (0.3182)  loss_scale: 65536.0000 (58927.1620)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0914  max mem: 15572
Epoch: [25]  [1670/2809]  eta: 0:10:45  lr: 0.000017  min_lr: 0.000000  loss: 3.8032 (3.8041)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (58966.7121)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0811  max mem: 15572
[2025-01-16 02:32:50,346] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:32:50,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:32:52,096] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 71900
[2025-01-16 02:32:52,097] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:32:52,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1680/2809]  eta: 0:10:39  lr: 0.000017  min_lr: 0.000000  loss: 3.6531 (3.8028)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (59161.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.1072  max mem: 15572
Epoch: [25]  [1690/2809]  eta: 0:10:33  lr: 0.000017  min_lr: 0.000000  loss: 3.6531 (3.8026)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (59199.4323)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0762  max mem: 15572
Epoch: [25]  [1700/2809]  eta: 0:10:27  lr: 0.000017  min_lr: 0.000000  loss: 3.6264 (3.8021)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (59236.6843)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1095  max mem: 15572
Epoch: [25]  [1710/2809]  eta: 0:10:21  lr: 0.000017  min_lr: 0.000000  loss: 3.6300 (3.8020)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (59273.5009)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1142  max mem: 15572
Epoch: [25]  [1720/2809]  eta: 0:10:15  lr: 0.000017  min_lr: 0.000000  loss: 3.7016 (3.8017)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (59309.8896)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0764  max mem: 15572
Epoch: [25]  [1730/2809]  eta: 0:10:10  lr: 0.000017  min_lr: 0.000000  loss: 3.8192 (3.8025)  class_acc: 0.3333 (0.3184)  loss_scale: 65536.0000 (59345.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1081  max mem: 15572
Epoch: [25]  [1740/2809]  eta: 0:10:04  lr: 0.000017  min_lr: 0.000000  loss: 3.8878 (3.8018)  class_acc: 0.3333 (0.3187)  loss_scale: 65536.0000 (59381.4130)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1281  max mem: 15572
Epoch: [25]  [1750/2809]  eta: 0:09:58  lr: 0.000017  min_lr: 0.000000  loss: 3.8878 (3.8025)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (59416.5620)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0514  max mem: 15572
Epoch: [25]  [1760/2809]  eta: 0:09:52  lr: 0.000017  min_lr: 0.000000  loss: 3.9848 (3.8031)  class_acc: 0.2500 (0.3184)  loss_scale: 65536.0000 (59451.3118)  weight_decay: 0.0500 (0.0500)  time: 0.4828  data: 0.0335  max mem: 15572
Epoch: [25]  [1770/2809]  eta: 0:09:46  lr: 0.000017  min_lr: 0.000000  loss: 3.9596 (3.8026)  class_acc: 0.2500 (0.3189)  loss_scale: 65536.0000 (59485.6691)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1041  max mem: 15572
[2025-01-16 02:33:46,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=72000, skipped=481, lr=[1.6438077232268055e-07, 1.6438077232268055e-07, 2.3482967474668654e-07, 2.3482967474668654e-07, 3.3547096392383795e-07, 3.3547096392383795e-07, 4.792442341769114e-07, 4.792442341769114e-07, 6.846346202527306e-07, 6.846346202527306e-07, 9.78049457503901e-07, 9.78049457503901e-07, 1.3972135107198584e-06, 1.3972135107198584e-06, 1.9960193010283693e-06, 1.9960193010283693e-06, 2.851456144326242e-06, 2.851456144326242e-06, 4.073508777608918e-06, 4.073508777608918e-06, 5.819298253727025e-06, 5.819298253727025e-06, 8.313283219610036e-06, 8.313283219610036e-06, 1.1876118885157196e-05, 1.1876118885157196e-05, 1.6965884121653138e-05, 1.6965884121653138e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:33:46,017] [INFO] [timer.py:260:stop] epoch=0/micro_step=72000/global_step=72000, RunningAvgSamplesPerSec=28.550136795701174, CurrSamplesPerSec=25.658388075892915, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [1780/2809]  eta: 0:09:41  lr: 0.000017  min_lr: 0.000000  loss: 3.9223 (3.8035)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (59519.6407)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1176  max mem: 15572
Epoch: [25]  [1790/2809]  eta: 0:09:35  lr: 0.000017  min_lr: 0.000000  loss: 3.9223 (3.8045)  class_acc: 0.2917 (0.3184)  loss_scale: 65536.0000 (59553.2328)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.1069  max mem: 15572
Epoch: [25]  [1800/2809]  eta: 0:09:29  lr: 0.000017  min_lr: 0.000000  loss: 3.7589 (3.8035)  class_acc: 0.3333 (0.3188)  loss_scale: 65536.0000 (59586.4520)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.0909  max mem: 15572
[2025-01-16 02:34:02,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:34:02,632] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:34:03,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72030
[2025-01-16 02:34:03,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:34:03,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1810/2809]  eta: 0:09:24  lr: 0.000017  min_lr: 0.000000  loss: 3.6667 (3.8029)  class_acc: 0.3750 (0.3189)  loss_scale: 65536.0000 (59655.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.0972  max mem: 15572
Epoch: [25]  [1820/2809]  eta: 0:09:18  lr: 0.000017  min_lr: 0.000000  loss: 3.6903 (3.8020)  class_acc: 0.3750 (0.3192)  loss_scale: 65536.0000 (59687.7847)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1390  max mem: 15572
Epoch: [25]  [1830/2809]  eta: 0:09:12  lr: 0.000017  min_lr: 0.000000  loss: 3.9005 (3.8023)  class_acc: 0.3333 (0.3192)  loss_scale: 65536.0000 (59719.7247)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1295  max mem: 15572
Epoch: [25]  [1840/2809]  eta: 0:09:07  lr: 0.000017  min_lr: 0.000000  loss: 3.5218 (3.8013)  class_acc: 0.3333 (0.3192)  loss_scale: 65536.0000 (59751.3178)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1321  max mem: 15572
Epoch: [25]  [1850/2809]  eta: 0:09:01  lr: 0.000017  min_lr: 0.000000  loss: 3.8817 (3.8014)  class_acc: 0.3333 (0.3192)  loss_scale: 65536.0000 (59782.5694)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1334  max mem: 15572
Epoch: [25]  [1860/2809]  eta: 0:08:56  lr: 0.000017  min_lr: 0.000000  loss: 3.9983 (3.8022)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (59813.4852)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1197  max mem: 15572
Epoch: [25]  [1870/2809]  eta: 0:08:50  lr: 0.000017  min_lr: 0.000000  loss: 4.0646 (3.8026)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (59844.0706)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1055  max mem: 15572
Epoch: [25]  [1880/2809]  eta: 0:08:44  lr: 0.000017  min_lr: 0.000000  loss: 3.6096 (3.8023)  class_acc: 0.3333 (0.3188)  loss_scale: 65536.0000 (59874.3307)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0906  max mem: 15572
Epoch: [25]  [1890/2809]  eta: 0:08:38  lr: 0.000017  min_lr: 0.000000  loss: 3.7564 (3.8029)  class_acc: 0.2917 (0.3188)  loss_scale: 65536.0000 (59904.2708)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0834  max mem: 15572
Epoch: [25]  [1900/2809]  eta: 0:08:32  lr: 0.000017  min_lr: 0.000000  loss: 3.8498 (3.8025)  class_acc: 0.2917 (0.3188)  loss_scale: 65536.0000 (59933.8958)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0867  max mem: 15572
Epoch: [25]  [1910/2809]  eta: 0:08:27  lr: 0.000017  min_lr: 0.000000  loss: 3.8498 (3.8024)  class_acc: 0.2917 (0.3189)  loss_scale: 65536.0000 (59963.2109)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1208  max mem: 15572
Epoch: [25]  [1920/2809]  eta: 0:08:21  lr: 0.000017  min_lr: 0.000000  loss: 3.8651 (3.8026)  class_acc: 0.2500 (0.3186)  loss_scale: 65536.0000 (59992.2207)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1061  max mem: 15572
Epoch: [25]  [1930/2809]  eta: 0:08:16  lr: 0.000017  min_lr: 0.000000  loss: 3.8651 (3.8019)  class_acc: 0.2500 (0.3187)  loss_scale: 65536.0000 (60020.9301)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.0993  max mem: 15572
[2025-01-16 02:35:14,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:35:14,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:35:15,999] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72162
[2025-01-16 02:35:15,999] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:35:15,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1940/2809]  eta: 0:08:10  lr: 0.000017  min_lr: 0.000000  loss: 3.6013 (3.8011)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (60150.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.0936  max mem: 15572
Epoch: [25]  [1950/2809]  eta: 0:08:04  lr: 0.000017  min_lr: 0.000000  loss: 3.8094 (3.8013)  class_acc: 0.2917 (0.3187)  loss_scale: 65536.0000 (60178.2389)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0859  max mem: 15572
Epoch: [25]  [1960/2809]  eta: 0:07:58  lr: 0.000017  min_lr: 0.000000  loss: 3.7586 (3.8001)  class_acc: 0.3333 (0.3190)  loss_scale: 65536.0000 (60205.5604)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1048  max mem: 15572
Epoch: [25]  [1970/2809]  eta: 0:07:53  lr: 0.000017  min_lr: 0.000000  loss: 3.7586 (3.8000)  class_acc: 0.3333 (0.3189)  loss_scale: 65536.0000 (60232.6048)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0630  max mem: 15572
Epoch: [25]  [1980/2809]  eta: 0:07:47  lr: 0.000017  min_lr: 0.000000  loss: 3.7963 (3.7989)  class_acc: 0.3333 (0.3193)  loss_scale: 65536.0000 (60259.3761)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1096  max mem: 15572
Epoch: [25]  [1990/2809]  eta: 0:07:41  lr: 0.000017  min_lr: 0.000000  loss: 3.8597 (3.7995)  class_acc: 0.3750 (0.3193)  loss_scale: 65536.0000 (60285.8785)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1266  max mem: 15572
Epoch: [25]  [2000/2809]  eta: 0:07:36  lr: 0.000017  min_lr: 0.000000  loss: 3.8597 (3.7989)  class_acc: 0.3333 (0.3195)  loss_scale: 65536.0000 (60312.1159)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0922  max mem: 15572
Epoch: [25]  [2010/2809]  eta: 0:07:30  lr: 0.000017  min_lr: 0.000000  loss: 4.0534 (3.7989)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (60338.0925)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1285  max mem: 15572
Epoch: [25]  [2020/2809]  eta: 0:07:24  lr: 0.000017  min_lr: 0.000000  loss: 4.0509 (3.7989)  class_acc: 0.2500 (0.3195)  loss_scale: 65536.0000 (60363.8120)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1056  max mem: 15572
Epoch: [25]  [2030/2809]  eta: 0:07:19  lr: 0.000017  min_lr: 0.000000  loss: 3.6997 (3.7977)  class_acc: 0.3333 (0.3199)  loss_scale: 65536.0000 (60389.2782)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0943  max mem: 15572
Epoch: [25]  [2040/2809]  eta: 0:07:13  lr: 0.000017  min_lr: 0.000000  loss: 3.7260 (3.7985)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (60414.4949)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1292  max mem: 15572
Epoch: [25]  [2050/2809]  eta: 0:07:07  lr: 0.000017  min_lr: 0.000000  loss: 3.9978 (3.7991)  class_acc: 0.2500 (0.3193)  loss_scale: 65536.0000 (60439.4656)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1080  max mem: 15572
Epoch: [25]  [2060/2809]  eta: 0:07:02  lr: 0.000017  min_lr: 0.000000  loss: 4.0478 (3.8002)  class_acc: 0.2500 (0.3190)  loss_scale: 65536.0000 (60464.1941)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0940  max mem: 15572
[2025-01-16 02:36:27,643] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:36:27,643] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:36:30,975] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72295
[2025-01-16 02:36:30,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:36:30,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [2070/2809]  eta: 0:06:56  lr: 0.000017  min_lr: 0.000000  loss: 3.9968 (3.8003)  class_acc: 0.2917 (0.3192)  loss_scale: 65536.0000 (60615.2622)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1458  max mem: 15572
[2025-01-16 02:36:35,043] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72304
[2025-01-16 02:36:35,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:36:35,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2080/2809]  eta: 0:06:50  lr: 0.000017  min_lr: 0.000000  loss: 3.8206 (3.8004)  class_acc: 0.3750 (0.3191)  loss_scale: 65536.0000 (60607.4157)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1201  max mem: 15572
Epoch: [25]  [2090/2809]  eta: 0:06:45  lr: 0.000017  min_lr: 0.000000  loss: 3.7553 (3.7994)  class_acc: 0.2917 (0.3192)  loss_scale: 32768.0000 (60474.2764)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1128  max mem: 15572
Epoch: [25]  [2100/2809]  eta: 0:06:39  lr: 0.000017  min_lr: 0.000000  loss: 3.6766 (3.7990)  class_acc: 0.3333 (0.3192)  loss_scale: 32768.0000 (60342.4046)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1424  max mem: 15572
Epoch: [25]  [2110/2809]  eta: 0:06:34  lr: 0.000017  min_lr: 0.000000  loss: 3.6499 (3.7977)  class_acc: 0.3333 (0.3195)  loss_scale: 32768.0000 (60211.7821)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1381  max mem: 15572
Epoch: [25]  [2120/2809]  eta: 0:06:28  lr: 0.000017  min_lr: 0.000000  loss: 3.6062 (3.7971)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (60082.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0969  max mem: 15572
Epoch: [25]  [2130/2809]  eta: 0:06:23  lr: 0.000017  min_lr: 0.000000  loss: 3.7966 (3.7968)  class_acc: 0.2917 (0.3195)  loss_scale: 32768.0000 (59954.2149)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1044  max mem: 15572
Epoch: [25]  [2140/2809]  eta: 0:06:17  lr: 0.000017  min_lr: 0.000000  loss: 3.6642 (3.7970)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (59827.2359)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1118  max mem: 15572
[2025-01-16 02:37:14,458] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72373
[2025-01-16 02:37:14,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 02:37:14,458] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [2150/2809]  eta: 0:06:11  lr: 0.000017  min_lr: 0.000000  loss: 3.7791 (3.7969)  class_acc: 0.3333 (0.3197)  loss_scale: 32768.0000 (59678.5867)  weight_decay: 0.0500 (0.0500)  time: 0.5000  data: 0.0577  max mem: 15572
Epoch: [25]  [2160/2809]  eta: 0:06:05  lr: 0.000017  min_lr: 0.000000  loss: 3.7791 (3.7970)  class_acc: 0.3333 (0.3199)  loss_scale: 16384.0000 (59478.2416)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.1071  max mem: 15572
Epoch: [25]  [2170/2809]  eta: 0:06:00  lr: 0.000017  min_lr: 0.000000  loss: 3.7002 (3.7973)  class_acc: 0.3333 (0.3197)  loss_scale: 16384.0000 (59279.7421)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1992  max mem: 15572
Epoch: [25]  [2180/2809]  eta: 0:05:54  lr: 0.000017  min_lr: 0.000000  loss: 3.6832 (3.7964)  class_acc: 0.3333 (0.3199)  loss_scale: 16384.0000 (59083.0628)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1444  max mem: 15572
Epoch: [25]  [2190/2809]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 3.7436 (3.7961)  class_acc: 0.3333 (0.3200)  loss_scale: 16384.0000 (58888.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5176  data: 0.0632  max mem: 15572
Epoch: [25]  [2200/2809]  eta: 0:05:43  lr: 0.000017  min_lr: 0.000000  loss: 3.6157 (3.7955)  class_acc: 0.3333 (0.3200)  loss_scale: 16384.0000 (58695.0659)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0893  max mem: 15572
Epoch: [25]  [2210/2809]  eta: 0:05:37  lr: 0.000017  min_lr: 0.000000  loss: 3.6654 (3.7955)  class_acc: 0.2917 (0.3200)  loss_scale: 16384.0000 (58503.6997)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0493  max mem: 15572
Epoch: [25]  [2220/2809]  eta: 0:05:31  lr: 0.000017  min_lr: 0.000000  loss: 3.7321 (3.7951)  class_acc: 0.2917 (0.3200)  loss_scale: 16384.0000 (58314.0567)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0552  max mem: 15572
Epoch: [25]  [2230/2809]  eta: 0:05:25  lr: 0.000017  min_lr: 0.000000  loss: 3.7847 (3.7949)  class_acc: 0.3333 (0.3201)  loss_scale: 16384.0000 (58126.1139)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.1052  max mem: 15572
Epoch: [25]  [2240/2809]  eta: 0:05:20  lr: 0.000017  min_lr: 0.000000  loss: 3.8563 (3.7958)  class_acc: 0.3333 (0.3199)  loss_scale: 16384.0000 (57939.8483)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0715  max mem: 15572
Epoch: [25]  [2250/2809]  eta: 0:05:14  lr: 0.000017  min_lr: 0.000000  loss: 3.7899 (3.7954)  class_acc: 0.2500 (0.3201)  loss_scale: 16384.0000 (57755.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1021  max mem: 15572
Epoch: [25]  [2260/2809]  eta: 0:05:08  lr: 0.000017  min_lr: 0.000000  loss: 3.6228 (3.7953)  class_acc: 0.2917 (0.3200)  loss_scale: 16384.0000 (57572.2601)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1538  max mem: 15572
Epoch: [25]  [2270/2809]  eta: 0:05:03  lr: 0.000017  min_lr: 0.000000  loss: 3.8790 (3.7955)  class_acc: 0.2500 (0.3198)  loss_scale: 16384.0000 (57390.8939)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1028  max mem: 15572
[2025-01-16 02:38:26,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:38:26,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [2280/2809]  eta: 0:04:57  lr: 0.000017  min_lr: 0.000000  loss: 3.8244 (3.7952)  class_acc: 0.3333 (0.3198)  loss_scale: 16384.0000 (57239.8492)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1494  max mem: 15572
Epoch: [25]  [2290/2809]  eta: 0:04:52  lr: 0.000017  min_lr: 0.000000  loss: 3.5335 (3.7945)  class_acc: 0.3333 (0.3196)  loss_scale: 32768.0000 (57133.0319)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1462  max mem: 15572
Epoch: [25]  [2300/2809]  eta: 0:04:46  lr: 0.000017  min_lr: 0.000000  loss: 3.6048 (3.7935)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (57027.1430)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1168  max mem: 15572
Epoch: [25]  [2310/2809]  eta: 0:04:40  lr: 0.000017  min_lr: 0.000000  loss: 3.6747 (3.7931)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (56922.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1316  max mem: 15572
Epoch: [25]  [2320/2809]  eta: 0:04:35  lr: 0.000017  min_lr: 0.000000  loss: 3.7412 (3.7930)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (56818.1025)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1077  max mem: 15572
Epoch: [25]  [2330/2809]  eta: 0:04:29  lr: 0.000017  min_lr: 0.000000  loss: 3.7342 (3.7928)  class_acc: 0.3333 (0.3201)  loss_scale: 32768.0000 (56714.9275)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0753  max mem: 15572
Epoch: [25]  [2340/2809]  eta: 0:04:24  lr: 0.000017  min_lr: 0.000000  loss: 3.5988 (3.7916)  class_acc: 0.3750 (0.3204)  loss_scale: 32768.0000 (56612.6339)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.1018  max mem: 15572
Epoch: [25]  [2350/2809]  eta: 0:04:18  lr: 0.000017  min_lr: 0.000000  loss: 3.5360 (3.7904)  class_acc: 0.3750 (0.3206)  loss_scale: 32768.0000 (56511.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1135  max mem: 15572
Epoch: [25]  [2360/2809]  eta: 0:04:12  lr: 0.000017  min_lr: 0.000000  loss: 3.6278 (3.7902)  class_acc: 0.3750 (0.3208)  loss_scale: 32768.0000 (56410.6463)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1035  max mem: 15572
Epoch: [25]  [2370/2809]  eta: 0:04:06  lr: 0.000017  min_lr: 0.000000  loss: 3.7592 (3.7904)  class_acc: 0.3750 (0.3207)  loss_scale: 32768.0000 (56310.9304)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.0940  max mem: 15572
Epoch: [25]  [2380/2809]  eta: 0:04:01  lr: 0.000017  min_lr: 0.000000  loss: 3.6520 (3.7889)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (56212.0521)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1034  max mem: 15572
Epoch: [25]  [2390/2809]  eta: 0:03:55  lr: 0.000017  min_lr: 0.000000  loss: 3.5308 (3.7883)  class_acc: 0.3333 (0.3208)  loss_scale: 32768.0000 (56114.0008)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1833  max mem: 15572
Epoch: [25]  [2400/2809]  eta: 0:03:50  lr: 0.000017  min_lr: 0.000000  loss: 3.6453 (3.7887)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (56016.7663)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1590  max mem: 15572
[2025-01-16 02:39:38,621] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:39:38,621] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2410/2809]  eta: 0:03:44  lr: 0.000017  min_lr: 0.000000  loss: 4.0127 (3.7890)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (56001.8847)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1474  max mem: 15572
Epoch: [25]  [2420/2809]  eta: 0:03:38  lr: 0.000017  min_lr: 0.000000  loss: 4.0116 (3.7896)  class_acc: 0.2917 (0.3206)  loss_scale: 65536.0000 (56041.2656)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1177  max mem: 15572
Epoch: [25]  [2430/2809]  eta: 0:03:33  lr: 0.000016  min_lr: 0.000000  loss: 4.0482 (3.7901)  class_acc: 0.2500 (0.3204)  loss_scale: 65536.0000 (56080.3225)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0721  max mem: 15572
Epoch: [25]  [2440/2809]  eta: 0:03:27  lr: 0.000016  min_lr: 0.000000  loss: 3.9143 (3.7903)  class_acc: 0.3333 (0.3204)  loss_scale: 65536.0000 (56119.0594)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1405  max mem: 15572
Epoch: [25]  [2450/2809]  eta: 0:03:22  lr: 0.000016  min_lr: 0.000000  loss: 3.7359 (3.7895)  class_acc: 0.3333 (0.3205)  loss_scale: 65536.0000 (56157.4802)  weight_decay: 0.0500 (0.0500)  time: 0.6305  data: 0.1894  max mem: 15572
Epoch: [25]  [2460/2809]  eta: 0:03:16  lr: 0.000016  min_lr: 0.000000  loss: 3.7508 (3.7896)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (56195.5888)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1433  max mem: 15572
Epoch: [25]  [2470/2809]  eta: 0:03:10  lr: 0.000016  min_lr: 0.000000  loss: 3.7404 (3.7890)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (56233.3889)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.1015  max mem: 15572
Epoch: [25]  [2480/2809]  eta: 0:03:05  lr: 0.000016  min_lr: 0.000000  loss: 3.7963 (3.7898)  class_acc: 0.2500 (0.3205)  loss_scale: 65536.0000 (56270.8843)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1377  max mem: 15572
Epoch: [25]  [2490/2809]  eta: 0:02:59  lr: 0.000016  min_lr: 0.000000  loss: 3.9057 (3.7893)  class_acc: 0.2500 (0.3206)  loss_scale: 65536.0000 (56308.0787)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1680  max mem: 15572
Epoch: [25]  [2500/2809]  eta: 0:02:54  lr: 0.000016  min_lr: 0.000000  loss: 3.6804 (3.7897)  class_acc: 0.3333 (0.3207)  loss_scale: 65536.0000 (56344.9756)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.1750  max mem: 15572
Epoch: [25]  [2510/2809]  eta: 0:02:48  lr: 0.000016  min_lr: 0.000000  loss: 3.8169 (3.7900)  class_acc: 0.3333 (0.3209)  loss_scale: 65536.0000 (56381.5787)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1039  max mem: 15572
Epoch: [25]  [2520/2809]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000000  loss: 3.8458 (3.7900)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (56417.8913)  weight_decay: 0.0500 (0.0500)  time: 0.5307  data: 0.0999  max mem: 15572
Epoch: [25]  [2530/2809]  eta: 0:02:37  lr: 0.000016  min_lr: 0.000000  loss: 3.6872 (3.7890)  class_acc: 0.3750 (0.3212)  loss_scale: 65536.0000 (56453.9170)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1427  max mem: 15572
[2025-01-16 02:40:52,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:40:52,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:40:54,892] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72764
[2025-01-16 02:40:54,892] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:40:54,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [2540/2809]  eta: 0:02:31  lr: 0.000016  min_lr: 0.000000  loss: 3.6091 (3.7896)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (56644.4077)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1361  max mem: 15572
Epoch: [25]  [2550/2809]  eta: 0:02:25  lr: 0.000016  min_lr: 0.000000  loss: 3.5709 (3.7893)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (56679.2630)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1431  max mem: 15572
Epoch: [25]  [2560/2809]  eta: 0:02:20  lr: 0.000016  min_lr: 0.000000  loss: 3.6981 (3.7898)  class_acc: 0.3333 (0.3211)  loss_scale: 65536.0000 (56713.8462)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1285  max mem: 15572
[2025-01-16 02:41:09,933] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72789
[2025-01-16 02:41:09,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:41:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2570/2809]  eta: 0:02:14  lr: 0.000016  min_lr: 0.000000  loss: 3.9246 (3.7902)  class_acc: 0.2917 (0.3209)  loss_scale: 65536.0000 (56658.9436)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1611  max mem: 15572
Epoch: [25]  [2580/2809]  eta: 0:02:09  lr: 0.000016  min_lr: 0.000000  loss: 3.8360 (3.7904)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (56566.3789)  weight_decay: 0.0500 (0.0500)  time: 0.6431  data: 0.2054  max mem: 15572
Epoch: [25]  [2590/2809]  eta: 0:02:03  lr: 0.000016  min_lr: 0.000000  loss: 3.9442 (3.7904)  class_acc: 0.2500 (0.3209)  loss_scale: 32768.0000 (56474.5288)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1231  max mem: 15572
Epoch: [25]  [2600/2809]  eta: 0:01:57  lr: 0.000016  min_lr: 0.000000  loss: 3.9194 (3.7902)  class_acc: 0.2500 (0.3208)  loss_scale: 32768.0000 (56383.3849)  weight_decay: 0.0500 (0.0500)  time: 0.4602  data: 0.0239  max mem: 15572
Epoch: [25]  [2610/2809]  eta: 0:01:52  lr: 0.000016  min_lr: 0.000000  loss: 3.7311 (3.7903)  class_acc: 0.2917 (0.3208)  loss_scale: 32768.0000 (56292.9391)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1055  max mem: 15572
Epoch: [25]  [2620/2809]  eta: 0:01:46  lr: 0.000016  min_lr: 0.000000  loss: 3.9204 (3.7905)  class_acc: 0.2500 (0.3206)  loss_scale: 32768.0000 (56203.1835)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.1752  max mem: 15572
Epoch: [25]  [2630/2809]  eta: 0:01:40  lr: 0.000016  min_lr: 0.000000  loss: 3.8300 (3.7903)  class_acc: 0.2917 (0.3209)  loss_scale: 32768.0000 (56114.1102)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.1392  max mem: 15572
Epoch: [25]  [2640/2809]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 3.7329 (3.7909)  class_acc: 0.2917 (0.3206)  loss_scale: 32768.0000 (56025.7115)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1149  max mem: 15572
Epoch: [25]  [2650/2809]  eta: 0:01:29  lr: 0.000016  min_lr: 0.000000  loss: 3.7558 (3.7902)  class_acc: 0.2500 (0.3207)  loss_scale: 32768.0000 (55937.9796)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1212  max mem: 15572
Epoch: [25]  [2660/2809]  eta: 0:01:24  lr: 0.000016  min_lr: 0.000000  loss: 3.9085 (3.7916)  class_acc: 0.2500 (0.3203)  loss_scale: 32768.0000 (55850.9072)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1430  max mem: 15572
Epoch: [25]  [2670/2809]  eta: 0:01:18  lr: 0.000016  min_lr: 0.000000  loss: 4.1527 (3.7923)  class_acc: 0.2083 (0.3201)  loss_scale: 32768.0000 (55764.4867)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0910  max mem: 15572
Epoch: [25]  [2680/2809]  eta: 0:01:12  lr: 0.000016  min_lr: 0.000000  loss: 3.8444 (3.7919)  class_acc: 0.2500 (0.3202)  loss_scale: 32768.0000 (55678.7109)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0504  max mem: 15572
Epoch: [25]  [2690/2809]  eta: 0:01:07  lr: 0.000016  min_lr: 0.000000  loss: 3.7806 (3.7918)  class_acc: 0.2917 (0.3202)  loss_scale: 32768.0000 (55593.5726)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1087  max mem: 15572
[2025-01-16 02:42:22,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:42:22,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [2700/2809]  eta: 0:01:01  lr: 0.000016  min_lr: 0.000000  loss: 3.7808 (3.7917)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (55606.1192)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1251  max mem: 15572
Epoch: [25]  [2710/2809]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 3.8124 (3.7919)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (55642.7473)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0825  max mem: 15572
Epoch: [25]  [2720/2809]  eta: 0:00:50  lr: 0.000016  min_lr: 0.000000  loss: 3.8124 (3.7927)  class_acc: 0.2917 (0.3200)  loss_scale: 65536.0000 (55679.1062)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1900  max mem: 15572
Epoch: [25]  [2730/2809]  eta: 0:00:44  lr: 0.000016  min_lr: 0.000000  loss: 3.7974 (3.7923)  class_acc: 0.2917 (0.3200)  loss_scale: 65536.0000 (55715.1988)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.2111  max mem: 15572
[2025-01-16 02:42:46,792] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 72960
[2025-01-16 02:42:46,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:42:46,792] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [2740/2809]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000000  loss: 3.6879 (3.7921)  class_acc: 0.2917 (0.3200)  loss_scale: 65536.0000 (55679.2995)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0608  max mem: 15572
Epoch: [25]  [2750/2809]  eta: 0:00:33  lr: 0.000016  min_lr: 0.000000  loss: 3.8419 (3.7921)  class_acc: 0.2917 (0.3201)  loss_scale: 32768.0000 (55596.0160)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0837  max mem: 15572
Epoch: [25]  [2760/2809]  eta: 0:00:27  lr: 0.000016  min_lr: 0.000000  loss: 3.9617 (3.7925)  class_acc: 0.2500 (0.3199)  loss_scale: 32768.0000 (55513.3357)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0888  max mem: 15572
Epoch: [25]  [2770/2809]  eta: 0:00:21  lr: 0.000016  min_lr: 0.000000  loss: 3.9167 (3.7929)  class_acc: 0.2500 (0.3198)  loss_scale: 32768.0000 (55431.2523)  weight_decay: 0.0500 (0.0500)  time: 0.4766  data: 0.0395  max mem: 15572
[2025-01-16 02:43:06,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=73000, skipped=489, lr=[1.5744952022753748e-07, 1.5744952022753748e-07, 2.2492788603933928e-07, 2.2492788603933928e-07, 3.2132555148477044e-07, 3.2132555148477044e-07, 4.5903650212110065e-07, 4.5903650212110065e-07, 6.557664316015724e-07, 6.557664316015724e-07, 9.368091880022463e-07, 9.368091880022463e-07, 1.338298840003209e-06, 1.338298840003209e-06, 1.91185548571887e-06, 1.91185548571887e-06, 2.731222122455529e-06, 2.731222122455529e-06, 3.9017458892221845e-06, 3.9017458892221845e-06, 5.573922698888835e-06, 5.573922698888835e-06, 7.962746712698336e-06, 7.962746712698336e-06, 1.137535244671191e-05, 1.137535244671191e-05, 1.625050349530273e-05, 1.625050349530273e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:43:06,306] [INFO] [timer.py:260:stop] epoch=0/micro_step=73000/global_step=73000, RunningAvgSamplesPerSec=28.54989650963584, CurrSamplesPerSec=22.093802305353304, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [25]  [2780/2809]  eta: 0:00:16  lr: 0.000016  min_lr: 0.000000  loss: 3.7996 (3.7931)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (55349.7591)  weight_decay: 0.0500 (0.0500)  time: 0.4644  data: 0.0156  max mem: 15572
Epoch: [25]  [2790/2809]  eta: 0:00:10  lr: 0.000016  min_lr: 0.000000  loss: 3.7935 (3.7929)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (55268.8499)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0335  max mem: 15572
Epoch: [25]  [2800/2809]  eta: 0:00:05  lr: 0.000016  min_lr: 0.000000  loss: 3.7734 (3.7929)  class_acc: 0.3333 (0.3198)  loss_scale: 32768.0000 (55188.5184)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1032  max mem: 15572
Epoch: [25]  [2808/2809]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 3.8493 (3.7939)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (55124.6650)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0786  max mem: 15572
Epoch: [25] Total time: 0:26:21 (0.5628 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 3.8493 (3.7939)  class_acc: 0.2917 (0.3196)  loss_scale: 32768.0000 (55124.6650)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:21:23  loss: 0.4054 (0.4054)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.7191  data: 4.5227  max mem: 15572
Val:  [ 10/272]  eta: 0:03:31  loss: 2.5095 (2.4044)  acc1: 44.4444 (39.3939)  acc5: 66.6667 (69.6970)  time: 0.8066  data: 0.6244  max mem: 15572
Val:  [ 20/272]  eta: 0:02:23  loss: 2.5095 (2.4219)  acc1: 44.4444 (44.4444)  acc5: 66.6667 (70.6349)  time: 0.3634  data: 0.1713  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 2.5572 (2.5229)  acc1: 44.4444 (40.5018)  acc5: 72.2222 (69.7133)  time: 0.2511  data: 0.0544  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 2.6050 (2.5489)  acc1: 33.3333 (38.0759)  acc5: 72.2222 (70.3252)  time: 0.2908  data: 0.1072  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 2.5295 (2.4529)  acc1: 38.8889 (40.6318)  acc5: 77.7778 (72.7669)  time: 0.3193  data: 0.1348  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 1.5289 (2.3434)  acc1: 61.1111 (43.6248)  acc5: 83.3333 (74.1348)  time: 0.3133  data: 0.1059  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.5912 (2.2571)  acc1: 61.1111 (46.1659)  acc5: 83.3333 (75.1956)  time: 0.3234  data: 0.1222  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.9608 (2.2715)  acc1: 55.5556 (45.8848)  acc5: 77.7778 (74.9657)  time: 0.2691  data: 0.0864  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.1048 (2.2628)  acc1: 50.0000 (46.6422)  acc5: 77.7778 (75.9463)  time: 0.2715  data: 0.0856  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.1666 (2.2916)  acc1: 50.0000 (45.8746)  acc5: 83.3333 (75.5226)  time: 0.3018  data: 0.1108  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.4865 (2.3587)  acc1: 33.3333 (44.1441)  acc5: 77.7778 (74.4745)  time: 0.3232  data: 0.1270  max mem: 15572
Val:  [120/272]  eta: 0:00:51  loss: 2.7744 (2.3868)  acc1: 27.7778 (43.5262)  acc5: 72.2222 (74.1965)  time: 0.2793  data: 0.0903  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.0696 (2.3507)  acc1: 50.0000 (44.4444)  acc5: 83.3333 (75.0212)  time: 0.2853  data: 0.0864  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.8148 (2.3433)  acc1: 55.5556 (44.7991)  acc5: 83.3333 (75.0591)  time: 0.3364  data: 0.1350  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.4128 (2.3472)  acc1: 38.8889 (44.2973)  acc5: 77.7778 (75.1656)  time: 0.3144  data: 0.1165  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.3976 (2.3418)  acc1: 44.4444 (44.6170)  acc5: 77.7778 (75.3278)  time: 0.3000  data: 0.1072  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.3294 (2.3584)  acc1: 38.8889 (44.0871)  acc5: 72.2222 (74.9838)  time: 0.3209  data: 0.1313  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.3184 (2.3460)  acc1: 38.8889 (44.1068)  acc5: 77.7778 (75.4758)  time: 0.3364  data: 0.1437  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4419 (2.4050)  acc1: 33.3333 (42.7283)  acc5: 77.7778 (73.9093)  time: 0.3427  data: 0.1507  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.6759 (2.4141)  acc1: 33.3333 (42.4268)  acc5: 61.1111 (73.6871)  time: 0.3042  data: 0.1107  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.2084 (2.4166)  acc1: 38.8889 (42.5750)  acc5: 72.2222 (73.5124)  time: 0.2597  data: 0.0747  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.4471 (2.4051)  acc1: 44.4444 (42.8356)  acc5: 77.7778 (73.7808)  time: 0.2823  data: 0.1075  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7984 (2.3734)  acc1: 66.6667 (43.9394)  acc5: 77.7778 (74.1462)  time: 0.3685  data: 0.1917  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6452 (2.3559)  acc1: 66.6667 (44.2370)  acc5: 83.3333 (74.5044)  time: 0.3540  data: 0.1667  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2696 (2.3671)  acc1: 44.4444 (43.7140)  acc5: 77.7778 (74.5684)  time: 0.3017  data: 0.1142  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1885 (2.3060)  acc1: 77.7778 (45.4449)  acc5: 88.8889 (75.3512)  time: 0.2914  data: 0.1047  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4306 (2.3013)  acc1: 66.6667 (45.3875)  acc5: 88.8889 (75.5843)  time: 0.2042  data: 0.0406  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4306 (2.3058)  acc1: 66.6667 (45.3615)  acc5: 88.8889 (75.5478)  time: 0.1976  data: 0.0406  max mem: 15572
Val: Total time: 0:01:26 (0.3197 s / it)
* Acc@1 45.361 Acc@5 75.548 loss 2.306
Accuracy of the network on the 4883 val videos: 45.4%
Max accuracy: 45.42%
Epoch: [26]  [   0/2809]  eta: 4:15:54  lr: 0.000016  min_lr: 0.000000  loss: 4.0048 (4.0048)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 5.4662  data: 5.0747  max mem: 15572
Epoch: [26]  [  10/2809]  eta: 0:39:26  lr: 0.000016  min_lr: 0.000000  loss: 4.0048 (3.9071)  class_acc: 0.2500 (0.2576)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8455  data: 0.4616  max mem: 15572
Epoch: [26]  [  20/2809]  eta: 0:31:08  lr: 0.000016  min_lr: 0.000000  loss: 3.7824 (3.7754)  class_acc: 0.2917 (0.2817)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4302  data: 0.0005  max mem: 15572
Epoch: [26]  [  30/2809]  eta: 0:28:00  lr: 0.000016  min_lr: 0.000000  loss: 3.6321 (3.7992)  class_acc: 0.2917 (0.2984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4724  data: 0.0008  max mem: 15572
Epoch: [26]  [  40/2809]  eta: 0:29:13  lr: 0.000016  min_lr: 0.000000  loss: 3.7714 (3.8073)  class_acc: 0.3333 (0.3181)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1257  max mem: 15572
Epoch: [26]  [  50/2809]  eta: 0:29:10  lr: 0.000016  min_lr: 0.000000  loss: 3.8245 (3.8062)  class_acc: 0.3333 (0.3129)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6809  data: 0.2201  max mem: 15572
[2025-01-16 02:45:25,053] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:45:25,054] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [  60/2809]  eta: 0:28:29  lr: 0.000016  min_lr: 0.000000  loss: 3.8579 (3.8138)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1457  max mem: 15572
Epoch: [26]  [  70/2809]  eta: 0:29:16  lr: 0.000016  min_lr: 0.000000  loss: 3.7722 (3.8019)  class_acc: 0.3750 (0.3204)  loss_scale: 65536.0000 (40152.3380)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.1991  max mem: 15572
Epoch: [26]  [  80/2809]  eta: 0:29:02  lr: 0.000016  min_lr: 0.000000  loss: 3.7363 (3.8092)  class_acc: 0.3333 (0.3164)  loss_scale: 65536.0000 (43286.1235)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.2315  max mem: 15572
Epoch: [26]  [  90/2809]  eta: 0:29:23  lr: 0.000016  min_lr: 0.000000  loss: 3.8839 (3.8127)  class_acc: 0.2500 (0.3164)  loss_scale: 65536.0000 (45731.1648)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.2261  max mem: 15572
Epoch: [26]  [ 100/2809]  eta: 0:29:03  lr: 0.000016  min_lr: 0.000000  loss: 3.7831 (3.8019)  class_acc: 0.3333 (0.3185)  loss_scale: 65536.0000 (47692.0396)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.2076  max mem: 15572
Epoch: [26]  [ 110/2809]  eta: 0:29:17  lr: 0.000016  min_lr: 0.000000  loss: 3.7796 (3.8109)  class_acc: 0.2500 (0.3127)  loss_scale: 65536.0000 (49299.6036)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.1896  max mem: 15572
Epoch: [26]  [ 120/2809]  eta: 0:29:22  lr: 0.000016  min_lr: 0.000000  loss: 3.7998 (3.8085)  class_acc: 0.2500 (0.3154)  loss_scale: 65536.0000 (50641.4545)  weight_decay: 0.0500 (0.0500)  time: 0.7150  data: 0.2330  max mem: 15572
Epoch: [26]  [ 130/2809]  eta: 0:29:30  lr: 0.000016  min_lr: 0.000000  loss: 3.9523 (3.8155)  class_acc: 0.2917 (0.3142)  loss_scale: 65536.0000 (51778.4427)  weight_decay: 0.0500 (0.0500)  time: 0.7160  data: 0.2444  max mem: 15572
Epoch: [26]  [ 140/2809]  eta: 0:29:25  lr: 0.000016  min_lr: 0.000000  loss: 3.8383 (3.8039)  class_acc: 0.2917 (0.3171)  loss_scale: 65536.0000 (52754.1560)  weight_decay: 0.0500 (0.0500)  time: 0.6990  data: 0.2548  max mem: 15572
Epoch: [26]  [ 150/2809]  eta: 0:28:34  lr: 0.000016  min_lr: 0.000000  loss: 3.7055 (3.8057)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (53600.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5400  data: 0.1191  max mem: 15572
Epoch: [26]  [ 160/2809]  eta: 0:27:53  lr: 0.000016  min_lr: 0.000000  loss: 3.8145 (3.8110)  class_acc: 0.2917 (0.3126)  loss_scale: 65536.0000 (54341.9627)  weight_decay: 0.0500 (0.0500)  time: 0.4209  data: 0.0077  max mem: 15572
Epoch: [26]  [ 170/2809]  eta: 0:27:32  lr: 0.000016  min_lr: 0.000000  loss: 3.9568 (3.8192)  class_acc: 0.2500 (0.3107)  loss_scale: 65536.0000 (54996.5848)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0643  max mem: 15572
Epoch: [26]  [ 180/2809]  eta: 0:27:00  lr: 0.000016  min_lr: 0.000000  loss: 3.8746 (3.8161)  class_acc: 0.2500 (0.3108)  loss_scale: 65536.0000 (55578.8729)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0708  max mem: 15572
[2025-01-16 02:46:43,200] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:46:43,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:46:47,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73224
[2025-01-16 02:46:47,552] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:46:47,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 190/2809]  eta: 0:26:46  lr: 0.000016  min_lr: 0.000000  loss: 3.7249 (3.8069)  class_acc: 0.3333 (0.3146)  loss_scale: 65536.0000 (58502.0314)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0850  max mem: 15572
Epoch: [26]  [ 200/2809]  eta: 0:26:43  lr: 0.000016  min_lr: 0.000000  loss: 3.7107 (3.8060)  class_acc: 0.2917 (0.3132)  loss_scale: 65536.0000 (58851.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1786  max mem: 15572
Epoch: [26]  [ 210/2809]  eta: 0:26:25  lr: 0.000016  min_lr: 0.000000  loss: 3.7219 (3.7973)  class_acc: 0.2917 (0.3146)  loss_scale: 65536.0000 (59168.7583)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1445  max mem: 15572
Epoch: [26]  [ 220/2809]  eta: 0:26:15  lr: 0.000016  min_lr: 0.000000  loss: 3.8000 (3.7966)  class_acc: 0.2917 (0.3145)  loss_scale: 65536.0000 (59456.8688)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.1173  max mem: 15572
[2025-01-16 02:47:07,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73259
[2025-01-16 02:47:07,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:47:07,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 230/2809]  eta: 0:26:06  lr: 0.000016  min_lr: 0.000000  loss: 3.7669 (3.7962)  class_acc: 0.3333 (0.3167)  loss_scale: 65536.0000 (58868.9177)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1644  max mem: 15572
Epoch: [26]  [ 240/2809]  eta: 0:26:05  lr: 0.000016  min_lr: 0.000000  loss: 3.8595 (3.8032)  class_acc: 0.3333 (0.3166)  loss_scale: 32768.0000 (57785.8921)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1919  max mem: 15572
Epoch: [26]  [ 250/2809]  eta: 0:25:46  lr: 0.000016  min_lr: 0.000000  loss: 3.8198 (3.7986)  class_acc: 0.2917 (0.3167)  loss_scale: 32768.0000 (56789.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1367  max mem: 15572
Epoch: [26]  [ 260/2809]  eta: 0:25:45  lr: 0.000016  min_lr: 0.000000  loss: 3.7966 (3.7971)  class_acc: 0.3333 (0.3180)  loss_scale: 32768.0000 (55868.8123)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1378  max mem: 15572
Epoch: [26]  [ 270/2809]  eta: 0:25:34  lr: 0.000016  min_lr: 0.000000  loss: 3.8782 (3.8015)  class_acc: 0.3333 (0.3167)  loss_scale: 32768.0000 (55016.3838)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1761  max mem: 15572
Epoch: [26]  [ 280/2809]  eta: 0:25:31  lr: 0.000016  min_lr: 0.000000  loss: 3.7185 (3.7992)  class_acc: 0.3333 (0.3190)  loss_scale: 32768.0000 (54224.6263)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1666  max mem: 15572
Epoch: [26]  [ 290/2809]  eta: 0:25:18  lr: 0.000016  min_lr: 0.000000  loss: 3.7095 (3.8008)  class_acc: 0.3333 (0.3189)  loss_scale: 32768.0000 (53487.2852)  weight_decay: 0.0500 (0.0500)  time: 0.5793  data: 0.1264  max mem: 15572
Epoch: [26]  [ 300/2809]  eta: 0:25:06  lr: 0.000016  min_lr: 0.000000  loss: 3.7494 (3.7971)  class_acc: 0.3333 (0.3203)  loss_scale: 32768.0000 (52798.9369)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0740  max mem: 15572
Epoch: [26]  [ 310/2809]  eta: 0:24:56  lr: 0.000016  min_lr: 0.000000  loss: 3.8232 (3.7961)  class_acc: 0.3333 (0.3209)  loss_scale: 32768.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0983  max mem: 15572
Epoch: [26]  [ 320/2809]  eta: 0:24:45  lr: 0.000016  min_lr: 0.000000  loss: 3.8806 (3.7967)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (51550.9034)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.0976  max mem: 15572
Epoch: [26]  [ 330/2809]  eta: 0:24:38  lr: 0.000016  min_lr: 0.000000  loss: 3.8751 (3.7961)  class_acc: 0.3333 (0.3231)  loss_scale: 32768.0000 (50983.4441)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1172  max mem: 15572
Epoch: [26]  [ 340/2809]  eta: 0:24:35  lr: 0.000016  min_lr: 0.000000  loss: 3.8520 (3.7959)  class_acc: 0.2917 (0.3221)  loss_scale: 32768.0000 (50449.2669)  weight_decay: 0.0500 (0.0500)  time: 0.6144  data: 0.1669  max mem: 15572
Epoch: [26]  [ 350/2809]  eta: 0:24:29  lr: 0.000016  min_lr: 0.000000  loss: 3.8514 (3.7890)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (49945.5271)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1756  max mem: 15572
[2025-01-16 02:48:22,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:48:22,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 360/2809]  eta: 0:24:22  lr: 0.000016  min_lr: 0.000000  loss: 3.8746 (3.7906)  class_acc: 0.2917 (0.3236)  loss_scale: 32768.0000 (50105.0859)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1489  max mem: 15572
Epoch: [26]  [ 370/2809]  eta: 0:24:21  lr: 0.000016  min_lr: 0.000000  loss: 3.6786 (3.7864)  class_acc: 0.3750 (0.3251)  loss_scale: 65536.0000 (50521.0135)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.1950  max mem: 15572
Epoch: [26]  [ 380/2809]  eta: 0:24:08  lr: 0.000016  min_lr: 0.000000  loss: 3.6000 (3.7856)  class_acc: 0.3750 (0.3255)  loss_scale: 65536.0000 (50915.1076)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1539  max mem: 15572
Epoch: [26]  [ 390/2809]  eta: 0:24:01  lr: 0.000016  min_lr: 0.000000  loss: 3.7789 (3.7836)  class_acc: 0.2917 (0.3266)  loss_scale: 65536.0000 (51289.0435)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0927  max mem: 15572
Epoch: [26]  [ 400/2809]  eta: 0:23:51  lr: 0.000016  min_lr: 0.000000  loss: 3.8853 (3.7883)  class_acc: 0.3333 (0.3260)  loss_scale: 65536.0000 (51644.3292)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1160  max mem: 15572
Epoch: [26]  [ 410/2809]  eta: 0:23:44  lr: 0.000016  min_lr: 0.000000  loss: 3.8248 (3.7840)  class_acc: 0.3333 (0.3265)  loss_scale: 65536.0000 (51982.3260)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1047  max mem: 15572
Epoch: [26]  [ 420/2809]  eta: 0:23:34  lr: 0.000016  min_lr: 0.000000  loss: 3.6754 (3.7830)  class_acc: 0.3333 (0.3266)  loss_scale: 65536.0000 (52304.2660)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0978  max mem: 15572
[2025-01-16 02:49:04,124] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73461
[2025-01-16 02:49:04,125] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:49:04,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 430/2809]  eta: 0:23:27  lr: 0.000016  min_lr: 0.000000  loss: 3.6754 (3.7831)  class_acc: 0.2500 (0.3259)  loss_scale: 65536.0000 (52307.1555)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1169  max mem: 15572
Epoch: [26]  [ 440/2809]  eta: 0:23:19  lr: 0.000016  min_lr: 0.000000  loss: 3.6792 (3.7801)  class_acc: 0.2917 (0.3272)  loss_scale: 32768.0000 (51864.0907)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1333  max mem: 15572
Epoch: [26]  [ 450/2809]  eta: 0:23:10  lr: 0.000016  min_lr: 0.000000  loss: 3.8505 (3.7822)  class_acc: 0.3333 (0.3266)  loss_scale: 32768.0000 (51440.6741)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1145  max mem: 15572
Epoch: [26]  [ 460/2809]  eta: 0:23:05  lr: 0.000016  min_lr: 0.000000  loss: 3.8728 (3.7803)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (51035.6269)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1387  max mem: 15572
Epoch: [26]  [ 470/2809]  eta: 0:22:56  lr: 0.000016  min_lr: 0.000000  loss: 3.8728 (3.7812)  class_acc: 0.2917 (0.3270)  loss_scale: 32768.0000 (50647.7792)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1165  max mem: 15572
Epoch: [26]  [ 480/2809]  eta: 0:22:49  lr: 0.000016  min_lr: 0.000000  loss: 3.9701 (3.7822)  class_acc: 0.2917 (0.3264)  loss_scale: 32768.0000 (50276.0582)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0929  max mem: 15572
Epoch: [26]  [ 490/2809]  eta: 0:22:41  lr: 0.000016  min_lr: 0.000000  loss: 3.8725 (3.7821)  class_acc: 0.2917 (0.3269)  loss_scale: 32768.0000 (49919.4786)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1057  max mem: 15572
Epoch: [26]  [ 500/2809]  eta: 0:22:32  lr: 0.000016  min_lr: 0.000000  loss: 3.8002 (3.7828)  class_acc: 0.2917 (0.3267)  loss_scale: 32768.0000 (49577.1337)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0866  max mem: 15572
Epoch: [26]  [ 510/2809]  eta: 0:22:25  lr: 0.000016  min_lr: 0.000000  loss: 3.7838 (3.7807)  class_acc: 0.2917 (0.3268)  loss_scale: 32768.0000 (49248.1879)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0985  max mem: 15572
Epoch: [26]  [ 520/2809]  eta: 0:22:20  lr: 0.000016  min_lr: 0.000000  loss: 3.9613 (3.7829)  class_acc: 0.2917 (0.3265)  loss_scale: 32768.0000 (48931.8695)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1544  max mem: 15572
Epoch: [26]  [ 530/2809]  eta: 0:22:13  lr: 0.000016  min_lr: 0.000000  loss: 3.9630 (3.7816)  class_acc: 0.2917 (0.3266)  loss_scale: 32768.0000 (48627.4652)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1431  max mem: 15572
Epoch: [26]  [ 540/2809]  eta: 0:22:09  lr: 0.000016  min_lr: 0.000000  loss: 3.6804 (3.7814)  class_acc: 0.3333 (0.3275)  loss_scale: 32768.0000 (48334.3142)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1325  max mem: 15572
Epoch: [26]  [ 550/2809]  eta: 0:22:03  lr: 0.000016  min_lr: 0.000000  loss: 3.7858 (3.7808)  class_acc: 0.3333 (0.3271)  loss_scale: 32768.0000 (48051.8040)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1566  max mem: 15572
[2025-01-16 02:50:16,901] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:50:16,902] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 560/2809]  eta: 0:21:56  lr: 0.000016  min_lr: 0.000000  loss: 3.8408 (3.7794)  class_acc: 0.3333 (0.3273)  loss_scale: 32768.0000 (48071.4153)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1243  max mem: 15572
Epoch: [26]  [ 570/2809]  eta: 0:21:48  lr: 0.000016  min_lr: 0.000000  loss: 3.8889 (3.7801)  class_acc: 0.3333 (0.3269)  loss_scale: 65536.0000 (48377.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1210  max mem: 15572
Epoch: [26]  [ 580/2809]  eta: 0:21:43  lr: 0.000016  min_lr: 0.000000  loss: 3.8577 (3.7791)  class_acc: 0.2917 (0.3270)  loss_scale: 65536.0000 (48672.6059)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1354  max mem: 15572
Epoch: [26]  [ 590/2809]  eta: 0:21:34  lr: 0.000016  min_lr: 0.000000  loss: 3.8577 (3.7811)  class_acc: 0.2917 (0.3268)  loss_scale: 65536.0000 (48957.9425)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1160  max mem: 15572
Epoch: [26]  [ 600/2809]  eta: 0:21:30  lr: 0.000016  min_lr: 0.000000  loss: 3.9388 (3.7819)  class_acc: 0.3333 (0.3272)  loss_scale: 65536.0000 (49233.7837)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1487  max mem: 15572
Epoch: [26]  [ 610/2809]  eta: 0:21:24  lr: 0.000016  min_lr: 0.000000  loss: 3.6815 (3.7790)  class_acc: 0.3750 (0.3278)  loss_scale: 65536.0000 (49500.5957)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1622  max mem: 15572
[2025-01-16 02:50:52,705] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73654
[2025-01-16 02:50:52,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:50:52,705] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 620/2809]  eta: 0:21:16  lr: 0.000016  min_lr: 0.000000  loss: 3.6815 (3.7802)  class_acc: 0.3750 (0.3280)  loss_scale: 65536.0000 (49706.0483)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0846  max mem: 15572
Epoch: [26]  [ 630/2809]  eta: 0:21:10  lr: 0.000016  min_lr: 0.000000  loss: 3.3932 (3.7732)  class_acc: 0.4167 (0.3299)  loss_scale: 32768.0000 (49437.6165)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1009  max mem: 15572
Epoch: [26]  [ 640/2809]  eta: 0:21:05  lr: 0.000016  min_lr: 0.000000  loss: 3.6035 (3.7749)  class_acc: 0.4167 (0.3300)  loss_scale: 32768.0000 (49177.5601)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1430  max mem: 15572
Epoch: [26]  [ 650/2809]  eta: 0:20:57  lr: 0.000016  min_lr: 0.000000  loss: 3.8312 (3.7724)  class_acc: 0.2917 (0.3299)  loss_scale: 32768.0000 (48925.4931)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.0953  max mem: 15572
Epoch: [26]  [ 660/2809]  eta: 0:20:50  lr: 0.000016  min_lr: 0.000000  loss: 3.8313 (3.7737)  class_acc: 0.2917 (0.3295)  loss_scale: 32768.0000 (48681.0530)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0766  max mem: 15572
Epoch: [26]  [ 670/2809]  eta: 0:20:45  lr: 0.000016  min_lr: 0.000000  loss: 3.7259 (3.7714)  class_acc: 0.3333 (0.3298)  loss_scale: 32768.0000 (48443.8987)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1231  max mem: 15572
Epoch: [26]  [ 680/2809]  eta: 0:20:38  lr: 0.000016  min_lr: 0.000000  loss: 3.6795 (3.7695)  class_acc: 0.3333 (0.3298)  loss_scale: 32768.0000 (48213.7093)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1275  max mem: 15572
Epoch: [26]  [ 690/2809]  eta: 0:20:32  lr: 0.000016  min_lr: 0.000000  loss: 3.6359 (3.7664)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (47990.1823)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1220  max mem: 15572
Epoch: [26]  [ 700/2809]  eta: 0:20:24  lr: 0.000016  min_lr: 0.000000  loss: 3.7684 (3.7673)  class_acc: 0.3333 (0.3297)  loss_scale: 32768.0000 (47773.0328)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.1146  max mem: 15572
Epoch: [26]  [ 710/2809]  eta: 0:20:17  lr: 0.000016  min_lr: 0.000000  loss: 3.9825 (3.7671)  class_acc: 0.2917 (0.3296)  loss_scale: 32768.0000 (47561.9916)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0929  max mem: 15572
Epoch: [26]  [ 720/2809]  eta: 0:20:14  lr: 0.000016  min_lr: 0.000000  loss: 3.7334 (3.7666)  class_acc: 0.3333 (0.3300)  loss_scale: 32768.0000 (47356.8044)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1591  max mem: 15572
Epoch: [26]  [ 730/2809]  eta: 0:20:06  lr: 0.000016  min_lr: 0.000000  loss: 3.9346 (3.7678)  class_acc: 0.3333 (0.3296)  loss_scale: 32768.0000 (47157.2312)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1477  max mem: 15572
Epoch: [26]  [ 740/2809]  eta: 0:19:59  lr: 0.000016  min_lr: 0.000000  loss: 3.7443 (3.7671)  class_acc: 0.3333 (0.3298)  loss_scale: 32768.0000 (46963.0445)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0788  max mem: 15572
[2025-01-16 02:52:04,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:52:04,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 750/2809]  eta: 0:19:50  lr: 0.000016  min_lr: 0.000000  loss: 3.6718 (3.7653)  class_acc: 0.4167 (0.3307)  loss_scale: 32768.0000 (46861.2943)  weight_decay: 0.0500 (0.0500)  time: 0.4927  data: 0.0651  max mem: 15572
Epoch: [26]  [ 760/2809]  eta: 0:19:43  lr: 0.000016  min_lr: 0.000000  loss: 3.8133 (3.7665)  class_acc: 0.2917 (0.3299)  loss_scale: 65536.0000 (47106.6912)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0626  max mem: 15572
Epoch: [26]  [ 770/2809]  eta: 0:19:39  lr: 0.000016  min_lr: 0.000000  loss: 3.9571 (3.7681)  class_acc: 0.2500 (0.3297)  loss_scale: 65536.0000 (47345.7224)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1388  max mem: 15572
Epoch: [26]  [ 780/2809]  eta: 0:19:31  lr: 0.000016  min_lr: 0.000000  loss: 3.5681 (3.7651)  class_acc: 0.3750 (0.3305)  loss_scale: 65536.0000 (47578.6325)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1374  max mem: 15572
Epoch: [26]  [ 790/2809]  eta: 0:19:25  lr: 0.000016  min_lr: 0.000000  loss: 3.6862 (3.7668)  class_acc: 0.3750 (0.3305)  loss_scale: 65536.0000 (47805.6536)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.1045  max mem: 15572
Epoch: [26]  [ 800/2809]  eta: 0:19:21  lr: 0.000016  min_lr: 0.000000  loss: 3.7640 (3.7658)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (48027.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1597  max mem: 15572
Epoch: [26]  [ 810/2809]  eta: 0:19:15  lr: 0.000016  min_lr: 0.000000  loss: 3.8315 (3.7675)  class_acc: 0.2500 (0.3300)  loss_scale: 65536.0000 (48242.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1669  max mem: 15572
Epoch: [26]  [ 820/2809]  eta: 0:19:08  lr: 0.000016  min_lr: 0.000000  loss: 3.8146 (3.7658)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (48453.5347)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0900  max mem: 15572
Epoch: [26]  [ 830/2809]  eta: 0:19:00  lr: 0.000016  min_lr: 0.000000  loss: 3.8146 (3.7668)  class_acc: 0.3750 (0.3304)  loss_scale: 65536.0000 (48659.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0287  max mem: 15572
Epoch: [26]  [ 840/2809]  eta: 0:18:54  lr: 0.000016  min_lr: 0.000000  loss: 3.8407 (3.7656)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (48859.7765)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0767  max mem: 15572
Epoch: [26]  [ 850/2809]  eta: 0:18:48  lr: 0.000016  min_lr: 0.000000  loss: 3.4829 (3.7620)  class_acc: 0.3750 (0.3313)  loss_scale: 65536.0000 (49055.7368)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1168  max mem: 15572
Epoch: [26]  [ 860/2809]  eta: 0:18:40  lr: 0.000016  min_lr: 0.000000  loss: 3.6797 (3.7623)  class_acc: 0.3750 (0.3315)  loss_scale: 65536.0000 (49247.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5188  data: 0.0742  max mem: 15572
Epoch: [26]  [ 870/2809]  eta: 0:18:35  lr: 0.000016  min_lr: 0.000000  loss: 3.8780 (3.7626)  class_acc: 0.3333 (0.3319)  loss_scale: 65536.0000 (49434.1584)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1231  max mem: 15572
[2025-01-16 02:53:14,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:53:14,831] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:53:15,256] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 73912
[2025-01-16 02:53:15,256] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:53:15,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 880/2809]  eta: 0:18:31  lr: 0.000016  min_lr: 0.000000  loss: 3.9990 (3.7645)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (49691.3144)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.2061  max mem: 15572
Epoch: [26]  [ 890/2809]  eta: 0:18:26  lr: 0.000016  min_lr: 0.000000  loss: 3.8391 (3.7642)  class_acc: 0.3333 (0.3316)  loss_scale: 65536.0000 (49869.1448)  weight_decay: 0.0500 (0.0500)  time: 0.6198  data: 0.1739  max mem: 15572
Epoch: [26]  [ 900/2809]  eta: 0:18:22  lr: 0.000016  min_lr: 0.000000  loss: 3.5256 (3.7617)  class_acc: 0.3750 (0.3320)  loss_scale: 65536.0000 (50043.0277)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1850  max mem: 15572
Epoch: [26]  [ 910/2809]  eta: 0:18:15  lr: 0.000016  min_lr: 0.000000  loss: 3.5818 (3.7634)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (50213.0933)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1661  max mem: 15572
Epoch: [26]  [ 920/2809]  eta: 0:18:08  lr: 0.000016  min_lr: 0.000000  loss: 3.8812 (3.7637)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (50379.4658)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.1045  max mem: 15572
Epoch: [26]  [ 930/2809]  eta: 0:18:04  lr: 0.000016  min_lr: 0.000000  loss: 3.9035 (3.7648)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (50542.2642)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1589  max mem: 15572
Epoch: [26]  [ 940/2809]  eta: 0:17:58  lr: 0.000016  min_lr: 0.000000  loss: 3.8267 (3.7651)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (50701.6026)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1672  max mem: 15572
Epoch: [26]  [ 950/2809]  eta: 0:17:51  lr: 0.000016  min_lr: 0.000000  loss: 3.7338 (3.7636)  class_acc: 0.3333 (0.3315)  loss_scale: 65536.0000 (50857.5899)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1093  max mem: 15572
Epoch: [26]  [ 960/2809]  eta: 0:17:44  lr: 0.000016  min_lr: 0.000000  loss: 3.8357 (3.7656)  class_acc: 0.2500 (0.3309)  loss_scale: 65536.0000 (51010.3309)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0911  max mem: 15572
[2025-01-16 02:54:06,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=74000, skipped=494, lr=[1.505895959851408e-07, 1.505895959851408e-07, 2.151279942644869e-07, 2.151279942644869e-07, 3.073257060921242e-07, 3.073257060921242e-07, 4.390367229887489e-07, 4.390367229887489e-07, 6.271953185553555e-07, 6.271953185553555e-07, 8.959933122219366e-07, 8.959933122219366e-07, 1.279990446031338e-06, 1.279990446031338e-06, 1.8285577800447688e-06, 1.8285577800447688e-06, 2.612225400063955e-06, 2.612225400063955e-06, 3.7317505715199367e-06, 3.7317505715199367e-06, 5.331072245028481e-06, 5.331072245028481e-06, 7.615817492897831e-06, 7.615817492897831e-06, 1.087973927556833e-05, 1.087973927556833e-05, 1.554248467938333e-05, 1.554248467938333e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 02:54:06,236] [INFO] [timer.py:260:stop] epoch=0/micro_step=74000/global_step=74000, RunningAvgSamplesPerSec=28.55392314379974, CurrSamplesPerSec=27.734878015717936, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [ 970/2809]  eta: 0:17:39  lr: 0.000016  min_lr: 0.000000  loss: 3.8737 (3.7655)  class_acc: 0.2500 (0.3312)  loss_scale: 65536.0000 (51159.9258)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1410  max mem: 15572
Epoch: [26]  [ 980/2809]  eta: 0:17:33  lr: 0.000016  min_lr: 0.000000  loss: 3.8586 (3.7660)  class_acc: 0.2917 (0.3309)  loss_scale: 65536.0000 (51306.4709)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1584  max mem: 15572
Epoch: [26]  [ 990/2809]  eta: 0:17:28  lr: 0.000016  min_lr: 0.000000  loss: 3.9136 (3.7682)  class_acc: 0.2917 (0.3305)  loss_scale: 65536.0000 (51450.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1234  max mem: 15572
Epoch: [26]  [1000/2809]  eta: 0:17:22  lr: 0.000016  min_lr: 0.000000  loss: 3.8508 (3.7678)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (51590.7772)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1185  max mem: 15572
[2025-01-16 02:54:30,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:54:30,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:54:33,457] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74044
[2025-01-16 02:54:33,457] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:54:33,457] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1010/2809]  eta: 0:17:16  lr: 0.000016  min_lr: 0.000000  loss: 3.7613 (3.7672)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (51923.1810)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1477  max mem: 15572
Epoch: [26]  [1020/2809]  eta: 0:17:10  lr: 0.000016  min_lr: 0.000000  loss: 3.7154 (3.7670)  class_acc: 0.2917 (0.3300)  loss_scale: 65536.0000 (52056.5093)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1223  max mem: 15572
Epoch: [26]  [1030/2809]  eta: 0:17:04  lr: 0.000015  min_lr: 0.000000  loss: 3.6951 (3.7674)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (52187.2512)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1031  max mem: 15572
Epoch: [26]  [1040/2809]  eta: 0:16:58  lr: 0.000015  min_lr: 0.000000  loss: 3.6794 (3.7678)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (52315.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1455  max mem: 15572
Epoch: [26]  [1050/2809]  eta: 0:16:51  lr: 0.000015  min_lr: 0.000000  loss: 3.7230 (3.7679)  class_acc: 0.3750 (0.3301)  loss_scale: 65536.0000 (52441.2712)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0859  max mem: 15572
Epoch: [26]  [1060/2809]  eta: 0:16:43  lr: 0.000015  min_lr: 0.000000  loss: 3.8265 (3.7700)  class_acc: 0.2500 (0.3293)  loss_scale: 65536.0000 (52564.6899)  weight_decay: 0.0500 (0.0500)  time: 0.4573  data: 0.0180  max mem: 15572
Epoch: [26]  [1070/2809]  eta: 0:16:37  lr: 0.000015  min_lr: 0.000000  loss: 3.9686 (3.7706)  class_acc: 0.2083 (0.3288)  loss_scale: 65536.0000 (52685.8039)  weight_decay: 0.0500 (0.0500)  time: 0.4959  data: 0.0602  max mem: 15572
Epoch: [26]  [1080/2809]  eta: 0:16:31  lr: 0.000015  min_lr: 0.000000  loss: 3.8879 (3.7727)  class_acc: 0.2083 (0.3279)  loss_scale: 65536.0000 (52804.6772)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1424  max mem: 15572
Epoch: [26]  [1090/2809]  eta: 0:16:25  lr: 0.000015  min_lr: 0.000000  loss: 3.9856 (3.7744)  class_acc: 0.2500 (0.3275)  loss_scale: 65536.0000 (52921.3712)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1324  max mem: 15572
Epoch: [26]  [1100/2809]  eta: 0:16:17  lr: 0.000015  min_lr: 0.000000  loss: 3.9671 (3.7746)  class_acc: 0.2500 (0.3277)  loss_scale: 65536.0000 (53035.9455)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0443  max mem: 15572
Epoch: [26]  [1110/2809]  eta: 0:16:11  lr: 0.000015  min_lr: 0.000000  loss: 3.8506 (3.7754)  class_acc: 0.3333 (0.3274)  loss_scale: 65536.0000 (53148.4572)  weight_decay: 0.0500 (0.0500)  time: 0.4853  data: 0.0351  max mem: 15572
Epoch: [26]  [1120/2809]  eta: 0:16:05  lr: 0.000015  min_lr: 0.000000  loss: 3.8631 (3.7757)  class_acc: 0.2917 (0.3272)  loss_scale: 65536.0000 (53258.9616)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1037  max mem: 15572
Epoch: [26]  [1130/2809]  eta: 0:16:00  lr: 0.000015  min_lr: 0.000000  loss: 3.9613 (3.7763)  class_acc: 0.3333 (0.3272)  loss_scale: 65536.0000 (53367.5119)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1569  max mem: 15572
[2025-01-16 02:55:43,644] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:55:43,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 02:55:44,125] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74174
[2025-01-16 02:55:44,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:55:44,126] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1140/2809]  eta: 0:15:55  lr: 0.000015  min_lr: 0.000000  loss: 3.6073 (3.7743)  class_acc: 0.3750 (0.3275)  loss_scale: 65536.0000 (53531.5968)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1888  max mem: 15572
Epoch: [26]  [1150/2809]  eta: 0:15:49  lr: 0.000015  min_lr: 0.000000  loss: 3.6738 (3.7759)  class_acc: 0.3750 (0.3274)  loss_scale: 65536.0000 (53635.8923)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1528  max mem: 15572
Epoch: [26]  [1160/2809]  eta: 0:15:43  lr: 0.000015  min_lr: 0.000000  loss: 3.8218 (3.7755)  class_acc: 0.3333 (0.3274)  loss_scale: 65536.0000 (53738.3910)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1046  max mem: 15572
Epoch: [26]  [1170/2809]  eta: 0:15:37  lr: 0.000015  min_lr: 0.000000  loss: 3.8218 (3.7763)  class_acc: 0.3333 (0.3272)  loss_scale: 65536.0000 (53839.1392)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0992  max mem: 15572
Epoch: [26]  [1180/2809]  eta: 0:15:32  lr: 0.000015  min_lr: 0.000000  loss: 3.8089 (3.7751)  class_acc: 0.2917 (0.3274)  loss_scale: 65536.0000 (53938.1812)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1094  max mem: 15572
Epoch: [26]  [1190/2809]  eta: 0:15:25  lr: 0.000015  min_lr: 0.000000  loss: 3.7663 (3.7738)  class_acc: 0.2917 (0.3272)  loss_scale: 65536.0000 (54035.5600)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1053  max mem: 15572
Epoch: [26]  [1200/2809]  eta: 0:15:19  lr: 0.000015  min_lr: 0.000000  loss: 3.7261 (3.7723)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (54131.3172)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0949  max mem: 15572
[2025-01-16 02:56:22,907] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74244
[2025-01-16 02:56:22,908] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 02:56:22,908] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1210/2809]  eta: 0:15:13  lr: 0.000015  min_lr: 0.000000  loss: 3.5998 (3.7695)  class_acc: 0.4167 (0.3283)  loss_scale: 65536.0000 (54198.4344)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1149  max mem: 15572
Epoch: [26]  [1220/2809]  eta: 0:15:08  lr: 0.000015  min_lr: 0.000000  loss: 3.6642 (3.7698)  class_acc: 0.4167 (0.3284)  loss_scale: 32768.0000 (54022.9189)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1216  max mem: 15572
Epoch: [26]  [1230/2809]  eta: 0:15:03  lr: 0.000015  min_lr: 0.000000  loss: 3.8562 (3.7708)  class_acc: 0.2917 (0.3281)  loss_scale: 32768.0000 (53850.2551)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1437  max mem: 15572
Epoch: [26]  [1240/2809]  eta: 0:14:56  lr: 0.000015  min_lr: 0.000000  loss: 3.9584 (3.7727)  class_acc: 0.2500 (0.3277)  loss_scale: 32768.0000 (53680.3739)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1144  max mem: 15572
Epoch: [26]  [1250/2809]  eta: 0:14:50  lr: 0.000015  min_lr: 0.000000  loss: 3.9929 (3.7737)  class_acc: 0.2500 (0.3274)  loss_scale: 32768.0000 (53513.2086)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.1109  max mem: 15572
Epoch: [26]  [1260/2809]  eta: 0:14:45  lr: 0.000015  min_lr: 0.000000  loss: 4.0616 (3.7760)  class_acc: 0.2500 (0.3267)  loss_scale: 32768.0000 (53348.6947)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1357  max mem: 15572
Epoch: [26]  [1270/2809]  eta: 0:14:38  lr: 0.000015  min_lr: 0.000000  loss: 3.9911 (3.7760)  class_acc: 0.2500 (0.3267)  loss_scale: 32768.0000 (53186.7695)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.0973  max mem: 15572
Epoch: [26]  [1280/2809]  eta: 0:14:32  lr: 0.000015  min_lr: 0.000000  loss: 3.9911 (3.7786)  class_acc: 0.2500 (0.3261)  loss_scale: 32768.0000 (53027.3724)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0867  max mem: 15572
Epoch: [26]  [1290/2809]  eta: 0:14:26  lr: 0.000015  min_lr: 0.000000  loss: 4.0770 (3.7806)  class_acc: 0.2083 (0.3255)  loss_scale: 32768.0000 (52870.4446)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0992  max mem: 15572
Epoch: [26]  [1300/2809]  eta: 0:14:21  lr: 0.000015  min_lr: 0.000000  loss: 4.0441 (3.7824)  class_acc: 0.2500 (0.3248)  loss_scale: 32768.0000 (52715.9293)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1759  max mem: 15572
Epoch: [26]  [1310/2809]  eta: 0:14:16  lr: 0.000015  min_lr: 0.000000  loss: 3.9946 (3.7813)  class_acc: 0.2500 (0.3252)  loss_scale: 32768.0000 (52563.7712)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.2189  max mem: 15572
Epoch: [26]  [1320/2809]  eta: 0:14:10  lr: 0.000015  min_lr: 0.000000  loss: 3.9399 (3.7838)  class_acc: 0.2500 (0.3243)  loss_scale: 32768.0000 (52413.9167)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1585  max mem: 15572
Epoch: [26]  [1330/2809]  eta: 0:14:04  lr: 0.000015  min_lr: 0.000000  loss: 4.0902 (3.7851)  class_acc: 0.2500 (0.3242)  loss_scale: 32768.0000 (52266.3140)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1220  max mem: 15572
[2025-01-16 02:57:35,870] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:57:35,872] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1340/2809]  eta: 0:13:58  lr: 0.000015  min_lr: 0.000000  loss: 3.8737 (3.7845)  class_acc: 0.3333 (0.3245)  loss_scale: 32768.0000 (52169.7837)  weight_decay: 0.0500 (0.0500)  time: 0.5359  data: 0.1068  max mem: 15572
Epoch: [26]  [1350/2809]  eta: 0:13:53  lr: 0.000015  min_lr: 0.000000  loss: 3.7314 (3.7852)  class_acc: 0.3750 (0.3246)  loss_scale: 65536.0000 (52268.7195)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1171  max mem: 15572
Epoch: [26]  [1360/2809]  eta: 0:13:47  lr: 0.000015  min_lr: 0.000000  loss: 3.9119 (3.7866)  class_acc: 0.2917 (0.3242)  loss_scale: 65536.0000 (52366.2013)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1518  max mem: 15572
Epoch: [26]  [1370/2809]  eta: 0:13:41  lr: 0.000015  min_lr: 0.000000  loss: 3.8588 (3.7855)  class_acc: 0.3333 (0.3248)  loss_scale: 65536.0000 (52462.2611)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1228  max mem: 15572
Epoch: [26]  [1380/2809]  eta: 0:13:35  lr: 0.000015  min_lr: 0.000000  loss: 3.7214 (3.7856)  class_acc: 0.2917 (0.3245)  loss_scale: 65536.0000 (52556.9298)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.1041  max mem: 15572
Epoch: [26]  [1390/2809]  eta: 0:13:28  lr: 0.000015  min_lr: 0.000000  loss: 3.7494 (3.7853)  class_acc: 0.2917 (0.3245)  loss_scale: 65536.0000 (52650.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0770  max mem: 15572
Epoch: [26]  [1400/2809]  eta: 0:13:23  lr: 0.000015  min_lr: 0.000000  loss: 3.7479 (3.7848)  class_acc: 0.3333 (0.3245)  loss_scale: 65536.0000 (52742.2127)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.0935  max mem: 15572
Epoch: [26]  [1410/2809]  eta: 0:13:16  lr: 0.000015  min_lr: 0.000000  loss: 3.8854 (3.7847)  class_acc: 0.2917 (0.3245)  loss_scale: 65536.0000 (52832.8845)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0841  max mem: 15572
Epoch: [26]  [1420/2809]  eta: 0:13:11  lr: 0.000015  min_lr: 0.000000  loss: 3.9122 (3.7850)  class_acc: 0.2500 (0.3244)  loss_scale: 65536.0000 (52922.2801)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0955  max mem: 15572
Epoch: [26]  [1430/2809]  eta: 0:13:05  lr: 0.000015  min_lr: 0.000000  loss: 3.7013 (3.7849)  class_acc: 0.2917 (0.3243)  loss_scale: 65536.0000 (53010.4263)  weight_decay: 0.0500 (0.0500)  time: 0.5564  data: 0.1182  max mem: 15572
Epoch: [26]  [1440/2809]  eta: 0:12:59  lr: 0.000015  min_lr: 0.000000  loss: 3.8919 (3.7856)  class_acc: 0.2917 (0.3242)  loss_scale: 65536.0000 (53097.3491)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1139  max mem: 15572
Epoch: [26]  [1450/2809]  eta: 0:12:53  lr: 0.000015  min_lr: 0.000000  loss: 3.9137 (3.7865)  class_acc: 0.2917 (0.3238)  loss_scale: 65536.0000 (53183.0737)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1289  max mem: 15572
Epoch: [26]  [1460/2809]  eta: 0:12:48  lr: 0.000015  min_lr: 0.000000  loss: 3.6763 (3.7857)  class_acc: 0.2917 (0.3239)  loss_scale: 65536.0000 (53267.6249)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1336  max mem: 15572
[2025-01-16 02:58:46,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 02:58:46,602] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [1470/2809]  eta: 0:12:42  lr: 0.000015  min_lr: 0.000000  loss: 3.6291 (3.7854)  class_acc: 0.3333 (0.3238)  loss_scale: 65536.0000 (53529.2345)  weight_decay: 0.0500 (0.0500)  time: 0.5554  data: 0.1148  max mem: 15572
[2025-01-16 02:58:49,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74506
[2025-01-16 02:58:49,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 02:58:49,119] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1480/2809]  eta: 0:12:36  lr: 0.000015  min_lr: 0.000000  loss: 3.7676 (3.7852)  class_acc: 0.3333 (0.3240)  loss_scale: 65536.0000 (53654.5577)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0802  max mem: 15572
Epoch: [26]  [1490/2809]  eta: 0:12:30  lr: 0.000015  min_lr: 0.000000  loss: 3.7027 (3.7858)  class_acc: 0.3333 (0.3242)  loss_scale: 65536.0000 (53734.2455)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0931  max mem: 15572
Epoch: [26]  [1500/2809]  eta: 0:12:25  lr: 0.000015  min_lr: 0.000000  loss: 3.8920 (3.7857)  class_acc: 0.2917 (0.3240)  loss_scale: 65536.0000 (53812.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5838  data: 0.1386  max mem: 15572
Epoch: [26]  [1510/2809]  eta: 0:12:19  lr: 0.000015  min_lr: 0.000000  loss: 3.7186 (3.7841)  class_acc: 0.3333 (0.3244)  loss_scale: 65536.0000 (53890.4567)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1615  max mem: 15572
Epoch: [26]  [1520/2809]  eta: 0:12:14  lr: 0.000015  min_lr: 0.000000  loss: 3.6152 (3.7831)  class_acc: 0.3333 (0.3246)  loss_scale: 65536.0000 (53967.0217)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1508  max mem: 15572
Epoch: [26]  [1530/2809]  eta: 0:12:08  lr: 0.000015  min_lr: 0.000000  loss: 3.6871 (3.7826)  class_acc: 0.2917 (0.3247)  loss_scale: 65536.0000 (54042.5865)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1441  max mem: 15572
Epoch: [26]  [1540/2809]  eta: 0:12:02  lr: 0.000015  min_lr: 0.000000  loss: 3.7703 (3.7838)  class_acc: 0.2083 (0.3241)  loss_scale: 65536.0000 (54117.1707)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1445  max mem: 15572
Epoch: [26]  [1550/2809]  eta: 0:11:57  lr: 0.000015  min_lr: 0.000000  loss: 4.0310 (3.7837)  class_acc: 0.2500 (0.3239)  loss_scale: 65536.0000 (54190.7930)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1493  max mem: 15572
Epoch: [26]  [1560/2809]  eta: 0:11:50  lr: 0.000015  min_lr: 0.000000  loss: 3.7661 (3.7845)  class_acc: 0.2500 (0.3238)  loss_scale: 65536.0000 (54263.4721)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0938  max mem: 15572
Epoch: [26]  [1570/2809]  eta: 0:11:45  lr: 0.000015  min_lr: 0.000000  loss: 4.0390 (3.7849)  class_acc: 0.2917 (0.3236)  loss_scale: 65536.0000 (54335.2260)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0921  max mem: 15572
Epoch: [26]  [1580/2809]  eta: 0:11:39  lr: 0.000015  min_lr: 0.000000  loss: 3.9525 (3.7843)  class_acc: 0.2917 (0.3237)  loss_scale: 65536.0000 (54406.0721)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1363  max mem: 15572
Epoch: [26]  [1590/2809]  eta: 0:11:33  lr: 0.000015  min_lr: 0.000000  loss: 3.6841 (3.7831)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (54476.0277)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1365  max mem: 15572
Epoch: [26]  [1600/2809]  eta: 0:11:28  lr: 0.000015  min_lr: 0.000000  loss: 3.7031 (3.7833)  class_acc: 0.3333 (0.3240)  loss_scale: 65536.0000 (54545.1093)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1383  max mem: 15572
[2025-01-16 03:00:02,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:00:02,588] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:00:03,108] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74636
[2025-01-16 03:00:03,108] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:00:03,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1610/2809]  eta: 0:11:23  lr: 0.000015  min_lr: 0.000000  loss: 3.9894 (3.7848)  class_acc: 0.2917 (0.3238)  loss_scale: 65536.0000 (54654.0137)  weight_decay: 0.0500 (0.0500)  time: 0.6245  data: 0.1971  max mem: 15572
Epoch: [26]  [1620/2809]  eta: 0:11:18  lr: 0.000015  min_lr: 0.000000  loss: 3.9991 (3.7858)  class_acc: 0.2917 (0.3236)  loss_scale: 65536.0000 (54721.1450)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.2207  max mem: 15572
Epoch: [26]  [1630/2809]  eta: 0:11:11  lr: 0.000015  min_lr: 0.000000  loss: 3.9749 (3.7865)  class_acc: 0.2917 (0.3235)  loss_scale: 65536.0000 (54787.4531)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1121  max mem: 15572
Epoch: [26]  [1640/2809]  eta: 0:11:05  lr: 0.000015  min_lr: 0.000000  loss: 3.8843 (3.7863)  class_acc: 0.2917 (0.3235)  loss_scale: 65536.0000 (54852.9531)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0611  max mem: 15572
Epoch: [26]  [1650/2809]  eta: 0:10:59  lr: 0.000015  min_lr: 0.000000  loss: 3.8325 (3.7861)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (54917.6596)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0731  max mem: 15572
Epoch: [26]  [1660/2809]  eta: 0:10:52  lr: 0.000015  min_lr: 0.000000  loss: 3.8934 (3.7869)  class_acc: 0.2917 (0.3232)  loss_scale: 65536.0000 (54981.5870)  weight_decay: 0.0500 (0.0500)  time: 0.4693  data: 0.0206  max mem: 15572
Epoch: [26]  [1670/2809]  eta: 0:10:47  lr: 0.000015  min_lr: 0.000000  loss: 3.8934 (3.7861)  class_acc: 0.3333 (0.3233)  loss_scale: 65536.0000 (55044.7493)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0615  max mem: 15572
Epoch: [26]  [1680/2809]  eta: 0:10:41  lr: 0.000015  min_lr: 0.000000  loss: 3.7221 (3.7867)  class_acc: 0.3333 (0.3230)  loss_scale: 65536.0000 (55107.1600)  weight_decay: 0.0500 (0.0500)  time: 0.5740  data: 0.1403  max mem: 15572
Epoch: [26]  [1690/2809]  eta: 0:10:36  lr: 0.000015  min_lr: 0.000000  loss: 3.9287 (3.7875)  class_acc: 0.2500 (0.3228)  loss_scale: 65536.0000 (55168.8326)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1943  max mem: 15572
[2025-01-16 03:00:56,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74728
[2025-01-16 03:00:56,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:00:56,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [1700/2809]  eta: 0:10:31  lr: 0.000015  min_lr: 0.000000  loss: 3.9161 (3.7878)  class_acc: 0.2917 (0.3230)  loss_scale: 65536.0000 (55094.9324)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1876  max mem: 15572
Epoch: [26]  [1710/2809]  eta: 0:10:25  lr: 0.000015  min_lr: 0.000000  loss: 3.7679 (3.7878)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (54964.4418)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.1715  max mem: 15572
Epoch: [26]  [1720/2809]  eta: 0:10:19  lr: 0.000015  min_lr: 0.000000  loss: 3.8084 (3.7878)  class_acc: 0.2917 (0.3226)  loss_scale: 32768.0000 (54835.4678)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1374  max mem: 15572
Epoch: [26]  [1730/2809]  eta: 0:10:14  lr: 0.000015  min_lr: 0.000000  loss: 3.8703 (3.7895)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (54707.9838)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1314  max mem: 15572
Epoch: [26]  [1740/2809]  eta: 0:10:08  lr: 0.000015  min_lr: 0.000000  loss: 3.8594 (3.7881)  class_acc: 0.2917 (0.3227)  loss_scale: 32768.0000 (54581.9644)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1493  max mem: 15572
Epoch: [26]  [1750/2809]  eta: 0:10:03  lr: 0.000015  min_lr: 0.000000  loss: 3.7051 (3.7886)  class_acc: 0.2917 (0.3224)  loss_scale: 32768.0000 (54457.3844)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1178  max mem: 15572
Epoch: [26]  [1760/2809]  eta: 0:09:56  lr: 0.000015  min_lr: 0.000000  loss: 3.9168 (3.7891)  class_acc: 0.2917 (0.3225)  loss_scale: 32768.0000 (54334.2192)  weight_decay: 0.0500 (0.0500)  time: 0.5029  data: 0.0618  max mem: 15572
Epoch: [26]  [1770/2809]  eta: 0:09:50  lr: 0.000015  min_lr: 0.000000  loss: 3.9687 (3.7904)  class_acc: 0.2917 (0.3222)  loss_scale: 32768.0000 (54212.4449)  weight_decay: 0.0500 (0.0500)  time: 0.4492  data: 0.0006  max mem: 15572
Epoch: [26]  [1780/2809]  eta: 0:09:44  lr: 0.000015  min_lr: 0.000000  loss: 4.1318 (3.7907)  class_acc: 0.2500 (0.3222)  loss_scale: 32768.0000 (54092.0382)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0324  max mem: 15572
Epoch: [26]  [1790/2809]  eta: 0:09:38  lr: 0.000015  min_lr: 0.000000  loss: 3.8199 (3.7902)  class_acc: 0.2917 (0.3223)  loss_scale: 32768.0000 (53972.9760)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0721  max mem: 15572
Epoch: [26]  [1800/2809]  eta: 0:09:32  lr: 0.000015  min_lr: 0.000000  loss: 3.7722 (3.7898)  class_acc: 0.3333 (0.3223)  loss_scale: 32768.0000 (53855.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0890  max mem: 15572
Epoch: [26]  [1810/2809]  eta: 0:09:26  lr: 0.000015  min_lr: 0.000000  loss: 3.8647 (3.7900)  class_acc: 0.3333 (0.3222)  loss_scale: 32768.0000 (53738.7962)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1117  max mem: 15572
Epoch: [26]  [1820/2809]  eta: 0:09:21  lr: 0.000015  min_lr: 0.000000  loss: 3.9720 (3.7906)  class_acc: 0.2500 (0.3220)  loss_scale: 32768.0000 (53623.6354)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1505  max mem: 15572
[2025-01-16 03:02:05,796] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:02:05,796] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [1830/2809]  eta: 0:09:16  lr: 0.000015  min_lr: 0.000000  loss: 3.9491 (3.7905)  class_acc: 0.2083 (0.3220)  loss_scale: 32768.0000 (53652.9022)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.2192  max mem: 15572
Epoch: [26]  [1840/2809]  eta: 0:09:10  lr: 0.000015  min_lr: 0.000000  loss: 3.7669 (3.7902)  class_acc: 0.2500 (0.3222)  loss_scale: 65536.0000 (53717.4492)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1563  max mem: 15572
Epoch: [26]  [1850/2809]  eta: 0:09:04  lr: 0.000015  min_lr: 0.000000  loss: 3.7102 (3.7893)  class_acc: 0.3750 (0.3226)  loss_scale: 65536.0000 (53781.2988)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0835  max mem: 15572
Epoch: [26]  [1860/2809]  eta: 0:08:58  lr: 0.000015  min_lr: 0.000000  loss: 3.8997 (3.7912)  class_acc: 0.2917 (0.3224)  loss_scale: 65536.0000 (53844.4621)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1259  max mem: 15572
Epoch: [26]  [1870/2809]  eta: 0:08:53  lr: 0.000015  min_lr: 0.000000  loss: 3.9775 (3.7909)  class_acc: 0.2917 (0.3223)  loss_scale: 65536.0000 (53906.9503)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1507  max mem: 15572
Epoch: [26]  [1880/2809]  eta: 0:08:47  lr: 0.000015  min_lr: 0.000000  loss: 3.9451 (3.7923)  class_acc: 0.3333 (0.3220)  loss_scale: 65536.0000 (53968.7741)  weight_decay: 0.0500 (0.0500)  time: 0.5373  data: 0.1106  max mem: 15572
Epoch: [26]  [1890/2809]  eta: 0:08:41  lr: 0.000015  min_lr: 0.000000  loss: 3.9289 (3.7921)  class_acc: 0.2500 (0.3219)  loss_scale: 65536.0000 (54029.9439)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0698  max mem: 15572
Epoch: [26]  [1900/2809]  eta: 0:08:36  lr: 0.000015  min_lr: 0.000000  loss: 3.7202 (3.7922)  class_acc: 0.2917 (0.3218)  loss_scale: 65536.0000 (54090.4703)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1875  max mem: 15572
Epoch: [26]  [1910/2809]  eta: 0:08:30  lr: 0.000015  min_lr: 0.000000  loss: 3.7870 (3.7925)  class_acc: 0.2500 (0.3215)  loss_scale: 65536.0000 (54150.3632)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.2314  max mem: 15572
Epoch: [26]  [1920/2809]  eta: 0:08:25  lr: 0.000015  min_lr: 0.000000  loss: 3.9907 (3.7930)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (54209.6325)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1396  max mem: 15572
Epoch: [26]  [1930/2809]  eta: 0:08:19  lr: 0.000015  min_lr: 0.000000  loss: 3.7439 (3.7923)  class_acc: 0.3333 (0.3214)  loss_scale: 65536.0000 (54268.2879)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1238  max mem: 15572
Epoch: [26]  [1940/2809]  eta: 0:08:14  lr: 0.000015  min_lr: 0.000000  loss: 3.7009 (3.7922)  class_acc: 0.3333 (0.3215)  loss_scale: 65536.0000 (54326.3390)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1556  max mem: 15572
Epoch: [26]  [1950/2809]  eta: 0:08:08  lr: 0.000015  min_lr: 0.000000  loss: 3.8090 (3.7930)  class_acc: 0.2917 (0.3212)  loss_scale: 65536.0000 (54383.7950)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1645  max mem: 15572
[2025-01-16 03:03:21,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:03:21,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:03:21,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 74986
[2025-01-16 03:03:21,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:03:21,588] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1960/2809]  eta: 0:08:03  lr: 0.000015  min_lr: 0.000000  loss: 3.8090 (3.7932)  class_acc: 0.2500 (0.3210)  loss_scale: 65536.0000 (54474.0847)  weight_decay: 0.0500 (0.0500)  time: 0.6346  data: 0.1791  max mem: 15572
[2025-01-16 03:03:29,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=75000, skipped=501, lr=[1.4380800353752938e-07, 1.4380800353752938e-07, 2.0544000505361344e-07, 2.0544000505361344e-07, 2.934857215051621e-07, 2.934857215051621e-07, 4.1926531643594586e-07, 4.1926531643594586e-07, 5.989504520513512e-07, 5.989504520513512e-07, 8.556435029305019e-07, 8.556435029305019e-07, 1.2223478613292883e-06, 1.2223478613292883e-06, 1.7462112304704122e-06, 1.7462112304704122e-06, 2.4945874721005886e-06, 2.4945874721005886e-06, 3.5636963887151274e-06, 3.5636963887151274e-06, 5.090994841021611e-06, 5.090994841021611e-06, 7.2728497728880156e-06, 7.2728497728880156e-06, 1.0389785389840023e-05, 1.0389785389840023e-05, 1.484255055691432e-05, 1.484255055691432e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:03:29,593] [INFO] [timer.py:260:stop] epoch=0/micro_step=75000/global_step=75000, RunningAvgSamplesPerSec=28.55852145808852, CurrSamplesPerSec=21.30658378551369, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [26]  [1970/2809]  eta: 0:07:57  lr: 0.000015  min_lr: 0.000000  loss: 3.6661 (3.7918)  class_acc: 0.3333 (0.3215)  loss_scale: 65536.0000 (54530.2080)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.0998  max mem: 15572
Epoch: [26]  [1980/2809]  eta: 0:07:50  lr: 0.000015  min_lr: 0.000000  loss: 3.6661 (3.7911)  class_acc: 0.3333 (0.3214)  loss_scale: 65536.0000 (54585.7648)  weight_decay: 0.0500 (0.0500)  time: 0.4511  data: 0.0006  max mem: 15572
Epoch: [26]  [1990/2809]  eta: 0:07:45  lr: 0.000015  min_lr: 0.000000  loss: 3.6888 (3.7911)  class_acc: 0.3333 (0.3214)  loss_scale: 65536.0000 (54640.7634)  weight_decay: 0.0500 (0.0500)  time: 0.5276  data: 0.0808  max mem: 15572
Epoch: [26]  [2000/2809]  eta: 0:07:39  lr: 0.000015  min_lr: 0.000000  loss: 3.7521 (3.7914)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (54695.2124)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1633  max mem: 15572
Epoch: [26]  [2010/2809]  eta: 0:07:34  lr: 0.000015  min_lr: 0.000000  loss: 3.8969 (3.7920)  class_acc: 0.2917 (0.3210)  loss_scale: 65536.0000 (54749.1198)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1496  max mem: 15572
Epoch: [26]  [2020/2809]  eta: 0:07:28  lr: 0.000015  min_lr: 0.000000  loss: 3.9088 (3.7917)  class_acc: 0.2500 (0.3208)  loss_scale: 65536.0000 (54802.4938)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1882  max mem: 15572
Epoch: [26]  [2030/2809]  eta: 0:07:22  lr: 0.000015  min_lr: 0.000000  loss: 3.7457 (3.7919)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (54855.3422)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1278  max mem: 15572
Epoch: [26]  [2040/2809]  eta: 0:07:16  lr: 0.000015  min_lr: 0.000000  loss: 3.7515 (3.7917)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (54907.6727)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0635  max mem: 15572
Epoch: [26]  [2050/2809]  eta: 0:07:11  lr: 0.000015  min_lr: 0.000000  loss: 3.7515 (3.7912)  class_acc: 0.2917 (0.3207)  loss_scale: 65536.0000 (54959.4929)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.1176  max mem: 15572
Epoch: [26]  [2060/2809]  eta: 0:07:05  lr: 0.000015  min_lr: 0.000000  loss: 3.7689 (3.7912)  class_acc: 0.2917 (0.3207)  loss_scale: 65536.0000 (55010.8103)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.1062  max mem: 15572
Epoch: [26]  [2070/2809]  eta: 0:06:59  lr: 0.000015  min_lr: 0.000000  loss: 3.8252 (3.7911)  class_acc: 0.3333 (0.3208)  loss_scale: 65536.0000 (55061.6321)  weight_decay: 0.0500 (0.0500)  time: 0.5026  data: 0.0577  max mem: 15572
Epoch: [26]  [2080/2809]  eta: 0:06:54  lr: 0.000015  min_lr: 0.000000  loss: 3.8726 (3.7913)  class_acc: 0.3333 (0.3208)  loss_scale: 65536.0000 (55111.9654)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1326  max mem: 15572
[2025-01-16 03:04:33,391] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:04:33,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:04:35,559] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75117
[2025-01-16 03:04:35,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:04:35,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2090/2809]  eta: 0:06:48  lr: 0.000015  min_lr: 0.000000  loss: 3.8623 (3.7919)  class_acc: 0.2917 (0.3206)  loss_scale: 65536.0000 (55224.5012)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.1868  max mem: 15572
Epoch: [26]  [2100/2809]  eta: 0:06:42  lr: 0.000015  min_lr: 0.000000  loss: 3.6964 (3.7914)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (55273.5802)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1294  max mem: 15572
Epoch: [26]  [2110/2809]  eta: 0:06:37  lr: 0.000015  min_lr: 0.000000  loss: 3.6525 (3.7896)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (55322.1942)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1465  max mem: 15572
Epoch: [26]  [2120/2809]  eta: 0:06:31  lr: 0.000015  min_lr: 0.000000  loss: 3.6964 (3.7897)  class_acc: 0.2917 (0.3213)  loss_scale: 65536.0000 (55370.3498)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1311  max mem: 15572
Epoch: [26]  [2130/2809]  eta: 0:06:25  lr: 0.000015  min_lr: 0.000000  loss: 3.7555 (3.7901)  class_acc: 0.2917 (0.3213)  loss_scale: 65536.0000 (55418.0535)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1166  max mem: 15572
Epoch: [26]  [2140/2809]  eta: 0:06:19  lr: 0.000015  min_lr: 0.000000  loss: 3.7780 (3.7901)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (55465.3115)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0997  max mem: 15572
Epoch: [26]  [2150/2809]  eta: 0:06:14  lr: 0.000015  min_lr: 0.000000  loss: 3.8068 (3.7908)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (55512.1302)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1334  max mem: 15572
Epoch: [26]  [2160/2809]  eta: 0:06:08  lr: 0.000015  min_lr: 0.000000  loss: 3.7982 (3.7904)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (55558.5155)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1416  max mem: 15572
Epoch: [26]  [2170/2809]  eta: 0:06:02  lr: 0.000015  min_lr: 0.000000  loss: 3.7409 (3.7901)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (55604.4735)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1004  max mem: 15572
Epoch: [26]  [2180/2809]  eta: 0:05:57  lr: 0.000015  min_lr: 0.000000  loss: 3.7555 (3.7901)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (55650.0101)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1123  max mem: 15572
Epoch: [26]  [2190/2809]  eta: 0:05:51  lr: 0.000015  min_lr: 0.000000  loss: 3.8889 (3.7913)  class_acc: 0.2500 (0.3209)  loss_scale: 65536.0000 (55695.1310)  weight_decay: 0.0500 (0.0500)  time: 0.5185  data: 0.0657  max mem: 15572
Epoch: [26]  [2200/2809]  eta: 0:05:45  lr: 0.000015  min_lr: 0.000000  loss: 4.0271 (3.7925)  class_acc: 0.2083 (0.3207)  loss_scale: 65536.0000 (55739.8419)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0579  max mem: 15572
Epoch: [26]  [2210/2809]  eta: 0:05:40  lr: 0.000015  min_lr: 0.000000  loss: 3.9597 (3.7922)  class_acc: 0.2917 (0.3208)  loss_scale: 65536.0000 (55784.1483)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1395  max mem: 15572
[2025-01-16 03:05:47,172] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:05:47,173] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:05:48,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75250
[2025-01-16 03:05:48,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:05:48,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2220/2809]  eta: 0:05:34  lr: 0.000015  min_lr: 0.000000  loss: 3.7091 (3.7922)  class_acc: 0.3333 (0.3210)  loss_scale: 65536.0000 (55946.0855)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1922  max mem: 15572
Epoch: [26]  [2230/2809]  eta: 0:05:28  lr: 0.000015  min_lr: 0.000000  loss: 3.7064 (3.7911)  class_acc: 0.3750 (0.3213)  loss_scale: 65536.0000 (55989.0704)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1606  max mem: 15572
Epoch: [26]  [2240/2809]  eta: 0:05:23  lr: 0.000015  min_lr: 0.000000  loss: 3.7447 (3.7915)  class_acc: 0.3750 (0.3213)  loss_scale: 65536.0000 (56031.6716)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1626  max mem: 15572
Epoch: [26]  [2250/2809]  eta: 0:05:17  lr: 0.000015  min_lr: 0.000000  loss: 3.8201 (3.7917)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (56073.8943)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0981  max mem: 15572
Epoch: [26]  [2260/2809]  eta: 0:05:11  lr: 0.000015  min_lr: 0.000000  loss: 3.7503 (3.7912)  class_acc: 0.3333 (0.3213)  loss_scale: 65536.0000 (56115.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5015  data: 0.0578  max mem: 15572
[2025-01-16 03:06:18,859] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75302
[2025-01-16 03:06:18,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:06:18,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2270/2809]  eta: 0:05:05  lr: 0.000015  min_lr: 0.000000  loss: 3.8055 (3.7920)  class_acc: 0.2917 (0.3210)  loss_scale: 65536.0000 (56113.9375)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0767  max mem: 15572
Epoch: [26]  [2280/2809]  eta: 0:05:00  lr: 0.000015  min_lr: 0.000000  loss: 3.8506 (3.7915)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (56011.5879)  weight_decay: 0.0500 (0.0500)  time: 0.5118  data: 0.0736  max mem: 15572
Epoch: [26]  [2290/2809]  eta: 0:04:54  lr: 0.000015  min_lr: 0.000000  loss: 3.9156 (3.7924)  class_acc: 0.3333 (0.3212)  loss_scale: 32768.0000 (55910.1318)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1390  max mem: 15572
Epoch: [26]  [2300/2809]  eta: 0:04:48  lr: 0.000015  min_lr: 0.000000  loss: 3.8689 (3.7920)  class_acc: 0.3333 (0.3215)  loss_scale: 32768.0000 (55809.5576)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1348  max mem: 15572
Epoch: [26]  [2310/2809]  eta: 0:04:43  lr: 0.000015  min_lr: 0.000000  loss: 3.6944 (3.7916)  class_acc: 0.3333 (0.3214)  loss_scale: 32768.0000 (55709.8537)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0866  max mem: 15572
Epoch: [26]  [2320/2809]  eta: 0:04:37  lr: 0.000015  min_lr: 0.000000  loss: 3.7413 (3.7908)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (55611.0090)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0826  max mem: 15572
Epoch: [26]  [2330/2809]  eta: 0:04:31  lr: 0.000015  min_lr: 0.000000  loss: 3.6384 (3.7907)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (55513.0124)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1407  max mem: 15572
Epoch: [26]  [2340/2809]  eta: 0:04:25  lr: 0.000015  min_lr: 0.000000  loss: 3.8456 (3.7899)  class_acc: 0.2917 (0.3215)  loss_scale: 32768.0000 (55415.8531)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1347  max mem: 15572
Epoch: [26]  [2350/2809]  eta: 0:04:20  lr: 0.000015  min_lr: 0.000000  loss: 3.5543 (3.7893)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (55319.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1021  max mem: 15572
Epoch: [26]  [2360/2809]  eta: 0:04:14  lr: 0.000015  min_lr: 0.000000  loss: 3.6291 (3.7891)  class_acc: 0.3333 (0.3217)  loss_scale: 32768.0000 (55224.0034)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1330  max mem: 15572
Epoch: [26]  [2370/2809]  eta: 0:04:09  lr: 0.000015  min_lr: 0.000000  loss: 3.8226 (3.7895)  class_acc: 0.2917 (0.3217)  loss_scale: 32768.0000 (55129.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5893  data: 0.1379  max mem: 15572
Epoch: [26]  [2380/2809]  eta: 0:04:03  lr: 0.000015  min_lr: 0.000000  loss: 3.8226 (3.7896)  class_acc: 0.2500 (0.3215)  loss_scale: 32768.0000 (55035.3767)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1239  max mem: 15572
Epoch: [26]  [2390/2809]  eta: 0:03:57  lr: 0.000015  min_lr: 0.000000  loss: 3.8407 (3.7900)  class_acc: 0.2500 (0.3213)  loss_scale: 32768.0000 (54942.2468)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1623  max mem: 15572
[2025-01-16 03:07:31,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:07:31,930] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2400/2809]  eta: 0:03:52  lr: 0.000015  min_lr: 0.000000  loss: 3.7221 (3.7891)  class_acc: 0.3333 (0.3216)  loss_scale: 32768.0000 (54904.4831)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1224  max mem: 15572
Epoch: [26]  [2410/2809]  eta: 0:03:46  lr: 0.000015  min_lr: 0.000000  loss: 3.5209 (3.7890)  class_acc: 0.3333 (0.3216)  loss_scale: 65536.0000 (54948.5790)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0649  max mem: 15572
Epoch: [26]  [2420/2809]  eta: 0:03:40  lr: 0.000015  min_lr: 0.000000  loss: 3.7925 (3.7889)  class_acc: 0.3333 (0.3216)  loss_scale: 65536.0000 (54992.3106)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.0961  max mem: 15572
Epoch: [26]  [2430/2809]  eta: 0:03:34  lr: 0.000015  min_lr: 0.000000  loss: 4.0141 (3.7898)  class_acc: 0.2500 (0.3212)  loss_scale: 65536.0000 (55035.6824)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0914  max mem: 15572
Epoch: [26]  [2440/2809]  eta: 0:03:29  lr: 0.000015  min_lr: 0.000000  loss: 4.0141 (3.7902)  class_acc: 0.2083 (0.3212)  loss_scale: 65536.0000 (55078.6989)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.0731  max mem: 15572
Epoch: [26]  [2450/2809]  eta: 0:03:23  lr: 0.000015  min_lr: 0.000000  loss: 3.7494 (3.7899)  class_acc: 0.2917 (0.3212)  loss_scale: 65536.0000 (55121.3643)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1056  max mem: 15572
Epoch: [26]  [2460/2809]  eta: 0:03:17  lr: 0.000014  min_lr: 0.000000  loss: 3.8922 (3.7905)  class_acc: 0.2917 (0.3210)  loss_scale: 65536.0000 (55163.6831)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1061  max mem: 15572
Epoch: [26]  [2470/2809]  eta: 0:03:12  lr: 0.000014  min_lr: 0.000000  loss: 3.9304 (3.7903)  class_acc: 0.2500 (0.3209)  loss_scale: 65536.0000 (55205.6592)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1163  max mem: 15572
Epoch: [26]  [2480/2809]  eta: 0:03:06  lr: 0.000014  min_lr: 0.000000  loss: 3.9399 (3.7910)  class_acc: 0.2500 (0.3209)  loss_scale: 65536.0000 (55247.2971)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1676  max mem: 15572
Epoch: [26]  [2490/2809]  eta: 0:03:01  lr: 0.000014  min_lr: 0.000000  loss: 4.0957 (3.7921)  class_acc: 0.2500 (0.3206)  loss_scale: 65536.0000 (55288.6006)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1755  max mem: 15572
Epoch: [26]  [2500/2809]  eta: 0:02:55  lr: 0.000014  min_lr: 0.000000  loss: 4.0729 (3.7923)  class_acc: 0.2500 (0.3205)  loss_scale: 65536.0000 (55329.5738)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1590  max mem: 15572
Epoch: [26]  [2510/2809]  eta: 0:02:49  lr: 0.000014  min_lr: 0.000000  loss: 3.6394 (3.7914)  class_acc: 0.2917 (0.3205)  loss_scale: 65536.0000 (55370.2206)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.0873  max mem: 15572
Epoch: [26]  [2520/2809]  eta: 0:02:43  lr: 0.000014  min_lr: 0.000000  loss: 3.6394 (3.7910)  class_acc: 0.2917 (0.3204)  loss_scale: 65536.0000 (55410.5450)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0385  max mem: 15572
[2025-01-16 03:08:44,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:08:44,485] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:08:44,895] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75560
[2025-01-16 03:08:44,895] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:08:44,895] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [2530/2809]  eta: 0:02:38  lr: 0.000014  min_lr: 0.000000  loss: 4.0081 (3.7917)  class_acc: 0.2500 (0.3201)  loss_scale: 65536.0000 (55476.4441)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0988  max mem: 15572
Epoch: [26]  [2540/2809]  eta: 0:02:32  lr: 0.000014  min_lr: 0.000000  loss: 4.0101 (3.7917)  class_acc: 0.2500 (0.3202)  loss_scale: 65536.0000 (55516.0331)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1686  max mem: 15572
[2025-01-16 03:08:56,745] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75581
[2025-01-16 03:08:56,745] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:08:56,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2550/2809]  eta: 0:02:26  lr: 0.000014  min_lr: 0.000000  loss: 3.9250 (3.7926)  class_acc: 0.2917 (0.3200)  loss_scale: 65536.0000 (55503.9310)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1150  max mem: 15572
Epoch: [26]  [2560/2809]  eta: 0:02:21  lr: 0.000014  min_lr: 0.000000  loss: 3.9877 (3.7934)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (55415.1535)  weight_decay: 0.0500 (0.0500)  time: 0.5022  data: 0.0486  max mem: 15572
Epoch: [26]  [2570/2809]  eta: 0:02:15  lr: 0.000014  min_lr: 0.000000  loss: 3.8766 (3.7935)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (55327.0665)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0201  max mem: 15572
Epoch: [26]  [2580/2809]  eta: 0:02:09  lr: 0.000014  min_lr: 0.000000  loss: 3.8805 (3.7941)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (55239.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.0988  max mem: 15572
Epoch: [26]  [2590/2809]  eta: 0:02:04  lr: 0.000014  min_lr: 0.000000  loss: 3.8672 (3.7944)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (55152.9325)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.1954  max mem: 15572
Epoch: [26]  [2600/2809]  eta: 0:01:58  lr: 0.000014  min_lr: 0.000000  loss: 3.7123 (3.7938)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (55066.8697)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1168  max mem: 15572
Epoch: [26]  [2610/2809]  eta: 0:01:52  lr: 0.000014  min_lr: 0.000000  loss: 3.7202 (3.7941)  class_acc: 0.3333 (0.3199)  loss_scale: 32768.0000 (54981.4661)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0693  max mem: 15572
Epoch: [26]  [2620/2809]  eta: 0:01:47  lr: 0.000014  min_lr: 0.000000  loss: 3.8664 (3.7943)  class_acc: 0.3333 (0.3200)  loss_scale: 32768.0000 (54896.7142)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1393  max mem: 15572
Epoch: [26]  [2630/2809]  eta: 0:01:41  lr: 0.000014  min_lr: 0.000000  loss: 3.8674 (3.7944)  class_acc: 0.2917 (0.3199)  loss_scale: 32768.0000 (54812.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1214  max mem: 15572
Epoch: [26]  [2640/2809]  eta: 0:01:35  lr: 0.000014  min_lr: 0.000000  loss: 3.9762 (3.7952)  class_acc: 0.2500 (0.3196)  loss_scale: 32768.0000 (54729.1359)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1124  max mem: 15572
Epoch: [26]  [2650/2809]  eta: 0:01:30  lr: 0.000014  min_lr: 0.000000  loss: 3.8937 (3.7949)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (54646.2950)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1516  max mem: 15572
Epoch: [26]  [2660/2809]  eta: 0:01:24  lr: 0.000014  min_lr: 0.000000  loss: 3.6346 (3.7943)  class_acc: 0.2917 (0.3198)  loss_scale: 32768.0000 (54564.0767)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1591  max mem: 15572
Epoch: [26]  [2670/2809]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000000  loss: 3.7475 (3.7949)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (54482.4740)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1049  max mem: 15572
[2025-01-16 03:10:10,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:10:10,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [2680/2809]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 3.8956 (3.7950)  class_acc: 0.2500 (0.3197)  loss_scale: 32768.0000 (54462.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1131  max mem: 15572
Epoch: [26]  [2690/2809]  eta: 0:01:07  lr: 0.000014  min_lr: 0.000000  loss: 3.9032 (3.7955)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (54503.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1225  max mem: 15572
Epoch: [26]  [2700/2809]  eta: 0:01:01  lr: 0.000014  min_lr: 0.000000  loss: 3.8664 (3.7952)  class_acc: 0.2917 (0.3197)  loss_scale: 65536.0000 (54544.5864)  weight_decay: 0.0500 (0.0500)  time: 0.4920  data: 0.0474  max mem: 15572
Epoch: [26]  [2710/2809]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 3.8664 (3.7961)  class_acc: 0.3333 (0.3197)  loss_scale: 65536.0000 (54585.1302)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0699  max mem: 15572
Epoch: [26]  [2720/2809]  eta: 0:00:50  lr: 0.000014  min_lr: 0.000000  loss: 3.8104 (3.7957)  class_acc: 0.3333 (0.3198)  loss_scale: 65536.0000 (54625.3760)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1242  max mem: 15572
Epoch: [26]  [2730/2809]  eta: 0:00:44  lr: 0.000014  min_lr: 0.000000  loss: 3.7957 (3.7959)  class_acc: 0.2917 (0.3197)  loss_scale: 65536.0000 (54665.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0973  max mem: 15572
Epoch: [26]  [2740/2809]  eta: 0:00:39  lr: 0.000014  min_lr: 0.000000  loss: 3.8080 (3.7957)  class_acc: 0.2917 (0.3198)  loss_scale: 65536.0000 (54704.9865)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0610  max mem: 15572
Epoch: [26]  [2750/2809]  eta: 0:00:33  lr: 0.000014  min_lr: 0.000000  loss: 3.8473 (3.7962)  class_acc: 0.2917 (0.3197)  loss_scale: 65536.0000 (54744.3577)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1010  max mem: 15572
Epoch: [26]  [2760/2809]  eta: 0:00:27  lr: 0.000014  min_lr: 0.000000  loss: 3.8742 (3.7960)  class_acc: 0.2917 (0.3198)  loss_scale: 65536.0000 (54783.4437)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1105  max mem: 15572
Epoch: [26]  [2770/2809]  eta: 0:00:22  lr: 0.000014  min_lr: 0.000000  loss: 3.9186 (3.7967)  class_acc: 0.2500 (0.3195)  loss_scale: 65536.0000 (54822.2476)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0939  max mem: 15572
Epoch: [26]  [2780/2809]  eta: 0:00:16  lr: 0.000014  min_lr: 0.000000  loss: 3.9325 (3.7964)  class_acc: 0.2500 (0.3194)  loss_scale: 65536.0000 (54860.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0758  max mem: 15572
[2025-01-16 03:11:10,384] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 75822
[2025-01-16 03:11:10,384] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:11:10,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [2790/2809]  eta: 0:00:10  lr: 0.000014  min_lr: 0.000000  loss: 3.6249 (3.7956)  class_acc: 0.2917 (0.3196)  loss_scale: 65536.0000 (54863.7994)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0834  max mem: 15572
Epoch: [26]  [2800/2809]  eta: 0:00:05  lr: 0.000014  min_lr: 0.000000  loss: 3.7149 (3.7954)  class_acc: 0.3750 (0.3198)  loss_scale: 32768.0000 (54784.9140)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.0968  max mem: 15572
Epoch: [26]  [2808/2809]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 3.7599 (3.7951)  class_acc: 0.3750 (0.3199)  loss_scale: 32768.0000 (54722.2100)  weight_decay: 0.0500 (0.0500)  time: 0.4870  data: 0.0447  max mem: 15572
Epoch: [26] Total time: 0:26:30 (0.5661 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 3.7599 (3.7951)  class_acc: 0.3750 (0.3199)  loss_scale: 32768.0000 (54722.2100)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:20:34  loss: 0.3867 (0.3867)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.5389  data: 4.3565  max mem: 15572
Val:  [ 10/272]  eta: 0:03:16  loss: 2.5200 (2.3991)  acc1: 33.3333 (36.8687)  acc5: 72.2222 (70.7071)  time: 0.7505  data: 0.5486  max mem: 15572
Val:  [ 20/272]  eta: 0:02:12  loss: 2.5170 (2.4020)  acc1: 38.8889 (42.8571)  acc5: 72.2222 (72.4868)  time: 0.3267  data: 0.1224  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 2.3667 (2.4963)  acc1: 44.4444 (39.9642)  acc5: 72.2222 (71.8638)  time: 0.2833  data: 0.0960  max mem: 15572
Val:  [ 40/272]  eta: 0:01:38  loss: 2.5622 (2.5279)  acc1: 22.2222 (37.3984)  acc5: 77.7778 (72.2222)  time: 0.3185  data: 0.1424  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 2.4772 (2.4328)  acc1: 33.3333 (40.3050)  acc5: 77.7778 (74.2919)  time: 0.3438  data: 0.1620  max mem: 15572
Val:  [ 60/272]  eta: 0:01:23  loss: 1.4475 (2.3066)  acc1: 66.6667 (43.8980)  acc5: 88.8889 (75.5009)  time: 0.3315  data: 0.1390  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.4014 (2.2064)  acc1: 66.6667 (47.1831)  acc5: 88.8889 (76.7606)  time: 0.3114  data: 0.1120  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.8182 (2.2277)  acc1: 61.1111 (46.9136)  acc5: 77.7778 (76.1317)  time: 0.2985  data: 0.0990  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 2.2307 (2.2285)  acc1: 50.0000 (47.4969)  acc5: 77.7778 (76.8620)  time: 0.3103  data: 0.1139  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.1032 (2.2546)  acc1: 50.0000 (46.9197)  acc5: 77.7778 (76.3476)  time: 0.3110  data: 0.1210  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 2.6255 (2.3332)  acc1: 22.2222 (44.6947)  acc5: 66.6667 (74.9750)  time: 0.3207  data: 0.1102  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 2.9572 (2.3671)  acc1: 22.2222 (43.8017)  acc5: 66.6667 (74.2883)  time: 0.3244  data: 0.1057  max mem: 15572
Val:  [130/272]  eta: 0:00:50  loss: 2.1510 (2.3285)  acc1: 44.4444 (44.8685)  acc5: 77.7778 (75.2332)  time: 0.3356  data: 0.1299  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 1.6674 (2.3196)  acc1: 61.1111 (45.2325)  acc5: 88.8889 (75.0591)  time: 0.3334  data: 0.1299  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.3206 (2.3222)  acc1: 38.8889 (44.8124)  acc5: 72.2222 (75.2759)  time: 0.3199  data: 0.1157  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.3206 (2.3114)  acc1: 44.4444 (45.4106)  acc5: 77.7778 (75.6039)  time: 0.3209  data: 0.1189  max mem: 15572
Val:  [170/272]  eta: 0:00:35  loss: 2.3278 (2.3277)  acc1: 44.4444 (44.8993)  acc5: 72.2222 (75.2437)  time: 0.3061  data: 0.1115  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.3090 (2.3179)  acc1: 33.3333 (44.8435)  acc5: 72.2222 (75.7213)  time: 0.3254  data: 0.1278  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.5103 (2.3746)  acc1: 33.3333 (43.6009)  acc5: 77.7778 (74.2874)  time: 0.2923  data: 0.0958  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.6857 (2.3867)  acc1: 33.3333 (43.2007)  acc5: 61.1111 (74.0741)  time: 0.2575  data: 0.0686  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.3202 (2.3910)  acc1: 44.4444 (43.3649)  acc5: 77.7778 (74.0126)  time: 0.2943  data: 0.0995  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.4627 (2.3803)  acc1: 44.4444 (43.6652)  acc5: 77.7778 (74.1579)  time: 0.3104  data: 0.1080  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.8245 (2.3559)  acc1: 55.5556 (44.5647)  acc5: 83.3333 (74.5070)  time: 0.3260  data: 0.1158  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.7295 (2.3428)  acc1: 55.5556 (44.6980)  acc5: 83.3333 (74.7810)  time: 0.3471  data: 0.1518  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.3659 (2.3562)  acc1: 38.8889 (44.0460)  acc5: 77.7778 (74.6791)  time: 0.3365  data: 0.1462  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.2139 (2.2969)  acc1: 72.2222 (45.7854)  acc5: 88.8889 (75.3938)  time: 0.3083  data: 0.1031  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.4524 (2.2960)  acc1: 72.2222 (45.6540)  acc5: 88.8889 (75.5228)  time: 0.2259  data: 0.0455  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.4524 (2.3007)  acc1: 55.5556 (45.6277)  acc5: 88.8889 (75.4864)  time: 0.2186  data: 0.0454  max mem: 15572
Val: Total time: 0:01:28 (0.3265 s / it)
* Acc@1 45.628 Acc@5 75.486 loss 2.301
Accuracy of the network on the 4883 val videos: 45.6%
[2025-01-16 03:12:49,202] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 03:12:49,207] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 03:12:49,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 03:12:51,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 03:12:51,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.63%
Epoch: [27]  [   0/2809]  eta: 4:57:09  lr: 0.000014  min_lr: 0.000000  loss: 3.3141 (3.3141)  class_acc: 0.5833 (0.5833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.3473  data: 5.9269  max mem: 15572
Epoch: [27]  [  10/2809]  eta: 0:43:31  lr: 0.000014  min_lr: 0.000000  loss: 3.7966 (3.7683)  class_acc: 0.3750 (0.3864)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9329  data: 0.5472  max mem: 15572
Epoch: [27]  [  20/2809]  eta: 0:32:22  lr: 0.000014  min_lr: 0.000000  loss: 3.7966 (3.7591)  class_acc: 0.3333 (0.3631)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4141  data: 0.0049  max mem: 15572
Epoch: [27]  [  30/2809]  eta: 0:28:22  lr: 0.000014  min_lr: 0.000000  loss: 3.8989 (3.7474)  class_acc: 0.3333 (0.3535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4364  data: 0.0005  max mem: 15572
Epoch: [27]  [  40/2809]  eta: 0:26:31  lr: 0.000014  min_lr: 0.000000  loss: 3.8636 (3.7466)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4470  data: 0.0005  max mem: 15572
Epoch: [27]  [  50/2809]  eta: 0:27:14  lr: 0.000014  min_lr: 0.000000  loss: 3.8103 (3.7616)  class_acc: 0.3333 (0.3382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1094  max mem: 15572
Epoch: [27]  [  60/2809]  eta: 0:27:49  lr: 0.000014  min_lr: 0.000000  loss: 3.8103 (3.7577)  class_acc: 0.2917 (0.3361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.2243  max mem: 15572
Epoch: [27]  [  70/2809]  eta: 0:28:52  lr: 0.000014  min_lr: 0.000000  loss: 3.7655 (3.7548)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7351  data: 0.2615  max mem: 15572
Epoch: [27]  [  80/2809]  eta: 0:28:36  lr: 0.000014  min_lr: 0.000000  loss: 3.8671 (3.7730)  class_acc: 0.3333 (0.3344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6949  data: 0.2051  max mem: 15572
Epoch: [27]  [  90/2809]  eta: 0:28:40  lr: 0.000014  min_lr: 0.000000  loss: 3.8441 (3.7505)  class_acc: 0.3750 (0.3443)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.1737  max mem: 15572
Epoch: [27]  [ 100/2809]  eta: 0:28:59  lr: 0.000014  min_lr: 0.000000  loss: 3.8333 (3.7641)  class_acc: 0.2917 (0.3379)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.2411  max mem: 15572
[2025-01-16 03:14:00,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:14:00,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [ 110/2809]  eta: 0:28:43  lr: 0.000014  min_lr: 0.000000  loss: 3.7326 (3.7551)  class_acc: 0.2917 (0.3378)  loss_scale: 32768.0000 (33653.6216)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2053  max mem: 15572
Epoch: [27]  [ 120/2809]  eta: 0:28:35  lr: 0.000014  min_lr: 0.000000  loss: 3.4839 (3.7332)  class_acc: 0.3750 (0.3454)  loss_scale: 65536.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1505  max mem: 15572
Epoch: [27]  [ 130/2809]  eta: 0:28:37  lr: 0.000014  min_lr: 0.000000  loss: 3.6688 (3.7485)  class_acc: 0.4167 (0.3454)  loss_scale: 65536.0000 (38521.1603)  weight_decay: 0.0500 (0.0500)  time: 0.6539  data: 0.1809  max mem: 15572
Epoch: [27]  [ 140/2809]  eta: 0:28:39  lr: 0.000014  min_lr: 0.000000  loss: 3.7677 (3.7393)  class_acc: 0.3750 (0.3469)  loss_scale: 65536.0000 (40437.1064)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.2243  max mem: 15572
Epoch: [27]  [ 150/2809]  eta: 0:28:17  lr: 0.000014  min_lr: 0.000000  loss: 3.5863 (3.7238)  class_acc: 0.3333 (0.3463)  loss_scale: 65536.0000 (42099.2848)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1727  max mem: 15572
[2025-01-16 03:14:31,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=76000, skipped=507, lr=[1.3711166685044318e-07, 1.3711166685044318e-07, 1.9587380978634742e-07, 1.9587380978634742e-07, 2.7981972826621064e-07, 2.7981972826621064e-07, 3.997424689517295e-07, 3.997424689517295e-07, 5.710606699310422e-07, 5.710606699310422e-07, 8.15800957044346e-07, 8.15800957044346e-07, 1.1654299386347801e-06, 1.1654299386347801e-06, 1.6648999123354003e-06, 1.6648999123354003e-06, 2.378428446193429e-06, 2.378428446193429e-06, 3.3977549231334703e-06, 3.3977549231334703e-06, 4.853935604476386e-06, 4.853935604476386e-06, 6.9341937206805526e-06, 6.9341937206805526e-06, 9.905991029543647e-06, 9.905991029543647e-06, 1.4151415756490925e-05, 1.4151415756490925e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:14:31,098] [INFO] [timer.py:260:stop] epoch=0/micro_step=76000/global_step=76000, RunningAvgSamplesPerSec=28.55635296038442, CurrSamplesPerSec=31.453721797721634, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [ 160/2809]  eta: 0:27:42  lr: 0.000014  min_lr: 0.000000  loss: 3.6675 (3.7401)  class_acc: 0.2500 (0.3427)  loss_scale: 65536.0000 (43554.9814)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0836  max mem: 15572
Epoch: [27]  [ 170/2809]  eta: 0:27:06  lr: 0.000014  min_lr: 0.000000  loss: 3.7313 (3.7284)  class_acc: 0.2917 (0.3458)  loss_scale: 65536.0000 (44840.4211)  weight_decay: 0.0500 (0.0500)  time: 0.4511  data: 0.0260  max mem: 15572
Epoch: [27]  [ 180/2809]  eta: 0:26:50  lr: 0.000014  min_lr: 0.000000  loss: 3.5541 (3.7242)  class_acc: 0.3750 (0.3483)  loss_scale: 65536.0000 (45983.8232)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0517  max mem: 15572
Epoch: [27]  [ 190/2809]  eta: 0:26:44  lr: 0.000014  min_lr: 0.000000  loss: 3.8493 (3.7287)  class_acc: 0.3333 (0.3464)  loss_scale: 65536.0000 (47007.4974)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1555  max mem: 15572
Epoch: [27]  [ 200/2809]  eta: 0:26:34  lr: 0.000014  min_lr: 0.000000  loss: 3.6587 (3.7263)  class_acc: 0.2917 (0.3468)  loss_scale: 65536.0000 (47929.3134)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1769  max mem: 15572
Epoch: [27]  [ 210/2809]  eta: 0:26:08  lr: 0.000014  min_lr: 0.000000  loss: 3.5457 (3.7135)  class_acc: 0.3333 (0.3503)  loss_scale: 65536.0000 (48763.7536)  weight_decay: 0.0500 (0.0500)  time: 0.5133  data: 0.0851  max mem: 15572
Epoch: [27]  [ 220/2809]  eta: 0:26:06  lr: 0.000014  min_lr: 0.000000  loss: 3.5013 (3.7129)  class_acc: 0.3333 (0.3505)  loss_scale: 65536.0000 (49522.6787)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1155  max mem: 15572
Epoch: [27]  [ 230/2809]  eta: 0:26:00  lr: 0.000014  min_lr: 0.000000  loss: 3.8516 (3.7171)  class_acc: 0.3333 (0.3483)  loss_scale: 65536.0000 (50215.8961)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1908  max mem: 15572
[2025-01-16 03:15:15,494] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:15:15,494] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:15:16,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76081
[2025-01-16 03:15:16,350] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:15:16,352] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 240/2809]  eta: 0:25:49  lr: 0.000014  min_lr: 0.000000  loss: 3.8516 (3.7225)  class_acc: 0.3333 (0.3472)  loss_scale: 65536.0000 (51395.4523)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1439  max mem: 15572
Epoch: [27]  [ 250/2809]  eta: 0:25:32  lr: 0.000014  min_lr: 0.000000  loss: 3.7930 (3.7293)  class_acc: 0.3333 (0.3468)  loss_scale: 65536.0000 (51958.8207)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.0809  max mem: 15572
Epoch: [27]  [ 260/2809]  eta: 0:25:30  lr: 0.000014  min_lr: 0.000000  loss: 3.7885 (3.7303)  class_acc: 0.3750 (0.3467)  loss_scale: 65536.0000 (52479.0192)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1230  max mem: 15572
Epoch: [27]  [ 270/2809]  eta: 0:25:11  lr: 0.000014  min_lr: 0.000000  loss: 3.7554 (3.7381)  class_acc: 0.2917 (0.3438)  loss_scale: 65536.0000 (52960.8266)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0987  max mem: 15572
Epoch: [27]  [ 280/2809]  eta: 0:24:59  lr: 0.000014  min_lr: 0.000000  loss: 3.7745 (3.7415)  class_acc: 0.2500 (0.3428)  loss_scale: 65536.0000 (53408.3416)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0485  max mem: 15572
Epoch: [27]  [ 290/2809]  eta: 0:24:47  lr: 0.000014  min_lr: 0.000000  loss: 3.7145 (3.7410)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (53825.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0990  max mem: 15572
Epoch: [27]  [ 300/2809]  eta: 0:24:39  lr: 0.000014  min_lr: 0.000000  loss: 3.6541 (3.7374)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (54214.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.1086  max mem: 15572
Epoch: [27]  [ 310/2809]  eta: 0:24:29  lr: 0.000014  min_lr: 0.000000  loss: 3.6541 (3.7362)  class_acc: 0.3750 (0.3454)  loss_scale: 65536.0000 (54578.2122)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0969  max mem: 15572
Epoch: [27]  [ 320/2809]  eta: 0:24:24  lr: 0.000014  min_lr: 0.000000  loss: 3.5649 (3.7281)  class_acc: 0.3333 (0.3457)  loss_scale: 65536.0000 (54919.5763)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1280  max mem: 15572
Epoch: [27]  [ 330/2809]  eta: 0:24:13  lr: 0.000014  min_lr: 0.000000  loss: 3.5649 (3.7324)  class_acc: 0.3333 (0.3448)  loss_scale: 65536.0000 (55240.3142)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1314  max mem: 15572
Epoch: [27]  [ 340/2809]  eta: 0:24:10  lr: 0.000014  min_lr: 0.000000  loss: 3.6278 (3.7358)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (55542.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1300  max mem: 15572
Epoch: [27]  [ 350/2809]  eta: 0:23:58  lr: 0.000014  min_lr: 0.000000  loss: 3.8699 (3.7382)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (55826.9630)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1231  max mem: 15572
Epoch: [27]  [ 360/2809]  eta: 0:23:53  lr: 0.000014  min_lr: 0.000000  loss: 3.8850 (3.7405)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (56095.9114)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1282  max mem: 15572
[2025-01-16 03:16:27,344] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:16:27,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:16:28,152] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76212
[2025-01-16 03:16:28,153] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:16:28,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 370/2809]  eta: 0:23:44  lr: 0.000014  min_lr: 0.000000  loss: 3.6595 (3.7338)  class_acc: 0.3750 (0.3446)  loss_scale: 65536.0000 (56703.6550)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1374  max mem: 15572
Epoch: [27]  [ 380/2809]  eta: 0:23:36  lr: 0.000014  min_lr: 0.000000  loss: 3.7645 (3.7417)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (56935.4751)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1066  max mem: 15572
Epoch: [27]  [ 390/2809]  eta: 0:23:28  lr: 0.000014  min_lr: 0.000000  loss: 4.0641 (3.7407)  class_acc: 0.2500 (0.3422)  loss_scale: 65536.0000 (57155.4373)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1344  max mem: 15572
Epoch: [27]  [ 400/2809]  eta: 0:23:22  lr: 0.000014  min_lr: 0.000000  loss: 3.6678 (3.7379)  class_acc: 0.3333 (0.3426)  loss_scale: 65536.0000 (57364.4289)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1437  max mem: 15572
Epoch: [27]  [ 410/2809]  eta: 0:23:19  lr: 0.000014  min_lr: 0.000000  loss: 3.6948 (3.7392)  class_acc: 0.2917 (0.3412)  loss_scale: 65536.0000 (57563.2506)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1462  max mem: 15572
Epoch: [27]  [ 420/2809]  eta: 0:23:16  lr: 0.000014  min_lr: 0.000000  loss: 3.7727 (3.7377)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (57752.6271)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1743  max mem: 15572
Epoch: [27]  [ 430/2809]  eta: 0:23:08  lr: 0.000014  min_lr: 0.000000  loss: 3.7727 (3.7376)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (57933.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1397  max mem: 15572
Epoch: [27]  [ 440/2809]  eta: 0:23:02  lr: 0.000014  min_lr: 0.000000  loss: 3.8339 (3.7425)  class_acc: 0.2500 (0.3396)  loss_scale: 65536.0000 (58105.6145)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.0969  max mem: 15572
Epoch: [27]  [ 450/2809]  eta: 0:22:51  lr: 0.000014  min_lr: 0.000000  loss: 3.9725 (3.7440)  class_acc: 0.2500 (0.3392)  loss_scale: 65536.0000 (58270.3681)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0767  max mem: 15572
Epoch: [27]  [ 460/2809]  eta: 0:22:43  lr: 0.000014  min_lr: 0.000000  loss: 3.7634 (3.7446)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (58427.9740)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0756  max mem: 15572
Epoch: [27]  [ 470/2809]  eta: 0:22:36  lr: 0.000014  min_lr: 0.000000  loss: 3.6724 (3.7433)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (58578.8875)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1221  max mem: 15572
Epoch: [27]  [ 480/2809]  eta: 0:22:30  lr: 0.000014  min_lr: 0.000000  loss: 3.5759 (3.7414)  class_acc: 0.3750 (0.3406)  loss_scale: 65536.0000 (58723.5260)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.1239  max mem: 15572
Epoch: [27]  [ 490/2809]  eta: 0:22:24  lr: 0.000014  min_lr: 0.000000  loss: 3.7956 (3.7422)  class_acc: 0.3750 (0.3395)  loss_scale: 65536.0000 (58862.2729)  weight_decay: 0.0500 (0.0500)  time: 0.5735  data: 0.1304  max mem: 15572
[2025-01-16 03:17:40,192] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:17:40,192] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:17:41,791] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76343
[2025-01-16 03:17:41,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:17:41,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 500/2809]  eta: 0:22:15  lr: 0.000014  min_lr: 0.000000  loss: 3.7956 (3.7412)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (59257.1018)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.0998  max mem: 15572
Epoch: [27]  [ 510/2809]  eta: 0:22:07  lr: 0.000014  min_lr: 0.000000  loss: 3.7580 (3.7427)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (59379.9765)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0719  max mem: 15572
Epoch: [27]  [ 520/2809]  eta: 0:22:00  lr: 0.000014  min_lr: 0.000000  loss: 3.7580 (3.7423)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (59498.1344)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0792  max mem: 15572
Epoch: [27]  [ 530/2809]  eta: 0:21:49  lr: 0.000014  min_lr: 0.000000  loss: 3.6264 (3.7376)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (59611.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0403  max mem: 15572
Epoch: [27]  [ 540/2809]  eta: 0:21:44  lr: 0.000014  min_lr: 0.000000  loss: 3.6277 (3.7400)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (59721.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0971  max mem: 15572
Epoch: [27]  [ 550/2809]  eta: 0:21:40  lr: 0.000014  min_lr: 0.000000  loss: 3.6796 (3.7393)  class_acc: 0.3333 (0.3415)  loss_scale: 65536.0000 (59826.8748)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1658  max mem: 15572
Epoch: [27]  [ 560/2809]  eta: 0:21:30  lr: 0.000014  min_lr: 0.000000  loss: 3.7387 (3.7409)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (59928.6417)  weight_decay: 0.0500 (0.0500)  time: 0.5458  data: 0.0905  max mem: 15572
Epoch: [27]  [ 570/2809]  eta: 0:21:26  lr: 0.000014  min_lr: 0.000000  loss: 3.9454 (3.7443)  class_acc: 0.2500 (0.3391)  loss_scale: 65536.0000 (60026.8441)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1081  max mem: 15572
Epoch: [27]  [ 580/2809]  eta: 0:21:20  lr: 0.000014  min_lr: 0.000000  loss: 3.8385 (3.7398)  class_acc: 0.2917 (0.3399)  loss_scale: 65536.0000 (60121.6661)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1562  max mem: 15572
Epoch: [27]  [ 590/2809]  eta: 0:21:15  lr: 0.000014  min_lr: 0.000000  loss: 3.7676 (3.7425)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (60213.2792)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1373  max mem: 15572
Epoch: [27]  [ 600/2809]  eta: 0:21:08  lr: 0.000014  min_lr: 0.000000  loss: 3.8749 (3.7430)  class_acc: 0.2500 (0.3390)  loss_scale: 65536.0000 (60301.8436)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1162  max mem: 15572
Epoch: [27]  [ 610/2809]  eta: 0:21:03  lr: 0.000014  min_lr: 0.000000  loss: 3.6774 (3.7421)  class_acc: 0.2917 (0.3393)  loss_scale: 65536.0000 (60387.5090)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1208  max mem: 15572
Epoch: [27]  [ 620/2809]  eta: 0:20:57  lr: 0.000014  min_lr: 0.000000  loss: 3.6355 (3.7403)  class_acc: 0.4167 (0.3403)  loss_scale: 65536.0000 (60470.4155)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1491  max mem: 15572
[2025-01-16 03:18:53,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:18:53,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:18:54,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76473
[2025-01-16 03:18:54,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:18:54,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 630/2809]  eta: 0:20:50  lr: 0.000014  min_lr: 0.000000  loss: 3.7614 (3.7398)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (60654.5547)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1220  max mem: 15572
Epoch: [27]  [ 640/2809]  eta: 0:20:43  lr: 0.000014  min_lr: 0.000000  loss: 3.5557 (3.7356)  class_acc: 0.3750 (0.3415)  loss_scale: 65536.0000 (60730.7083)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.1004  max mem: 15572
Epoch: [27]  [ 650/2809]  eta: 0:20:35  lr: 0.000014  min_lr: 0.000000  loss: 3.5489 (3.7349)  class_acc: 0.3750 (0.3411)  loss_scale: 65536.0000 (60804.5223)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0857  max mem: 15572
Epoch: [27]  [ 660/2809]  eta: 0:20:29  lr: 0.000014  min_lr: 0.000000  loss: 3.8790 (3.7374)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (60876.1029)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.1019  max mem: 15572
Epoch: [27]  [ 670/2809]  eta: 0:20:21  lr: 0.000014  min_lr: 0.000000  loss: 3.8790 (3.7382)  class_acc: 0.2917 (0.3405)  loss_scale: 65536.0000 (60945.5499)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0943  max mem: 15572
Epoch: [27]  [ 680/2809]  eta: 0:20:16  lr: 0.000014  min_lr: 0.000000  loss: 3.6147 (3.7337)  class_acc: 0.3750 (0.3419)  loss_scale: 65536.0000 (61012.9574)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1061  max mem: 15572
Epoch: [27]  [ 690/2809]  eta: 0:20:13  lr: 0.000014  min_lr: 0.000000  loss: 3.6785 (3.7337)  class_acc: 0.4167 (0.3423)  loss_scale: 65536.0000 (61078.4139)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1940  max mem: 15572
Epoch: [27]  [ 700/2809]  eta: 0:20:07  lr: 0.000014  min_lr: 0.000000  loss: 3.8631 (3.7367)  class_acc: 0.3333 (0.3417)  loss_scale: 65536.0000 (61142.0029)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1771  max mem: 15572
Epoch: [27]  [ 710/2809]  eta: 0:20:02  lr: 0.000014  min_lr: 0.000000  loss: 3.9412 (3.7380)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (61203.8031)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1436  max mem: 15572
Epoch: [27]  [ 720/2809]  eta: 0:19:57  lr: 0.000014  min_lr: 0.000000  loss: 3.8271 (3.7355)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (61263.8890)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1521  max mem: 15572
Epoch: [27]  [ 730/2809]  eta: 0:19:49  lr: 0.000014  min_lr: 0.000000  loss: 3.6585 (3.7352)  class_acc: 0.3333 (0.3418)  loss_scale: 65536.0000 (61322.3311)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0934  max mem: 15572
Epoch: [27]  [ 740/2809]  eta: 0:19:44  lr: 0.000014  min_lr: 0.000000  loss: 3.8223 (3.7376)  class_acc: 0.2917 (0.3413)  loss_scale: 65536.0000 (61379.1957)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1278  max mem: 15572
Epoch: [27]  [ 750/2809]  eta: 0:19:39  lr: 0.000014  min_lr: 0.000000  loss: 3.8815 (3.7400)  class_acc: 0.2917 (0.3405)  loss_scale: 65536.0000 (61434.5459)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1717  max mem: 15572
[2025-01-16 03:20:07,433] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:20:07,433] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [ 760/2809]  eta: 0:19:33  lr: 0.000014  min_lr: 0.000000  loss: 3.7412 (3.7402)  class_acc: 0.2500 (0.3405)  loss_scale: 65536.0000 (61660.6781)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1203  max mem: 15572
[2025-01-16 03:20:08,864] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76605
[2025-01-16 03:20:08,864] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:20:08,864] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 770/2809]  eta: 0:19:26  lr: 0.000014  min_lr: 0.000000  loss: 3.7700 (3.7409)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (61795.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1058  max mem: 15572
Epoch: [27]  [ 780/2809]  eta: 0:19:21  lr: 0.000014  min_lr: 0.000000  loss: 3.8406 (3.7432)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (61843.8310)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1237  max mem: 15572
Epoch: [27]  [ 790/2809]  eta: 0:19:14  lr: 0.000014  min_lr: 0.000000  loss: 4.0634 (3.7464)  class_acc: 0.2500 (0.3395)  loss_scale: 65536.0000 (61890.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1145  max mem: 15572
Epoch: [27]  [ 800/2809]  eta: 0:19:10  lr: 0.000014  min_lr: 0.000000  loss: 3.7305 (3.7443)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (61936.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1502  max mem: 15572
Epoch: [27]  [ 810/2809]  eta: 0:19:04  lr: 0.000014  min_lr: 0.000000  loss: 3.6138 (3.7418)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (61980.4094)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1542  max mem: 15572
Epoch: [27]  [ 820/2809]  eta: 0:18:56  lr: 0.000014  min_lr: 0.000000  loss: 3.6691 (3.7423)  class_acc: 0.3750 (0.3404)  loss_scale: 65536.0000 (62023.7174)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0497  max mem: 15572
Epoch: [27]  [ 830/2809]  eta: 0:18:48  lr: 0.000014  min_lr: 0.000000  loss: 3.7261 (3.7418)  class_acc: 0.3333 (0.3402)  loss_scale: 65536.0000 (62065.9832)  weight_decay: 0.0500 (0.0500)  time: 0.4722  data: 0.0136  max mem: 15572
Epoch: [27]  [ 840/2809]  eta: 0:18:42  lr: 0.000014  min_lr: 0.000000  loss: 3.9326 (3.7448)  class_acc: 0.2500 (0.3390)  loss_scale: 65536.0000 (62107.2438)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0964  max mem: 15572
Epoch: [27]  [ 850/2809]  eta: 0:18:35  lr: 0.000014  min_lr: 0.000000  loss: 3.8937 (3.7455)  class_acc: 0.2917 (0.3392)  loss_scale: 65536.0000 (62147.5347)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.1126  max mem: 15572
Epoch: [27]  [ 860/2809]  eta: 0:18:30  lr: 0.000014  min_lr: 0.000000  loss: 3.8937 (3.7467)  class_acc: 0.2917 (0.3384)  loss_scale: 65536.0000 (62186.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1153  max mem: 15572
Epoch: [27]  [ 870/2809]  eta: 0:18:23  lr: 0.000014  min_lr: 0.000000  loss: 3.9490 (3.7467)  class_acc: 0.2500 (0.3380)  loss_scale: 65536.0000 (62225.3410)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1295  max mem: 15572
Epoch: [27]  [ 880/2809]  eta: 0:18:19  lr: 0.000014  min_lr: 0.000000  loss: 3.9490 (3.7478)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (62262.9194)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1356  max mem: 15572
Epoch: [27]  [ 890/2809]  eta: 0:18:11  lr: 0.000014  min_lr: 0.000000  loss: 3.9664 (3.7484)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (62299.6543)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1051  max mem: 15572
[2025-01-16 03:21:19,327] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:21:19,328] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:21:21,497] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76736
[2025-01-16 03:21:21,497] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:21:21,497] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 900/2809]  eta: 0:18:07  lr: 0.000014  min_lr: 0.000000  loss: 3.7645 (3.7469)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (62481.0477)  weight_decay: 0.0500 (0.0500)  time: 0.5565  data: 0.1120  max mem: 15572
Epoch: [27]  [ 910/2809]  eta: 0:18:03  lr: 0.000014  min_lr: 0.000000  loss: 3.7645 (3.7477)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (62514.5818)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.2086  max mem: 15572
Epoch: [27]  [ 920/2809]  eta: 0:17:58  lr: 0.000014  min_lr: 0.000000  loss: 3.7849 (3.7464)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (62547.3876)  weight_decay: 0.0500 (0.0500)  time: 0.6318  data: 0.1932  max mem: 15572
Epoch: [27]  [ 930/2809]  eta: 0:17:52  lr: 0.000014  min_lr: 0.000000  loss: 3.8033 (3.7470)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (62579.4887)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1263  max mem: 15572
Epoch: [27]  [ 940/2809]  eta: 0:17:49  lr: 0.000014  min_lr: 0.000000  loss: 3.8010 (3.7470)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (62610.9075)  weight_decay: 0.0500 (0.0500)  time: 0.6371  data: 0.1695  max mem: 15572
Epoch: [27]  [ 950/2809]  eta: 0:17:43  lr: 0.000014  min_lr: 0.000000  loss: 3.8032 (3.7480)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (62641.6656)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.1744  max mem: 15572
Epoch: [27]  [ 960/2809]  eta: 0:17:36  lr: 0.000014  min_lr: 0.000000  loss: 3.6892 (3.7478)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (62671.7836)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0482  max mem: 15572
Epoch: [27]  [ 970/2809]  eta: 0:17:28  lr: 0.000014  min_lr: 0.000000  loss: 3.6892 (3.7499)  class_acc: 0.2917 (0.3374)  loss_scale: 65536.0000 (62701.2812)  weight_decay: 0.0500 (0.0500)  time: 0.4889  data: 0.0156  max mem: 15572
Epoch: [27]  [ 980/2809]  eta: 0:17:25  lr: 0.000014  min_lr: 0.000000  loss: 4.0488 (3.7506)  class_acc: 0.2500 (0.3372)  loss_scale: 65536.0000 (62730.1774)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1356  max mem: 15572
Epoch: [27]  [ 990/2809]  eta: 0:17:19  lr: 0.000014  min_lr: 0.000000  loss: 3.9027 (3.7518)  class_acc: 0.3750 (0.3376)  loss_scale: 65536.0000 (62758.4904)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1877  max mem: 15572
Epoch: [27]  [1000/2809]  eta: 0:17:12  lr: 0.000014  min_lr: 0.000000  loss: 3.7085 (3.7506)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (62786.2378)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0984  max mem: 15572
Epoch: [27]  [1010/2809]  eta: 0:17:06  lr: 0.000014  min_lr: 0.000000  loss: 3.6710 (3.7488)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (62813.4362)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0802  max mem: 15572
Epoch: [27]  [1020/2809]  eta: 0:17:02  lr: 0.000014  min_lr: 0.000000  loss: 3.6046 (3.7499)  class_acc: 0.2917 (0.3377)  loss_scale: 65536.0000 (62840.1019)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1521  max mem: 15572
[2025-01-16 03:22:37,548] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:22:37,549] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:22:38,895] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76868
[2025-01-16 03:22:38,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:22:38,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1030/2809]  eta: 0:16:56  lr: 0.000014  min_lr: 0.000000  loss: 3.8435 (3.7489)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (63056.9467)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1491  max mem: 15572
Epoch: [27]  [1040/2809]  eta: 0:16:50  lr: 0.000014  min_lr: 0.000000  loss: 3.7338 (3.7479)  class_acc: 0.2917 (0.3379)  loss_scale: 65536.0000 (63080.7608)  weight_decay: 0.0500 (0.0500)  time: 0.5470  data: 0.0754  max mem: 15572
[2025-01-16 03:22:51,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 76890
[2025-01-16 03:22:51,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:22:51,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1050/2809]  eta: 0:16:44  lr: 0.000014  min_lr: 0.000000  loss: 3.6558 (3.7483)  class_acc: 0.3750 (0.3381)  loss_scale: 65536.0000 (62979.4101)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1073  max mem: 15572
Epoch: [27]  [1060/2809]  eta: 0:16:38  lr: 0.000014  min_lr: 0.000000  loss: 3.7690 (3.7486)  class_acc: 0.3333 (0.3384)  loss_scale: 32768.0000 (62694.6654)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1108  max mem: 15572
Epoch: [27]  [1070/2809]  eta: 0:16:33  lr: 0.000014  min_lr: 0.000000  loss: 3.7747 (3.7468)  class_acc: 0.3333 (0.3388)  loss_scale: 32768.0000 (62415.2381)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1088  max mem: 15572
Epoch: [27]  [1080/2809]  eta: 0:16:27  lr: 0.000014  min_lr: 0.000000  loss: 3.7747 (3.7479)  class_acc: 0.2917 (0.3385)  loss_scale: 32768.0000 (62140.9806)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1435  max mem: 15572
Epoch: [27]  [1090/2809]  eta: 0:16:21  lr: 0.000014  min_lr: 0.000000  loss: 3.8009 (3.7482)  class_acc: 0.2917 (0.3381)  loss_scale: 32768.0000 (61871.7507)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1254  max mem: 15572
Epoch: [27]  [1100/2809]  eta: 0:16:15  lr: 0.000014  min_lr: 0.000000  loss: 3.7761 (3.7484)  class_acc: 0.2083 (0.3375)  loss_scale: 32768.0000 (61607.4114)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1234  max mem: 15572
Epoch: [27]  [1110/2809]  eta: 0:16:10  lr: 0.000014  min_lr: 0.000000  loss: 3.7333 (3.7493)  class_acc: 0.2500 (0.3372)  loss_scale: 32768.0000 (61347.8308)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.1274  max mem: 15572
Epoch: [27]  [1120/2809]  eta: 0:16:03  lr: 0.000013  min_lr: 0.000000  loss: 3.6469 (3.7481)  class_acc: 0.3333 (0.3376)  loss_scale: 32768.0000 (61092.8814)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1047  max mem: 15572
Epoch: [27]  [1130/2809]  eta: 0:15:57  lr: 0.000013  min_lr: 0.000000  loss: 3.6282 (3.7476)  class_acc: 0.2500 (0.3372)  loss_scale: 32768.0000 (60842.4403)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0855  max mem: 15572
Epoch: [27]  [1140/2809]  eta: 0:15:50  lr: 0.000013  min_lr: 0.000000  loss: 3.8411 (3.7468)  class_acc: 0.2917 (0.3371)  loss_scale: 32768.0000 (60596.3891)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0651  max mem: 15572
Epoch: [27]  [1150/2809]  eta: 0:15:45  lr: 0.000013  min_lr: 0.000000  loss: 3.7240 (3.7468)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (60354.6134)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1247  max mem: 15572
[2025-01-16 03:23:50,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=77000, skipped=515, lr=[1.3050742284399558e-07, 1.3050742284399558e-07, 1.8643917549142228e-07, 1.8643917549142228e-07, 2.663416792734604e-07, 2.663416792734604e-07, 3.804881132478006e-07, 3.804881132478006e-07, 5.435544474968581e-07, 5.435544474968581e-07, 7.765063535669401e-07, 7.765063535669401e-07, 1.1092947908099145e-06, 1.1092947908099145e-06, 1.5847068440141638e-06, 1.5847068440141638e-06, 2.263866920020234e-06, 2.263866920020234e-06, 3.234095600028906e-06, 3.234095600028906e-06, 4.620136571469866e-06, 4.620136571469866e-06, 6.600195102099809e-06, 6.600195102099809e-06, 9.42885014585687e-06, 9.42885014585687e-06, 1.3469785922652673e-05, 1.3469785922652673e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:23:50,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=77000/global_step=77000, RunningAvgSamplesPerSec=28.55905804463561, CurrSamplesPerSec=30.720468757152666, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [27]  [1160/2809]  eta: 0:15:39  lr: 0.000013  min_lr: 0.000000  loss: 3.7240 (3.7467)  class_acc: 0.3333 (0.3369)  loss_scale: 32768.0000 (60117.0026)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1688  max mem: 15572
Epoch: [27]  [1170/2809]  eta: 0:15:34  lr: 0.000013  min_lr: 0.000000  loss: 3.7710 (3.7477)  class_acc: 0.2917 (0.3363)  loss_scale: 32768.0000 (59883.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1437  max mem: 15572
[2025-01-16 03:24:02,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:24:02,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1180/2809]  eta: 0:15:29  lr: 0.000013  min_lr: 0.000000  loss: 3.8001 (3.7483)  class_acc: 0.2917 (0.3362)  loss_scale: 32768.0000 (59792.5826)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1675  max mem: 15572
Epoch: [27]  [1190/2809]  eta: 0:15:24  lr: 0.000013  min_lr: 0.000000  loss: 3.9603 (3.7505)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (59840.8060)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.1725  max mem: 15572
Epoch: [27]  [1200/2809]  eta: 0:15:18  lr: 0.000013  min_lr: 0.000000  loss: 3.8510 (3.7490)  class_acc: 0.3333 (0.3360)  loss_scale: 65536.0000 (59888.2265)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1257  max mem: 15572
Epoch: [27]  [1210/2809]  eta: 0:15:12  lr: 0.000013  min_lr: 0.000000  loss: 3.5925 (3.7488)  class_acc: 0.3333 (0.3354)  loss_scale: 65536.0000 (59934.8637)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0859  max mem: 15572
Epoch: [27]  [1220/2809]  eta: 0:15:06  lr: 0.000013  min_lr: 0.000000  loss: 3.8658 (3.7504)  class_acc: 0.2500 (0.3353)  loss_scale: 65536.0000 (59980.7371)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1026  max mem: 15572
Epoch: [27]  [1230/2809]  eta: 0:15:00  lr: 0.000013  min_lr: 0.000000  loss: 3.9758 (3.7521)  class_acc: 0.2500 (0.3352)  loss_scale: 65536.0000 (60025.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1133  max mem: 15572
Epoch: [27]  [1240/2809]  eta: 0:14:53  lr: 0.000013  min_lr: 0.000000  loss: 4.1253 (3.7553)  class_acc: 0.2500 (0.3345)  loss_scale: 65536.0000 (60070.2659)  weight_decay: 0.0500 (0.0500)  time: 0.5088  data: 0.0672  max mem: 15572
Epoch: [27]  [1250/2809]  eta: 0:14:47  lr: 0.000013  min_lr: 0.000000  loss: 4.0918 (3.7565)  class_acc: 0.2500 (0.3339)  loss_scale: 65536.0000 (60113.9568)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0532  max mem: 15572
Epoch: [27]  [1260/2809]  eta: 0:14:43  lr: 0.000013  min_lr: 0.000000  loss: 3.9158 (3.7567)  class_acc: 0.2917 (0.3338)  loss_scale: 65536.0000 (60156.9548)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.1630  max mem: 15572
Epoch: [27]  [1270/2809]  eta: 0:14:37  lr: 0.000013  min_lr: 0.000000  loss: 3.9090 (3.7580)  class_acc: 0.2917 (0.3335)  loss_scale: 65536.0000 (60199.2762)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1902  max mem: 15572
Epoch: [27]  [1280/2809]  eta: 0:14:32  lr: 0.000013  min_lr: 0.000000  loss: 3.9696 (3.7585)  class_acc: 0.2917 (0.3332)  loss_scale: 65536.0000 (60240.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1390  max mem: 15572
Epoch: [27]  [1290/2809]  eta: 0:14:27  lr: 0.000013  min_lr: 0.000000  loss: 3.7150 (3.7563)  class_acc: 0.2917 (0.3335)  loss_scale: 65536.0000 (60281.9520)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.2049  max mem: 15572
Epoch: [27]  [1300/2809]  eta: 0:14:21  lr: 0.000013  min_lr: 0.000000  loss: 3.6985 (3.7565)  class_acc: 0.2917 (0.3332)  loss_scale: 65536.0000 (60322.3367)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1607  max mem: 15572
[2025-01-16 03:25:16,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:25:16,564] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:25:17,537] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77149
[2025-01-16 03:25:17,537] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:25:17,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1310/2809]  eta: 0:14:15  lr: 0.000013  min_lr: 0.000000  loss: 4.0468 (3.7590)  class_acc: 0.2500 (0.3327)  loss_scale: 65536.0000 (60462.0839)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0760  max mem: 15572
Epoch: [27]  [1320/2809]  eta: 0:14:09  lr: 0.000013  min_lr: 0.000000  loss: 4.0468 (3.7592)  class_acc: 0.2500 (0.3326)  loss_scale: 65536.0000 (60500.4936)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0863  max mem: 15572
Epoch: [27]  [1330/2809]  eta: 0:14:04  lr: 0.000013  min_lr: 0.000000  loss: 3.8606 (3.7609)  class_acc: 0.2917 (0.3320)  loss_scale: 65536.0000 (60538.3261)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1567  max mem: 15572
Epoch: [27]  [1340/2809]  eta: 0:13:57  lr: 0.000013  min_lr: 0.000000  loss: 3.9153 (3.7623)  class_acc: 0.2500 (0.3315)  loss_scale: 65536.0000 (60575.5943)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1433  max mem: 15572
Epoch: [27]  [1350/2809]  eta: 0:13:52  lr: 0.000013  min_lr: 0.000000  loss: 3.8877 (3.7626)  class_acc: 0.2917 (0.3317)  loss_scale: 65536.0000 (60612.3109)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0739  max mem: 15572
Epoch: [27]  [1360/2809]  eta: 0:13:46  lr: 0.000013  min_lr: 0.000000  loss: 3.8360 (3.7633)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (60648.4879)  weight_decay: 0.0500 (0.0500)  time: 0.5661  data: 0.1037  max mem: 15572
Epoch: [27]  [1370/2809]  eta: 0:13:40  lr: 0.000013  min_lr: 0.000000  loss: 3.9433 (3.7644)  class_acc: 0.2917 (0.3311)  loss_scale: 65536.0000 (60684.1371)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1066  max mem: 15572
Epoch: [27]  [1380/2809]  eta: 0:13:34  lr: 0.000013  min_lr: 0.000000  loss: 3.9714 (3.7642)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (60719.2701)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0879  max mem: 15572
Epoch: [27]  [1390/2809]  eta: 0:13:28  lr: 0.000013  min_lr: 0.000000  loss: 3.8274 (3.7647)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (60753.8979)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.0846  max mem: 15572
Epoch: [27]  [1400/2809]  eta: 0:13:22  lr: 0.000013  min_lr: 0.000000  loss: 3.8274 (3.7655)  class_acc: 0.2500 (0.3305)  loss_scale: 65536.0000 (60788.0314)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0668  max mem: 15572
Epoch: [27]  [1410/2809]  eta: 0:13:17  lr: 0.000013  min_lr: 0.000000  loss: 3.7261 (3.7658)  class_acc: 0.2500 (0.3305)  loss_scale: 65536.0000 (60821.6811)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.0935  max mem: 15572
Epoch: [27]  [1420/2809]  eta: 0:13:10  lr: 0.000013  min_lr: 0.000000  loss: 3.7228 (3.7662)  class_acc: 0.2917 (0.3302)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0785  max mem: 15572
Epoch: [27]  [1430/2809]  eta: 0:13:05  lr: 0.000013  min_lr: 0.000000  loss: 3.6895 (3.7642)  class_acc: 0.2917 (0.3305)  loss_scale: 65536.0000 (60887.5695)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0922  max mem: 15572
[2025-01-16 03:26:29,391] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:26:29,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:26:31,555] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77279
[2025-01-16 03:26:31,556] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:26:31,556] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1440/2809]  eta: 0:12:59  lr: 0.000013  min_lr: 0.000000  loss: 3.5650 (3.7641)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (60965.3074)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1768  max mem: 15572
Epoch: [27]  [1450/2809]  eta: 0:12:53  lr: 0.000013  min_lr: 0.000000  loss: 3.7399 (3.7645)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (60996.8077)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.1162  max mem: 15572
Epoch: [27]  [1460/2809]  eta: 0:12:47  lr: 0.000013  min_lr: 0.000000  loss: 3.7238 (3.7651)  class_acc: 0.2917 (0.3301)  loss_scale: 65536.0000 (61027.8768)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0701  max mem: 15572
Epoch: [27]  [1470/2809]  eta: 0:12:41  lr: 0.000013  min_lr: 0.000000  loss: 3.8083 (3.7655)  class_acc: 0.2917 (0.3300)  loss_scale: 65536.0000 (61058.5235)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1183  max mem: 15572
Epoch: [27]  [1480/2809]  eta: 0:12:36  lr: 0.000013  min_lr: 0.000000  loss: 3.8083 (3.7662)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (61088.7562)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1942  max mem: 15572
Epoch: [27]  [1490/2809]  eta: 0:12:30  lr: 0.000013  min_lr: 0.000000  loss: 3.7604 (3.7657)  class_acc: 0.2917 (0.3300)  loss_scale: 65536.0000 (61118.5835)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1354  max mem: 15572
Epoch: [27]  [1500/2809]  eta: 0:12:24  lr: 0.000013  min_lr: 0.000000  loss: 3.7547 (3.7661)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (61148.0133)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0700  max mem: 15572
Epoch: [27]  [1510/2809]  eta: 0:12:19  lr: 0.000013  min_lr: 0.000000  loss: 3.9123 (3.7677)  class_acc: 0.2917 (0.3299)  loss_scale: 65536.0000 (61177.0536)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1241  max mem: 15572
Epoch: [27]  [1520/2809]  eta: 0:12:13  lr: 0.000013  min_lr: 0.000000  loss: 3.9220 (3.7677)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (61205.7120)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.1126  max mem: 15572
Epoch: [27]  [1530/2809]  eta: 0:12:07  lr: 0.000013  min_lr: 0.000000  loss: 3.7808 (3.7668)  class_acc: 0.3333 (0.3300)  loss_scale: 65536.0000 (61233.9961)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1345  max mem: 15572
Epoch: [27]  [1540/2809]  eta: 0:12:02  lr: 0.000013  min_lr: 0.000000  loss: 3.6578 (3.7663)  class_acc: 0.3750 (0.3303)  loss_scale: 65536.0000 (61261.9130)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1577  max mem: 15572
[2025-01-16 03:27:33,272] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77389
[2025-01-16 03:27:33,272] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:27:33,272] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1550/2809]  eta: 0:11:56  lr: 0.000013  min_lr: 0.000000  loss: 3.6213 (3.7661)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (61183.8349)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1210  max mem: 15572
Epoch: [27]  [1560/2809]  eta: 0:11:50  lr: 0.000013  min_lr: 0.000000  loss: 3.9168 (3.7677)  class_acc: 0.2917 (0.3301)  loss_scale: 32768.0000 (61001.7988)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1010  max mem: 15572
Epoch: [27]  [1570/2809]  eta: 0:11:44  lr: 0.000013  min_lr: 0.000000  loss: 3.9207 (3.7683)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (60822.0802)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0834  max mem: 15572
Epoch: [27]  [1580/2809]  eta: 0:11:38  lr: 0.000013  min_lr: 0.000000  loss: 3.9207 (3.7692)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (60644.6350)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0823  max mem: 15572
Epoch: [27]  [1590/2809]  eta: 0:11:32  lr: 0.000013  min_lr: 0.000000  loss: 3.8427 (3.7686)  class_acc: 0.3333 (0.3304)  loss_scale: 32768.0000 (60469.4205)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0867  max mem: 15572
Epoch: [27]  [1600/2809]  eta: 0:11:26  lr: 0.000013  min_lr: 0.000000  loss: 3.6184 (3.7671)  class_acc: 0.3750 (0.3307)  loss_scale: 32768.0000 (60296.3948)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1110  max mem: 15572
Epoch: [27]  [1610/2809]  eta: 0:11:21  lr: 0.000013  min_lr: 0.000000  loss: 3.5243 (3.7664)  class_acc: 0.3750 (0.3311)  loss_scale: 32768.0000 (60125.5171)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1685  max mem: 15572
Epoch: [27]  [1620/2809]  eta: 0:11:16  lr: 0.000013  min_lr: 0.000000  loss: 3.6144 (3.7652)  class_acc: 0.3750 (0.3314)  loss_scale: 32768.0000 (59956.7477)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1794  max mem: 15572
Epoch: [27]  [1630/2809]  eta: 0:11:10  lr: 0.000013  min_lr: 0.000000  loss: 3.6712 (3.7654)  class_acc: 0.3750 (0.3315)  loss_scale: 32768.0000 (59790.0478)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1806  max mem: 15572
Epoch: [27]  [1640/2809]  eta: 0:11:04  lr: 0.000013  min_lr: 0.000000  loss: 3.8404 (3.7657)  class_acc: 0.3333 (0.3316)  loss_scale: 32768.0000 (59625.3796)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1384  max mem: 15572
Epoch: [27]  [1650/2809]  eta: 0:10:59  lr: 0.000013  min_lr: 0.000000  loss: 3.7096 (3.7645)  class_acc: 0.3333 (0.3318)  loss_scale: 32768.0000 (59462.7062)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1370  max mem: 15572
Epoch: [27]  [1660/2809]  eta: 0:10:53  lr: 0.000013  min_lr: 0.000000  loss: 3.6535 (3.7639)  class_acc: 0.2917 (0.3317)  loss_scale: 32768.0000 (59301.9916)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1536  max mem: 15572
Epoch: [27]  [1670/2809]  eta: 0:10:47  lr: 0.000013  min_lr: 0.000000  loss: 3.8357 (3.7636)  class_acc: 0.2500 (0.3316)  loss_scale: 32768.0000 (59143.2005)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0737  max mem: 15572
[2025-01-16 03:28:44,454] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:28:44,454] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1680/2809]  eta: 0:10:41  lr: 0.000013  min_lr: 0.000000  loss: 3.8357 (3.7636)  class_acc: 0.3333 (0.3318)  loss_scale: 32768.0000 (59103.2576)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0806  max mem: 15572
Epoch: [27]  [1690/2809]  eta: 0:10:35  lr: 0.000013  min_lr: 0.000000  loss: 3.5600 (3.7620)  class_acc: 0.3333 (0.3321)  loss_scale: 65536.0000 (59141.2986)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0922  max mem: 15572
[2025-01-16 03:28:54,404] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77536
[2025-01-16 03:28:54,404] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:28:54,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1700/2809]  eta: 0:10:30  lr: 0.000013  min_lr: 0.000000  loss: 3.4386 (3.7607)  class_acc: 0.3750 (0.3322)  loss_scale: 65536.0000 (59024.7807)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1009  max mem: 15572
Epoch: [27]  [1710/2809]  eta: 0:10:24  lr: 0.000013  min_lr: 0.000000  loss: 3.6188 (3.7612)  class_acc: 0.3750 (0.3321)  loss_scale: 32768.0000 (58871.3220)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1107  max mem: 15572
Epoch: [27]  [1720/2809]  eta: 0:10:18  lr: 0.000013  min_lr: 0.000000  loss: 3.7377 (3.7614)  class_acc: 0.3333 (0.3319)  loss_scale: 32768.0000 (58719.6467)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0998  max mem: 15572
Epoch: [27]  [1730/2809]  eta: 0:10:12  lr: 0.000013  min_lr: 0.000000  loss: 3.7594 (3.7613)  class_acc: 0.3333 (0.3319)  loss_scale: 32768.0000 (58569.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1034  max mem: 15572
Epoch: [27]  [1740/2809]  eta: 0:10:07  lr: 0.000013  min_lr: 0.000000  loss: 3.6814 (3.7612)  class_acc: 0.2917 (0.3318)  loss_scale: 32768.0000 (58421.5233)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1129  max mem: 15572
Epoch: [27]  [1750/2809]  eta: 0:10:01  lr: 0.000013  min_lr: 0.000000  loss: 3.8499 (3.7621)  class_acc: 0.2917 (0.3317)  loss_scale: 32768.0000 (58275.0154)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1085  max mem: 15572
Epoch: [27]  [1760/2809]  eta: 0:09:55  lr: 0.000013  min_lr: 0.000000  loss: 3.9139 (3.7625)  class_acc: 0.2917 (0.3315)  loss_scale: 32768.0000 (58130.1715)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0829  max mem: 15572
Epoch: [27]  [1770/2809]  eta: 0:09:49  lr: 0.000013  min_lr: 0.000000  loss: 3.8170 (3.7623)  class_acc: 0.2917 (0.3316)  loss_scale: 32768.0000 (57986.9633)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0605  max mem: 15572
Epoch: [27]  [1780/2809]  eta: 0:09:43  lr: 0.000013  min_lr: 0.000000  loss: 3.6471 (3.7609)  class_acc: 0.3750 (0.3321)  loss_scale: 32768.0000 (57845.3633)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0706  max mem: 15572
Epoch: [27]  [1790/2809]  eta: 0:09:38  lr: 0.000013  min_lr: 0.000000  loss: 3.6471 (3.7617)  class_acc: 0.2917 (0.3318)  loss_scale: 32768.0000 (57705.3445)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1135  max mem: 15572
Epoch: [27]  [1800/2809]  eta: 0:09:32  lr: 0.000013  min_lr: 0.000000  loss: 3.8722 (3.7608)  class_acc: 0.2917 (0.3319)  loss_scale: 32768.0000 (57566.8806)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1341  max mem: 15572
Epoch: [27]  [1810/2809]  eta: 0:09:26  lr: 0.000013  min_lr: 0.000000  loss: 3.7778 (3.7605)  class_acc: 0.2917 (0.3320)  loss_scale: 32768.0000 (57429.9459)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1386  max mem: 15572
Epoch: [27]  [1820/2809]  eta: 0:09:21  lr: 0.000013  min_lr: 0.000000  loss: 3.7422 (3.7600)  class_acc: 0.3333 (0.3323)  loss_scale: 32768.0000 (57294.5151)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1210  max mem: 15572
[2025-01-16 03:30:06,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:30:06,933] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [1830/2809]  eta: 0:09:15  lr: 0.000013  min_lr: 0.000000  loss: 3.6972 (3.7597)  class_acc: 0.3750 (0.3323)  loss_scale: 32768.0000 (57321.6297)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1026  max mem: 15572
Epoch: [27]  [1840/2809]  eta: 0:09:10  lr: 0.000013  min_lr: 0.000000  loss: 3.7965 (3.7599)  class_acc: 0.3333 (0.3323)  loss_scale: 65536.0000 (57366.2488)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1360  max mem: 15572
Epoch: [27]  [1850/2809]  eta: 0:09:04  lr: 0.000013  min_lr: 0.000000  loss: 3.7551 (3.7579)  class_acc: 0.3333 (0.3325)  loss_scale: 65536.0000 (57410.3857)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1834  max mem: 15572
Epoch: [27]  [1860/2809]  eta: 0:08:58  lr: 0.000013  min_lr: 0.000000  loss: 3.5937 (3.7578)  class_acc: 0.3750 (0.3326)  loss_scale: 65536.0000 (57454.0484)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1412  max mem: 15572
Epoch: [27]  [1870/2809]  eta: 0:08:53  lr: 0.000013  min_lr: 0.000000  loss: 3.6822 (3.7569)  class_acc: 0.3750 (0.3327)  loss_scale: 65536.0000 (57497.2443)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.1168  max mem: 15572
Epoch: [27]  [1880/2809]  eta: 0:08:47  lr: 0.000013  min_lr: 0.000000  loss: 3.7629 (3.7568)  class_acc: 0.3333 (0.3327)  loss_scale: 65536.0000 (57539.9809)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1877  max mem: 15572
Epoch: [27]  [1890/2809]  eta: 0:08:41  lr: 0.000013  min_lr: 0.000000  loss: 3.8435 (3.7552)  class_acc: 0.3333 (0.3328)  loss_scale: 65536.0000 (57582.2655)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1170  max mem: 15572
[2025-01-16 03:30:49,666] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 77740
[2025-01-16 03:30:49,667] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:30:49,667] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [1900/2809]  eta: 0:08:35  lr: 0.000013  min_lr: 0.000000  loss: 3.7055 (3.7564)  class_acc: 0.2500 (0.3323)  loss_scale: 65536.0000 (57555.1562)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0401  max mem: 15572
Epoch: [27]  [1910/2809]  eta: 0:08:30  lr: 0.000013  min_lr: 0.000000  loss: 3.7055 (3.7556)  class_acc: 0.2500 (0.3323)  loss_scale: 32768.0000 (57425.4485)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1019  max mem: 15572
Epoch: [27]  [1920/2809]  eta: 0:08:24  lr: 0.000013  min_lr: 0.000000  loss: 3.8339 (3.7566)  class_acc: 0.2917 (0.3321)  loss_scale: 32768.0000 (57297.0911)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1434  max mem: 15572
Epoch: [27]  [1930/2809]  eta: 0:08:18  lr: 0.000013  min_lr: 0.000000  loss: 4.0496 (3.7588)  class_acc: 0.2500 (0.3316)  loss_scale: 32768.0000 (57170.0632)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1467  max mem: 15572
Epoch: [27]  [1940/2809]  eta: 0:08:13  lr: 0.000013  min_lr: 0.000000  loss: 3.8292 (3.7579)  class_acc: 0.3333 (0.3320)  loss_scale: 32768.0000 (57044.3442)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1054  max mem: 15572
Epoch: [27]  [1950/2809]  eta: 0:08:07  lr: 0.000013  min_lr: 0.000000  loss: 3.6438 (3.7577)  class_acc: 0.3333 (0.3322)  loss_scale: 32768.0000 (56919.9139)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0650  max mem: 15572
Epoch: [27]  [1960/2809]  eta: 0:08:01  lr: 0.000013  min_lr: 0.000000  loss: 3.9845 (3.7582)  class_acc: 0.2917 (0.3321)  loss_scale: 32768.0000 (56796.7527)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0955  max mem: 15572
Epoch: [27]  [1970/2809]  eta: 0:07:55  lr: 0.000013  min_lr: 0.000000  loss: 3.7012 (3.7573)  class_acc: 0.3333 (0.3323)  loss_scale: 32768.0000 (56674.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1298  max mem: 15572
Epoch: [27]  [1980/2809]  eta: 0:07:50  lr: 0.000013  min_lr: 0.000000  loss: 3.7012 (3.7582)  class_acc: 0.2917 (0.3318)  loss_scale: 32768.0000 (56554.1605)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1474  max mem: 15572
Epoch: [27]  [1990/2809]  eta: 0:07:44  lr: 0.000013  min_lr: 0.000000  loss: 3.9056 (3.7581)  class_acc: 0.2500 (0.3316)  loss_scale: 32768.0000 (56434.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1209  max mem: 15572
Epoch: [27]  [2000/2809]  eta: 0:07:39  lr: 0.000013  min_lr: 0.000000  loss: 3.7759 (3.7575)  class_acc: 0.2917 (0.3316)  loss_scale: 32768.0000 (56316.4178)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1112  max mem: 15572
Epoch: [27]  [2010/2809]  eta: 0:07:33  lr: 0.000013  min_lr: 0.000000  loss: 3.7966 (3.7582)  class_acc: 0.3333 (0.3316)  loss_scale: 32768.0000 (56199.3197)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1016  max mem: 15572
Epoch: [27]  [2020/2809]  eta: 0:07:27  lr: 0.000013  min_lr: 0.000000  loss: 3.9067 (3.7589)  class_acc: 0.2917 (0.3315)  loss_scale: 32768.0000 (56083.3805)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1386  max mem: 15572
[2025-01-16 03:32:02,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:32:02,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2030/2809]  eta: 0:07:21  lr: 0.000013  min_lr: 0.000000  loss: 4.0902 (3.7600)  class_acc: 0.2917 (0.3312)  loss_scale: 32768.0000 (56049.2526)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1191  max mem: 15572
Epoch: [27]  [2040/2809]  eta: 0:07:16  lr: 0.000013  min_lr: 0.000000  loss: 3.9288 (3.7598)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (56095.7335)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0719  max mem: 15572
Epoch: [27]  [2050/2809]  eta: 0:07:10  lr: 0.000013  min_lr: 0.000000  loss: 3.6367 (3.7597)  class_acc: 0.4167 (0.3316)  loss_scale: 65536.0000 (56141.7611)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1077  max mem: 15572
Epoch: [27]  [2060/2809]  eta: 0:07:04  lr: 0.000013  min_lr: 0.000000  loss: 3.8827 (3.7603)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (56187.3421)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0929  max mem: 15572
Epoch: [27]  [2070/2809]  eta: 0:06:59  lr: 0.000013  min_lr: 0.000000  loss: 3.8841 (3.7601)  class_acc: 0.2500 (0.3312)  loss_scale: 65536.0000 (56232.4829)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1292  max mem: 15572
Epoch: [27]  [2080/2809]  eta: 0:06:53  lr: 0.000013  min_lr: 0.000000  loss: 3.7762 (3.7604)  class_acc: 0.2500 (0.3310)  loss_scale: 65536.0000 (56277.1898)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1281  max mem: 15572
Epoch: [27]  [2090/2809]  eta: 0:06:47  lr: 0.000013  min_lr: 0.000000  loss: 4.0451 (3.7618)  class_acc: 0.2500 (0.3308)  loss_scale: 65536.0000 (56321.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.0921  max mem: 15572
Epoch: [27]  [2100/2809]  eta: 0:06:41  lr: 0.000013  min_lr: 0.000000  loss: 4.0451 (3.7619)  class_acc: 0.3333 (0.3309)  loss_scale: 65536.0000 (56365.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0663  max mem: 15572
Epoch: [27]  [2110/2809]  eta: 0:06:36  lr: 0.000013  min_lr: 0.000000  loss: 3.7484 (3.7618)  class_acc: 0.3333 (0.3308)  loss_scale: 65536.0000 (56408.7693)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0812  max mem: 15572
Epoch: [27]  [2120/2809]  eta: 0:06:30  lr: 0.000013  min_lr: 0.000000  loss: 3.5999 (3.7623)  class_acc: 0.3333 (0.3308)  loss_scale: 65536.0000 (56451.8020)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1438  max mem: 15572
Epoch: [27]  [2130/2809]  eta: 0:06:25  lr: 0.000013  min_lr: 0.000000  loss: 3.6817 (3.7618)  class_acc: 0.4167 (0.3310)  loss_scale: 65536.0000 (56494.4308)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1632  max mem: 15572
Epoch: [27]  [2140/2809]  eta: 0:06:19  lr: 0.000013  min_lr: 0.000000  loss: 3.6817 (3.7618)  class_acc: 0.4167 (0.3313)  loss_scale: 65536.0000 (56536.6614)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0983  max mem: 15572
Epoch: [27]  [2150/2809]  eta: 0:06:13  lr: 0.000013  min_lr: 0.000000  loss: 3.6204 (3.7613)  class_acc: 0.2917 (0.3312)  loss_scale: 65536.0000 (56578.4993)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0833  max mem: 15572
[2025-01-16 03:33:14,001] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:33:14,002] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:33:17,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=78000, skipped=520, lr=[1.2400201441221816e-07, 1.2400201441221816e-07, 1.7714573487459742e-07, 1.7714573487459742e-07, 2.5306533553513916e-07, 2.5306533553513916e-07, 3.615219079073417e-07, 3.615219079073417e-07, 5.164598684390596e-07, 5.164598684390596e-07, 7.377998120557995e-07, 7.377998120557995e-07, 1.053999731508285e-06, 1.053999731508285e-06, 1.505713902154693e-06, 1.505713902154693e-06, 2.15101986022099e-06, 2.15101986022099e-06, 3.0728855146014146e-06, 3.0728855146014146e-06, 4.389836449430592e-06, 4.389836449430592e-06, 6.2711949277579896e-06, 6.2711949277579896e-06, 8.95884989679713e-06, 8.95884989679713e-06, 1.279835699542447e-05, 1.279835699542447e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:33:17,488] [INFO] [timer.py:260:stop] epoch=0/micro_step=78000/global_step=78000, RunningAvgSamplesPerSec=28.55734489155655, CurrSamplesPerSec=28.682678975937087, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 03:33:18,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78002
[2025-01-16 03:33:18,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:33:18,672] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 03:33:19,059] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78003
[2025-01-16 03:33:19,059] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:33:19,059] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2160/2809]  eta: 0:06:08  lr: 0.000013  min_lr: 0.000000  loss: 3.5027 (3.7611)  class_acc: 0.2917 (0.3314)  loss_scale: 65536.0000 (56756.4202)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1830  max mem: 15572
Epoch: [27]  [2170/2809]  eta: 0:06:02  lr: 0.000013  min_lr: 0.000000  loss: 3.6614 (3.7608)  class_acc: 0.3333 (0.3314)  loss_scale: 32768.0000 (56645.9254)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.1742  max mem: 15572
Epoch: [27]  [2180/2809]  eta: 0:05:56  lr: 0.000013  min_lr: 0.000000  loss: 3.7084 (3.7609)  class_acc: 0.2917 (0.3312)  loss_scale: 32768.0000 (56536.4438)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0736  max mem: 15572
Epoch: [27]  [2190/2809]  eta: 0:05:50  lr: 0.000013  min_lr: 0.000000  loss: 3.9415 (3.7614)  class_acc: 0.2500 (0.3309)  loss_scale: 32768.0000 (56427.9617)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0853  max mem: 15572
Epoch: [27]  [2200/2809]  eta: 0:05:45  lr: 0.000013  min_lr: 0.000000  loss: 3.8885 (3.7612)  class_acc: 0.2500 (0.3309)  loss_scale: 32768.0000 (56320.4652)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1321  max mem: 15572
Epoch: [27]  [2210/2809]  eta: 0:05:39  lr: 0.000013  min_lr: 0.000000  loss: 3.6655 (3.7609)  class_acc: 0.3333 (0.3308)  loss_scale: 32768.0000 (56213.9412)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.1660  max mem: 15572
Epoch: [27]  [2220/2809]  eta: 0:05:33  lr: 0.000013  min_lr: 0.000000  loss: 3.7818 (3.7616)  class_acc: 0.3333 (0.3308)  loss_scale: 32768.0000 (56108.3764)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1248  max mem: 15572
Epoch: [27]  [2230/2809]  eta: 0:05:28  lr: 0.000013  min_lr: 0.000000  loss: 3.6631 (3.7608)  class_acc: 0.3333 (0.3309)  loss_scale: 32768.0000 (56003.7580)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1261  max mem: 15572
Epoch: [27]  [2240/2809]  eta: 0:05:22  lr: 0.000013  min_lr: 0.000000  loss: 3.6517 (3.7615)  class_acc: 0.3333 (0.3307)  loss_scale: 32768.0000 (55900.0732)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1667  max mem: 15572
Epoch: [27]  [2250/2809]  eta: 0:05:17  lr: 0.000013  min_lr: 0.000000  loss: 3.9791 (3.7621)  class_acc: 0.2917 (0.3307)  loss_scale: 32768.0000 (55797.3096)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1248  max mem: 15572
Epoch: [27]  [2260/2809]  eta: 0:05:11  lr: 0.000013  min_lr: 0.000000  loss: 3.9324 (3.7630)  class_acc: 0.2500 (0.3303)  loss_scale: 32768.0000 (55695.4551)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0869  max mem: 15572
Epoch: [27]  [2270/2809]  eta: 0:05:06  lr: 0.000013  min_lr: 0.000000  loss: 3.7795 (3.7632)  class_acc: 0.2917 (0.3305)  loss_scale: 32768.0000 (55594.4976)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.1677  max mem: 15572
Epoch: [27]  [2280/2809]  eta: 0:05:00  lr: 0.000013  min_lr: 0.000000  loss: 3.6952 (3.7633)  class_acc: 0.3750 (0.3305)  loss_scale: 32768.0000 (55494.4253)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1863  max mem: 15572
[2025-01-16 03:34:32,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:34:32,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2290/2809]  eta: 0:04:54  lr: 0.000013  min_lr: 0.000000  loss: 3.7043 (3.7632)  class_acc: 0.3333 (0.3305)  loss_scale: 32768.0000 (55423.8324)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1168  max mem: 15572
Epoch: [27]  [2300/2809]  eta: 0:04:48  lr: 0.000013  min_lr: 0.000000  loss: 3.8484 (3.7639)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (55467.7792)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0927  max mem: 15572
Epoch: [27]  [2310/2809]  eta: 0:04:43  lr: 0.000013  min_lr: 0.000000  loss: 3.8589 (3.7644)  class_acc: 0.2500 (0.3303)  loss_scale: 65536.0000 (55511.3457)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1113  max mem: 15572
Epoch: [27]  [2320/2809]  eta: 0:04:37  lr: 0.000013  min_lr: 0.000000  loss: 3.6291 (3.7644)  class_acc: 0.2500 (0.3304)  loss_scale: 65536.0000 (55554.5368)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1419  max mem: 15572
Epoch: [27]  [2330/2809]  eta: 0:04:31  lr: 0.000013  min_lr: 0.000000  loss: 3.7356 (3.7650)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (55597.3574)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1283  max mem: 15572
Epoch: [27]  [2340/2809]  eta: 0:04:26  lr: 0.000013  min_lr: 0.000000  loss: 3.8245 (3.7654)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (55639.8120)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1563  max mem: 15572
Epoch: [27]  [2350/2809]  eta: 0:04:20  lr: 0.000013  min_lr: 0.000000  loss: 3.8830 (3.7661)  class_acc: 0.2500 (0.3300)  loss_scale: 65536.0000 (55681.9056)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1413  max mem: 15572
Epoch: [27]  [2360/2809]  eta: 0:04:14  lr: 0.000013  min_lr: 0.000000  loss: 3.9518 (3.7658)  class_acc: 0.2500 (0.3300)  loss_scale: 65536.0000 (55723.6425)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0568  max mem: 15572
Epoch: [27]  [2370/2809]  eta: 0:04:09  lr: 0.000013  min_lr: 0.000000  loss: 3.8887 (3.7665)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (55765.0274)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0803  max mem: 15572
Epoch: [27]  [2380/2809]  eta: 0:04:03  lr: 0.000013  min_lr: 0.000000  loss: 3.8036 (3.7670)  class_acc: 0.3333 (0.3298)  loss_scale: 65536.0000 (55806.0647)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1696  max mem: 15572
Epoch: [27]  [2390/2809]  eta: 0:03:57  lr: 0.000013  min_lr: 0.000000  loss: 3.7040 (3.7664)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (55846.7587)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1605  max mem: 15572
Epoch: [27]  [2400/2809]  eta: 0:03:52  lr: 0.000013  min_lr: 0.000000  loss: 3.6592 (3.7666)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (55887.1137)  weight_decay: 0.0500 (0.0500)  time: 0.5403  data: 0.1047  max mem: 15572
Epoch: [27]  [2410/2809]  eta: 0:03:46  lr: 0.000013  min_lr: 0.000000  loss: 3.6264 (3.7647)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (55927.1340)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0892  max mem: 15572
[2025-01-16 03:35:44,902] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:35:44,902] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:35:45,360] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78261
[2025-01-16 03:35:45,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:35:45,361] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2420/2809]  eta: 0:03:40  lr: 0.000013  min_lr: 0.000000  loss: 3.5219 (3.7646)  class_acc: 0.3750 (0.3301)  loss_scale: 65536.0000 (55993.8934)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.1000  max mem: 15572
Epoch: [27]  [2430/2809]  eta: 0:03:34  lr: 0.000013  min_lr: 0.000000  loss: 3.5867 (3.7636)  class_acc: 0.3750 (0.3303)  loss_scale: 65536.0000 (56033.1452)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0749  max mem: 15572
Epoch: [27]  [2440/2809]  eta: 0:03:29  lr: 0.000013  min_lr: 0.000000  loss: 3.6447 (3.7637)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (56072.0754)  weight_decay: 0.0500 (0.0500)  time: 0.5028  data: 0.0545  max mem: 15572
Epoch: [27]  [2450/2809]  eta: 0:03:23  lr: 0.000013  min_lr: 0.000000  loss: 3.8263 (3.7635)  class_acc: 0.3333 (0.3304)  loss_scale: 65536.0000 (56110.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0959  max mem: 15572
Epoch: [27]  [2460/2809]  eta: 0:03:18  lr: 0.000013  min_lr: 0.000000  loss: 3.8263 (3.7640)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (56148.9866)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.2012  max mem: 15572
Epoch: [27]  [2470/2809]  eta: 0:03:12  lr: 0.000013  min_lr: 0.000000  loss: 3.8872 (3.7638)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (56186.9753)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1923  max mem: 15572
[2025-01-16 03:36:19,093] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78319
[2025-01-16 03:36:19,093] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:36:19,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [27]  [2480/2809]  eta: 0:03:06  lr: 0.000013  min_lr: 0.000000  loss: 3.8062 (3.7638)  class_acc: 0.2917 (0.3304)  loss_scale: 65536.0000 (56158.6199)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1332  max mem: 15572
Epoch: [27]  [2490/2809]  eta: 0:03:01  lr: 0.000013  min_lr: 0.000000  loss: 3.7560 (3.7637)  class_acc: 0.2917 (0.3305)  loss_scale: 32768.0000 (56064.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1654  max mem: 15572
Epoch: [27]  [2500/2809]  eta: 0:02:55  lr: 0.000013  min_lr: 0.000000  loss: 3.6684 (3.7629)  class_acc: 0.4167 (0.3309)  loss_scale: 32768.0000 (55971.5698)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1481  max mem: 15572
Epoch: [27]  [2510/2809]  eta: 0:02:49  lr: 0.000013  min_lr: 0.000000  loss: 3.5965 (3.7623)  class_acc: 0.4167 (0.3309)  loss_scale: 32768.0000 (55879.1621)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0878  max mem: 15572
Epoch: [27]  [2520/2809]  eta: 0:02:44  lr: 0.000013  min_lr: 0.000000  loss: 3.7390 (3.7630)  class_acc: 0.2500 (0.3307)  loss_scale: 32768.0000 (55787.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0992  max mem: 15572
Epoch: [27]  [2530/2809]  eta: 0:02:38  lr: 0.000013  min_lr: 0.000000  loss: 3.9755 (3.7634)  class_acc: 0.2500 (0.3306)  loss_scale: 32768.0000 (55696.5373)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1169  max mem: 15572
Epoch: [27]  [2540/2809]  eta: 0:02:32  lr: 0.000013  min_lr: 0.000000  loss: 3.8508 (3.7635)  class_acc: 0.3333 (0.3307)  loss_scale: 32768.0000 (55606.3030)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.0976  max mem: 15572
Epoch: [27]  [2550/2809]  eta: 0:02:27  lr: 0.000013  min_lr: 0.000000  loss: 3.8474 (3.7635)  class_acc: 0.2917 (0.3307)  loss_scale: 32768.0000 (55516.7762)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1173  max mem: 15572
Epoch: [27]  [2560/2809]  eta: 0:02:21  lr: 0.000013  min_lr: 0.000000  loss: 3.6714 (3.7624)  class_acc: 0.2917 (0.3310)  loss_scale: 32768.0000 (55427.9485)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1087  max mem: 15572
Epoch: [27]  [2570/2809]  eta: 0:02:15  lr: 0.000013  min_lr: 0.000000  loss: 3.7533 (3.7626)  class_acc: 0.3333 (0.3311)  loss_scale: 32768.0000 (55339.8117)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0808  max mem: 15572
Epoch: [27]  [2580/2809]  eta: 0:02:09  lr: 0.000013  min_lr: 0.000000  loss: 3.8014 (3.7625)  class_acc: 0.3333 (0.3312)  loss_scale: 32768.0000 (55252.3580)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0748  max mem: 15572
Epoch: [27]  [2590/2809]  eta: 0:02:04  lr: 0.000013  min_lr: 0.000000  loss: 3.4755 (3.7615)  class_acc: 0.3333 (0.3314)  loss_scale: 32768.0000 (55165.5793)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0749  max mem: 15572
Epoch: [27]  [2600/2809]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 3.6930 (3.7618)  class_acc: 0.3333 (0.3313)  loss_scale: 32768.0000 (55079.4679)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1056  max mem: 15572
[2025-01-16 03:37:32,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:37:32,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [27]  [2610/2809]  eta: 0:01:52  lr: 0.000012  min_lr: 0.000000  loss: 4.0634 (3.7620)  class_acc: 0.2500 (0.3313)  loss_scale: 32768.0000 (55069.3160)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1527  max mem: 15572
Epoch: [27]  [2620/2809]  eta: 0:01:47  lr: 0.000012  min_lr: 0.000000  loss: 3.7729 (3.7615)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (55109.2499)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1535  max mem: 15572
Epoch: [27]  [2630/2809]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000000  loss: 3.7613 (3.7611)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (55148.8803)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1394  max mem: 15572
Epoch: [27]  [2640/2809]  eta: 0:01:35  lr: 0.000012  min_lr: 0.000000  loss: 3.7309 (3.7612)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (55188.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1107  max mem: 15572
Epoch: [27]  [2650/2809]  eta: 0:01:30  lr: 0.000012  min_lr: 0.000000  loss: 3.7609 (3.7617)  class_acc: 0.2917 (0.3312)  loss_scale: 65536.0000 (55227.2441)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0993  max mem: 15572
Epoch: [27]  [2660/2809]  eta: 0:01:24  lr: 0.000012  min_lr: 0.000000  loss: 3.9379 (3.7626)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (55265.9842)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1259  max mem: 15572
Epoch: [27]  [2670/2809]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000000  loss: 3.9378 (3.7624)  class_acc: 0.3333 (0.3312)  loss_scale: 65536.0000 (55304.4343)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0994  max mem: 15572
Epoch: [27]  [2680/2809]  eta: 0:01:13  lr: 0.000012  min_lr: 0.000000  loss: 3.6796 (3.7622)  class_acc: 0.3333 (0.3312)  loss_scale: 65536.0000 (55342.5975)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.1256  max mem: 15572
Epoch: [27]  [2690/2809]  eta: 0:01:07  lr: 0.000012  min_lr: 0.000000  loss: 3.7765 (3.7626)  class_acc: 0.3333 (0.3312)  loss_scale: 65536.0000 (55380.4771)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1024  max mem: 15572
Epoch: [27]  [2700/2809]  eta: 0:01:01  lr: 0.000012  min_lr: 0.000000  loss: 4.0953 (3.7634)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (55418.0763)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.0674  max mem: 15572
Epoch: [27]  [2710/2809]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 4.0953 (3.7639)  class_acc: 0.2500 (0.3310)  loss_scale: 65536.0000 (55455.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.0595  max mem: 15572
Epoch: [27]  [2720/2809]  eta: 0:00:50  lr: 0.000012  min_lr: 0.000000  loss: 3.9259 (3.7644)  class_acc: 0.2500 (0.3307)  loss_scale: 65536.0000 (55492.4454)  weight_decay: 0.0500 (0.0500)  time: 0.5201  data: 0.0568  max mem: 15572
Epoch: [27]  [2730/2809]  eta: 0:00:44  lr: 0.000012  min_lr: 0.000000  loss: 3.8412 (3.7645)  class_acc: 0.2917 (0.3306)  loss_scale: 65536.0000 (55529.2215)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1492  max mem: 15572
[2025-01-16 03:38:44,875] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:38:44,876] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:38:47,922] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78581
[2025-01-16 03:38:47,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:38:47,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [2740/2809]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000000  loss: 4.0295 (3.7660)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (55685.2769)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1399  max mem: 15572
Epoch: [27]  [2750/2809]  eta: 0:00:33  lr: 0.000012  min_lr: 0.000000  loss: 4.0101 (3.7663)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (55721.0847)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.0929  max mem: 15572
Epoch: [27]  [2760/2809]  eta: 0:00:27  lr: 0.000012  min_lr: 0.000000  loss: 3.8307 (3.7666)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (55756.6331)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1301  max mem: 15572
Epoch: [27]  [2770/2809]  eta: 0:00:22  lr: 0.000012  min_lr: 0.000000  loss: 3.8622 (3.7670)  class_acc: 0.2917 (0.3301)  loss_scale: 65536.0000 (55791.9249)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.1820  max mem: 15572
Epoch: [27]  [2780/2809]  eta: 0:00:16  lr: 0.000012  min_lr: 0.000000  loss: 3.8442 (3.7669)  class_acc: 0.2917 (0.3301)  loss_scale: 65536.0000 (55826.9630)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.2457  max mem: 15572
Epoch: [27]  [2790/2809]  eta: 0:00:10  lr: 0.000012  min_lr: 0.000000  loss: 3.8762 (3.7672)  class_acc: 0.2917 (0.3298)  loss_scale: 65536.0000 (55861.7499)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1532  max mem: 15572
Epoch: [27]  [2800/2809]  eta: 0:00:05  lr: 0.000012  min_lr: 0.000000  loss: 3.9389 (3.7679)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (55896.2885)  weight_decay: 0.0500 (0.0500)  time: 0.4663  data: 0.0393  max mem: 15572
Epoch: [27]  [2808/2809]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 3.8890 (3.7681)  class_acc: 0.2917 (0.3298)  loss_scale: 65536.0000 (55923.7423)  weight_decay: 0.0500 (0.0500)  time: 0.4250  data: 0.0268  max mem: 15572
Epoch: [27] Total time: 0:26:35 (0.5679 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 3.8890 (3.7681)  class_acc: 0.2917 (0.3298)  loss_scale: 65536.0000 (55923.7423)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:15:36  loss: 0.3100 (0.3100)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.4417  data: 3.2753  max mem: 15572
Val:  [ 10/272]  eta: 0:02:56  loss: 2.2612 (2.2547)  acc1: 44.4444 (41.9192)  acc5: 77.7778 (73.2323)  time: 0.6739  data: 0.4769  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.2612 (2.3244)  acc1: 44.4444 (45.5026)  acc5: 77.7778 (73.2804)  time: 0.3535  data: 0.1611  max mem: 15572
Val:  [ 30/272]  eta: 0:01:46  loss: 2.4702 (2.4186)  acc1: 44.4444 (42.4731)  acc5: 72.2222 (72.9391)  time: 0.3083  data: 0.1117  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 2.5312 (2.4555)  acc1: 27.7778 (40.5149)  acc5: 72.2222 (73.0352)  time: 0.3356  data: 0.1262  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 2.3897 (2.3781)  acc1: 33.3333 (42.3747)  acc5: 72.2222 (74.7277)  time: 0.3322  data: 0.1220  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.5737 (2.2744)  acc1: 61.1111 (45.4463)  acc5: 88.8889 (75.8652)  time: 0.2969  data: 0.0902  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.5896 (2.2021)  acc1: 66.6667 (47.7308)  acc5: 83.3333 (76.6823)  time: 0.3299  data: 0.1160  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.9154 (2.2117)  acc1: 55.5556 (47.5309)  acc5: 77.7778 (76.4060)  time: 0.3470  data: 0.1318  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.0324 (2.2071)  acc1: 55.5556 (47.9853)  acc5: 77.7778 (77.1673)  time: 0.2638  data: 0.0620  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.0362 (2.2356)  acc1: 55.5556 (47.2497)  acc5: 83.3333 (76.8977)  time: 0.2577  data: 0.0610  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.5696 (2.3073)  acc1: 22.2222 (44.9950)  acc5: 72.2222 (75.8759)  time: 0.3055  data: 0.1082  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.7731 (2.3456)  acc1: 22.2222 (44.2608)  acc5: 72.2222 (75.2984)  time: 0.3164  data: 0.1201  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.1409 (2.3115)  acc1: 50.0000 (45.2926)  acc5: 77.7778 (75.9118)  time: 0.3491  data: 0.1490  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.7192 (2.3053)  acc1: 61.1111 (45.7053)  acc5: 77.7778 (75.7683)  time: 0.3335  data: 0.1332  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.4212 (2.3139)  acc1: 33.3333 (45.1067)  acc5: 72.2222 (75.9014)  time: 0.3354  data: 0.1363  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.4117 (2.3052)  acc1: 44.4444 (45.4796)  acc5: 77.7778 (76.1215)  time: 0.3341  data: 0.1381  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4117 (2.3258)  acc1: 44.4444 (44.8343)  acc5: 72.2222 (75.6985)  time: 0.3129  data: 0.1035  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.3835 (2.3157)  acc1: 38.8889 (44.9355)  acc5: 72.2222 (75.9055)  time: 0.3309  data: 0.0981  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.4625 (2.3701)  acc1: 38.8889 (43.8336)  acc5: 66.6667 (74.5201)  time: 0.3530  data: 0.1325  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.5544 (2.3747)  acc1: 38.8889 (43.6153)  acc5: 72.2222 (74.4887)  time: 0.3447  data: 0.1482  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.1522 (2.3783)  acc1: 50.0000 (43.7072)  acc5: 77.7778 (74.3813)  time: 0.2599  data: 0.0733  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.0585 (2.3675)  acc1: 50.0000 (43.9165)  acc5: 72.2222 (74.4344)  time: 0.2333  data: 0.0339  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.8737 (2.3400)  acc1: 61.1111 (44.8533)  acc5: 77.7778 (74.6994)  time: 0.3173  data: 0.1101  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6742 (2.3243)  acc1: 55.5556 (45.0438)  acc5: 83.3333 (75.0576)  time: 0.3757  data: 0.1599  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2554 (2.3354)  acc1: 38.8889 (44.3780)  acc5: 77.7778 (75.0111)  time: 0.3375  data: 0.1301  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1836 (2.2762)  acc1: 72.2222 (46.0834)  acc5: 88.8889 (75.7769)  time: 0.3175  data: 0.1416  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2866 (2.2725)  acc1: 72.2222 (46.0845)  acc5: 88.8889 (75.9328)  time: 0.2540  data: 0.0905  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2866 (2.2768)  acc1: 66.6667 (46.0578)  acc5: 88.8889 (75.9164)  time: 0.1586  data: 0.0003  max mem: 15572
Val: Total time: 0:01:29 (0.3274 s / it)
* Acc@1 46.058 Acc@5 75.916 loss 2.277
Accuracy of the network on the 4883 val videos: 46.1%
[2025-01-16 03:40:56,080] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 03:40:56,084] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 03:40:56,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 03:40:58,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 03:40:58,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.06%
Epoch: [28]  [   0/2809]  eta: 2:45:15  lr: 0.000012  min_lr: 0.000000  loss: 3.9390 (3.9390)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.5299  data: 3.1258  max mem: 15572
Epoch: [28]  [  10/2809]  eta: 0:37:27  lr: 0.000012  min_lr: 0.000000  loss: 3.7872 (3.6650)  class_acc: 0.3333 (0.3485)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8031  data: 0.3816  max mem: 15572
Epoch: [28]  [  20/2809]  eta: 0:29:51  lr: 0.000012  min_lr: 0.000000  loss: 3.7061 (3.7176)  class_acc: 0.3333 (0.3274)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4979  data: 0.0541  max mem: 15572
Epoch: [28]  [  30/2809]  eta: 0:30:13  lr: 0.000012  min_lr: 0.000000  loss: 3.6379 (3.7062)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1021  max mem: 15572
Epoch: [28]  [  40/2809]  eta: 0:30:37  lr: 0.000012  min_lr: 0.000000  loss: 3.5870 (3.6893)  class_acc: 0.3750 (0.3465)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.2270  max mem: 15572
Epoch: [28]  [  50/2809]  eta: 0:31:05  lr: 0.000012  min_lr: 0.000000  loss: 3.7448 (3.7119)  class_acc: 0.3333 (0.3382)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.2502  max mem: 15572
[2025-01-16 03:41:37,065] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:41:37,065] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:41:37,501] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78711
[2025-01-16 03:41:37,501] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:41:37,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [  60/2809]  eta: 0:30:43  lr: 0.000012  min_lr: 0.000000  loss: 3.7555 (3.7125)  class_acc: 0.3333 (0.3313)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6848  data: 0.2031  max mem: 15572
Epoch: [28]  [  70/2809]  eta: 0:30:06  lr: 0.000012  min_lr: 0.000000  loss: 3.6817 (3.7247)  class_acc: 0.2917 (0.3222)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6177  data: 0.1591  max mem: 15572
Epoch: [28]  [  80/2809]  eta: 0:30:15  lr: 0.000012  min_lr: 0.000000  loss: 3.8220 (3.7489)  class_acc: 0.2917 (0.3169)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6490  data: 0.2108  max mem: 15572
Epoch: [28]  [  90/2809]  eta: 0:29:48  lr: 0.000012  min_lr: 0.000000  loss: 3.8220 (3.7339)  class_acc: 0.3333 (0.3242)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.2017  max mem: 15572
Epoch: [28]  [ 100/2809]  eta: 0:29:28  lr: 0.000012  min_lr: 0.000000  loss: 3.8464 (3.7481)  class_acc: 0.3750 (0.3247)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.1456  max mem: 15572
Epoch: [28]  [ 110/2809]  eta: 0:29:03  lr: 0.000012  min_lr: 0.000000  loss: 3.8700 (3.7489)  class_acc: 0.3333 (0.3236)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1446  max mem: 15572
Epoch: [28]  [ 120/2809]  eta: 0:29:09  lr: 0.000012  min_lr: 0.000000  loss: 3.9794 (3.7733)  class_acc: 0.2500 (0.3199)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.1986  max mem: 15572
[2025-01-16 03:42:21,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 78779
[2025-01-16 03:42:21,171] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:42:21,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 130/2809]  eta: 0:28:48  lr: 0.000012  min_lr: 0.000000  loss: 3.9535 (3.7660)  class_acc: 0.2917 (0.3203)  loss_scale: 65536.0000 (65035.7252)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1682  max mem: 15572
Epoch: [28]  [ 140/2809]  eta: 0:28:47  lr: 0.000012  min_lr: 0.000000  loss: 3.5544 (3.7493)  class_acc: 0.3333 (0.3236)  loss_scale: 32768.0000 (62747.2340)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.1642  max mem: 15572
Epoch: [28]  [ 150/2809]  eta: 0:27:53  lr: 0.000012  min_lr: 0.000000  loss: 3.4812 (3.7305)  class_acc: 0.3750 (0.3303)  loss_scale: 32768.0000 (60761.8543)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.1243  max mem: 15572
Epoch: [28]  [ 160/2809]  eta: 0:27:11  lr: 0.000012  min_lr: 0.000000  loss: 3.7438 (3.7387)  class_acc: 0.3333 (0.3292)  loss_scale: 32768.0000 (59023.1056)  weight_decay: 0.0500 (0.0500)  time: 0.3954  data: 0.0004  max mem: 15572
Epoch: [28]  [ 170/2809]  eta: 0:26:39  lr: 0.000012  min_lr: 0.000000  loss: 3.8928 (3.7331)  class_acc: 0.3333 (0.3331)  loss_scale: 32768.0000 (57487.7193)  weight_decay: 0.0500 (0.0500)  time: 0.4307  data: 0.0006  max mem: 15572
Epoch: [28]  [ 180/2809]  eta: 0:26:15  lr: 0.000012  min_lr: 0.000000  loss: 3.5479 (3.7217)  class_acc: 0.4167 (0.3379)  loss_scale: 32768.0000 (56121.9890)  weight_decay: 0.0500 (0.0500)  time: 0.4651  data: 0.0392  max mem: 15572
Epoch: [28]  [ 190/2809]  eta: 0:26:17  lr: 0.000012  min_lr: 0.000000  loss: 3.7820 (3.7371)  class_acc: 0.3333 (0.3349)  loss_scale: 32768.0000 (54899.2670)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1407  max mem: 15572
Epoch: [28]  [ 200/2809]  eta: 0:26:09  lr: 0.000012  min_lr: 0.000000  loss: 3.9504 (3.7417)  class_acc: 0.2500 (0.3344)  loss_scale: 32768.0000 (53798.2090)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1736  max mem: 15572
Epoch: [28]  [ 210/2809]  eta: 0:26:00  lr: 0.000012  min_lr: 0.000000  loss: 3.9470 (3.7466)  class_acc: 0.3333 (0.3355)  loss_scale: 32768.0000 (52801.5166)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1463  max mem: 15572
Epoch: [28]  [ 220/2809]  eta: 0:25:34  lr: 0.000012  min_lr: 0.000000  loss: 3.7917 (3.7478)  class_acc: 0.3333 (0.3365)  loss_scale: 32768.0000 (51895.0226)  weight_decay: 0.0500 (0.0500)  time: 0.5035  data: 0.0748  max mem: 15572
Epoch: [28]  [ 230/2809]  eta: 0:25:26  lr: 0.000012  min_lr: 0.000000  loss: 3.7917 (3.7506)  class_acc: 0.2917 (0.3364)  loss_scale: 32768.0000 (51067.0130)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0578  max mem: 15572
Epoch: [28]  [ 240/2809]  eta: 0:25:20  lr: 0.000012  min_lr: 0.000000  loss: 3.5584 (3.7444)  class_acc: 0.2917 (0.3363)  loss_scale: 32768.0000 (50307.7178)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1446  max mem: 15572
Epoch: [28]  [ 250/2809]  eta: 0:25:12  lr: 0.000012  min_lr: 0.000000  loss: 3.6364 (3.7456)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (49608.9243)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1481  max mem: 15572
[2025-01-16 03:43:30,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:43:30,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 260/2809]  eta: 0:24:56  lr: 0.000012  min_lr: 0.000000  loss: 3.8854 (3.7496)  class_acc: 0.3333 (0.3365)  loss_scale: 32768.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0902  max mem: 15572
Epoch: [28]  [ 270/2809]  eta: 0:24:47  lr: 0.000012  min_lr: 0.000000  loss: 3.8833 (3.7552)  class_acc: 0.2917 (0.3353)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5227  data: 0.0979  max mem: 15572
Epoch: [28]  [ 280/2809]  eta: 0:24:40  lr: 0.000012  min_lr: 0.000000  loss: 3.8833 (3.7627)  class_acc: 0.2917 (0.3336)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1207  max mem: 15572
Epoch: [28]  [ 290/2809]  eta: 0:24:27  lr: 0.000012  min_lr: 0.000000  loss: 3.8245 (3.7553)  class_acc: 0.2917 (0.3356)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0823  max mem: 15572
Epoch: [28]  [ 300/2809]  eta: 0:24:23  lr: 0.000012  min_lr: 0.000000  loss: 3.7739 (3.7568)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1139  max mem: 15572
Epoch: [28]  [ 310/2809]  eta: 0:24:19  lr: 0.000012  min_lr: 0.000000  loss: 3.8672 (3.7557)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.1676  max mem: 15572
Epoch: [28]  [ 320/2809]  eta: 0:24:17  lr: 0.000012  min_lr: 0.000000  loss: 3.4821 (3.7418)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1915  max mem: 15572
Epoch: [28]  [ 330/2809]  eta: 0:24:03  lr: 0.000012  min_lr: 0.000000  loss: 3.2995 (3.7374)  class_acc: 0.4167 (0.3410)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1248  max mem: 15572
Epoch: [28]  [ 340/2809]  eta: 0:24:01  lr: 0.000012  min_lr: 0.000000  loss: 3.6421 (3.7381)  class_acc: 0.3333 (0.3414)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1178  max mem: 15572
[2025-01-16 03:44:21,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=79000, skipped=527, lr=[1.1760208353860594e-07, 1.1760208353860594e-07, 1.680029764837228e-07, 1.680029764837228e-07, 2.40004252119604e-07, 2.40004252119604e-07, 3.4286321731372005e-07, 3.4286321731372005e-07, 4.898045961624572e-07, 4.898045961624572e-07, 6.997208516606532e-07, 6.997208516606532e-07, 9.99601216658076e-07, 9.99601216658076e-07, 1.428001738082966e-06, 1.428001738082966e-06, 2.0400024829756658e-06, 2.0400024829756658e-06, 2.9142892613938086e-06, 2.9142892613938086e-06, 4.1632703734197264e-06, 4.1632703734197264e-06, 5.947529104885324e-06, 5.947529104885324e-06, 8.496470149836178e-06, 8.496470149836178e-06, 1.2137814499765969e-05, 1.2137814499765969e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:44:21,960] [INFO] [timer.py:260:stop] epoch=0/micro_step=79000/global_step=79000, RunningAvgSamplesPerSec=28.55632688932864, CurrSamplesPerSec=29.84299764963701, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [ 350/2809]  eta: 0:23:52  lr: 0.000012  min_lr: 0.000000  loss: 3.5714 (3.7376)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (53679.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1428  max mem: 15572
Epoch: [28]  [ 360/2809]  eta: 0:23:53  lr: 0.000012  min_lr: 0.000000  loss: 3.6798 (3.7375)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (54008.1994)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1629  max mem: 15572
Epoch: [28]  [ 370/2809]  eta: 0:23:45  lr: 0.000012  min_lr: 0.000000  loss: 3.6798 (3.7357)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (54318.9218)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1830  max mem: 15572
Epoch: [28]  [ 380/2809]  eta: 0:23:35  lr: 0.000012  min_lr: 0.000000  loss: 3.5911 (3.7340)  class_acc: 0.3333 (0.3430)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.1076  max mem: 15572
[2025-01-16 03:44:42,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:44:42,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:44:43,034] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79037
[2025-01-16 03:44:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:44:43,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 390/2809]  eta: 0:23:31  lr: 0.000012  min_lr: 0.000000  loss: 3.7448 (3.7389)  class_acc: 0.2917 (0.3419)  loss_scale: 65536.0000 (55060.2967)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1321  max mem: 15572
Epoch: [28]  [ 400/2809]  eta: 0:23:30  lr: 0.000012  min_lr: 0.000000  loss: 3.7448 (3.7348)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (55321.5362)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.1976  max mem: 15572
Epoch: [28]  [ 410/2809]  eta: 0:23:17  lr: 0.000012  min_lr: 0.000000  loss: 3.5959 (3.7347)  class_acc: 0.3750 (0.3439)  loss_scale: 65536.0000 (55570.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1239  max mem: 15572
Epoch: [28]  [ 420/2809]  eta: 0:23:13  lr: 0.000012  min_lr: 0.000000  loss: 3.6888 (3.7359)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (55806.7838)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0995  max mem: 15572
Epoch: [28]  [ 430/2809]  eta: 0:23:07  lr: 0.000012  min_lr: 0.000000  loss: 3.6888 (3.7320)  class_acc: 0.3750 (0.3463)  loss_scale: 65536.0000 (56032.5197)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1561  max mem: 15572
Epoch: [28]  [ 440/2809]  eta: 0:23:03  lr: 0.000012  min_lr: 0.000000  loss: 3.4265 (3.7283)  class_acc: 0.3750 (0.3467)  loss_scale: 65536.0000 (56248.0181)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1505  max mem: 15572
Epoch: [28]  [ 450/2809]  eta: 0:22:55  lr: 0.000012  min_lr: 0.000000  loss: 3.4265 (3.7237)  class_acc: 0.3333 (0.3479)  loss_scale: 65536.0000 (56453.9601)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1425  max mem: 15572
Epoch: [28]  [ 460/2809]  eta: 0:22:48  lr: 0.000012  min_lr: 0.000000  loss: 3.8780 (3.7256)  class_acc: 0.3333 (0.3469)  loss_scale: 65536.0000 (56650.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1319  max mem: 15572
Epoch: [28]  [ 470/2809]  eta: 0:22:36  lr: 0.000012  min_lr: 0.000000  loss: 3.9654 (3.7258)  class_acc: 0.2500 (0.3455)  loss_scale: 65536.0000 (56839.6093)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0843  max mem: 15572
Epoch: [28]  [ 480/2809]  eta: 0:22:30  lr: 0.000012  min_lr: 0.000000  loss: 3.9907 (3.7288)  class_acc: 0.2917 (0.3452)  loss_scale: 65536.0000 (57020.4075)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0862  max mem: 15572
Epoch: [28]  [ 490/2809]  eta: 0:22:24  lr: 0.000012  min_lr: 0.000000  loss: 3.9694 (3.7297)  class_acc: 0.2917 (0.3442)  loss_scale: 65536.0000 (57193.8411)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1357  max mem: 15572
Epoch: [28]  [ 500/2809]  eta: 0:22:16  lr: 0.000012  min_lr: 0.000000  loss: 3.8168 (3.7290)  class_acc: 0.2917 (0.3444)  loss_scale: 65536.0000 (57360.3513)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1245  max mem: 15572
Epoch: [28]  [ 510/2809]  eta: 0:22:09  lr: 0.000012  min_lr: 0.000000  loss: 3.8303 (3.7306)  class_acc: 0.3333 (0.3439)  loss_scale: 65536.0000 (57520.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1017  max mem: 15572
[2025-01-16 03:45:55,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:45:55,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 520/2809]  eta: 0:22:01  lr: 0.000012  min_lr: 0.000000  loss: 3.8799 (3.7320)  class_acc: 0.3750 (0.3439)  loss_scale: 65536.0000 (58554.7179)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0915  max mem: 15572
[2025-01-16 03:46:00,522] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79174
[2025-01-16 03:46:00,523] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:46:00,523] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 530/2809]  eta: 0:21:51  lr: 0.000012  min_lr: 0.000000  loss: 3.8799 (3.7312)  class_acc: 0.3750 (0.3444)  loss_scale: 65536.0000 (58809.6121)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0727  max mem: 15572
Epoch: [28]  [ 540/2809]  eta: 0:21:47  lr: 0.000012  min_lr: 0.000000  loss: 3.7100 (3.7325)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (58933.9445)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.0955  max mem: 15572
Epoch: [28]  [ 550/2809]  eta: 0:21:38  lr: 0.000012  min_lr: 0.000000  loss: 3.7780 (3.7343)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (59053.7641)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1183  max mem: 15572
Epoch: [28]  [ 560/2809]  eta: 0:21:31  lr: 0.000012  min_lr: 0.000000  loss: 3.7780 (3.7341)  class_acc: 0.3750 (0.3441)  loss_scale: 65536.0000 (59169.3119)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0855  max mem: 15572
Epoch: [28]  [ 570/2809]  eta: 0:21:27  lr: 0.000012  min_lr: 0.000000  loss: 3.8301 (3.7367)  class_acc: 0.2917 (0.3430)  loss_scale: 65536.0000 (59280.8126)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1427  max mem: 15572
Epoch: [28]  [ 580/2809]  eta: 0:21:16  lr: 0.000012  min_lr: 0.000000  loss: 3.9128 (3.7417)  class_acc: 0.2500 (0.3416)  loss_scale: 65536.0000 (59388.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1024  max mem: 15572
Epoch: [28]  [ 590/2809]  eta: 0:21:10  lr: 0.000012  min_lr: 0.000000  loss: 3.8600 (3.7412)  class_acc: 0.2917 (0.3414)  loss_scale: 65536.0000 (59492.4941)  weight_decay: 0.0500 (0.0500)  time: 0.4964  data: 0.0533  max mem: 15572
Epoch: [28]  [ 600/2809]  eta: 0:21:01  lr: 0.000012  min_lr: 0.000000  loss: 4.0126 (3.7481)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (59593.0516)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0747  max mem: 15572
Epoch: [28]  [ 610/2809]  eta: 0:20:57  lr: 0.000012  min_lr: 0.000000  loss: 4.0051 (3.7477)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (59690.3175)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1053  max mem: 15572
Epoch: [28]  [ 620/2809]  eta: 0:20:52  lr: 0.000012  min_lr: 0.000000  loss: 3.7089 (3.7487)  class_acc: 0.3333 (0.3402)  loss_scale: 65536.0000 (59784.4509)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1609  max mem: 15572
[2025-01-16 03:46:58,732] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79281
[2025-01-16 03:46:58,733] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:46:58,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 630/2809]  eta: 0:20:44  lr: 0.000012  min_lr: 0.000000  loss: 3.5761 (3.7447)  class_acc: 0.3750 (0.3409)  loss_scale: 65536.0000 (59771.7401)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1084  max mem: 15572
Epoch: [28]  [ 640/2809]  eta: 0:20:40  lr: 0.000012  min_lr: 0.000000  loss: 3.5753 (3.7452)  class_acc: 0.3750 (0.3414)  loss_scale: 32768.0000 (59350.4649)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1220  max mem: 15572
Epoch: [28]  [ 650/2809]  eta: 0:20:34  lr: 0.000012  min_lr: 0.000000  loss: 3.7358 (3.7446)  class_acc: 0.3750 (0.3418)  loss_scale: 32768.0000 (58942.1321)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1381  max mem: 15572
Epoch: [28]  [ 660/2809]  eta: 0:20:25  lr: 0.000012  min_lr: 0.000000  loss: 3.7540 (3.7472)  class_acc: 0.3750 (0.3417)  loss_scale: 32768.0000 (58546.1543)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0695  max mem: 15572
Epoch: [28]  [ 670/2809]  eta: 0:20:20  lr: 0.000012  min_lr: 0.000000  loss: 3.9811 (3.7459)  class_acc: 0.3333 (0.3414)  loss_scale: 32768.0000 (58161.9791)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.0917  max mem: 15572
Epoch: [28]  [ 680/2809]  eta: 0:20:14  lr: 0.000012  min_lr: 0.000000  loss: 3.7892 (3.7457)  class_acc: 0.2500 (0.3411)  loss_scale: 32768.0000 (57789.0866)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1251  max mem: 15572
Epoch: [28]  [ 690/2809]  eta: 0:20:07  lr: 0.000012  min_lr: 0.000000  loss: 3.7892 (3.7470)  class_acc: 0.2917 (0.3404)  loss_scale: 32768.0000 (57426.9870)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1046  max mem: 15572
Epoch: [28]  [ 700/2809]  eta: 0:20:02  lr: 0.000012  min_lr: 0.000000  loss: 3.8528 (3.7478)  class_acc: 0.2500 (0.3400)  loss_scale: 32768.0000 (57075.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1279  max mem: 15572
Epoch: [28]  [ 710/2809]  eta: 0:19:58  lr: 0.000012  min_lr: 0.000000  loss: 3.9163 (3.7499)  class_acc: 0.2500 (0.3393)  loss_scale: 32768.0000 (56733.3446)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1584  max mem: 15572
Epoch: [28]  [ 720/2809]  eta: 0:19:51  lr: 0.000012  min_lr: 0.000000  loss: 3.8931 (3.7527)  class_acc: 0.2083 (0.3381)  loss_scale: 32768.0000 (56400.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1277  max mem: 15572
Epoch: [28]  [ 730/2809]  eta: 0:19:45  lr: 0.000012  min_lr: 0.000000  loss: 3.8016 (3.7513)  class_acc: 0.2500 (0.3380)  loss_scale: 32768.0000 (56077.6580)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1255  max mem: 15572
Epoch: [28]  [ 740/2809]  eta: 0:19:39  lr: 0.000012  min_lr: 0.000000  loss: 3.6867 (3.7508)  class_acc: 0.2917 (0.3378)  loss_scale: 32768.0000 (55763.0877)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1396  max mem: 15572
Epoch: [28]  [ 750/2809]  eta: 0:19:37  lr: 0.000012  min_lr: 0.000000  loss: 3.6867 (3.7480)  class_acc: 0.3333 (0.3377)  loss_scale: 32768.0000 (55456.8948)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1765  max mem: 15572
[2025-01-16 03:48:12,669] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:48:12,670] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [ 760/2809]  eta: 0:19:30  lr: 0.000012  min_lr: 0.000000  loss: 3.6103 (3.7458)  class_acc: 0.3750 (0.3380)  loss_scale: 32768.0000 (55287.9264)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1592  max mem: 15572
Epoch: [28]  [ 770/2809]  eta: 0:19:22  lr: 0.000012  min_lr: 0.000000  loss: 3.6092 (3.7463)  class_acc: 0.3750 (0.3379)  loss_scale: 65536.0000 (55420.8457)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0784  max mem: 15572
Epoch: [28]  [ 780/2809]  eta: 0:19:17  lr: 0.000012  min_lr: 0.000000  loss: 4.0571 (3.7510)  class_acc: 0.2917 (0.3368)  loss_scale: 65536.0000 (55550.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1102  max mem: 15572
Epoch: [28]  [ 790/2809]  eta: 0:19:07  lr: 0.000012  min_lr: 0.000000  loss: 4.0982 (3.7524)  class_acc: 0.2917 (0.3367)  loss_scale: 65536.0000 (55676.6018)  weight_decay: 0.0500 (0.0500)  time: 0.5013  data: 0.0771  max mem: 15572
Epoch: [28]  [ 800/2809]  eta: 0:19:01  lr: 0.000012  min_lr: 0.000000  loss: 3.8785 (3.7534)  class_acc: 0.2917 (0.3364)  loss_scale: 65536.0000 (55799.6904)  weight_decay: 0.0500 (0.0500)  time: 0.4811  data: 0.0555  max mem: 15572
Epoch: [28]  [ 810/2809]  eta: 0:18:54  lr: 0.000012  min_lr: 0.000000  loss: 3.9773 (3.7585)  class_acc: 0.2500 (0.3350)  loss_scale: 65536.0000 (55919.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.1060  max mem: 15572
Epoch: [28]  [ 820/2809]  eta: 0:18:48  lr: 0.000012  min_lr: 0.000000  loss: 3.9674 (3.7579)  class_acc: 0.2500 (0.3348)  loss_scale: 65536.0000 (56036.8721)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0966  max mem: 15572
Epoch: [28]  [ 830/2809]  eta: 0:18:42  lr: 0.000012  min_lr: 0.000000  loss: 3.9444 (3.7606)  class_acc: 0.2500 (0.3342)  loss_scale: 65536.0000 (56151.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1131  max mem: 15572
Epoch: [28]  [ 840/2809]  eta: 0:18:34  lr: 0.000012  min_lr: 0.000000  loss: 3.9444 (3.7609)  class_acc: 0.2917 (0.3340)  loss_scale: 65536.0000 (56262.7729)  weight_decay: 0.0500 (0.0500)  time: 0.5279  data: 0.1113  max mem: 15572
Epoch: [28]  [ 850/2809]  eta: 0:18:29  lr: 0.000012  min_lr: 0.000000  loss: 3.6072 (3.7604)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (56371.7415)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0974  max mem: 15572
Epoch: [28]  [ 860/2809]  eta: 0:18:24  lr: 0.000012  min_lr: 0.000000  loss: 3.7775 (3.7618)  class_acc: 0.3750 (0.3348)  loss_scale: 65536.0000 (56478.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1320  max mem: 15572
Epoch: [28]  [ 870/2809]  eta: 0:18:19  lr: 0.000012  min_lr: 0.000000  loss: 3.7963 (3.7625)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (56582.1722)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1541  max mem: 15572
Epoch: [28]  [ 880/2809]  eta: 0:18:12  lr: 0.000012  min_lr: 0.000000  loss: 3.6568 (3.7590)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (56683.8048)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1171  max mem: 15572
[2025-01-16 03:49:21,755] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:49:21,756] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 890/2809]  eta: 0:18:06  lr: 0.000012  min_lr: 0.000000  loss: 3.5267 (3.7577)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (57150.9226)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0998  max mem: 15572
[2025-01-16 03:49:24,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79543
[2025-01-16 03:49:24,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:49:24,061] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 900/2809]  eta: 0:18:00  lr: 0.000012  min_lr: 0.000000  loss: 3.6501 (3.7576)  class_acc: 0.3333 (0.3350)  loss_scale: 65536.0000 (57243.9867)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1297  max mem: 15572
Epoch: [28]  [ 910/2809]  eta: 0:17:56  lr: 0.000012  min_lr: 0.000000  loss: 3.6950 (3.7569)  class_acc: 0.3333 (0.3354)  loss_scale: 65536.0000 (57335.0077)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1608  max mem: 15572
[2025-01-16 03:49:40,193] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79568
[2025-01-16 03:49:40,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:49:40,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [ 920/2809]  eta: 0:17:52  lr: 0.000012  min_lr: 0.000000  loss: 3.8466 (3.7583)  class_acc: 0.2917 (0.3350)  loss_scale: 65536.0000 (57246.1585)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.2124  max mem: 15572
Epoch: [28]  [ 930/2809]  eta: 0:17:48  lr: 0.000012  min_lr: 0.000000  loss: 3.7925 (3.7578)  class_acc: 0.2917 (0.3348)  loss_scale: 32768.0000 (56983.2352)  weight_decay: 0.0500 (0.0500)  time: 0.6493  data: 0.2201  max mem: 15572
Epoch: [28]  [ 940/2809]  eta: 0:17:40  lr: 0.000012  min_lr: 0.000000  loss: 3.6443 (3.7568)  class_acc: 0.2917 (0.3343)  loss_scale: 32768.0000 (56725.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1140  max mem: 15572
Epoch: [28]  [ 950/2809]  eta: 0:17:37  lr: 0.000012  min_lr: 0.000000  loss: 3.6502 (3.7556)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (56473.9769)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1453  max mem: 15572
Epoch: [28]  [ 960/2809]  eta: 0:17:29  lr: 0.000012  min_lr: 0.000000  loss: 3.6502 (3.7534)  class_acc: 0.4167 (0.3354)  loss_scale: 32768.0000 (56227.2966)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1500  max mem: 15572
Epoch: [28]  [ 970/2809]  eta: 0:17:25  lr: 0.000012  min_lr: 0.000000  loss: 3.8390 (3.7559)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (55985.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1099  max mem: 15572
Epoch: [28]  [ 980/2809]  eta: 0:17:19  lr: 0.000012  min_lr: 0.000000  loss: 3.8326 (3.7545)  class_acc: 0.2917 (0.3351)  loss_scale: 32768.0000 (55749.0234)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1445  max mem: 15572
Epoch: [28]  [ 990/2809]  eta: 0:17:15  lr: 0.000012  min_lr: 0.000000  loss: 3.6284 (3.7525)  class_acc: 0.3333 (0.3358)  loss_scale: 32768.0000 (55517.1261)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1460  max mem: 15572
Epoch: [28]  [1000/2809]  eta: 0:17:10  lr: 0.000012  min_lr: 0.000000  loss: 3.8052 (3.7540)  class_acc: 0.3333 (0.3358)  loss_scale: 32768.0000 (55289.8621)  weight_decay: 0.0500 (0.0500)  time: 0.6432  data: 0.1974  max mem: 15572
Epoch: [28]  [1010/2809]  eta: 0:17:05  lr: 0.000012  min_lr: 0.000000  loss: 3.8052 (3.7522)  class_acc: 0.2917 (0.3359)  loss_scale: 32768.0000 (55067.0940)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1862  max mem: 15572
Epoch: [28]  [1020/2809]  eta: 0:16:58  lr: 0.000012  min_lr: 0.000000  loss: 3.7099 (3.7499)  class_acc: 0.3333 (0.3365)  loss_scale: 32768.0000 (54848.6895)  weight_decay: 0.0500 (0.0500)  time: 0.5382  data: 0.0968  max mem: 15572
Epoch: [28]  [1030/2809]  eta: 0:16:52  lr: 0.000012  min_lr: 0.000000  loss: 3.5445 (3.7487)  class_acc: 0.3333 (0.3370)  loss_scale: 32768.0000 (54634.5218)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0709  max mem: 15572
Epoch: [28]  [1040/2809]  eta: 0:16:45  lr: 0.000012  min_lr: 0.000000  loss: 3.8031 (3.7518)  class_acc: 0.3333 (0.3367)  loss_scale: 32768.0000 (54424.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0887  max mem: 15572
[2025-01-16 03:50:53,690] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:50:53,691] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1050/2809]  eta: 0:16:39  lr: 0.000012  min_lr: 0.000000  loss: 3.8031 (3.7493)  class_acc: 0.3333 (0.3378)  loss_scale: 32768.0000 (54405.4805)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0749  max mem: 15572
Epoch: [28]  [1060/2809]  eta: 0:16:33  lr: 0.000012  min_lr: 0.000000  loss: 3.5394 (3.7497)  class_acc: 0.3750 (0.3377)  loss_scale: 65536.0000 (54510.3864)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.0742  max mem: 15572
Epoch: [28]  [1070/2809]  eta: 0:16:28  lr: 0.000012  min_lr: 0.000000  loss: 3.5795 (3.7472)  class_acc: 0.3750 (0.3384)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1006  max mem: 15572
Epoch: [28]  [1080/2809]  eta: 0:16:21  lr: 0.000012  min_lr: 0.000000  loss: 3.5168 (3.7467)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (54714.3756)  weight_decay: 0.0500 (0.0500)  time: 0.5572  data: 0.1086  max mem: 15572
Epoch: [28]  [1090/2809]  eta: 0:16:16  lr: 0.000012  min_lr: 0.000000  loss: 3.8816 (3.7493)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (54813.5655)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.1140  max mem: 15572
Epoch: [28]  [1100/2809]  eta: 0:16:09  lr: 0.000012  min_lr: 0.000000  loss: 3.8816 (3.7489)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (54910.9537)  weight_decay: 0.0500 (0.0500)  time: 0.5303  data: 0.0892  max mem: 15572
Epoch: [28]  [1110/2809]  eta: 0:16:05  lr: 0.000012  min_lr: 0.000000  loss: 3.7205 (3.7492)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (55006.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1414  max mem: 15572
Epoch: [28]  [1120/2809]  eta: 0:15:58  lr: 0.000012  min_lr: 0.000000  loss: 3.6183 (3.7467)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (55100.5174)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1285  max mem: 15572
Epoch: [28]  [1130/2809]  eta: 0:15:52  lr: 0.000012  min_lr: 0.000000  loss: 3.6183 (3.7462)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (55192.7851)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.0711  max mem: 15572
Epoch: [28]  [1140/2809]  eta: 0:15:44  lr: 0.000012  min_lr: 0.000000  loss: 3.5951 (3.7442)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (55283.4356)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0656  max mem: 15572
Epoch: [28]  [1150/2809]  eta: 0:15:40  lr: 0.000012  min_lr: 0.000000  loss: 3.7830 (3.7446)  class_acc: 0.3750 (0.3393)  loss_scale: 65536.0000 (55372.5109)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0936  max mem: 15572
Epoch: [28]  [1160/2809]  eta: 0:15:33  lr: 0.000012  min_lr: 0.000000  loss: 3.9833 (3.7477)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (55460.0517)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1288  max mem: 15572
Epoch: [28]  [1170/2809]  eta: 0:15:27  lr: 0.000012  min_lr: 0.000000  loss: 4.0193 (3.7484)  class_acc: 0.2500 (0.3381)  loss_scale: 65536.0000 (55546.0974)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0635  max mem: 15572
[2025-01-16 03:52:04,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:52:04,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:52:04,759] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79826
[2025-01-16 03:52:04,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:52:04,760] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1180/2809]  eta: 0:15:21  lr: 0.000012  min_lr: 0.000000  loss: 3.6578 (3.7477)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (55686.1778)  weight_decay: 0.0500 (0.0500)  time: 0.5422  data: 0.0984  max mem: 15572
Epoch: [28]  [1190/2809]  eta: 0:15:16  lr: 0.000012  min_lr: 0.000000  loss: 3.6133 (3.7472)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (55768.8799)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1394  max mem: 15572
Epoch: [28]  [1200/2809]  eta: 0:15:10  lr: 0.000012  min_lr: 0.000000  loss: 3.7394 (3.7495)  class_acc: 0.3333 (0.3382)  loss_scale: 65536.0000 (55850.2048)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1128  max mem: 15572
Epoch: [28]  [1210/2809]  eta: 0:15:04  lr: 0.000012  min_lr: 0.000000  loss: 3.7394 (3.7480)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (55930.1866)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0827  max mem: 15572
Epoch: [28]  [1220/2809]  eta: 0:14:58  lr: 0.000012  min_lr: 0.000000  loss: 3.7202 (3.7489)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (56008.8583)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0769  max mem: 15572
Epoch: [28]  [1230/2809]  eta: 0:14:52  lr: 0.000012  min_lr: 0.000000  loss: 3.6552 (3.7475)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (56086.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1196  max mem: 15572
Epoch: [28]  [1240/2809]  eta: 0:14:48  lr: 0.000012  min_lr: 0.000000  loss: 3.4413 (3.7464)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (56162.3981)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.2116  max mem: 15572
Epoch: [28]  [1250/2809]  eta: 0:14:41  lr: 0.000012  min_lr: 0.000000  loss: 3.7208 (3.7473)  class_acc: 0.2917 (0.3376)  loss_scale: 65536.0000 (56237.3269)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1305  max mem: 15572
Epoch: [28]  [1260/2809]  eta: 0:14:35  lr: 0.000012  min_lr: 0.000000  loss: 3.7717 (3.7476)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (56311.0674)  weight_decay: 0.0500 (0.0500)  time: 0.4983  data: 0.0428  max mem: 15572
Epoch: [28]  [1270/2809]  eta: 0:14:29  lr: 0.000012  min_lr: 0.000000  loss: 3.8235 (3.7482)  class_acc: 0.2917 (0.3371)  loss_scale: 65536.0000 (56383.6475)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0805  max mem: 15572
Epoch: [28]  [1280/2809]  eta: 0:14:24  lr: 0.000012  min_lr: 0.000000  loss: 3.9671 (3.7500)  class_acc: 0.2083 (0.3368)  loss_scale: 65536.0000 (56455.0945)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0937  max mem: 15572
Epoch: [28]  [1290/2809]  eta: 0:14:18  lr: 0.000012  min_lr: 0.000000  loss: 3.8477 (3.7502)  class_acc: 0.2500 (0.3366)  loss_scale: 65536.0000 (56525.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1232  max mem: 15572
Epoch: [28]  [1300/2809]  eta: 0:14:12  lr: 0.000012  min_lr: 0.000000  loss: 3.8477 (3.7514)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (56594.6933)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1283  max mem: 15572
[2025-01-16 03:53:15,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:53:15,615] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 03:53:19,990] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 79962
[2025-01-16 03:53:19,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 03:53:19,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1310/2809]  eta: 0:14:07  lr: 0.000012  min_lr: 0.000000  loss: 3.8456 (3.7512)  class_acc: 0.2917 (0.3359)  loss_scale: 65536.0000 (57012.8207)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1288  max mem: 15572
Epoch: [28]  [1320/2809]  eta: 0:14:01  lr: 0.000012  min_lr: 0.000000  loss: 3.8938 (3.7538)  class_acc: 0.2500 (0.3349)  loss_scale: 65536.0000 (57077.3414)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1515  max mem: 15572
Epoch: [28]  [1330/2809]  eta: 0:13:56  lr: 0.000011  min_lr: 0.000000  loss: 3.9284 (3.7541)  class_acc: 0.2500 (0.3346)  loss_scale: 65536.0000 (57140.8926)  weight_decay: 0.0500 (0.0500)  time: 0.5828  data: 0.1350  max mem: 15572
Epoch: [28]  [1340/2809]  eta: 0:13:49  lr: 0.000011  min_lr: 0.000000  loss: 3.7491 (3.7543)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (57203.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0599  max mem: 15572
[2025-01-16 03:53:41,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=80000, skipped=534, lr=[1.1131416451469152e-07, 1.1131416451469152e-07, 1.590202350209879e-07, 1.590202350209879e-07, 2.2717176431569702e-07, 2.2717176431569702e-07, 3.245310918795672e-07, 3.245310918795672e-07, 4.636158455422389e-07, 4.636158455422389e-07, 6.62308350774627e-07, 6.62308350774627e-07, 9.461547868208958e-07, 9.461547868208958e-07, 1.3516496954584228e-06, 1.3516496954584228e-06, 1.930928136369175e-06, 1.930928136369175e-06, 2.758468766241679e-06, 2.758468766241679e-06, 3.9406696660595414e-06, 3.9406696660595414e-06, 5.629528094370775e-06, 5.629528094370775e-06, 8.04218299195825e-06, 8.04218299195825e-06, 1.1488832845654643e-05, 1.1488832845654643e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 03:53:41,320] [INFO] [timer.py:260:stop] epoch=0/micro_step=80000/global_step=80000, RunningAvgSamplesPerSec=28.560448323243456, CurrSamplesPerSec=32.316100570216356, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [28]  [1350/2809]  eta: 0:13:44  lr: 0.000011  min_lr: 0.000000  loss: 3.7803 (3.7536)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (57265.1725)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0947  max mem: 15572
Epoch: [28]  [1360/2809]  eta: 0:13:38  lr: 0.000011  min_lr: 0.000000  loss: 3.8036 (3.7534)  class_acc: 0.2917 (0.3340)  loss_scale: 65536.0000 (57325.9427)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1388  max mem: 15572
Epoch: [28]  [1370/2809]  eta: 0:13:33  lr: 0.000011  min_lr: 0.000000  loss: 3.8036 (3.7540)  class_acc: 0.2917 (0.3342)  loss_scale: 65536.0000 (57385.8264)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1345  max mem: 15572
Epoch: [28]  [1380/2809]  eta: 0:13:27  lr: 0.000011  min_lr: 0.000000  loss: 3.5603 (3.7533)  class_acc: 0.3333 (0.3342)  loss_scale: 65536.0000 (57444.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.1437  max mem: 15572
Epoch: [28]  [1390/2809]  eta: 0:13:22  lr: 0.000011  min_lr: 0.000000  loss: 3.6404 (3.7544)  class_acc: 0.2917 (0.3338)  loss_scale: 65536.0000 (57503.0108)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1480  max mem: 15572
Epoch: [28]  [1400/2809]  eta: 0:13:17  lr: 0.000011  min_lr: 0.000000  loss: 3.6404 (3.7536)  class_acc: 0.2917 (0.3338)  loss_scale: 65536.0000 (57560.3483)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1692  max mem: 15572
Epoch: [28]  [1410/2809]  eta: 0:13:11  lr: 0.000011  min_lr: 0.000000  loss: 3.5423 (3.7516)  class_acc: 0.4167 (0.3346)  loss_scale: 65536.0000 (57616.8731)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1360  max mem: 15572
Epoch: [28]  [1420/2809]  eta: 0:13:06  lr: 0.000011  min_lr: 0.000000  loss: 3.6055 (3.7522)  class_acc: 0.4167 (0.3346)  loss_scale: 65536.0000 (57672.6024)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1466  max mem: 15572
[2025-01-16 03:54:28,013] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80079
[2025-01-16 03:54:28,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:54:28,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1430/2809]  eta: 0:13:00  lr: 0.000011  min_lr: 0.000000  loss: 3.7318 (3.7521)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (57635.9581)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1308  max mem: 15572
Epoch: [28]  [1440/2809]  eta: 0:12:55  lr: 0.000011  min_lr: 0.000000  loss: 3.7580 (3.7531)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (57463.3838)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1345  max mem: 15572
Epoch: [28]  [1450/2809]  eta: 0:12:49  lr: 0.000011  min_lr: 0.000000  loss: 3.7555 (3.7513)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (57293.1881)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1250  max mem: 15572
Epoch: [28]  [1460/2809]  eta: 0:12:43  lr: 0.000011  min_lr: 0.000000  loss: 3.7284 (3.7529)  class_acc: 0.2917 (0.3343)  loss_scale: 32768.0000 (57125.3224)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0538  max mem: 15572
Epoch: [28]  [1470/2809]  eta: 0:12:37  lr: 0.000011  min_lr: 0.000000  loss: 3.8390 (3.7530)  class_acc: 0.2500 (0.3343)  loss_scale: 32768.0000 (56959.7390)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1150  max mem: 15572
Epoch: [28]  [1480/2809]  eta: 0:12:32  lr: 0.000011  min_lr: 0.000000  loss: 3.7456 (3.7524)  class_acc: 0.2500 (0.3342)  loss_scale: 32768.0000 (56796.3916)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1470  max mem: 15572
Epoch: [28]  [1490/2809]  eta: 0:12:27  lr: 0.000011  min_lr: 0.000000  loss: 3.7456 (3.7522)  class_acc: 0.2917 (0.3342)  loss_scale: 32768.0000 (56635.2354)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1600  max mem: 15572
Epoch: [28]  [1500/2809]  eta: 0:12:21  lr: 0.000011  min_lr: 0.000000  loss: 3.7536 (3.7515)  class_acc: 0.3750 (0.3344)  loss_scale: 32768.0000 (56476.2265)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1866  max mem: 15572
Epoch: [28]  [1510/2809]  eta: 0:12:14  lr: 0.000011  min_lr: 0.000000  loss: 3.7609 (3.7520)  class_acc: 0.2917 (0.3342)  loss_scale: 32768.0000 (56319.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0850  max mem: 15572
Epoch: [28]  [1520/2809]  eta: 0:12:09  lr: 0.000011  min_lr: 0.000000  loss: 3.7981 (3.7510)  class_acc: 0.2917 (0.3344)  loss_scale: 32768.0000 (56164.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5131  data: 0.0824  max mem: 15572
Epoch: [28]  [1530/2809]  eta: 0:12:04  lr: 0.000011  min_lr: 0.000000  loss: 3.3645 (3.7487)  class_acc: 0.3333 (0.3350)  loss_scale: 32768.0000 (56011.6630)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1408  max mem: 15572
Epoch: [28]  [1540/2809]  eta: 0:11:58  lr: 0.000011  min_lr: 0.000000  loss: 3.6621 (3.7494)  class_acc: 0.3333 (0.3348)  loss_scale: 32768.0000 (55860.8280)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1070  max mem: 15572
Epoch: [28]  [1550/2809]  eta: 0:11:52  lr: 0.000011  min_lr: 0.000000  loss: 3.8475 (3.7491)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (55711.9381)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1222  max mem: 15572
[2025-01-16 03:55:40,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:55:40,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1560/2809]  eta: 0:11:46  lr: 0.000011  min_lr: 0.000000  loss: 3.9156 (3.7505)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (55669.9142)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0783  max mem: 15572
Epoch: [28]  [1570/2809]  eta: 0:11:40  lr: 0.000011  min_lr: 0.000000  loss: 3.9694 (3.7512)  class_acc: 0.2917 (0.3345)  loss_scale: 65536.0000 (55732.7155)  weight_decay: 0.0500 (0.0500)  time: 0.5017  data: 0.0503  max mem: 15572
Epoch: [28]  [1580/2809]  eta: 0:11:34  lr: 0.000011  min_lr: 0.000000  loss: 3.8483 (3.7512)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (55794.7223)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0707  max mem: 15572
[2025-01-16 03:55:58,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80242
[2025-01-16 03:55:58,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:55:58,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1590/2809]  eta: 0:11:29  lr: 0.000011  min_lr: 0.000000  loss: 3.8397 (3.7516)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (55835.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1524  max mem: 15572
Epoch: [28]  [1600/2809]  eta: 0:11:23  lr: 0.000011  min_lr: 0.000000  loss: 3.8119 (3.7516)  class_acc: 0.2917 (0.3344)  loss_scale: 32768.0000 (55691.2730)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.2052  max mem: 15572
Epoch: [28]  [1610/2809]  eta: 0:11:18  lr: 0.000011  min_lr: 0.000000  loss: 3.7343 (3.7521)  class_acc: 0.2917 (0.3342)  loss_scale: 32768.0000 (55548.9808)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.1575  max mem: 15572
Epoch: [28]  [1620/2809]  eta: 0:11:12  lr: 0.000011  min_lr: 0.000000  loss: 3.7727 (3.7521)  class_acc: 0.2917 (0.3342)  loss_scale: 32768.0000 (55408.4442)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1473  max mem: 15572
Epoch: [28]  [1630/2809]  eta: 0:11:07  lr: 0.000011  min_lr: 0.000000  loss: 3.9058 (3.7523)  class_acc: 0.2917 (0.3340)  loss_scale: 32768.0000 (55269.6309)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1398  max mem: 15572
Epoch: [28]  [1640/2809]  eta: 0:11:01  lr: 0.000011  min_lr: 0.000000  loss: 3.6937 (3.7521)  class_acc: 0.2917 (0.3338)  loss_scale: 32768.0000 (55132.5094)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1457  max mem: 15572
Epoch: [28]  [1650/2809]  eta: 0:10:56  lr: 0.000011  min_lr: 0.000000  loss: 3.7052 (3.7536)  class_acc: 0.2917 (0.3337)  loss_scale: 32768.0000 (54997.0491)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1323  max mem: 15572
Epoch: [28]  [1660/2809]  eta: 0:10:50  lr: 0.000011  min_lr: 0.000000  loss: 3.7057 (3.7527)  class_acc: 0.3333 (0.3339)  loss_scale: 32768.0000 (54863.2197)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1546  max mem: 15572
Epoch: [28]  [1670/2809]  eta: 0:10:44  lr: 0.000011  min_lr: 0.000000  loss: 3.7371 (3.7534)  class_acc: 0.3333 (0.3338)  loss_scale: 32768.0000 (54730.9922)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.1200  max mem: 15572
Epoch: [28]  [1680/2809]  eta: 0:10:39  lr: 0.000011  min_lr: 0.000000  loss: 3.9941 (3.7542)  class_acc: 0.2917 (0.3335)  loss_scale: 32768.0000 (54600.3379)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.1214  max mem: 15572
Epoch: [28]  [1690/2809]  eta: 0:10:33  lr: 0.000011  min_lr: 0.000000  loss: 3.7997 (3.7548)  class_acc: 0.2917 (0.3334)  loss_scale: 32768.0000 (54471.2289)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1725  max mem: 15572
Epoch: [28]  [1700/2809]  eta: 0:10:28  lr: 0.000011  min_lr: 0.000000  loss: 3.7147 (3.7540)  class_acc: 0.3333 (0.3335)  loss_scale: 32768.0000 (54343.6379)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1545  max mem: 15572
Epoch: [28]  [1710/2809]  eta: 0:10:22  lr: 0.000011  min_lr: 0.000000  loss: 3.5772 (3.7534)  class_acc: 0.3750 (0.3336)  loss_scale: 32768.0000 (54217.5383)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1591  max mem: 15572
[2025-01-16 03:57:13,721] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:57:13,721] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1720/2809]  eta: 0:10:16  lr: 0.000011  min_lr: 0.000000  loss: 3.5950 (3.7533)  class_acc: 0.3750 (0.3336)  loss_scale: 32768.0000 (54130.9843)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0939  max mem: 15572
Epoch: [28]  [1730/2809]  eta: 0:10:11  lr: 0.000011  min_lr: 0.000000  loss: 3.8170 (3.7534)  class_acc: 0.2917 (0.3335)  loss_scale: 65536.0000 (54196.8712)  weight_decay: 0.0500 (0.0500)  time: 0.5143  data: 0.0465  max mem: 15572
Epoch: [28]  [1740/2809]  eta: 0:10:05  lr: 0.000011  min_lr: 0.000000  loss: 3.7712 (3.7539)  class_acc: 0.2917 (0.3335)  loss_scale: 65536.0000 (54262.0011)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.0787  max mem: 15572
Epoch: [28]  [1750/2809]  eta: 0:10:00  lr: 0.000011  min_lr: 0.000000  loss: 3.7712 (3.7537)  class_acc: 0.3333 (0.3335)  loss_scale: 65536.0000 (54326.3872)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1541  max mem: 15572
Epoch: [28]  [1760/2809]  eta: 0:09:54  lr: 0.000011  min_lr: 0.000000  loss: 3.8600 (3.7548)  class_acc: 0.2917 (0.3332)  loss_scale: 65536.0000 (54390.0420)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1777  max mem: 15572
Epoch: [28]  [1770/2809]  eta: 0:09:49  lr: 0.000011  min_lr: 0.000000  loss: 3.8239 (3.7535)  class_acc: 0.3333 (0.3337)  loss_scale: 65536.0000 (54452.9780)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1541  max mem: 15572
Epoch: [28]  [1780/2809]  eta: 0:09:43  lr: 0.000011  min_lr: 0.000000  loss: 3.6404 (3.7531)  class_acc: 0.3750 (0.3338)  loss_scale: 65536.0000 (54515.2072)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1388  max mem: 15572
Epoch: [28]  [1790/2809]  eta: 0:09:37  lr: 0.000011  min_lr: 0.000000  loss: 3.7674 (3.7541)  class_acc: 0.2500 (0.3334)  loss_scale: 65536.0000 (54576.7415)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1288  max mem: 15572
[2025-01-16 03:58:00,604] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80452
[2025-01-16 03:58:00,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 03:58:00,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [1800/2809]  eta: 0:09:32  lr: 0.000011  min_lr: 0.000000  loss: 3.7674 (3.7537)  class_acc: 0.2500 (0.3334)  loss_scale: 65536.0000 (54619.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1451  max mem: 15572
Epoch: [28]  [1810/2809]  eta: 0:09:26  lr: 0.000011  min_lr: 0.000000  loss: 3.4070 (3.7533)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (54498.7388)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1487  max mem: 15572
Epoch: [28]  [1820/2809]  eta: 0:09:20  lr: 0.000011  min_lr: 0.000000  loss: 3.8998 (3.7553)  class_acc: 0.2917 (0.3327)  loss_scale: 32768.0000 (54379.4047)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0863  max mem: 15572
Epoch: [28]  [1830/2809]  eta: 0:09:14  lr: 0.000011  min_lr: 0.000000  loss: 3.8354 (3.7543)  class_acc: 0.2917 (0.3328)  loss_scale: 32768.0000 (54261.3741)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0484  max mem: 15572
Epoch: [28]  [1840/2809]  eta: 0:09:08  lr: 0.000011  min_lr: 0.000000  loss: 3.6937 (3.7546)  class_acc: 0.3333 (0.3328)  loss_scale: 32768.0000 (54144.6257)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.0956  max mem: 15572
Epoch: [28]  [1850/2809]  eta: 0:09:03  lr: 0.000011  min_lr: 0.000000  loss: 3.7760 (3.7543)  class_acc: 0.3333 (0.3329)  loss_scale: 32768.0000 (54029.1388)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1565  max mem: 15572
Epoch: [28]  [1860/2809]  eta: 0:08:57  lr: 0.000011  min_lr: 0.000000  loss: 3.8754 (3.7552)  class_acc: 0.2917 (0.3326)  loss_scale: 32768.0000 (53914.8931)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1100  max mem: 15572
Epoch: [28]  [1870/2809]  eta: 0:08:51  lr: 0.000011  min_lr: 0.000000  loss: 3.8754 (3.7549)  class_acc: 0.2500 (0.3327)  loss_scale: 32768.0000 (53801.8685)  weight_decay: 0.0500 (0.0500)  time: 0.5052  data: 0.0594  max mem: 15572
Epoch: [28]  [1880/2809]  eta: 0:08:45  lr: 0.000011  min_lr: 0.000000  loss: 3.7748 (3.7551)  class_acc: 0.3750 (0.3328)  loss_scale: 32768.0000 (53690.0457)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0747  max mem: 15572
Epoch: [28]  [1890/2809]  eta: 0:08:39  lr: 0.000011  min_lr: 0.000000  loss: 3.7748 (3.7559)  class_acc: 0.3750 (0.3328)  loss_scale: 32768.0000 (53579.4056)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0539  max mem: 15572
Epoch: [28]  [1900/2809]  eta: 0:08:34  lr: 0.000011  min_lr: 0.000000  loss: 3.7485 (3.7562)  class_acc: 0.2917 (0.3324)  loss_scale: 32768.0000 (53469.9295)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0741  max mem: 15572
Epoch: [28]  [1910/2809]  eta: 0:08:28  lr: 0.000011  min_lr: 0.000000  loss: 3.9826 (3.7568)  class_acc: 0.2917 (0.3324)  loss_scale: 32768.0000 (53361.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1005  max mem: 15572
Epoch: [28]  [1920/2809]  eta: 0:08:22  lr: 0.000011  min_lr: 0.000000  loss: 3.7949 (3.7572)  class_acc: 0.3333 (0.3324)  loss_scale: 32768.0000 (53254.3967)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0801  max mem: 15572
[2025-01-16 03:59:09,203] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 03:59:09,204] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [1930/2809]  eta: 0:08:16  lr: 0.000011  min_lr: 0.000000  loss: 3.7935 (3.7577)  class_acc: 0.2917 (0.3323)  loss_scale: 32768.0000 (53182.2434)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0732  max mem: 15572
Epoch: [28]  [1940/2809]  eta: 0:08:10  lr: 0.000011  min_lr: 0.000000  loss: 3.8633 (3.7583)  class_acc: 0.2917 (0.3322)  loss_scale: 65536.0000 (53245.8897)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1300  max mem: 15572
Epoch: [28]  [1950/2809]  eta: 0:08:05  lr: 0.000011  min_lr: 0.000000  loss: 3.9156 (3.7589)  class_acc: 0.2500 (0.3319)  loss_scale: 65536.0000 (53308.8836)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1311  max mem: 15572
Epoch: [28]  [1960/2809]  eta: 0:07:59  lr: 0.000011  min_lr: 0.000000  loss: 3.6822 (3.7586)  class_acc: 0.2500 (0.3318)  loss_scale: 65536.0000 (53371.2351)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1036  max mem: 15572
Epoch: [28]  [1970/2809]  eta: 0:07:53  lr: 0.000011  min_lr: 0.000000  loss: 3.6666 (3.7581)  class_acc: 0.3333 (0.3319)  loss_scale: 65536.0000 (53432.9538)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1395  max mem: 15572
Epoch: [28]  [1980/2809]  eta: 0:07:48  lr: 0.000011  min_lr: 0.000000  loss: 3.8342 (3.7569)  class_acc: 0.3333 (0.3320)  loss_scale: 65536.0000 (53494.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1811  max mem: 15572
Epoch: [28]  [1990/2809]  eta: 0:07:42  lr: 0.000011  min_lr: 0.000000  loss: 3.6769 (3.7563)  class_acc: 0.2917 (0.3321)  loss_scale: 65536.0000 (53554.5314)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1522  max mem: 15572
Epoch: [28]  [2000/2809]  eta: 0:07:37  lr: 0.000011  min_lr: 0.000000  loss: 3.7680 (3.7578)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (53614.4088)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1260  max mem: 15572
Epoch: [28]  [2010/2809]  eta: 0:07:31  lr: 0.000011  min_lr: 0.000000  loss: 3.9830 (3.7579)  class_acc: 0.2500 (0.3315)  loss_scale: 65536.0000 (53673.6907)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1047  max mem: 15572
Epoch: [28]  [2020/2809]  eta: 0:07:25  lr: 0.000011  min_lr: 0.000000  loss: 3.8721 (3.7583)  class_acc: 0.2917 (0.3314)  loss_scale: 65536.0000 (53732.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1197  max mem: 15572
Epoch: [28]  [2030/2809]  eta: 0:07:20  lr: 0.000011  min_lr: 0.000000  loss: 3.7594 (3.7584)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (53790.5032)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1474  max mem: 15572
Epoch: [28]  [2040/2809]  eta: 0:07:14  lr: 0.000011  min_lr: 0.000000  loss: 3.8240 (3.7587)  class_acc: 0.2917 (0.3312)  loss_scale: 65536.0000 (53848.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0987  max mem: 15572
Epoch: [28]  [2050/2809]  eta: 0:07:08  lr: 0.000011  min_lr: 0.000000  loss: 3.8265 (3.7582)  class_acc: 0.2500 (0.3311)  loss_scale: 65536.0000 (53905.0375)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0699  max mem: 15572
[2025-01-16 04:00:23,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:00:23,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:00:24,192] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80711
[2025-01-16 04:00:24,192] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:00:24,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2060/2809]  eta: 0:07:03  lr: 0.000011  min_lr: 0.000000  loss: 3.5249 (3.7576)  class_acc: 0.3333 (0.3313)  loss_scale: 65536.0000 (54025.0674)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1692  max mem: 15572
Epoch: [28]  [2070/2809]  eta: 0:06:57  lr: 0.000011  min_lr: 0.000000  loss: 3.6348 (3.7569)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (54080.6490)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1621  max mem: 15572
Epoch: [28]  [2080/2809]  eta: 0:06:52  lr: 0.000011  min_lr: 0.000000  loss: 3.8354 (3.7574)  class_acc: 0.3333 (0.3313)  loss_scale: 65536.0000 (54135.6963)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1098  max mem: 15572
Epoch: [28]  [2090/2809]  eta: 0:06:46  lr: 0.000011  min_lr: 0.000000  loss: 3.9149 (3.7581)  class_acc: 0.3333 (0.3313)  loss_scale: 65536.0000 (54190.2171)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1523  max mem: 15572
Epoch: [28]  [2100/2809]  eta: 0:06:40  lr: 0.000011  min_lr: 0.000000  loss: 3.8859 (3.7581)  class_acc: 0.2917 (0.3312)  loss_scale: 65536.0000 (54244.2189)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1542  max mem: 15572
Epoch: [28]  [2110/2809]  eta: 0:06:34  lr: 0.000011  min_lr: 0.000000  loss: 3.7627 (3.7578)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (54297.7091)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0939  max mem: 15572
Epoch: [28]  [2120/2809]  eta: 0:06:29  lr: 0.000011  min_lr: 0.000000  loss: 3.6279 (3.7577)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54350.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0880  max mem: 15572
Epoch: [28]  [2130/2809]  eta: 0:06:23  lr: 0.000011  min_lr: 0.000000  loss: 3.7165 (3.7577)  class_acc: 0.2917 (0.3309)  loss_scale: 65536.0000 (54403.1835)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1072  max mem: 15572
Epoch: [28]  [2140/2809]  eta: 0:06:18  lr: 0.000011  min_lr: 0.000000  loss: 3.7212 (3.7576)  class_acc: 0.3333 (0.3309)  loss_scale: 65536.0000 (54455.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1282  max mem: 15572
Epoch: [28]  [2150/2809]  eta: 0:06:12  lr: 0.000011  min_lr: 0.000000  loss: 3.7773 (3.7586)  class_acc: 0.3333 (0.3308)  loss_scale: 65536.0000 (54506.6964)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1216  max mem: 15572
Epoch: [28]  [2160/2809]  eta: 0:06:06  lr: 0.000011  min_lr: 0.000000  loss: 4.1389 (3.7592)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54557.7344)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0928  max mem: 15572
Epoch: [28]  [2170/2809]  eta: 0:06:01  lr: 0.000011  min_lr: 0.000000  loss: 4.0510 (3.7594)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54608.3022)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1460  max mem: 15572
Epoch: [28]  [2180/2809]  eta: 0:05:55  lr: 0.000011  min_lr: 0.000000  loss: 3.9242 (3.7599)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (54658.4062)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1397  max mem: 15572
[2025-01-16 04:01:37,587] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:01:37,587] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:01:38,023] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80841
[2025-01-16 04:01:38,023] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:01:38,023] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2190/2809]  eta: 0:05:50  lr: 0.000011  min_lr: 0.000000  loss: 3.6670 (3.7598)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (54737.9644)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1490  max mem: 15572
Epoch: [28]  [2200/2809]  eta: 0:05:44  lr: 0.000011  min_lr: 0.000000  loss: 3.6649 (3.7603)  class_acc: 0.3750 (0.3307)  loss_scale: 65536.0000 (54787.0241)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1720  max mem: 15572
Epoch: [28]  [2210/2809]  eta: 0:05:38  lr: 0.000011  min_lr: 0.000000  loss: 3.8384 (3.7599)  class_acc: 0.3333 (0.3308)  loss_scale: 65536.0000 (54835.6400)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0873  max mem: 15572
[2025-01-16 04:01:53,564] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 80872
[2025-01-16 04:01:53,564] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:01:53,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2220/2809]  eta: 0:05:32  lr: 0.000011  min_lr: 0.000000  loss: 3.5557 (3.7590)  class_acc: 0.3333 (0.3310)  loss_scale: 65536.0000 (54869.0644)  weight_decay: 0.0500 (0.0500)  time: 0.4451  data: 0.0129  max mem: 15572
Epoch: [28]  [2230/2809]  eta: 0:05:26  lr: 0.000011  min_lr: 0.000000  loss: 3.7253 (3.7593)  class_acc: 0.2917 (0.3308)  loss_scale: 32768.0000 (54770.0009)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0886  max mem: 15572
Epoch: [28]  [2240/2809]  eta: 0:05:21  lr: 0.000011  min_lr: 0.000000  loss: 3.8954 (3.7597)  class_acc: 0.2917 (0.3309)  loss_scale: 32768.0000 (54671.8215)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1483  max mem: 15572
Epoch: [28]  [2250/2809]  eta: 0:05:15  lr: 0.000011  min_lr: 0.000000  loss: 3.8845 (3.7601)  class_acc: 0.3333 (0.3309)  loss_scale: 32768.0000 (54574.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1201  max mem: 15572
Epoch: [28]  [2260/2809]  eta: 0:05:09  lr: 0.000011  min_lr: 0.000000  loss: 3.7489 (3.7596)  class_acc: 0.3333 (0.3310)  loss_scale: 32768.0000 (54478.0681)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1181  max mem: 15572
Epoch: [28]  [2270/2809]  eta: 0:05:04  lr: 0.000011  min_lr: 0.000000  loss: 3.6946 (3.7595)  class_acc: 0.2917 (0.3309)  loss_scale: 32768.0000 (54382.4712)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1443  max mem: 15572
Epoch: [28]  [2280/2809]  eta: 0:04:59  lr: 0.000011  min_lr: 0.000000  loss: 3.7117 (3.7597)  class_acc: 0.2917 (0.3309)  loss_scale: 32768.0000 (54287.7124)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.1868  max mem: 15572
Epoch: [28]  [2290/2809]  eta: 0:04:53  lr: 0.000011  min_lr: 0.000000  loss: 3.7677 (3.7595)  class_acc: 0.2917 (0.3307)  loss_scale: 32768.0000 (54193.7809)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1388  max mem: 15572
Epoch: [28]  [2300/2809]  eta: 0:04:47  lr: 0.000011  min_lr: 0.000000  loss: 3.7709 (3.7602)  class_acc: 0.3333 (0.3307)  loss_scale: 32768.0000 (54100.6658)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1357  max mem: 15572
Epoch: [28]  [2310/2809]  eta: 0:04:42  lr: 0.000011  min_lr: 0.000000  loss: 3.8199 (3.7596)  class_acc: 0.2917 (0.3306)  loss_scale: 32768.0000 (54008.3566)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1880  max mem: 15572
Epoch: [28]  [2320/2809]  eta: 0:04:36  lr: 0.000011  min_lr: 0.000000  loss: 3.9351 (3.7603)  class_acc: 0.2500 (0.3305)  loss_scale: 32768.0000 (53916.8427)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1367  max mem: 15572
Epoch: [28]  [2330/2809]  eta: 0:04:30  lr: 0.000011  min_lr: 0.000000  loss: 3.9354 (3.7607)  class_acc: 0.2917 (0.3305)  loss_scale: 32768.0000 (53826.1141)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1225  max mem: 15572
Epoch: [28]  [2340/2809]  eta: 0:04:25  lr: 0.000011  min_lr: 0.000000  loss: 3.7337 (3.7607)  class_acc: 0.3333 (0.3305)  loss_scale: 32768.0000 (53736.1606)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1040  max mem: 15572
[2025-01-16 04:03:06,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=81000, skipped=540, lr=[1.0514467726857202e-07, 1.0514467726857202e-07, 1.5020668181224575e-07, 1.5020668181224575e-07, 2.1458097401749398e-07, 2.1458097401749398e-07, 3.0654424859642e-07, 3.0654424859642e-07, 4.379203551377428e-07, 4.379203551377428e-07, 6.256005073396326e-07, 6.256005073396326e-07, 8.937150104851895e-07, 8.937150104851895e-07, 1.2767357292645567e-06, 1.2767357292645567e-06, 1.8239081846636523e-06, 1.8239081846636523e-06, 2.605583120948075e-06, 2.605583120948075e-06, 3.722261601354393e-06, 3.722261601354393e-06, 5.317516573363419e-06, 5.317516573363419e-06, 7.5964522476620275e-06, 7.5964522476620275e-06, 1.0852074639517183e-05, 1.0852074639517183e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:03:06,617] [INFO] [timer.py:260:stop] epoch=0/micro_step=81000/global_step=81000, RunningAvgSamplesPerSec=28.56323315097633, CurrSamplesPerSec=23.555951391992735, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 04:03:07,512] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:03:07,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2350/2809]  eta: 0:04:19  lr: 0.000011  min_lr: 0.000000  loss: 3.7025 (3.7609)  class_acc: 0.3333 (0.3306)  loss_scale: 32768.0000 (53674.8481)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0806  max mem: 15572
Epoch: [28]  [2360/2809]  eta: 0:04:13  lr: 0.000011  min_lr: 0.000000  loss: 3.7928 (3.7616)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (53725.0860)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0689  max mem: 15572
Epoch: [28]  [2370/2809]  eta: 0:04:07  lr: 0.000011  min_lr: 0.000000  loss: 3.7007 (3.7602)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (53774.9000)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0985  max mem: 15572
Epoch: [28]  [2380/2809]  eta: 0:04:02  lr: 0.000011  min_lr: 0.000000  loss: 3.6090 (3.7598)  class_acc: 0.4167 (0.3309)  loss_scale: 65536.0000 (53824.2957)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1506  max mem: 15572
Epoch: [28]  [2390/2809]  eta: 0:03:56  lr: 0.000011  min_lr: 0.000000  loss: 3.7694 (3.7605)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (53873.2781)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1560  max mem: 15572
Epoch: [28]  [2400/2809]  eta: 0:03:51  lr: 0.000011  min_lr: 0.000000  loss: 3.7765 (3.7602)  class_acc: 0.3333 (0.3307)  loss_scale: 65536.0000 (53921.8526)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.1852  max mem: 15572
Epoch: [28]  [2410/2809]  eta: 0:03:45  lr: 0.000011  min_lr: 0.000000  loss: 3.6822 (3.7594)  class_acc: 0.3333 (0.3307)  loss_scale: 65536.0000 (53970.0241)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1216  max mem: 15572
Epoch: [28]  [2420/2809]  eta: 0:03:39  lr: 0.000011  min_lr: 0.000000  loss: 3.8156 (3.7599)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54017.7976)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0881  max mem: 15572
Epoch: [28]  [2430/2809]  eta: 0:03:34  lr: 0.000011  min_lr: 0.000000  loss: 3.9350 (3.7597)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54065.1781)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.1043  max mem: 15572
Epoch: [28]  [2440/2809]  eta: 0:03:28  lr: 0.000011  min_lr: 0.000000  loss: 3.7681 (3.7593)  class_acc: 0.3333 (0.3310)  loss_scale: 65536.0000 (54112.1704)  weight_decay: 0.0500 (0.0500)  time: 0.4953  data: 0.0735  max mem: 15572
Epoch: [28]  [2450/2809]  eta: 0:03:22  lr: 0.000011  min_lr: 0.000000  loss: 3.7231 (3.7590)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (54158.7793)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1460  max mem: 15572
Epoch: [28]  [2460/2809]  eta: 0:03:17  lr: 0.000011  min_lr: 0.000000  loss: 3.8219 (3.7588)  class_acc: 0.3750 (0.3312)  loss_scale: 65536.0000 (54205.0093)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1455  max mem: 15572
Epoch: [28]  [2470/2809]  eta: 0:03:11  lr: 0.000011  min_lr: 0.000000  loss: 3.8219 (3.7583)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (54250.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0570  max mem: 15572
[2025-01-16 04:04:17,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:04:17,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:04:18,396] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81130
[2025-01-16 04:04:18,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:04:18,396] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2480/2809]  eta: 0:03:05  lr: 0.000011  min_lr: 0.000000  loss: 3.6626 (3.7577)  class_acc: 0.2917 (0.3316)  loss_scale: 65536.0000 (54322.7666)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0552  max mem: 15572
Epoch: [28]  [2490/2809]  eta: 0:02:59  lr: 0.000011  min_lr: 0.000000  loss: 3.7734 (3.7583)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (54367.7816)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0996  max mem: 15572
Epoch: [28]  [2500/2809]  eta: 0:02:54  lr: 0.000011  min_lr: 0.000000  loss: 3.9488 (3.7593)  class_acc: 0.2500 (0.3312)  loss_scale: 65536.0000 (54412.4366)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1091  max mem: 15572
Epoch: [28]  [2510/2809]  eta: 0:02:48  lr: 0.000011  min_lr: 0.000000  loss: 3.9281 (3.7593)  class_acc: 0.2500 (0.3309)  loss_scale: 65536.0000 (54456.7360)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1344  max mem: 15572
Epoch: [28]  [2520/2809]  eta: 0:02:43  lr: 0.000011  min_lr: 0.000000  loss: 3.7403 (3.7592)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (54500.6839)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1465  max mem: 15572
Epoch: [28]  [2530/2809]  eta: 0:02:37  lr: 0.000011  min_lr: 0.000000  loss: 3.9352 (3.7596)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54544.2845)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1624  max mem: 15572
Epoch: [28]  [2540/2809]  eta: 0:02:31  lr: 0.000011  min_lr: 0.000000  loss: 3.9352 (3.7599)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (54587.5419)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1773  max mem: 15572
Epoch: [28]  [2550/2809]  eta: 0:02:26  lr: 0.000011  min_lr: 0.000000  loss: 3.8547 (3.7597)  class_acc: 0.3333 (0.3307)  loss_scale: 65536.0000 (54630.4602)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1116  max mem: 15572
Epoch: [28]  [2560/2809]  eta: 0:02:20  lr: 0.000011  min_lr: 0.000000  loss: 3.8547 (3.7602)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (54673.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0812  max mem: 15572
Epoch: [28]  [2570/2809]  eta: 0:02:15  lr: 0.000011  min_lr: 0.000000  loss: 3.9160 (3.7598)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (54715.2952)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.1606  max mem: 15572
Epoch: [28]  [2580/2809]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 3.7563 (3.7601)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (54757.2197)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.1764  max mem: 15572
Epoch: [28]  [2590/2809]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000000  loss: 3.9117 (3.7605)  class_acc: 0.3333 (0.3308)  loss_scale: 65536.0000 (54798.8205)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1087  max mem: 15572
Epoch: [28]  [2600/2809]  eta: 0:01:58  lr: 0.000011  min_lr: 0.000000  loss: 3.7333 (3.7599)  class_acc: 0.3750 (0.3310)  loss_scale: 65536.0000 (54840.1015)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1041  max mem: 15572
[2025-01-16 04:05:33,369] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:05:33,370] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:05:33,795] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81260
[2025-01-16 04:05:33,796] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:05:33,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [2610/2809]  eta: 0:01:52  lr: 0.000011  min_lr: 0.000000  loss: 3.7726 (3.7609)  class_acc: 0.3333 (0.3309)  loss_scale: 65536.0000 (54906.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5952  data: 0.1383  max mem: 15572
Epoch: [28]  [2620/2809]  eta: 0:01:46  lr: 0.000011  min_lr: 0.000000  loss: 3.8231 (3.7602)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (54946.7226)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1457  max mem: 15572
[2025-01-16 04:05:43,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81274
[2025-01-16 04:05:43,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:05:43,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [2630/2809]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000000  loss: 3.7732 (3.7602)  class_acc: 0.2917 (0.3311)  loss_scale: 65536.0000 (54874.8795)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.1445  max mem: 15572
Epoch: [28]  [2640/2809]  eta: 0:01:35  lr: 0.000011  min_lr: 0.000000  loss: 3.9130 (3.7608)  class_acc: 0.2917 (0.3309)  loss_scale: 32768.0000 (54791.1730)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1756  max mem: 15572
Epoch: [28]  [2650/2809]  eta: 0:01:29  lr: 0.000011  min_lr: 0.000000  loss: 3.6964 (3.7595)  class_acc: 0.3333 (0.3313)  loss_scale: 32768.0000 (54708.0981)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1295  max mem: 15572
Epoch: [28]  [2660/2809]  eta: 0:01:24  lr: 0.000011  min_lr: 0.000000  loss: 3.5569 (3.7588)  class_acc: 0.4167 (0.3316)  loss_scale: 32768.0000 (54625.6475)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1373  max mem: 15572
Epoch: [28]  [2670/2809]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000000  loss: 3.5569 (3.7582)  class_acc: 0.3750 (0.3317)  loss_scale: 32768.0000 (54543.8143)  weight_decay: 0.0500 (0.0500)  time: 0.5821  data: 0.1258  max mem: 15572
Epoch: [28]  [2680/2809]  eta: 0:01:12  lr: 0.000011  min_lr: 0.000000  loss: 3.5528 (3.7576)  class_acc: 0.3333 (0.3318)  loss_scale: 32768.0000 (54462.5916)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.0774  max mem: 15572
Epoch: [28]  [2690/2809]  eta: 0:01:07  lr: 0.000011  min_lr: 0.000000  loss: 3.5735 (3.7575)  class_acc: 0.3333 (0.3318)  loss_scale: 32768.0000 (54381.9725)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.1408  max mem: 15572
Epoch: [28]  [2700/2809]  eta: 0:01:01  lr: 0.000011  min_lr: 0.000000  loss: 3.6643 (3.7572)  class_acc: 0.2917 (0.3319)  loss_scale: 32768.0000 (54301.9504)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0947  max mem: 15572
Epoch: [28]  [2710/2809]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 3.8848 (3.7584)  class_acc: 0.2917 (0.3316)  loss_scale: 32768.0000 (54222.5186)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.0930  max mem: 15572
Epoch: [28]  [2720/2809]  eta: 0:00:50  lr: 0.000011  min_lr: 0.000000  loss: 3.7746 (3.7579)  class_acc: 0.2500 (0.3315)  loss_scale: 32768.0000 (54143.6707)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1690  max mem: 15572
Epoch: [28]  [2730/2809]  eta: 0:00:44  lr: 0.000011  min_lr: 0.000000  loss: 3.7094 (3.7574)  class_acc: 0.2917 (0.3316)  loss_scale: 32768.0000 (54065.4002)  weight_decay: 0.0500 (0.0500)  time: 0.5827  data: 0.1464  max mem: 15572
Epoch: [28]  [2740/2809]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000000  loss: 3.6821 (3.7573)  class_acc: 0.3333 (0.3316)  loss_scale: 32768.0000 (53987.7008)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1057  max mem: 15572
Epoch: [28]  [2750/2809]  eta: 0:00:33  lr: 0.000011  min_lr: 0.000000  loss: 3.6248 (3.7565)  class_acc: 0.3333 (0.3317)  loss_scale: 32768.0000 (53910.5663)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1131  max mem: 15572
[2025-01-16 04:06:57,929] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:06:57,929] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [28]  [2760/2809]  eta: 0:00:27  lr: 0.000011  min_lr: 0.000000  loss: 3.5490 (3.7561)  class_acc: 0.3750 (0.3317)  loss_scale: 32768.0000 (53952.6722)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1617  max mem: 15572
Epoch: [28]  [2770/2809]  eta: 0:00:22  lr: 0.000011  min_lr: 0.000000  loss: 3.6227 (3.7555)  class_acc: 0.3750 (0.3319)  loss_scale: 65536.0000 (53994.4742)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1408  max mem: 15572
Epoch: [28]  [2780/2809]  eta: 0:00:16  lr: 0.000011  min_lr: 0.000000  loss: 3.7472 (3.7560)  class_acc: 0.3750 (0.3320)  loss_scale: 65536.0000 (54035.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1144  max mem: 15572
Epoch: [28]  [2790/2809]  eta: 0:00:10  lr: 0.000011  min_lr: 0.000000  loss: 3.7472 (3.7555)  class_acc: 0.3333 (0.3321)  loss_scale: 65536.0000 (54077.1795)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1047  max mem: 15572
Epoch: [28]  [2800/2809]  eta: 0:00:05  lr: 0.000011  min_lr: 0.000000  loss: 3.9110 (3.7566)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (54118.0893)  weight_decay: 0.0500 (0.0500)  time: 0.5080  data: 0.0697  max mem: 15572
Epoch: [28]  [2808/2809]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 3.9110 (3.7566)  class_acc: 0.2500 (0.3320)  loss_scale: 65536.0000 (54150.6073)  weight_decay: 0.0500 (0.0500)  time: 0.4321  data: 0.0350  max mem: 15572
Epoch: [28] Total time: 0:26:27 (0.5653 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 3.9110 (3.7566)  class_acc: 0.2500 (0.3320)  loss_scale: 65536.0000 (54150.6073)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:15:53  loss: 0.3181 (0.3181)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.5056  data: 3.3300  max mem: 15572
Val:  [ 10/272]  eta: 0:02:48  loss: 2.3643 (2.2408)  acc1: 44.4444 (42.9293)  acc5: 77.7778 (75.7576)  time: 0.6426  data: 0.4474  max mem: 15572
Val:  [ 20/272]  eta: 0:02:04  loss: 2.2656 (2.2852)  acc1: 44.4444 (46.0317)  acc5: 72.2222 (74.6032)  time: 0.3453  data: 0.1530  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 2.2656 (2.3778)  acc1: 50.0000 (42.6523)  acc5: 72.2222 (73.8351)  time: 0.3180  data: 0.1207  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.5176 (2.4331)  acc1: 33.3333 (40.7859)  acc5: 72.2222 (74.1192)  time: 0.3040  data: 0.1064  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.4320 (2.3484)  acc1: 38.8889 (42.7015)  acc5: 77.7778 (75.9259)  time: 0.3164  data: 0.1360  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.4721 (2.2502)  acc1: 61.1111 (45.3552)  acc5: 88.8889 (76.6849)  time: 0.3258  data: 0.1462  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.5929 (2.1794)  acc1: 66.6667 (47.7308)  acc5: 83.3333 (77.6213)  time: 0.3278  data: 0.1442  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 1.8548 (2.1872)  acc1: 55.5556 (47.6680)  acc5: 83.3333 (77.7092)  time: 0.2860  data: 0.0994  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.0191 (2.1805)  acc1: 50.0000 (48.2906)  acc5: 83.3333 (78.3883)  time: 0.2865  data: 0.0973  max mem: 15572
Val:  [100/272]  eta: 0:00:58  loss: 2.0172 (2.2004)  acc1: 61.1111 (48.0198)  acc5: 83.3333 (78.2728)  time: 0.2956  data: 0.1152  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.3949 (2.2785)  acc1: 22.2222 (45.9459)  acc5: 77.7778 (76.8268)  time: 0.2991  data: 0.1024  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.0398 (2.3213)  acc1: 16.6667 (44.9495)  acc5: 66.6667 (76.1708)  time: 0.3382  data: 0.1354  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.1490 (2.2860)  acc1: 44.4444 (46.0136)  acc5: 83.3333 (76.8024)  time: 0.3135  data: 0.1288  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.7380 (2.2803)  acc1: 50.0000 (46.3357)  acc5: 83.3333 (76.4381)  time: 0.3049  data: 0.1150  max mem: 15572
Val:  [150/272]  eta: 0:00:40  loss: 2.3912 (2.2895)  acc1: 33.3333 (45.6586)  acc5: 77.7778 (76.4533)  time: 0.3157  data: 0.1215  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.3912 (2.2801)  acc1: 44.4444 (46.3078)  acc5: 77.7778 (76.5700)  time: 0.2855  data: 0.0981  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.4415 (2.3057)  acc1: 44.4444 (45.6140)  acc5: 72.2222 (76.0234)  time: 0.3033  data: 0.1071  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.3342 (2.2938)  acc1: 33.3333 (45.3653)  acc5: 77.7778 (76.4273)  time: 0.3264  data: 0.1189  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.3231 (2.3455)  acc1: 33.3333 (44.1827)  acc5: 77.7778 (75.0436)  time: 0.3264  data: 0.1322  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.5161 (2.3546)  acc1: 27.7778 (43.7535)  acc5: 66.6667 (74.8480)  time: 0.3255  data: 0.1393  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.0456 (2.3602)  acc1: 44.4444 (43.8652)  acc5: 77.7778 (74.6972)  time: 0.2810  data: 0.0912  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.2687 (2.3494)  acc1: 50.0000 (44.1931)  acc5: 72.2222 (74.8115)  time: 0.3061  data: 0.1139  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7999 (2.3179)  acc1: 61.1111 (45.2381)  acc5: 83.3333 (75.1804)  time: 0.3642  data: 0.1716  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6455 (2.3026)  acc1: 61.1111 (45.4818)  acc5: 83.3333 (75.4034)  time: 0.3307  data: 0.1393  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.3539 (2.3166)  acc1: 38.8889 (44.8207)  acc5: 77.7778 (75.2545)  time: 0.2721  data: 0.0708  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.3158 (2.2596)  acc1: 66.6667 (46.5092)  acc5: 88.8889 (76.0111)  time: 0.2841  data: 0.0804  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3530 (2.2557)  acc1: 66.6667 (46.4740)  acc5: 88.8889 (76.2198)  time: 0.2410  data: 0.0678  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3530 (2.2604)  acc1: 66.6667 (46.4469)  acc5: 88.8889 (76.1827)  time: 0.2340  data: 0.0678  max mem: 15572
Val: Total time: 0:01:26 (0.3189 s / it)
* Acc@1 46.447 Acc@5 76.183 loss 2.260
Accuracy of the network on the 4883 val videos: 46.4%
[2025-01-16 04:08:53,246] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 04:08:53,250] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 04:08:53,250] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 04:08:56,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 04:08:56,050] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.45%
Epoch: [29]  [   0/2809]  eta: 6:16:06  lr: 0.000011  min_lr: 0.000000  loss: 4.1089 (4.1089)  class_acc: 0.2083 (0.2083)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 8.0337  data: 7.5992  max mem: 15572
Epoch: [29]  [  10/2809]  eta: 0:50:31  lr: 0.000011  min_lr: 0.000000  loss: 4.0131 (3.8910)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0831  data: 0.6911  max mem: 15572
Epoch: [29]  [  20/2809]  eta: 0:35:38  lr: 0.000011  min_lr: 0.000000  loss: 3.7635 (3.7722)  class_acc: 0.2917 (0.3234)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4034  data: 0.0003  max mem: 15572
Epoch: [29]  [  30/2809]  eta: 0:30:46  lr: 0.000011  min_lr: 0.000000  loss: 3.6901 (3.6958)  class_acc: 0.4167 (0.3562)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4340  data: 0.0006  max mem: 15572
Epoch: [29]  [  40/2809]  eta: 0:28:23  lr: 0.000011  min_lr: 0.000000  loss: 3.6336 (3.6712)  class_acc: 0.3750 (0.3567)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4558  data: 0.0097  max mem: 15572
Epoch: [29]  [  50/2809]  eta: 0:28:54  lr: 0.000011  min_lr: 0.000000  loss: 3.4421 (3.6453)  class_acc: 0.3750 (0.3644)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1129  max mem: 15572
Epoch: [29]  [  60/2809]  eta: 0:28:51  lr: 0.000011  min_lr: 0.000000  loss: 3.7325 (3.6629)  class_acc: 0.3750 (0.3620)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.1923  max mem: 15572
[2025-01-16 04:09:40,559] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:09:40,560] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [  70/2809]  eta: 0:28:36  lr: 0.000011  min_lr: 0.000000  loss: 3.7444 (3.6632)  class_acc: 0.3333 (0.3586)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1703  max mem: 15572
[2025-01-16 04:09:41,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81533
[2025-01-16 04:09:41,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:09:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [  80/2809]  eta: 0:28:53  lr: 0.000011  min_lr: 0.000000  loss: 3.7357 (3.6564)  class_acc: 0.3333 (0.3611)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.6514  data: 0.1847  max mem: 15572
Epoch: [29]  [  90/2809]  eta: 0:29:08  lr: 0.000011  min_lr: 0.000000  loss: 3.7357 (3.6637)  class_acc: 0.3333 (0.3544)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.2248  max mem: 15572
Epoch: [29]  [ 100/2809]  eta: 0:29:08  lr: 0.000010  min_lr: 0.000000  loss: 3.9559 (3.6942)  class_acc: 0.2500 (0.3465)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6872  data: 0.2047  max mem: 15572
Epoch: [29]  [ 110/2809]  eta: 0:29:13  lr: 0.000010  min_lr: 0.000000  loss: 4.0047 (3.7007)  class_acc: 0.2917 (0.3472)  loss_scale: 65536.0000 (66716.8288)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.1919  max mem: 15572
Epoch: [29]  [ 120/2809]  eta: 0:29:10  lr: 0.000010  min_lr: 0.000000  loss: 3.9621 (3.6974)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.2188  max mem: 15572
Epoch: [29]  [ 130/2809]  eta: 0:29:14  lr: 0.000010  min_lr: 0.000000  loss: 3.7096 (3.7061)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (66536.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.2251  max mem: 15572
Epoch: [29]  [ 140/2809]  eta: 0:29:18  lr: 0.000010  min_lr: 0.000000  loss: 3.7556 (3.7151)  class_acc: 0.3333 (0.3434)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  time: 0.7061  data: 0.2423  max mem: 15572
Epoch: [29]  [ 150/2809]  eta: 0:29:06  lr: 0.000010  min_lr: 0.000000  loss: 3.7342 (3.7079)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (66404.0265)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.2418  max mem: 15572
Epoch: [29]  [ 160/2809]  eta: 0:28:15  lr: 0.000010  min_lr: 0.000000  loss: 3.6813 (3.7061)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (66350.1118)  weight_decay: 0.0500 (0.0500)  time: 0.5094  data: 0.1148  max mem: 15572
Epoch: [29]  [ 170/2809]  eta: 0:27:34  lr: 0.000010  min_lr: 0.000000  loss: 3.7412 (3.7122)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (66302.5029)  weight_decay: 0.0500 (0.0500)  time: 0.4009  data: 0.0004  max mem: 15572
Epoch: [29]  [ 180/2809]  eta: 0:27:08  lr: 0.000010  min_lr: 0.000000  loss: 3.7931 (3.7113)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (66260.1547)  weight_decay: 0.0500 (0.0500)  time: 0.4526  data: 0.0008  max mem: 15572
Epoch: [29]  [ 190/2809]  eta: 0:26:39  lr: 0.000010  min_lr: 0.000000  loss: 3.6236 (3.7130)  class_acc: 0.3333 (0.3432)  loss_scale: 65536.0000 (66222.2408)  weight_decay: 0.0500 (0.0500)  time: 0.4706  data: 0.0010  max mem: 15572
Epoch: [29]  [ 200/2809]  eta: 0:26:26  lr: 0.000010  min_lr: 0.000000  loss: 3.8398 (3.7300)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (66188.0995)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0625  max mem: 15572
[2025-01-16 04:10:58,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:10:58,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:11:02,783] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81666
[2025-01-16 04:11:02,783] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:11:02,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 210/2809]  eta: 0:26:24  lr: 0.000010  min_lr: 0.000000  loss: 3.9282 (3.7379)  class_acc: 0.2500 (0.3361)  loss_scale: 65536.0000 (67399.5829)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1783  max mem: 15572
Epoch: [29]  [ 220/2809]  eta: 0:26:16  lr: 0.000010  min_lr: 0.000000  loss: 3.9068 (3.7469)  class_acc: 0.2917 (0.3348)  loss_scale: 65536.0000 (67315.2579)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.1997  max mem: 15572
Epoch: [29]  [ 230/2809]  eta: 0:26:10  lr: 0.000010  min_lr: 0.000000  loss: 3.7689 (3.7441)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (67238.2338)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.1711  max mem: 15572
[2025-01-16 04:11:23,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81701
[2025-01-16 04:11:23,315] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:11:23,315] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 240/2809]  eta: 0:26:08  lr: 0.000010  min_lr: 0.000000  loss: 3.7179 (3.7468)  class_acc: 0.2500 (0.3347)  loss_scale: 65536.0000 (67031.6349)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.2001  max mem: 15572
Epoch: [29]  [ 250/2809]  eta: 0:25:49  lr: 0.000010  min_lr: 0.000000  loss: 3.7525 (3.7480)  class_acc: 0.2917 (0.3342)  loss_scale: 32768.0000 (65666.5498)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1414  max mem: 15572
Epoch: [29]  [ 260/2809]  eta: 0:25:27  lr: 0.000010  min_lr: 0.000000  loss: 3.7877 (3.7538)  class_acc: 0.2917 (0.3319)  loss_scale: 32768.0000 (64406.0690)  weight_decay: 0.0500 (0.0500)  time: 0.4623  data: 0.0294  max mem: 15572
Epoch: [29]  [ 270/2809]  eta: 0:25:13  lr: 0.000010  min_lr: 0.000000  loss: 3.7475 (3.7492)  class_acc: 0.2917 (0.3329)  loss_scale: 32768.0000 (63238.6125)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0420  max mem: 15572
Epoch: [29]  [ 280/2809]  eta: 0:25:05  lr: 0.000010  min_lr: 0.000000  loss: 3.7142 (3.7473)  class_acc: 0.3333 (0.3348)  loss_scale: 32768.0000 (62154.2491)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0968  max mem: 15572
Epoch: [29]  [ 290/2809]  eta: 0:24:57  lr: 0.000010  min_lr: 0.000000  loss: 3.6409 (3.7514)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (61144.4124)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1294  max mem: 15572
Epoch: [29]  [ 300/2809]  eta: 0:24:57  lr: 0.000010  min_lr: 0.000000  loss: 3.4882 (3.7454)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (60201.6744)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1816  max mem: 15572
Epoch: [29]  [ 310/2809]  eta: 0:24:41  lr: 0.000010  min_lr: 0.000000  loss: 3.8114 (3.7468)  class_acc: 0.2917 (0.3335)  loss_scale: 32768.0000 (59319.5627)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1331  max mem: 15572
Epoch: [29]  [ 320/2809]  eta: 0:24:33  lr: 0.000010  min_lr: 0.000000  loss: 3.8685 (3.7456)  class_acc: 0.2500 (0.3329)  loss_scale: 32768.0000 (58492.4112)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0867  max mem: 15572
Epoch: [29]  [ 330/2809]  eta: 0:24:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8855 (3.7481)  class_acc: 0.2917 (0.3325)  loss_scale: 32768.0000 (57715.2387)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0774  max mem: 15572
Epoch: [29]  [ 340/2809]  eta: 0:24:14  lr: 0.000010  min_lr: 0.000000  loss: 3.7601 (3.7484)  class_acc: 0.3333 (0.3328)  loss_scale: 32768.0000 (56983.6481)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.1087  max mem: 15572
Epoch: [29]  [ 350/2809]  eta: 0:24:07  lr: 0.000010  min_lr: 0.000000  loss: 3.8575 (3.7535)  class_acc: 0.2917 (0.3316)  loss_scale: 32768.0000 (56293.7436)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1573  max mem: 15572
Epoch: [29]  [ 360/2809]  eta: 0:24:05  lr: 0.000010  min_lr: 0.000000  loss: 3.8575 (3.7532)  class_acc: 0.3333 (0.3322)  loss_scale: 32768.0000 (55642.0609)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1671  max mem: 15572
[2025-01-16 04:12:33,503] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:12:33,504] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 370/2809]  eta: 0:23:51  lr: 0.000010  min_lr: 0.000000  loss: 3.6633 (3.7521)  class_acc: 0.3333 (0.3334)  loss_scale: 32768.0000 (55202.1563)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1287  max mem: 15572
Epoch: [29]  [ 380/2809]  eta: 0:23:46  lr: 0.000010  min_lr: 0.000000  loss: 3.6633 (3.7501)  class_acc: 0.3750 (0.3349)  loss_scale: 65536.0000 (55473.3858)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.0981  max mem: 15572
Epoch: [29]  [ 390/2809]  eta: 0:23:33  lr: 0.000010  min_lr: 0.000000  loss: 3.6277 (3.7481)  class_acc: 0.3750 (0.3363)  loss_scale: 65536.0000 (55730.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.0805  max mem: 15572
Epoch: [29]  [ 400/2809]  eta: 0:23:22  lr: 0.000010  min_lr: 0.000000  loss: 3.7125 (3.7535)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (55975.2618)  weight_decay: 0.0500 (0.0500)  time: 0.4837  data: 0.0331  max mem: 15572
Epoch: [29]  [ 410/2809]  eta: 0:23:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8427 (3.7509)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (56207.8832)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.0887  max mem: 15572
Epoch: [29]  [ 420/2809]  eta: 0:23:15  lr: 0.000010  min_lr: 0.000000  loss: 3.6437 (3.7488)  class_acc: 0.3750 (0.3368)  loss_scale: 65536.0000 (56429.4537)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1598  max mem: 15572
Epoch: [29]  [ 430/2809]  eta: 0:23:10  lr: 0.000010  min_lr: 0.000000  loss: 3.5636 (3.7438)  class_acc: 0.3750 (0.3372)  loss_scale: 65536.0000 (56640.7425)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1719  max mem: 15572
Epoch: [29]  [ 440/2809]  eta: 0:23:01  lr: 0.000010  min_lr: 0.000000  loss: 3.6256 (3.7447)  class_acc: 0.3333 (0.3365)  loss_scale: 65536.0000 (56842.4490)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1221  max mem: 15572
Epoch: [29]  [ 450/2809]  eta: 0:22:53  lr: 0.000010  min_lr: 0.000000  loss: 3.6256 (3.7418)  class_acc: 0.3750 (0.3372)  loss_scale: 65536.0000 (57035.2106)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0921  max mem: 15572
Epoch: [29]  [ 460/2809]  eta: 0:22:46  lr: 0.000010  min_lr: 0.000000  loss: 3.7048 (3.7429)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (57219.6095)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0985  max mem: 15572
Epoch: [29]  [ 470/2809]  eta: 0:22:39  lr: 0.000010  min_lr: 0.000000  loss: 3.7869 (3.7427)  class_acc: 0.2917 (0.3362)  loss_scale: 65536.0000 (57396.1783)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1133  max mem: 15572
Epoch: [29]  [ 480/2809]  eta: 0:22:34  lr: 0.000010  min_lr: 0.000000  loss: 3.8680 (3.7518)  class_acc: 0.2917 (0.3345)  loss_scale: 65536.0000 (57565.4054)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1438  max mem: 15572
Epoch: [29]  [ 490/2809]  eta: 0:22:31  lr: 0.000010  min_lr: 0.000000  loss: 3.9117 (3.7510)  class_acc: 0.2083 (0.3344)  loss_scale: 65536.0000 (57727.7393)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.1843  max mem: 15572
[2025-01-16 04:13:47,588] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:13:47,589] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:13:48,919] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 81961
[2025-01-16 04:13:48,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:13:48,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 500/2809]  eta: 0:22:28  lr: 0.000010  min_lr: 0.000000  loss: 3.8092 (3.7538)  class_acc: 0.2917 (0.3337)  loss_scale: 65536.0000 (58276.0240)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.2056  max mem: 15572
Epoch: [29]  [ 510/2809]  eta: 0:22:22  lr: 0.000010  min_lr: 0.000000  loss: 3.9300 (3.7575)  class_acc: 0.2500 (0.3330)  loss_scale: 65536.0000 (58418.0978)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.1697  max mem: 15572
Epoch: [29]  [ 520/2809]  eta: 0:22:14  lr: 0.000010  min_lr: 0.000000  loss: 3.9227 (3.7589)  class_acc: 0.2917 (0.3333)  loss_scale: 65536.0000 (58554.7179)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1136  max mem: 15572
Epoch: [29]  [ 530/2809]  eta: 0:22:10  lr: 0.000010  min_lr: 0.000000  loss: 3.9012 (3.7602)  class_acc: 0.3333 (0.3331)  loss_scale: 65536.0000 (58686.1921)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1390  max mem: 15572
[2025-01-16 04:14:09,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=82000, skipped=547, lr=[9.909992081020063e-08, 9.909992081020063e-08, 1.4157131544314378e-07, 1.4157131544314378e-07, 2.0224473634734827e-07, 2.0224473634734827e-07, 2.889210519247833e-07, 2.889210519247833e-07, 4.1274435989254753e-07, 4.1274435989254753e-07, 5.896347998464965e-07, 5.896347998464965e-07, 8.423354283521379e-07, 8.423354283521379e-07, 1.20333632621734e-06, 1.20333632621734e-06, 1.7190518945962e-06, 1.7190518945962e-06, 2.4557884208517147e-06, 2.4557884208517147e-06, 3.5082691726453065e-06, 3.5082691726453065e-06, 5.01181310377901e-06, 5.01181310377901e-06, 7.159733005398586e-06, 7.159733005398586e-06, 1.0228190007712266e-05, 1.0228190007712266e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:14:09,908] [INFO] [timer.py:260:stop] epoch=0/micro_step=82000/global_step=82000, RunningAvgSamplesPerSec=28.565467973384937, CurrSamplesPerSec=22.094801284994727, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [ 540/2809]  eta: 0:21:59  lr: 0.000010  min_lr: 0.000000  loss: 3.9589 (3.7654)  class_acc: 0.2917 (0.3322)  loss_scale: 65536.0000 (58812.8059)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.0952  max mem: 15572
Epoch: [29]  [ 550/2809]  eta: 0:21:53  lr: 0.000010  min_lr: 0.000000  loss: 3.9625 (3.7687)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (58934.8240)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0690  max mem: 15572
Epoch: [29]  [ 560/2809]  eta: 0:21:48  lr: 0.000010  min_lr: 0.000000  loss: 3.8688 (3.7688)  class_acc: 0.2917 (0.3309)  loss_scale: 65536.0000 (59052.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.1552  max mem: 15572
Epoch: [29]  [ 570/2809]  eta: 0:21:41  lr: 0.000010  min_lr: 0.000000  loss: 3.6508 (3.7677)  class_acc: 0.2917 (0.3311)  loss_scale: 65536.0000 (59166.0385)  weight_decay: 0.0500 (0.0500)  time: 0.5799  data: 0.1489  max mem: 15572
Epoch: [29]  [ 580/2809]  eta: 0:21:38  lr: 0.000010  min_lr: 0.000000  loss: 3.7472 (3.7678)  class_acc: 0.2917 (0.3308)  loss_scale: 65536.0000 (59275.6764)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.1537  max mem: 15572
Epoch: [29]  [ 590/2809]  eta: 0:21:29  lr: 0.000010  min_lr: 0.000000  loss: 3.9101 (3.7722)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (59381.6041)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1308  max mem: 15572
Epoch: [29]  [ 600/2809]  eta: 0:21:23  lr: 0.000010  min_lr: 0.000000  loss: 4.0799 (3.7746)  class_acc: 0.3333 (0.3300)  loss_scale: 65536.0000 (59484.0067)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.1060  max mem: 15572
Epoch: [29]  [ 610/2809]  eta: 0:21:17  lr: 0.000010  min_lr: 0.000000  loss: 3.8845 (3.7750)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (59583.0573)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1258  max mem: 15572
Epoch: [29]  [ 620/2809]  eta: 0:21:09  lr: 0.000010  min_lr: 0.000000  loss: 3.9239 (3.7785)  class_acc: 0.2917 (0.3293)  loss_scale: 65536.0000 (59678.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1125  max mem: 15572
[2025-01-16 04:15:02,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:15:02,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:15:03,011] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82091
[2025-01-16 04:15:03,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:15:03,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 630/2809]  eta: 0:21:06  lr: 0.000010  min_lr: 0.000000  loss: 3.9571 (3.7809)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (59875.6006)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1686  max mem: 15572
Epoch: [29]  [ 640/2809]  eta: 0:21:01  lr: 0.000010  min_lr: 0.000000  loss: 3.8269 (3.7804)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (59963.9064)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.1785  max mem: 15572
Epoch: [29]  [ 650/2809]  eta: 0:20:55  lr: 0.000010  min_lr: 0.000000  loss: 3.8269 (3.7806)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (60049.4992)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1350  max mem: 15572
Epoch: [29]  [ 660/2809]  eta: 0:20:48  lr: 0.000010  min_lr: 0.000000  loss: 3.8788 (3.7808)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (60132.5023)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1039  max mem: 15572
Epoch: [29]  [ 670/2809]  eta: 0:20:42  lr: 0.000010  min_lr: 0.000000  loss: 3.8774 (3.7822)  class_acc: 0.2500 (0.3284)  loss_scale: 65536.0000 (60213.0313)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.0885  max mem: 15572
Epoch: [29]  [ 680/2809]  eta: 0:20:34  lr: 0.000010  min_lr: 0.000000  loss: 3.8216 (3.7810)  class_acc: 0.2917 (0.3283)  loss_scale: 65536.0000 (60291.1953)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.1061  max mem: 15572
Epoch: [29]  [ 690/2809]  eta: 0:20:29  lr: 0.000010  min_lr: 0.000000  loss: 3.6686 (3.7809)  class_acc: 0.3333 (0.3288)  loss_scale: 65536.0000 (60367.0970)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1198  max mem: 15572
Epoch: [29]  [ 700/2809]  eta: 0:20:22  lr: 0.000010  min_lr: 0.000000  loss: 3.5815 (3.7804)  class_acc: 0.3750 (0.3294)  loss_scale: 65536.0000 (60440.8331)  weight_decay: 0.0500 (0.0500)  time: 0.5663  data: 0.1049  max mem: 15572
Epoch: [29]  [ 710/2809]  eta: 0:20:19  lr: 0.000010  min_lr: 0.000000  loss: 3.6568 (3.7811)  class_acc: 0.2917 (0.3291)  loss_scale: 65536.0000 (60512.4951)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1547  max mem: 15572
Epoch: [29]  [ 720/2809]  eta: 0:20:09  lr: 0.000010  min_lr: 0.000000  loss: 3.7517 (3.7786)  class_acc: 0.2917 (0.3296)  loss_scale: 65536.0000 (60582.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1389  max mem: 15572
Epoch: [29]  [ 730/2809]  eta: 0:20:03  lr: 0.000010  min_lr: 0.000000  loss: 3.6409 (3.7774)  class_acc: 0.2917 (0.3298)  loss_scale: 65536.0000 (60649.9371)  weight_decay: 0.0500 (0.0500)  time: 0.5114  data: 0.0831  max mem: 15572
Epoch: [29]  [ 740/2809]  eta: 0:19:54  lr: 0.000010  min_lr: 0.000000  loss: 3.6258 (3.7756)  class_acc: 0.3333 (0.3309)  loss_scale: 65536.0000 (60715.8758)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0727  max mem: 15572
Epoch: [29]  [ 750/2809]  eta: 0:19:48  lr: 0.000010  min_lr: 0.000000  loss: 3.6258 (3.7716)  class_acc: 0.3750 (0.3322)  loss_scale: 65536.0000 (60780.0586)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0711  max mem: 15572
[2025-01-16 04:16:15,310] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:16:15,310] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [ 760/2809]  eta: 0:19:43  lr: 0.000010  min_lr: 0.000000  loss: 3.5931 (3.7711)  class_acc: 0.3750 (0.3322)  loss_scale: 65536.0000 (61014.7911)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1427  max mem: 15572
[2025-01-16 04:16:17,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82224
[2025-01-16 04:16:17,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:16:17,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 770/2809]  eta: 0:19:35  lr: 0.000010  min_lr: 0.000000  loss: 3.6675 (3.7708)  class_acc: 0.3333 (0.3324)  loss_scale: 65536.0000 (61243.4345)  weight_decay: 0.0500 (0.0500)  time: 0.5511  data: 0.1261  max mem: 15572
Epoch: [29]  [ 780/2809]  eta: 0:19:29  lr: 0.000010  min_lr: 0.000000  loss: 3.5899 (3.7681)  class_acc: 0.3333 (0.3326)  loss_scale: 65536.0000 (61298.3969)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0924  max mem: 15572
Epoch: [29]  [ 790/2809]  eta: 0:19:24  lr: 0.000010  min_lr: 0.000000  loss: 3.5932 (3.7678)  class_acc: 0.3750 (0.3328)  loss_scale: 65536.0000 (61351.9697)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1401  max mem: 15572
Epoch: [29]  [ 800/2809]  eta: 0:19:19  lr: 0.000010  min_lr: 0.000000  loss: 3.8059 (3.7691)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (61404.2047)  weight_decay: 0.0500 (0.0500)  time: 0.6109  data: 0.1718  max mem: 15572
Epoch: [29]  [ 810/2809]  eta: 0:19:11  lr: 0.000010  min_lr: 0.000000  loss: 3.8982 (3.7694)  class_acc: 0.2500 (0.3315)  loss_scale: 65536.0000 (61455.1517)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0973  max mem: 15572
Epoch: [29]  [ 820/2809]  eta: 0:19:07  lr: 0.000010  min_lr: 0.000000  loss: 3.7289 (3.7670)  class_acc: 0.2917 (0.3322)  loss_scale: 65536.0000 (61504.8575)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1080  max mem: 15572
Epoch: [29]  [ 830/2809]  eta: 0:19:01  lr: 0.000010  min_lr: 0.000000  loss: 3.6645 (3.7656)  class_acc: 0.3750 (0.3328)  loss_scale: 65536.0000 (61553.3670)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.1375  max mem: 15572
Epoch: [29]  [ 840/2809]  eta: 0:18:55  lr: 0.000010  min_lr: 0.000000  loss: 3.8363 (3.7674)  class_acc: 0.2917 (0.3323)  loss_scale: 65536.0000 (61600.7229)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1104  max mem: 15572
Epoch: [29]  [ 850/2809]  eta: 0:18:49  lr: 0.000010  min_lr: 0.000000  loss: 3.8741 (3.7671)  class_acc: 0.2917 (0.3324)  loss_scale: 65536.0000 (61646.9659)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1318  max mem: 15572
Epoch: [29]  [ 860/2809]  eta: 0:18:42  lr: 0.000010  min_lr: 0.000000  loss: 3.8468 (3.7673)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (61692.1347)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.1059  max mem: 15572
Epoch: [29]  [ 870/2809]  eta: 0:18:35  lr: 0.000010  min_lr: 0.000000  loss: 3.8468 (3.7681)  class_acc: 0.2500 (0.3316)  loss_scale: 65536.0000 (61736.2664)  weight_decay: 0.0500 (0.0500)  time: 0.5038  data: 0.0691  max mem: 15572
Epoch: [29]  [ 880/2809]  eta: 0:18:29  lr: 0.000010  min_lr: 0.000000  loss: 3.7824 (3.7669)  class_acc: 0.2917 (0.3317)  loss_scale: 65536.0000 (61779.3961)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0940  max mem: 15572
Epoch: [29]  [ 890/2809]  eta: 0:18:22  lr: 0.000010  min_lr: 0.000000  loss: 3.6949 (3.7665)  class_acc: 0.3333 (0.3322)  loss_scale: 65536.0000 (61821.5578)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1117  max mem: 15572
[2025-01-16 04:17:29,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:17:29,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:17:29,620] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82354
[2025-01-16 04:17:29,620] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:17:29,620] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 900/2809]  eta: 0:18:18  lr: 0.000010  min_lr: 0.000000  loss: 3.7598 (3.7672)  class_acc: 0.2917 (0.3318)  loss_scale: 65536.0000 (61935.5205)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1525  max mem: 15572
Epoch: [29]  [ 910/2809]  eta: 0:18:11  lr: 0.000010  min_lr: 0.000000  loss: 3.9648 (3.7684)  class_acc: 0.2917 (0.3314)  loss_scale: 65536.0000 (61975.0428)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1449  max mem: 15572
Epoch: [29]  [ 920/2809]  eta: 0:18:05  lr: 0.000010  min_lr: 0.000000  loss: 3.8720 (3.7673)  class_acc: 0.2917 (0.3317)  loss_scale: 65536.0000 (62013.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0895  max mem: 15572
Epoch: [29]  [ 930/2809]  eta: 0:17:58  lr: 0.000010  min_lr: 0.000000  loss: 3.8720 (3.7678)  class_acc: 0.2917 (0.3317)  loss_scale: 65536.0000 (62051.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0769  max mem: 15572
Epoch: [29]  [ 940/2809]  eta: 0:17:51  lr: 0.000010  min_lr: 0.000000  loss: 3.8173 (3.7667)  class_acc: 0.3333 (0.3318)  loss_scale: 65536.0000 (62088.5696)  weight_decay: 0.0500 (0.0500)  time: 0.5075  data: 0.0642  max mem: 15572
Epoch: [29]  [ 950/2809]  eta: 0:17:47  lr: 0.000010  min_lr: 0.000000  loss: 3.7688 (3.7686)  class_acc: 0.2917 (0.3314)  loss_scale: 65536.0000 (62124.8202)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1454  max mem: 15572
Epoch: [29]  [ 960/2809]  eta: 0:17:41  lr: 0.000010  min_lr: 0.000000  loss: 3.9992 (3.7714)  class_acc: 0.2500 (0.3309)  loss_scale: 65536.0000 (62160.3163)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.1829  max mem: 15572
Epoch: [29]  [ 970/2809]  eta: 0:17:36  lr: 0.000010  min_lr: 0.000000  loss: 3.9008 (3.7712)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (62195.0814)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1377  max mem: 15572
Epoch: [29]  [ 980/2809]  eta: 0:17:30  lr: 0.000010  min_lr: 0.000000  loss: 3.7031 (3.7713)  class_acc: 0.3333 (0.3306)  loss_scale: 65536.0000 (62229.1376)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1250  max mem: 15572
Epoch: [29]  [ 990/2809]  eta: 0:17:24  lr: 0.000010  min_lr: 0.000000  loss: 3.9437 (3.7737)  class_acc: 0.2083 (0.3298)  loss_scale: 65536.0000 (62262.5066)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1259  max mem: 15572
Epoch: [29]  [1000/2809]  eta: 0:17:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8941 (3.7748)  class_acc: 0.2083 (0.3292)  loss_scale: 65536.0000 (62295.2088)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1105  max mem: 15572
Epoch: [29]  [1010/2809]  eta: 0:17:12  lr: 0.000010  min_lr: 0.000000  loss: 3.8150 (3.7737)  class_acc: 0.2917 (0.3290)  loss_scale: 65536.0000 (62327.2641)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1100  max mem: 15572
Epoch: [29]  [1020/2809]  eta: 0:17:04  lr: 0.000010  min_lr: 0.000000  loss: 3.5124 (3.7721)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (62358.6915)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0763  max mem: 15572
[2025-01-16 04:18:42,159] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:18:42,159] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:18:43,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82485
[2025-01-16 04:18:43,103] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:18:43,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1030/2809]  eta: 0:16:58  lr: 0.000010  min_lr: 0.000000  loss: 3.6219 (3.7713)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (62516.6402)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0553  max mem: 15572
Epoch: [29]  [1040/2809]  eta: 0:16:52  lr: 0.000010  min_lr: 0.000000  loss: 3.6780 (3.7714)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (62545.6446)  weight_decay: 0.0500 (0.0500)  time: 0.5479  data: 0.1094  max mem: 15572
Epoch: [29]  [1050/2809]  eta: 0:16:47  lr: 0.000010  min_lr: 0.000000  loss: 3.9155 (3.7739)  class_acc: 0.3333 (0.3291)  loss_scale: 65536.0000 (62574.0971)  weight_decay: 0.0500 (0.0500)  time: 0.6070  data: 0.1720  max mem: 15572
Epoch: [29]  [1060/2809]  eta: 0:16:42  lr: 0.000010  min_lr: 0.000000  loss: 3.9303 (3.7747)  class_acc: 0.2500 (0.3287)  loss_scale: 65536.0000 (62602.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1700  max mem: 15572
Epoch: [29]  [1070/2809]  eta: 0:16:36  lr: 0.000010  min_lr: 0.000000  loss: 3.7969 (3.7744)  class_acc: 0.2500 (0.3288)  loss_scale: 65536.0000 (62629.4080)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1110  max mem: 15572
Epoch: [29]  [1080/2809]  eta: 0:16:29  lr: 0.000010  min_lr: 0.000000  loss: 3.9913 (3.7754)  class_acc: 0.2500 (0.3287)  loss_scale: 65536.0000 (62656.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0651  max mem: 15572
Epoch: [29]  [1090/2809]  eta: 0:16:25  lr: 0.000010  min_lr: 0.000000  loss: 3.9913 (3.7777)  class_acc: 0.2500 (0.3279)  loss_scale: 65536.0000 (62682.6911)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1332  max mem: 15572
Epoch: [29]  [1100/2809]  eta: 0:16:20  lr: 0.000010  min_lr: 0.000000  loss: 3.8091 (3.7762)  class_acc: 0.2917 (0.3283)  loss_scale: 65536.0000 (62708.6067)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.1913  max mem: 15572
Epoch: [29]  [1110/2809]  eta: 0:16:13  lr: 0.000010  min_lr: 0.000000  loss: 3.6861 (3.7749)  class_acc: 0.3750 (0.3288)  loss_scale: 65536.0000 (62734.0558)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1264  max mem: 15572
Epoch: [29]  [1120/2809]  eta: 0:16:07  lr: 0.000010  min_lr: 0.000000  loss: 3.6639 (3.7738)  class_acc: 0.3750 (0.3291)  loss_scale: 65536.0000 (62759.0508)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.1109  max mem: 15572
Epoch: [29]  [1130/2809]  eta: 0:16:02  lr: 0.000010  min_lr: 0.000000  loss: 3.8469 (3.7754)  class_acc: 0.3333 (0.3288)  loss_scale: 65536.0000 (62783.6039)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1494  max mem: 15572
Epoch: [29]  [1140/2809]  eta: 0:15:55  lr: 0.000010  min_lr: 0.000000  loss: 4.0282 (3.7765)  class_acc: 0.2917 (0.3288)  loss_scale: 65536.0000 (62807.7266)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1097  max mem: 15572
Epoch: [29]  [1150/2809]  eta: 0:15:49  lr: 0.000010  min_lr: 0.000000  loss: 3.9619 (3.7773)  class_acc: 0.2917 (0.3287)  loss_scale: 65536.0000 (62831.4301)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.1080  max mem: 15572
[2025-01-16 04:19:56,424] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:19:56,424] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:19:56,823] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82615
[2025-01-16 04:19:56,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:19:56,823] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1160/2809]  eta: 0:15:41  lr: 0.000010  min_lr: 0.000000  loss: 3.9619 (3.7781)  class_acc: 0.2500 (0.3284)  loss_scale: 65536.0000 (62911.1731)  weight_decay: 0.0500 (0.0500)  time: 0.5046  data: 0.0867  max mem: 15572
Epoch: [29]  [1170/2809]  eta: 0:15:35  lr: 0.000010  min_lr: 0.000000  loss: 4.1078 (3.7799)  class_acc: 0.2500 (0.3280)  loss_scale: 65536.0000 (62933.5884)  weight_decay: 0.0500 (0.0500)  time: 0.4682  data: 0.0390  max mem: 15572
Epoch: [29]  [1180/2809]  eta: 0:15:28  lr: 0.000010  min_lr: 0.000000  loss: 3.8401 (3.7791)  class_acc: 0.2917 (0.3282)  loss_scale: 65536.0000 (62955.6240)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0721  max mem: 15572
Epoch: [29]  [1190/2809]  eta: 0:15:23  lr: 0.000010  min_lr: 0.000000  loss: 3.7590 (3.7796)  class_acc: 0.3333 (0.3283)  loss_scale: 65536.0000 (62977.2897)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1230  max mem: 15572
Epoch: [29]  [1200/2809]  eta: 0:15:16  lr: 0.000010  min_lr: 0.000000  loss: 3.6412 (3.7776)  class_acc: 0.3333 (0.3282)  loss_scale: 65536.0000 (62998.5945)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1338  max mem: 15572
Epoch: [29]  [1210/2809]  eta: 0:15:11  lr: 0.000010  min_lr: 0.000000  loss: 3.5245 (3.7770)  class_acc: 0.2917 (0.3285)  loss_scale: 65536.0000 (63019.5475)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1130  max mem: 15572
Epoch: [29]  [1220/2809]  eta: 0:15:04  lr: 0.000010  min_lr: 0.000000  loss: 3.8080 (3.7778)  class_acc: 0.3333 (0.3284)  loss_scale: 65536.0000 (63040.1572)  weight_decay: 0.0500 (0.0500)  time: 0.5292  data: 0.0877  max mem: 15572
Epoch: [29]  [1230/2809]  eta: 0:14:59  lr: 0.000010  min_lr: 0.000000  loss: 3.8080 (3.7783)  class_acc: 0.3333 (0.3283)  loss_scale: 65536.0000 (63060.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0934  max mem: 15572
[2025-01-16 04:20:40,967] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82699
[2025-01-16 04:20:40,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:20:40,968] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1240/2809]  eta: 0:14:51  lr: 0.000010  min_lr: 0.000000  loss: 3.8744 (3.7797)  class_acc: 0.2917 (0.3279)  loss_scale: 65536.0000 (63001.1668)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0846  max mem: 15572
Epoch: [29]  [1250/2809]  eta: 0:14:46  lr: 0.000010  min_lr: 0.000000  loss: 3.8504 (3.7787)  class_acc: 0.3333 (0.3284)  loss_scale: 32768.0000 (62759.4948)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0776  max mem: 15572
Epoch: [29]  [1260/2809]  eta: 0:14:40  lr: 0.000010  min_lr: 0.000000  loss: 3.6871 (3.7793)  class_acc: 0.3333 (0.3284)  loss_scale: 32768.0000 (62521.6558)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1090  max mem: 15572
Epoch: [29]  [1270/2809]  eta: 0:14:35  lr: 0.000010  min_lr: 0.000000  loss: 3.7325 (3.7775)  class_acc: 0.3333 (0.3287)  loss_scale: 32768.0000 (62287.5594)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1396  max mem: 15572
Epoch: [29]  [1280/2809]  eta: 0:14:29  lr: 0.000010  min_lr: 0.000000  loss: 3.6900 (3.7763)  class_acc: 0.3333 (0.3291)  loss_scale: 32768.0000 (62057.1179)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1384  max mem: 15572
Epoch: [29]  [1290/2809]  eta: 0:14:23  lr: 0.000010  min_lr: 0.000000  loss: 3.6831 (3.7770)  class_acc: 0.3333 (0.3288)  loss_scale: 32768.0000 (61830.2463)  weight_decay: 0.0500 (0.0500)  time: 0.5379  data: 0.1073  max mem: 15572
Epoch: [29]  [1300/2809]  eta: 0:14:18  lr: 0.000010  min_lr: 0.000000  loss: 3.8630 (3.7775)  class_acc: 0.2917 (0.3287)  loss_scale: 32768.0000 (61606.8624)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1737  max mem: 15572
Epoch: [29]  [1310/2809]  eta: 0:14:13  lr: 0.000010  min_lr: 0.000000  loss: 3.8195 (3.7773)  class_acc: 0.2917 (0.3284)  loss_scale: 32768.0000 (61386.8863)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1766  max mem: 15572
Epoch: [29]  [1320/2809]  eta: 0:14:07  lr: 0.000010  min_lr: 0.000000  loss: 3.7861 (3.7765)  class_acc: 0.3333 (0.3286)  loss_scale: 32768.0000 (61170.2407)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1301  max mem: 15572
Epoch: [29]  [1330/2809]  eta: 0:14:01  lr: 0.000010  min_lr: 0.000000  loss: 3.7861 (3.7759)  class_acc: 0.3333 (0.3286)  loss_scale: 32768.0000 (60956.8505)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.1132  max mem: 15572
Epoch: [29]  [1340/2809]  eta: 0:13:55  lr: 0.000010  min_lr: 0.000000  loss: 3.7499 (3.7761)  class_acc: 0.3750 (0.3287)  loss_scale: 32768.0000 (60746.6428)  weight_decay: 0.0500 (0.0500)  time: 0.5490  data: 0.1013  max mem: 15572
Epoch: [29]  [1350/2809]  eta: 0:13:50  lr: 0.000010  min_lr: 0.000000  loss: 3.7499 (3.7763)  class_acc: 0.3750 (0.3287)  loss_scale: 32768.0000 (60539.5470)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1578  max mem: 15572
Epoch: [29]  [1360/2809]  eta: 0:13:45  lr: 0.000010  min_lr: 0.000000  loss: 3.7075 (3.7752)  class_acc: 0.3750 (0.3289)  loss_scale: 32768.0000 (60335.4945)  weight_decay: 0.0500 (0.0500)  time: 0.6093  data: 0.1703  max mem: 15572
[2025-01-16 04:21:54,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:21:54,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1370/2809]  eta: 0:13:38  lr: 0.000010  min_lr: 0.000000  loss: 3.6989 (3.7756)  class_acc: 0.3333 (0.3286)  loss_scale: 32768.0000 (60230.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0611  max mem: 15572
Epoch: [29]  [1380/2809]  eta: 0:13:32  lr: 0.000010  min_lr: 0.000000  loss: 3.8949 (3.7762)  class_acc: 0.2500 (0.3281)  loss_scale: 65536.0000 (60268.4432)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0882  max mem: 15572
Epoch: [29]  [1390/2809]  eta: 0:13:27  lr: 0.000010  min_lr: 0.000000  loss: 4.0061 (3.7780)  class_acc: 0.2500 (0.3277)  loss_scale: 65536.0000 (60306.3120)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1429  max mem: 15572
Epoch: [29]  [1400/2809]  eta: 0:13:21  lr: 0.000010  min_lr: 0.000000  loss: 3.8596 (3.7775)  class_acc: 0.3333 (0.3280)  loss_scale: 65536.0000 (60343.6403)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1170  max mem: 15572
Epoch: [29]  [1410/2809]  eta: 0:13:15  lr: 0.000010  min_lr: 0.000000  loss: 3.6299 (3.7764)  class_acc: 0.3333 (0.3280)  loss_scale: 65536.0000 (60380.4394)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1530  max mem: 15572
Epoch: [29]  [1420/2809]  eta: 0:13:11  lr: 0.000010  min_lr: 0.000000  loss: 3.6959 (3.7773)  class_acc: 0.2917 (0.3279)  loss_scale: 65536.0000 (60416.7206)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.2115  max mem: 15572
Epoch: [29]  [1430/2809]  eta: 0:13:05  lr: 0.000010  min_lr: 0.000000  loss: 3.8399 (3.7773)  class_acc: 0.2917 (0.3277)  loss_scale: 65536.0000 (60452.4948)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.1765  max mem: 15572
Epoch: [29]  [1440/2809]  eta: 0:12:58  lr: 0.000010  min_lr: 0.000000  loss: 3.6364 (3.7770)  class_acc: 0.2917 (0.3279)  loss_scale: 65536.0000 (60487.7724)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0988  max mem: 15572
Epoch: [29]  [1450/2809]  eta: 0:12:53  lr: 0.000010  min_lr: 0.000000  loss: 3.7267 (3.7766)  class_acc: 0.3333 (0.3279)  loss_scale: 65536.0000 (60522.5637)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1330  max mem: 15572
Epoch: [29]  [1460/2809]  eta: 0:12:48  lr: 0.000010  min_lr: 0.000000  loss: 3.5517 (3.7757)  class_acc: 0.3750 (0.3284)  loss_scale: 65536.0000 (60556.8789)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.1565  max mem: 15572
[2025-01-16 04:22:53,648] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 82930
[2025-01-16 04:22:53,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:22:53,648] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1470/2809]  eta: 0:12:42  lr: 0.000010  min_lr: 0.000000  loss: 3.8324 (3.7768)  class_acc: 0.3750 (0.3282)  loss_scale: 65536.0000 (60546.1754)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1346  max mem: 15572
Epoch: [29]  [1480/2809]  eta: 0:12:36  lr: 0.000010  min_lr: 0.000000  loss: 3.9085 (3.7776)  class_acc: 0.2917 (0.3282)  loss_scale: 32768.0000 (60358.6117)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1376  max mem: 15572
Epoch: [29]  [1490/2809]  eta: 0:12:31  lr: 0.000010  min_lr: 0.000000  loss: 3.8546 (3.7774)  class_acc: 0.3333 (0.3284)  loss_scale: 32768.0000 (60173.5641)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1289  max mem: 15572
Epoch: [29]  [1500/2809]  eta: 0:12:25  lr: 0.000010  min_lr: 0.000000  loss: 3.5231 (3.7750)  class_acc: 0.3750 (0.3291)  loss_scale: 32768.0000 (59990.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5953  data: 0.1535  max mem: 15572
Epoch: [29]  [1510/2809]  eta: 0:12:20  lr: 0.000010  min_lr: 0.000000  loss: 3.5231 (3.7757)  class_acc: 0.3750 (0.3293)  loss_scale: 32768.0000 (59810.8167)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.2095  max mem: 15572
Epoch: [29]  [1520/2809]  eta: 0:12:14  lr: 0.000010  min_lr: 0.000000  loss: 3.7055 (3.7745)  class_acc: 0.3750 (0.3297)  loss_scale: 32768.0000 (59633.0204)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1581  max mem: 15572
Epoch: [29]  [1530/2809]  eta: 0:12:09  lr: 0.000010  min_lr: 0.000000  loss: 3.7996 (3.7741)  class_acc: 0.3750 (0.3298)  loss_scale: 32768.0000 (59457.5467)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1179  max mem: 15572
[2025-01-16 04:23:33,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=83000, skipped=554, lr=[9.318606680013476e-08, 9.318606680013476e-08, 1.331229525716211e-07, 1.331229525716211e-07, 1.901756465308873e-07, 1.901756465308873e-07, 2.7167949504412473e-07, 2.7167949504412473e-07, 3.8811356434874963e-07, 3.8811356434874963e-07, 5.544479490696423e-07, 5.544479490696423e-07, 7.920684986709176e-07, 7.920684986709176e-07, 1.1315264266727396e-06, 1.1315264266727396e-06, 1.6164663238181993e-06, 1.6164663238181993e-06, 2.309237605454571e-06, 2.309237605454571e-06, 3.2989108649351014e-06, 3.2989108649351014e-06, 4.712729807050145e-06, 4.712729807050145e-06, 6.732471152928779e-06, 6.732471152928779e-06, 9.6178159327554e-06, 9.6178159327554e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:23:33,096] [INFO] [timer.py:260:stop] epoch=0/micro_step=83000/global_step=83000, RunningAvgSamplesPerSec=28.56917375664305, CurrSamplesPerSec=29.96252455620245, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [1540/2809]  eta: 0:12:03  lr: 0.000010  min_lr: 0.000000  loss: 3.8383 (3.7753)  class_acc: 0.2917 (0.3293)  loss_scale: 32768.0000 (59284.3504)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1143  max mem: 15572
Epoch: [29]  [1550/2809]  eta: 0:11:57  lr: 0.000010  min_lr: 0.000000  loss: 3.7622 (3.7734)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (59113.3875)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0874  max mem: 15572
Epoch: [29]  [1560/2809]  eta: 0:11:50  lr: 0.000010  min_lr: 0.000000  loss: 3.6358 (3.7736)  class_acc: 0.3333 (0.3298)  loss_scale: 32768.0000 (58944.6150)  weight_decay: 0.0500 (0.0500)  time: 0.5172  data: 0.0777  max mem: 15572
Epoch: [29]  [1570/2809]  eta: 0:11:44  lr: 0.000010  min_lr: 0.000000  loss: 3.6509 (3.7727)  class_acc: 0.3333 (0.3299)  loss_scale: 32768.0000 (58777.9911)  weight_decay: 0.0500 (0.0500)  time: 0.5192  data: 0.0830  max mem: 15572
Epoch: [29]  [1580/2809]  eta: 0:11:39  lr: 0.000010  min_lr: 0.000000  loss: 3.7856 (3.7728)  class_acc: 0.3750 (0.3300)  loss_scale: 32768.0000 (58613.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5442  data: 0.1118  max mem: 15572
Epoch: [29]  [1590/2809]  eta: 0:11:32  lr: 0.000010  min_lr: 0.000000  loss: 3.7352 (3.7715)  class_acc: 0.3750 (0.3302)  loss_scale: 32768.0000 (58451.0270)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0875  max mem: 15572
[2025-01-16 04:24:04,762] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:24:04,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1600/2809]  eta: 0:11:26  lr: 0.000010  min_lr: 0.000000  loss: 3.3853 (3.7705)  class_acc: 0.2917 (0.3302)  loss_scale: 32768.0000 (58352.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5153  data: 0.0604  max mem: 15572
Epoch: [29]  [1610/2809]  eta: 0:11:21  lr: 0.000010  min_lr: 0.000000  loss: 3.6004 (3.7703)  class_acc: 0.2917 (0.3302)  loss_scale: 65536.0000 (58396.6034)  weight_decay: 0.0500 (0.0500)  time: 0.5340  data: 0.0870  max mem: 15572
Epoch: [29]  [1620/2809]  eta: 0:11:14  lr: 0.000010  min_lr: 0.000000  loss: 3.6601 (3.7704)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (58440.6465)  weight_decay: 0.0500 (0.0500)  time: 0.5196  data: 0.0844  max mem: 15572
Epoch: [29]  [1630/2809]  eta: 0:11:08  lr: 0.000010  min_lr: 0.000000  loss: 3.7927 (3.7706)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (58484.1496)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0737  max mem: 15572
Epoch: [29]  [1640/2809]  eta: 0:11:03  lr: 0.000010  min_lr: 0.000000  loss: 3.7349 (3.7693)  class_acc: 0.3333 (0.3303)  loss_scale: 65536.0000 (58527.1225)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1156  max mem: 15572
[2025-01-16 04:24:33,454] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83110
[2025-01-16 04:24:33,455] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:24:33,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1650/2809]  eta: 0:10:57  lr: 0.000010  min_lr: 0.000000  loss: 3.7806 (3.7703)  class_acc: 0.2917 (0.3302)  loss_scale: 65536.0000 (58529.8801)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1647  max mem: 15572
Epoch: [29]  [1660/2809]  eta: 0:10:52  lr: 0.000010  min_lr: 0.000000  loss: 3.9143 (3.7708)  class_acc: 0.2917 (0.3299)  loss_scale: 32768.0000 (58374.7815)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.1697  max mem: 15572
Epoch: [29]  [1670/2809]  eta: 0:10:47  lr: 0.000010  min_lr: 0.000000  loss: 3.9232 (3.7715)  class_acc: 0.2917 (0.3296)  loss_scale: 32768.0000 (58221.5392)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1931  max mem: 15572
Epoch: [29]  [1680/2809]  eta: 0:10:41  lr: 0.000010  min_lr: 0.000000  loss: 3.8589 (3.7708)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (58070.1202)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1564  max mem: 15572
Epoch: [29]  [1690/2809]  eta: 0:10:36  lr: 0.000010  min_lr: 0.000000  loss: 3.6613 (3.7699)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (57920.4920)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.0977  max mem: 15572
Epoch: [29]  [1700/2809]  eta: 0:10:30  lr: 0.000010  min_lr: 0.000000  loss: 3.6327 (3.7693)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (57772.6232)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0975  max mem: 15572
Epoch: [29]  [1710/2809]  eta: 0:10:24  lr: 0.000010  min_lr: 0.000000  loss: 3.7840 (3.7694)  class_acc: 0.3750 (0.3303)  loss_scale: 32768.0000 (57626.4828)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0961  max mem: 15572
Epoch: [29]  [1720/2809]  eta: 0:10:18  lr: 0.000010  min_lr: 0.000000  loss: 3.7840 (3.7686)  class_acc: 0.3333 (0.3304)  loss_scale: 32768.0000 (57482.0407)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1143  max mem: 15572
Epoch: [29]  [1730/2809]  eta: 0:10:12  lr: 0.000010  min_lr: 0.000000  loss: 3.7814 (3.7689)  class_acc: 0.3333 (0.3303)  loss_scale: 32768.0000 (57339.2675)  weight_decay: 0.0500 (0.0500)  time: 0.5547  data: 0.1208  max mem: 15572
Epoch: [29]  [1740/2809]  eta: 0:10:07  lr: 0.000009  min_lr: 0.000000  loss: 3.6372 (3.7683)  class_acc: 0.3333 (0.3304)  loss_scale: 32768.0000 (57198.1344)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1489  max mem: 15572
Epoch: [29]  [1750/2809]  eta: 0:10:01  lr: 0.000009  min_lr: 0.000000  loss: 3.7974 (3.7695)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (57058.6134)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1407  max mem: 15572
Epoch: [29]  [1760/2809]  eta: 0:09:55  lr: 0.000009  min_lr: 0.000000  loss: 3.9087 (3.7705)  class_acc: 0.2917 (0.3299)  loss_scale: 32768.0000 (56920.6769)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.0974  max mem: 15572
Epoch: [29]  [1770/2809]  eta: 0:09:49  lr: 0.000009  min_lr: 0.000000  loss: 3.8150 (3.7703)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (56784.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0809  max mem: 15572
[2025-01-16 04:25:46,386] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:25:46,387] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [1780/2809]  eta: 0:09:43  lr: 0.000009  min_lr: 0.000000  loss: 3.7649 (3.7706)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (56704.6468)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0725  max mem: 15572
Epoch: [29]  [1790/2809]  eta: 0:09:38  lr: 0.000009  min_lr: 0.000000  loss: 3.7590 (3.7695)  class_acc: 0.3333 (0.3301)  loss_scale: 65536.0000 (56753.9564)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.1133  max mem: 15572
Epoch: [29]  [1800/2809]  eta: 0:09:33  lr: 0.000009  min_lr: 0.000000  loss: 3.9818 (3.7709)  class_acc: 0.2500 (0.3298)  loss_scale: 65536.0000 (56802.7185)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1604  max mem: 15572
Epoch: [29]  [1810/2809]  eta: 0:09:27  lr: 0.000009  min_lr: 0.000000  loss: 4.0716 (3.7714)  class_acc: 0.2500 (0.3297)  loss_scale: 65536.0000 (56850.9420)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1512  max mem: 15572
Epoch: [29]  [1820/2809]  eta: 0:09:21  lr: 0.000009  min_lr: 0.000000  loss: 3.9015 (3.7717)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (56898.6359)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1493  max mem: 15572
Epoch: [29]  [1830/2809]  eta: 0:09:15  lr: 0.000009  min_lr: 0.000000  loss: 3.9015 (3.7727)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (56945.8088)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.1089  max mem: 15572
Epoch: [29]  [1840/2809]  eta: 0:09:10  lr: 0.000009  min_lr: 0.000000  loss: 3.7881 (3.7726)  class_acc: 0.3333 (0.3295)  loss_scale: 65536.0000 (56992.4693)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1682  max mem: 15572
Epoch: [29]  [1850/2809]  eta: 0:09:05  lr: 0.000009  min_lr: 0.000000  loss: 3.8269 (3.7727)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57038.6256)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.2077  max mem: 15572
Epoch: [29]  [1860/2809]  eta: 0:08:59  lr: 0.000009  min_lr: 0.000000  loss: 3.6029 (3.7719)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57084.2859)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1414  max mem: 15572
Epoch: [29]  [1870/2809]  eta: 0:08:53  lr: 0.000009  min_lr: 0.000000  loss: 3.6378 (3.7722)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57129.4580)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0714  max mem: 15572
Epoch: [29]  [1880/2809]  eta: 0:08:47  lr: 0.000009  min_lr: 0.000000  loss: 3.6952 (3.7711)  class_acc: 0.3333 (0.3297)  loss_scale: 65536.0000 (57174.1499)  weight_decay: 0.0500 (0.0500)  time: 0.4955  data: 0.0503  max mem: 15572
[2025-01-16 04:26:48,841] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83350
[2025-01-16 04:26:48,842] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:26:48,843] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [1890/2809]  eta: 0:08:41  lr: 0.000009  min_lr: 0.000000  loss: 3.6784 (3.7708)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (57183.7123)  weight_decay: 0.0500 (0.0500)  time: 0.4841  data: 0.0555  max mem: 15572
Epoch: [29]  [1900/2809]  eta: 0:08:35  lr: 0.000009  min_lr: 0.000000  loss: 3.6777 (3.7702)  class_acc: 0.3750 (0.3301)  loss_scale: 32768.0000 (57055.2762)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0830  max mem: 15572
Epoch: [29]  [1910/2809]  eta: 0:08:29  lr: 0.000009  min_lr: 0.000000  loss: 3.6915 (3.7699)  class_acc: 0.3333 (0.3301)  loss_scale: 32768.0000 (56928.1842)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1366  max mem: 15572
Epoch: [29]  [1920/2809]  eta: 0:08:24  lr: 0.000009  min_lr: 0.000000  loss: 3.6944 (3.7691)  class_acc: 0.3750 (0.3307)  loss_scale: 32768.0000 (56802.4154)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1210  max mem: 15572
Epoch: [29]  [1930/2809]  eta: 0:08:18  lr: 0.000009  min_lr: 0.000000  loss: 3.6423 (3.7684)  class_acc: 0.4167 (0.3307)  loss_scale: 32768.0000 (56677.9492)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1577  max mem: 15572
Epoch: [29]  [1940/2809]  eta: 0:08:13  lr: 0.000009  min_lr: 0.000000  loss: 3.7657 (3.7685)  class_acc: 0.2917 (0.3306)  loss_scale: 32768.0000 (56554.7656)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1395  max mem: 15572
Epoch: [29]  [1950/2809]  eta: 0:08:07  lr: 0.000009  min_lr: 0.000000  loss: 3.7657 (3.7690)  class_acc: 0.2917 (0.3305)  loss_scale: 32768.0000 (56432.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0937  max mem: 15572
Epoch: [29]  [1960/2809]  eta: 0:08:01  lr: 0.000009  min_lr: 0.000000  loss: 3.8435 (3.7696)  class_acc: 0.2917 (0.3302)  loss_scale: 32768.0000 (56312.1673)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0780  max mem: 15572
Epoch: [29]  [1970/2809]  eta: 0:07:55  lr: 0.000009  min_lr: 0.000000  loss: 3.7192 (3.7687)  class_acc: 0.2917 (0.3302)  loss_scale: 32768.0000 (56192.7144)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0959  max mem: 15572
Epoch: [29]  [1980/2809]  eta: 0:07:49  lr: 0.000009  min_lr: 0.000000  loss: 3.6318 (3.7686)  class_acc: 0.2500 (0.3301)  loss_scale: 32768.0000 (56074.4674)  weight_decay: 0.0500 (0.0500)  time: 0.5387  data: 0.1078  max mem: 15572
Epoch: [29]  [1990/2809]  eta: 0:07:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8633 (3.7687)  class_acc: 0.2917 (0.3300)  loss_scale: 32768.0000 (55957.4083)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1275  max mem: 15572
Epoch: [29]  [2000/2809]  eta: 0:07:39  lr: 0.000009  min_lr: 0.000000  loss: 3.8125 (3.7691)  class_acc: 0.3333 (0.3299)  loss_scale: 32768.0000 (55841.5192)  weight_decay: 0.0500 (0.0500)  time: 0.6390  data: 0.2055  max mem: 15572
Epoch: [29]  [2010/2809]  eta: 0:07:33  lr: 0.000009  min_lr: 0.000000  loss: 3.8809 (3.7699)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (55726.7827)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1667  max mem: 15572
[2025-01-16 04:28:02,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:28:02,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2020/2809]  eta: 0:07:27  lr: 0.000009  min_lr: 0.000000  loss: 3.8349 (3.7693)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (55661.8229)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1070  max mem: 15572
Epoch: [29]  [2030/2809]  eta: 0:07:21  lr: 0.000009  min_lr: 0.000000  loss: 3.6328 (3.7695)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (55710.4402)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1146  max mem: 15572
Epoch: [29]  [2040/2809]  eta: 0:07:16  lr: 0.000009  min_lr: 0.000000  loss: 3.8868 (3.7701)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (55758.5811)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1016  max mem: 15572
Epoch: [29]  [2050/2809]  eta: 0:07:10  lr: 0.000009  min_lr: 0.000000  loss: 3.8266 (3.7699)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (55806.2526)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1038  max mem: 15572
Epoch: [29]  [2060/2809]  eta: 0:07:05  lr: 0.000009  min_lr: 0.000000  loss: 3.7291 (3.7703)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (55853.4614)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1248  max mem: 15572
Epoch: [29]  [2070/2809]  eta: 0:06:59  lr: 0.000009  min_lr: 0.000000  loss: 3.7366 (3.7703)  class_acc: 0.2917 (0.3293)  loss_scale: 65536.0000 (55900.2144)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0930  max mem: 15572
Epoch: [29]  [2080/2809]  eta: 0:06:53  lr: 0.000009  min_lr: 0.000000  loss: 3.7659 (3.7702)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (55946.5180)  weight_decay: 0.0500 (0.0500)  time: 0.5388  data: 0.0870  max mem: 15572
Epoch: [29]  [2090/2809]  eta: 0:06:47  lr: 0.000009  min_lr: 0.000000  loss: 3.7659 (3.7696)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (55992.3788)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1121  max mem: 15572
Epoch: [29]  [2100/2809]  eta: 0:06:42  lr: 0.000009  min_lr: 0.000000  loss: 3.8990 (3.7706)  class_acc: 0.2917 (0.3289)  loss_scale: 65536.0000 (56037.8030)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1469  max mem: 15572
Epoch: [29]  [2110/2809]  eta: 0:06:36  lr: 0.000009  min_lr: 0.000000  loss: 3.8756 (3.7704)  class_acc: 0.3333 (0.3291)  loss_scale: 65536.0000 (56082.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1290  max mem: 15572
Epoch: [29]  [2120/2809]  eta: 0:06:30  lr: 0.000009  min_lr: 0.000000  loss: 3.7598 (3.7706)  class_acc: 0.3333 (0.3289)  loss_scale: 65536.0000 (56127.3663)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0999  max mem: 15572
Epoch: [29]  [2130/2809]  eta: 0:06:25  lr: 0.000009  min_lr: 0.000000  loss: 3.7689 (3.7706)  class_acc: 0.2917 (0.3291)  loss_scale: 65536.0000 (56171.5176)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1151  max mem: 15572
Epoch: [29]  [2140/2809]  eta: 0:06:19  lr: 0.000009  min_lr: 0.000000  loss: 3.7324 (3.7703)  class_acc: 0.2917 (0.3290)  loss_scale: 65536.0000 (56215.2564)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1420  max mem: 15572
[2025-01-16 04:29:15,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:29:15,937] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [2150/2809]  eta: 0:06:14  lr: 0.000009  min_lr: 0.000000  loss: 3.8415 (3.7713)  class_acc: 0.2917 (0.3290)  loss_scale: 65536.0000 (56410.9270)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1577  max mem: 15572
[2025-01-16 04:29:19,887] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83616
[2025-01-16 04:29:19,888] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:29:19,888] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2160/2809]  eta: 0:06:08  lr: 0.000009  min_lr: 0.000000  loss: 3.8444 (3.7716)  class_acc: 0.3333 (0.3291)  loss_scale: 65536.0000 (56574.4600)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1074  max mem: 15572
Epoch: [29]  [2170/2809]  eta: 0:06:02  lr: 0.000009  min_lr: 0.000000  loss: 3.7504 (3.7714)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (56615.7384)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1028  max mem: 15572
Epoch: [29]  [2180/2809]  eta: 0:05:56  lr: 0.000009  min_lr: 0.000000  loss: 3.7268 (3.7714)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (56656.6382)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1283  max mem: 15572
Epoch: [29]  [2190/2809]  eta: 0:05:51  lr: 0.000009  min_lr: 0.000000  loss: 3.7012 (3.7706)  class_acc: 0.3750 (0.3295)  loss_scale: 65536.0000 (56697.1648)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1269  max mem: 15572
Epoch: [29]  [2200/2809]  eta: 0:05:45  lr: 0.000009  min_lr: 0.000000  loss: 3.7240 (3.7710)  class_acc: 0.3750 (0.3295)  loss_scale: 65536.0000 (56737.3230)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.0958  max mem: 15572
Epoch: [29]  [2210/2809]  eta: 0:05:39  lr: 0.000009  min_lr: 0.000000  loss: 3.7552 (3.7710)  class_acc: 0.3750 (0.3297)  loss_scale: 65536.0000 (56777.1180)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1104  max mem: 15572
Epoch: [29]  [2220/2809]  eta: 0:05:34  lr: 0.000009  min_lr: 0.000000  loss: 3.6604 (3.7711)  class_acc: 0.2917 (0.3297)  loss_scale: 65536.0000 (56816.5547)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1245  max mem: 15572
Epoch: [29]  [2230/2809]  eta: 0:05:28  lr: 0.000009  min_lr: 0.000000  loss: 3.8431 (3.7714)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (56855.6378)  weight_decay: 0.0500 (0.0500)  time: 0.5010  data: 0.0632  max mem: 15572
Epoch: [29]  [2240/2809]  eta: 0:05:22  lr: 0.000009  min_lr: 0.000000  loss: 3.8774 (3.7716)  class_acc: 0.2500 (0.3295)  loss_scale: 65536.0000 (56894.3722)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1213  max mem: 15572
Epoch: [29]  [2250/2809]  eta: 0:05:16  lr: 0.000009  min_lr: 0.000000  loss: 3.7707 (3.7720)  class_acc: 0.2917 (0.3293)  loss_scale: 65536.0000 (56932.7623)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1346  max mem: 15572
Epoch: [29]  [2260/2809]  eta: 0:05:11  lr: 0.000009  min_lr: 0.000000  loss: 3.7000 (3.7715)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (56970.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0751  max mem: 15572
Epoch: [29]  [2270/2809]  eta: 0:05:05  lr: 0.000009  min_lr: 0.000000  loss: 3.6814 (3.7711)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57008.5284)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1134  max mem: 15572
Epoch: [29]  [2280/2809]  eta: 0:04:59  lr: 0.000009  min_lr: 0.000000  loss: 3.7385 (3.7709)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57045.9132)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1340  max mem: 15572
[2025-01-16 04:30:31,461] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:30:31,461] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [2290/2809]  eta: 0:04:54  lr: 0.000009  min_lr: 0.000000  loss: 3.8521 (3.7718)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (57283.2126)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.1536  max mem: 15572
[2025-01-16 04:30:36,631] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83753
[2025-01-16 04:30:36,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:30:36,632] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2300/2809]  eta: 0:04:48  lr: 0.000009  min_lr: 0.000000  loss: 3.9519 (3.7721)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (57347.5602)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1385  max mem: 15572
Epoch: [29]  [2310/2809]  eta: 0:04:43  lr: 0.000009  min_lr: 0.000000  loss: 3.5777 (3.7708)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57382.9926)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1311  max mem: 15572
Epoch: [29]  [2320/2809]  eta: 0:04:37  lr: 0.000009  min_lr: 0.000000  loss: 3.6748 (3.7707)  class_acc: 0.3750 (0.3295)  loss_scale: 65536.0000 (57418.1198)  weight_decay: 0.0500 (0.0500)  time: 0.5720  data: 0.1075  max mem: 15572
Epoch: [29]  [2330/2809]  eta: 0:04:31  lr: 0.000009  min_lr: 0.000000  loss: 3.8014 (3.7706)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57452.9455)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1388  max mem: 15572
Epoch: [29]  [2340/2809]  eta: 0:04:25  lr: 0.000009  min_lr: 0.000000  loss: 3.8300 (3.7711)  class_acc: 0.3333 (0.3295)  loss_scale: 65536.0000 (57487.4737)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1416  max mem: 15572
[2025-01-16 04:31:09,879] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 83810
[2025-01-16 04:31:09,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:31:09,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [2350/2809]  eta: 0:04:20  lr: 0.000009  min_lr: 0.000000  loss: 3.8637 (3.7711)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (57493.8324)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.1043  max mem: 15572
Epoch: [29]  [2360/2809]  eta: 0:04:14  lr: 0.000009  min_lr: 0.000000  loss: 3.8363 (3.7714)  class_acc: 0.2917 (0.3293)  loss_scale: 32768.0000 (57389.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1501  max mem: 15572
Epoch: [29]  [2370/2809]  eta: 0:04:09  lr: 0.000009  min_lr: 0.000000  loss: 3.8050 (3.7714)  class_acc: 0.2500 (0.3291)  loss_scale: 32768.0000 (57285.2636)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1349  max mem: 15572
Epoch: [29]  [2380/2809]  eta: 0:04:03  lr: 0.000009  min_lr: 0.000000  loss: 3.8807 (3.7716)  class_acc: 0.2917 (0.3291)  loss_scale: 32768.0000 (57182.2932)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1509  max mem: 15572
Epoch: [29]  [2390/2809]  eta: 0:03:57  lr: 0.000009  min_lr: 0.000000  loss: 3.9563 (3.7719)  class_acc: 0.2917 (0.3291)  loss_scale: 32768.0000 (57080.1840)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1756  max mem: 15572
Epoch: [29]  [2400/2809]  eta: 0:03:52  lr: 0.000009  min_lr: 0.000000  loss: 3.7758 (3.7709)  class_acc: 0.2917 (0.3292)  loss_scale: 32768.0000 (56978.9254)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1022  max mem: 15572
Epoch: [29]  [2410/2809]  eta: 0:03:46  lr: 0.000009  min_lr: 0.000000  loss: 3.6650 (3.7713)  class_acc: 0.3333 (0.3291)  loss_scale: 32768.0000 (56878.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1010  max mem: 15572
Epoch: [29]  [2420/2809]  eta: 0:03:40  lr: 0.000009  min_lr: 0.000000  loss: 4.0157 (3.7722)  class_acc: 0.2500 (0.3289)  loss_scale: 32768.0000 (56778.9178)  weight_decay: 0.0500 (0.0500)  time: 0.5654  data: 0.1292  max mem: 15572
Epoch: [29]  [2430/2809]  eta: 0:03:35  lr: 0.000009  min_lr: 0.000000  loss: 3.9376 (3.7719)  class_acc: 0.2917 (0.3290)  loss_scale: 32768.0000 (56680.1481)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1191  max mem: 15572
Epoch: [29]  [2440/2809]  eta: 0:03:29  lr: 0.000009  min_lr: 0.000000  loss: 3.6156 (3.7710)  class_acc: 0.3333 (0.3291)  loss_scale: 32768.0000 (56582.1876)  weight_decay: 0.0500 (0.0500)  time: 0.5847  data: 0.1251  max mem: 15572
Epoch: [29]  [2450/2809]  eta: 0:03:23  lr: 0.000009  min_lr: 0.000000  loss: 3.5469 (3.7710)  class_acc: 0.3333 (0.3290)  loss_scale: 32768.0000 (56485.0265)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.0982  max mem: 15572
Epoch: [29]  [2460/2809]  eta: 0:03:18  lr: 0.000009  min_lr: 0.000000  loss: 3.6177 (3.7703)  class_acc: 0.3333 (0.3292)  loss_scale: 32768.0000 (56388.6550)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1328  max mem: 15572
Epoch: [29]  [2470/2809]  eta: 0:03:12  lr: 0.000009  min_lr: 0.000000  loss: 3.6177 (3.7699)  class_acc: 0.3333 (0.3293)  loss_scale: 32768.0000 (56293.0635)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1418  max mem: 15572
[2025-01-16 04:32:24,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:32:24,267] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [2480/2809]  eta: 0:03:06  lr: 0.000009  min_lr: 0.000000  loss: 3.8228 (3.7703)  class_acc: 0.3333 (0.3292)  loss_scale: 32768.0000 (56237.8654)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0926  max mem: 15572
Epoch: [29]  [2490/2809]  eta: 0:03:01  lr: 0.000009  min_lr: 0.000000  loss: 3.6663 (3.7695)  class_acc: 0.3333 (0.3293)  loss_scale: 65536.0000 (56275.1923)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0909  max mem: 15572
Epoch: [29]  [2500/2809]  eta: 0:02:55  lr: 0.000009  min_lr: 0.000000  loss: 3.6614 (3.7693)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (56312.2207)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.1024  max mem: 15572
Epoch: [29]  [2510/2809]  eta: 0:02:49  lr: 0.000009  min_lr: 0.000000  loss: 3.5772 (3.7688)  class_acc: 0.4167 (0.3295)  loss_scale: 65536.0000 (56348.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1506  max mem: 15572
Epoch: [29]  [2520/2809]  eta: 0:02:44  lr: 0.000009  min_lr: 0.000000  loss: 3.6750 (3.7686)  class_acc: 0.4167 (0.3296)  loss_scale: 65536.0000 (56385.3963)  weight_decay: 0.0500 (0.0500)  time: 0.6281  data: 0.1631  max mem: 15572
Epoch: [29]  [2530/2809]  eta: 0:02:38  lr: 0.000009  min_lr: 0.000000  loss: 3.7385 (3.7691)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (56421.5504)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1688  max mem: 15572
[2025-01-16 04:32:59,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=84000, skipped=559, lr=[8.740915324830693e-08, 8.740915324830693e-08, 1.2487021892615276e-07, 1.2487021892615276e-07, 1.7838602703736113e-07, 1.7838602703736113e-07, 2.5483718148194445e-07, 2.5483718148194445e-07, 3.6405311640277783e-07, 3.6405311640277783e-07, 5.20075880575397e-07, 5.20075880575397e-07, 7.429655436791385e-07, 7.429655436791385e-07, 1.0613793481130551e-06, 1.0613793481130551e-06, 1.5162562115900786e-06, 1.5162562115900786e-06, 2.166080302271541e-06, 2.166080302271541e-06, 3.0944004318164876e-06, 3.0944004318164876e-06, 4.420572045452126e-06, 4.420572045452126e-06, 6.315102922074465e-06, 6.315102922074465e-06, 9.021575602963522e-06, 9.021575602963522e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:32:59,671] [INFO] [timer.py:260:stop] epoch=0/micro_step=84000/global_step=84000, RunningAvgSamplesPerSec=28.5689686779393, CurrSamplesPerSec=28.90570146723362, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [29]  [2540/2809]  eta: 0:02:32  lr: 0.000009  min_lr: 0.000000  loss: 3.8672 (3.7693)  class_acc: 0.3333 (0.3297)  loss_scale: 65536.0000 (56457.4199)  weight_decay: 0.0500 (0.0500)  time: 0.5938  data: 0.1528  max mem: 15572
Epoch: [29]  [2550/2809]  eta: 0:02:27  lr: 0.000009  min_lr: 0.000000  loss: 3.8775 (3.7697)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (56493.0082)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1169  max mem: 15572
Epoch: [29]  [2560/2809]  eta: 0:02:21  lr: 0.000009  min_lr: 0.000000  loss: 3.8924 (3.7701)  class_acc: 0.2500 (0.3292)  loss_scale: 65536.0000 (56528.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1219  max mem: 15572
Epoch: [29]  [2570/2809]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000000  loss: 3.7892 (3.7692)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (56563.3543)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0981  max mem: 15572
Epoch: [29]  [2580/2809]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 3.8812 (3.7703)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (56598.1186)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1268  max mem: 15572
Epoch: [29]  [2590/2809]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000000  loss: 3.9167 (3.7699)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (56632.6144)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1361  max mem: 15572
Epoch: [29]  [2600/2809]  eta: 0:01:58  lr: 0.000009  min_lr: 0.000000  loss: 3.8144 (3.7704)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (56666.8451)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1091  max mem: 15572
[2025-01-16 04:33:37,786] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:33:37,786] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:33:40,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84070
[2025-01-16 04:33:40,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:33:40,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2610/2809]  eta: 0:01:53  lr: 0.000009  min_lr: 0.000000  loss: 3.7156 (3.7701)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (56776.1134)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1400  max mem: 15572
Epoch: [29]  [2620/2809]  eta: 0:01:47  lr: 0.000009  min_lr: 0.000000  loss: 3.7156 (3.7703)  class_acc: 0.2917 (0.3292)  loss_scale: 65536.0000 (56809.5353)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0833  max mem: 15572
Epoch: [29]  [2630/2809]  eta: 0:01:41  lr: 0.000009  min_lr: 0.000000  loss: 3.9909 (3.7714)  class_acc: 0.2917 (0.3291)  loss_scale: 65536.0000 (56842.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0246  max mem: 15572
Epoch: [29]  [2640/2809]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 3.9909 (3.7717)  class_acc: 0.2500 (0.3288)  loss_scale: 65536.0000 (56875.6198)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.0500  max mem: 15572
Epoch: [29]  [2650/2809]  eta: 0:01:30  lr: 0.000009  min_lr: 0.000000  loss: 3.6558 (3.7707)  class_acc: 0.3750 (0.3292)  loss_scale: 65536.0000 (56908.2882)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0368  max mem: 15572
Epoch: [29]  [2660/2809]  eta: 0:01:24  lr: 0.000009  min_lr: 0.000000  loss: 3.7056 (3.7712)  class_acc: 0.3750 (0.3292)  loss_scale: 65536.0000 (56940.7110)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.0825  max mem: 15572
Epoch: [29]  [2670/2809]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 3.9069 (3.7707)  class_acc: 0.2917 (0.3294)  loss_scale: 65536.0000 (56972.8911)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.0719  max mem: 15572
Epoch: [29]  [2680/2809]  eta: 0:01:13  lr: 0.000009  min_lr: 0.000000  loss: 3.8340 (3.7707)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57004.8310)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.0658  max mem: 15572
Epoch: [29]  [2690/2809]  eta: 0:01:07  lr: 0.000009  min_lr: 0.000000  loss: 3.6819 (3.7702)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57036.5336)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.0860  max mem: 15572
Epoch: [29]  [2700/2809]  eta: 0:01:01  lr: 0.000009  min_lr: 0.000000  loss: 3.6086 (3.7705)  class_acc: 0.3333 (0.3294)  loss_scale: 65536.0000 (57068.0015)  weight_decay: 0.0500 (0.0500)  time: 0.5200  data: 0.0753  max mem: 15572
Epoch: [29]  [2710/2809]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 3.6635 (3.7698)  class_acc: 0.3333 (0.3295)  loss_scale: 65536.0000 (57099.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1198  max mem: 15572
Epoch: [29]  [2720/2809]  eta: 0:00:50  lr: 0.000009  min_lr: 0.000000  loss: 3.8205 (3.7703)  class_acc: 0.3750 (0.3295)  loss_scale: 65536.0000 (57130.2433)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1336  max mem: 15572
Epoch: [29]  [2730/2809]  eta: 0:00:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8972 (3.7706)  class_acc: 0.3333 (0.3295)  loss_scale: 65536.0000 (57161.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.1412  max mem: 15572
[2025-01-16 04:34:52,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:34:52,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:34:53,232] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84200
[2025-01-16 04:34:53,232] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:34:53,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [2740/2809]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000000  loss: 3.7867 (3.7703)  class_acc: 0.3750 (0.3296)  loss_scale: 65536.0000 (57215.4863)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1574  max mem: 15572
Epoch: [29]  [2750/2809]  eta: 0:00:33  lr: 0.000009  min_lr: 0.000000  loss: 3.7317 (3.7701)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57245.7317)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0986  max mem: 15572
Epoch: [29]  [2760/2809]  eta: 0:00:27  lr: 0.000009  min_lr: 0.000000  loss: 3.8236 (3.7696)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (57275.7581)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1337  max mem: 15572
Epoch: [29]  [2770/2809]  eta: 0:00:22  lr: 0.000009  min_lr: 0.000000  loss: 3.8236 (3.7698)  class_acc: 0.3333 (0.3298)  loss_scale: 65536.0000 (57305.5677)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1201  max mem: 15572
Epoch: [29]  [2780/2809]  eta: 0:00:16  lr: 0.000009  min_lr: 0.000000  loss: 3.8638 (3.7704)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57335.1629)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0741  max mem: 15572
Epoch: [29]  [2790/2809]  eta: 0:00:10  lr: 0.000009  min_lr: 0.000000  loss: 3.9559 (3.7703)  class_acc: 0.2917 (0.3295)  loss_scale: 65536.0000 (57364.5460)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1612  max mem: 15572
Epoch: [29]  [2800/2809]  eta: 0:00:05  lr: 0.000009  min_lr: 0.000000  loss: 3.5004 (3.7697)  class_acc: 0.3333 (0.3296)  loss_scale: 65536.0000 (57393.7194)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1171  max mem: 15572
Epoch: [29]  [2808/2809]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 3.8355 (3.7700)  class_acc: 0.2500 (0.3294)  loss_scale: 65536.0000 (57416.9085)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0301  max mem: 15572
Epoch: [29] Total time: 0:26:34 (0.5676 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 3.8355 (3.7700)  class_acc: 0.2500 (0.3294)  loss_scale: 65536.0000 (57416.9085)  weight_decay: 0.0500 (0.0500)
[2025-01-16 04:35:30,416] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-01-16 04:35:30,418] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-29/mp_rank_00_model_states.pt
[2025-01-16 04:35:30,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-29/mp_rank_00_model_states.pt...
[2025-01-16 04:35:30,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-29/mp_rank_00_model_states.pt.
[2025-01-16 04:35:30,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/272]  eta: 0:20:05  loss: 0.4230 (0.4230)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.4318  data: 4.2565  max mem: 15572
Val:  [ 10/272]  eta: 0:03:20  loss: 2.3857 (2.2093)  acc1: 44.4444 (46.9697)  acc5: 77.7778 (73.7374)  time: 0.7641  data: 0.5680  max mem: 15572
Val:  [ 20/272]  eta: 0:02:08  loss: 2.3602 (2.2350)  acc1: 44.4444 (48.1481)  acc5: 77.7778 (74.6032)  time: 0.3145  data: 0.1201  max mem: 15572
Val:  [ 30/272]  eta: 0:01:45  loss: 2.3602 (2.3394)  acc1: 50.0000 (44.6237)  acc5: 77.7778 (74.3728)  time: 0.2580  data: 0.0574  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.4514 (2.3887)  acc1: 33.3333 (42.1409)  acc5: 77.7778 (74.3902)  time: 0.2859  data: 0.0844  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.4439 (2.3188)  acc1: 33.3333 (43.8998)  acc5: 77.7778 (75.8170)  time: 0.3747  data: 0.1812  max mem: 15572
Val:  [ 60/272]  eta: 0:01:21  loss: 1.4928 (2.2224)  acc1: 66.6667 (46.9035)  acc5: 88.8889 (76.8670)  time: 0.3554  data: 0.1668  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.5581 (2.1510)  acc1: 66.6667 (49.1393)  acc5: 83.3333 (77.5430)  time: 0.2985  data: 0.1132  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.8761 (2.1702)  acc1: 55.5556 (48.8340)  acc5: 77.7778 (77.2977)  time: 0.2950  data: 0.0920  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.1897 (2.1849)  acc1: 50.0000 (48.9011)  acc5: 77.7778 (77.7778)  time: 0.2792  data: 0.0639  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.1778 (2.2132)  acc1: 50.0000 (48.2398)  acc5: 83.3333 (77.2827)  time: 0.3115  data: 0.1102  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.4744 (2.2972)  acc1: 22.2222 (45.7958)  acc5: 61.1111 (75.6256)  time: 0.2931  data: 0.1047  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 3.0295 (2.3388)  acc1: 22.2222 (44.9495)  acc5: 61.1111 (74.9770)  time: 0.3248  data: 0.1381  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.1369 (2.3064)  acc1: 50.0000 (45.8439)  acc5: 77.7778 (75.6997)  time: 0.3747  data: 0.1694  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 1.7183 (2.3005)  acc1: 55.5556 (46.2569)  acc5: 88.8889 (75.4137)  time: 0.3516  data: 0.1170  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.2694 (2.3069)  acc1: 38.8889 (45.7322)  acc5: 77.7778 (75.7174)  time: 0.2816  data: 0.0684  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2694 (2.2931)  acc1: 44.4444 (46.3768)  acc5: 77.7778 (76.0525)  time: 0.2550  data: 0.0720  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.3687 (2.3127)  acc1: 44.4444 (45.7765)  acc5: 77.7778 (75.7635)  time: 0.3192  data: 0.1326  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.2640 (2.3025)  acc1: 38.8889 (45.7336)  acc5: 77.7778 (76.1203)  time: 0.3275  data: 0.1401  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.2576 (2.3521)  acc1: 38.8889 (44.5899)  acc5: 77.7778 (74.8691)  time: 0.3480  data: 0.1656  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.5117 (2.3576)  acc1: 38.8889 (44.3892)  acc5: 72.2222 (74.6545)  time: 0.3856  data: 0.2050  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.0692 (2.3634)  acc1: 44.4444 (44.3391)  acc5: 77.7778 (74.5919)  time: 0.3434  data: 0.1434  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1839 (2.3511)  acc1: 50.0000 (44.5450)  acc5: 77.7778 (74.8115)  time: 0.3187  data: 0.1043  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.7179 (2.3227)  acc1: 61.1111 (45.5267)  acc5: 83.3333 (75.1804)  time: 0.3114  data: 0.1074  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6375 (2.3077)  acc1: 61.1111 (45.8506)  acc5: 83.3333 (75.5187)  time: 0.3189  data: 0.1020  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.1928 (2.3188)  acc1: 38.8889 (45.1749)  acc5: 83.3333 (75.4980)  time: 0.3542  data: 0.1362  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.2537 (2.2620)  acc1: 72.2222 (46.8072)  acc5: 88.8889 (76.2239)  time: 0.3124  data: 0.1208  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2838 (2.2553)  acc1: 66.6667 (46.8020)  acc5: 88.8889 (76.4248)  time: 0.2036  data: 0.0299  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2838 (2.2603)  acc1: 66.6667 (46.7745)  acc5: 88.8889 (76.3875)  time: 0.1980  data: 0.0298  max mem: 15572
Val: Total time: 0:01:29 (0.3295 s / it)
* Acc@1 46.775 Acc@5 76.387 loss 2.260
Accuracy of the network on the 4883 val videos: 46.8%
[2025-01-16 04:37:00,455] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 04:37:00,461] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 04:37:00,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 04:37:03,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 04:37:03,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.77%
Epoch: [30]  [   0/2809]  eta: 5:36:49  lr: 0.000009  min_lr: 0.000000  loss: 2.9207 (2.9207)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 7.1946  data: 6.8027  max mem: 15572
Epoch: [30]  [  10/2809]  eta: 0:46:48  lr: 0.000009  min_lr: 0.000000  loss: 3.8214 (3.7717)  class_acc: 0.3333 (0.3295)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0034  data: 0.6188  max mem: 15572
Epoch: [30]  [  20/2809]  eta: 0:33:40  lr: 0.000009  min_lr: 0.000000  loss: 3.8900 (3.7830)  class_acc: 0.2917 (0.3135)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4009  data: 0.0005  max mem: 15572
Epoch: [30]  [  30/2809]  eta: 0:29:19  lr: 0.000009  min_lr: 0.000000  loss: 3.8900 (3.7266)  class_acc: 0.2917 (0.3185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4296  data: 0.0005  max mem: 15572
Epoch: [30]  [  40/2809]  eta: 0:28:40  lr: 0.000009  min_lr: 0.000000  loss: 3.8791 (3.7703)  class_acc: 0.2500 (0.3028)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0692  max mem: 15572
Epoch: [30]  [  50/2809]  eta: 0:29:35  lr: 0.000009  min_lr: 0.000000  loss: 3.8545 (3.7761)  class_acc: 0.2500 (0.2958)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6593  data: 0.1969  max mem: 15572
[2025-01-16 04:37:40,154] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:37:40,155] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [  60/2809]  eta: 0:29:07  lr: 0.000009  min_lr: 0.000000  loss: 3.8014 (3.7865)  class_acc: 0.2917 (0.2992)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.2035  max mem: 15572
Epoch: [30]  [  70/2809]  eta: 0:29:26  lr: 0.000009  min_lr: 0.000000  loss: 3.8297 (3.7705)  class_acc: 0.2917 (0.3052)  loss_scale: 131072.0000 (76612.5070)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2015  max mem: 15572
[2025-01-16 04:37:55,738] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84350
[2025-01-16 04:37:55,738] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:37:55,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [  80/2809]  eta: 0:29:25  lr: 0.000009  min_lr: 0.000000  loss: 3.8297 (3.7735)  class_acc: 0.2500 (0.3061)  loss_scale: 131072.0000 (82526.8148)  weight_decay: 0.0500 (0.0500)  time: 0.6813  data: 0.2342  max mem: 15572
Epoch: [30]  [  90/2809]  eta: 0:28:45  lr: 0.000009  min_lr: 0.000000  loss: 3.8039 (3.7579)  class_acc: 0.3333 (0.3178)  loss_scale: 65536.0000 (80659.6923)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1425  max mem: 15572
Epoch: [30]  [ 100/2809]  eta: 0:29:07  lr: 0.000009  min_lr: 0.000000  loss: 3.5684 (3.7423)  class_acc: 0.3333 (0.3259)  loss_scale: 65536.0000 (79162.2970)  weight_decay: 0.0500 (0.0500)  time: 0.6375  data: 0.1758  max mem: 15572
Epoch: [30]  [ 110/2809]  eta: 0:29:02  lr: 0.000009  min_lr: 0.000000  loss: 3.5684 (3.7263)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (77934.7027)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.2515  max mem: 15572
Epoch: [30]  [ 120/2809]  eta: 0:29:00  lr: 0.000009  min_lr: 0.000000  loss: 3.8038 (3.7447)  class_acc: 0.2917 (0.3254)  loss_scale: 65536.0000 (76910.0165)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.2023  max mem: 15572
Epoch: [30]  [ 130/2809]  eta: 0:28:44  lr: 0.000009  min_lr: 0.000000  loss: 3.8038 (3.7456)  class_acc: 0.2500 (0.3247)  loss_scale: 65536.0000 (76041.7710)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1712  max mem: 15572
Epoch: [30]  [ 140/2809]  eta: 0:28:54  lr: 0.000009  min_lr: 0.000000  loss: 3.7365 (3.7544)  class_acc: 0.2917 (0.3236)  loss_scale: 65536.0000 (75296.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.2118  max mem: 15572
Epoch: [30]  [ 150/2809]  eta: 0:28:42  lr: 0.000009  min_lr: 0.000000  loss: 3.9889 (3.7709)  class_acc: 0.2917 (0.3206)  loss_scale: 65536.0000 (74650.2781)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.2249  max mem: 15572
Epoch: [30]  [ 160/2809]  eta: 0:27:56  lr: 0.000009  min_lr: 0.000000  loss: 3.8648 (3.7657)  class_acc: 0.2917 (0.3214)  loss_scale: 65536.0000 (74084.1739)  weight_decay: 0.0500 (0.0500)  time: 0.5123  data: 0.0921  max mem: 15572
Epoch: [30]  [ 170/2809]  eta: 0:27:23  lr: 0.000009  min_lr: 0.000000  loss: 3.6777 (3.7569)  class_acc: 0.3333 (0.3241)  loss_scale: 65536.0000 (73584.2807)  weight_decay: 0.0500 (0.0500)  time: 0.4327  data: 0.0221  max mem: 15572
Epoch: [30]  [ 180/2809]  eta: 0:27:04  lr: 0.000009  min_lr: 0.000000  loss: 3.6163 (3.7569)  class_acc: 0.3750 (0.3246)  loss_scale: 65536.0000 (73139.6243)  weight_decay: 0.0500 (0.0500)  time: 0.4977  data: 0.0821  max mem: 15572
Epoch: [30]  [ 190/2809]  eta: 0:26:50  lr: 0.000009  min_lr: 0.000000  loss: 3.7102 (3.7616)  class_acc: 0.3750 (0.3255)  loss_scale: 65536.0000 (72741.5288)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1134  max mem: 15572
Epoch: [30]  [ 200/2809]  eta: 0:26:35  lr: 0.000009  min_lr: 0.000000  loss: 3.8370 (3.7811)  class_acc: 0.2500 (0.3215)  loss_scale: 65536.0000 (72383.0448)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1184  max mem: 15572
[2025-01-16 04:39:12,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:39:12,233] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:39:12,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84480
[2025-01-16 04:39:12,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:39:12,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 210/2809]  eta: 0:26:32  lr: 0.000009  min_lr: 0.000000  loss: 3.9191 (3.7795)  class_acc: 0.2917 (0.3219)  loss_scale: 65536.0000 (72369.1374)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1726  max mem: 15572
Epoch: [30]  [ 220/2809]  eta: 0:26:21  lr: 0.000009  min_lr: 0.000000  loss: 3.8448 (3.7797)  class_acc: 0.2917 (0.3211)  loss_scale: 65536.0000 (72059.9457)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1787  max mem: 15572
Epoch: [30]  [ 230/2809]  eta: 0:26:22  lr: 0.000009  min_lr: 0.000000  loss: 4.0081 (3.7912)  class_acc: 0.2500 (0.3184)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.1855  max mem: 15572
Epoch: [30]  [ 240/2809]  eta: 0:26:17  lr: 0.000009  min_lr: 0.000000  loss: 4.0081 (3.7925)  class_acc: 0.2917 (0.3190)  loss_scale: 65536.0000 (71518.5394)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.1948  max mem: 15572
Epoch: [30]  [ 250/2809]  eta: 0:26:00  lr: 0.000009  min_lr: 0.000000  loss: 3.7267 (3.7858)  class_acc: 0.3333 (0.3212)  loss_scale: 65536.0000 (71280.1912)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1228  max mem: 15572
Epoch: [30]  [ 260/2809]  eta: 0:25:56  lr: 0.000009  min_lr: 0.000000  loss: 3.7267 (3.7882)  class_acc: 0.3333 (0.3217)  loss_scale: 65536.0000 (71060.1073)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1269  max mem: 15572
Epoch: [30]  [ 270/2809]  eta: 0:25:42  lr: 0.000009  min_lr: 0.000000  loss: 3.6814 (3.7766)  class_acc: 0.3333 (0.3250)  loss_scale: 65536.0000 (70856.2657)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1151  max mem: 15572
Epoch: [30]  [ 280/2809]  eta: 0:25:35  lr: 0.000009  min_lr: 0.000000  loss: 3.4931 (3.7745)  class_acc: 0.3750 (0.3256)  loss_scale: 65536.0000 (70666.9324)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1057  max mem: 15572
Epoch: [30]  [ 290/2809]  eta: 0:25:28  lr: 0.000009  min_lr: 0.000000  loss: 3.5928 (3.7670)  class_acc: 0.3333 (0.3276)  loss_scale: 65536.0000 (70490.6117)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1473  max mem: 15572
Epoch: [30]  [ 300/2809]  eta: 0:25:09  lr: 0.000009  min_lr: 0.000000  loss: 3.8470 (3.7721)  class_acc: 0.2917 (0.3254)  loss_scale: 65536.0000 (70326.0066)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0838  max mem: 15572
Epoch: [30]  [ 310/2809]  eta: 0:24:57  lr: 0.000009  min_lr: 0.000000  loss: 4.0265 (3.7737)  class_acc: 0.2500 (0.3245)  loss_scale: 65536.0000 (70171.9871)  weight_decay: 0.0500 (0.0500)  time: 0.4880  data: 0.0649  max mem: 15572
Epoch: [30]  [ 320/2809]  eta: 0:24:56  lr: 0.000009  min_lr: 0.000000  loss: 3.7710 (3.7732)  class_acc: 0.3333 (0.3250)  loss_scale: 65536.0000 (70027.5639)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1790  max mem: 15572
Epoch: [30]  [ 330/2809]  eta: 0:24:46  lr: 0.000009  min_lr: 0.000000  loss: 3.8345 (3.7752)  class_acc: 0.2917 (0.3240)  loss_scale: 65536.0000 (69891.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1854  max mem: 15572
[2025-01-16 04:40:26,538] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:40:26,539] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:40:26,985] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84610
[2025-01-16 04:40:26,985] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:40:26,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 340/2809]  eta: 0:24:33  lr: 0.000009  min_lr: 0.000000  loss: 3.8524 (3.7756)  class_acc: 0.2917 (0.3238)  loss_scale: 65536.0000 (69956.3167)  weight_decay: 0.0500 (0.0500)  time: 0.5274  data: 0.1089  max mem: 15572
Epoch: [30]  [ 350/2809]  eta: 0:24:20  lr: 0.000009  min_lr: 0.000000  loss: 3.7830 (3.7718)  class_acc: 0.3750 (0.3254)  loss_scale: 65536.0000 (69830.3818)  weight_decay: 0.0500 (0.0500)  time: 0.5021  data: 0.0761  max mem: 15572
Epoch: [30]  [ 360/2809]  eta: 0:24:06  lr: 0.000009  min_lr: 0.000000  loss: 3.5662 (3.7670)  class_acc: 0.3333 (0.3268)  loss_scale: 65536.0000 (69711.4238)  weight_decay: 0.0500 (0.0500)  time: 0.4832  data: 0.0273  max mem: 15572
Epoch: [30]  [ 370/2809]  eta: 0:24:02  lr: 0.000009  min_lr: 0.000000  loss: 3.6166 (3.7637)  class_acc: 0.3333 (0.3284)  loss_scale: 65536.0000 (69598.8787)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.0904  max mem: 15572
Epoch: [30]  [ 380/2809]  eta: 0:23:54  lr: 0.000009  min_lr: 0.000000  loss: 3.7017 (3.7643)  class_acc: 0.3333 (0.3288)  loss_scale: 65536.0000 (69492.2415)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1526  max mem: 15572
Epoch: [30]  [ 390/2809]  eta: 0:23:52  lr: 0.000009  min_lr: 0.000000  loss: 3.8062 (3.7640)  class_acc: 0.3333 (0.3302)  loss_scale: 65536.0000 (69391.0588)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1727  max mem: 15572
Epoch: [30]  [ 400/2809]  eta: 0:23:53  lr: 0.000009  min_lr: 0.000000  loss: 3.9454 (3.7686)  class_acc: 0.2917 (0.3286)  loss_scale: 65536.0000 (69294.9227)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.2410  max mem: 15572
Epoch: [30]  [ 410/2809]  eta: 0:23:43  lr: 0.000009  min_lr: 0.000000  loss: 3.9968 (3.7737)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (69203.4647)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1762  max mem: 15572
Epoch: [30]  [ 420/2809]  eta: 0:23:33  lr: 0.000009  min_lr: 0.000000  loss: 4.0018 (3.7801)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (69116.3515)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0877  max mem: 15572
Epoch: [30]  [ 430/2809]  eta: 0:23:27  lr: 0.000009  min_lr: 0.000000  loss: 3.7421 (3.7711)  class_acc: 0.2917 (0.3284)  loss_scale: 65536.0000 (69033.2807)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1335  max mem: 15572
Epoch: [30]  [ 440/2809]  eta: 0:23:21  lr: 0.000009  min_lr: 0.000000  loss: 3.6653 (3.7744)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (68953.9773)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1647  max mem: 15572
Epoch: [30]  [ 450/2809]  eta: 0:23:15  lr: 0.000009  min_lr: 0.000000  loss: 3.9039 (3.7752)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (68878.1907)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1503  max mem: 15572
Epoch: [30]  [ 460/2809]  eta: 0:23:13  lr: 0.000009  min_lr: 0.000000  loss: 3.9039 (3.7716)  class_acc: 0.2917 (0.3275)  loss_scale: 65536.0000 (68805.6920)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1900  max mem: 15572
[2025-01-16 04:41:41,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:41:41,025] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:41:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84740
[2025-01-16 04:41:41,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:41:41,430] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 470/2809]  eta: 0:23:00  lr: 0.000009  min_lr: 0.000000  loss: 3.6041 (3.7692)  class_acc: 0.2917 (0.3286)  loss_scale: 65536.0000 (68875.4140)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1229  max mem: 15572
Epoch: [30]  [ 480/2809]  eta: 0:22:56  lr: 0.000009  min_lr: 0.000000  loss: 3.6738 (3.7649)  class_acc: 0.4167 (0.3299)  loss_scale: 65536.0000 (68805.9875)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.1012  max mem: 15572
Epoch: [30]  [ 490/2809]  eta: 0:22:47  lr: 0.000009  min_lr: 0.000000  loss: 3.6738 (3.7660)  class_acc: 0.2917 (0.3287)  loss_scale: 65536.0000 (68739.3890)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1382  max mem: 15572
Epoch: [30]  [ 500/2809]  eta: 0:22:39  lr: 0.000009  min_lr: 0.000000  loss: 3.6274 (3.7624)  class_acc: 0.2500 (0.3298)  loss_scale: 65536.0000 (68675.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5365  data: 0.0845  max mem: 15572
Epoch: [30]  [ 510/2809]  eta: 0:22:31  lr: 0.000009  min_lr: 0.000000  loss: 3.5810 (3.7582)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (68614.0117)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0809  max mem: 15572
Epoch: [30]  [ 520/2809]  eta: 0:22:21  lr: 0.000009  min_lr: 0.000000  loss: 3.6095 (3.7583)  class_acc: 0.3333 (0.3305)  loss_scale: 65536.0000 (68554.9328)  weight_decay: 0.0500 (0.0500)  time: 0.5152  data: 0.0675  max mem: 15572
Epoch: [30]  [ 530/2809]  eta: 0:22:17  lr: 0.000009  min_lr: 0.000000  loss: 3.8652 (3.7593)  class_acc: 0.2917 (0.3301)  loss_scale: 65536.0000 (68498.0791)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1178  max mem: 15572
Epoch: [30]  [ 540/2809]  eta: 0:22:08  lr: 0.000009  min_lr: 0.000000  loss: 3.8785 (3.7610)  class_acc: 0.2917 (0.3299)  loss_scale: 65536.0000 (68443.3272)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1130  max mem: 15572
Epoch: [30]  [ 550/2809]  eta: 0:22:02  lr: 0.000009  min_lr: 0.000000  loss: 3.8785 (3.7587)  class_acc: 0.2917 (0.3302)  loss_scale: 65536.0000 (68390.5626)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0865  max mem: 15572
Epoch: [30]  [ 560/2809]  eta: 0:21:55  lr: 0.000009  min_lr: 0.000000  loss: 3.8177 (3.7581)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (68339.6791)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1198  max mem: 15572
Epoch: [30]  [ 570/2809]  eta: 0:21:49  lr: 0.000009  min_lr: 0.000000  loss: 3.8177 (3.7592)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (68290.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1291  max mem: 15572
Epoch: [30]  [ 580/2809]  eta: 0:21:43  lr: 0.000009  min_lr: 0.000000  loss: 3.7713 (3.7576)  class_acc: 0.2917 (0.3318)  loss_scale: 65536.0000 (68243.1670)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1226  max mem: 15572
Epoch: [30]  [ 590/2809]  eta: 0:21:35  lr: 0.000009  min_lr: 0.000000  loss: 3.6961 (3.7587)  class_acc: 0.3333 (0.3319)  loss_scale: 65536.0000 (68197.3604)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0707  max mem: 15572
[2025-01-16 04:42:53,508] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:42:53,509] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:42:53,944] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 84870
[2025-01-16 04:42:53,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:42:53,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 600/2809]  eta: 0:21:28  lr: 0.000009  min_lr: 0.000000  loss: 3.7205 (3.7574)  class_acc: 0.2917 (0.3320)  loss_scale: 65536.0000 (68262.1231)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0828  max mem: 15572
Epoch: [30]  [ 610/2809]  eta: 0:21:22  lr: 0.000009  min_lr: 0.000000  loss: 3.8012 (3.7598)  class_acc: 0.2917 (0.3316)  loss_scale: 65536.0000 (68217.5057)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1383  max mem: 15572
Epoch: [30]  [ 620/2809]  eta: 0:21:13  lr: 0.000009  min_lr: 0.000000  loss: 3.6901 (3.7571)  class_acc: 0.3333 (0.3320)  loss_scale: 65536.0000 (68174.3253)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1011  max mem: 15572
Epoch: [30]  [ 630/2809]  eta: 0:21:06  lr: 0.000008  min_lr: 0.000000  loss: 3.7568 (3.7594)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (68132.5135)  weight_decay: 0.0500 (0.0500)  time: 0.5115  data: 0.0757  max mem: 15572
Epoch: [30]  [ 640/2809]  eta: 0:20:59  lr: 0.000008  min_lr: 0.000000  loss: 3.8747 (3.7610)  class_acc: 0.2917 (0.3316)  loss_scale: 65536.0000 (68092.0062)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1066  max mem: 15572
Epoch: [30]  [ 650/2809]  eta: 0:20:51  lr: 0.000008  min_lr: 0.000000  loss: 3.9478 (3.7621)  class_acc: 0.3333 (0.3317)  loss_scale: 65536.0000 (68052.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.0857  max mem: 15572
Epoch: [30]  [ 660/2809]  eta: 0:20:45  lr: 0.000008  min_lr: 0.000000  loss: 3.7679 (3.7616)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (68014.6687)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0991  max mem: 15572
Epoch: [30]  [ 670/2809]  eta: 0:20:37  lr: 0.000008  min_lr: 0.000000  loss: 3.8567 (3.7622)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (67977.7288)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.1111  max mem: 15572
Epoch: [30]  [ 680/2809]  eta: 0:20:32  lr: 0.000008  min_lr: 0.000000  loss: 3.8549 (3.7628)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (67941.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1334  max mem: 15572
Epoch: [30]  [ 690/2809]  eta: 0:20:23  lr: 0.000008  min_lr: 0.000000  loss: 3.7873 (3.7639)  class_acc: 0.2500 (0.3312)  loss_scale: 65536.0000 (67907.0564)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1136  max mem: 15572
Epoch: [30]  [ 700/2809]  eta: 0:20:18  lr: 0.000008  min_lr: 0.000000  loss: 3.9457 (3.7655)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (67873.2325)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1024  max mem: 15572
Epoch: [30]  [ 710/2809]  eta: 0:20:13  lr: 0.000008  min_lr: 0.000000  loss: 3.8946 (3.7659)  class_acc: 0.3333 (0.3317)  loss_scale: 65536.0000 (67840.3601)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1623  max mem: 15572
Epoch: [30]  [ 720/2809]  eta: 0:20:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7342 (3.7636)  class_acc: 0.3333 (0.3321)  loss_scale: 65536.0000 (67808.3994)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1630  max mem: 15572
[2025-01-16 04:44:05,217] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:44:05,217] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:44:05,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=85000, skipped=566, lr=[8.177507834925266e-08, 8.177507834925266e-08, 1.1682154049893238e-07, 1.1682154049893238e-07, 1.6688791499847485e-07, 1.6688791499847485e-07, 2.3841130714067836e-07, 2.3841130714067836e-07, 3.4058758162954053e-07, 3.4058758162954053e-07, 4.865536880422007e-07, 4.865536880422007e-07, 6.95076697203144e-07, 6.95076697203144e-07, 9.929667102902058e-07, 9.929667102902058e-07, 1.4185238718431511e-06, 1.4185238718431511e-06, 2.026462674061645e-06, 2.026462674061645e-06, 2.894946677230921e-06, 2.894946677230921e-06, 4.135638110329887e-06, 4.135638110329887e-06, 5.908054443328411e-06, 5.908054443328411e-06, 8.440077776183445e-06, 8.440077776183445e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:44:05,228] [INFO] [timer.py:260:stop] epoch=0/micro_step=85000/global_step=85000, RunningAvgSamplesPerSec=28.569124280476686, CurrSamplesPerSec=32.29037158694313, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 04:44:05,644] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85000
[2025-01-16 04:44:05,645] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:44:05,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 730/2809]  eta: 0:20:00  lr: 0.000008  min_lr: 0.000000  loss: 3.6202 (3.7629)  class_acc: 0.3333 (0.3320)  loss_scale: 65536.0000 (67866.9658)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.1221  max mem: 15572
Epoch: [30]  [ 740/2809]  eta: 0:19:58  lr: 0.000008  min_lr: 0.000000  loss: 3.6202 (3.7615)  class_acc: 0.3333 (0.3322)  loss_scale: 65536.0000 (67835.5088)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1767  max mem: 15572
Epoch: [30]  [ 750/2809]  eta: 0:19:52  lr: 0.000008  min_lr: 0.000000  loss: 3.7971 (3.7622)  class_acc: 0.2917 (0.3319)  loss_scale: 65536.0000 (67804.8895)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.1994  max mem: 15572
Epoch: [30]  [ 760/2809]  eta: 0:19:46  lr: 0.000008  min_lr: 0.000000  loss: 3.8091 (3.7634)  class_acc: 0.2500 (0.3319)  loss_scale: 65536.0000 (67775.0749)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1417  max mem: 15572
Epoch: [30]  [ 770/2809]  eta: 0:19:42  lr: 0.000008  min_lr: 0.000000  loss: 3.7554 (3.7632)  class_acc: 0.2917 (0.3318)  loss_scale: 65536.0000 (67746.0337)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.1877  max mem: 15572
Epoch: [30]  [ 780/2809]  eta: 0:19:35  lr: 0.000008  min_lr: 0.000000  loss: 3.7202 (3.7627)  class_acc: 0.3333 (0.3317)  loss_scale: 65536.0000 (67717.7362)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.1694  max mem: 15572
Epoch: [30]  [ 790/2809]  eta: 0:19:27  lr: 0.000008  min_lr: 0.000000  loss: 3.6540 (3.7597)  class_acc: 0.3750 (0.3324)  loss_scale: 65536.0000 (67690.1542)  weight_decay: 0.0500 (0.0500)  time: 0.5011  data: 0.0745  max mem: 15572
Epoch: [30]  [ 800/2809]  eta: 0:19:21  lr: 0.000008  min_lr: 0.000000  loss: 3.5839 (3.7582)  class_acc: 0.4167 (0.3333)  loss_scale: 65536.0000 (67663.2609)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0817  max mem: 15572
Epoch: [30]  [ 810/2809]  eta: 0:19:14  lr: 0.000008  min_lr: 0.000000  loss: 3.7190 (3.7568)  class_acc: 0.4167 (0.3340)  loss_scale: 65536.0000 (67637.0308)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1143  max mem: 15572
Epoch: [30]  [ 820/2809]  eta: 0:19:08  lr: 0.000008  min_lr: 0.000000  loss: 3.6233 (3.7544)  class_acc: 0.3750 (0.3340)  loss_scale: 65536.0000 (67611.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1081  max mem: 15572
Epoch: [30]  [ 830/2809]  eta: 0:19:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7215 (3.7591)  class_acc: 0.2917 (0.3329)  loss_scale: 65536.0000 (67586.4645)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.0899  max mem: 15572
Epoch: [30]  [ 840/2809]  eta: 0:18:54  lr: 0.000008  min_lr: 0.000000  loss: 4.0115 (3.7595)  class_acc: 0.2917 (0.3330)  loss_scale: 65536.0000 (67562.0832)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0638  max mem: 15572
Epoch: [30]  [ 850/2809]  eta: 0:18:48  lr: 0.000008  min_lr: 0.000000  loss: 3.9193 (3.7577)  class_acc: 0.3333 (0.3337)  loss_scale: 65536.0000 (67538.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.1007  max mem: 15572
[2025-01-16 04:45:19,399] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:45:19,399] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 860/2809]  eta: 0:18:43  lr: 0.000008  min_lr: 0.000000  loss: 3.4848 (3.7548)  class_acc: 0.3750 (0.3343)  loss_scale: 65536.0000 (67667.2520)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1487  max mem: 15572
[2025-01-16 04:45:23,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85136
[2025-01-16 04:45:23,137] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:45:23,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 870/2809]  eta: 0:18:36  lr: 0.000008  min_lr: 0.000000  loss: 3.6321 (3.7570)  class_acc: 0.3333 (0.3341)  loss_scale: 65536.0000 (68018.9943)  weight_decay: 0.0500 (0.0500)  time: 0.5566  data: 0.1198  max mem: 15572
Epoch: [30]  [ 880/2809]  eta: 0:18:30  lr: 0.000008  min_lr: 0.000000  loss: 3.9769 (3.7573)  class_acc: 0.2917 (0.3342)  loss_scale: 65536.0000 (67990.8104)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1142  max mem: 15572
Epoch: [30]  [ 890/2809]  eta: 0:18:24  lr: 0.000008  min_lr: 0.000000  loss: 3.7385 (3.7557)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1290  max mem: 15572
Epoch: [30]  [ 900/2809]  eta: 0:18:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7962 (3.7575)  class_acc: 0.3333 (0.3343)  loss_scale: 65536.0000 (67936.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1046  max mem: 15572
Epoch: [30]  [ 910/2809]  eta: 0:18:11  lr: 0.000008  min_lr: 0.000000  loss: 3.8780 (3.7575)  class_acc: 0.2917 (0.3342)  loss_scale: 65536.0000 (67909.9715)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0924  max mem: 15572
Epoch: [30]  [ 920/2809]  eta: 0:18:05  lr: 0.000008  min_lr: 0.000000  loss: 3.6934 (3.7573)  class_acc: 0.3750 (0.3346)  loss_scale: 65536.0000 (67884.1954)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1118  max mem: 15572
Epoch: [30]  [ 930/2809]  eta: 0:18:00  lr: 0.000008  min_lr: 0.000000  loss: 3.6054 (3.7561)  class_acc: 0.3750 (0.3348)  loss_scale: 65536.0000 (67858.9731)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1455  max mem: 15572
Epoch: [30]  [ 940/2809]  eta: 0:17:55  lr: 0.000008  min_lr: 0.000000  loss: 3.6054 (3.7544)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (67834.2869)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1515  max mem: 15572
Epoch: [30]  [ 950/2809]  eta: 0:17:49  lr: 0.000008  min_lr: 0.000000  loss: 3.6818 (3.7556)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (67810.1199)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1392  max mem: 15572
Epoch: [30]  [ 960/2809]  eta: 0:17:42  lr: 0.000008  min_lr: 0.000000  loss: 3.9425 (3.7568)  class_acc: 0.2917 (0.3340)  loss_scale: 65536.0000 (67786.4558)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0948  max mem: 15572
Epoch: [30]  [ 970/2809]  eta: 0:17:35  lr: 0.000008  min_lr: 0.000000  loss: 3.7365 (3.7574)  class_acc: 0.3333 (0.3341)  loss_scale: 65536.0000 (67763.2791)  weight_decay: 0.0500 (0.0500)  time: 0.4976  data: 0.0667  max mem: 15572
Epoch: [30]  [ 980/2809]  eta: 0:17:32  lr: 0.000008  min_lr: 0.000000  loss: 3.6082 (3.7555)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (67740.5749)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1865  max mem: 15572
Epoch: [30]  [ 990/2809]  eta: 0:17:27  lr: 0.000008  min_lr: 0.000000  loss: 3.7273 (3.7546)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (67718.3290)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.2364  max mem: 15572
[2025-01-16 04:46:37,202] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:46:37,202] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:46:39,090] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85269
[2025-01-16 04:46:39,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:46:39,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1000/2809]  eta: 0:17:20  lr: 0.000008  min_lr: 0.000000  loss: 3.8034 (3.7542)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (67958.4096)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1453  max mem: 15572
Epoch: [30]  [1010/2809]  eta: 0:17:15  lr: 0.000008  min_lr: 0.000000  loss: 3.7594 (3.7528)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (67934.4491)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1392  max mem: 15572
Epoch: [30]  [1020/2809]  eta: 0:17:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7594 (3.7534)  class_acc: 0.2917 (0.3348)  loss_scale: 65536.0000 (67910.9579)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1442  max mem: 15572
Epoch: [30]  [1030/2809]  eta: 0:17:02  lr: 0.000008  min_lr: 0.000000  loss: 3.7304 (3.7528)  class_acc: 0.3750 (0.3354)  loss_scale: 65536.0000 (67887.9224)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1078  max mem: 15572
Epoch: [30]  [1040/2809]  eta: 0:16:57  lr: 0.000008  min_lr: 0.000000  loss: 3.5832 (3.7512)  class_acc: 0.3750 (0.3359)  loss_scale: 65536.0000 (67865.3295)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1300  max mem: 15572
Epoch: [30]  [1050/2809]  eta: 0:16:51  lr: 0.000008  min_lr: 0.000000  loss: 3.5832 (3.7519)  class_acc: 0.3333 (0.3362)  loss_scale: 65536.0000 (67843.1665)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1404  max mem: 15572
Epoch: [30]  [1060/2809]  eta: 0:16:45  lr: 0.000008  min_lr: 0.000000  loss: 3.6978 (3.7521)  class_acc: 0.3750 (0.3369)  loss_scale: 65536.0000 (67821.4213)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1193  max mem: 15572
Epoch: [30]  [1070/2809]  eta: 0:16:38  lr: 0.000008  min_lr: 0.000000  loss: 3.7171 (3.7532)  class_acc: 0.2917 (0.3364)  loss_scale: 65536.0000 (67800.0822)  weight_decay: 0.0500 (0.0500)  time: 0.5322  data: 0.0971  max mem: 15572
Epoch: [30]  [1080/2809]  eta: 0:16:33  lr: 0.000008  min_lr: 0.000000  loss: 3.7171 (3.7506)  class_acc: 0.3333 (0.3371)  loss_scale: 65536.0000 (67779.1378)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1288  max mem: 15572
Epoch: [30]  [1090/2809]  eta: 0:16:27  lr: 0.000008  min_lr: 0.000000  loss: 3.7540 (3.7528)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (67758.5775)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1674  max mem: 15572
Epoch: [30]  [1100/2809]  eta: 0:16:21  lr: 0.000008  min_lr: 0.000000  loss: 3.8436 (3.7518)  class_acc: 0.2500 (0.3366)  loss_scale: 65536.0000 (67738.3906)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1056  max mem: 15572
Epoch: [30]  [1110/2809]  eta: 0:16:16  lr: 0.000008  min_lr: 0.000000  loss: 3.7177 (3.7525)  class_acc: 0.2500 (0.3360)  loss_scale: 65536.0000 (67718.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1072  max mem: 15572
Epoch: [30]  [1120/2809]  eta: 0:16:10  lr: 0.000008  min_lr: 0.000000  loss: 3.8707 (3.7537)  class_acc: 0.2083 (0.3352)  loss_scale: 65536.0000 (67699.0972)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1260  max mem: 15572
[2025-01-16 04:47:51,221] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:47:51,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:47:52,440] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85400
[2025-01-16 04:47:52,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:47:52,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1130/2809]  eta: 0:16:03  lr: 0.000008  min_lr: 0.000000  loss: 3.6253 (3.7515)  class_acc: 0.2500 (0.3357)  loss_scale: 65536.0000 (67795.8621)  weight_decay: 0.0500 (0.0500)  time: 0.5096  data: 0.0691  max mem: 15572
Epoch: [30]  [1140/2809]  eta: 0:15:56  lr: 0.000008  min_lr: 0.000000  loss: 3.6253 (3.7525)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (67776.0561)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0613  max mem: 15572
Epoch: [30]  [1150/2809]  eta: 0:15:52  lr: 0.000008  min_lr: 0.000000  loss: 3.5972 (3.7505)  class_acc: 0.3750 (0.3356)  loss_scale: 65536.0000 (67756.5943)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.1629  max mem: 15572
Epoch: [30]  [1160/2809]  eta: 0:15:45  lr: 0.000008  min_lr: 0.000000  loss: 3.8263 (3.7509)  class_acc: 0.3750 (0.3350)  loss_scale: 65536.0000 (67737.4677)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1373  max mem: 15572
Epoch: [30]  [1170/2809]  eta: 0:15:39  lr: 0.000008  min_lr: 0.000000  loss: 3.9085 (3.7535)  class_acc: 0.2083 (0.3345)  loss_scale: 65536.0000 (67718.6678)  weight_decay: 0.0500 (0.0500)  time: 0.5258  data: 0.0879  max mem: 15572
Epoch: [30]  [1180/2809]  eta: 0:15:32  lr: 0.000008  min_lr: 0.000000  loss: 3.8955 (3.7537)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (67700.1863)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1118  max mem: 15572
Epoch: [30]  [1190/2809]  eta: 0:15:28  lr: 0.000008  min_lr: 0.000000  loss: 3.6932 (3.7547)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (67682.0151)  weight_decay: 0.0500 (0.0500)  time: 0.6058  data: 0.1717  max mem: 15572
Epoch: [30]  [1200/2809]  eta: 0:15:22  lr: 0.000008  min_lr: 0.000000  loss: 3.9568 (3.7557)  class_acc: 0.2917 (0.3342)  loss_scale: 65536.0000 (67664.1465)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.1915  max mem: 15572
Epoch: [30]  [1210/2809]  eta: 0:15:16  lr: 0.000008  min_lr: 0.000000  loss: 3.6388 (3.7524)  class_acc: 0.3750 (0.3353)  loss_scale: 65536.0000 (67646.5731)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.1037  max mem: 15572
Epoch: [30]  [1220/2809]  eta: 0:15:10  lr: 0.000008  min_lr: 0.000000  loss: 3.6457 (3.7545)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (67629.2875)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1241  max mem: 15572
Epoch: [30]  [1230/2809]  eta: 0:15:06  lr: 0.000008  min_lr: 0.000000  loss: 3.9324 (3.7533)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (67612.2827)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.1962  max mem: 15572
Epoch: [30]  [1240/2809]  eta: 0:15:00  lr: 0.000008  min_lr: 0.000000  loss: 3.7029 (3.7520)  class_acc: 0.3750 (0.3353)  loss_scale: 65536.0000 (67595.5520)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1840  max mem: 15572
Epoch: [30]  [1250/2809]  eta: 0:14:54  lr: 0.000008  min_lr: 0.000000  loss: 3.8050 (3.7521)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (67579.0887)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1105  max mem: 15572
[2025-01-16 04:49:05,319] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:49:05,320] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [1260/2809]  eta: 0:14:47  lr: 0.000008  min_lr: 0.000000  loss: 3.7522 (3.7512)  class_acc: 0.3333 (0.3357)  loss_scale: 65536.0000 (67666.8295)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0691  max mem: 15572
[2025-01-16 04:49:07,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85533
[2025-01-16 04:49:07,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:49:07,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1270/2809]  eta: 0:14:43  lr: 0.000008  min_lr: 0.000000  loss: 3.7119 (3.7512)  class_acc: 0.3333 (0.3351)  loss_scale: 65536.0000 (67753.1896)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1511  max mem: 15572
Epoch: [30]  [1280/2809]  eta: 0:14:36  lr: 0.000008  min_lr: 0.000000  loss: 3.7119 (3.7519)  class_acc: 0.2500 (0.3346)  loss_scale: 65536.0000 (67735.8813)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1639  max mem: 15572
Epoch: [30]  [1290/2809]  eta: 0:14:31  lr: 0.000008  min_lr: 0.000000  loss: 4.0726 (3.7559)  class_acc: 0.2083 (0.3338)  loss_scale: 65536.0000 (67718.8412)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1347  max mem: 15572
Epoch: [30]  [1300/2809]  eta: 0:14:25  lr: 0.000008  min_lr: 0.000000  loss: 3.9700 (3.7564)  class_acc: 0.2500 (0.3338)  loss_scale: 65536.0000 (67702.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1104  max mem: 15572
Epoch: [30]  [1310/2809]  eta: 0:14:20  lr: 0.000008  min_lr: 0.000000  loss: 3.8674 (3.7569)  class_acc: 0.2917 (0.3336)  loss_scale: 65536.0000 (67685.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1510  max mem: 15572
Epoch: [30]  [1320/2809]  eta: 0:14:14  lr: 0.000008  min_lr: 0.000000  loss: 3.8104 (3.7564)  class_acc: 0.3333 (0.3341)  loss_scale: 65536.0000 (67669.2687)  weight_decay: 0.0500 (0.0500)  time: 0.6207  data: 0.1756  max mem: 15572
Epoch: [30]  [1330/2809]  eta: 0:14:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7805 (3.7569)  class_acc: 0.4167 (0.3340)  loss_scale: 65536.0000 (67653.2412)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.0918  max mem: 15572
[2025-01-16 04:49:48,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85603
[2025-01-16 04:49:48,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:49:48,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1340/2809]  eta: 0:14:04  lr: 0.000008  min_lr: 0.000000  loss: 3.7477 (3.7556)  class_acc: 0.3333 (0.3342)  loss_scale: 65536.0000 (67441.9687)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1716  max mem: 15572
Epoch: [30]  [1350/2809]  eta: 0:13:57  lr: 0.000008  min_lr: 0.000000  loss: 3.7035 (3.7555)  class_acc: 0.2917 (0.3339)  loss_scale: 32768.0000 (67185.3146)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1716  max mem: 15572
Epoch: [30]  [1360/2809]  eta: 0:13:51  lr: 0.000008  min_lr: 0.000000  loss: 3.8759 (3.7553)  class_acc: 0.2917 (0.3341)  loss_scale: 32768.0000 (66932.4320)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0627  max mem: 15572
Epoch: [30]  [1370/2809]  eta: 0:13:44  lr: 0.000008  min_lr: 0.000000  loss: 3.6626 (3.7548)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (66683.2385)  weight_decay: 0.0500 (0.0500)  time: 0.4978  data: 0.0456  max mem: 15572
Epoch: [30]  [1380/2809]  eta: 0:13:38  lr: 0.000008  min_lr: 0.000000  loss: 3.7064 (3.7553)  class_acc: 0.3333 (0.3341)  loss_scale: 32768.0000 (66437.6539)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0437  max mem: 15572
Epoch: [30]  [1390/2809]  eta: 0:13:32  lr: 0.000008  min_lr: 0.000000  loss: 4.0499 (3.7576)  class_acc: 0.2083 (0.3331)  loss_scale: 32768.0000 (66195.6003)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0793  max mem: 15572
Epoch: [30]  [1400/2809]  eta: 0:13:26  lr: 0.000008  min_lr: 0.000000  loss: 3.9617 (3.7563)  class_acc: 0.2500 (0.3334)  loss_scale: 32768.0000 (65957.0021)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1251  max mem: 15572
Epoch: [30]  [1410/2809]  eta: 0:13:19  lr: 0.000008  min_lr: 0.000000  loss: 3.5088 (3.7560)  class_acc: 0.3333 (0.3335)  loss_scale: 32768.0000 (65721.7860)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0782  max mem: 15572
Epoch: [30]  [1420/2809]  eta: 0:13:13  lr: 0.000008  min_lr: 0.000000  loss: 3.5088 (3.7544)  class_acc: 0.3333 (0.3340)  loss_scale: 32768.0000 (65489.8804)  weight_decay: 0.0500 (0.0500)  time: 0.5228  data: 0.0638  max mem: 15572
Epoch: [30]  [1430/2809]  eta: 0:13:08  lr: 0.000008  min_lr: 0.000000  loss: 3.7106 (3.7551)  class_acc: 0.3750 (0.3337)  loss_scale: 32768.0000 (65261.2159)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1035  max mem: 15572
Epoch: [30]  [1440/2809]  eta: 0:13:02  lr: 0.000008  min_lr: 0.000000  loss: 3.7190 (3.7545)  class_acc: 0.2917 (0.3335)  loss_scale: 32768.0000 (65035.7252)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1303  max mem: 15572
Epoch: [30]  [1450/2809]  eta: 0:12:56  lr: 0.000008  min_lr: 0.000000  loss: 3.6735 (3.7554)  class_acc: 0.2917 (0.3332)  loss_scale: 32768.0000 (64813.3425)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1501  max mem: 15572
Epoch: [30]  [1460/2809]  eta: 0:12:51  lr: 0.000008  min_lr: 0.000000  loss: 3.8226 (3.7571)  class_acc: 0.2917 (0.3328)  loss_scale: 32768.0000 (64594.0041)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1457  max mem: 15572
[2025-01-16 04:50:59,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:50:59,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1470/2809]  eta: 0:12:45  lr: 0.000008  min_lr: 0.000000  loss: 3.8277 (3.7567)  class_acc: 0.3333 (0.3330)  loss_scale: 32768.0000 (64578.1319)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1273  max mem: 15572
Epoch: [30]  [1480/2809]  eta: 0:12:39  lr: 0.000008  min_lr: 0.000000  loss: 3.8277 (3.7569)  class_acc: 0.3333 (0.3331)  loss_scale: 65536.0000 (64584.5996)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1345  max mem: 15572
Epoch: [30]  [1490/2809]  eta: 0:12:34  lr: 0.000008  min_lr: 0.000000  loss: 3.8458 (3.7574)  class_acc: 0.3333 (0.3335)  loss_scale: 65536.0000 (64590.9805)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1247  max mem: 15572
Epoch: [30]  [1500/2809]  eta: 0:12:28  lr: 0.000008  min_lr: 0.000000  loss: 3.9093 (3.7583)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (64597.2765)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1043  max mem: 15572
Epoch: [30]  [1510/2809]  eta: 0:12:22  lr: 0.000008  min_lr: 0.000000  loss: 3.7661 (3.7580)  class_acc: 0.3333 (0.3335)  loss_scale: 65536.0000 (64603.4891)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1081  max mem: 15572
Epoch: [30]  [1520/2809]  eta: 0:12:15  lr: 0.000008  min_lr: 0.000000  loss: 3.5574 (3.7569)  class_acc: 0.3750 (0.3339)  loss_scale: 65536.0000 (64609.6200)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0771  max mem: 15572
Epoch: [30]  [1530/2809]  eta: 0:12:09  lr: 0.000008  min_lr: 0.000000  loss: 3.5080 (3.7566)  class_acc: 0.3750 (0.3339)  loss_scale: 65536.0000 (64615.6708)  weight_decay: 0.0500 (0.0500)  time: 0.5076  data: 0.0803  max mem: 15572
[2025-01-16 04:51:38,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 85802
[2025-01-16 04:51:38,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:51:38,408] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1540/2809]  eta: 0:12:04  lr: 0.000008  min_lr: 0.000000  loss: 3.7121 (3.7567)  class_acc: 0.3333 (0.3340)  loss_scale: 65536.0000 (64430.2661)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1273  max mem: 15572
Epoch: [30]  [1550/2809]  eta: 0:11:58  lr: 0.000008  min_lr: 0.000000  loss: 3.8389 (3.7577)  class_acc: 0.2917 (0.3338)  loss_scale: 32768.0000 (64226.1251)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1078  max mem: 15572
Epoch: [30]  [1560/2809]  eta: 0:11:52  lr: 0.000008  min_lr: 0.000000  loss: 3.8753 (3.7579)  class_acc: 0.2917 (0.3337)  loss_scale: 32768.0000 (64024.5996)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1361  max mem: 15572
Epoch: [30]  [1570/2809]  eta: 0:11:47  lr: 0.000008  min_lr: 0.000000  loss: 3.8753 (3.7584)  class_acc: 0.2917 (0.3335)  loss_scale: 32768.0000 (63825.6397)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1515  max mem: 15572
Epoch: [30]  [1580/2809]  eta: 0:11:41  lr: 0.000008  min_lr: 0.000000  loss: 3.9408 (3.7600)  class_acc: 0.2500 (0.3331)  loss_scale: 32768.0000 (63629.1967)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1048  max mem: 15572
Epoch: [30]  [1590/2809]  eta: 0:11:36  lr: 0.000008  min_lr: 0.000000  loss: 4.0677 (3.7613)  class_acc: 0.2083 (0.3330)  loss_scale: 32768.0000 (63435.2231)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1114  max mem: 15572
Epoch: [30]  [1600/2809]  eta: 0:11:30  lr: 0.000008  min_lr: 0.000000  loss: 3.8669 (3.7617)  class_acc: 0.2500 (0.3330)  loss_scale: 32768.0000 (63243.6727)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1194  max mem: 15572
Epoch: [30]  [1610/2809]  eta: 0:11:25  lr: 0.000008  min_lr: 0.000000  loss: 3.8001 (3.7615)  class_acc: 0.3333 (0.3330)  loss_scale: 32768.0000 (63054.5003)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1395  max mem: 15572
Epoch: [30]  [1620/2809]  eta: 0:11:18  lr: 0.000008  min_lr: 0.000000  loss: 3.7560 (3.7611)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (62867.6619)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1093  max mem: 15572
Epoch: [30]  [1630/2809]  eta: 0:11:13  lr: 0.000008  min_lr: 0.000000  loss: 3.6174 (3.7598)  class_acc: 0.3750 (0.3335)  loss_scale: 32768.0000 (62683.1147)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1118  max mem: 15572
Epoch: [30]  [1640/2809]  eta: 0:11:07  lr: 0.000008  min_lr: 0.000000  loss: 3.6903 (3.7600)  class_acc: 0.3333 (0.3334)  loss_scale: 32768.0000 (62500.8166)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.0834  max mem: 15572
Epoch: [30]  [1650/2809]  eta: 0:11:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7565 (3.7598)  class_acc: 0.2917 (0.3335)  loss_scale: 32768.0000 (62320.7268)  weight_decay: 0.0500 (0.0500)  time: 0.4798  data: 0.0283  max mem: 15572
Epoch: [30]  [1660/2809]  eta: 0:10:55  lr: 0.000008  min_lr: 0.000000  loss: 3.6533 (3.7590)  class_acc: 0.3333 (0.3338)  loss_scale: 32768.0000 (62142.8055)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1314  max mem: 15572
[2025-01-16 04:52:52,398] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:52:52,399] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1670/2809]  eta: 0:10:50  lr: 0.000008  min_lr: 0.000000  loss: 3.6370 (3.7582)  class_acc: 0.3333 (0.3341)  loss_scale: 32768.0000 (62163.1119)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.2098  max mem: 15572
Epoch: [30]  [1680/2809]  eta: 0:10:44  lr: 0.000008  min_lr: 0.000000  loss: 3.7223 (3.7575)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (62183.1767)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1729  max mem: 15572
Epoch: [30]  [1690/2809]  eta: 0:10:39  lr: 0.000008  min_lr: 0.000000  loss: 3.8818 (3.7582)  class_acc: 0.2917 (0.3342)  loss_scale: 65536.0000 (62203.0041)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1327  max mem: 15572
Epoch: [30]  [1700/2809]  eta: 0:10:33  lr: 0.000008  min_lr: 0.000000  loss: 3.7325 (3.7573)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (62222.5985)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1042  max mem: 15572
Epoch: [30]  [1710/2809]  eta: 0:10:27  lr: 0.000008  min_lr: 0.000000  loss: 3.8081 (3.7582)  class_acc: 0.3750 (0.3344)  loss_scale: 65536.0000 (62241.9638)  weight_decay: 0.0500 (0.0500)  time: 0.5187  data: 0.0823  max mem: 15572
Epoch: [30]  [1720/2809]  eta: 0:10:22  lr: 0.000008  min_lr: 0.000000  loss: 3.9112 (3.7587)  class_acc: 0.2500 (0.3341)  loss_scale: 65536.0000 (62261.1040)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1418  max mem: 15572
[2025-01-16 04:53:30,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=86000, skipped=573, lr=[7.628959446008848e-08, 7.628959446008848e-08, 1.0898513494298357e-07, 1.0898513494298357e-07, 1.5569304991854797e-07, 1.5569304991854797e-07, 2.2241864274078283e-07, 2.2241864274078283e-07, 3.177409182011183e-07, 3.177409182011183e-07, 4.5391559743016906e-07, 4.5391559743016906e-07, 6.484508534716701e-07, 6.484508534716701e-07, 9.26358362102386e-07, 9.26358362102386e-07, 1.3233690887176942e-06, 1.3233690887176942e-06, 1.8905272695967064e-06, 1.8905272695967064e-06, 2.700753242281009e-06, 2.700753242281009e-06, 3.858218917544299e-06, 3.858218917544299e-06, 5.5117413107775706e-06, 5.5117413107775706e-06, 7.873916158253672e-06, 7.873916158253672e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 04:53:30,755] [INFO] [timer.py:260:stop] epoch=0/micro_step=86000/global_step=86000, RunningAvgSamplesPerSec=28.571473448048998, CurrSamplesPerSec=30.39022231292632, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [1730/2809]  eta: 0:10:16  lr: 0.000008  min_lr: 0.000000  loss: 3.7911 (3.7586)  class_acc: 0.3333 (0.3342)  loss_scale: 65536.0000 (62280.0231)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1676  max mem: 15572
Epoch: [30]  [1740/2809]  eta: 0:10:10  lr: 0.000008  min_lr: 0.000000  loss: 3.8116 (3.7595)  class_acc: 0.3333 (0.3341)  loss_scale: 65536.0000 (62298.7249)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0972  max mem: 15572
Epoch: [30]  [1750/2809]  eta: 0:10:03  lr: 0.000008  min_lr: 0.000000  loss: 3.8771 (3.7593)  class_acc: 0.3333 (0.3343)  loss_scale: 65536.0000 (62317.2130)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0550  max mem: 15572
Epoch: [30]  [1760/2809]  eta: 0:09:58  lr: 0.000008  min_lr: 0.000000  loss: 3.6908 (3.7581)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (62335.4912)  weight_decay: 0.0500 (0.0500)  time: 0.5244  data: 0.0831  max mem: 15572
Epoch: [30]  [1770/2809]  eta: 0:09:52  lr: 0.000008  min_lr: 0.000000  loss: 3.7098 (3.7576)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (62353.5630)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1064  max mem: 15572
Epoch: [30]  [1780/2809]  eta: 0:09:46  lr: 0.000008  min_lr: 0.000000  loss: 3.6413 (3.7565)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (62371.4318)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1160  max mem: 15572
[2025-01-16 04:54:04,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:54:04,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [1790/2809]  eta: 0:09:40  lr: 0.000008  min_lr: 0.000000  loss: 3.6622 (3.7576)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (62462.2848)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1287  max mem: 15572
[2025-01-16 04:54:05,789] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86061
[2025-01-16 04:54:05,789] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:54:05,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 04:54:08,949] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86067
[2025-01-16 04:54:08,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:54:08,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [1800/2809]  eta: 0:09:35  lr: 0.000008  min_lr: 0.000000  loss: 3.8655 (3.7575)  class_acc: 0.3750 (0.3347)  loss_scale: 65536.0000 (62406.5741)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1111  max mem: 15572
Epoch: [30]  [1810/2809]  eta: 0:09:29  lr: 0.000008  min_lr: 0.000000  loss: 3.8655 (3.7575)  class_acc: 0.2917 (0.3344)  loss_scale: 32768.0000 (62242.9155)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1037  max mem: 15572
Epoch: [30]  [1820/2809]  eta: 0:09:24  lr: 0.000008  min_lr: 0.000000  loss: 3.9450 (3.7580)  class_acc: 0.2917 (0.3343)  loss_scale: 32768.0000 (62081.0544)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.1629  max mem: 15572
Epoch: [30]  [1830/2809]  eta: 0:09:18  lr: 0.000008  min_lr: 0.000000  loss: 3.9565 (3.7574)  class_acc: 0.3333 (0.3347)  loss_scale: 32768.0000 (61920.9612)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.2087  max mem: 15572
Epoch: [30]  [1840/2809]  eta: 0:09:12  lr: 0.000008  min_lr: 0.000000  loss: 3.6688 (3.7566)  class_acc: 0.3750 (0.3350)  loss_scale: 32768.0000 (61762.6073)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1214  max mem: 15572
Epoch: [30]  [1850/2809]  eta: 0:09:06  lr: 0.000008  min_lr: 0.000000  loss: 3.6688 (3.7560)  class_acc: 0.3333 (0.3349)  loss_scale: 32768.0000 (61605.9643)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0501  max mem: 15572
Epoch: [30]  [1860/2809]  eta: 0:09:01  lr: 0.000008  min_lr: 0.000000  loss: 3.7014 (3.7564)  class_acc: 0.3333 (0.3352)  loss_scale: 32768.0000 (61451.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1008  max mem: 15572
Epoch: [30]  [1870/2809]  eta: 0:08:55  lr: 0.000008  min_lr: 0.000000  loss: 3.5231 (3.7552)  class_acc: 0.4167 (0.3353)  loss_scale: 32768.0000 (61297.7018)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0939  max mem: 15572
Epoch: [30]  [1880/2809]  eta: 0:08:49  lr: 0.000008  min_lr: 0.000000  loss: 3.4769 (3.7544)  class_acc: 0.3750 (0.3353)  loss_scale: 32768.0000 (61146.0287)  weight_decay: 0.0500 (0.0500)  time: 0.4897  data: 0.0476  max mem: 15572
Epoch: [30]  [1890/2809]  eta: 0:08:43  lr: 0.000008  min_lr: 0.000000  loss: 3.8095 (3.7550)  class_acc: 0.2917 (0.3352)  loss_scale: 32768.0000 (60995.9598)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0797  max mem: 15572
Epoch: [30]  [1900/2809]  eta: 0:08:37  lr: 0.000008  min_lr: 0.000000  loss: 3.8597 (3.7544)  class_acc: 0.2917 (0.3352)  loss_scale: 32768.0000 (60847.4698)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1319  max mem: 15572
Epoch: [30]  [1910/2809]  eta: 0:08:32  lr: 0.000008  min_lr: 0.000000  loss: 4.0049 (3.7563)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (60700.5338)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.2049  max mem: 15572
Epoch: [30]  [1920/2809]  eta: 0:08:26  lr: 0.000008  min_lr: 0.000000  loss: 3.9430 (3.7556)  class_acc: 0.2500 (0.3346)  loss_scale: 32768.0000 (60555.1275)  weight_decay: 0.0500 (0.0500)  time: 0.6342  data: 0.1864  max mem: 15572
[2025-01-16 04:55:22,361] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:55:22,361] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [1930/2809]  eta: 0:08:20  lr: 0.000008  min_lr: 0.000000  loss: 3.6748 (3.7554)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (60496.0746)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.0952  max mem: 15572
Epoch: [30]  [1940/2809]  eta: 0:08:15  lr: 0.000008  min_lr: 0.000000  loss: 3.7733 (3.7555)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (60522.0402)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0942  max mem: 15572
Epoch: [30]  [1950/2809]  eta: 0:08:09  lr: 0.000008  min_lr: 0.000000  loss: 3.7225 (3.7553)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (60547.7396)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1096  max mem: 15572
Epoch: [30]  [1960/2809]  eta: 0:08:03  lr: 0.000008  min_lr: 0.000000  loss: 3.6932 (3.7539)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (60573.1770)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0986  max mem: 15572
Epoch: [30]  [1970/2809]  eta: 0:07:58  lr: 0.000008  min_lr: 0.000000  loss: 3.4962 (3.7534)  class_acc: 0.3333 (0.3350)  loss_scale: 65536.0000 (60598.3562)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1149  max mem: 15572
Epoch: [30]  [1980/2809]  eta: 0:07:52  lr: 0.000008  min_lr: 0.000000  loss: 3.4846 (3.7526)  class_acc: 0.3333 (0.3351)  loss_scale: 65536.0000 (60623.2812)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1394  max mem: 15572
Epoch: [30]  [1990/2809]  eta: 0:07:46  lr: 0.000008  min_lr: 0.000000  loss: 3.8715 (3.7542)  class_acc: 0.2917 (0.3348)  loss_scale: 65536.0000 (60647.9558)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1057  max mem: 15572
Epoch: [30]  [2000/2809]  eta: 0:07:40  lr: 0.000008  min_lr: 0.000000  loss: 3.5541 (3.7524)  class_acc: 0.2917 (0.3352)  loss_scale: 65536.0000 (60672.3838)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1226  max mem: 15572
Epoch: [30]  [2010/2809]  eta: 0:07:34  lr: 0.000008  min_lr: 0.000000  loss: 3.4723 (3.7518)  class_acc: 0.4167 (0.3352)  loss_scale: 65536.0000 (60696.5689)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1130  max mem: 15572
Epoch: [30]  [2020/2809]  eta: 0:07:29  lr: 0.000008  min_lr: 0.000000  loss: 3.5528 (3.7508)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (60720.5146)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0875  max mem: 15572
Epoch: [30]  [2030/2809]  eta: 0:07:23  lr: 0.000008  min_lr: 0.000000  loss: 3.6575 (3.7517)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (60744.2245)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1121  max mem: 15572
Epoch: [30]  [2040/2809]  eta: 0:07:17  lr: 0.000008  min_lr: 0.000000  loss: 3.8079 (3.7515)  class_acc: 0.3333 (0.3350)  loss_scale: 65536.0000 (60767.7021)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0906  max mem: 15572
Epoch: [30]  [2050/2809]  eta: 0:07:11  lr: 0.000008  min_lr: 0.000000  loss: 3.6952 (3.7519)  class_acc: 0.3333 (0.3350)  loss_scale: 65536.0000 (60790.9508)  weight_decay: 0.0500 (0.0500)  time: 0.5243  data: 0.0768  max mem: 15572
[2025-01-16 04:56:33,063] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:56:33,063] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:56:33,913] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86326
[2025-01-16 04:56:33,913] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:56:33,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2060/2809]  eta: 0:07:05  lr: 0.000008  min_lr: 0.000000  loss: 3.6952 (3.7519)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (60877.5701)  weight_decay: 0.0500 (0.0500)  time: 0.5142  data: 0.0754  max mem: 15572
Epoch: [30]  [2070/2809]  eta: 0:07:00  lr: 0.000008  min_lr: 0.000000  loss: 3.7881 (3.7519)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (60900.0637)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0885  max mem: 15572
Epoch: [30]  [2080/2809]  eta: 0:06:54  lr: 0.000008  min_lr: 0.000000  loss: 3.7989 (3.7514)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (60922.3412)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1372  max mem: 15572
Epoch: [30]  [2090/2809]  eta: 0:06:48  lr: 0.000008  min_lr: 0.000000  loss: 3.7989 (3.7506)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (60944.4055)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1554  max mem: 15572
Epoch: [30]  [2100/2809]  eta: 0:06:43  lr: 0.000008  min_lr: 0.000000  loss: 3.7055 (3.7499)  class_acc: 0.2917 (0.3354)  loss_scale: 65536.0000 (60966.2599)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1030  max mem: 15572
Epoch: [30]  [2110/2809]  eta: 0:06:37  lr: 0.000008  min_lr: 0.000000  loss: 3.8552 (3.7501)  class_acc: 0.2917 (0.3352)  loss_scale: 65536.0000 (60987.9072)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1159  max mem: 15572
Epoch: [30]  [2120/2809]  eta: 0:06:32  lr: 0.000008  min_lr: 0.000000  loss: 3.6750 (3.7501)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (61009.3503)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.1492  max mem: 15572
Epoch: [30]  [2130/2809]  eta: 0:06:25  lr: 0.000008  min_lr: 0.000000  loss: 3.6750 (3.7501)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (61030.5922)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0684  max mem: 15572
Epoch: [30]  [2140/2809]  eta: 0:06:20  lr: 0.000008  min_lr: 0.000000  loss: 3.8649 (3.7507)  class_acc: 0.2917 (0.3350)  loss_scale: 65536.0000 (61051.6357)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0814  max mem: 15572
Epoch: [30]  [2150/2809]  eta: 0:06:14  lr: 0.000008  min_lr: 0.000000  loss: 3.9513 (3.7513)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (61072.4835)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1450  max mem: 15572
Epoch: [30]  [2160/2809]  eta: 0:06:09  lr: 0.000008  min_lr: 0.000000  loss: 3.7974 (3.7516)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (61093.1384)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1485  max mem: 15572
Epoch: [30]  [2170/2809]  eta: 0:06:03  lr: 0.000008  min_lr: 0.000000  loss: 3.7901 (3.7518)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (61113.6029)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1615  max mem: 15572
Epoch: [30]  [2180/2809]  eta: 0:05:57  lr: 0.000008  min_lr: 0.000000  loss: 3.7769 (3.7514)  class_acc: 0.2917 (0.3348)  loss_scale: 65536.0000 (61133.8799)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.1306  max mem: 15572
[2025-01-16 04:57:48,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:57:48,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:57:49,883] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86458
[2025-01-16 04:57:49,883] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:57:49,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2190/2809]  eta: 0:05:52  lr: 0.000008  min_lr: 0.000000  loss: 3.8565 (3.7518)  class_acc: 0.2917 (0.3347)  loss_scale: 65536.0000 (61243.7061)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.0948  max mem: 15572
Epoch: [30]  [2200/2809]  eta: 0:05:46  lr: 0.000008  min_lr: 0.000000  loss: 3.8267 (3.7520)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (61263.2076)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0958  max mem: 15572
Epoch: [30]  [2210/2809]  eta: 0:05:40  lr: 0.000008  min_lr: 0.000000  loss: 3.6715 (3.7512)  class_acc: 0.3750 (0.3351)  loss_scale: 65536.0000 (61282.5328)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1333  max mem: 15572
Epoch: [30]  [2220/2809]  eta: 0:05:35  lr: 0.000008  min_lr: 0.000000  loss: 3.5490 (3.7508)  class_acc: 0.3750 (0.3351)  loss_scale: 65536.0000 (61301.6839)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1672  max mem: 15572
Epoch: [30]  [2230/2809]  eta: 0:05:29  lr: 0.000008  min_lr: 0.000000  loss: 3.5071 (3.7478)  class_acc: 0.3750 (0.3358)  loss_scale: 65536.0000 (61320.6634)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1576  max mem: 15572
Epoch: [30]  [2240/2809]  eta: 0:05:23  lr: 0.000008  min_lr: 0.000000  loss: 3.5129 (3.7488)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (61339.4734)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1430  max mem: 15572
Epoch: [30]  [2250/2809]  eta: 0:05:18  lr: 0.000008  min_lr: 0.000000  loss: 3.9685 (3.7496)  class_acc: 0.2500 (0.3352)  loss_scale: 65536.0000 (61358.1164)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1397  max mem: 15572
Epoch: [30]  [2260/2809]  eta: 0:05:12  lr: 0.000008  min_lr: 0.000000  loss: 3.8215 (3.7496)  class_acc: 0.3333 (0.3351)  loss_scale: 65536.0000 (61376.5944)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1452  max mem: 15572
Epoch: [30]  [2270/2809]  eta: 0:05:06  lr: 0.000008  min_lr: 0.000000  loss: 3.7999 (3.7491)  class_acc: 0.2917 (0.3350)  loss_scale: 65536.0000 (61394.9097)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1847  max mem: 15572
Epoch: [30]  [2280/2809]  eta: 0:05:00  lr: 0.000008  min_lr: 0.000000  loss: 3.8167 (3.7506)  class_acc: 0.2500 (0.3345)  loss_scale: 65536.0000 (61413.0644)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1056  max mem: 15572
Epoch: [30]  [2290/2809]  eta: 0:04:55  lr: 0.000008  min_lr: 0.000000  loss: 3.7802 (3.7496)  class_acc: 0.2917 (0.3347)  loss_scale: 65536.0000 (61431.0607)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0350  max mem: 15572
Epoch: [30]  [2300/2809]  eta: 0:04:49  lr: 0.000008  min_lr: 0.000000  loss: 3.6915 (3.7499)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (61448.9005)  weight_decay: 0.0500 (0.0500)  time: 0.5190  data: 0.0651  max mem: 15572
Epoch: [30]  [2310/2809]  eta: 0:04:43  lr: 0.000008  min_lr: 0.000000  loss: 3.7624 (3.7507)  class_acc: 0.2917 (0.3345)  loss_scale: 65536.0000 (61466.5859)  weight_decay: 0.0500 (0.0500)  time: 0.5091  data: 0.0740  max mem: 15572
[2025-01-16 04:59:00,736] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 04:59:00,736] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 04:59:02,626] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86589
[2025-01-16 04:59:02,627] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 04:59:02,627] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [2320/2809]  eta: 0:04:37  lr: 0.000008  min_lr: 0.000000  loss: 3.7101 (3.7495)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (61540.5911)  weight_decay: 0.0500 (0.0500)  time: 0.5325  data: 0.0934  max mem: 15572
Epoch: [30]  [2330/2809]  eta: 0:04:32  lr: 0.000008  min_lr: 0.000000  loss: 3.8103 (3.7502)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (61557.7314)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1163  max mem: 15572
Epoch: [30]  [2340/2809]  eta: 0:04:26  lr: 0.000008  min_lr: 0.000000  loss: 3.9585 (3.7510)  class_acc: 0.2500 (0.3343)  loss_scale: 65536.0000 (61574.7253)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1317  max mem: 15572
Epoch: [30]  [2350/2809]  eta: 0:04:20  lr: 0.000008  min_lr: 0.000000  loss: 3.9061 (3.7506)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (61591.5746)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1243  max mem: 15572
Epoch: [30]  [2360/2809]  eta: 0:04:15  lr: 0.000008  min_lr: 0.000000  loss: 3.8437 (3.7504)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (61608.2812)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1242  max mem: 15572
Epoch: [30]  [2370/2809]  eta: 0:04:09  lr: 0.000008  min_lr: 0.000000  loss: 3.8437 (3.7503)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (61624.8469)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1382  max mem: 15572
[2025-01-16 04:59:35,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86645
[2025-01-16 04:59:35,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 04:59:35,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2380/2809]  eta: 0:04:03  lr: 0.000008  min_lr: 0.000000  loss: 3.7869 (3.7500)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (61558.6997)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1960  max mem: 15572
Epoch: [30]  [2390/2809]  eta: 0:03:58  lr: 0.000008  min_lr: 0.000000  loss: 3.7626 (3.7499)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (61438.2869)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1895  max mem: 15572
Epoch: [30]  [2400/2809]  eta: 0:03:52  lr: 0.000008  min_lr: 0.000000  loss: 3.5827 (3.7487)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (61318.8771)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.1206  max mem: 15572
Epoch: [30]  [2410/2809]  eta: 0:03:46  lr: 0.000007  min_lr: 0.000000  loss: 3.6965 (3.7484)  class_acc: 0.3750 (0.3351)  loss_scale: 32768.0000 (61200.4579)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1463  max mem: 15572
Epoch: [30]  [2420/2809]  eta: 0:03:41  lr: 0.000007  min_lr: 0.000000  loss: 3.7255 (3.7480)  class_acc: 0.3750 (0.3353)  loss_scale: 32768.0000 (61083.0169)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1463  max mem: 15572
Epoch: [30]  [2430/2809]  eta: 0:03:35  lr: 0.000007  min_lr: 0.000000  loss: 3.4975 (3.7470)  class_acc: 0.4167 (0.3356)  loss_scale: 32768.0000 (60966.5422)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1194  max mem: 15572
Epoch: [30]  [2440/2809]  eta: 0:03:29  lr: 0.000007  min_lr: 0.000000  loss: 3.7209 (3.7467)  class_acc: 0.4167 (0.3359)  loss_scale: 32768.0000 (60851.0217)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1303  max mem: 15572
Epoch: [30]  [2450/2809]  eta: 0:03:24  lr: 0.000007  min_lr: 0.000000  loss: 3.7739 (3.7470)  class_acc: 0.4167 (0.3361)  loss_scale: 32768.0000 (60736.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1559  max mem: 15572
Epoch: [30]  [2460/2809]  eta: 0:03:18  lr: 0.000007  min_lr: 0.000000  loss: 3.6234 (3.7459)  class_acc: 0.4167 (0.3363)  loss_scale: 32768.0000 (60622.7972)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1779  max mem: 15572
Epoch: [30]  [2470/2809]  eta: 0:03:12  lr: 0.000007  min_lr: 0.000000  loss: 3.5143 (3.7453)  class_acc: 0.3333 (0.3365)  loss_scale: 32768.0000 (60510.0704)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0915  max mem: 15572
Epoch: [30]  [2480/2809]  eta: 0:03:07  lr: 0.000007  min_lr: 0.000000  loss: 3.5730 (3.7455)  class_acc: 0.3333 (0.3362)  loss_scale: 32768.0000 (60398.2523)  weight_decay: 0.0500 (0.0500)  time: 0.4813  data: 0.0417  max mem: 15572
Epoch: [30]  [2490/2809]  eta: 0:03:01  lr: 0.000007  min_lr: 0.000000  loss: 3.5730 (3.7449)  class_acc: 0.3333 (0.3364)  loss_scale: 32768.0000 (60287.3320)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.1005  max mem: 15572
Epoch: [30]  [2500/2809]  eta: 0:02:55  lr: 0.000007  min_lr: 0.000000  loss: 3.5922 (3.7447)  class_acc: 0.3750 (0.3365)  loss_scale: 32768.0000 (60177.2987)  weight_decay: 0.0500 (0.0500)  time: 0.5157  data: 0.0682  max mem: 15572
[2025-01-16 05:00:48,667] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:00:48,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2510/2809]  eta: 0:02:49  lr: 0.000007  min_lr: 0.000000  loss: 3.7602 (3.7443)  class_acc: 0.3333 (0.3365)  loss_scale: 32768.0000 (60159.4902)  weight_decay: 0.0500 (0.0500)  time: 0.5522  data: 0.1073  max mem: 15572
Epoch: [30]  [2520/2809]  eta: 0:02:44  lr: 0.000007  min_lr: 0.000000  loss: 3.7602 (3.7447)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (60180.8171)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1431  max mem: 15572
Epoch: [30]  [2530/2809]  eta: 0:02:38  lr: 0.000007  min_lr: 0.000000  loss: 3.7593 (3.7447)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (60201.9755)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1268  max mem: 15572
Epoch: [30]  [2540/2809]  eta: 0:02:32  lr: 0.000007  min_lr: 0.000000  loss: 3.6061 (3.7447)  class_acc: 0.3333 (0.3363)  loss_scale: 65536.0000 (60222.9673)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1107  max mem: 15572
Epoch: [30]  [2550/2809]  eta: 0:02:27  lr: 0.000007  min_lr: 0.000000  loss: 3.6850 (3.7446)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (60243.7946)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.0993  max mem: 15572
Epoch: [30]  [2560/2809]  eta: 0:02:21  lr: 0.000007  min_lr: 0.000000  loss: 3.7681 (3.7452)  class_acc: 0.2917 (0.3363)  loss_scale: 65536.0000 (60264.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1390  max mem: 15572
Epoch: [30]  [2570/2809]  eta: 0:02:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8429 (3.7453)  class_acc: 0.2500 (0.3361)  loss_scale: 65536.0000 (60284.9630)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1364  max mem: 15572
Epoch: [30]  [2580/2809]  eta: 0:02:10  lr: 0.000007  min_lr: 0.000000  loss: 3.7520 (3.7443)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (60305.3080)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1367  max mem: 15572
Epoch: [30]  [2590/2809]  eta: 0:02:04  lr: 0.000007  min_lr: 0.000000  loss: 3.5818 (3.7438)  class_acc: 0.3333 (0.3365)  loss_scale: 65536.0000 (60325.4959)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1587  max mem: 15572
Epoch: [30]  [2600/2809]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000000  loss: 3.6902 (3.7446)  class_acc: 0.3333 (0.3362)  loss_scale: 65536.0000 (60345.5286)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1572  max mem: 15572
[2025-01-16 05:01:47,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 86877
[2025-01-16 05:01:47,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:01:47,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2610/2809]  eta: 0:01:53  lr: 0.000007  min_lr: 0.000000  loss: 3.9001 (3.7445)  class_acc: 0.2917 (0.3363)  loss_scale: 65536.0000 (60315.2080)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1536  max mem: 15572
Epoch: [30]  [2620/2809]  eta: 0:01:47  lr: 0.000007  min_lr: 0.000000  loss: 3.8368 (3.7454)  class_acc: 0.3333 (0.3363)  loss_scale: 32768.0000 (60210.1061)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1280  max mem: 15572
Epoch: [30]  [2630/2809]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 3.8368 (3.7457)  class_acc: 0.3333 (0.3364)  loss_scale: 32768.0000 (60105.8031)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1318  max mem: 15572
Epoch: [30]  [2640/2809]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 3.9646 (3.7466)  class_acc: 0.2500 (0.3361)  loss_scale: 32768.0000 (60002.2900)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1437  max mem: 15572
Epoch: [30]  [2650/2809]  eta: 0:01:30  lr: 0.000007  min_lr: 0.000000  loss: 3.9334 (3.7469)  class_acc: 0.2500 (0.3361)  loss_scale: 32768.0000 (59899.5579)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.1336  max mem: 15572
Epoch: [30]  [2660/2809]  eta: 0:01:24  lr: 0.000007  min_lr: 0.000000  loss: 3.8363 (3.7472)  class_acc: 0.3333 (0.3360)  loss_scale: 32768.0000 (59797.5979)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1243  max mem: 15572
Epoch: [30]  [2670/2809]  eta: 0:01:19  lr: 0.000007  min_lr: 0.000000  loss: 3.9481 (3.7478)  class_acc: 0.2917 (0.3358)  loss_scale: 32768.0000 (59696.4013)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.0847  max mem: 15572
Epoch: [30]  [2680/2809]  eta: 0:01:13  lr: 0.000007  min_lr: 0.000000  loss: 3.7752 (3.7476)  class_acc: 0.2917 (0.3359)  loss_scale: 32768.0000 (59595.9597)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1265  max mem: 15572
Epoch: [30]  [2690/2809]  eta: 0:01:07  lr: 0.000007  min_lr: 0.000000  loss: 3.6694 (3.7471)  class_acc: 0.3750 (0.3359)  loss_scale: 32768.0000 (59496.2646)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1102  max mem: 15572
Epoch: [30]  [2700/2809]  eta: 0:01:01  lr: 0.000007  min_lr: 0.000000  loss: 3.7101 (3.7470)  class_acc: 0.2917 (0.3358)  loss_scale: 32768.0000 (59397.3077)  weight_decay: 0.0500 (0.0500)  time: 0.4990  data: 0.0681  max mem: 15572
Epoch: [30]  [2710/2809]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 3.8339 (3.7472)  class_acc: 0.2917 (0.3358)  loss_scale: 32768.0000 (59299.0808)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0686  max mem: 15572
Epoch: [30]  [2720/2809]  eta: 0:00:50  lr: 0.000007  min_lr: 0.000000  loss: 3.7518 (3.7472)  class_acc: 0.3333 (0.3358)  loss_scale: 32768.0000 (59201.5759)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1010  max mem: 15572
[2025-01-16 05:02:56,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=87000, skipped=580, lr=[7.09583022273898e-08, 7.09583022273898e-08, 1.0136900318198544e-07, 1.0136900318198544e-07, 1.4481286168855063e-07, 1.4481286168855063e-07, 2.068755166979295e-07, 2.068755166979295e-07, 2.955364524256136e-07, 2.955364524256136e-07, 4.2219493203659086e-07, 4.2219493203659086e-07, 6.031356171951298e-07, 6.031356171951298e-07, 8.61622310278757e-07, 8.61622310278757e-07, 1.2308890146839386e-06, 1.2308890146839386e-06, 1.7584128781199124e-06, 1.7584128781199124e-06, 2.5120183973141607e-06, 2.5120183973141607e-06, 3.588597710448801e-06, 3.588597710448801e-06, 5.1265681577840024e-06, 5.1265681577840024e-06, 7.323668796834289e-06, 7.323668796834289e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:02:56,649] [INFO] [timer.py:260:stop] epoch=0/micro_step=87000/global_step=87000, RunningAvgSamplesPerSec=28.571846361338462, CurrSamplesPerSec=24.596525127657795, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [30]  [2730/2809]  eta: 0:00:44  lr: 0.000007  min_lr: 0.000000  loss: 3.7746 (3.7476)  class_acc: 0.3333 (0.3360)  loss_scale: 32768.0000 (59104.7851)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1300  max mem: 15572
[2025-01-16 05:03:01,030] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:03:01,031] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [2740/2809]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 3.6998 (3.7466)  class_acc: 0.3750 (0.3362)  loss_scale: 32768.0000 (59068.4743)  weight_decay: 0.0500 (0.0500)  time: 0.5797  data: 0.1064  max mem: 15572
Epoch: [30]  [2750/2809]  eta: 0:00:33  lr: 0.000007  min_lr: 0.000000  loss: 3.4087 (3.7454)  class_acc: 0.3750 (0.3364)  loss_scale: 65536.0000 (59091.9840)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0787  max mem: 15572
Epoch: [30]  [2760/2809]  eta: 0:00:27  lr: 0.000007  min_lr: 0.000000  loss: 3.7104 (3.7461)  class_acc: 0.3750 (0.3365)  loss_scale: 65536.0000 (59115.3234)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0614  max mem: 15572
Epoch: [30]  [2770/2809]  eta: 0:00:22  lr: 0.000007  min_lr: 0.000000  loss: 3.8719 (3.7462)  class_acc: 0.3750 (0.3366)  loss_scale: 65536.0000 (59138.4944)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.0828  max mem: 15572
Epoch: [30]  [2780/2809]  eta: 0:00:16  lr: 0.000007  min_lr: 0.000000  loss: 3.8719 (3.7462)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (59161.4987)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0976  max mem: 15572
[2025-01-16 05:03:27,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87054
[2025-01-16 05:03:27,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:03:27,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [2790/2809]  eta: 0:00:10  lr: 0.000007  min_lr: 0.000000  loss: 3.8887 (3.7462)  class_acc: 0.3333 (0.3367)  loss_scale: 65536.0000 (59102.1541)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1136  max mem: 15572
Epoch: [30]  [2800/2809]  eta: 0:00:05  lr: 0.000007  min_lr: 0.000000  loss: 3.6852 (3.7463)  class_acc: 0.3333 (0.3366)  loss_scale: 32768.0000 (59008.1371)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1206  max mem: 15572
Epoch: [30]  [2808/2809]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 3.6700 (3.7462)  class_acc: 0.2917 (0.3366)  loss_scale: 32768.0000 (58933.4055)  weight_decay: 0.0500 (0.0500)  time: 0.4763  data: 0.0564  max mem: 15572
Epoch: [30] Total time: 0:26:36 (0.5683 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 3.6700 (3.7462)  class_acc: 0.2917 (0.3366)  loss_scale: 32768.0000 (58933.4055)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:08  loss: 0.3903 (0.3903)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.7804  data: 3.5198  max mem: 15572
Val:  [ 10/272]  eta: 0:02:45  loss: 2.4620 (2.2751)  acc1: 50.0000 (44.4444)  acc5: 72.2222 (71.7172)  time: 0.6307  data: 0.4544  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.2482 (2.2658)  acc1: 50.0000 (47.8836)  acc5: 72.2222 (73.0159)  time: 0.3369  data: 0.1615  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 2.2936 (2.3702)  acc1: 50.0000 (43.7276)  acc5: 72.2222 (72.7599)  time: 0.3225  data: 0.1344  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 2.5632 (2.4354)  acc1: 33.3333 (41.0569)  acc5: 77.7778 (73.0352)  time: 0.3293  data: 0.1406  max mem: 15572
Val:  [ 50/272]  eta: 0:01:26  loss: 2.4741 (2.3619)  acc1: 38.8889 (42.4837)  acc5: 77.7778 (74.6187)  time: 0.3252  data: 0.1402  max mem: 15572
Val:  [ 60/272]  eta: 0:01:18  loss: 1.4356 (2.2533)  acc1: 61.1111 (45.6284)  acc5: 83.3333 (75.6831)  time: 0.2794  data: 0.1053  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.4798 (2.1774)  acc1: 66.6667 (47.8873)  acc5: 83.3333 (76.5258)  time: 0.3202  data: 0.1427  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.8865 (2.1950)  acc1: 55.5556 (47.6680)  acc5: 77.7778 (76.1317)  time: 0.3717  data: 0.1766  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.1603 (2.2061)  acc1: 50.0000 (47.9853)  acc5: 77.7778 (76.6789)  time: 0.3187  data: 0.1258  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.1409 (2.2292)  acc1: 55.5556 (47.3047)  acc5: 83.3333 (76.7327)  time: 0.2824  data: 0.0862  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.3118 (2.3005)  acc1: 27.7778 (45.2452)  acc5: 77.7778 (75.6757)  time: 0.3164  data: 0.1207  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 2.8408 (2.3371)  acc1: 16.6667 (44.3067)  acc5: 72.2222 (75.2984)  time: 0.3289  data: 0.1250  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.2191 (2.2987)  acc1: 44.4444 (45.4623)  acc5: 77.7778 (76.1238)  time: 0.2997  data: 0.0854  max mem: 15572
Val:  [140/272]  eta: 0:00:44  loss: 1.7022 (2.2864)  acc1: 55.5556 (45.8235)  acc5: 88.8889 (76.0835)  time: 0.2699  data: 0.0718  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.2581 (2.2901)  acc1: 38.8889 (45.2907)  acc5: 77.7778 (76.3429)  time: 0.3084  data: 0.1195  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2581 (2.2811)  acc1: 44.4444 (45.8247)  acc5: 77.7778 (76.6391)  time: 0.3120  data: 0.1180  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4442 (2.3033)  acc1: 44.4444 (45.2242)  acc5: 72.2222 (76.1858)  time: 0.2943  data: 0.1061  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.2930 (2.2925)  acc1: 33.3333 (45.0583)  acc5: 77.7778 (76.6114)  time: 0.3335  data: 0.1426  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.2930 (2.3468)  acc1: 33.3333 (43.8627)  acc5: 77.7778 (75.2472)  time: 0.3303  data: 0.1251  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.5781 (2.3556)  acc1: 33.3333 (43.5323)  acc5: 72.2222 (74.9309)  time: 0.3036  data: 0.1018  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.0949 (2.3561)  acc1: 44.4444 (43.8389)  acc5: 77.7778 (74.8815)  time: 0.3126  data: 0.1162  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1038 (2.3421)  acc1: 50.0000 (44.0925)  acc5: 77.7778 (75.0628)  time: 0.3143  data: 0.1244  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7991 (2.3113)  acc1: 61.1111 (45.1419)  acc5: 83.3333 (75.3728)  time: 0.3307  data: 0.1525  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6664 (2.2988)  acc1: 61.1111 (45.2513)  acc5: 83.3333 (75.6570)  time: 0.3383  data: 0.1614  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2506 (2.3097)  acc1: 33.3333 (44.5994)  acc5: 77.7778 (75.6751)  time: 0.3506  data: 0.1656  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1634 (2.2508)  acc1: 72.2222 (46.3176)  acc5: 88.8889 (76.4155)  time: 0.2982  data: 0.1139  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3192 (2.2449)  acc1: 72.2222 (46.4740)  acc5: 88.8889 (76.6503)  time: 0.1842  data: 0.0211  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3192 (2.2501)  acc1: 66.6667 (46.4469)  acc5: 88.8889 (76.6127)  time: 0.1777  data: 0.0210  max mem: 15572
Val: Total time: 0:01:27 (0.3222 s / it)
* Acc@1 46.447 Acc@5 76.613 loss 2.250
Accuracy of the network on the 4883 val videos: 46.4%
Max accuracy: 46.77%
Epoch: [31]  [   0/2809]  eta: 4:08:41  lr: 0.000007  min_lr: 0.000000  loss: 4.4006 (4.4006)  class_acc: 0.2500 (0.2500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 5.3122  data: 4.9046  max mem: 15572
Epoch: [31]  [  10/2809]  eta: 0:39:29  lr: 0.000007  min_lr: 0.000000  loss: 3.4259 (3.5764)  class_acc: 0.3750 (0.3902)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8465  data: 0.4463  max mem: 15572
Epoch: [31]  [  20/2809]  eta: 0:30:52  lr: 0.000007  min_lr: 0.000000  loss: 3.4404 (3.6550)  class_acc: 0.3750 (0.3750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4318  data: 0.0054  max mem: 15572
Epoch: [31]  [  30/2809]  eta: 0:31:01  lr: 0.000007  min_lr: 0.000000  loss: 3.8735 (3.6767)  class_acc: 0.3333 (0.3508)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5726  data: 0.1225  max mem: 15572
Epoch: [31]  [  40/2809]  eta: 0:30:25  lr: 0.000007  min_lr: 0.000000  loss: 3.9554 (3.7726)  class_acc: 0.2500 (0.3384)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.2045  max mem: 15572
Epoch: [31]  [  50/2809]  eta: 0:29:58  lr: 0.000007  min_lr: 0.000000  loss: 4.0505 (3.7699)  class_acc: 0.2500 (0.3309)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1828  max mem: 15572
Epoch: [31]  [  60/2809]  eta: 0:29:54  lr: 0.000007  min_lr: 0.000000  loss: 3.8064 (3.7464)  class_acc: 0.3333 (0.3381)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.1873  max mem: 15572
Epoch: [31]  [  70/2809]  eta: 0:29:45  lr: 0.000007  min_lr: 0.000000  loss: 3.6917 (3.7343)  class_acc: 0.3750 (0.3462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.1702  max mem: 15572
Epoch: [31]  [  80/2809]  eta: 0:29:27  lr: 0.000007  min_lr: 0.000000  loss: 3.6996 (3.7195)  class_acc: 0.4583 (0.3529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1558  max mem: 15572
Epoch: [31]  [  90/2809]  eta: 0:29:36  lr: 0.000007  min_lr: 0.000000  loss: 3.5370 (3.7032)  class_acc: 0.4583 (0.3558)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.1904  max mem: 15572
Epoch: [31]  [ 100/2809]  eta: 0:29:34  lr: 0.000007  min_lr: 0.000000  loss: 3.7207 (3.7328)  class_acc: 0.3333 (0.3498)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6851  data: 0.2304  max mem: 15572
[2025-01-16 05:06:17,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:06:17,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 110/2809]  eta: 0:29:24  lr: 0.000007  min_lr: 0.000000  loss: 4.0907 (3.7608)  class_acc: 0.2500 (0.3431)  loss_scale: 32768.0000 (34834.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.2121  max mem: 15572
Epoch: [31]  [ 120/2809]  eta: 0:29:34  lr: 0.000007  min_lr: 0.000000  loss: 3.8255 (3.7566)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.2088  max mem: 15572
Epoch: [31]  [ 130/2809]  eta: 0:29:26  lr: 0.000007  min_lr: 0.000000  loss: 3.5493 (3.7428)  class_acc: 0.3750 (0.3438)  loss_scale: 65536.0000 (39521.7099)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.2065  max mem: 15572
Epoch: [31]  [ 140/2809]  eta: 0:29:05  lr: 0.000007  min_lr: 0.000000  loss: 3.6413 (3.7492)  class_acc: 0.3750 (0.3413)  loss_scale: 65536.0000 (41366.6950)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1568  max mem: 15572
Epoch: [31]  [ 150/2809]  eta: 0:28:21  lr: 0.000007  min_lr: 0.000000  loss: 3.7417 (3.7488)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (42967.3113)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0818  max mem: 15572
Epoch: [31]  [ 160/2809]  eta: 0:27:43  lr: 0.000007  min_lr: 0.000000  loss: 3.7707 (3.7486)  class_acc: 0.3750 (0.3473)  loss_scale: 65536.0000 (44369.0932)  weight_decay: 0.0500 (0.0500)  time: 0.4426  data: 0.0289  max mem: 15572
Epoch: [31]  [ 170/2809]  eta: 0:27:16  lr: 0.000007  min_lr: 0.000000  loss: 3.8105 (3.7558)  class_acc: 0.3750 (0.3462)  loss_scale: 65536.0000 (45606.9240)  weight_decay: 0.0500 (0.0500)  time: 0.4695  data: 0.0372  max mem: 15572
Epoch: [31]  [ 180/2809]  eta: 0:27:10  lr: 0.000007  min_lr: 0.000000  loss: 3.8090 (3.7598)  class_acc: 0.2917 (0.3428)  loss_scale: 65536.0000 (46707.9779)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1214  max mem: 15572
Epoch: [31]  [ 190/2809]  eta: 0:26:40  lr: 0.000007  min_lr: 0.000000  loss: 3.8587 (3.7697)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (47693.7382)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0957  max mem: 15572
Epoch: [31]  [ 200/2809]  eta: 0:26:23  lr: 0.000007  min_lr: 0.000000  loss: 3.7728 (3.7609)  class_acc: 0.3750 (0.3439)  loss_scale: 65536.0000 (48581.4129)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0480  max mem: 15572
Epoch: [31]  [ 210/2809]  eta: 0:26:14  lr: 0.000007  min_lr: 0.000000  loss: 3.7681 (3.7641)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (49384.9479)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1226  max mem: 15572
Epoch: [31]  [ 220/2809]  eta: 0:26:07  lr: 0.000007  min_lr: 0.000000  loss: 3.7203 (3.7536)  class_acc: 0.3333 (0.3452)  loss_scale: 65536.0000 (50115.7647)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1638  max mem: 15572
Epoch: [31]  [ 230/2809]  eta: 0:25:58  lr: 0.000007  min_lr: 0.000000  loss: 3.7203 (3.7624)  class_acc: 0.3333 (0.3425)  loss_scale: 65536.0000 (50783.3074)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1600  max mem: 15572
[2025-01-16 05:07:27,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:07:27,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:07:30,693] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87314
[2025-01-16 05:07:30,694] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:07:30,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 240/2809]  eta: 0:25:50  lr: 0.000007  min_lr: 0.000000  loss: 3.9383 (3.7623)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (52211.2531)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1480  max mem: 15572
Epoch: [31]  [ 250/2809]  eta: 0:25:32  lr: 0.000007  min_lr: 0.000000  loss: 3.8292 (3.7645)  class_acc: 0.2917 (0.3425)  loss_scale: 65536.0000 (52742.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1080  max mem: 15572
Epoch: [31]  [ 260/2809]  eta: 0:25:27  lr: 0.000007  min_lr: 0.000000  loss: 3.9483 (3.7715)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (53232.3065)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1110  max mem: 15572
Epoch: [31]  [ 270/2809]  eta: 0:25:14  lr: 0.000007  min_lr: 0.000000  loss: 3.9483 (3.7634)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (53686.3173)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1202  max mem: 15572
Epoch: [31]  [ 280/2809]  eta: 0:25:07  lr: 0.000007  min_lr: 0.000000  loss: 3.4930 (3.7576)  class_acc: 0.3750 (0.3449)  loss_scale: 65536.0000 (54108.0142)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1101  max mem: 15572
Epoch: [31]  [ 290/2809]  eta: 0:25:00  lr: 0.000007  min_lr: 0.000000  loss: 3.7731 (3.7647)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (54500.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1381  max mem: 15572
Epoch: [31]  [ 300/2809]  eta: 0:24:51  lr: 0.000007  min_lr: 0.000000  loss: 3.7731 (3.7631)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (54867.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1215  max mem: 15572
Epoch: [31]  [ 310/2809]  eta: 0:24:48  lr: 0.000007  min_lr: 0.000000  loss: 3.7138 (3.7617)  class_acc: 0.3333 (0.3424)  loss_scale: 65536.0000 (55210.3923)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1473  max mem: 15572
Epoch: [31]  [ 320/2809]  eta: 0:24:44  lr: 0.000007  min_lr: 0.000000  loss: 3.6935 (3.7562)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (55532.0623)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1775  max mem: 15572
Epoch: [31]  [ 330/2809]  eta: 0:24:35  lr: 0.000007  min_lr: 0.000000  loss: 3.5946 (3.7528)  class_acc: 0.3750 (0.3443)  loss_scale: 65536.0000 (55834.2961)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1274  max mem: 15572
Epoch: [31]  [ 340/2809]  eta: 0:24:29  lr: 0.000007  min_lr: 0.000000  loss: 3.7564 (3.7540)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (56118.8035)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1211  max mem: 15572
Epoch: [31]  [ 350/2809]  eta: 0:24:20  lr: 0.000007  min_lr: 0.000000  loss: 3.9067 (3.7610)  class_acc: 0.2500 (0.3405)  loss_scale: 65536.0000 (56387.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1374  max mem: 15572
Epoch: [31]  [ 360/2809]  eta: 0:24:12  lr: 0.000007  min_lr: 0.000000  loss: 3.8995 (3.7535)  class_acc: 0.2917 (0.3423)  loss_scale: 65536.0000 (56640.5319)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1179  max mem: 15572
[2025-01-16 05:08:43,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:08:43,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:08:43,846] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87444
[2025-01-16 05:08:43,846] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:08:43,846] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 370/2809]  eta: 0:24:03  lr: 0.000007  min_lr: 0.000000  loss: 3.6136 (3.7542)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (57056.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.0938  max mem: 15572
Epoch: [31]  [ 380/2809]  eta: 0:23:55  lr: 0.000007  min_lr: 0.000000  loss: 3.6498 (3.7470)  class_acc: 0.3333 (0.3443)  loss_scale: 65536.0000 (57279.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1031  max mem: 15572
Epoch: [31]  [ 390/2809]  eta: 0:23:46  lr: 0.000007  min_lr: 0.000000  loss: 3.5759 (3.7423)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (57490.6598)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1243  max mem: 15572
Epoch: [31]  [ 400/2809]  eta: 0:23:33  lr: 0.000007  min_lr: 0.000000  loss: 3.7053 (3.7380)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (57691.2918)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0733  max mem: 15572
Epoch: [31]  [ 410/2809]  eta: 0:23:29  lr: 0.000007  min_lr: 0.000000  loss: 3.8672 (3.7372)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (57882.1606)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1033  max mem: 15572
Epoch: [31]  [ 420/2809]  eta: 0:23:21  lr: 0.000007  min_lr: 0.000000  loss: 3.9361 (3.7436)  class_acc: 0.2083 (0.3411)  loss_scale: 65536.0000 (58063.9620)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1435  max mem: 15572
Epoch: [31]  [ 430/2809]  eta: 0:23:10  lr: 0.000007  min_lr: 0.000000  loss: 3.7485 (3.7338)  class_acc: 0.3333 (0.3430)  loss_scale: 65536.0000 (58237.3271)  weight_decay: 0.0500 (0.0500)  time: 0.5211  data: 0.0859  max mem: 15572
Epoch: [31]  [ 440/2809]  eta: 0:23:08  lr: 0.000007  min_lr: 0.000000  loss: 3.6188 (3.7310)  class_acc: 0.3750 (0.3432)  loss_scale: 65536.0000 (58402.8299)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1242  max mem: 15572
Epoch: [31]  [ 450/2809]  eta: 0:23:01  lr: 0.000007  min_lr: 0.000000  loss: 3.7258 (3.7341)  class_acc: 0.2917 (0.3424)  loss_scale: 65536.0000 (58560.9933)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1504  max mem: 15572
Epoch: [31]  [ 460/2809]  eta: 0:22:47  lr: 0.000007  min_lr: 0.000000  loss: 3.7258 (3.7332)  class_acc: 0.3333 (0.3420)  loss_scale: 65536.0000 (58712.2950)  weight_decay: 0.0500 (0.0500)  time: 0.4965  data: 0.0605  max mem: 15572
Epoch: [31]  [ 470/2809]  eta: 0:22:40  lr: 0.000007  min_lr: 0.000000  loss: 3.7609 (3.7328)  class_acc: 0.3333 (0.3418)  loss_scale: 65536.0000 (58857.1720)  weight_decay: 0.0500 (0.0500)  time: 0.4899  data: 0.0617  max mem: 15572
Epoch: [31]  [ 480/2809]  eta: 0:22:33  lr: 0.000007  min_lr: 0.000000  loss: 3.8306 (3.7326)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (58996.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1296  max mem: 15572
Epoch: [31]  [ 490/2809]  eta: 0:22:32  lr: 0.000007  min_lr: 0.000000  loss: 3.7532 (3.7358)  class_acc: 0.3333 (0.3416)  loss_scale: 65536.0000 (59129.2220)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.1838  max mem: 15572
[2025-01-16 05:09:55,594] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:09:55,594] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:09:56,030] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87574
[2025-01-16 05:09:56,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:09:56,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 500/2809]  eta: 0:22:23  lr: 0.000007  min_lr: 0.000000  loss: 3.9203 (3.7427)  class_acc: 0.2917 (0.3393)  loss_scale: 65536.0000 (59387.9122)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1443  max mem: 15572
Epoch: [31]  [ 510/2809]  eta: 0:22:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8426 (3.7405)  class_acc: 0.3333 (0.3401)  loss_scale: 65536.0000 (59508.2270)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0813  max mem: 15572
Epoch: [31]  [ 520/2809]  eta: 0:22:11  lr: 0.000007  min_lr: 0.000000  loss: 3.5982 (3.7374)  class_acc: 0.4167 (0.3416)  loss_scale: 65536.0000 (59623.9232)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1388  max mem: 15572
Epoch: [31]  [ 530/2809]  eta: 0:22:01  lr: 0.000007  min_lr: 0.000000  loss: 3.9325 (3.7388)  class_acc: 0.2917 (0.3413)  loss_scale: 65536.0000 (59735.2618)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1146  max mem: 15572
Epoch: [31]  [ 540/2809]  eta: 0:21:54  lr: 0.000007  min_lr: 0.000000  loss: 3.9510 (3.7404)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (59842.4843)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0839  max mem: 15572
Epoch: [31]  [ 550/2809]  eta: 0:21:48  lr: 0.000007  min_lr: 0.000000  loss: 3.9344 (3.7429)  class_acc: 0.2917 (0.3398)  loss_scale: 65536.0000 (59945.8149)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1246  max mem: 15572
Epoch: [31]  [ 560/2809]  eta: 0:21:41  lr: 0.000007  min_lr: 0.000000  loss: 3.7569 (3.7415)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (60045.4617)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1037  max mem: 15572
Epoch: [31]  [ 570/2809]  eta: 0:21:33  lr: 0.000007  min_lr: 0.000000  loss: 3.5764 (3.7422)  class_acc: 0.3333 (0.3410)  loss_scale: 65536.0000 (60141.6182)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0564  max mem: 15572
Epoch: [31]  [ 580/2809]  eta: 0:21:26  lr: 0.000007  min_lr: 0.000000  loss: 3.4688 (3.7358)  class_acc: 0.4167 (0.3438)  loss_scale: 65536.0000 (60234.4647)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0659  max mem: 15572
Epoch: [31]  [ 590/2809]  eta: 0:21:21  lr: 0.000007  min_lr: 0.000000  loss: 3.5973 (3.7375)  class_acc: 0.4167 (0.3435)  loss_scale: 65536.0000 (60324.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1147  max mem: 15572
Epoch: [31]  [ 600/2809]  eta: 0:21:12  lr: 0.000007  min_lr: 0.000000  loss: 3.7567 (3.7381)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (60410.8885)  weight_decay: 0.0500 (0.0500)  time: 0.5392  data: 0.0865  max mem: 15572
Epoch: [31]  [ 610/2809]  eta: 0:21:09  lr: 0.000007  min_lr: 0.000000  loss: 3.8603 (3.7355)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (60494.7692)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1193  max mem: 15572
Epoch: [31]  [ 620/2809]  eta: 0:21:01  lr: 0.000007  min_lr: 0.000000  loss: 3.7153 (3.7357)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (60575.9485)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1294  max mem: 15572
[2025-01-16 05:11:07,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:11:07,306] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:11:07,719] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87704
[2025-01-16 05:11:07,719] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:11:07,719] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 630/2809]  eta: 0:20:53  lr: 0.000007  min_lr: 0.000000  loss: 3.8810 (3.7369)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (60758.4152)  weight_decay: 0.0500 (0.0500)  time: 0.5111  data: 0.0553  max mem: 15572
Epoch: [31]  [ 640/2809]  eta: 0:20:48  lr: 0.000007  min_lr: 0.000000  loss: 3.8884 (3.7391)  class_acc: 0.2917 (0.3418)  loss_scale: 65536.0000 (60832.9485)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.0858  max mem: 15572
Epoch: [31]  [ 650/2809]  eta: 0:20:42  lr: 0.000007  min_lr: 0.000000  loss: 3.7320 (3.7381)  class_acc: 0.3333 (0.3425)  loss_scale: 65536.0000 (60905.1920)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1226  max mem: 15572
Epoch: [31]  [ 660/2809]  eta: 0:20:35  lr: 0.000007  min_lr: 0.000000  loss: 3.7017 (3.7395)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (60975.2496)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1099  max mem: 15572
[2025-01-16 05:11:27,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87740
[2025-01-16 05:11:27,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:11:27,868] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 670/2809]  eta: 0:20:31  lr: 0.000007  min_lr: 0.000000  loss: 3.7175 (3.7397)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (60554.8733)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1296  max mem: 15572
Epoch: [31]  [ 680/2809]  eta: 0:20:25  lr: 0.000007  min_lr: 0.000000  loss: 3.8704 (3.7421)  class_acc: 0.2917 (0.3415)  loss_scale: 32768.0000 (60146.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1383  max mem: 15572
Epoch: [31]  [ 690/2809]  eta: 0:20:19  lr: 0.000007  min_lr: 0.000000  loss: 3.9914 (3.7447)  class_acc: 0.2500 (0.3405)  loss_scale: 32768.0000 (59750.6223)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1163  max mem: 15572
Epoch: [31]  [ 700/2809]  eta: 0:20:11  lr: 0.000007  min_lr: 0.000000  loss: 3.9558 (3.7420)  class_acc: 0.3333 (0.3418)  loss_scale: 32768.0000 (59365.7061)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0940  max mem: 15572
Epoch: [31]  [ 710/2809]  eta: 0:20:05  lr: 0.000007  min_lr: 0.000000  loss: 3.7323 (3.7436)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (58991.6174)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.1017  max mem: 15572
Epoch: [31]  [ 720/2809]  eta: 0:19:59  lr: 0.000007  min_lr: 0.000000  loss: 3.9286 (3.7456)  class_acc: 0.2917 (0.3406)  loss_scale: 32768.0000 (58627.9057)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1289  max mem: 15572
Epoch: [31]  [ 730/2809]  eta: 0:19:53  lr: 0.000007  min_lr: 0.000000  loss: 3.8303 (3.7450)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (58274.1450)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1280  max mem: 15572
Epoch: [31]  [ 740/2809]  eta: 0:19:48  lr: 0.000007  min_lr: 0.000000  loss: 3.6451 (3.7435)  class_acc: 0.3333 (0.3407)  loss_scale: 32768.0000 (57929.9325)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1448  max mem: 15572
Epoch: [31]  [ 750/2809]  eta: 0:19:41  lr: 0.000007  min_lr: 0.000000  loss: 3.6830 (3.7458)  class_acc: 0.2500 (0.3397)  loss_scale: 32768.0000 (57594.8868)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1293  max mem: 15572
Epoch: [31]  [ 760/2809]  eta: 0:19:37  lr: 0.000007  min_lr: 0.000000  loss: 3.7964 (3.7453)  class_acc: 0.2500 (0.3395)  loss_scale: 32768.0000 (57268.6465)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.1593  max mem: 15572
Epoch: [31]  [ 770/2809]  eta: 0:19:34  lr: 0.000007  min_lr: 0.000000  loss: 3.7295 (3.7440)  class_acc: 0.2500 (0.3397)  loss_scale: 32768.0000 (56950.8690)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.2286  max mem: 15572
Epoch: [31]  [ 780/2809]  eta: 0:19:26  lr: 0.000007  min_lr: 0.000000  loss: 3.6905 (3.7430)  class_acc: 0.2917 (0.3400)  loss_scale: 32768.0000 (56641.2292)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1206  max mem: 15572
[2025-01-16 05:12:41,691] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:12:41,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [ 790/2809]  eta: 0:19:18  lr: 0.000007  min_lr: 0.000000  loss: 3.7887 (3.7436)  class_acc: 0.3333 (0.3401)  loss_scale: 32768.0000 (56380.8445)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0196  max mem: 15572
Epoch: [31]  [ 800/2809]  eta: 0:19:11  lr: 0.000007  min_lr: 0.000000  loss: 3.8658 (3.7461)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (56495.1411)  weight_decay: 0.0500 (0.0500)  time: 0.4995  data: 0.0501  max mem: 15572
Epoch: [31]  [ 810/2809]  eta: 0:19:07  lr: 0.000007  min_lr: 0.000000  loss: 3.7828 (3.7469)  class_acc: 0.2500 (0.3388)  loss_scale: 65536.0000 (56606.6190)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1338  max mem: 15572
Epoch: [31]  [ 820/2809]  eta: 0:19:00  lr: 0.000007  min_lr: 0.000000  loss: 3.7127 (3.7460)  class_acc: 0.2917 (0.3389)  loss_scale: 65536.0000 (56715.3812)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.1542  max mem: 15572
Epoch: [31]  [ 830/2809]  eta: 0:18:52  lr: 0.000007  min_lr: 0.000000  loss: 3.8725 (3.7462)  class_acc: 0.2500 (0.3382)  loss_scale: 65536.0000 (56821.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0826  max mem: 15572
Epoch: [31]  [ 840/2809]  eta: 0:18:47  lr: 0.000007  min_lr: 0.000000  loss: 3.4757 (3.7419)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (56925.1463)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1057  max mem: 15572
Epoch: [31]  [ 850/2809]  eta: 0:18:39  lr: 0.000007  min_lr: 0.000000  loss: 3.7588 (3.7457)  class_acc: 0.2917 (0.3384)  loss_scale: 65536.0000 (57026.3314)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0882  max mem: 15572
Epoch: [31]  [ 860/2809]  eta: 0:18:35  lr: 0.000007  min_lr: 0.000000  loss: 3.9303 (3.7458)  class_acc: 0.2500 (0.3384)  loss_scale: 65536.0000 (57125.1661)  weight_decay: 0.0500 (0.0500)  time: 0.5450  data: 0.1057  max mem: 15572
Epoch: [31]  [ 870/2809]  eta: 0:18:29  lr: 0.000007  min_lr: 0.000000  loss: 3.7762 (3.7452)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (57221.7313)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1657  max mem: 15572
Epoch: [31]  [ 880/2809]  eta: 0:18:24  lr: 0.000007  min_lr: 0.000000  loss: 3.8825 (3.7479)  class_acc: 0.3333 (0.3382)  loss_scale: 65536.0000 (57316.1044)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1583  max mem: 15572
Epoch: [31]  [ 890/2809]  eta: 0:18:18  lr: 0.000007  min_lr: 0.000000  loss: 3.8825 (3.7454)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (57408.3591)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.1493  max mem: 15572
Epoch: [31]  [ 900/2809]  eta: 0:18:10  lr: 0.000007  min_lr: 0.000000  loss: 3.9005 (3.7492)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (57498.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0789  max mem: 15572
Epoch: [31]  [ 910/2809]  eta: 0:18:04  lr: 0.000007  min_lr: 0.000000  loss: 3.9818 (3.7506)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (57586.7925)  weight_decay: 0.0500 (0.0500)  time: 0.5089  data: 0.0729  max mem: 15572
[2025-01-16 05:13:51,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:13:51,963] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:13:53,673] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 87998
[2025-01-16 05:13:53,674] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:13:53,674] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 05:13:54,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=88000, skipped=587, lr=[6.578664486896344e-08, 6.578664486896344e-08, 9.398092124137635e-08, 9.398092124137635e-08, 1.3425845891625195e-07, 1.3425845891625195e-07, 1.9179779845178852e-07, 1.9179779845178852e-07, 2.7399685493112645e-07, 2.7399685493112645e-07, 3.914240784730378e-07, 3.914240784730378e-07, 5.591772549614826e-07, 5.591772549614826e-07, 7.988246499449752e-07, 7.988246499449752e-07, 1.1411780713499645e-06, 1.1411780713499645e-06, 1.6302543876428069e-06, 1.6302543876428069e-06, 2.328934839489724e-06, 2.328934839489724e-06, 3.327049770699606e-06, 3.327049770699606e-06, 4.752928243856581e-06, 4.752928243856581e-06, 6.7898974912236865e-06, 6.7898974912236865e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:13:54,174] [INFO] [timer.py:260:stop] epoch=0/micro_step=88000/global_step=88000, RunningAvgSamplesPerSec=28.571377718172222, CurrSamplesPerSec=25.84830984910073, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [ 920/2809]  eta: 0:17:59  lr: 0.000007  min_lr: 0.000000  loss: 3.8877 (3.7508)  class_acc: 0.2500 (0.3372)  loss_scale: 65536.0000 (57744.2606)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1355  max mem: 15572
Epoch: [31]  [ 930/2809]  eta: 0:17:54  lr: 0.000007  min_lr: 0.000000  loss: 3.7737 (3.7485)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (57827.9527)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1388  max mem: 15572
Epoch: [31]  [ 940/2809]  eta: 0:17:47  lr: 0.000007  min_lr: 0.000000  loss: 3.5369 (3.7465)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (57909.8661)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0957  max mem: 15572
Epoch: [31]  [ 950/2809]  eta: 0:17:39  lr: 0.000007  min_lr: 0.000000  loss: 3.7354 (3.7481)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (57990.0568)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0587  max mem: 15572
Epoch: [31]  [ 960/2809]  eta: 0:17:35  lr: 0.000007  min_lr: 0.000000  loss: 3.9122 (3.7467)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (58068.5786)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1140  max mem: 15572
[2025-01-16 05:14:17,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88041
[2025-01-16 05:14:17,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:14:17,186] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [ 970/2809]  eta: 0:17:29  lr: 0.000007  min_lr: 0.000000  loss: 3.7228 (3.7456)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (57841.7631)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1691  max mem: 15572
Epoch: [31]  [ 980/2809]  eta: 0:17:24  lr: 0.000007  min_lr: 0.000000  loss: 3.6660 (3.7437)  class_acc: 0.3750 (0.3381)  loss_scale: 32768.0000 (57586.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1488  max mem: 15572
Epoch: [31]  [ 990/2809]  eta: 0:17:17  lr: 0.000007  min_lr: 0.000000  loss: 3.7183 (3.7447)  class_acc: 0.3750 (0.3379)  loss_scale: 32768.0000 (57335.7336)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0999  max mem: 15572
Epoch: [31]  [1000/2809]  eta: 0:17:13  lr: 0.000007  min_lr: 0.000000  loss: 3.7237 (3.7426)  class_acc: 0.3750 (0.3385)  loss_scale: 32768.0000 (57090.3017)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1507  max mem: 15572
Epoch: [31]  [1010/2809]  eta: 0:17:06  lr: 0.000007  min_lr: 0.000000  loss: 3.6402 (3.7416)  class_acc: 0.4167 (0.3388)  loss_scale: 32768.0000 (56849.7250)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1410  max mem: 15572
Epoch: [31]  [1020/2809]  eta: 0:16:59  lr: 0.000007  min_lr: 0.000000  loss: 3.8212 (3.7431)  class_acc: 0.3750 (0.3387)  loss_scale: 32768.0000 (56613.8609)  weight_decay: 0.0500 (0.0500)  time: 0.4989  data: 0.0442  max mem: 15572
Epoch: [31]  [1030/2809]  eta: 0:16:54  lr: 0.000007  min_lr: 0.000000  loss: 3.8212 (3.7424)  class_acc: 0.3333 (0.3390)  loss_scale: 32768.0000 (56382.5723)  weight_decay: 0.0500 (0.0500)  time: 0.5652  data: 0.1248  max mem: 15572
Epoch: [31]  [1040/2809]  eta: 0:16:47  lr: 0.000007  min_lr: 0.000000  loss: 3.6985 (3.7418)  class_acc: 0.3750 (0.3393)  loss_scale: 32768.0000 (56155.7272)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1392  max mem: 15572
Epoch: [31]  [1050/2809]  eta: 0:16:42  lr: 0.000007  min_lr: 0.000000  loss: 3.6036 (3.7402)  class_acc: 0.3333 (0.3393)  loss_scale: 32768.0000 (55933.1989)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1030  max mem: 15572
Epoch: [31]  [1060/2809]  eta: 0:16:35  lr: 0.000007  min_lr: 0.000000  loss: 3.5024 (3.7398)  class_acc: 0.3333 (0.3389)  loss_scale: 32768.0000 (55714.8652)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0822  max mem: 15572
Epoch: [31]  [1070/2809]  eta: 0:16:29  lr: 0.000007  min_lr: 0.000000  loss: 3.6171 (3.7403)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (55500.6088)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0720  max mem: 15572
Epoch: [31]  [1080/2809]  eta: 0:16:23  lr: 0.000007  min_lr: 0.000000  loss: 3.6231 (3.7399)  class_acc: 0.3333 (0.3393)  loss_scale: 32768.0000 (55290.3164)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1392  max mem: 15572
Epoch: [31]  [1090/2809]  eta: 0:16:18  lr: 0.000007  min_lr: 0.000000  loss: 3.7885 (3.7399)  class_acc: 0.3333 (0.3394)  loss_scale: 32768.0000 (55083.8790)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1471  max mem: 15572
[2025-01-16 05:15:29,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:15:29,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1100/2809]  eta: 0:16:12  lr: 0.000007  min_lr: 0.000000  loss: 3.8425 (3.7390)  class_acc: 0.3750 (0.3395)  loss_scale: 32768.0000 (55178.8120)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1033  max mem: 15572
Epoch: [31]  [1110/2809]  eta: 0:16:07  lr: 0.000007  min_lr: 0.000000  loss: 3.8845 (3.7419)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (55272.0360)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1073  max mem: 15572
[2025-01-16 05:15:42,010] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88193
[2025-01-16 05:15:42,011] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:15:42,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1120/2809]  eta: 0:16:01  lr: 0.000007  min_lr: 0.000000  loss: 4.0544 (3.7436)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (55158.9795)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1193  max mem: 15572
Epoch: [31]  [1130/2809]  eta: 0:15:54  lr: 0.000007  min_lr: 0.000000  loss: 3.6674 (3.7410)  class_acc: 0.3333 (0.3394)  loss_scale: 32768.0000 (54961.0044)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0772  max mem: 15572
Epoch: [31]  [1140/2809]  eta: 0:15:48  lr: 0.000007  min_lr: 0.000000  loss: 3.5653 (3.7405)  class_acc: 0.3333 (0.3394)  loss_scale: 32768.0000 (54766.4996)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0963  max mem: 15572
Epoch: [31]  [1150/2809]  eta: 0:15:42  lr: 0.000007  min_lr: 0.000000  loss: 3.5653 (3.7377)  class_acc: 0.3333 (0.3398)  loss_scale: 32768.0000 (54575.3745)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1274  max mem: 15572
Epoch: [31]  [1160/2809]  eta: 0:15:37  lr: 0.000007  min_lr: 0.000000  loss: 3.5687 (3.7383)  class_acc: 0.3333 (0.3398)  loss_scale: 32768.0000 (54387.5418)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1446  max mem: 15572
Epoch: [31]  [1170/2809]  eta: 0:15:31  lr: 0.000007  min_lr: 0.000000  loss: 3.5687 (3.7373)  class_acc: 0.3750 (0.3401)  loss_scale: 32768.0000 (54202.9172)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1310  max mem: 15572
Epoch: [31]  [1180/2809]  eta: 0:15:25  lr: 0.000007  min_lr: 0.000000  loss: 3.5109 (3.7364)  class_acc: 0.3750 (0.3403)  loss_scale: 32768.0000 (54021.4191)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.1034  max mem: 15572
Epoch: [31]  [1190/2809]  eta: 0:15:19  lr: 0.000007  min_lr: 0.000000  loss: 3.8008 (3.7380)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (53842.9689)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.1128  max mem: 15572
Epoch: [31]  [1200/2809]  eta: 0:15:13  lr: 0.000007  min_lr: 0.000000  loss: 3.7422 (3.7370)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (53667.4904)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0993  max mem: 15572
Epoch: [31]  [1210/2809]  eta: 0:15:07  lr: 0.000007  min_lr: 0.000000  loss: 3.8339 (3.7385)  class_acc: 0.2917 (0.3400)  loss_scale: 32768.0000 (53494.9100)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1138  max mem: 15572
Epoch: [31]  [1220/2809]  eta: 0:15:02  lr: 0.000007  min_lr: 0.000000  loss: 3.8047 (3.7367)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (53325.1564)  weight_decay: 0.0500 (0.0500)  time: 0.5776  data: 0.1280  max mem: 15572
Epoch: [31]  [1230/2809]  eta: 0:14:55  lr: 0.000007  min_lr: 0.000000  loss: 3.7834 (3.7367)  class_acc: 0.3750 (0.3412)  loss_scale: 32768.0000 (53158.1608)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0845  max mem: 15572
Epoch: [31]  [1240/2809]  eta: 0:14:52  lr: 0.000007  min_lr: 0.000000  loss: 3.7834 (3.7361)  class_acc: 0.3333 (0.3413)  loss_scale: 32768.0000 (52993.8566)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1763  max mem: 15572
[2025-01-16 05:16:55,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:16:55,240] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1250/2809]  eta: 0:14:46  lr: 0.000007  min_lr: 0.000000  loss: 3.5677 (3.7348)  class_acc: 0.3750 (0.3413)  loss_scale: 32768.0000 (53041.7266)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1772  max mem: 15572
[2025-01-16 05:16:59,446] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88330
[2025-01-16 05:16:59,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:16:59,447] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1260/2809]  eta: 0:14:38  lr: 0.000007  min_lr: 0.000000  loss: 3.5112 (3.7325)  class_acc: 0.3750 (0.3417)  loss_scale: 32768.0000 (52880.9516)  weight_decay: 0.0500 (0.0500)  time: 0.4804  data: 0.0397  max mem: 15572
Epoch: [31]  [1270/2809]  eta: 0:14:33  lr: 0.000007  min_lr: 0.000000  loss: 3.5944 (3.7329)  class_acc: 0.3750 (0.3417)  loss_scale: 32768.0000 (52722.7065)  weight_decay: 0.0500 (0.0500)  time: 0.5215  data: 0.0950  max mem: 15572
Epoch: [31]  [1280/2809]  eta: 0:14:27  lr: 0.000007  min_lr: 0.000000  loss: 3.8404 (3.7342)  class_acc: 0.3333 (0.3412)  loss_scale: 32768.0000 (52566.9321)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1217  max mem: 15572
Epoch: [31]  [1290/2809]  eta: 0:14:21  lr: 0.000007  min_lr: 0.000000  loss: 3.8663 (3.7346)  class_acc: 0.2917 (0.3410)  loss_scale: 32768.0000 (52413.5709)  weight_decay: 0.0500 (0.0500)  time: 0.5204  data: 0.0877  max mem: 15572
Epoch: [31]  [1300/2809]  eta: 0:14:15  lr: 0.000007  min_lr: 0.000000  loss: 3.8479 (3.7354)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (52262.5673)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1285  max mem: 15572
Epoch: [31]  [1310/2809]  eta: 0:14:10  lr: 0.000007  min_lr: 0.000000  loss: 3.8479 (3.7367)  class_acc: 0.2917 (0.3405)  loss_scale: 32768.0000 (52113.8673)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.1496  max mem: 15572
Epoch: [31]  [1320/2809]  eta: 0:14:05  lr: 0.000007  min_lr: 0.000000  loss: 3.7845 (3.7377)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (51967.4186)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1159  max mem: 15572
Epoch: [31]  [1330/2809]  eta: 0:13:58  lr: 0.000007  min_lr: 0.000000  loss: 3.7783 (3.7380)  class_acc: 0.2917 (0.3401)  loss_scale: 32768.0000 (51823.1705)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0622  max mem: 15572
Epoch: [31]  [1340/2809]  eta: 0:13:52  lr: 0.000007  min_lr: 0.000000  loss: 3.7678 (3.7371)  class_acc: 0.2500 (0.3401)  loss_scale: 32768.0000 (51681.0738)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0692  max mem: 15572
Epoch: [31]  [1350/2809]  eta: 0:13:46  lr: 0.000007  min_lr: 0.000000  loss: 3.8289 (3.7383)  class_acc: 0.2500 (0.3399)  loss_scale: 32768.0000 (51541.0807)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1083  max mem: 15572
Epoch: [31]  [1360/2809]  eta: 0:13:42  lr: 0.000007  min_lr: 0.000000  loss: 3.8289 (3.7385)  class_acc: 0.2917 (0.3400)  loss_scale: 32768.0000 (51403.1447)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.1566  max mem: 15572
Epoch: [31]  [1370/2809]  eta: 0:13:36  lr: 0.000007  min_lr: 0.000000  loss: 3.7623 (3.7387)  class_acc: 0.3750 (0.3403)  loss_scale: 32768.0000 (51267.2210)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.1668  max mem: 15572
[2025-01-16 05:18:11,654] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:18:11,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [1380/2809]  eta: 0:13:30  lr: 0.000007  min_lr: 0.000000  loss: 3.6085 (3.7378)  class_acc: 0.3750 (0.3406)  loss_scale: 32768.0000 (51156.9935)  weight_decay: 0.0500 (0.0500)  time: 0.5587  data: 0.1269  max mem: 15572
Epoch: [31]  [1390/2809]  eta: 0:13:25  lr: 0.000007  min_lr: 0.000000  loss: 3.4249 (3.7366)  class_acc: 0.4167 (0.3409)  loss_scale: 65536.0000 (51260.3652)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1100  max mem: 15572
Epoch: [31]  [1400/2809]  eta: 0:13:18  lr: 0.000007  min_lr: 0.000000  loss: 3.7208 (3.7374)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (51362.2612)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0801  max mem: 15572
Epoch: [31]  [1410/2809]  eta: 0:13:12  lr: 0.000007  min_lr: 0.000000  loss: 3.7529 (3.7368)  class_acc: 0.2917 (0.3406)  loss_scale: 65536.0000 (51462.7130)  weight_decay: 0.0500 (0.0500)  time: 0.4931  data: 0.0435  max mem: 15572
Epoch: [31]  [1420/2809]  eta: 0:13:06  lr: 0.000007  min_lr: 0.000000  loss: 3.7291 (3.7367)  class_acc: 0.2917 (0.3406)  loss_scale: 65536.0000 (51561.7509)  weight_decay: 0.0500 (0.0500)  time: 0.5375  data: 0.1084  max mem: 15572
Epoch: [31]  [1430/2809]  eta: 0:13:01  lr: 0.000007  min_lr: 0.000000  loss: 3.8625 (3.7383)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (51659.4046)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.1935  max mem: 15572
Epoch: [31]  [1440/2809]  eta: 0:12:56  lr: 0.000007  min_lr: 0.000000  loss: 3.7761 (3.7372)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (51755.7030)  weight_decay: 0.0500 (0.0500)  time: 0.5981  data: 0.1681  max mem: 15572
Epoch: [31]  [1450/2809]  eta: 0:12:50  lr: 0.000007  min_lr: 0.000000  loss: 3.7761 (3.7388)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (51850.6740)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1398  max mem: 15572
Epoch: [31]  [1460/2809]  eta: 0:12:45  lr: 0.000007  min_lr: 0.000000  loss: 4.0578 (3.7396)  class_acc: 0.2917 (0.3394)  loss_scale: 65536.0000 (51944.3450)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.1913  max mem: 15572
Epoch: [31]  [1470/2809]  eta: 0:12:39  lr: 0.000007  min_lr: 0.000000  loss: 3.7685 (3.7390)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (52036.7424)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1765  max mem: 15572
Epoch: [31]  [1480/2809]  eta: 0:12:33  lr: 0.000006  min_lr: 0.000000  loss: 3.7685 (3.7407)  class_acc: 0.2917 (0.3390)  loss_scale: 65536.0000 (52127.8920)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0679  max mem: 15572
Epoch: [31]  [1490/2809]  eta: 0:12:27  lr: 0.000006  min_lr: 0.000000  loss: 3.8672 (3.7402)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (52217.8189)  weight_decay: 0.0500 (0.0500)  time: 0.4860  data: 0.0306  max mem: 15572
Epoch: [31]  [1500/2809]  eta: 0:12:22  lr: 0.000006  min_lr: 0.000000  loss: 3.7482 (3.7396)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (52306.5476)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1102  max mem: 15572
[2025-01-16 05:19:23,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:19:23,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:19:23,931] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88588
[2025-01-16 05:19:23,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:19:23,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1510/2809]  eta: 0:12:16  lr: 0.000006  min_lr: 0.000000  loss: 3.8435 (3.7402)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (52437.4745)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1454  max mem: 15572
Epoch: [31]  [1520/2809]  eta: 0:12:10  lr: 0.000006  min_lr: 0.000000  loss: 3.9041 (3.7411)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (52523.5924)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.1105  max mem: 15572
Epoch: [31]  [1530/2809]  eta: 0:12:04  lr: 0.000006  min_lr: 0.000000  loss: 3.9134 (3.7412)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (52608.5852)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1204  max mem: 15572
Epoch: [31]  [1540/2809]  eta: 0:11:59  lr: 0.000006  min_lr: 0.000000  loss: 3.9609 (3.7430)  class_acc: 0.2500 (0.3383)  loss_scale: 65536.0000 (52692.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1152  max mem: 15572
Epoch: [31]  [1550/2809]  eta: 0:11:53  lr: 0.000006  min_lr: 0.000000  loss: 3.9231 (3.7430)  class_acc: 0.2500 (0.3385)  loss_scale: 65536.0000 (52775.2830)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1067  max mem: 15572
Epoch: [31]  [1560/2809]  eta: 0:11:48  lr: 0.000006  min_lr: 0.000000  loss: 3.8332 (3.7431)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (52857.0301)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1485  max mem: 15572
Epoch: [31]  [1570/2809]  eta: 0:11:42  lr: 0.000006  min_lr: 0.000000  loss: 3.6185 (3.7427)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (52937.7365)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1981  max mem: 15572
Epoch: [31]  [1580/2809]  eta: 0:11:37  lr: 0.000006  min_lr: 0.000000  loss: 3.6828 (3.7434)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53017.4219)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1688  max mem: 15572
Epoch: [31]  [1590/2809]  eta: 0:11:31  lr: 0.000006  min_lr: 0.000000  loss: 3.8665 (3.7434)  class_acc: 0.2500 (0.3382)  loss_scale: 65536.0000 (53096.1056)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1398  max mem: 15572
Epoch: [31]  [1600/2809]  eta: 0:11:25  lr: 0.000006  min_lr: 0.000000  loss: 3.8138 (3.7438)  class_acc: 0.2500 (0.3381)  loss_scale: 65536.0000 (53173.8064)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1073  max mem: 15572
Epoch: [31]  [1610/2809]  eta: 0:11:19  lr: 0.000006  min_lr: 0.000000  loss: 3.9324 (3.7437)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (53250.5425)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0700  max mem: 15572
Epoch: [31]  [1620/2809]  eta: 0:11:13  lr: 0.000006  min_lr: 0.000000  loss: 3.7371 (3.7437)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (53326.3319)  weight_decay: 0.0500 (0.0500)  time: 0.4897  data: 0.0431  max mem: 15572
Epoch: [31]  [1630/2809]  eta: 0:11:09  lr: 0.000006  min_lr: 0.000000  loss: 3.7371 (3.7439)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (53401.1919)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1667  max mem: 15572
[2025-01-16 05:20:37,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:20:37,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:20:38,297] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88718
[2025-01-16 05:20:38,298] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:20:38,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1640/2809]  eta: 0:11:03  lr: 0.000006  min_lr: 0.000000  loss: 3.9789 (3.7439)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (53515.0762)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1902  max mem: 15572
Epoch: [31]  [1650/2809]  eta: 0:10:57  lr: 0.000006  min_lr: 0.000000  loss: 4.0219 (3.7453)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (53587.8861)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1274  max mem: 15572
Epoch: [31]  [1660/2809]  eta: 0:10:52  lr: 0.000006  min_lr: 0.000000  loss: 3.8099 (3.7436)  class_acc: 0.3750 (0.3384)  loss_scale: 65536.0000 (53659.8194)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1523  max mem: 15572
Epoch: [31]  [1670/2809]  eta: 0:10:45  lr: 0.000006  min_lr: 0.000000  loss: 3.6464 (3.7433)  class_acc: 0.3750 (0.3383)  loss_scale: 65536.0000 (53730.8917)  weight_decay: 0.0500 (0.0500)  time: 0.5262  data: 0.0903  max mem: 15572
Epoch: [31]  [1680/2809]  eta: 0:10:40  lr: 0.000006  min_lr: 0.000000  loss: 3.7492 (3.7432)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (53801.1184)  weight_decay: 0.0500 (0.0500)  time: 0.5500  data: 0.1065  max mem: 15572
Epoch: [31]  [1690/2809]  eta: 0:10:35  lr: 0.000006  min_lr: 0.000000  loss: 3.7831 (3.7433)  class_acc: 0.4167 (0.3387)  loss_scale: 65536.0000 (53870.5145)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.1745  max mem: 15572
Epoch: [31]  [1700/2809]  eta: 0:10:29  lr: 0.000006  min_lr: 0.000000  loss: 3.6809 (3.7434)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (53939.0947)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1278  max mem: 15572
Epoch: [31]  [1710/2809]  eta: 0:10:23  lr: 0.000006  min_lr: 0.000000  loss: 3.5829 (3.7422)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (54006.8732)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0911  max mem: 15572
Epoch: [31]  [1720/2809]  eta: 0:10:18  lr: 0.000006  min_lr: 0.000000  loss: 3.8379 (3.7425)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (54073.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1293  max mem: 15572
Epoch: [31]  [1730/2809]  eta: 0:10:12  lr: 0.000006  min_lr: 0.000000  loss: 3.6626 (3.7420)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (54140.0809)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1105  max mem: 15572
Epoch: [31]  [1740/2809]  eta: 0:10:06  lr: 0.000006  min_lr: 0.000000  loss: 3.3598 (3.7409)  class_acc: 0.3750 (0.3390)  loss_scale: 65536.0000 (54205.5370)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0758  max mem: 15572
Epoch: [31]  [1750/2809]  eta: 0:10:00  lr: 0.000006  min_lr: 0.000000  loss: 3.7048 (3.7409)  class_acc: 0.3750 (0.3393)  loss_scale: 65536.0000 (54270.2456)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0930  max mem: 15572
Epoch: [31]  [1760/2809]  eta: 0:09:54  lr: 0.000006  min_lr: 0.000000  loss: 3.8332 (3.7422)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (54334.2192)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1198  max mem: 15572
[2025-01-16 05:21:50,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:21:50,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:21:50,535] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88848
[2025-01-16 05:21:50,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:21:50,536] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1770/2809]  eta: 0:09:48  lr: 0.000006  min_lr: 0.000000  loss: 3.6233 (3.7404)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (54434.4754)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0953  max mem: 15572
Epoch: [31]  [1780/2809]  eta: 0:09:43  lr: 0.000006  min_lr: 0.000000  loss: 3.7277 (3.7413)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (54496.8085)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1255  max mem: 15572
Epoch: [31]  [1790/2809]  eta: 0:09:38  lr: 0.000006  min_lr: 0.000000  loss: 3.8789 (3.7421)  class_acc: 0.2917 (0.3388)  loss_scale: 65536.0000 (54558.4456)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.2047  max mem: 15572
Epoch: [31]  [1800/2809]  eta: 0:09:32  lr: 0.000006  min_lr: 0.000000  loss: 3.8709 (3.7432)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (54619.3981)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1819  max mem: 15572
Epoch: [31]  [1810/2809]  eta: 0:09:26  lr: 0.000006  min_lr: 0.000000  loss: 3.8094 (3.7434)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (54679.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5300  data: 0.0791  max mem: 15572
Epoch: [31]  [1820/2809]  eta: 0:09:21  lr: 0.000006  min_lr: 0.000000  loss: 3.5723 (3.7426)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (54739.2949)  weight_decay: 0.0500 (0.0500)  time: 0.5646  data: 0.1085  max mem: 15572
Epoch: [31]  [1830/2809]  eta: 0:09:15  lr: 0.000006  min_lr: 0.000000  loss: 3.5049 (3.7419)  class_acc: 0.3750 (0.3394)  loss_scale: 65536.0000 (54798.2611)  weight_decay: 0.0500 (0.0500)  time: 0.6139  data: 0.1701  max mem: 15572
Epoch: [31]  [1840/2809]  eta: 0:09:09  lr: 0.000006  min_lr: 0.000000  loss: 3.5692 (3.7413)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (54856.5866)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1277  max mem: 15572
Epoch: [31]  [1850/2809]  eta: 0:09:04  lr: 0.000006  min_lr: 0.000000  loss: 3.9196 (3.7421)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (54914.2820)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1076  max mem: 15572
Epoch: [31]  [1860/2809]  eta: 0:08:58  lr: 0.000006  min_lr: 0.000000  loss: 3.8681 (3.7420)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (54971.3573)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0890  max mem: 15572
Epoch: [31]  [1870/2809]  eta: 0:08:53  lr: 0.000006  min_lr: 0.000000  loss: 3.7057 (3.7410)  class_acc: 0.3333 (0.3397)  loss_scale: 65536.0000 (55027.8226)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1591  max mem: 15572
Epoch: [31]  [1880/2809]  eta: 0:08:47  lr: 0.000006  min_lr: 0.000000  loss: 3.6430 (3.7407)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (55083.6874)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.2120  max mem: 15572
Epoch: [31]  [1890/2809]  eta: 0:08:41  lr: 0.000006  min_lr: 0.000000  loss: 3.7818 (3.7409)  class_acc: 0.2500 (0.3394)  loss_scale: 65536.0000 (55138.9614)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1277  max mem: 15572
[2025-01-16 05:23:05,624] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:23:05,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [1900/2809]  eta: 0:08:35  lr: 0.000006  min_lr: 0.000000  loss: 3.7818 (3.7408)  class_acc: 0.2917 (0.3394)  loss_scale: 65536.0000 (55297.0773)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0753  max mem: 15572
[2025-01-16 05:23:07,636] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 88980
[2025-01-16 05:23:07,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:23:07,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [1910/2809]  eta: 0:08:30  lr: 0.000006  min_lr: 0.000000  loss: 3.9088 (3.7405)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (55350.6562)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0876  max mem: 15572
[2025-01-16 05:23:18,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=89000, skipped=594, lr=[6.077990261635452e-08, 6.077990261635452e-08, 8.68284323090779e-08, 8.68284323090779e-08, 1.24040617584397e-07, 1.24040617584397e-07, 1.772008822634243e-07, 1.772008822634243e-07, 2.531441175191776e-07, 2.531441175191776e-07, 3.6163445359882513e-07, 3.6163445359882513e-07, 5.166206479983216e-07, 5.166206479983216e-07, 7.380294971404596e-07, 7.380294971404596e-07, 1.0543278530577993e-06, 1.0543278530577993e-06, 1.506182647225428e-06, 1.506182647225428e-06, 2.1516894960363255e-06, 2.1516894960363255e-06, 3.073842137194751e-06, 3.073842137194751e-06, 4.391203053135359e-06, 4.391203053135359e-06, 6.273147218764799e-06, 6.273147218764799e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:23:18,157] [INFO] [timer.py:260:stop] epoch=0/micro_step=89000/global_step=89000, RunningAvgSamplesPerSec=28.57246760081671, CurrSamplesPerSec=27.62674541467578, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [31]  [1920/2809]  eta: 0:08:24  lr: 0.000006  min_lr: 0.000000  loss: 3.9088 (3.7413)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (55403.6773)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0967  max mem: 15572
Epoch: [31]  [1930/2809]  eta: 0:08:18  lr: 0.000006  min_lr: 0.000000  loss: 3.9199 (3.7417)  class_acc: 0.2917 (0.3392)  loss_scale: 65536.0000 (55456.1491)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0899  max mem: 15572
Epoch: [31]  [1940/2809]  eta: 0:08:12  lr: 0.000006  min_lr: 0.000000  loss: 3.6999 (3.7408)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (55508.0804)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0964  max mem: 15572
Epoch: [31]  [1950/2809]  eta: 0:08:07  lr: 0.000006  min_lr: 0.000000  loss: 3.6452 (3.7407)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (55559.4792)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1382  max mem: 15572
Epoch: [31]  [1960/2809]  eta: 0:08:01  lr: 0.000006  min_lr: 0.000000  loss: 3.8319 (3.7412)  class_acc: 0.2500 (0.3389)  loss_scale: 65536.0000 (55610.3539)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1170  max mem: 15572
Epoch: [31]  [1970/2809]  eta: 0:07:55  lr: 0.000006  min_lr: 0.000000  loss: 3.8763 (3.7406)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (55660.7123)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0573  max mem: 15572
Epoch: [31]  [1980/2809]  eta: 0:07:49  lr: 0.000006  min_lr: 0.000000  loss: 3.9314 (3.7414)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (55710.5623)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.1134  max mem: 15572
[2025-01-16 05:23:53,893] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89064
[2025-01-16 05:23:53,893] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:23:53,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [1990/2809]  eta: 0:07:43  lr: 0.000006  min_lr: 0.000000  loss: 3.8594 (3.7407)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (55661.1632)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1146  max mem: 15572
Epoch: [31]  [2000/2809]  eta: 0:07:38  lr: 0.000006  min_lr: 0.000000  loss: 3.7552 (3.7404)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (55546.7546)  weight_decay: 0.0500 (0.0500)  time: 0.5347  data: 0.0870  max mem: 15572
Epoch: [31]  [2010/2809]  eta: 0:07:32  lr: 0.000006  min_lr: 0.000000  loss: 3.6100 (3.7392)  class_acc: 0.2917 (0.3391)  loss_scale: 32768.0000 (55433.4838)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1322  max mem: 15572
Epoch: [31]  [2020/2809]  eta: 0:07:27  lr: 0.000006  min_lr: 0.000000  loss: 3.4793 (3.7382)  class_acc: 0.3333 (0.3392)  loss_scale: 32768.0000 (55321.3340)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1349  max mem: 15572
Epoch: [31]  [2030/2809]  eta: 0:07:21  lr: 0.000006  min_lr: 0.000000  loss: 3.4952 (3.7379)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (55210.2885)  weight_decay: 0.0500 (0.0500)  time: 0.5788  data: 0.1249  max mem: 15572
Epoch: [31]  [2040/2809]  eta: 0:07:15  lr: 0.000006  min_lr: 0.000000  loss: 3.5917 (3.7374)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (55100.3312)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1051  max mem: 15572
Epoch: [31]  [2050/2809]  eta: 0:07:10  lr: 0.000006  min_lr: 0.000000  loss: 3.7667 (3.7383)  class_acc: 0.3333 (0.3389)  loss_scale: 32768.0000 (54991.4461)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0822  max mem: 15572
Epoch: [31]  [2060/2809]  eta: 0:07:04  lr: 0.000006  min_lr: 0.000000  loss: 3.8130 (3.7382)  class_acc: 0.3333 (0.3389)  loss_scale: 32768.0000 (54883.6177)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0842  max mem: 15572
Epoch: [31]  [2070/2809]  eta: 0:06:58  lr: 0.000006  min_lr: 0.000000  loss: 3.7365 (3.7390)  class_acc: 0.3333 (0.3387)  loss_scale: 32768.0000 (54776.8305)  weight_decay: 0.0500 (0.0500)  time: 0.5634  data: 0.1300  max mem: 15572
Epoch: [31]  [2080/2809]  eta: 0:06:53  lr: 0.000006  min_lr: 0.000000  loss: 3.6938 (3.7387)  class_acc: 0.3333 (0.3387)  loss_scale: 32768.0000 (54671.0697)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1922  max mem: 15572
Epoch: [31]  [2090/2809]  eta: 0:06:47  lr: 0.000006  min_lr: 0.000000  loss: 3.3890 (3.7371)  class_acc: 0.3750 (0.3388)  loss_scale: 32768.0000 (54566.3204)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1377  max mem: 15572
Epoch: [31]  [2100/2809]  eta: 0:06:41  lr: 0.000006  min_lr: 0.000000  loss: 3.4988 (3.7365)  class_acc: 0.3750 (0.3391)  loss_scale: 32768.0000 (54462.5683)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.1194  max mem: 15572
Epoch: [31]  [2110/2809]  eta: 0:06:35  lr: 0.000006  min_lr: 0.000000  loss: 3.6058 (3.7363)  class_acc: 0.3750 (0.3391)  loss_scale: 32768.0000 (54359.7991)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1047  max mem: 15572
[2025-01-16 05:25:06,769] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:25:06,769] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2120/2809]  eta: 0:06:30  lr: 0.000006  min_lr: 0.000000  loss: 3.6073 (3.7360)  class_acc: 0.2917 (0.3391)  loss_scale: 32768.0000 (54366.1443)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1079  max mem: 15572
Epoch: [31]  [2130/2809]  eta: 0:06:24  lr: 0.000006  min_lr: 0.000000  loss: 3.6137 (3.7362)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (54418.5603)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1462  max mem: 15572
Epoch: [31]  [2140/2809]  eta: 0:06:19  lr: 0.000006  min_lr: 0.000000  loss: 3.6137 (3.7359)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (54470.4867)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1180  max mem: 15572
Epoch: [31]  [2150/2809]  eta: 0:06:13  lr: 0.000006  min_lr: 0.000000  loss: 3.6855 (3.7359)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (54521.9303)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.0933  max mem: 15572
Epoch: [31]  [2160/2809]  eta: 0:06:07  lr: 0.000006  min_lr: 0.000000  loss: 3.6855 (3.7351)  class_acc: 0.2500 (0.3392)  loss_scale: 65536.0000 (54572.8977)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0813  max mem: 15572
[2025-01-16 05:25:38,248] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89248
[2025-01-16 05:25:38,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:25:38,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2170/2809]  eta: 0:06:02  lr: 0.000006  min_lr: 0.000000  loss: 3.5479 (3.7340)  class_acc: 0.2917 (0.3395)  loss_scale: 65536.0000 (54593.2087)  weight_decay: 0.0500 (0.0500)  time: 0.5752  data: 0.1443  max mem: 15572
Epoch: [31]  [2180/2809]  eta: 0:05:56  lr: 0.000006  min_lr: 0.000000  loss: 3.6273 (3.7345)  class_acc: 0.2917 (0.3392)  loss_scale: 32768.0000 (54493.1389)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.1498  max mem: 15572
Epoch: [31]  [2190/2809]  eta: 0:05:50  lr: 0.000006  min_lr: 0.000000  loss: 3.9708 (3.7351)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (54393.9827)  weight_decay: 0.0500 (0.0500)  time: 0.5804  data: 0.1333  max mem: 15572
Epoch: [31]  [2200/2809]  eta: 0:05:45  lr: 0.000006  min_lr: 0.000000  loss: 3.9714 (3.7359)  class_acc: 0.2917 (0.3388)  loss_scale: 32768.0000 (54295.7274)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1487  max mem: 15572
Epoch: [31]  [2210/2809]  eta: 0:05:39  lr: 0.000006  min_lr: 0.000000  loss: 3.9875 (3.7362)  class_acc: 0.2917 (0.3388)  loss_scale: 32768.0000 (54198.3609)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1754  max mem: 15572
Epoch: [31]  [2220/2809]  eta: 0:05:34  lr: 0.000006  min_lr: 0.000000  loss: 4.0185 (3.7374)  class_acc: 0.2500 (0.3384)  loss_scale: 32768.0000 (54101.8712)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1512  max mem: 15572
Epoch: [31]  [2230/2809]  eta: 0:05:28  lr: 0.000006  min_lr: 0.000000  loss: 3.8591 (3.7376)  class_acc: 0.2917 (0.3384)  loss_scale: 32768.0000 (54006.2465)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1480  max mem: 15572
Epoch: [31]  [2240/2809]  eta: 0:05:22  lr: 0.000006  min_lr: 0.000000  loss: 3.7137 (3.7375)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (53911.4752)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1261  max mem: 15572
Epoch: [31]  [2250/2809]  eta: 0:05:16  lr: 0.000006  min_lr: 0.000000  loss: 3.7025 (3.7374)  class_acc: 0.3750 (0.3385)  loss_scale: 32768.0000 (53817.5460)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0770  max mem: 15572
Epoch: [31]  [2260/2809]  eta: 0:05:11  lr: 0.000006  min_lr: 0.000000  loss: 3.7038 (3.7376)  class_acc: 0.3333 (0.3385)  loss_scale: 32768.0000 (53724.4476)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.1674  max mem: 15572
Epoch: [31]  [2270/2809]  eta: 0:05:05  lr: 0.000006  min_lr: 0.000000  loss: 3.7038 (3.7372)  class_acc: 0.3333 (0.3385)  loss_scale: 32768.0000 (53632.1691)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1325  max mem: 15572
Epoch: [31]  [2280/2809]  eta: 0:04:59  lr: 0.000006  min_lr: 0.000000  loss: 3.8109 (3.7377)  class_acc: 0.2500 (0.3381)  loss_scale: 32768.0000 (53540.6997)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0805  max mem: 15572
Epoch: [31]  [2290/2809]  eta: 0:04:54  lr: 0.000006  min_lr: 0.000000  loss: 3.8647 (3.7378)  class_acc: 0.2500 (0.3381)  loss_scale: 32768.0000 (53450.0288)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1415  max mem: 15572
[2025-01-16 05:26:53,675] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:26:53,675] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2300/2809]  eta: 0:04:48  lr: 0.000006  min_lr: 0.000000  loss: 3.6878 (3.7371)  class_acc: 0.3333 (0.3380)  loss_scale: 32768.0000 (53402.8683)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1996  max mem: 15572
Epoch: [31]  [2310/2809]  eta: 0:04:43  lr: 0.000006  min_lr: 0.000000  loss: 3.7871 (3.7372)  class_acc: 0.2917 (0.3379)  loss_scale: 65536.0000 (53455.3700)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1245  max mem: 15572
Epoch: [31]  [2320/2809]  eta: 0:04:37  lr: 0.000006  min_lr: 0.000000  loss: 3.6227 (3.7365)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (53507.4192)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0880  max mem: 15572
Epoch: [31]  [2330/2809]  eta: 0:04:31  lr: 0.000006  min_lr: 0.000000  loss: 3.6227 (3.7367)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (53559.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1477  max mem: 15572
Epoch: [31]  [2340/2809]  eta: 0:04:26  lr: 0.000006  min_lr: 0.000000  loss: 3.9885 (3.7371)  class_acc: 0.2917 (0.3379)  loss_scale: 65536.0000 (53610.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1049  max mem: 15572
Epoch: [31]  [2350/2809]  eta: 0:04:20  lr: 0.000006  min_lr: 0.000000  loss: 4.0542 (3.7384)  class_acc: 0.2917 (0.3376)  loss_scale: 65536.0000 (53660.9103)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0809  max mem: 15572
Epoch: [31]  [2360/2809]  eta: 0:04:14  lr: 0.000006  min_lr: 0.000000  loss: 3.8764 (3.7385)  class_acc: 0.2917 (0.3377)  loss_scale: 65536.0000 (53711.2071)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0949  max mem: 15572
Epoch: [31]  [2370/2809]  eta: 0:04:09  lr: 0.000006  min_lr: 0.000000  loss: 3.8944 (3.7392)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (53761.0797)  weight_decay: 0.0500 (0.0500)  time: 0.5855  data: 0.1465  max mem: 15572
Epoch: [31]  [2380/2809]  eta: 0:04:03  lr: 0.000006  min_lr: 0.000000  loss: 3.9095 (3.7397)  class_acc: 0.2500 (0.3373)  loss_scale: 65536.0000 (53810.5334)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.1797  max mem: 15572
Epoch: [31]  [2390/2809]  eta: 0:03:57  lr: 0.000006  min_lr: 0.000000  loss: 3.8026 (3.7392)  class_acc: 0.3333 (0.3375)  loss_scale: 65536.0000 (53859.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1346  max mem: 15572
[2025-01-16 05:27:48,805] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89473
[2025-01-16 05:27:48,805] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:27:48,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [2400/2809]  eta: 0:03:52  lr: 0.000006  min_lr: 0.000000  loss: 3.8096 (3.7402)  class_acc: 0.3333 (0.3373)  loss_scale: 65536.0000 (53812.6714)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1423  max mem: 15572
Epoch: [31]  [2410/2809]  eta: 0:03:46  lr: 0.000006  min_lr: 0.000000  loss: 3.8792 (3.7399)  class_acc: 0.2917 (0.3372)  loss_scale: 32768.0000 (53725.3853)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1071  max mem: 15572
Epoch: [31]  [2420/2809]  eta: 0:03:40  lr: 0.000006  min_lr: 0.000000  loss: 3.8595 (3.7402)  class_acc: 0.2500 (0.3371)  loss_scale: 32768.0000 (53638.8203)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.0649  max mem: 15572
Epoch: [31]  [2430/2809]  eta: 0:03:35  lr: 0.000006  min_lr: 0.000000  loss: 3.8951 (3.7409)  class_acc: 0.2500 (0.3369)  loss_scale: 32768.0000 (53552.9675)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1113  max mem: 15572
Epoch: [31]  [2440/2809]  eta: 0:03:29  lr: 0.000006  min_lr: 0.000000  loss: 3.7031 (3.7406)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (53467.8181)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1199  max mem: 15572
Epoch: [31]  [2450/2809]  eta: 0:03:23  lr: 0.000006  min_lr: 0.000000  loss: 3.6118 (3.7407)  class_acc: 0.3750 (0.3371)  loss_scale: 32768.0000 (53383.3635)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0737  max mem: 15572
Epoch: [31]  [2460/2809]  eta: 0:03:18  lr: 0.000006  min_lr: 0.000000  loss: 3.6754 (3.7400)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (53299.5953)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1046  max mem: 15572
Epoch: [31]  [2470/2809]  eta: 0:03:12  lr: 0.000006  min_lr: 0.000000  loss: 3.6754 (3.7402)  class_acc: 0.2917 (0.3370)  loss_scale: 32768.0000 (53216.5051)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1290  max mem: 15572
Epoch: [31]  [2480/2809]  eta: 0:03:06  lr: 0.000006  min_lr: 0.000000  loss: 3.9604 (3.7412)  class_acc: 0.2917 (0.3368)  loss_scale: 32768.0000 (53134.0846)  weight_decay: 0.0500 (0.0500)  time: 0.5250  data: 0.0856  max mem: 15572
Epoch: [31]  [2490/2809]  eta: 0:03:01  lr: 0.000006  min_lr: 0.000000  loss: 3.9077 (3.7403)  class_acc: 0.3333 (0.3370)  loss_scale: 32768.0000 (53052.3260)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1710  max mem: 15572
Epoch: [31]  [2500/2809]  eta: 0:02:55  lr: 0.000006  min_lr: 0.000000  loss: 3.6699 (3.7407)  class_acc: 0.3750 (0.3370)  loss_scale: 32768.0000 (52971.2211)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1769  max mem: 15572
Epoch: [31]  [2510/2809]  eta: 0:02:49  lr: 0.000006  min_lr: 0.000000  loss: 3.8929 (3.7413)  class_acc: 0.2500 (0.3368)  loss_scale: 32768.0000 (52890.7622)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1108  max mem: 15572
Epoch: [31]  [2520/2809]  eta: 0:02:44  lr: 0.000006  min_lr: 0.000000  loss: 3.9499 (3.7418)  class_acc: 0.2500 (0.3367)  loss_scale: 32768.0000 (52810.9417)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1581  max mem: 15572
[2025-01-16 05:29:02,302] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:29:02,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [2530/2809]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 3.7612 (3.7422)  class_acc: 0.2917 (0.3366)  loss_scale: 32768.0000 (52835.3252)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1626  max mem: 15572
Epoch: [31]  [2540/2809]  eta: 0:02:32  lr: 0.000006  min_lr: 0.000000  loss: 3.7847 (3.7428)  class_acc: 0.2500 (0.3365)  loss_scale: 65536.0000 (52885.3081)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1027  max mem: 15572
Epoch: [31]  [2550/2809]  eta: 0:02:26  lr: 0.000006  min_lr: 0.000000  loss: 3.7847 (3.7423)  class_acc: 0.2917 (0.3366)  loss_scale: 65536.0000 (52934.8993)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0792  max mem: 15572
Epoch: [31]  [2560/2809]  eta: 0:02:21  lr: 0.000006  min_lr: 0.000000  loss: 3.6956 (3.7425)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (52984.1031)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0890  max mem: 15572
Epoch: [31]  [2570/2809]  eta: 0:02:15  lr: 0.000006  min_lr: 0.000000  loss: 3.9032 (3.7434)  class_acc: 0.2500 (0.3363)  loss_scale: 65536.0000 (53032.9242)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.1698  max mem: 15572
Epoch: [31]  [2580/2809]  eta: 0:02:09  lr: 0.000006  min_lr: 0.000000  loss: 3.9032 (3.7436)  class_acc: 0.2500 (0.3361)  loss_scale: 65536.0000 (53081.3669)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.1639  max mem: 15572
Epoch: [31]  [2590/2809]  eta: 0:02:04  lr: 0.000006  min_lr: 0.000000  loss: 3.7633 (3.7437)  class_acc: 0.2917 (0.3362)  loss_scale: 65536.0000 (53129.4357)  weight_decay: 0.0500 (0.0500)  time: 0.5137  data: 0.0591  max mem: 15572
Epoch: [31]  [2600/2809]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000000  loss: 3.7722 (3.7437)  class_acc: 0.2500 (0.3362)  loss_scale: 65536.0000 (53177.1349)  weight_decay: 0.0500 (0.0500)  time: 0.5376  data: 0.0849  max mem: 15572
Epoch: [31]  [2610/2809]  eta: 0:01:52  lr: 0.000006  min_lr: 0.000000  loss: 3.7662 (3.7435)  class_acc: 0.2917 (0.3361)  loss_scale: 65536.0000 (53224.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5964  data: 0.1437  max mem: 15572
Epoch: [31]  [2620/2809]  eta: 0:01:47  lr: 0.000006  min_lr: 0.000000  loss: 3.8157 (3.7440)  class_acc: 0.2917 (0.3357)  loss_scale: 65536.0000 (53271.4414)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1051  max mem: 15572
Epoch: [31]  [2630/2809]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000000  loss: 3.7404 (3.7425)  class_acc: 0.2917 (0.3360)  loss_scale: 65536.0000 (53318.0570)  weight_decay: 0.0500 (0.0500)  time: 0.5197  data: 0.0675  max mem: 15572
Epoch: [31]  [2640/2809]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000000  loss: 3.4147 (3.7426)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (53364.3196)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1367  max mem: 15572
Epoch: [31]  [2650/2809]  eta: 0:01:30  lr: 0.000006  min_lr: 0.000000  loss: 3.8782 (3.7426)  class_acc: 0.3750 (0.3361)  loss_scale: 65536.0000 (53410.2331)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1419  max mem: 15572
[2025-01-16 05:30:13,551] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:30:13,551] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:30:15,597] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89734
[2025-01-16 05:30:15,597] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:30:15,597] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [2660/2809]  eta: 0:01:24  lr: 0.000006  min_lr: 0.000000  loss: 3.7574 (3.7422)  class_acc: 0.3750 (0.3362)  loss_scale: 65536.0000 (53554.3149)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0898  max mem: 15572
Epoch: [31]  [2670/2809]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000000  loss: 3.7897 (3.7425)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (53599.1733)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0934  max mem: 15572
Epoch: [31]  [2680/2809]  eta: 0:01:13  lr: 0.000006  min_lr: 0.000000  loss: 3.9489 (3.7429)  class_acc: 0.2917 (0.3360)  loss_scale: 65536.0000 (53643.6971)  weight_decay: 0.0500 (0.0500)  time: 0.5377  data: 0.1060  max mem: 15572
Epoch: [31]  [2690/2809]  eta: 0:01:07  lr: 0.000006  min_lr: 0.000000  loss: 4.1352 (3.7447)  class_acc: 0.2500 (0.3356)  loss_scale: 65536.0000 (53687.8900)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1450  max mem: 15572
Epoch: [31]  [2700/2809]  eta: 0:01:01  lr: 0.000006  min_lr: 0.000000  loss: 3.9384 (3.7443)  class_acc: 0.2500 (0.3356)  loss_scale: 65536.0000 (53731.7556)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1275  max mem: 15572
Epoch: [31]  [2710/2809]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 3.5953 (3.7433)  class_acc: 0.3750 (0.3359)  loss_scale: 65536.0000 (53775.2977)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1216  max mem: 15572
Epoch: [31]  [2720/2809]  eta: 0:00:50  lr: 0.000006  min_lr: 0.000000  loss: 3.7826 (3.7439)  class_acc: 0.3333 (0.3356)  loss_scale: 65536.0000 (53818.5197)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.2074  max mem: 15572
Epoch: [31]  [2730/2809]  eta: 0:00:44  lr: 0.000006  min_lr: 0.000000  loss: 3.9986 (3.7446)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (53861.4251)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.2026  max mem: 15572
Epoch: [31]  [2740/2809]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000000  loss: 3.8423 (3.7447)  class_acc: 0.3333 (0.3356)  loss_scale: 65536.0000 (53904.0175)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0848  max mem: 15572
Epoch: [31]  [2750/2809]  eta: 0:00:33  lr: 0.000006  min_lr: 0.000000  loss: 3.7921 (3.7447)  class_acc: 0.3333 (0.3356)  loss_scale: 65536.0000 (53946.3003)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0791  max mem: 15572
Epoch: [31]  [2760/2809]  eta: 0:00:27  lr: 0.000006  min_lr: 0.000000  loss: 3.8426 (3.7451)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (53988.2767)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1497  max mem: 15572
Epoch: [31]  [2770/2809]  eta: 0:00:22  lr: 0.000006  min_lr: 0.000000  loss: 3.7297 (3.7442)  class_acc: 0.3333 (0.3357)  loss_scale: 65536.0000 (54029.9502)  weight_decay: 0.0500 (0.0500)  time: 0.5794  data: 0.1311  max mem: 15572
Epoch: [31]  [2780/2809]  eta: 0:00:16  lr: 0.000006  min_lr: 0.000000  loss: 3.7135 (3.7440)  class_acc: 0.3750 (0.3359)  loss_scale: 65536.0000 (54071.3240)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0642  max mem: 15572
[2025-01-16 05:31:30,161] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:31:30,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:31:30,557] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89864
[2025-01-16 05:31:30,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:31:30,558] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [2790/2809]  eta: 0:00:10  lr: 0.000006  min_lr: 0.000000  loss: 3.6750 (3.7427)  class_acc: 0.4167 (0.3362)  loss_scale: 65536.0000 (54135.8825)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1542  max mem: 15572
Epoch: [31]  [2800/2809]  eta: 0:00:05  lr: 0.000006  min_lr: 0.000000  loss: 3.7367 (3.7435)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (54176.5826)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1373  max mem: 15572
Epoch: [31]  [2808/2809]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 3.7745 (3.7432)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (54208.9341)  weight_decay: 0.0500 (0.0500)  time: 0.4823  data: 0.0702  max mem: 15572
Epoch: [31] Total time: 0:26:34 (0.5677 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 3.7745 (3.7432)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (54208.9341)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:17:52  loss: 0.4235 (0.4235)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.9414  data: 3.7531  max mem: 15572
Val:  [ 10/272]  eta: 0:03:18  loss: 2.3188 (2.2780)  acc1: 44.4444 (43.4343)  acc5: 77.7778 (75.7576)  time: 0.7583  data: 0.5604  max mem: 15572
Val:  [ 20/272]  eta: 0:02:08  loss: 2.3188 (2.3279)  acc1: 44.4444 (46.2963)  acc5: 77.7778 (73.5450)  time: 0.3391  data: 0.1436  max mem: 15572
Val:  [ 30/272]  eta: 0:01:44  loss: 2.3780 (2.4185)  acc1: 44.4444 (42.1147)  acc5: 72.2222 (72.4014)  time: 0.2500  data: 0.0573  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.6005 (2.4575)  acc1: 27.7778 (39.5664)  acc5: 72.2222 (72.4932)  time: 0.2907  data: 0.0995  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.4138 (2.3714)  acc1: 33.3333 (41.7211)  acc5: 77.7778 (74.7277)  time: 0.3153  data: 0.1306  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.4433 (2.2618)  acc1: 61.1111 (44.9909)  acc5: 88.8889 (75.9563)  time: 0.3364  data: 0.1411  max mem: 15572
Val:  [ 70/272]  eta: 0:01:16  loss: 1.5066 (2.1749)  acc1: 66.6667 (47.4961)  acc5: 88.8889 (77.3865)  time: 0.3575  data: 0.1555  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.8208 (2.1915)  acc1: 61.1111 (47.0508)  acc5: 83.3333 (77.0919)  time: 0.3404  data: 0.1334  max mem: 15572
Val:  [ 90/272]  eta: 0:01:08  loss: 2.1705 (2.1898)  acc1: 44.4444 (47.4969)  acc5: 77.7778 (77.7167)  time: 0.3765  data: 0.1596  max mem: 15572
Val:  [100/272]  eta: 0:01:03  loss: 2.1025 (2.2155)  acc1: 50.0000 (46.9747)  acc5: 83.3333 (77.3927)  time: 0.3598  data: 0.1381  max mem: 15572
Val:  [110/272]  eta: 0:00:58  loss: 2.4722 (2.2900)  acc1: 22.2222 (45.0450)  acc5: 66.6667 (76.1762)  time: 0.2835  data: 0.0671  max mem: 15572
Val:  [120/272]  eta: 0:00:55  loss: 2.9590 (2.3265)  acc1: 22.2222 (44.3985)  acc5: 66.6667 (75.8494)  time: 0.3262  data: 0.1337  max mem: 15572
Val:  [130/272]  eta: 0:00:52  loss: 2.1741 (2.2881)  acc1: 44.4444 (45.4623)  acc5: 77.7778 (76.6327)  time: 0.4055  data: 0.2179  max mem: 15572
Val:  [140/272]  eta: 0:00:47  loss: 1.6877 (2.2810)  acc1: 50.0000 (45.9023)  acc5: 88.8889 (76.4775)  time: 0.3656  data: 0.1735  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 2.3019 (2.2860)  acc1: 44.4444 (45.5850)  acc5: 77.7778 (76.7476)  time: 0.3217  data: 0.1229  max mem: 15572
Val:  [160/272]  eta: 0:00:39  loss: 2.3019 (2.2775)  acc1: 50.0000 (46.1353)  acc5: 77.7778 (76.9841)  time: 0.3095  data: 0.1064  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 2.3211 (2.2959)  acc1: 44.4444 (45.6140)  acc5: 72.2222 (76.6407)  time: 0.2915  data: 0.0995  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 2.3211 (2.2888)  acc1: 38.8889 (45.4880)  acc5: 77.7778 (76.9490)  time: 0.3505  data: 0.1686  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.3277 (2.3444)  acc1: 38.8889 (44.2990)  acc5: 77.7778 (75.5090)  time: 0.3146  data: 0.1226  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.5446 (2.3528)  acc1: 38.8889 (43.9193)  acc5: 72.2222 (75.2902)  time: 0.2685  data: 0.0695  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.1768 (2.3527)  acc1: 44.4444 (44.1285)  acc5: 77.7778 (75.2238)  time: 0.2971  data: 0.1037  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1905 (2.3419)  acc1: 44.4444 (44.3439)  acc5: 77.7778 (75.2388)  time: 0.3101  data: 0.1212  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.6698 (2.3115)  acc1: 61.1111 (45.3583)  acc5: 83.3333 (75.5892)  time: 0.3234  data: 0.1286  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6119 (2.2962)  acc1: 61.1111 (45.6201)  acc5: 83.3333 (75.8875)  time: 0.3185  data: 0.1217  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2598 (2.3095)  acc1: 44.4444 (44.9757)  acc5: 77.7778 (75.8300)  time: 0.3013  data: 0.1137  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.3429 (2.2516)  acc1: 72.2222 (46.6369)  acc5: 88.8889 (76.5858)  time: 0.2606  data: 0.0827  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3906 (2.2460)  acc1: 66.6667 (46.6995)  acc5: 88.8889 (76.7528)  time: 0.1881  data: 0.0338  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3906 (2.2502)  acc1: 66.6667 (46.6721)  acc5: 88.8889 (76.7151)  time: 0.1807  data: 0.0337  max mem: 15572
Val: Total time: 0:01:29 (0.3277 s / it)
* Acc@1 46.672 Acc@5 76.715 loss 2.250
Accuracy of the network on the 4883 val videos: 46.7%
Max accuracy: 46.77%
Epoch: [32]  [   0/2809]  eta: 2:47:27  lr: 0.000006  min_lr: 0.000000  loss: 3.7503 (3.7503)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 3.5768  data: 3.1881  max mem: 15572
Epoch: [32]  [  10/2809]  eta: 0:36:27  lr: 0.000006  min_lr: 0.000000  loss: 3.7503 (3.6859)  class_acc: 0.3333 (0.3523)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7814  data: 0.3517  max mem: 15572
Epoch: [32]  [  20/2809]  eta: 0:31:54  lr: 0.000006  min_lr: 0.000000  loss: 3.8680 (3.7490)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.1013  max mem: 15572
Epoch: [32]  [  30/2809]  eta: 0:30:26  lr: 0.000006  min_lr: 0.000000  loss: 3.9148 (3.7046)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.1452  max mem: 15572
Epoch: [32]  [  40/2809]  eta: 0:31:13  lr: 0.000006  min_lr: 0.000000  loss: 3.7669 (3.6727)  class_acc: 0.3333 (0.3476)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.2208  max mem: 15572
Epoch: [32]  [  50/2809]  eta: 0:32:09  lr: 0.000006  min_lr: 0.000000  loss: 3.7705 (3.7011)  class_acc: 0.3333 (0.3440)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7651  data: 0.3223  max mem: 15572
Epoch: [32]  [  60/2809]  eta: 0:31:34  lr: 0.000006  min_lr: 0.000000  loss: 3.8445 (3.7150)  class_acc: 0.3750 (0.3422)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7150  data: 0.2880  max mem: 15572
[2025-01-16 05:33:55,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 89952
[2025-01-16 05:33:55,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:33:55,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [  70/2809]  eta: 0:30:41  lr: 0.000006  min_lr: 0.000000  loss: 3.7608 (3.7087)  class_acc: 0.3750 (0.3451)  loss_scale: 65536.0000 (62305.3521)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.1698  max mem: 15572
Epoch: [32]  [  80/2809]  eta: 0:31:22  lr: 0.000006  min_lr: 0.000000  loss: 3.8680 (3.7569)  class_acc: 0.2917 (0.3359)  loss_scale: 32768.0000 (58658.7654)  weight_decay: 0.0500 (0.0500)  time: 0.6921  data: 0.2498  max mem: 15572
Epoch: [32]  [  90/2809]  eta: 0:31:11  lr: 0.000006  min_lr: 0.000000  loss: 3.8680 (3.7547)  class_acc: 0.2500 (0.3347)  loss_scale: 32768.0000 (55813.6264)  weight_decay: 0.0500 (0.0500)  time: 0.7445  data: 0.2980  max mem: 15572
Epoch: [32]  [ 100/2809]  eta: 0:31:04  lr: 0.000006  min_lr: 0.000000  loss: 3.7753 (3.7432)  class_acc: 0.2917 (0.3366)  loss_scale: 32768.0000 (53531.8812)  weight_decay: 0.0500 (0.0500)  time: 0.6807  data: 0.2244  max mem: 15572
Epoch: [32]  [ 110/2809]  eta: 0:30:44  lr: 0.000006  min_lr: 0.000000  loss: 3.4568 (3.7238)  class_acc: 0.3333 (0.3393)  loss_scale: 32768.0000 (51661.2613)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.2020  max mem: 15572
[2025-01-16 05:34:27,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=90000, skipped=600, lr=[5.594318732376175e-08, 5.594318732376175e-08, 7.991883903394536e-08, 7.991883903394536e-08, 1.141697700484934e-07, 1.141697700484934e-07, 1.6309967149784771e-07, 1.6309967149784771e-07, 2.3299953071121103e-07, 2.3299953071121103e-07, 3.328564724445872e-07, 3.328564724445872e-07, 4.755092463494103e-07, 4.755092463494103e-07, 6.792989233563005e-07, 6.792989233563005e-07, 9.704270333661435e-07, 9.704270333661435e-07, 1.3863243333802054e-06, 1.3863243333802054e-06, 1.980463333400293e-06, 1.980463333400293e-06, 2.8292333334289905e-06, 2.8292333334289905e-06, 4.041761904898559e-06, 4.041761904898559e-06, 5.773945578426512e-06, 5.773945578426512e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:34:27,555] [INFO] [timer.py:260:stop] epoch=0/micro_step=90000/global_step=90000, RunningAvgSamplesPerSec=28.57202407993806, CurrSamplesPerSec=25.834935922936083, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [ 120/2809]  eta: 0:30:36  lr: 0.000006  min_lr: 0.000000  loss: 3.7705 (3.7242)  class_acc: 0.2917 (0.3378)  loss_scale: 32768.0000 (50099.8347)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.1765  max mem: 15572
Epoch: [32]  [ 130/2809]  eta: 0:30:23  lr: 0.000006  min_lr: 0.000000  loss: 3.7275 (3.7116)  class_acc: 0.2917 (0.3406)  loss_scale: 32768.0000 (48776.7939)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.1893  max mem: 15572
Epoch: [32]  [ 140/2809]  eta: 0:29:24  lr: 0.000006  min_lr: 0.000000  loss: 3.6965 (3.7035)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (47641.4184)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.1000  max mem: 15572
Epoch: [32]  [ 150/2809]  eta: 0:28:36  lr: 0.000006  min_lr: 0.000000  loss: 3.7683 (3.7039)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (46656.4238)  weight_decay: 0.0500 (0.0500)  time: 0.4138  data: 0.0003  max mem: 15572
Epoch: [32]  [ 160/2809]  eta: 0:28:12  lr: 0.000006  min_lr: 0.000000  loss: 3.5913 (3.6903)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (45793.7888)  weight_decay: 0.0500 (0.0500)  time: 0.4835  data: 0.0455  max mem: 15572
Epoch: [32]  [ 170/2809]  eta: 0:28:14  lr: 0.000006  min_lr: 0.000000  loss: 3.4491 (3.6882)  class_acc: 0.3750 (0.3438)  loss_scale: 32768.0000 (45032.0468)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1762  max mem: 15572
Epoch: [32]  [ 180/2809]  eta: 0:27:54  lr: 0.000006  min_lr: 0.000000  loss: 3.7251 (3.7004)  class_acc: 0.3750 (0.3423)  loss_scale: 32768.0000 (44354.4751)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.1886  max mem: 15572
Epoch: [32]  [ 190/2809]  eta: 0:27:29  lr: 0.000006  min_lr: 0.000000  loss: 3.9993 (3.7106)  class_acc: 0.2917 (0.3392)  loss_scale: 32768.0000 (43747.8534)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0902  max mem: 15572
[2025-01-16 05:35:12,738] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:35:12,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 200/2809]  eta: 0:27:21  lr: 0.000006  min_lr: 0.000000  loss: 3.6949 (3.7082)  class_acc: 0.3333 (0.3412)  loss_scale: 32768.0000 (44505.7910)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1317  max mem: 15572
Epoch: [32]  [ 210/2809]  eta: 0:27:16  lr: 0.000006  min_lr: 0.000000  loss: 3.6696 (3.6980)  class_acc: 0.3750 (0.3434)  loss_scale: 65536.0000 (45502.4834)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1949  max mem: 15572
Epoch: [32]  [ 220/2809]  eta: 0:26:56  lr: 0.000006  min_lr: 0.000000  loss: 3.7391 (3.6994)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (46408.9774)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1355  max mem: 15572
Epoch: [32]  [ 230/2809]  eta: 0:26:40  lr: 0.000006  min_lr: 0.000000  loss: 3.7816 (3.7000)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (47236.9870)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0985  max mem: 15572
Epoch: [32]  [ 240/2809]  eta: 0:26:34  lr: 0.000006  min_lr: 0.000000  loss: 3.7395 (3.7013)  class_acc: 0.3750 (0.3432)  loss_scale: 65536.0000 (47996.2822)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1473  max mem: 15572
Epoch: [32]  [ 250/2809]  eta: 0:26:13  lr: 0.000006  min_lr: 0.000000  loss: 3.7395 (3.7030)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (48695.0757)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1007  max mem: 15572
Epoch: [32]  [ 260/2809]  eta: 0:25:57  lr: 0.000006  min_lr: 0.000000  loss: 3.6287 (3.6917)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (49340.3218)  weight_decay: 0.0500 (0.0500)  time: 0.4935  data: 0.0547  max mem: 15572
Epoch: [32]  [ 270/2809]  eta: 0:25:48  lr: 0.000006  min_lr: 0.000000  loss: 3.3814 (3.6849)  class_acc: 0.3750 (0.3458)  loss_scale: 65536.0000 (49937.9483)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1070  max mem: 15572
Epoch: [32]  [ 280/2809]  eta: 0:25:30  lr: 0.000006  min_lr: 0.000000  loss: 3.5705 (3.6837)  class_acc: 0.4167 (0.3461)  loss_scale: 65536.0000 (50493.0391)  weight_decay: 0.0500 (0.0500)  time: 0.5265  data: 0.0742  max mem: 15572
Epoch: [32]  [ 290/2809]  eta: 0:25:20  lr: 0.000006  min_lr: 0.000000  loss: 3.8128 (3.6888)  class_acc: 0.3333 (0.3454)  loss_scale: 65536.0000 (51009.9794)  weight_decay: 0.0500 (0.0500)  time: 0.5191  data: 0.0582  max mem: 15572
Epoch: [32]  [ 300/2809]  eta: 0:25:18  lr: 0.000006  min_lr: 0.000000  loss: 3.8734 (3.6870)  class_acc: 0.3333 (0.3465)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1479  max mem: 15572
Epoch: [32]  [ 310/2809]  eta: 0:25:06  lr: 0.000006  min_lr: 0.000000  loss: 3.8476 (3.6926)  class_acc: 0.3333 (0.3459)  loss_scale: 65536.0000 (51944.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1447  max mem: 15572
Epoch: [32]  [ 320/2809]  eta: 0:24:51  lr: 0.000006  min_lr: 0.000000  loss: 3.8785 (3.6943)  class_acc: 0.3333 (0.3464)  loss_scale: 65536.0000 (52367.5514)  weight_decay: 0.0500 (0.0500)  time: 0.5113  data: 0.0611  max mem: 15572
[2025-01-16 05:36:24,043] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:36:24,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:36:25,614] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90212
[2025-01-16 05:36:25,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:36:25,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 330/2809]  eta: 0:24:48  lr: 0.000006  min_lr: 0.000000  loss: 3.5979 (3.6880)  class_acc: 0.3750 (0.3486)  loss_scale: 65536.0000 (53359.3716)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1100  max mem: 15572
Epoch: [32]  [ 340/2809]  eta: 0:24:45  lr: 0.000006  min_lr: 0.000000  loss: 3.5979 (3.6885)  class_acc: 0.3333 (0.3478)  loss_scale: 65536.0000 (53716.4575)  weight_decay: 0.0500 (0.0500)  time: 0.6379  data: 0.1973  max mem: 15572
Epoch: [32]  [ 350/2809]  eta: 0:24:35  lr: 0.000006  min_lr: 0.000000  loss: 3.7927 (3.6935)  class_acc: 0.3333 (0.3481)  loss_scale: 65536.0000 (54053.1966)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.1660  max mem: 15572
Epoch: [32]  [ 360/2809]  eta: 0:24:29  lr: 0.000006  min_lr: 0.000000  loss: 3.7927 (3.6952)  class_acc: 0.3333 (0.3479)  loss_scale: 65536.0000 (54371.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1486  max mem: 15572
Epoch: [32]  [ 370/2809]  eta: 0:24:20  lr: 0.000006  min_lr: 0.000000  loss: 3.6513 (3.6948)  class_acc: 0.3333 (0.3489)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1338  max mem: 15572
Epoch: [32]  [ 380/2809]  eta: 0:24:17  lr: 0.000006  min_lr: 0.000000  loss: 3.6540 (3.6930)  class_acc: 0.3750 (0.3504)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.5974  data: 0.1562  max mem: 15572
Epoch: [32]  [ 390/2809]  eta: 0:24:05  lr: 0.000006  min_lr: 0.000000  loss: 3.7490 (3.6944)  class_acc: 0.3333 (0.3492)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1483  max mem: 15572
Epoch: [32]  [ 400/2809]  eta: 0:24:02  lr: 0.000006  min_lr: 0.000000  loss: 3.7771 (3.6982)  class_acc: 0.3333 (0.3489)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1542  max mem: 15572
Epoch: [32]  [ 410/2809]  eta: 0:23:52  lr: 0.000006  min_lr: 0.000000  loss: 3.6733 (3.6969)  class_acc: 0.3333 (0.3497)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1576  max mem: 15572
Epoch: [32]  [ 420/2809]  eta: 0:23:44  lr: 0.000006  min_lr: 0.000000  loss: 3.4223 (3.6931)  class_acc: 0.3750 (0.3505)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0925  max mem: 15572
Epoch: [32]  [ 430/2809]  eta: 0:23:35  lr: 0.000006  min_lr: 0.000000  loss: 3.5151 (3.6946)  class_acc: 0.3750 (0.3495)  loss_scale: 65536.0000 (56184.5754)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1152  max mem: 15572
[2025-01-16 05:37:33,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90328
[2025-01-16 05:37:33,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:37:33,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 440/2809]  eta: 0:23:27  lr: 0.000006  min_lr: 0.000000  loss: 3.5951 (3.6937)  class_acc: 0.2917 (0.3488)  loss_scale: 65536.0000 (56322.3220)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1244  max mem: 15572
Epoch: [32]  [ 450/2809]  eta: 0:23:19  lr: 0.000006  min_lr: 0.000000  loss: 3.7942 (3.6946)  class_acc: 0.2917 (0.3483)  loss_scale: 32768.0000 (55800.0532)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1189  max mem: 15572
Epoch: [32]  [ 460/2809]  eta: 0:23:09  lr: 0.000006  min_lr: 0.000000  loss: 3.8521 (3.7009)  class_acc: 0.2917 (0.3471)  loss_scale: 32768.0000 (55300.4425)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0914  max mem: 15572
Epoch: [32]  [ 470/2809]  eta: 0:23:06  lr: 0.000006  min_lr: 0.000000  loss: 3.7791 (3.6989)  class_acc: 0.3333 (0.3468)  loss_scale: 32768.0000 (54822.0467)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1302  max mem: 15572
Epoch: [32]  [ 480/2809]  eta: 0:22:59  lr: 0.000006  min_lr: 0.000000  loss: 3.6117 (3.6966)  class_acc: 0.3333 (0.3476)  loss_scale: 32768.0000 (54363.5426)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1637  max mem: 15572
Epoch: [32]  [ 490/2809]  eta: 0:22:50  lr: 0.000006  min_lr: 0.000000  loss: 3.6497 (3.6991)  class_acc: 0.3333 (0.3472)  loss_scale: 32768.0000 (53923.7149)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1029  max mem: 15572
Epoch: [32]  [ 500/2809]  eta: 0:22:40  lr: 0.000006  min_lr: 0.000000  loss: 3.8507 (3.6991)  class_acc: 0.3333 (0.3477)  loss_scale: 32768.0000 (53501.4451)  weight_decay: 0.0500 (0.0500)  time: 0.5097  data: 0.0719  max mem: 15572
Epoch: [32]  [ 510/2809]  eta: 0:22:34  lr: 0.000006  min_lr: 0.000000  loss: 3.6772 (3.6969)  class_acc: 0.3750 (0.3484)  loss_scale: 32768.0000 (53095.7025)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.1078  max mem: 15572
Epoch: [32]  [ 520/2809]  eta: 0:22:30  lr: 0.000006  min_lr: 0.000000  loss: 3.8676 (3.7020)  class_acc: 0.2917 (0.3472)  loss_scale: 32768.0000 (52705.5355)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.1627  max mem: 15572
Epoch: [32]  [ 530/2809]  eta: 0:22:23  lr: 0.000006  min_lr: 0.000000  loss: 3.8880 (3.7075)  class_acc: 0.2917 (0.3469)  loss_scale: 32768.0000 (52330.0640)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.1511  max mem: 15572
Epoch: [32]  [ 540/2809]  eta: 0:22:14  lr: 0.000006  min_lr: 0.000000  loss: 4.0104 (3.7124)  class_acc: 0.2917 (0.3459)  loss_scale: 32768.0000 (51968.4732)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0956  max mem: 15572
Epoch: [32]  [ 550/2809]  eta: 0:22:03  lr: 0.000006  min_lr: 0.000000  loss: 4.0484 (3.7152)  class_acc: 0.2917 (0.3449)  loss_scale: 32768.0000 (51620.0073)  weight_decay: 0.0500 (0.0500)  time: 0.4958  data: 0.0560  max mem: 15572
Epoch: [32]  [ 560/2809]  eta: 0:22:00  lr: 0.000006  min_lr: 0.000000  loss: 3.8650 (3.7153)  class_acc: 0.2917 (0.3442)  loss_scale: 32768.0000 (51283.9643)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1187  max mem: 15572
[2025-01-16 05:38:44,882] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:38:44,883] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [ 570/2809]  eta: 0:21:54  lr: 0.000006  min_lr: 0.000000  loss: 3.6283 (3.7112)  class_acc: 0.2917 (0.3439)  loss_scale: 32768.0000 (51074.4658)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.1622  max mem: 15572
Epoch: [32]  [ 580/2809]  eta: 0:21:46  lr: 0.000006  min_lr: 0.000000  loss: 3.6450 (3.7113)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (51323.3735)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.1150  max mem: 15572
Epoch: [32]  [ 590/2809]  eta: 0:21:40  lr: 0.000006  min_lr: 0.000000  loss: 3.8653 (3.7154)  class_acc: 0.3333 (0.3443)  loss_scale: 65536.0000 (51563.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5529  data: 0.0999  max mem: 15572
Epoch: [32]  [ 600/2809]  eta: 0:21:32  lr: 0.000006  min_lr: 0.000000  loss: 3.8189 (3.7145)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (51796.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5561  data: 0.1118  max mem: 15572
Epoch: [32]  [ 610/2809]  eta: 0:21:24  lr: 0.000006  min_lr: 0.000000  loss: 3.7139 (3.7170)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (52021.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0922  max mem: 15572
Epoch: [32]  [ 620/2809]  eta: 0:21:17  lr: 0.000006  min_lr: 0.000000  loss: 3.8377 (3.7174)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (52238.8406)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0654  max mem: 15572
Epoch: [32]  [ 630/2809]  eta: 0:21:11  lr: 0.000006  min_lr: 0.000000  loss: 3.7558 (3.7139)  class_acc: 0.3333 (0.3434)  loss_scale: 65536.0000 (52449.5721)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.0909  max mem: 15572
Epoch: [32]  [ 640/2809]  eta: 0:21:03  lr: 0.000006  min_lr: 0.000000  loss: 3.8181 (3.7158)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (52653.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0939  max mem: 15572
Epoch: [32]  [ 650/2809]  eta: 0:20:56  lr: 0.000006  min_lr: 0.000000  loss: 3.8872 (3.7170)  class_acc: 0.3333 (0.3424)  loss_scale: 65536.0000 (52851.6129)  weight_decay: 0.0500 (0.0500)  time: 0.5367  data: 0.0924  max mem: 15572
Epoch: [32]  [ 660/2809]  eta: 0:20:50  lr: 0.000006  min_lr: 0.000000  loss: 3.9178 (3.7176)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (53043.5098)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1255  max mem: 15572
Epoch: [32]  [ 670/2809]  eta: 0:20:43  lr: 0.000006  min_lr: 0.000000  loss: 3.9178 (3.7176)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (53229.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1259  max mem: 15572
Epoch: [32]  [ 680/2809]  eta: 0:20:40  lr: 0.000005  min_lr: 0.000000  loss: 3.7049 (3.7153)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (53410.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.1606  max mem: 15572
Epoch: [32]  [ 690/2809]  eta: 0:20:35  lr: 0.000005  min_lr: 0.000000  loss: 3.6839 (3.7143)  class_acc: 0.3750 (0.3442)  loss_scale: 65536.0000 (53585.8755)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.1893  max mem: 15572
[2025-01-16 05:39:58,031] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:39:58,031] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:39:58,441] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90586
[2025-01-16 05:39:58,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:39:58,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 700/2809]  eta: 0:20:27  lr: 0.000005  min_lr: 0.000000  loss: 3.6121 (3.7119)  class_acc: 0.4167 (0.3454)  loss_scale: 65536.0000 (53849.8374)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1289  max mem: 15572
Epoch: [32]  [ 710/2809]  eta: 0:20:21  lr: 0.000005  min_lr: 0.000000  loss: 3.6121 (3.7112)  class_acc: 0.4583 (0.3464)  loss_scale: 65536.0000 (54014.1997)  weight_decay: 0.0500 (0.0500)  time: 0.5436  data: 0.1071  max mem: 15572
Epoch: [32]  [ 720/2809]  eta: 0:20:15  lr: 0.000005  min_lr: 0.000000  loss: 3.5401 (3.7082)  class_acc: 0.4167 (0.3464)  loss_scale: 65536.0000 (54174.0028)  weight_decay: 0.0500 (0.0500)  time: 0.5742  data: 0.1171  max mem: 15572
Epoch: [32]  [ 730/2809]  eta: 0:20:06  lr: 0.000005  min_lr: 0.000000  loss: 3.4597 (3.7069)  class_acc: 0.3333 (0.3468)  loss_scale: 65536.0000 (54329.4337)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0870  max mem: 15572
Epoch: [32]  [ 740/2809]  eta: 0:20:03  lr: 0.000005  min_lr: 0.000000  loss: 3.7892 (3.7081)  class_acc: 0.3333 (0.3460)  loss_scale: 65536.0000 (54480.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1217  max mem: 15572
Epoch: [32]  [ 750/2809]  eta: 0:19:58  lr: 0.000005  min_lr: 0.000000  loss: 3.8013 (3.7080)  class_acc: 0.3750 (0.3466)  loss_scale: 65536.0000 (54627.8775)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1615  max mem: 15572
Epoch: [32]  [ 760/2809]  eta: 0:19:50  lr: 0.000005  min_lr: 0.000000  loss: 3.5957 (3.7062)  class_acc: 0.3750 (0.3472)  loss_scale: 65536.0000 (54771.2168)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1125  max mem: 15572
Epoch: [32]  [ 770/2809]  eta: 0:19:44  lr: 0.000005  min_lr: 0.000000  loss: 3.5957 (3.7063)  class_acc: 0.3333 (0.3473)  loss_scale: 65536.0000 (54910.8379)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0949  max mem: 15572
Epoch: [32]  [ 780/2809]  eta: 0:19:37  lr: 0.000005  min_lr: 0.000000  loss: 3.5695 (3.7029)  class_acc: 0.3333 (0.3476)  loss_scale: 65536.0000 (55046.8835)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.0919  max mem: 15572
Epoch: [32]  [ 790/2809]  eta: 0:19:31  lr: 0.000005  min_lr: 0.000000  loss: 3.2630 (3.7009)  class_acc: 0.3750 (0.3480)  loss_scale: 65536.0000 (55179.4893)  weight_decay: 0.0500 (0.0500)  time: 0.5538  data: 0.1045  max mem: 15572
Epoch: [32]  [ 800/2809]  eta: 0:19:25  lr: 0.000005  min_lr: 0.000000  loss: 3.5472 (3.7013)  class_acc: 0.3750 (0.3485)  loss_scale: 65536.0000 (55308.7840)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1439  max mem: 15572
Epoch: [32]  [ 810/2809]  eta: 0:19:18  lr: 0.000005  min_lr: 0.000000  loss: 3.6811 (3.7033)  class_acc: 0.3750 (0.3483)  loss_scale: 65536.0000 (55434.8903)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1149  max mem: 15572
[2025-01-16 05:41:04,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90702
[2025-01-16 05:41:04,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:41:04,408] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [ 820/2809]  eta: 0:19:12  lr: 0.000005  min_lr: 0.000000  loss: 3.7248 (3.7054)  class_acc: 0.2917 (0.3476)  loss_scale: 65536.0000 (55278.5384)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.0878  max mem: 15572
Epoch: [32]  [ 830/2809]  eta: 0:19:05  lr: 0.000005  min_lr: 0.000000  loss: 4.0209 (3.7094)  class_acc: 0.2500 (0.3469)  loss_scale: 32768.0000 (55007.6534)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1054  max mem: 15572
Epoch: [32]  [ 840/2809]  eta: 0:18:59  lr: 0.000005  min_lr: 0.000000  loss: 3.7927 (3.7077)  class_acc: 0.2917 (0.3475)  loss_scale: 32768.0000 (54743.2105)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0979  max mem: 15572
Epoch: [32]  [ 850/2809]  eta: 0:18:50  lr: 0.000005  min_lr: 0.000000  loss: 3.6021 (3.7050)  class_acc: 0.3333 (0.3481)  loss_scale: 32768.0000 (54484.9824)  weight_decay: 0.0500 (0.0500)  time: 0.4956  data: 0.0414  max mem: 15572
[2025-01-16 05:41:26,804] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 90747
[2025-01-16 05:41:26,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-01-16 05:41:26,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 860/2809]  eta: 0:18:42  lr: 0.000005  min_lr: 0.000000  loss: 3.7941 (3.7068)  class_acc: 0.2917 (0.3474)  loss_scale: 32768.0000 (54194.6945)  weight_decay: 0.0500 (0.0500)  time: 0.4575  data: 0.0007  max mem: 15572
Epoch: [32]  [ 870/2809]  eta: 0:18:36  lr: 0.000005  min_lr: 0.000000  loss: 3.7941 (3.7067)  class_acc: 0.3333 (0.3476)  loss_scale: 16384.0000 (53760.5878)  weight_decay: 0.0500 (0.0500)  time: 0.5149  data: 0.0676  max mem: 15572
Epoch: [32]  [ 880/2809]  eta: 0:18:29  lr: 0.000005  min_lr: 0.000000  loss: 3.7238 (3.7068)  class_acc: 0.3333 (0.3475)  loss_scale: 16384.0000 (53336.3360)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.1148  max mem: 15572
Epoch: [32]  [ 890/2809]  eta: 0:18:23  lr: 0.000005  min_lr: 0.000000  loss: 3.8136 (3.7076)  class_acc: 0.3333 (0.3471)  loss_scale: 16384.0000 (52921.6072)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1280  max mem: 15572
Epoch: [32]  [ 900/2809]  eta: 0:18:18  lr: 0.000005  min_lr: 0.000000  loss: 3.9312 (3.7119)  class_acc: 0.2500 (0.3462)  loss_scale: 16384.0000 (52516.0844)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1634  max mem: 15572
Epoch: [32]  [ 910/2809]  eta: 0:18:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7893 (3.7073)  class_acc: 0.3750 (0.3475)  loss_scale: 16384.0000 (52119.4643)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1797  max mem: 15572
Epoch: [32]  [ 920/2809]  eta: 0:18:06  lr: 0.000005  min_lr: 0.000000  loss: 3.5167 (3.7080)  class_acc: 0.4583 (0.3477)  loss_scale: 16384.0000 (51731.4571)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1376  max mem: 15572
Epoch: [32]  [ 930/2809]  eta: 0:18:01  lr: 0.000005  min_lr: 0.000000  loss: 3.6533 (3.7073)  class_acc: 0.3750 (0.3479)  loss_scale: 16384.0000 (51351.7852)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1170  max mem: 15572
Epoch: [32]  [ 940/2809]  eta: 0:17:56  lr: 0.000005  min_lr: 0.000000  loss: 3.8508 (3.7108)  class_acc: 0.2917 (0.3469)  loss_scale: 16384.0000 (50980.1828)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1542  max mem: 15572
Epoch: [32]  [ 950/2809]  eta: 0:17:51  lr: 0.000005  min_lr: 0.000000  loss: 3.8508 (3.7095)  class_acc: 0.2500 (0.3469)  loss_scale: 16384.0000 (50616.3954)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.1679  max mem: 15572
Epoch: [32]  [ 960/2809]  eta: 0:17:45  lr: 0.000005  min_lr: 0.000000  loss: 3.7096 (3.7088)  class_acc: 0.3333 (0.3472)  loss_scale: 16384.0000 (50260.1790)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1279  max mem: 15572
Epoch: [32]  [ 970/2809]  eta: 0:17:39  lr: 0.000005  min_lr: 0.000000  loss: 3.9185 (3.7131)  class_acc: 0.2500 (0.3461)  loss_scale: 16384.0000 (49911.2997)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1172  max mem: 15572
Epoch: [32]  [ 980/2809]  eta: 0:17:35  lr: 0.000005  min_lr: 0.000000  loss: 3.8320 (3.7131)  class_acc: 0.2917 (0.3464)  loss_scale: 16384.0000 (49569.5331)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1607  max mem: 15572
[2025-01-16 05:42:42,207] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:42:42,207] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 990/2809]  eta: 0:17:29  lr: 0.000005  min_lr: 0.000000  loss: 3.6792 (3.7140)  class_acc: 0.3750 (0.3463)  loss_scale: 16384.0000 (49284.2624)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1417  max mem: 15572
Epoch: [32]  [1000/2809]  eta: 0:17:22  lr: 0.000005  min_lr: 0.000000  loss: 3.8415 (3.7168)  class_acc: 0.2500 (0.3453)  loss_scale: 32768.0000 (49119.2647)  weight_decay: 0.0500 (0.0500)  time: 0.5573  data: 0.0877  max mem: 15572
Epoch: [32]  [1010/2809]  eta: 0:17:17  lr: 0.000005  min_lr: 0.000000  loss: 3.8254 (3.7152)  class_acc: 0.3333 (0.3458)  loss_scale: 32768.0000 (48957.5312)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1038  max mem: 15572
Epoch: [32]  [1020/2809]  eta: 0:17:12  lr: 0.000005  min_lr: 0.000000  loss: 3.4937 (3.7139)  class_acc: 0.4167 (0.3461)  loss_scale: 32768.0000 (48798.9657)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1779  max mem: 15572
Epoch: [32]  [1030/2809]  eta: 0:17:07  lr: 0.000005  min_lr: 0.000000  loss: 3.6574 (3.7159)  class_acc: 0.3333 (0.3455)  loss_scale: 32768.0000 (48643.4762)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1690  max mem: 15572
Epoch: [32]  [1040/2809]  eta: 0:16:59  lr: 0.000005  min_lr: 0.000000  loss: 3.7224 (3.7151)  class_acc: 0.3333 (0.3457)  loss_scale: 32768.0000 (48490.9741)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.0923  max mem: 15572
Epoch: [32]  [1050/2809]  eta: 0:16:54  lr: 0.000005  min_lr: 0.000000  loss: 3.7812 (3.7173)  class_acc: 0.2917 (0.3453)  loss_scale: 32768.0000 (48341.3739)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1113  max mem: 15572
Epoch: [32]  [1060/2809]  eta: 0:16:50  lr: 0.000005  min_lr: 0.000000  loss: 3.8325 (3.7178)  class_acc: 0.2917 (0.3451)  loss_scale: 32768.0000 (48194.5938)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1786  max mem: 15572
Epoch: [32]  [1070/2809]  eta: 0:16:43  lr: 0.000005  min_lr: 0.000000  loss: 3.8325 (3.7187)  class_acc: 0.3750 (0.3453)  loss_scale: 32768.0000 (48050.5546)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1345  max mem: 15572
Epoch: [32]  [1080/2809]  eta: 0:16:37  lr: 0.000005  min_lr: 0.000000  loss: 3.7994 (3.7185)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (47909.1804)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.0918  max mem: 15572
Epoch: [32]  [1090/2809]  eta: 0:16:32  lr: 0.000005  min_lr: 0.000000  loss: 3.8250 (3.7182)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (47770.3978)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1346  max mem: 15572
Epoch: [32]  [1100/2809]  eta: 0:16:25  lr: 0.000005  min_lr: 0.000000  loss: 3.8929 (3.7197)  class_acc: 0.3333 (0.3451)  loss_scale: 32768.0000 (47634.1362)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1285  max mem: 15572
Epoch: [32]  [1110/2809]  eta: 0:16:20  lr: 0.000005  min_lr: 0.000000  loss: 3.8144 (3.7192)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (47500.3276)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1248  max mem: 15572
[2025-01-16 05:43:53,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=91000, skipped=605, lr=[5.128143724886411e-08, 5.128143724886411e-08, 7.325919606980588e-08, 7.325919606980588e-08, 1.0465599438543698e-07, 1.0465599438543698e-07, 1.4950856340776714e-07, 1.4950856340776714e-07, 2.135836620110959e-07, 2.135836620110959e-07, 3.051195171587084e-07, 3.051195171587084e-07, 4.3588502451244066e-07, 4.3588502451244066e-07, 6.226928921606296e-07, 6.226928921606296e-07, 8.895612745151851e-07, 8.895612745151851e-07, 1.2708018207359789e-06, 1.2708018207359789e-06, 1.8154311724799698e-06, 1.8154311724799698e-06, 2.593473103542814e-06, 2.593473103542814e-06, 3.704961576489735e-06, 3.704961576489735e-06, 5.292802252128193e-06, 5.292802252128193e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:43:53,293] [INFO] [timer.py:260:stop] epoch=0/micro_step=91000/global_step=91000, RunningAvgSamplesPerSec=28.572072192399563, CurrSamplesPerSec=20.656050469209006, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
[2025-01-16 05:43:57,557] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:43:57,558] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1120/2809]  eta: 0:16:15  lr: 0.000005  min_lr: 0.000000  loss: 3.4727 (3.7187)  class_acc: 0.3750 (0.3455)  loss_scale: 32768.0000 (47515.0616)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1941  max mem: 15572
Epoch: [32]  [1130/2809]  eta: 0:16:09  lr: 0.000005  min_lr: 0.000000  loss: 3.4654 (3.7180)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (47674.3979)  weight_decay: 0.0500 (0.0500)  time: 0.5958  data: 0.1625  max mem: 15572
Epoch: [32]  [1140/2809]  eta: 0:16:03  lr: 0.000005  min_lr: 0.000000  loss: 3.4327 (3.7175)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (47830.9413)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0881  max mem: 15572
Epoch: [32]  [1150/2809]  eta: 0:15:56  lr: 0.000005  min_lr: 0.000000  loss: 3.8059 (3.7175)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (47984.7646)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1018  max mem: 15572
Epoch: [32]  [1160/2809]  eta: 0:15:50  lr: 0.000005  min_lr: 0.000000  loss: 3.8084 (3.7179)  class_acc: 0.2917 (0.3447)  loss_scale: 65536.0000 (48135.9380)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.0947  max mem: 15572
Epoch: [32]  [1170/2809]  eta: 0:15:44  lr: 0.000005  min_lr: 0.000000  loss: 3.9301 (3.7198)  class_acc: 0.2500 (0.3446)  loss_scale: 65536.0000 (48284.5295)  weight_decay: 0.0500 (0.0500)  time: 0.5353  data: 0.0980  max mem: 15572
Epoch: [32]  [1180/2809]  eta: 0:15:38  lr: 0.000005  min_lr: 0.000000  loss: 3.9301 (3.7203)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (48430.6046)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1321  max mem: 15572
Epoch: [32]  [1190/2809]  eta: 0:15:33  lr: 0.000005  min_lr: 0.000000  loss: 3.9232 (3.7223)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (48574.2267)  weight_decay: 0.0500 (0.0500)  time: 0.6081  data: 0.1612  max mem: 15572
Epoch: [32]  [1200/2809]  eta: 0:15:26  lr: 0.000005  min_lr: 0.000000  loss: 3.8555 (3.7217)  class_acc: 0.2917 (0.3441)  loss_scale: 65536.0000 (48715.4571)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1376  max mem: 15572
Epoch: [32]  [1210/2809]  eta: 0:15:20  lr: 0.000005  min_lr: 0.000000  loss: 3.8213 (3.7217)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (48854.3551)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0996  max mem: 15572
Epoch: [32]  [1220/2809]  eta: 0:15:14  lr: 0.000005  min_lr: 0.000000  loss: 3.8580 (3.7218)  class_acc: 0.3750 (0.3444)  loss_scale: 65536.0000 (48990.9779)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0936  max mem: 15572
Epoch: [32]  [1230/2809]  eta: 0:15:08  lr: 0.000005  min_lr: 0.000000  loss: 3.6225 (3.7205)  class_acc: 0.3750 (0.3447)  loss_scale: 65536.0000 (49125.3810)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1076  max mem: 15572
Epoch: [32]  [1240/2809]  eta: 0:15:03  lr: 0.000005  min_lr: 0.000000  loss: 3.6033 (3.7203)  class_acc: 0.3750 (0.3445)  loss_scale: 65536.0000 (49257.6180)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1609  max mem: 15572
[2025-01-16 05:45:07,990] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:45:07,991] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:45:08,885] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91134
[2025-01-16 05:45:08,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:45:08,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1250/2809]  eta: 0:14:57  lr: 0.000005  min_lr: 0.000000  loss: 3.8947 (3.7226)  class_acc: 0.2917 (0.3438)  loss_scale: 65536.0000 (49492.5148)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1458  max mem: 15572
Epoch: [32]  [1260/2809]  eta: 0:14:52  lr: 0.000005  min_lr: 0.000000  loss: 3.9654 (3.7242)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (49619.7431)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1421  max mem: 15572
Epoch: [32]  [1270/2809]  eta: 0:14:46  lr: 0.000005  min_lr: 0.000000  loss: 3.8197 (3.7245)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (49744.9693)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1765  max mem: 15572
Epoch: [32]  [1280/2809]  eta: 0:14:41  lr: 0.000005  min_lr: 0.000000  loss: 3.8323 (3.7256)  class_acc: 0.2917 (0.3430)  loss_scale: 65536.0000 (49868.2404)  weight_decay: 0.0500 (0.0500)  time: 0.5942  data: 0.1691  max mem: 15572
Epoch: [32]  [1290/2809]  eta: 0:14:34  lr: 0.000005  min_lr: 0.000000  loss: 3.8323 (3.7264)  class_acc: 0.2500 (0.3425)  loss_scale: 65536.0000 (49989.6019)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1040  max mem: 15572
Epoch: [32]  [1300/2809]  eta: 0:14:27  lr: 0.000005  min_lr: 0.000000  loss: 3.6652 (3.7250)  class_acc: 0.3333 (0.3424)  loss_scale: 65536.0000 (50109.0976)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0475  max mem: 15572
Epoch: [32]  [1310/2809]  eta: 0:14:22  lr: 0.000005  min_lr: 0.000000  loss: 3.6881 (3.7258)  class_acc: 0.3333 (0.3424)  loss_scale: 65536.0000 (50226.7704)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1163  max mem: 15572
Epoch: [32]  [1320/2809]  eta: 0:14:17  lr: 0.000005  min_lr: 0.000000  loss: 3.8088 (3.7263)  class_acc: 0.3333 (0.3420)  loss_scale: 65536.0000 (50342.6616)  weight_decay: 0.0500 (0.0500)  time: 0.6163  data: 0.1916  max mem: 15572
Epoch: [32]  [1330/2809]  eta: 0:14:11  lr: 0.000005  min_lr: 0.000000  loss: 3.7662 (3.7257)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (50456.8114)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1733  max mem: 15572
Epoch: [32]  [1340/2809]  eta: 0:14:05  lr: 0.000005  min_lr: 0.000000  loss: 3.9550 (3.7272)  class_acc: 0.3333 (0.3418)  loss_scale: 65536.0000 (50569.2588)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1059  max mem: 15572
[2025-01-16 05:46:03,589] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91229
[2025-01-16 05:46:03,589] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:46:03,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1350/2809]  eta: 0:13:58  lr: 0.000005  min_lr: 0.000000  loss: 3.8196 (3.7254)  class_acc: 0.2917 (0.3422)  loss_scale: 32768.0000 (50437.4952)  weight_decay: 0.0500 (0.0500)  time: 0.4889  data: 0.0384  max mem: 15572
Epoch: [32]  [1360/2809]  eta: 0:13:51  lr: 0.000005  min_lr: 0.000000  loss: 3.5338 (3.7252)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (50307.6679)  weight_decay: 0.0500 (0.0500)  time: 0.4729  data: 0.0315  max mem: 15572
Epoch: [32]  [1370/2809]  eta: 0:13:45  lr: 0.000005  min_lr: 0.000000  loss: 3.8977 (3.7278)  class_acc: 0.2917 (0.3415)  loss_scale: 32768.0000 (50179.7345)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0675  max mem: 15572
Epoch: [32]  [1380/2809]  eta: 0:13:39  lr: 0.000005  min_lr: 0.000000  loss: 3.9521 (3.7291)  class_acc: 0.2500 (0.3409)  loss_scale: 32768.0000 (50053.6539)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0933  max mem: 15572
Epoch: [32]  [1390/2809]  eta: 0:13:32  lr: 0.000005  min_lr: 0.000000  loss: 3.7964 (3.7284)  class_acc: 0.3333 (0.3414)  loss_scale: 32768.0000 (49929.3861)  weight_decay: 0.0500 (0.0500)  time: 0.5216  data: 0.0752  max mem: 15572
Epoch: [32]  [1400/2809]  eta: 0:13:27  lr: 0.000005  min_lr: 0.000000  loss: 3.5926 (3.7275)  class_acc: 0.4167 (0.3419)  loss_scale: 32768.0000 (49806.8922)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0957  max mem: 15572
Epoch: [32]  [1410/2809]  eta: 0:13:21  lr: 0.000005  min_lr: 0.000000  loss: 3.6419 (3.7285)  class_acc: 0.3750 (0.3418)  loss_scale: 32768.0000 (49686.1347)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1342  max mem: 15572
Epoch: [32]  [1420/2809]  eta: 0:13:16  lr: 0.000005  min_lr: 0.000000  loss: 3.8358 (3.7288)  class_acc: 0.3333 (0.3418)  loss_scale: 32768.0000 (49567.0767)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1271  max mem: 15572
Epoch: [32]  [1430/2809]  eta: 0:13:10  lr: 0.000005  min_lr: 0.000000  loss: 3.7086 (3.7287)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (49449.6827)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1658  max mem: 15572
Epoch: [32]  [1440/2809]  eta: 0:13:04  lr: 0.000005  min_lr: 0.000000  loss: 3.6898 (3.7292)  class_acc: 0.3333 (0.3418)  loss_scale: 32768.0000 (49333.9181)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.1516  max mem: 15572
Epoch: [32]  [1450/2809]  eta: 0:12:59  lr: 0.000005  min_lr: 0.000000  loss: 3.6054 (3.7271)  class_acc: 0.3750 (0.3423)  loss_scale: 32768.0000 (49219.7491)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1443  max mem: 15572
Epoch: [32]  [1460/2809]  eta: 0:12:52  lr: 0.000005  min_lr: 0.000000  loss: 3.5376 (3.7273)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (49107.1431)  weight_decay: 0.0500 (0.0500)  time: 0.5445  data: 0.1123  max mem: 15572
[2025-01-16 05:47:14,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:47:14,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1470/2809]  eta: 0:12:46  lr: 0.000005  min_lr: 0.000000  loss: 3.8355 (3.7270)  class_acc: 0.2917 (0.3419)  loss_scale: 32768.0000 (49018.3440)  weight_decay: 0.0500 (0.0500)  time: 0.5058  data: 0.0673  max mem: 15572
Epoch: [32]  [1480/2809]  eta: 0:12:41  lr: 0.000005  min_lr: 0.000000  loss: 3.5770 (3.7264)  class_acc: 0.2917 (0.3419)  loss_scale: 65536.0000 (49129.8744)  weight_decay: 0.0500 (0.0500)  time: 0.5775  data: 0.1133  max mem: 15572
Epoch: [32]  [1490/2809]  eta: 0:12:36  lr: 0.000005  min_lr: 0.000000  loss: 3.8904 (3.7281)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (49239.9088)  weight_decay: 0.0500 (0.0500)  time: 0.6125  data: 0.1376  max mem: 15572
Epoch: [32]  [1500/2809]  eta: 0:12:30  lr: 0.000005  min_lr: 0.000000  loss: 3.8744 (3.7273)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (49348.4770)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1413  max mem: 15572
Epoch: [32]  [1510/2809]  eta: 0:12:24  lr: 0.000005  min_lr: 0.000000  loss: 3.7216 (3.7274)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (49455.6082)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1134  max mem: 15572
Epoch: [32]  [1520/2809]  eta: 0:12:18  lr: 0.000005  min_lr: 0.000000  loss: 3.8656 (3.7276)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (49561.3307)  weight_decay: 0.0500 (0.0500)  time: 0.5299  data: 0.1112  max mem: 15572
Epoch: [32]  [1530/2809]  eta: 0:12:12  lr: 0.000005  min_lr: 0.000000  loss: 3.7447 (3.7278)  class_acc: 0.3333 (0.3417)  loss_scale: 65536.0000 (49665.6721)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1271  max mem: 15572
Epoch: [32]  [1540/2809]  eta: 0:12:06  lr: 0.000005  min_lr: 0.000000  loss: 3.7746 (3.7290)  class_acc: 0.2917 (0.3413)  loss_scale: 65536.0000 (49768.6593)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1365  max mem: 15572
Epoch: [32]  [1550/2809]  eta: 0:12:01  lr: 0.000005  min_lr: 0.000000  loss: 4.0341 (3.7299)  class_acc: 0.2500 (0.3410)  loss_scale: 65536.0000 (49870.3185)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1568  max mem: 15572
Epoch: [32]  [1560/2809]  eta: 0:11:55  lr: 0.000005  min_lr: 0.000000  loss: 3.8064 (3.7297)  class_acc: 0.3750 (0.3411)  loss_scale: 65536.0000 (49970.6752)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1277  max mem: 15572
Epoch: [32]  [1570/2809]  eta: 0:11:49  lr: 0.000005  min_lr: 0.000000  loss: 3.8182 (3.7304)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (50069.7543)  weight_decay: 0.0500 (0.0500)  time: 0.5206  data: 0.0724  max mem: 15572
Epoch: [32]  [1580/2809]  eta: 0:11:43  lr: 0.000005  min_lr: 0.000000  loss: 3.7527 (3.7290)  class_acc: 0.3750 (0.3416)  loss_scale: 65536.0000 (50167.5800)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0830  max mem: 15572
Epoch: [32]  [1590/2809]  eta: 0:11:37  lr: 0.000005  min_lr: 0.000000  loss: 3.7756 (3.7295)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (50264.1760)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1037  max mem: 15572
[2025-01-16 05:48:26,005] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:48:26,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [1600/2809]  eta: 0:11:31  lr: 0.000005  min_lr: 0.000000  loss: 3.6224 (3.7281)  class_acc: 0.3333 (0.3416)  loss_scale: 65536.0000 (50482.3685)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0825  max mem: 15572
[2025-01-16 05:48:29,382] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91490
[2025-01-16 05:48:29,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:48:29,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1610/2809]  eta: 0:11:25  lr: 0.000005  min_lr: 0.000000  loss: 3.4535 (3.7268)  class_acc: 0.3750 (0.3418)  loss_scale: 65536.0000 (50616.4916)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1159  max mem: 15572
Epoch: [32]  [1620/2809]  eta: 0:11:20  lr: 0.000005  min_lr: 0.000000  loss: 3.7428 (3.7270)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (50708.5305)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.1642  max mem: 15572
Epoch: [32]  [1630/2809]  eta: 0:11:14  lr: 0.000005  min_lr: 0.000000  loss: 3.7596 (3.7279)  class_acc: 0.2500 (0.3410)  loss_scale: 65536.0000 (50799.4408)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1092  max mem: 15572
Epoch: [32]  [1640/2809]  eta: 0:11:09  lr: 0.000005  min_lr: 0.000000  loss: 4.0640 (3.7297)  class_acc: 0.2083 (0.3405)  loss_scale: 65536.0000 (50889.2431)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1150  max mem: 15572
Epoch: [32]  [1650/2809]  eta: 0:11:03  lr: 0.000005  min_lr: 0.000000  loss: 3.9459 (3.7300)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (50977.9576)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1377  max mem: 15572
Epoch: [32]  [1660/2809]  eta: 0:10:57  lr: 0.000005  min_lr: 0.000000  loss: 3.8439 (3.7305)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (51065.6039)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0958  max mem: 15572
Epoch: [32]  [1670/2809]  eta: 0:10:51  lr: 0.000005  min_lr: 0.000000  loss: 3.7006 (3.7298)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (51152.2011)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0928  max mem: 15572
Epoch: [32]  [1680/2809]  eta: 0:10:45  lr: 0.000005  min_lr: 0.000000  loss: 3.7006 (3.7295)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51237.7680)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0977  max mem: 15572
Epoch: [32]  [1690/2809]  eta: 0:10:39  lr: 0.000005  min_lr: 0.000000  loss: 3.9081 (3.7300)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (51322.3229)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1341  max mem: 15572
Epoch: [32]  [1700/2809]  eta: 0:10:34  lr: 0.000005  min_lr: 0.000000  loss: 3.8747 (3.7302)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (51405.8836)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1407  max mem: 15572
Epoch: [32]  [1710/2809]  eta: 0:10:28  lr: 0.000005  min_lr: 0.000000  loss: 3.8747 (3.7312)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (51488.4676)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1341  max mem: 15572
Epoch: [32]  [1720/2809]  eta: 0:10:23  lr: 0.000005  min_lr: 0.000000  loss: 3.8998 (3.7311)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (51570.0918)  weight_decay: 0.0500 (0.0500)  time: 0.6225  data: 0.1751  max mem: 15572
Epoch: [32]  [1730/2809]  eta: 0:10:17  lr: 0.000005  min_lr: 0.000000  loss: 3.9260 (3.7328)  class_acc: 0.3333 (0.3397)  loss_scale: 65536.0000 (51650.7730)  weight_decay: 0.0500 (0.0500)  time: 0.6180  data: 0.1584  max mem: 15572
[2025-01-16 05:49:44,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:49:44,321] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:49:44,735] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91620
[2025-01-16 05:49:44,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:49:44,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [1740/2809]  eta: 0:10:11  lr: 0.000005  min_lr: 0.000000  loss: 3.9260 (3.7342)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (51768.1700)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.1303  max mem: 15572
Epoch: [32]  [1750/2809]  eta: 0:10:06  lr: 0.000005  min_lr: 0.000000  loss: 3.7574 (3.7338)  class_acc: 0.2917 (0.3393)  loss_scale: 65536.0000 (51846.7984)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.1575  max mem: 15572
Epoch: [32]  [1760/2809]  eta: 0:10:00  lr: 0.000005  min_lr: 0.000000  loss: 3.6017 (3.7338)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (51924.5338)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1290  max mem: 15572
Epoch: [32]  [1770/2809]  eta: 0:09:55  lr: 0.000005  min_lr: 0.000000  loss: 3.6521 (3.7343)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (52001.3913)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1326  max mem: 15572
Epoch: [32]  [1780/2809]  eta: 0:09:49  lr: 0.000005  min_lr: 0.000000  loss: 3.6462 (3.7339)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (52077.3857)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1443  max mem: 15572
Epoch: [32]  [1790/2809]  eta: 0:09:43  lr: 0.000005  min_lr: 0.000000  loss: 3.5813 (3.7337)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (52152.5315)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0844  max mem: 15572
Epoch: [32]  [1800/2809]  eta: 0:09:37  lr: 0.000005  min_lr: 0.000000  loss: 3.7670 (3.7347)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (52226.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1117  max mem: 15572
Epoch: [32]  [1810/2809]  eta: 0:09:32  lr: 0.000005  min_lr: 0.000000  loss: 4.0364 (3.7341)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (52300.3335)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.1597  max mem: 15572
Epoch: [32]  [1820/2809]  eta: 0:09:26  lr: 0.000005  min_lr: 0.000000  loss: 3.4883 (3.7340)  class_acc: 0.3750 (0.3394)  loss_scale: 65536.0000 (52373.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1361  max mem: 15572
Epoch: [32]  [1830/2809]  eta: 0:09:20  lr: 0.000005  min_lr: 0.000000  loss: 3.8692 (3.7350)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (52444.9066)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1267  max mem: 15572
Epoch: [32]  [1840/2809]  eta: 0:09:14  lr: 0.000005  min_lr: 0.000000  loss: 3.7396 (3.7351)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (52516.0152)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0945  max mem: 15572
[2025-01-16 05:50:46,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91730
[2025-01-16 05:50:46,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:50:46,407] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [1850/2809]  eta: 0:09:08  lr: 0.000005  min_lr: 0.000000  loss: 3.7262 (3.7352)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (52427.0297)  weight_decay: 0.0500 (0.0500)  time: 0.4836  data: 0.0463  max mem: 15572
Epoch: [32]  [1860/2809]  eta: 0:09:02  lr: 0.000005  min_lr: 0.000000  loss: 3.6492 (3.7339)  class_acc: 0.3333 (0.3392)  loss_scale: 32768.0000 (52321.3928)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1190  max mem: 15572
Epoch: [32]  [1870/2809]  eta: 0:08:57  lr: 0.000005  min_lr: 0.000000  loss: 3.6533 (3.7351)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (52216.8851)  weight_decay: 0.0500 (0.0500)  time: 0.6127  data: 0.1642  max mem: 15572
Epoch: [32]  [1880/2809]  eta: 0:08:51  lr: 0.000005  min_lr: 0.000000  loss: 3.6941 (3.7338)  class_acc: 0.2917 (0.3394)  loss_scale: 32768.0000 (52113.4886)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.1421  max mem: 15572
Epoch: [32]  [1890/2809]  eta: 0:08:45  lr: 0.000005  min_lr: 0.000000  loss: 3.4567 (3.7332)  class_acc: 0.4167 (0.3397)  loss_scale: 32768.0000 (52011.1856)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.1152  max mem: 15572
Epoch: [32]  [1900/2809]  eta: 0:08:40  lr: 0.000005  min_lr: 0.000000  loss: 3.7461 (3.7340)  class_acc: 0.3750 (0.3394)  loss_scale: 32768.0000 (51909.9590)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1290  max mem: 15572
Epoch: [32]  [1910/2809]  eta: 0:08:33  lr: 0.000005  min_lr: 0.000000  loss: 3.7653 (3.7325)  class_acc: 0.3750 (0.3399)  loss_scale: 32768.0000 (51809.7917)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.1131  max mem: 15572
Epoch: [32]  [1920/2809]  eta: 0:08:27  lr: 0.000005  min_lr: 0.000000  loss: 3.6235 (3.7319)  class_acc: 0.3750 (0.3401)  loss_scale: 32768.0000 (51710.6674)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0553  max mem: 15572
Epoch: [32]  [1930/2809]  eta: 0:08:22  lr: 0.000005  min_lr: 0.000000  loss: 3.5494 (3.7311)  class_acc: 0.3750 (0.3402)  loss_scale: 32768.0000 (51612.5697)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.1341  max mem: 15572
Epoch: [32]  [1940/2809]  eta: 0:08:16  lr: 0.000005  min_lr: 0.000000  loss: 3.6764 (3.7318)  class_acc: 0.3333 (0.3400)  loss_scale: 32768.0000 (51515.4827)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1042  max mem: 15572
Epoch: [32]  [1950/2809]  eta: 0:08:10  lr: 0.000005  min_lr: 0.000000  loss: 3.6819 (3.7316)  class_acc: 0.3333 (0.3401)  loss_scale: 32768.0000 (51419.3911)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0588  max mem: 15572
Epoch: [32]  [1960/2809]  eta: 0:08:05  lr: 0.000005  min_lr: 0.000000  loss: 3.7978 (3.7314)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (51324.2794)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1290  max mem: 15572
Epoch: [32]  [1970/2809]  eta: 0:07:59  lr: 0.000005  min_lr: 0.000000  loss: 3.6943 (3.7303)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (51230.1329)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1486  max mem: 15572
[2025-01-16 05:52:00,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:52:00,442] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [1980/2809]  eta: 0:07:53  lr: 0.000005  min_lr: 0.000000  loss: 3.5723 (3.7292)  class_acc: 0.3750 (0.3404)  loss_scale: 32768.0000 (51302.3483)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1539  max mem: 15572
Epoch: [32]  [1990/2809]  eta: 0:07:48  lr: 0.000005  min_lr: 0.000000  loss: 3.5062 (3.7286)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (51373.8383)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.1707  max mem: 15572
Epoch: [32]  [2000/2809]  eta: 0:07:42  lr: 0.000005  min_lr: 0.000000  loss: 3.7698 (3.7292)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (51444.6137)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0952  max mem: 15572
Epoch: [32]  [2010/2809]  eta: 0:07:36  lr: 0.000005  min_lr: 0.000000  loss: 3.8612 (3.7299)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (51514.6852)  weight_decay: 0.0500 (0.0500)  time: 0.4752  data: 0.0412  max mem: 15572
Epoch: [32]  [2020/2809]  eta: 0:07:30  lr: 0.000005  min_lr: 0.000000  loss: 3.7768 (3.7300)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (51584.0633)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.0749  max mem: 15572
Epoch: [32]  [2030/2809]  eta: 0:07:24  lr: 0.000005  min_lr: 0.000000  loss: 3.6481 (3.7299)  class_acc: 0.3333 (0.3401)  loss_scale: 65536.0000 (51652.7582)  weight_decay: 0.0500 (0.0500)  time: 0.5576  data: 0.0882  max mem: 15572
Epoch: [32]  [2040/2809]  eta: 0:07:18  lr: 0.000005  min_lr: 0.000000  loss: 3.4592 (3.7289)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (51720.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1029  max mem: 15572
Epoch: [32]  [2050/2809]  eta: 0:07:13  lr: 0.000005  min_lr: 0.000000  loss: 3.8446 (3.7299)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (51788.1385)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0768  max mem: 15572
Epoch: [32]  [2060/2809]  eta: 0:07:07  lr: 0.000005  min_lr: 0.000000  loss: 3.8949 (3.7297)  class_acc: 0.2917 (0.3406)  loss_scale: 65536.0000 (51854.8433)  weight_decay: 0.0500 (0.0500)  time: 0.5638  data: 0.1099  max mem: 15572
Epoch: [32]  [2070/2809]  eta: 0:07:02  lr: 0.000005  min_lr: 0.000000  loss: 3.8175 (3.7295)  class_acc: 0.3750 (0.3408)  loss_scale: 65536.0000 (51920.9039)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.1655  max mem: 15572
Epoch: [32]  [2080/2809]  eta: 0:06:56  lr: 0.000005  min_lr: 0.000000  loss: 3.6805 (3.7291)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51986.3296)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1235  max mem: 15572
Epoch: [32]  [2090/2809]  eta: 0:06:50  lr: 0.000005  min_lr: 0.000000  loss: 3.7064 (3.7290)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (52051.1296)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1003  max mem: 15572
[2025-01-16 05:53:10,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:53:10,675] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [2100/2809]  eta: 0:06:44  lr: 0.000005  min_lr: 0.000000  loss: 3.8128 (3.7302)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (52177.6982)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1000  max mem: 15572
[2025-01-16 05:53:12,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 91991
[2025-01-16 05:53:12,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:53:12,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2110/2809]  eta: 0:06:38  lr: 0.000005  min_lr: 0.000000  loss: 3.8024 (3.7297)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (52303.0677)  weight_decay: 0.0500 (0.0500)  time: 0.5226  data: 0.0872  max mem: 15572
[2025-01-16 05:53:16,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=92000, skipped=611, lr=[4.679941201088924e-08, 4.679941201088924e-08, 6.685630287269892e-08, 6.685630287269892e-08, 9.550900410385561e-08, 9.550900410385561e-08, 1.3644143443407946e-07, 1.3644143443407946e-07, 1.949163349058278e-07, 1.949163349058278e-07, 2.784519070083254e-07, 2.784519070083254e-07, 3.9778843858332207e-07, 3.9778843858332207e-07, 5.682691979761744e-07, 5.682691979761744e-07, 8.118131399659635e-07, 8.118131399659635e-07, 1.1597330570942337e-06, 1.1597330570942337e-06, 1.6567615101346195e-06, 1.6567615101346195e-06, 2.366802157335171e-06, 2.366802157335171e-06, 3.3811459390502447e-06, 3.3811459390502447e-06, 4.830208484357493e-06, 4.830208484357493e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 05:53:16,947] [INFO] [timer.py:260:stop] epoch=0/micro_step=92000/global_step=92000, RunningAvgSamplesPerSec=28.572084325277096, CurrSamplesPerSec=23.80733012822332, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [32]  [2120/2809]  eta: 0:06:32  lr: 0.000005  min_lr: 0.000000  loss: 3.6751 (3.7302)  class_acc: 0.2917 (0.3402)  loss_scale: 65536.0000 (52365.4578)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0830  max mem: 15572
Epoch: [32]  [2130/2809]  eta: 0:06:27  lr: 0.000005  min_lr: 0.000000  loss: 3.9018 (3.7307)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (52427.2623)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0806  max mem: 15572
[2025-01-16 05:53:29,804] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92022
[2025-01-16 05:53:29,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:53:29,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2140/2809]  eta: 0:06:21  lr: 0.000005  min_lr: 0.000000  loss: 3.6741 (3.7297)  class_acc: 0.3333 (0.3401)  loss_scale: 65536.0000 (52381.3545)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.1014  max mem: 15572
Epoch: [32]  [2150/2809]  eta: 0:06:15  lr: 0.000005  min_lr: 0.000000  loss: 3.6464 (3.7292)  class_acc: 0.3750 (0.3403)  loss_scale: 32768.0000 (52290.1720)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.1083  max mem: 15572
Epoch: [32]  [2160/2809]  eta: 0:06:09  lr: 0.000005  min_lr: 0.000000  loss: 3.8033 (3.7295)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (52199.8334)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1400  max mem: 15572
Epoch: [32]  [2170/2809]  eta: 0:06:04  lr: 0.000005  min_lr: 0.000000  loss: 3.7767 (3.7290)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (52110.3270)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1576  max mem: 15572
Epoch: [32]  [2180/2809]  eta: 0:05:58  lr: 0.000005  min_lr: 0.000000  loss: 3.7704 (3.7295)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (52021.6414)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1174  max mem: 15572
Epoch: [32]  [2190/2809]  eta: 0:05:52  lr: 0.000005  min_lr: 0.000000  loss: 3.9279 (3.7297)  class_acc: 0.2917 (0.3406)  loss_scale: 32768.0000 (51933.7654)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0733  max mem: 15572
Epoch: [32]  [2200/2809]  eta: 0:05:46  lr: 0.000005  min_lr: 0.000000  loss: 3.7742 (3.7302)  class_acc: 0.3750 (0.3405)  loss_scale: 32768.0000 (51846.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0995  max mem: 15572
Epoch: [32]  [2210/2809]  eta: 0:05:41  lr: 0.000005  min_lr: 0.000000  loss: 3.6270 (3.7287)  class_acc: 0.3750 (0.3408)  loss_scale: 32768.0000 (51760.3980)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1179  max mem: 15572
Epoch: [32]  [2220/2809]  eta: 0:05:35  lr: 0.000005  min_lr: 0.000000  loss: 3.6270 (3.7281)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (51674.8852)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1482  max mem: 15572
Epoch: [32]  [2230/2809]  eta: 0:05:29  lr: 0.000005  min_lr: 0.000000  loss: 3.7132 (3.7281)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (51590.1390)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.1890  max mem: 15572
Epoch: [32]  [2240/2809]  eta: 0:05:24  lr: 0.000005  min_lr: 0.000000  loss: 3.6804 (3.7274)  class_acc: 0.3750 (0.3413)  loss_scale: 32768.0000 (51506.1490)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.1911  max mem: 15572
Epoch: [32]  [2250/2809]  eta: 0:05:18  lr: 0.000005  min_lr: 0.000000  loss: 3.7518 (3.7279)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (51422.9054)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.1897  max mem: 15572
Epoch: [32]  [2260/2809]  eta: 0:05:13  lr: 0.000005  min_lr: 0.000000  loss: 3.6870 (3.7269)  class_acc: 0.3333 (0.3413)  loss_scale: 32768.0000 (51340.3981)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1122  max mem: 15572
[2025-01-16 05:54:45,345] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:54:45,345] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2270/2809]  eta: 0:05:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7776 (3.7281)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (51374.0484)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1621  max mem: 15572
Epoch: [32]  [2280/2809]  eta: 0:05:02  lr: 0.000005  min_lr: 0.000000  loss: 3.8731 (3.7278)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (51436.1350)  weight_decay: 0.0500 (0.0500)  time: 0.6476  data: 0.2199  max mem: 15572
Epoch: [32]  [2290/2809]  eta: 0:04:56  lr: 0.000005  min_lr: 0.000000  loss: 3.7714 (3.7281)  class_acc: 0.3333 (0.3413)  loss_scale: 65536.0000 (51497.6796)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1032  max mem: 15572
Epoch: [32]  [2300/2809]  eta: 0:04:50  lr: 0.000005  min_lr: 0.000000  loss: 3.6099 (3.7279)  class_acc: 0.3333 (0.3414)  loss_scale: 65536.0000 (51558.6893)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0668  max mem: 15572
Epoch: [32]  [2310/2809]  eta: 0:04:44  lr: 0.000005  min_lr: 0.000000  loss: 3.6883 (3.7282)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (51619.1709)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1609  max mem: 15572
Epoch: [32]  [2320/2809]  eta: 0:04:38  lr: 0.000005  min_lr: 0.000000  loss: 3.7092 (3.7285)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (51679.1314)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1243  max mem: 15572
[2025-01-16 05:55:21,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92216
[2025-01-16 05:55:21,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 05:55:21,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [2330/2809]  eta: 0:04:33  lr: 0.000005  min_lr: 0.000000  loss: 3.8989 (3.7295)  class_acc: 0.2917 (0.3409)  loss_scale: 65536.0000 (51696.4050)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0987  max mem: 15572
Epoch: [32]  [2340/2809]  eta: 0:04:27  lr: 0.000005  min_lr: 0.000000  loss: 3.8867 (3.7297)  class_acc: 0.2917 (0.3410)  loss_scale: 32768.0000 (51615.5489)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1936  max mem: 15572
Epoch: [32]  [2350/2809]  eta: 0:04:21  lr: 0.000005  min_lr: 0.000000  loss: 3.7272 (3.7299)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (51535.3807)  weight_decay: 0.0500 (0.0500)  time: 0.5718  data: 0.1452  max mem: 15572
Epoch: [32]  [2360/2809]  eta: 0:04:16  lr: 0.000005  min_lr: 0.000000  loss: 3.6827 (3.7304)  class_acc: 0.2500 (0.3406)  loss_scale: 32768.0000 (51455.8916)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.1221  max mem: 15572
Epoch: [32]  [2370/2809]  eta: 0:04:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8262 (3.7304)  class_acc: 0.2500 (0.3404)  loss_scale: 32768.0000 (51377.0730)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1732  max mem: 15572
Epoch: [32]  [2380/2809]  eta: 0:04:04  lr: 0.000005  min_lr: 0.000000  loss: 3.7853 (3.7307)  class_acc: 0.2500 (0.3404)  loss_scale: 32768.0000 (51298.9164)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1165  max mem: 15572
Epoch: [32]  [2390/2809]  eta: 0:03:58  lr: 0.000005  min_lr: 0.000000  loss: 3.7407 (3.7311)  class_acc: 0.2500 (0.3401)  loss_scale: 32768.0000 (51221.4136)  weight_decay: 0.0500 (0.0500)  time: 0.4942  data: 0.0561  max mem: 15572
Epoch: [32]  [2400/2809]  eta: 0:03:53  lr: 0.000005  min_lr: 0.000000  loss: 3.6196 (3.7299)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (51144.5564)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1579  max mem: 15572
Epoch: [32]  [2410/2809]  eta: 0:03:47  lr: 0.000005  min_lr: 0.000000  loss: 3.6483 (3.7301)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (51068.3368)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1307  max mem: 15572
Epoch: [32]  [2420/2809]  eta: 0:03:41  lr: 0.000005  min_lr: 0.000000  loss: 3.8364 (3.7307)  class_acc: 0.2917 (0.3402)  loss_scale: 32768.0000 (50992.7468)  weight_decay: 0.0500 (0.0500)  time: 0.4537  data: 0.0219  max mem: 15572
Epoch: [32]  [2430/2809]  eta: 0:03:36  lr: 0.000005  min_lr: 0.000000  loss: 3.8373 (3.7311)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (50917.7787)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.1130  max mem: 15572
Epoch: [32]  [2440/2809]  eta: 0:03:30  lr: 0.000005  min_lr: 0.000000  loss: 3.7173 (3.7312)  class_acc: 0.2917 (0.3401)  loss_scale: 32768.0000 (50843.4248)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1698  max mem: 15572
Epoch: [32]  [2450/2809]  eta: 0:03:24  lr: 0.000005  min_lr: 0.000000  loss: 3.7278 (3.7316)  class_acc: 0.2917 (0.3399)  loss_scale: 32768.0000 (50769.6777)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.1394  max mem: 15572
[2025-01-16 05:56:34,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:56:34,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [2460/2809]  eta: 0:03:19  lr: 0.000005  min_lr: 0.000000  loss: 3.7781 (3.7316)  class_acc: 0.2917 (0.3401)  loss_scale: 32768.0000 (50749.7895)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1058  max mem: 15572
Epoch: [32]  [2470/2809]  eta: 0:03:13  lr: 0.000005  min_lr: 0.000000  loss: 3.7031 (3.7315)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (50809.6285)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.0911  max mem: 15572
Epoch: [32]  [2480/2809]  eta: 0:03:07  lr: 0.000005  min_lr: 0.000000  loss: 3.6629 (3.7314)  class_acc: 0.2917 (0.3399)  loss_scale: 65536.0000 (50868.9851)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1564  max mem: 15572
Epoch: [32]  [2490/2809]  eta: 0:03:02  lr: 0.000005  min_lr: 0.000000  loss: 3.6414 (3.7314)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (50927.8651)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.1996  max mem: 15572
Epoch: [32]  [2500/2809]  eta: 0:02:56  lr: 0.000005  min_lr: 0.000000  loss: 3.4782 (3.7303)  class_acc: 0.3750 (0.3401)  loss_scale: 65536.0000 (50986.2743)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1380  max mem: 15572
Epoch: [32]  [2510/2809]  eta: 0:02:50  lr: 0.000005  min_lr: 0.000000  loss: 3.5434 (3.7303)  class_acc: 0.4167 (0.3401)  loss_scale: 65536.0000 (51044.2182)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1051  max mem: 15572
Epoch: [32]  [2520/2809]  eta: 0:02:45  lr: 0.000005  min_lr: 0.000000  loss: 3.6464 (3.7308)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (51101.7025)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1534  max mem: 15572
Epoch: [32]  [2530/2809]  eta: 0:02:39  lr: 0.000005  min_lr: 0.000000  loss: 3.5850 (3.7297)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (51158.7325)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0975  max mem: 15572
Epoch: [32]  [2540/2809]  eta: 0:02:33  lr: 0.000005  min_lr: 0.000000  loss: 3.6362 (3.7295)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (51215.3137)  weight_decay: 0.0500 (0.0500)  time: 0.4631  data: 0.0097  max mem: 15572
Epoch: [32]  [2550/2809]  eta: 0:02:27  lr: 0.000005  min_lr: 0.000000  loss: 3.6478 (3.7289)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (51271.4512)  weight_decay: 0.0500 (0.0500)  time: 0.4986  data: 0.0505  max mem: 15572
Epoch: [32]  [2560/2809]  eta: 0:02:21  lr: 0.000005  min_lr: 0.000000  loss: 3.5724 (3.7289)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (51327.1503)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1147  max mem: 15572
Epoch: [32]  [2570/2809]  eta: 0:02:16  lr: 0.000005  min_lr: 0.000000  loss: 3.8256 (3.7292)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51382.4162)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1129  max mem: 15572
Epoch: [32]  [2580/2809]  eta: 0:02:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8783 (3.7294)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (51437.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0942  max mem: 15572
[2025-01-16 05:57:46,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:57:46,666] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:57:47,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92474
[2025-01-16 05:57:47,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:57:47,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2590/2809]  eta: 0:02:04  lr: 0.000005  min_lr: 0.000000  loss: 3.9115 (3.7296)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (51516.9618)  weight_decay: 0.0500 (0.0500)  time: 0.5502  data: 0.1002  max mem: 15572
Epoch: [32]  [2600/2809]  eta: 0:01:59  lr: 0.000005  min_lr: 0.000000  loss: 3.7074 (3.7293)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (51570.8604)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0816  max mem: 15572
Epoch: [32]  [2610/2809]  eta: 0:01:53  lr: 0.000005  min_lr: 0.000000  loss: 3.6093 (3.7289)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (51624.3462)  weight_decay: 0.0500 (0.0500)  time: 0.5165  data: 0.0878  max mem: 15572
Epoch: [32]  [2620/2809]  eta: 0:01:47  lr: 0.000005  min_lr: 0.000000  loss: 3.8134 (3.7289)  class_acc: 0.3333 (0.3411)  loss_scale: 65536.0000 (51677.4239)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1197  max mem: 15572
Epoch: [32]  [2630/2809]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000000  loss: 3.9474 (3.7297)  class_acc: 0.3333 (0.3410)  loss_scale: 65536.0000 (51730.0981)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1302  max mem: 15572
Epoch: [32]  [2640/2809]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 3.8486 (3.7291)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (51782.3733)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1627  max mem: 15572
Epoch: [32]  [2650/2809]  eta: 0:01:30  lr: 0.000005  min_lr: 0.000000  loss: 3.8486 (3.7299)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (51834.2542)  weight_decay: 0.0500 (0.0500)  time: 0.5650  data: 0.1005  max mem: 15572
Epoch: [32]  [2660/2809]  eta: 0:01:24  lr: 0.000005  min_lr: 0.000000  loss: 3.8493 (3.7298)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (51885.7452)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0937  max mem: 15572
Epoch: [32]  [2670/2809]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000000  loss: 3.6574 (3.7295)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (51936.8506)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.2134  max mem: 15572
Epoch: [32]  [2680/2809]  eta: 0:01:13  lr: 0.000005  min_lr: 0.000000  loss: 3.8917 (3.7310)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (51987.5748)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.1207  max mem: 15572
Epoch: [32]  [2690/2809]  eta: 0:01:07  lr: 0.000005  min_lr: 0.000000  loss: 3.7981 (3.7303)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (52037.9220)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0493  max mem: 15572
Epoch: [32]  [2700/2809]  eta: 0:01:02  lr: 0.000005  min_lr: 0.000000  loss: 3.6074 (3.7303)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52087.8963)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1429  max mem: 15572
Epoch: [32]  [2710/2809]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 3.6074 (3.7300)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52137.5020)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1293  max mem: 15572
[2025-01-16 05:59:00,199] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 05:59:00,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 05:59:01,248] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92604
[2025-01-16 05:59:01,249] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 05:59:01,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [2720/2809]  eta: 0:00:50  lr: 0.000005  min_lr: 0.000000  loss: 3.5478 (3.7299)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (52210.8284)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1033  max mem: 15572
Epoch: [32]  [2730/2809]  eta: 0:00:45  lr: 0.000005  min_lr: 0.000000  loss: 3.6370 (3.7301)  class_acc: 0.2917 (0.3406)  loss_scale: 65536.0000 (52259.6207)  weight_decay: 0.0500 (0.0500)  time: 0.5764  data: 0.1233  max mem: 15572
Epoch: [32]  [2740/2809]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000000  loss: 3.6689 (3.7301)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (52308.0569)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.1933  max mem: 15572
Epoch: [32]  [2750/2809]  eta: 0:00:33  lr: 0.000005  min_lr: 0.000000  loss: 3.7152 (3.7302)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (52356.1410)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.1745  max mem: 15572
Epoch: [32]  [2760/2809]  eta: 0:00:27  lr: 0.000005  min_lr: 0.000000  loss: 3.7677 (3.7307)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (52403.8769)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1442  max mem: 15572
Epoch: [32]  [2770/2809]  eta: 0:00:22  lr: 0.000005  min_lr: 0.000000  loss: 3.9563 (3.7312)  class_acc: 0.2500 (0.3403)  loss_scale: 65536.0000 (52451.2681)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1210  max mem: 15572
Epoch: [32]  [2780/2809]  eta: 0:00:16  lr: 0.000005  min_lr: 0.000000  loss: 3.9563 (3.7319)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (52498.3186)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0738  max mem: 15572
Epoch: [32]  [2790/2809]  eta: 0:00:10  lr: 0.000005  min_lr: 0.000000  loss: 3.8782 (3.7314)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (52545.0319)  weight_decay: 0.0500 (0.0500)  time: 0.5364  data: 0.0668  max mem: 15572
Epoch: [32]  [2800/2809]  eta: 0:00:05  lr: 0.000005  min_lr: 0.000000  loss: 3.3494 (3.7311)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (52591.4116)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0862  max mem: 15572
Epoch: [32]  [2808/2809]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 3.6842 (3.7316)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (52628.2777)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0806  max mem: 15572
Epoch: [32] Total time: 0:26:41 (0.5701 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 3.6842 (3.7316)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (52628.2777)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:26:46  loss: 0.3740 (0.3740)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.9048  data: 5.7274  max mem: 15572
Val:  [ 10/272]  eta: 0:03:16  loss: 2.2951 (2.2379)  acc1: 50.0000 (45.9596)  acc5: 77.7778 (74.2424)  time: 0.7502  data: 0.5642  max mem: 15572
Val:  [ 20/272]  eta: 0:02:13  loss: 2.2849 (2.2678)  acc1: 50.0000 (48.1481)  acc5: 72.2222 (74.6032)  time: 0.2625  data: 0.0796  max mem: 15572
Val:  [ 30/272]  eta: 0:01:55  loss: 2.2849 (2.3626)  acc1: 50.0000 (44.6237)  acc5: 72.2222 (73.8351)  time: 0.3291  data: 0.1406  max mem: 15572
Val:  [ 40/272]  eta: 0:01:40  loss: 2.5169 (2.4230)  acc1: 27.7778 (41.4634)  acc5: 77.7778 (73.5772)  time: 0.3291  data: 0.1345  max mem: 15572
Val:  [ 50/272]  eta: 0:01:32  loss: 2.4234 (2.3509)  acc1: 33.3333 (43.0283)  acc5: 77.7778 (74.9455)  time: 0.3154  data: 0.1236  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 1.4871 (2.2456)  acc1: 61.1111 (45.9016)  acc5: 83.3333 (75.7741)  time: 0.3320  data: 0.1327  max mem: 15572
Val:  [ 70/272]  eta: 0:01:17  loss: 1.4884 (2.1671)  acc1: 66.6667 (48.2786)  acc5: 83.3333 (76.9171)  time: 0.2999  data: 0.1030  max mem: 15572
Val:  [ 80/272]  eta: 0:01:11  loss: 1.7921 (2.1785)  acc1: 55.5556 (48.1481)  acc5: 77.7778 (76.7490)  time: 0.2828  data: 0.0884  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.1161 (2.1854)  acc1: 50.0000 (48.2906)  acc5: 77.7778 (77.5336)  time: 0.2884  data: 0.0957  max mem: 15572
Val:  [100/272]  eta: 0:01:00  loss: 2.0794 (2.2080)  acc1: 50.0000 (47.7448)  acc5: 83.3333 (77.1727)  time: 0.2851  data: 0.0924  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.3937 (2.2821)  acc1: 22.2222 (45.5956)  acc5: 72.2222 (76.2763)  time: 0.2897  data: 0.1026  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.8326 (2.3177)  acc1: 22.2222 (44.8118)  acc5: 72.2222 (75.8494)  time: 0.2861  data: 0.1154  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.0711 (2.2827)  acc1: 44.4444 (45.6319)  acc5: 83.3333 (76.5479)  time: 0.3157  data: 0.1449  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.7139 (2.2748)  acc1: 55.5556 (46.0599)  acc5: 83.3333 (76.4381)  time: 0.3467  data: 0.1704  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.3855 (2.2804)  acc1: 38.8889 (45.5850)  acc5: 77.7778 (76.6004)  time: 0.3166  data: 0.1364  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2921 (2.2688)  acc1: 44.4444 (46.2733)  acc5: 77.7778 (76.8461)  time: 0.3077  data: 0.1169  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.2921 (2.2865)  acc1: 44.4444 (45.8415)  acc5: 72.2222 (76.3808)  time: 0.3437  data: 0.1490  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.2882 (2.2766)  acc1: 38.8889 (45.7336)  acc5: 77.7778 (76.7342)  time: 0.3385  data: 0.1365  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.2882 (2.3294)  acc1: 33.3333 (44.3281)  acc5: 72.2222 (75.3345)  time: 0.3334  data: 0.1217  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.5826 (2.3391)  acc1: 33.3333 (44.0575)  acc5: 66.6667 (75.0415)  time: 0.3404  data: 0.1427  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.1346 (2.3427)  acc1: 44.4444 (44.2338)  acc5: 77.7778 (74.9605)  time: 0.2933  data: 0.0933  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1346 (2.3297)  acc1: 50.0000 (44.4444)  acc5: 83.3333 (75.2388)  time: 0.2672  data: 0.0556  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7629 (2.2983)  acc1: 61.1111 (45.5267)  acc5: 83.3333 (75.6133)  time: 0.3065  data: 0.1001  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6085 (2.2840)  acc1: 61.1111 (45.7123)  acc5: 83.3333 (75.7722)  time: 0.2692  data: 0.0699  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.3339 (2.2949)  acc1: 44.4444 (45.0642)  acc5: 77.7778 (75.8079)  time: 0.1867  data: 0.0005  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1891 (2.2383)  acc1: 72.2222 (46.7007)  acc5: 88.8889 (76.5219)  time: 0.1691  data: 0.0004  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3659 (2.2344)  acc1: 72.2222 (46.7610)  acc5: 88.8889 (76.6913)  time: 0.1495  data: 0.0002  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3659 (2.2395)  acc1: 66.6667 (46.7336)  acc5: 88.8889 (76.6537)  time: 0.1423  data: 0.0002  max mem: 15572
Val: Total time: 0:01:24 (0.3090 s / it)
* Acc@1 46.734 Acc@5 76.654 loss 2.240
Accuracy of the network on the 4883 val videos: 46.7%
Max accuracy: 46.77%
Epoch: [33]  [   0/2809]  eta: 7:22:13  lr: 0.000005  min_lr: 0.000000  loss: 3.5407 (3.5407)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 9.4459  data: 8.9348  max mem: 15572
Epoch: [33]  [  10/2809]  eta: 1:10:38  lr: 0.000005  min_lr: 0.000000  loss: 3.5954 (3.7384)  class_acc: 0.3750 (0.3409)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5142  data: 1.0494  max mem: 15572
Epoch: [33]  [  20/2809]  eta: 0:54:17  lr: 0.000005  min_lr: 0.000000  loss: 3.7219 (3.8209)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7541  data: 0.2882  max mem: 15572
Epoch: [33]  [  30/2809]  eta: 0:46:23  lr: 0.000005  min_lr: 0.000000  loss: 3.7964 (3.8492)  class_acc: 0.3333 (0.3159)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7199  data: 0.2451  max mem: 15572
[2025-01-16 06:01:50,233] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:01:50,234] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:01:50,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92734
[2025-01-16 06:01:50,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:01:50,648] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [  40/2809]  eta: 0:43:21  lr: 0.000005  min_lr: 0.000000  loss: 3.7964 (3.7899)  class_acc: 0.3333 (0.3252)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.6996  data: 0.2392  max mem: 15572
Epoch: [33]  [  50/2809]  eta: 0:40:23  lr: 0.000004  min_lr: 0.000000  loss: 3.5773 (3.7804)  class_acc: 0.3333 (0.3292)  loss_scale: 65536.0000 (66821.0196)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.2357  max mem: 15572
Epoch: [33]  [  60/2809]  eta: 0:38:16  lr: 0.000004  min_lr: 0.000000  loss: 3.7027 (3.7963)  class_acc: 0.2917 (0.3238)  loss_scale: 65536.0000 (66610.3607)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1561  max mem: 15572
Epoch: [33]  [  70/2809]  eta: 0:36:55  lr: 0.000004  min_lr: 0.000000  loss: 3.8486 (3.7741)  class_acc: 0.3333 (0.3351)  loss_scale: 65536.0000 (66459.0423)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1592  max mem: 15572
Epoch: [33]  [  80/2809]  eta: 0:36:32  lr: 0.000004  min_lr: 0.000000  loss: 3.7626 (3.7662)  class_acc: 0.3750 (0.3349)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  time: 0.7055  data: 0.2377  max mem: 15572
Epoch: [33]  [  90/2809]  eta: 0:35:36  lr: 0.000004  min_lr: 0.000000  loss: 3.6986 (3.7608)  class_acc: 0.2500 (0.3338)  loss_scale: 65536.0000 (66256.1758)  weight_decay: 0.0500 (0.0500)  time: 0.7033  data: 0.2490  max mem: 15572
Epoch: [33]  [ 100/2809]  eta: 0:34:46  lr: 0.000004  min_lr: 0.000000  loss: 3.6986 (3.7467)  class_acc: 0.2917 (0.3350)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.1800  max mem: 15572
Epoch: [33]  [ 110/2809]  eta: 0:33:26  lr: 0.000004  min_lr: 0.000000  loss: 3.6970 (3.7439)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (66126.4144)  weight_decay: 0.0500 (0.0500)  time: 0.5524  data: 0.0988  max mem: 15572
Epoch: [33]  [ 120/2809]  eta: 0:32:15  lr: 0.000004  min_lr: 0.000000  loss: 3.5846 (3.7364)  class_acc: 0.3333 (0.3357)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.4645  data: 0.0202  max mem: 15572
Epoch: [33]  [ 130/2809]  eta: 0:31:12  lr: 0.000004  min_lr: 0.000000  loss: 3.7990 (3.7404)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (66036.2748)  weight_decay: 0.0500 (0.0500)  time: 0.4511  data: 0.0036  max mem: 15572
Epoch: [33]  [ 140/2809]  eta: 0:30:26  lr: 0.000004  min_lr: 0.000000  loss: 3.7990 (3.7330)  class_acc: 0.3333 (0.3363)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  time: 0.4687  data: 0.0300  max mem: 15572
Epoch: [33]  [ 150/2809]  eta: 0:30:10  lr: 0.000004  min_lr: 0.000000  loss: 3.7330 (3.7367)  class_acc: 0.2917 (0.3350)  loss_scale: 65536.0000 (65970.0132)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1166  max mem: 15572
Epoch: [33]  [ 160/2809]  eta: 0:29:39  lr: 0.000004  min_lr: 0.000000  loss: 3.6907 (3.7327)  class_acc: 0.3333 (0.3375)  loss_scale: 65536.0000 (65943.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1239  max mem: 15572
[2025-01-16 06:03:09,743] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:03:09,743] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:03:10,599] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92865
[2025-01-16 06:03:10,599] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:03:10,599] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 170/2809]  eta: 0:29:31  lr: 0.000004  min_lr: 0.000000  loss: 3.8134 (3.7315)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (66685.7544)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.1482  max mem: 15572
Epoch: [33]  [ 180/2809]  eta: 0:29:03  lr: 0.000004  min_lr: 0.000000  loss: 3.8627 (3.7277)  class_acc: 0.3750 (0.3398)  loss_scale: 65536.0000 (66622.2320)  weight_decay: 0.0500 (0.0500)  time: 0.5939  data: 0.1521  max mem: 15572
Epoch: [33]  [ 190/2809]  eta: 0:28:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7325 (3.7280)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (66565.3613)  weight_decay: 0.0500 (0.0500)  time: 0.5915  data: 0.1581  max mem: 15572
Epoch: [33]  [ 200/2809]  eta: 0:28:40  lr: 0.000004  min_lr: 0.000000  loss: 3.7657 (3.7270)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (66514.1493)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.1885  max mem: 15572
Epoch: [33]  [ 210/2809]  eta: 0:28:20  lr: 0.000004  min_lr: 0.000000  loss: 3.7001 (3.7155)  class_acc: 0.3750 (0.3432)  loss_scale: 65536.0000 (66467.7915)  weight_decay: 0.0500 (0.0500)  time: 0.5698  data: 0.1303  max mem: 15572
Epoch: [33]  [ 220/2809]  eta: 0:28:04  lr: 0.000004  min_lr: 0.000000  loss: 3.6738 (3.7166)  class_acc: 0.4167 (0.3448)  loss_scale: 65536.0000 (66425.6290)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1185  max mem: 15572
[2025-01-16 06:03:45,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 92926
[2025-01-16 06:03:45,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:03:45,530] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 230/2809]  eta: 0:27:46  lr: 0.000004  min_lr: 0.000000  loss: 3.6841 (3.7193)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (66103.4113)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1129  max mem: 15572
Epoch: [33]  [ 240/2809]  eta: 0:27:29  lr: 0.000004  min_lr: 0.000000  loss: 3.7865 (3.7196)  class_acc: 0.2917 (0.3439)  loss_scale: 32768.0000 (64720.1992)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.1069  max mem: 15572
Epoch: [33]  [ 250/2809]  eta: 0:27:15  lr: 0.000004  min_lr: 0.000000  loss: 3.9453 (3.7331)  class_acc: 0.2500 (0.3420)  loss_scale: 32768.0000 (63447.2032)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1117  max mem: 15572
Epoch: [33]  [ 260/2809]  eta: 0:26:56  lr: 0.000004  min_lr: 0.000000  loss: 3.9751 (3.7347)  class_acc: 0.2500 (0.3421)  loss_scale: 32768.0000 (62271.7548)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1025  max mem: 15572
Epoch: [33]  [ 270/2809]  eta: 0:26:51  lr: 0.000004  min_lr: 0.000000  loss: 3.9135 (3.7382)  class_acc: 0.2500 (0.3409)  loss_scale: 32768.0000 (61183.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1564  max mem: 15572
Epoch: [33]  [ 280/2809]  eta: 0:26:42  lr: 0.000004  min_lr: 0.000000  loss: 3.8433 (3.7329)  class_acc: 0.2917 (0.3427)  loss_scale: 32768.0000 (60171.8434)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1961  max mem: 15572
Epoch: [33]  [ 290/2809]  eta: 0:26:22  lr: 0.000004  min_lr: 0.000000  loss: 3.8069 (3.7369)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (59230.1306)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.1135  max mem: 15572
Epoch: [33]  [ 300/2809]  eta: 0:26:09  lr: 0.000004  min_lr: 0.000000  loss: 3.7762 (3.7384)  class_acc: 0.2500 (0.3393)  loss_scale: 32768.0000 (58350.9900)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0834  max mem: 15572
[2025-01-16 06:04:26,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=93000, skipped=618, lr=[4.25016877310702e-08, 4.25016877310702e-08, 6.071669675867172e-08, 6.071669675867172e-08, 8.67381382266739e-08, 8.67381382266739e-08, 1.2391162603810558e-07, 1.2391162603810558e-07, 1.770166086258651e-07, 1.770166086258651e-07, 2.528808694655216e-07, 2.528808694655216e-07, 3.6125838495074515e-07, 3.6125838495074515e-07, 5.160834070724932e-07, 5.160834070724932e-07, 7.372620101035616e-07, 7.372620101035616e-07, 1.053231443005088e-06, 1.053231443005088e-06, 1.5046163471501259e-06, 1.5046163471501259e-06, 2.14945192450018e-06, 2.14945192450018e-06, 3.070645606428829e-06, 3.070645606428829e-06, 4.386636580612613e-06, 4.386636580612613e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:04:26,991] [INFO] [timer.py:260:stop] epoch=0/micro_step=93000/global_step=93000, RunningAvgSamplesPerSec=28.57150751143903, CurrSamplesPerSec=31.459718128154933, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [ 310/2809]  eta: 0:25:57  lr: 0.000004  min_lr: 0.000000  loss: 3.7762 (3.7405)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (57528.3859)  weight_decay: 0.0500 (0.0500)  time: 0.5504  data: 0.1103  max mem: 15572
Epoch: [33]  [ 320/2809]  eta: 0:25:41  lr: 0.000004  min_lr: 0.000000  loss: 3.8988 (3.7490)  class_acc: 0.2917 (0.3368)  loss_scale: 32768.0000 (56757.0343)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0839  max mem: 15572
Epoch: [33]  [ 330/2809]  eta: 0:25:33  lr: 0.000004  min_lr: 0.000000  loss: 3.9162 (3.7517)  class_acc: 0.2917 (0.3352)  loss_scale: 32768.0000 (56032.2900)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1006  max mem: 15572
Epoch: [33]  [ 340/2809]  eta: 0:25:28  lr: 0.000004  min_lr: 0.000000  loss: 3.7998 (3.7503)  class_acc: 0.3333 (0.3355)  loss_scale: 32768.0000 (55350.0528)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1698  max mem: 15572
Epoch: [33]  [ 350/2809]  eta: 0:25:15  lr: 0.000004  min_lr: 0.000000  loss: 3.7016 (3.7491)  class_acc: 0.3333 (0.3350)  loss_scale: 32768.0000 (54706.6895)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1432  max mem: 15572
[2025-01-16 06:04:57,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:04:57,553] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 360/2809]  eta: 0:25:03  lr: 0.000004  min_lr: 0.000000  loss: 3.7260 (3.7532)  class_acc: 0.3333 (0.3345)  loss_scale: 32768.0000 (54371.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5275  data: 0.0966  max mem: 15572
Epoch: [33]  [ 370/2809]  eta: 0:25:00  lr: 0.000004  min_lr: 0.000000  loss: 3.6485 (3.7493)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.5951  data: 0.1622  max mem: 15572
Epoch: [33]  [ 380/2809]  eta: 0:24:45  lr: 0.000004  min_lr: 0.000000  loss: 3.5313 (3.7489)  class_acc: 0.3333 (0.3342)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1242  max mem: 15572
Epoch: [33]  [ 390/2809]  eta: 0:24:38  lr: 0.000004  min_lr: 0.000000  loss: 3.5265 (3.7442)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0965  max mem: 15572
Epoch: [33]  [ 400/2809]  eta: 0:24:28  lr: 0.000004  min_lr: 0.000000  loss: 3.7428 (3.7458)  class_acc: 0.2917 (0.3330)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5739  data: 0.1285  max mem: 15572
Epoch: [33]  [ 410/2809]  eta: 0:24:24  lr: 0.000004  min_lr: 0.000000  loss: 3.8353 (3.7455)  class_acc: 0.2917 (0.3337)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1346  max mem: 15572
Epoch: [33]  [ 420/2809]  eta: 0:24:15  lr: 0.000004  min_lr: 0.000000  loss: 3.9305 (3.7479)  class_acc: 0.3333 (0.3338)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1492  max mem: 15572
Epoch: [33]  [ 430/2809]  eta: 0:24:11  lr: 0.000004  min_lr: 0.000000  loss: 3.8767 (3.7481)  class_acc: 0.3333 (0.3337)  loss_scale: 65536.0000 (56184.5754)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1599  max mem: 15572
Epoch: [33]  [ 440/2809]  eta: 0:24:04  lr: 0.000004  min_lr: 0.000000  loss: 3.7637 (3.7521)  class_acc: 0.2917 (0.3318)  loss_scale: 65536.0000 (56396.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.1616  max mem: 15572
Epoch: [33]  [ 450/2809]  eta: 0:23:56  lr: 0.000004  min_lr: 0.000000  loss: 3.8581 (3.7531)  class_acc: 0.2917 (0.3312)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1177  max mem: 15572
Epoch: [33]  [ 460/2809]  eta: 0:23:44  lr: 0.000004  min_lr: 0.000000  loss: 3.7552 (3.7512)  class_acc: 0.2917 (0.3317)  loss_scale: 65536.0000 (56793.1280)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0860  max mem: 15572
[2025-01-16 06:05:59,609] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93164
[2025-01-16 06:05:59,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:05:59,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [ 470/2809]  eta: 0:23:36  lr: 0.000004  min_lr: 0.000000  loss: 3.6406 (3.7478)  class_acc: 0.3750 (0.3329)  loss_scale: 65536.0000 (56700.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0758  max mem: 15572
Epoch: [33]  [ 480/2809]  eta: 0:23:28  lr: 0.000004  min_lr: 0.000000  loss: 3.5872 (3.7423)  class_acc: 0.3750 (0.3347)  loss_scale: 32768.0000 (56202.9106)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0972  max mem: 15572
Epoch: [33]  [ 490/2809]  eta: 0:23:19  lr: 0.000004  min_lr: 0.000000  loss: 3.6240 (3.7433)  class_acc: 0.3750 (0.3349)  loss_scale: 32768.0000 (55725.6212)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1080  max mem: 15572
Epoch: [33]  [ 500/2809]  eta: 0:23:10  lr: 0.000004  min_lr: 0.000000  loss: 3.7623 (3.7377)  class_acc: 0.3333 (0.3352)  loss_scale: 32768.0000 (55267.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1153  max mem: 15572
Epoch: [33]  [ 510/2809]  eta: 0:23:08  lr: 0.000004  min_lr: 0.000000  loss: 3.5977 (3.7379)  class_acc: 0.3333 (0.3349)  loss_scale: 32768.0000 (54827.0841)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.1939  max mem: 15572
Epoch: [33]  [ 520/2809]  eta: 0:22:58  lr: 0.000004  min_lr: 0.000000  loss: 3.7716 (3.7390)  class_acc: 0.2917 (0.3345)  loss_scale: 32768.0000 (54403.6852)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1525  max mem: 15572
Epoch: [33]  [ 530/2809]  eta: 0:22:51  lr: 0.000004  min_lr: 0.000000  loss: 3.3751 (3.7345)  class_acc: 0.3333 (0.3347)  loss_scale: 32768.0000 (53996.2335)  weight_decay: 0.0500 (0.0500)  time: 0.5440  data: 0.0835  max mem: 15572
Epoch: [33]  [ 540/2809]  eta: 0:22:46  lr: 0.000004  min_lr: 0.000000  loss: 3.6855 (3.7332)  class_acc: 0.3750 (0.3351)  loss_scale: 32768.0000 (53603.8447)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1569  max mem: 15572
Epoch: [33]  [ 550/2809]  eta: 0:22:38  lr: 0.000004  min_lr: 0.000000  loss: 3.7409 (3.7323)  class_acc: 0.3750 (0.3353)  loss_scale: 32768.0000 (53225.6987)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1442  max mem: 15572
Epoch: [33]  [ 560/2809]  eta: 0:22:30  lr: 0.000004  min_lr: 0.000000  loss: 3.7434 (3.7327)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (52861.0339)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.1137  max mem: 15572
Epoch: [33]  [ 570/2809]  eta: 0:22:25  lr: 0.000004  min_lr: 0.000000  loss: 3.7441 (3.7335)  class_acc: 0.2917 (0.3338)  loss_scale: 32768.0000 (52509.1419)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1353  max mem: 15572
Epoch: [33]  [ 580/2809]  eta: 0:22:12  lr: 0.000004  min_lr: 0.000000  loss: 3.7427 (3.7324)  class_acc: 0.3333 (0.3349)  loss_scale: 32768.0000 (52169.3632)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0762  max mem: 15572
Epoch: [33]  [ 590/2809]  eta: 0:22:03  lr: 0.000004  min_lr: 0.000000  loss: 3.6793 (3.7315)  class_acc: 0.3750 (0.3356)  loss_scale: 32768.0000 (51841.0829)  weight_decay: 0.0500 (0.0500)  time: 0.4730  data: 0.0287  max mem: 15572
[2025-01-16 06:07:11,963] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:07:11,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 600/2809]  eta: 0:21:53  lr: 0.000004  min_lr: 0.000000  loss: 3.3104 (3.7281)  class_acc: 0.3333 (0.3362)  loss_scale: 32768.0000 (51796.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0571  max mem: 15572
Epoch: [33]  [ 610/2809]  eta: 0:21:46  lr: 0.000004  min_lr: 0.000000  loss: 3.2954 (3.7277)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (52021.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5326  data: 0.0835  max mem: 15572
Epoch: [33]  [ 620/2809]  eta: 0:21:43  lr: 0.000004  min_lr: 0.000000  loss: 3.6498 (3.7247)  class_acc: 0.4167 (0.3371)  loss_scale: 65536.0000 (52238.8406)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1601  max mem: 15572
Epoch: [33]  [ 630/2809]  eta: 0:21:33  lr: 0.000004  min_lr: 0.000000  loss: 3.5993 (3.7224)  class_acc: 0.4167 (0.3376)  loss_scale: 65536.0000 (52449.5721)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1294  max mem: 15572
Epoch: [33]  [ 640/2809]  eta: 0:21:26  lr: 0.000004  min_lr: 0.000000  loss: 3.8913 (3.7250)  class_acc: 0.3333 (0.3371)  loss_scale: 65536.0000 (52653.7285)  weight_decay: 0.0500 (0.0500)  time: 0.5198  data: 0.0809  max mem: 15572
Epoch: [33]  [ 650/2809]  eta: 0:21:21  lr: 0.000004  min_lr: 0.000000  loss: 3.8913 (3.7272)  class_acc: 0.2917 (0.3370)  loss_scale: 65536.0000 (52851.6129)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1341  max mem: 15572
Epoch: [33]  [ 660/2809]  eta: 0:21:11  lr: 0.000004  min_lr: 0.000000  loss: 3.7205 (3.7262)  class_acc: 0.2917 (0.3369)  loss_scale: 65536.0000 (53043.5098)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1021  max mem: 15572
Epoch: [33]  [ 670/2809]  eta: 0:21:06  lr: 0.000004  min_lr: 0.000000  loss: 3.6492 (3.7263)  class_acc: 0.2917 (0.3366)  loss_scale: 65536.0000 (53229.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1164  max mem: 15572
Epoch: [33]  [ 680/2809]  eta: 0:20:58  lr: 0.000004  min_lr: 0.000000  loss: 3.7788 (3.7275)  class_acc: 0.3333 (0.3372)  loss_scale: 65536.0000 (53410.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1443  max mem: 15572
Epoch: [33]  [ 690/2809]  eta: 0:20:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7215 (3.7271)  class_acc: 0.3333 (0.3373)  loss_scale: 65536.0000 (53585.8755)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.1773  max mem: 15572
Epoch: [33]  [ 700/2809]  eta: 0:20:45  lr: 0.000004  min_lr: 0.000000  loss: 3.6807 (3.7256)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (53756.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1250  max mem: 15572
Epoch: [33]  [ 710/2809]  eta: 0:20:38  lr: 0.000004  min_lr: 0.000000  loss: 3.6500 (3.7251)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (53922.0253)  weight_decay: 0.0500 (0.0500)  time: 0.4862  data: 0.0423  max mem: 15572
Epoch: [33]  [ 720/2809]  eta: 0:20:33  lr: 0.000004  min_lr: 0.000000  loss: 3.6869 (3.7249)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (54083.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1372  max mem: 15572
[2025-01-16 06:08:25,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:08:25,807] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:08:26,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93422
[2025-01-16 06:08:26,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:08:26,272] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 730/2809]  eta: 0:20:27  lr: 0.000004  min_lr: 0.000000  loss: 3.5954 (3.7227)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (54329.4337)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.1592  max mem: 15572
Epoch: [33]  [ 740/2809]  eta: 0:20:21  lr: 0.000004  min_lr: 0.000000  loss: 3.6568 (3.7238)  class_acc: 0.3333 (0.3382)  loss_scale: 65536.0000 (54480.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1479  max mem: 15572
Epoch: [33]  [ 750/2809]  eta: 0:20:16  lr: 0.000004  min_lr: 0.000000  loss: 3.7955 (3.7265)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (54627.8775)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.1834  max mem: 15572
Epoch: [33]  [ 760/2809]  eta: 0:20:07  lr: 0.000004  min_lr: 0.000000  loss: 3.7880 (3.7271)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (54771.2168)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1143  max mem: 15572
Epoch: [33]  [ 770/2809]  eta: 0:20:03  lr: 0.000004  min_lr: 0.000000  loss: 3.6652 (3.7263)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (54910.8379)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1073  max mem: 15572
Epoch: [33]  [ 780/2809]  eta: 0:19:56  lr: 0.000004  min_lr: 0.000000  loss: 3.7604 (3.7277)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (55046.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1383  max mem: 15572
Epoch: [33]  [ 790/2809]  eta: 0:19:47  lr: 0.000004  min_lr: 0.000000  loss: 3.7604 (3.7267)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (55179.4893)  weight_decay: 0.0500 (0.0500)  time: 0.5053  data: 0.0612  max mem: 15572
Epoch: [33]  [ 800/2809]  eta: 0:19:38  lr: 0.000004  min_lr: 0.000000  loss: 3.7266 (3.7265)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (55308.7840)  weight_decay: 0.0500 (0.0500)  time: 0.4714  data: 0.0281  max mem: 15572
Epoch: [33]  [ 810/2809]  eta: 0:19:32  lr: 0.000004  min_lr: 0.000000  loss: 3.7497 (3.7266)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (55434.8903)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0795  max mem: 15572
Epoch: [33]  [ 820/2809]  eta: 0:19:26  lr: 0.000004  min_lr: 0.000000  loss: 3.6205 (3.7242)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (55557.9245)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.1186  max mem: 15572
Epoch: [33]  [ 830/2809]  eta: 0:19:20  lr: 0.000004  min_lr: 0.000000  loss: 3.8146 (3.7260)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (55677.9976)  weight_decay: 0.0500 (0.0500)  time: 0.5796  data: 0.1212  max mem: 15572
Epoch: [33]  [ 840/2809]  eta: 0:19:15  lr: 0.000004  min_lr: 0.000000  loss: 3.8488 (3.7272)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (55795.2152)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1328  max mem: 15572
Epoch: [33]  [ 850/2809]  eta: 0:19:10  lr: 0.000004  min_lr: 0.000000  loss: 3.7622 (3.7262)  class_acc: 0.2917 (0.3374)  loss_scale: 65536.0000 (55909.6780)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.1432  max mem: 15572
[2025-01-16 06:09:38,378] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:09:38,378] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:09:38,782] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93552
[2025-01-16 06:09:38,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:09:38,782] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 860/2809]  eta: 0:19:01  lr: 0.000004  min_lr: 0.000000  loss: 3.5287 (3.7250)  class_acc: 0.3750 (0.3379)  loss_scale: 65536.0000 (56097.5981)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0801  max mem: 15572
Epoch: [33]  [ 870/2809]  eta: 0:18:54  lr: 0.000004  min_lr: 0.000000  loss: 3.9055 (3.7249)  class_acc: 0.3750 (0.3385)  loss_scale: 65536.0000 (56205.9610)  weight_decay: 0.0500 (0.0500)  time: 0.4907  data: 0.0501  max mem: 15572
Epoch: [33]  [ 880/2809]  eta: 0:18:46  lr: 0.000004  min_lr: 0.000000  loss: 3.7948 (3.7254)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56311.8638)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0822  max mem: 15572
Epoch: [33]  [ 890/2809]  eta: 0:18:41  lr: 0.000004  min_lr: 0.000000  loss: 3.7910 (3.7253)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (56415.3895)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1064  max mem: 15572
Epoch: [33]  [ 900/2809]  eta: 0:18:33  lr: 0.000004  min_lr: 0.000000  loss: 3.6683 (3.7237)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (56516.6171)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.0886  max mem: 15572
Epoch: [33]  [ 910/2809]  eta: 0:18:28  lr: 0.000004  min_lr: 0.000000  loss: 3.5740 (3.7217)  class_acc: 0.3750 (0.3396)  loss_scale: 65536.0000 (56615.6224)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.0671  max mem: 15572
Epoch: [33]  [ 920/2809]  eta: 0:18:21  lr: 0.000004  min_lr: 0.000000  loss: 3.6324 (3.7218)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (56712.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1028  max mem: 15572
Epoch: [33]  [ 930/2809]  eta: 0:18:13  lr: 0.000004  min_lr: 0.000000  loss: 3.7330 (3.7218)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (56807.2524)  weight_decay: 0.0500 (0.0500)  time: 0.5119  data: 0.0690  max mem: 15572
Epoch: [33]  [ 940/2809]  eta: 0:18:10  lr: 0.000004  min_lr: 0.000000  loss: 3.8051 (3.7226)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (56900.0128)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1392  max mem: 15572
Epoch: [33]  [ 950/2809]  eta: 0:18:02  lr: 0.000004  min_lr: 0.000000  loss: 3.7904 (3.7232)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (56990.8223)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.1696  max mem: 15572
Epoch: [33]  [ 960/2809]  eta: 0:17:57  lr: 0.000004  min_lr: 0.000000  loss: 3.8176 (3.7239)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (57079.7419)  weight_decay: 0.0500 (0.0500)  time: 0.5535  data: 0.1029  max mem: 15572
Epoch: [33]  [ 970/2809]  eta: 0:17:50  lr: 0.000004  min_lr: 0.000000  loss: 3.8352 (3.7258)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (57166.8301)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.0923  max mem: 15572
Epoch: [33]  [ 980/2809]  eta: 0:17:44  lr: 0.000004  min_lr: 0.000000  loss: 3.8352 (3.7264)  class_acc: 0.2917 (0.3374)  loss_scale: 65536.0000 (57252.1427)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0993  max mem: 15572
[2025-01-16 06:10:49,924] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:10:49,924] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:10:50,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93683
[2025-01-16 06:10:50,740] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:10:50,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 990/2809]  eta: 0:17:36  lr: 0.000004  min_lr: 0.000000  loss: 3.7431 (3.7263)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (57467.9960)  weight_decay: 0.0500 (0.0500)  time: 0.5219  data: 0.0860  max mem: 15572
Epoch: [33]  [1000/2809]  eta: 0:17:30  lr: 0.000004  min_lr: 0.000000  loss: 3.7249 (3.7268)  class_acc: 0.2917 (0.3373)  loss_scale: 65536.0000 (57548.5954)  weight_decay: 0.0500 (0.0500)  time: 0.5253  data: 0.0970  max mem: 15572
Epoch: [33]  [1010/2809]  eta: 0:17:24  lr: 0.000004  min_lr: 0.000000  loss: 3.8033 (3.7276)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (57627.6004)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1419  max mem: 15572
Epoch: [33]  [1020/2809]  eta: 0:17:18  lr: 0.000004  min_lr: 0.000000  loss: 3.8479 (3.7284)  class_acc: 0.3750 (0.3375)  loss_scale: 65536.0000 (57705.0578)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.1268  max mem: 15572
Epoch: [33]  [1030/2809]  eta: 0:17:13  lr: 0.000004  min_lr: 0.000000  loss: 3.7975 (3.7267)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (57781.0126)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1427  max mem: 15572
Epoch: [33]  [1040/2809]  eta: 0:17:06  lr: 0.000004  min_lr: 0.000000  loss: 3.7004 (3.7263)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (57855.5082)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1426  max mem: 15572
Epoch: [33]  [1050/2809]  eta: 0:16:59  lr: 0.000004  min_lr: 0.000000  loss: 3.8572 (3.7273)  class_acc: 0.2917 (0.3372)  loss_scale: 65536.0000 (57928.5861)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0917  max mem: 15572
Epoch: [33]  [1060/2809]  eta: 0:16:54  lr: 0.000004  min_lr: 0.000000  loss: 3.8158 (3.7274)  class_acc: 0.2917 (0.3371)  loss_scale: 65536.0000 (58000.2865)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1174  max mem: 15572
Epoch: [33]  [1070/2809]  eta: 0:16:49  lr: 0.000004  min_lr: 0.000000  loss: 3.4921 (3.7254)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (58070.6480)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1541  max mem: 15572
Epoch: [33]  [1080/2809]  eta: 0:16:43  lr: 0.000004  min_lr: 0.000000  loss: 3.4921 (3.7240)  class_acc: 0.3750 (0.3383)  loss_scale: 65536.0000 (58139.7077)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1389  max mem: 15572
Epoch: [33]  [1090/2809]  eta: 0:16:37  lr: 0.000004  min_lr: 0.000000  loss: 3.7561 (3.7269)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (58207.5014)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1167  max mem: 15572
Epoch: [33]  [1100/2809]  eta: 0:16:31  lr: 0.000004  min_lr: 0.000000  loss: 3.7586 (3.7267)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (58274.0636)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1199  max mem: 15572
Epoch: [33]  [1110/2809]  eta: 0:16:25  lr: 0.000004  min_lr: 0.000000  loss: 3.6034 (3.7259)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (58339.4275)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1222  max mem: 15572
[2025-01-16 06:12:04,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:12:04,827] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:12:07,015] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93817
[2025-01-16 06:12:07,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:12:07,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1120/2809]  eta: 0:16:19  lr: 0.000004  min_lr: 0.000000  loss: 3.6939 (3.7268)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (58695.9358)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1102  max mem: 15572
Epoch: [33]  [1130/2809]  eta: 0:16:13  lr: 0.000004  min_lr: 0.000000  loss: 3.7805 (3.7273)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (58756.4138)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1329  max mem: 15572
Epoch: [33]  [1140/2809]  eta: 0:16:07  lr: 0.000004  min_lr: 0.000000  loss: 3.8290 (3.7280)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (58815.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1335  max mem: 15572
[2025-01-16 06:12:22,168] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93843
[2025-01-16 06:12:22,169] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:12:22,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1150/2809]  eta: 0:16:01  lr: 0.000004  min_lr: 0.000000  loss: 3.6754 (3.7262)  class_acc: 0.3750 (0.3391)  loss_scale: 65536.0000 (58731.8714)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1205  max mem: 15572
Epoch: [33]  [1160/2809]  eta: 0:15:55  lr: 0.000004  min_lr: 0.000000  loss: 3.6754 (3.7255)  class_acc: 0.4167 (0.3393)  loss_scale: 32768.0000 (58508.2377)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1450  max mem: 15572
Epoch: [33]  [1170/2809]  eta: 0:15:49  lr: 0.000004  min_lr: 0.000000  loss: 3.7851 (3.7270)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (58288.4236)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1321  max mem: 15572
Epoch: [33]  [1180/2809]  eta: 0:15:42  lr: 0.000004  min_lr: 0.000000  loss: 3.8652 (3.7284)  class_acc: 0.2500 (0.3384)  loss_scale: 32768.0000 (58072.3319)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0955  max mem: 15572
Epoch: [33]  [1190/2809]  eta: 0:15:36  lr: 0.000004  min_lr: 0.000000  loss: 3.8585 (3.7290)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (57859.8690)  weight_decay: 0.0500 (0.0500)  time: 0.5481  data: 0.1084  max mem: 15572
Epoch: [33]  [1200/2809]  eta: 0:15:29  lr: 0.000004  min_lr: 0.000000  loss: 3.6847 (3.7253)  class_acc: 0.3750 (0.3390)  loss_scale: 32768.0000 (57650.9442)  weight_decay: 0.0500 (0.0500)  time: 0.5047  data: 0.0707  max mem: 15572
Epoch: [33]  [1210/2809]  eta: 0:15:23  lr: 0.000004  min_lr: 0.000000  loss: 3.3264 (3.7238)  class_acc: 0.3750 (0.3395)  loss_scale: 32768.0000 (57445.4699)  weight_decay: 0.0500 (0.0500)  time: 0.5145  data: 0.0820  max mem: 15572
Epoch: [33]  [1220/2809]  eta: 0:15:18  lr: 0.000004  min_lr: 0.000000  loss: 3.6123 (3.7232)  class_acc: 0.3750 (0.3398)  loss_scale: 32768.0000 (57243.3612)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1436  max mem: 15572
Epoch: [33]  [1230/2809]  eta: 0:15:12  lr: 0.000004  min_lr: 0.000000  loss: 3.7904 (3.7237)  class_acc: 0.3333 (0.3397)  loss_scale: 32768.0000 (57044.5361)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1269  max mem: 15572
Epoch: [33]  [1240/2809]  eta: 0:15:06  lr: 0.000004  min_lr: 0.000000  loss: 3.8190 (3.7244)  class_acc: 0.2917 (0.3393)  loss_scale: 32768.0000 (56848.9154)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1238  max mem: 15572
Epoch: [33]  [1250/2809]  eta: 0:14:59  lr: 0.000004  min_lr: 0.000000  loss: 3.6240 (3.7237)  class_acc: 0.3333 (0.3397)  loss_scale: 32768.0000 (56656.4221)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1321  max mem: 15572
Epoch: [33]  [1260/2809]  eta: 0:14:54  lr: 0.000004  min_lr: 0.000000  loss: 3.7651 (3.7230)  class_acc: 0.3750 (0.3400)  loss_scale: 32768.0000 (56466.9818)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1353  max mem: 15572
Epoch: [33]  [1270/2809]  eta: 0:14:48  lr: 0.000004  min_lr: 0.000000  loss: 3.9553 (3.7250)  class_acc: 0.2917 (0.3394)  loss_scale: 32768.0000 (56280.5224)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1349  max mem: 15572
[2025-01-16 06:13:33,545] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:13:33,545] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 06:13:34,464] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 93974
[2025-01-16 06:13:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:13:34,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1280/2809]  eta: 0:14:41  lr: 0.000004  min_lr: 0.000000  loss: 3.9749 (3.7253)  class_acc: 0.2917 (0.3397)  loss_scale: 32768.0000 (56148.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0941  max mem: 15572
Epoch: [33]  [1290/2809]  eta: 0:14:36  lr: 0.000004  min_lr: 0.000000  loss: 3.9460 (3.7271)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (55967.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.1223  max mem: 15572
Epoch: [33]  [1300/2809]  eta: 0:14:29  lr: 0.000004  min_lr: 0.000000  loss: 3.8206 (3.7270)  class_acc: 0.3333 (0.3396)  loss_scale: 32768.0000 (55788.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1333  max mem: 15572
[2025-01-16 06:13:47,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=94000, skipped=625, lr=[3.8392652360453044e-08, 3.8392652360453044e-08, 5.4846646229218644e-08, 5.4846646229218644e-08, 7.835235175602664e-08, 7.835235175602664e-08, 1.1193193108003806e-07, 1.1193193108003806e-07, 1.5990275868576867e-07, 1.5990275868576867e-07, 2.2843251240824097e-07, 2.2843251240824097e-07, 3.263321605832014e-07, 3.263321605832014e-07, 4.661888008331449e-07, 4.661888008331449e-07, 6.65984001190207e-07, 6.65984001190207e-07, 9.514057159860102e-07, 9.514057159860102e-07, 1.3591510228371572e-06, 1.3591510228371572e-06, 1.9416443183387965e-06, 1.9416443183387965e-06, 2.773777597626852e-06, 2.773777597626852e-06, 3.9625394251812176e-06, 3.9625394251812176e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:13:47,775] [INFO] [timer.py:260:stop] epoch=0/micro_step=94000/global_step=94000, RunningAvgSamplesPerSec=28.571728624592648, CurrSamplesPerSec=26.653643719229237, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [1310/2809]  eta: 0:14:23  lr: 0.000004  min_lr: 0.000000  loss: 3.8388 (3.7277)  class_acc: 0.3333 (0.3396)  loss_scale: 32768.0000 (55613.1198)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0774  max mem: 15572
Epoch: [33]  [1320/2809]  eta: 0:14:17  lr: 0.000004  min_lr: 0.000000  loss: 3.8705 (3.7287)  class_acc: 0.2917 (0.3392)  loss_scale: 32768.0000 (55440.1817)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1264  max mem: 15572
Epoch: [33]  [1330/2809]  eta: 0:14:10  lr: 0.000004  min_lr: 0.000000  loss: 3.8220 (3.7292)  class_acc: 0.2917 (0.3392)  loss_scale: 32768.0000 (55269.8422)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.1032  max mem: 15572
Epoch: [33]  [1340/2809]  eta: 0:14:05  lr: 0.000004  min_lr: 0.000000  loss: 3.7251 (3.7284)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (55102.0433)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.1062  max mem: 15572
Epoch: [33]  [1350/2809]  eta: 0:14:00  lr: 0.000004  min_lr: 0.000000  loss: 3.5712 (3.7275)  class_acc: 0.3750 (0.3399)  loss_scale: 32768.0000 (54936.7283)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1464  max mem: 15572
Epoch: [33]  [1360/2809]  eta: 0:13:52  lr: 0.000004  min_lr: 0.000000  loss: 3.7836 (3.7279)  class_acc: 0.3333 (0.3400)  loss_scale: 32768.0000 (54773.8428)  weight_decay: 0.0500 (0.0500)  time: 0.5214  data: 0.0542  max mem: 15572
Epoch: [33]  [1370/2809]  eta: 0:13:45  lr: 0.000004  min_lr: 0.000000  loss: 3.8359 (3.7279)  class_acc: 0.2917 (0.3398)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.4452  data: 0.0009  max mem: 15572
Epoch: [33]  [1380/2809]  eta: 0:13:39  lr: 0.000004  min_lr: 0.000000  loss: 3.8029 (3.7280)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (54455.1484)  weight_decay: 0.0500 (0.0500)  time: 0.4802  data: 0.0227  max mem: 15572
Epoch: [33]  [1390/2809]  eta: 0:13:33  lr: 0.000004  min_lr: 0.000000  loss: 3.7667 (3.7279)  class_acc: 0.4167 (0.3407)  loss_scale: 32768.0000 (54299.2380)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0572  max mem: 15572
Epoch: [33]  [1400/2809]  eta: 0:13:28  lr: 0.000004  min_lr: 0.000000  loss: 3.6285 (3.7268)  class_acc: 0.3750 (0.3412)  loss_scale: 32768.0000 (54145.5532)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1539  max mem: 15572
[2025-01-16 06:14:44,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:14:44,318] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1410/2809]  eta: 0:13:22  lr: 0.000004  min_lr: 0.000000  loss: 3.7305 (3.7285)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (54110.1630)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.1434  max mem: 15572
Epoch: [33]  [1420/2809]  eta: 0:13:16  lr: 0.000004  min_lr: 0.000000  loss: 3.8979 (3.7304)  class_acc: 0.2917 (0.3405)  loss_scale: 65536.0000 (54190.5700)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0716  max mem: 15572
Epoch: [33]  [1430/2809]  eta: 0:13:10  lr: 0.000004  min_lr: 0.000000  loss: 3.7370 (3.7304)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (54269.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5824  data: 0.1449  max mem: 15572
Epoch: [33]  [1440/2809]  eta: 0:13:05  lr: 0.000004  min_lr: 0.000000  loss: 3.6104 (3.7288)  class_acc: 0.3750 (0.3408)  loss_scale: 65536.0000 (54348.0361)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1764  max mem: 15572
Epoch: [33]  [1450/2809]  eta: 0:13:00  lr: 0.000004  min_lr: 0.000000  loss: 3.5974 (3.7280)  class_acc: 0.3750 (0.3409)  loss_scale: 65536.0000 (54425.1413)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.1597  max mem: 15572
Epoch: [33]  [1460/2809]  eta: 0:12:53  lr: 0.000004  min_lr: 0.000000  loss: 3.6940 (3.7276)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (54501.1910)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.1031  max mem: 15572
Epoch: [33]  [1470/2809]  eta: 0:12:47  lr: 0.000004  min_lr: 0.000000  loss: 3.8263 (3.7285)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (54576.2067)  weight_decay: 0.0500 (0.0500)  time: 0.4932  data: 0.0545  max mem: 15572
Epoch: [33]  [1480/2809]  eta: 0:12:41  lr: 0.000004  min_lr: 0.000000  loss: 3.8263 (3.7276)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (54650.2093)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1357  max mem: 15572
Epoch: [33]  [1490/2809]  eta: 0:12:36  lr: 0.000004  min_lr: 0.000000  loss: 3.6879 (3.7267)  class_acc: 0.3333 (0.3414)  loss_scale: 65536.0000 (54723.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.1925  max mem: 15572
Epoch: [33]  [1500/2809]  eta: 0:12:31  lr: 0.000004  min_lr: 0.000000  loss: 3.7068 (3.7270)  class_acc: 0.3333 (0.3412)  loss_scale: 65536.0000 (54795.2565)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.1757  max mem: 15572
Epoch: [33]  [1510/2809]  eta: 0:12:25  lr: 0.000004  min_lr: 0.000000  loss: 3.7246 (3.7270)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (54866.3402)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1742  max mem: 15572
Epoch: [33]  [1520/2809]  eta: 0:12:19  lr: 0.000004  min_lr: 0.000000  loss: 3.7884 (3.7284)  class_acc: 0.2917 (0.3409)  loss_scale: 65536.0000 (54936.4892)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.1005  max mem: 15572
Epoch: [33]  [1530/2809]  eta: 0:12:14  lr: 0.000004  min_lr: 0.000000  loss: 3.8986 (3.7290)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (55005.7218)  weight_decay: 0.0500 (0.0500)  time: 0.5730  data: 0.1246  max mem: 15572
[2025-01-16 06:15:58,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:15:58,863] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:16:00,370] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94234
[2025-01-16 06:16:00,370] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:16:00,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1540/2809]  eta: 0:12:08  lr: 0.000004  min_lr: 0.000000  loss: 3.7645 (3.7279)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (55201.6405)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1513  max mem: 15572
[2025-01-16 06:16:04,489] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94241
[2025-01-16 06:16:04,489] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:16:04,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [1550/2809]  eta: 0:12:02  lr: 0.000004  min_lr: 0.000000  loss: 3.7645 (3.7283)  class_acc: 0.3750 (0.3408)  loss_scale: 65536.0000 (55120.3817)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0869  max mem: 15572
Epoch: [33]  [1560/2809]  eta: 0:11:57  lr: 0.000004  min_lr: 0.000000  loss: 3.8539 (3.7282)  class_acc: 0.2917 (0.3409)  loss_scale: 32768.0000 (54977.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1572  max mem: 15572
Epoch: [33]  [1570/2809]  eta: 0:11:50  lr: 0.000004  min_lr: 0.000000  loss: 3.7374 (3.7276)  class_acc: 0.3333 (0.3412)  loss_scale: 32768.0000 (54835.8192)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.1101  max mem: 15572
Epoch: [33]  [1580/2809]  eta: 0:11:43  lr: 0.000004  min_lr: 0.000000  loss: 3.7469 (3.7286)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (54696.2378)  weight_decay: 0.0500 (0.0500)  time: 0.4632  data: 0.0006  max mem: 15572
Epoch: [33]  [1590/2809]  eta: 0:11:37  lr: 0.000004  min_lr: 0.000000  loss: 3.9294 (3.7287)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (54558.4111)  weight_decay: 0.0500 (0.0500)  time: 0.4605  data: 0.0157  max mem: 15572
Epoch: [33]  [1600/2809]  eta: 0:11:31  lr: 0.000004  min_lr: 0.000000  loss: 3.9211 (3.7297)  class_acc: 0.2917 (0.3405)  loss_scale: 32768.0000 (54422.3061)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0676  max mem: 15572
Epoch: [33]  [1610/2809]  eta: 0:11:26  lr: 0.000004  min_lr: 0.000000  loss: 3.7754 (3.7286)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (54287.8908)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.1601  max mem: 15572
Epoch: [33]  [1620/2809]  eta: 0:11:20  lr: 0.000004  min_lr: 0.000000  loss: 3.7149 (3.7293)  class_acc: 0.3333 (0.3406)  loss_scale: 32768.0000 (54155.1339)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1358  max mem: 15572
Epoch: [33]  [1630/2809]  eta: 0:11:14  lr: 0.000004  min_lr: 0.000000  loss: 3.7926 (3.7287)  class_acc: 0.2917 (0.3409)  loss_scale: 32768.0000 (54024.0049)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.0912  max mem: 15572
Epoch: [33]  [1640/2809]  eta: 0:11:08  lr: 0.000004  min_lr: 0.000000  loss: 3.7329 (3.7292)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (53894.4741)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.1039  max mem: 15572
Epoch: [33]  [1650/2809]  eta: 0:11:02  lr: 0.000004  min_lr: 0.000000  loss: 3.6463 (3.7289)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (53766.5124)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0987  max mem: 15572
Epoch: [33]  [1660/2809]  eta: 0:10:56  lr: 0.000004  min_lr: 0.000000  loss: 3.8633 (3.7313)  class_acc: 0.2917 (0.3402)  loss_scale: 32768.0000 (53640.0915)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0986  max mem: 15572
Epoch: [33]  [1670/2809]  eta: 0:10:50  lr: 0.000004  min_lr: 0.000000  loss: 4.0279 (3.7315)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (53515.1837)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0483  max mem: 15572
[2025-01-16 06:17:14,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:17:14,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [1680/2809]  eta: 0:10:45  lr: 0.000004  min_lr: 0.000000  loss: 3.6755 (3.7311)  class_acc: 0.3333 (0.3403)  loss_scale: 32768.0000 (53547.7073)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.0902  max mem: 15572
Epoch: [33]  [1690/2809]  eta: 0:10:39  lr: 0.000004  min_lr: 0.000000  loss: 3.7965 (3.7312)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (53618.6020)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1450  max mem: 15572
Epoch: [33]  [1700/2809]  eta: 0:10:33  lr: 0.000004  min_lr: 0.000000  loss: 3.9966 (3.7331)  class_acc: 0.2083 (0.3391)  loss_scale: 65536.0000 (53688.6631)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1411  max mem: 15572
Epoch: [33]  [1710/2809]  eta: 0:10:28  lr: 0.000004  min_lr: 0.000000  loss: 3.9759 (3.7335)  class_acc: 0.2083 (0.3388)  loss_scale: 65536.0000 (53757.9053)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1236  max mem: 15572
Epoch: [33]  [1720/2809]  eta: 0:10:22  lr: 0.000004  min_lr: 0.000000  loss: 3.8246 (3.7340)  class_acc: 0.2917 (0.3383)  loss_scale: 65536.0000 (53826.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1047  max mem: 15572
Epoch: [33]  [1730/2809]  eta: 0:10:16  lr: 0.000004  min_lr: 0.000000  loss: 3.8248 (3.7342)  class_acc: 0.2500 (0.3381)  loss_scale: 65536.0000 (53893.9896)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1310  max mem: 15572
Epoch: [33]  [1740/2809]  eta: 0:10:10  lr: 0.000004  min_lr: 0.000000  loss: 3.9701 (3.7354)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (53960.8593)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1291  max mem: 15572
Epoch: [33]  [1750/2809]  eta: 0:10:04  lr: 0.000004  min_lr: 0.000000  loss: 3.9621 (3.7359)  class_acc: 0.2500 (0.3377)  loss_scale: 65536.0000 (54026.9652)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.0952  max mem: 15572
Epoch: [33]  [1760/2809]  eta: 0:09:58  lr: 0.000004  min_lr: 0.000000  loss: 3.8986 (3.7362)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (54092.3203)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0587  max mem: 15572
Epoch: [33]  [1770/2809]  eta: 0:09:52  lr: 0.000004  min_lr: 0.000000  loss: 3.9037 (3.7372)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (54156.9373)  weight_decay: 0.0500 (0.0500)  time: 0.4690  data: 0.0559  max mem: 15572
Epoch: [33]  [1780/2809]  eta: 0:09:47  lr: 0.000004  min_lr: 0.000000  loss: 3.9037 (3.7380)  class_acc: 0.2917 (0.3376)  loss_scale: 65536.0000 (54220.8287)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1427  max mem: 15572
Epoch: [33]  [1790/2809]  eta: 0:09:41  lr: 0.000004  min_lr: 0.000000  loss: 3.8224 (3.7382)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (54284.0067)  weight_decay: 0.0500 (0.0500)  time: 0.6539  data: 0.1952  max mem: 15572
Epoch: [33]  [1800/2809]  eta: 0:09:35  lr: 0.000004  min_lr: 0.000000  loss: 3.8224 (3.7385)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (54346.4831)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1281  max mem: 15572
[2025-01-16 06:18:27,478] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:18:27,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:18:28,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94500
[2025-01-16 06:18:28,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:18:28,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1810/2809]  eta: 0:09:30  lr: 0.000004  min_lr: 0.000000  loss: 3.5415 (3.7373)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (54480.6449)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1232  max mem: 15572
Epoch: [33]  [1820/2809]  eta: 0:09:24  lr: 0.000004  min_lr: 0.000000  loss: 3.7031 (3.7382)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (54541.3553)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1434  max mem: 15572
Epoch: [33]  [1830/2809]  eta: 0:09:18  lr: 0.000004  min_lr: 0.000000  loss: 3.7011 (3.7375)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (54601.4025)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1307  max mem: 15572
Epoch: [33]  [1840/2809]  eta: 0:09:13  lr: 0.000004  min_lr: 0.000000  loss: 3.6119 (3.7374)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (54660.7974)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1239  max mem: 15572
Epoch: [33]  [1850/2809]  eta: 0:09:07  lr: 0.000004  min_lr: 0.000000  loss: 3.8759 (3.7376)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (54719.5505)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.1636  max mem: 15572
Epoch: [33]  [1860/2809]  eta: 0:09:02  lr: 0.000004  min_lr: 0.000000  loss: 3.9027 (3.7384)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (54777.6722)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.2074  max mem: 15572
Epoch: [33]  [1870/2809]  eta: 0:08:56  lr: 0.000004  min_lr: 0.000000  loss: 3.5337 (3.7359)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (54835.1726)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1279  max mem: 15572
Epoch: [33]  [1880/2809]  eta: 0:08:50  lr: 0.000004  min_lr: 0.000000  loss: 3.3418 (3.7348)  class_acc: 0.4167 (0.3386)  loss_scale: 65536.0000 (54892.0617)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0778  max mem: 15572
Epoch: [33]  [1890/2809]  eta: 0:08:44  lr: 0.000004  min_lr: 0.000000  loss: 3.8615 (3.7349)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (54948.3490)  weight_decay: 0.0500 (0.0500)  time: 0.5045  data: 0.0780  max mem: 15572
Epoch: [33]  [1900/2809]  eta: 0:08:38  lr: 0.000004  min_lr: 0.000000  loss: 3.8837 (3.7355)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (55004.0442)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1324  max mem: 15572
Epoch: [33]  [1910/2809]  eta: 0:08:33  lr: 0.000004  min_lr: 0.000000  loss: 3.9273 (3.7367)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (55059.1565)  weight_decay: 0.0500 (0.0500)  time: 0.6083  data: 0.1591  max mem: 15572
Epoch: [33]  [1920/2809]  eta: 0:08:27  lr: 0.000004  min_lr: 0.000000  loss: 3.9385 (3.7385)  class_acc: 0.2500 (0.3378)  loss_scale: 65536.0000 (55113.6950)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1214  max mem: 15572
Epoch: [33]  [1930/2809]  eta: 0:08:22  lr: 0.000004  min_lr: 0.000000  loss: 3.7933 (3.7378)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (55167.6686)  weight_decay: 0.0500 (0.0500)  time: 0.5853  data: 0.1311  max mem: 15572
[2025-01-16 06:19:41,043] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:19:41,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:19:41,476] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94630
[2025-01-16 06:19:41,477] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:19:41,477] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1940/2809]  eta: 0:08:16  lr: 0.000004  min_lr: 0.000000  loss: 3.5502 (3.7373)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (55254.8501)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1300  max mem: 15572
Epoch: [33]  [1950/2809]  eta: 0:08:10  lr: 0.000004  min_lr: 0.000000  loss: 3.5394 (3.7373)  class_acc: 0.3333 (0.3379)  loss_scale: 65536.0000 (55307.5469)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.1026  max mem: 15572
Epoch: [33]  [1960/2809]  eta: 0:08:04  lr: 0.000004  min_lr: 0.000000  loss: 3.7084 (3.7372)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (55359.7063)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.1133  max mem: 15572
Epoch: [33]  [1970/2809]  eta: 0:07:58  lr: 0.000004  min_lr: 0.000000  loss: 3.6766 (3.7367)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (55411.3364)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1161  max mem: 15572
Epoch: [33]  [1980/2809]  eta: 0:07:52  lr: 0.000004  min_lr: 0.000000  loss: 3.6766 (3.7370)  class_acc: 0.3333 (0.3378)  loss_scale: 65536.0000 (55462.4452)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0804  max mem: 15572
Epoch: [33]  [1990/2809]  eta: 0:07:47  lr: 0.000004  min_lr: 0.000000  loss: 3.5842 (3.7367)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (55513.0407)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0957  max mem: 15572
[2025-01-16 06:20:18,698] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94696
[2025-01-16 06:20:18,698] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:20:18,698] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2000/2809]  eta: 0:07:41  lr: 0.000004  min_lr: 0.000000  loss: 3.6821 (3.7372)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (55530.3788)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1428  max mem: 15572
Epoch: [33]  [2010/2809]  eta: 0:07:35  lr: 0.000004  min_lr: 0.000000  loss: 3.7222 (3.7383)  class_acc: 0.2917 (0.3373)  loss_scale: 32768.0000 (55417.1895)  weight_decay: 0.0500 (0.0500)  time: 0.5328  data: 0.0860  max mem: 15572
Epoch: [33]  [2020/2809]  eta: 0:07:29  lr: 0.000004  min_lr: 0.000000  loss: 4.0583 (3.7394)  class_acc: 0.2917 (0.3371)  loss_scale: 32768.0000 (55305.1202)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0537  max mem: 15572
Epoch: [33]  [2030/2809]  eta: 0:07:23  lr: 0.000004  min_lr: 0.000000  loss: 3.6050 (3.7378)  class_acc: 0.3333 (0.3374)  loss_scale: 32768.0000 (55194.1546)  weight_decay: 0.0500 (0.0500)  time: 0.5146  data: 0.0724  max mem: 15572
Epoch: [33]  [2040/2809]  eta: 0:07:18  lr: 0.000004  min_lr: 0.000000  loss: 3.6147 (3.7374)  class_acc: 0.4167 (0.3376)  loss_scale: 32768.0000 (55084.2763)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1119  max mem: 15572
Epoch: [33]  [2050/2809]  eta: 0:07:12  lr: 0.000004  min_lr: 0.000000  loss: 3.6147 (3.7357)  class_acc: 0.3333 (0.3379)  loss_scale: 32768.0000 (54975.4695)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1375  max mem: 15572
Epoch: [33]  [2060/2809]  eta: 0:07:06  lr: 0.000004  min_lr: 0.000000  loss: 3.6907 (3.7367)  class_acc: 0.3333 (0.3378)  loss_scale: 32768.0000 (54867.7186)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0854  max mem: 15572
Epoch: [33]  [2070/2809]  eta: 0:07:00  lr: 0.000004  min_lr: 0.000000  loss: 3.7883 (3.7359)  class_acc: 0.2500 (0.3379)  loss_scale: 32768.0000 (54761.0082)  weight_decay: 0.0500 (0.0500)  time: 0.5374  data: 0.0931  max mem: 15572
Epoch: [33]  [2080/2809]  eta: 0:06:54  lr: 0.000004  min_lr: 0.000000  loss: 3.8463 (3.7374)  class_acc: 0.2917 (0.3375)  loss_scale: 32768.0000 (54655.3234)  weight_decay: 0.0500 (0.0500)  time: 0.5203  data: 0.0855  max mem: 15572
Epoch: [33]  [2090/2809]  eta: 0:06:49  lr: 0.000004  min_lr: 0.000000  loss: 4.0308 (3.7384)  class_acc: 0.2917 (0.3373)  loss_scale: 32768.0000 (54550.6495)  weight_decay: 0.0500 (0.0500)  time: 0.5316  data: 0.0779  max mem: 15572
Epoch: [33]  [2100/2809]  eta: 0:06:43  lr: 0.000004  min_lr: 0.000000  loss: 3.9232 (3.7386)  class_acc: 0.2917 (0.3373)  loss_scale: 32768.0000 (54446.9719)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1521  max mem: 15572
Epoch: [33]  [2110/2809]  eta: 0:06:38  lr: 0.000004  min_lr: 0.000000  loss: 3.6316 (3.7376)  class_acc: 0.3333 (0.3376)  loss_scale: 32768.0000 (54344.2766)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.2185  max mem: 15572
Epoch: [33]  [2120/2809]  eta: 0:06:32  lr: 0.000004  min_lr: 0.000000  loss: 3.5297 (3.7375)  class_acc: 0.3750 (0.3376)  loss_scale: 32768.0000 (54242.5497)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1355  max mem: 15572
[2025-01-16 06:21:29,849] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:21:29,849] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2130/2809]  eta: 0:06:26  lr: 0.000004  min_lr: 0.000000  loss: 3.6968 (3.7370)  class_acc: 0.3333 (0.3377)  loss_scale: 32768.0000 (54187.9080)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0431  max mem: 15572
Epoch: [33]  [2140/2809]  eta: 0:06:20  lr: 0.000004  min_lr: 0.000000  loss: 3.7561 (3.7371)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (54240.9117)  weight_decay: 0.0500 (0.0500)  time: 0.5086  data: 0.0541  max mem: 15572
Epoch: [33]  [2150/2809]  eta: 0:06:15  lr: 0.000004  min_lr: 0.000000  loss: 3.9088 (3.7379)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (54293.4226)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1000  max mem: 15572
Epoch: [33]  [2160/2809]  eta: 0:06:09  lr: 0.000004  min_lr: 0.000000  loss: 3.9088 (3.7382)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (54345.4475)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1261  max mem: 15572
Epoch: [33]  [2170/2809]  eta: 0:06:03  lr: 0.000004  min_lr: 0.000000  loss: 3.7594 (3.7385)  class_acc: 0.3333 (0.3377)  loss_scale: 65536.0000 (54396.9931)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1104  max mem: 15572
Epoch: [33]  [2180/2809]  eta: 0:05:57  lr: 0.000004  min_lr: 0.000000  loss: 3.8170 (3.7393)  class_acc: 0.2917 (0.3374)  loss_scale: 65536.0000 (54448.0660)  weight_decay: 0.0500 (0.0500)  time: 0.5193  data: 0.0861  max mem: 15572
Epoch: [33]  [2190/2809]  eta: 0:05:52  lr: 0.000004  min_lr: 0.000000  loss: 3.8224 (3.7388)  class_acc: 0.2917 (0.3375)  loss_scale: 65536.0000 (54498.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1122  max mem: 15572
Epoch: [33]  [2200/2809]  eta: 0:05:46  lr: 0.000004  min_lr: 0.000000  loss: 3.7555 (3.7398)  class_acc: 0.2917 (0.3371)  loss_scale: 65536.0000 (54548.8196)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1284  max mem: 15572
Epoch: [33]  [2210/2809]  eta: 0:05:40  lr: 0.000004  min_lr: 0.000000  loss: 3.7562 (3.7397)  class_acc: 0.2500 (0.3372)  loss_scale: 65536.0000 (54598.5129)  weight_decay: 0.0500 (0.0500)  time: 0.5207  data: 0.0639  max mem: 15572
[2025-01-16 06:22:17,855] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 94914
[2025-01-16 06:22:17,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:22:17,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2220/2809]  eta: 0:05:34  lr: 0.000004  min_lr: 0.000000  loss: 3.5944 (3.7384)  class_acc: 0.4167 (0.3375)  loss_scale: 65536.0000 (54588.7438)  weight_decay: 0.0500 (0.0500)  time: 0.4980  data: 0.0432  max mem: 15572
Epoch: [33]  [2230/2809]  eta: 0:05:29  lr: 0.000004  min_lr: 0.000000  loss: 3.7368 (3.7387)  class_acc: 0.3333 (0.3373)  loss_scale: 32768.0000 (54490.9368)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1284  max mem: 15572
Epoch: [33]  [2240/2809]  eta: 0:05:23  lr: 0.000004  min_lr: 0.000000  loss: 3.8381 (3.7384)  class_acc: 0.2917 (0.3373)  loss_scale: 32768.0000 (54394.0027)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1450  max mem: 15572
Epoch: [33]  [2250/2809]  eta: 0:05:17  lr: 0.000004  min_lr: 0.000000  loss: 3.6997 (3.7384)  class_acc: 0.3333 (0.3373)  loss_scale: 32768.0000 (54297.9298)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0975  max mem: 15572
Epoch: [33]  [2260/2809]  eta: 0:05:11  lr: 0.000004  min_lr: 0.000000  loss: 3.7732 (3.7386)  class_acc: 0.3333 (0.3372)  loss_scale: 32768.0000 (54202.7068)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.1193  max mem: 15572
Epoch: [33]  [2270/2809]  eta: 0:05:06  lr: 0.000004  min_lr: 0.000000  loss: 3.7858 (3.7379)  class_acc: 0.3333 (0.3374)  loss_scale: 32768.0000 (54108.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1178  max mem: 15572
Epoch: [33]  [2280/2809]  eta: 0:05:00  lr: 0.000004  min_lr: 0.000000  loss: 3.6368 (3.7379)  class_acc: 0.3750 (0.3374)  loss_scale: 32768.0000 (54014.7655)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1251  max mem: 15572
Epoch: [33]  [2290/2809]  eta: 0:04:54  lr: 0.000004  min_lr: 0.000000  loss: 3.6861 (3.7379)  class_acc: 0.2500 (0.3370)  loss_scale: 32768.0000 (53922.0253)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.1442  max mem: 15572
Epoch: [33]  [2300/2809]  eta: 0:04:49  lr: 0.000004  min_lr: 0.000000  loss: 3.6343 (3.7385)  class_acc: 0.2500 (0.3369)  loss_scale: 32768.0000 (53830.0913)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1365  max mem: 15572
[2025-01-16 06:23:06,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=95000, skipped=631, lr=[3.4476501199824503e-08, 3.4476501199824503e-08, 4.925214457117787e-08, 4.925214457117787e-08, 7.036020653025411e-08, 7.036020653025411e-08, 1.0051458075750587e-07, 1.0051458075750587e-07, 1.435922582250084e-07, 1.435922582250084e-07, 2.0513179746429771e-07, 2.0513179746429771e-07, 2.930454249489968e-07, 2.930454249489968e-07, 4.186363213557097e-07, 4.186363213557097e-07, 5.980518876510139e-07, 5.980518876510139e-07, 8.543598395014485e-07, 8.543598395014485e-07, 1.2205140564306406e-06, 1.2205140564306406e-06, 1.7435915091866297e-06, 1.7435915091866297e-06, 2.4908450131237567e-06, 2.4908450131237567e-06, 3.5583500187482243e-06, 3.5583500187482243e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:23:06,480] [INFO] [timer.py:260:stop] epoch=0/micro_step=95000/global_step=95000, RunningAvgSamplesPerSec=28.57130368336139, CurrSamplesPerSec=31.06320442289011, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [33]  [2310/2809]  eta: 0:04:43  lr: 0.000004  min_lr: 0.000000  loss: 3.6060 (3.7384)  class_acc: 0.2917 (0.3370)  loss_scale: 32768.0000 (53738.9528)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1951  max mem: 15572
Epoch: [33]  [2320/2809]  eta: 0:04:38  lr: 0.000004  min_lr: 0.000000  loss: 3.4765 (3.7377)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (53648.5997)  weight_decay: 0.0500 (0.0500)  time: 0.6112  data: 0.1634  max mem: 15572
Epoch: [33]  [2330/2809]  eta: 0:04:32  lr: 0.000004  min_lr: 0.000000  loss: 3.4452 (3.7371)  class_acc: 0.3750 (0.3374)  loss_scale: 32768.0000 (53559.0219)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0796  max mem: 15572
Epoch: [33]  [2340/2809]  eta: 0:04:26  lr: 0.000004  min_lr: 0.000000  loss: 3.6934 (3.7371)  class_acc: 0.3333 (0.3374)  loss_scale: 32768.0000 (53470.2093)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.0948  max mem: 15572
[2025-01-16 06:23:30,958] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:23:30,959] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2350/2809]  eta: 0:04:20  lr: 0.000004  min_lr: 0.000000  loss: 3.7107 (3.7369)  class_acc: 0.3333 (0.3376)  loss_scale: 32768.0000 (53451.8418)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1239  max mem: 15572
Epoch: [33]  [2360/2809]  eta: 0:04:15  lr: 0.000004  min_lr: 0.000000  loss: 3.8955 (3.7380)  class_acc: 0.2917 (0.3373)  loss_scale: 65536.0000 (53503.0241)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1519  max mem: 15572
Epoch: [33]  [2370/2809]  eta: 0:04:09  lr: 0.000004  min_lr: 0.000000  loss: 3.8907 (3.7379)  class_acc: 0.2917 (0.3373)  loss_scale: 65536.0000 (53553.7748)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1198  max mem: 15572
Epoch: [33]  [2380/2809]  eta: 0:04:03  lr: 0.000004  min_lr: 0.000000  loss: 3.7426 (3.7373)  class_acc: 0.3750 (0.3374)  loss_scale: 65536.0000 (53604.0991)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0822  max mem: 15572
Epoch: [33]  [2390/2809]  eta: 0:03:57  lr: 0.000004  min_lr: 0.000000  loss: 3.8815 (3.7377)  class_acc: 0.3750 (0.3375)  loss_scale: 65536.0000 (53654.0025)  weight_decay: 0.0500 (0.0500)  time: 0.4939  data: 0.0449  max mem: 15572
Epoch: [33]  [2400/2809]  eta: 0:03:52  lr: 0.000004  min_lr: 0.000000  loss: 3.6598 (3.7369)  class_acc: 0.4167 (0.3379)  loss_scale: 65536.0000 (53703.4902)  weight_decay: 0.0500 (0.0500)  time: 0.4952  data: 0.0293  max mem: 15572
Epoch: [33]  [2410/2809]  eta: 0:03:46  lr: 0.000004  min_lr: 0.000000  loss: 3.7036 (3.7373)  class_acc: 0.3750 (0.3379)  loss_scale: 65536.0000 (53752.5674)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0700  max mem: 15572
Epoch: [33]  [2420/2809]  eta: 0:03:40  lr: 0.000004  min_lr: 0.000000  loss: 3.8195 (3.7370)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (53801.2392)  weight_decay: 0.0500 (0.0500)  time: 0.5023  data: 0.0443  max mem: 15572
Epoch: [33]  [2430/2809]  eta: 0:03:34  lr: 0.000004  min_lr: 0.000000  loss: 3.4963 (3.7357)  class_acc: 0.4167 (0.3386)  loss_scale: 65536.0000 (53849.5105)  weight_decay: 0.0500 (0.0500)  time: 0.4910  data: 0.0488  max mem: 15572
Epoch: [33]  [2440/2809]  eta: 0:03:29  lr: 0.000004  min_lr: 0.000000  loss: 3.6252 (3.7361)  class_acc: 0.4167 (0.3385)  loss_scale: 65536.0000 (53897.3863)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1538  max mem: 15572
Epoch: [33]  [2450/2809]  eta: 0:03:23  lr: 0.000004  min_lr: 0.000000  loss: 3.7439 (3.7360)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53944.8715)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1415  max mem: 15572
Epoch: [33]  [2460/2809]  eta: 0:03:17  lr: 0.000003  min_lr: 0.000000  loss: 3.7450 (3.7356)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (53991.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0863  max mem: 15572
Epoch: [33]  [2470/2809]  eta: 0:03:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7881 (3.7362)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (54038.6888)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0729  max mem: 15572
[2025-01-16 06:24:40,241] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:24:40,241] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:24:41,553] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95174
[2025-01-16 06:24:41,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:24:41,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [2480/2809]  eta: 0:03:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7881 (3.7362)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (54164.2757)  weight_decay: 0.0500 (0.0500)  time: 0.5469  data: 0.1186  max mem: 15572
Epoch: [33]  [2490/2809]  eta: 0:03:00  lr: 0.000003  min_lr: 0.000000  loss: 3.7199 (3.7368)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (54209.9269)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1578  max mem: 15572
Epoch: [33]  [2500/2809]  eta: 0:02:55  lr: 0.000003  min_lr: 0.000000  loss: 3.8235 (3.7373)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (54255.2131)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1364  max mem: 15572
Epoch: [33]  [2510/2809]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000000  loss: 3.8482 (3.7375)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (54300.1386)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1295  max mem: 15572
Epoch: [33]  [2520/2809]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000000  loss: 3.8036 (3.7373)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (54344.7077)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1044  max mem: 15572
Epoch: [33]  [2530/2809]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6759 (3.7370)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (54388.9245)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1094  max mem: 15572
Epoch: [33]  [2540/2809]  eta: 0:02:32  lr: 0.000003  min_lr: 0.000000  loss: 3.6759 (3.7371)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (54432.7934)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1192  max mem: 15572
Epoch: [33]  [2550/2809]  eta: 0:02:26  lr: 0.000003  min_lr: 0.000000  loss: 3.8234 (3.7375)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (54476.3183)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1548  max mem: 15572
[2025-01-16 06:25:28,244] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95255
[2025-01-16 06:25:28,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:25:28,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2560/2809]  eta: 0:02:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7728 (3.7373)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (54481.1183)  weight_decay: 0.0500 (0.0500)  time: 0.5666  data: 0.1397  max mem: 15572
Epoch: [33]  [2570/2809]  eta: 0:02:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7376 (3.7380)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (54396.6643)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1588  max mem: 15572
Epoch: [33]  [2580/2809]  eta: 0:02:09  lr: 0.000003  min_lr: 0.000000  loss: 3.7584 (3.7377)  class_acc: 0.3333 (0.3387)  loss_scale: 32768.0000 (54312.8648)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.1637  max mem: 15572
Epoch: [33]  [2590/2809]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000000  loss: 3.5075 (3.7371)  class_acc: 0.3750 (0.3389)  loss_scale: 32768.0000 (54229.7121)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0991  max mem: 15572
Epoch: [33]  [2600/2809]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000000  loss: 3.5488 (3.7374)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (54147.1988)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0953  max mem: 15572
Epoch: [33]  [2610/2809]  eta: 0:01:52  lr: 0.000003  min_lr: 0.000000  loss: 3.8067 (3.7371)  class_acc: 0.3333 (0.3389)  loss_scale: 32768.0000 (54065.3175)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1133  max mem: 15572
Epoch: [33]  [2620/2809]  eta: 0:01:47  lr: 0.000003  min_lr: 0.000000  loss: 3.8067 (3.7376)  class_acc: 0.3333 (0.3388)  loss_scale: 32768.0000 (53984.0610)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1891  max mem: 15572
Epoch: [33]  [2630/2809]  eta: 0:01:41  lr: 0.000003  min_lr: 0.000000  loss: 3.6445 (3.7368)  class_acc: 0.2917 (0.3389)  loss_scale: 32768.0000 (53903.4223)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.1878  max mem: 15572
Epoch: [33]  [2640/2809]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 3.5643 (3.7367)  class_acc: 0.2917 (0.3389)  loss_scale: 32768.0000 (53823.3942)  weight_decay: 0.0500 (0.0500)  time: 0.5220  data: 0.0645  max mem: 15572
Epoch: [33]  [2650/2809]  eta: 0:01:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7977 (3.7370)  class_acc: 0.2917 (0.3388)  loss_scale: 32768.0000 (53743.9698)  weight_decay: 0.0500 (0.0500)  time: 0.4902  data: 0.0486  max mem: 15572
Epoch: [33]  [2660/2809]  eta: 0:01:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6590 (3.7362)  class_acc: 0.3750 (0.3389)  loss_scale: 32768.0000 (53665.1424)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1749  max mem: 15572
Epoch: [33]  [2670/2809]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 3.3892 (3.7355)  class_acc: 0.3750 (0.3390)  loss_scale: 32768.0000 (53586.9053)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.1823  max mem: 15572
Epoch: [33]  [2680/2809]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6108 (3.7356)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (53509.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1310  max mem: 15572
[2025-01-16 06:26:42,598] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:26:42,598] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [2690/2809]  eta: 0:01:07  lr: 0.000003  min_lr: 0.000000  loss: 3.5210 (3.7350)  class_acc: 0.3750 (0.3393)  loss_scale: 32768.0000 (53480.8829)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1324  max mem: 15572
Epoch: [33]  [2700/2809]  eta: 0:01:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6052 (3.7355)  class_acc: 0.3750 (0.3393)  loss_scale: 65536.0000 (53525.5150)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0977  max mem: 15572
Epoch: [33]  [2710/2809]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 3.9166 (3.7362)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (53569.8178)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0887  max mem: 15572
Epoch: [33]  [2720/2809]  eta: 0:00:50  lr: 0.000003  min_lr: 0.000000  loss: 3.8853 (3.7359)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (53613.7949)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0840  max mem: 15572
Epoch: [33]  [2730/2809]  eta: 0:00:44  lr: 0.000003  min_lr: 0.000000  loss: 3.8923 (3.7369)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53657.4500)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1075  max mem: 15572
Epoch: [33]  [2740/2809]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 3.8924 (3.7370)  class_acc: 0.2917 (0.3388)  loss_scale: 65536.0000 (53700.7866)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1137  max mem: 15572
Epoch: [33]  [2750/2809]  eta: 0:00:33  lr: 0.000003  min_lr: 0.000000  loss: 3.5712 (3.7357)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53743.8081)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1329  max mem: 15572
[2025-01-16 06:27:22,790] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95454
[2025-01-16 06:27:22,790] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:27:22,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [2760/2809]  eta: 0:00:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7044 (3.7365)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53739.0453)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1481  max mem: 15572
Epoch: [33]  [2770/2809]  eta: 0:00:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7044 (3.7351)  class_acc: 0.3750 (0.3393)  loss_scale: 32768.0000 (53663.3649)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1116  max mem: 15572
Epoch: [33]  [2780/2809]  eta: 0:00:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5818 (3.7351)  class_acc: 0.4167 (0.3396)  loss_scale: 32768.0000 (53588.2287)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.0992  max mem: 15572
Epoch: [33]  [2790/2809]  eta: 0:00:10  lr: 0.000003  min_lr: 0.000000  loss: 3.8511 (3.7356)  class_acc: 0.4167 (0.3397)  loss_scale: 32768.0000 (53513.6310)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1277  max mem: 15572
Epoch: [33]  [2800/2809]  eta: 0:00:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8627 (3.7362)  class_acc: 0.3333 (0.3396)  loss_scale: 32768.0000 (53439.5659)  weight_decay: 0.0500 (0.0500)  time: 0.5567  data: 0.1062  max mem: 15572
Epoch: [33]  [2808/2809]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 4.0152 (3.7365)  class_acc: 0.2500 (0.3394)  loss_scale: 32768.0000 (53380.6935)  weight_decay: 0.0500 (0.0500)  time: 0.4713  data: 0.0587  max mem: 15572
Epoch: [33] Total time: 0:26:33 (0.5672 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 4.0152 (3.7365)  class_acc: 0.2500 (0.3394)  loss_scale: 32768.0000 (53380.6935)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:16:55  loss: 0.3668 (0.3668)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.7319  data: 3.5456  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 2.4268 (2.2736)  acc1: 38.8889 (42.9293)  acc5: 72.2222 (72.7273)  time: 0.7443  data: 0.5410  max mem: 15572
Val:  [ 20/272]  eta: 0:02:06  loss: 2.3079 (2.3121)  acc1: 44.4444 (45.5026)  acc5: 72.2222 (73.8095)  time: 0.3416  data: 0.1354  max mem: 15572
Val:  [ 30/272]  eta: 0:01:41  loss: 2.3079 (2.3843)  acc1: 44.4444 (43.7276)  acc5: 77.7778 (73.2975)  time: 0.2409  data: 0.0413  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.5464 (2.4323)  acc1: 33.3333 (41.1924)  acc5: 77.7778 (73.4417)  time: 0.2973  data: 0.0940  max mem: 15572
Val:  [ 50/272]  eta: 0:01:25  loss: 2.4203 (2.3593)  acc1: 33.3333 (43.1373)  acc5: 77.7778 (74.9455)  time: 0.3359  data: 0.1248  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.5086 (2.2503)  acc1: 66.6667 (46.0838)  acc5: 88.8889 (76.2295)  time: 0.3135  data: 0.1132  max mem: 15572
Val:  [ 70/272]  eta: 0:01:13  loss: 1.5238 (2.1710)  acc1: 66.6667 (48.4351)  acc5: 83.3333 (77.2300)  time: 0.3135  data: 0.1169  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 1.8388 (2.1816)  acc1: 55.5556 (48.3539)  acc5: 77.7778 (77.0919)  time: 0.3160  data: 0.1157  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.1033 (2.1800)  acc1: 50.0000 (48.5958)  acc5: 77.7778 (77.5946)  time: 0.3372  data: 0.1372  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.0981 (2.2071)  acc1: 50.0000 (48.0198)  acc5: 83.3333 (77.2827)  time: 0.3645  data: 0.1694  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.4744 (2.2815)  acc1: 22.2222 (45.7457)  acc5: 72.2222 (76.0761)  time: 0.2946  data: 0.0914  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.8622 (2.3171)  acc1: 16.6667 (45.1331)  acc5: 72.2222 (75.4362)  time: 0.2818  data: 0.0747  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.0827 (2.2832)  acc1: 44.4444 (45.9712)  acc5: 77.7778 (76.1238)  time: 0.3715  data: 0.1712  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 1.6291 (2.2757)  acc1: 55.5556 (46.3751)  acc5: 88.8889 (75.9653)  time: 0.3891  data: 0.1906  max mem: 15572
Val:  [150/272]  eta: 0:00:43  loss: 2.2972 (2.2810)  acc1: 38.8889 (45.6954)  acc5: 77.7778 (76.1589)  time: 0.3630  data: 0.1629  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.2720 (2.2708)  acc1: 44.4444 (46.2043)  acc5: 77.7778 (76.2250)  time: 0.2815  data: 0.0735  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4014 (2.2914)  acc1: 44.4444 (45.5491)  acc5: 72.2222 (75.7635)  time: 0.2511  data: 0.0403  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.3435 (2.2815)  acc1: 38.8889 (45.4880)  acc5: 77.7778 (76.0896)  time: 0.3347  data: 0.1409  max mem: 15572
Val:  [190/272]  eta: 0:00:28  loss: 2.3435 (2.3342)  acc1: 38.8889 (44.2699)  acc5: 77.7778 (74.6946)  time: 0.3875  data: 0.1932  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 2.4768 (2.3408)  acc1: 38.8889 (44.0575)  acc5: 66.6667 (74.5163)  time: 0.3843  data: 0.1744  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.1712 (2.3463)  acc1: 44.4444 (44.0758)  acc5: 72.2222 (74.3813)  time: 0.3262  data: 0.1145  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.3829 (2.3353)  acc1: 44.4444 (44.4193)  acc5: 77.7778 (74.5349)  time: 0.3101  data: 0.0815  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.7176 (2.3042)  acc1: 66.6667 (45.4305)  acc5: 83.3333 (74.9639)  time: 0.3482  data: 0.1317  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.5333 (2.2889)  acc1: 61.1111 (45.8045)  acc5: 83.3333 (75.3573)  time: 0.3070  data: 0.1104  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.1634 (2.3002)  acc1: 38.8889 (45.1527)  acc5: 83.3333 (75.3431)  time: 0.3175  data: 0.0997  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.1941 (2.2436)  acc1: 66.6667 (46.7433)  acc5: 88.8889 (76.0962)  time: 0.2850  data: 0.0773  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3655 (2.2378)  acc1: 66.6667 (46.9455)  acc5: 88.8889 (76.3018)  time: 0.1728  data: 0.0055  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3655 (2.2415)  acc1: 66.6667 (46.9179)  acc5: 88.8889 (76.2646)  time: 0.1633  data: 0.0054  max mem: 15572
Val: Total time: 0:01:29 (0.3303 s / it)
* Acc@1 46.918 Acc@5 76.265 loss 2.241
Accuracy of the network on the 4883 val videos: 46.9%
[2025-01-16 06:29:19,639] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 06:29:19,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 06:29:19,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 06:29:22,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 06:29:22,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.92%
Epoch: [34]  [   0/2809]  eta: 3:09:58  lr: 0.000003  min_lr: 0.000000  loss: 3.4381 (3.4381)  class_acc: 0.4167 (0.4167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0579  data: 3.6576  max mem: 15572
Epoch: [34]  [  10/2809]  eta: 0:34:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6956 (3.7197)  class_acc: 0.4167 (0.4167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7330  data: 0.3356  max mem: 15572
Epoch: [34]  [  20/2809]  eta: 0:29:04  lr: 0.000003  min_lr: 0.000000  loss: 3.7457 (3.6632)  class_acc: 0.3750 (0.3770)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4539  data: 0.0021  max mem: 15572
Epoch: [34]  [  30/2809]  eta: 0:26:13  lr: 0.000003  min_lr: 0.000000  loss: 3.8070 (3.6767)  class_acc: 0.2917 (0.3696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4745  data: 0.0009  max mem: 15572
Epoch: [34]  [  40/2809]  eta: 0:27:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6574 (3.6533)  class_acc: 0.3333 (0.3628)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.0915  max mem: 15572
Epoch: [34]  [  50/2809]  eta: 0:28:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6574 (3.6626)  class_acc: 0.3333 (0.3546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6901  data: 0.2213  max mem: 15572
Epoch: [34]  [  60/2809]  eta: 0:29:59  lr: 0.000003  min_lr: 0.000000  loss: 3.8166 (3.6850)  class_acc: 0.3333 (0.3511)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7885  data: 0.3272  max mem: 15572
Epoch: [34]  [  70/2809]  eta: 0:30:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7752 (3.6766)  class_acc: 0.3333 (0.3562)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8083  data: 0.3447  max mem: 15572
[2025-01-16 06:30:13,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:30:13,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [  80/2809]  eta: 0:29:07  lr: 0.000003  min_lr: 0.000000  loss: 3.7752 (3.6964)  class_acc: 0.3333 (0.3524)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.1482  max mem: 15572
Epoch: [34]  [  90/2809]  eta: 0:29:05  lr: 0.000003  min_lr: 0.000000  loss: 3.7095 (3.6688)  class_acc: 0.3750 (0.3594)  loss_scale: 65536.0000 (37809.2308)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1046  max mem: 15572
Epoch: [34]  [ 100/2809]  eta: 0:29:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6668 (3.6775)  class_acc: 0.3750 (0.3589)  loss_scale: 65536.0000 (40554.4554)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.2228  max mem: 15572
Epoch: [34]  [ 110/2809]  eta: 0:29:25  lr: 0.000003  min_lr: 0.000000  loss: 3.6638 (3.6753)  class_acc: 0.3333 (0.3592)  loss_scale: 65536.0000 (42805.0450)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.2441  max mem: 15572
Epoch: [34]  [ 120/2809]  eta: 0:29:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6535 (3.6752)  class_acc: 0.3333 (0.3581)  loss_scale: 65536.0000 (44683.6364)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1805  max mem: 15572
Epoch: [34]  [ 130/2809]  eta: 0:28:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6295 (3.6682)  class_acc: 0.4167 (0.3601)  loss_scale: 65536.0000 (46275.4198)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1531  max mem: 15572
Epoch: [34]  [ 140/2809]  eta: 0:29:04  lr: 0.000003  min_lr: 0.000000  loss: 3.4638 (3.6602)  class_acc: 0.3333 (0.3605)  loss_scale: 65536.0000 (47641.4184)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.2465  max mem: 15572
Epoch: [34]  [ 150/2809]  eta: 0:28:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6340 (3.6670)  class_acc: 0.2917 (0.3560)  loss_scale: 65536.0000 (48826.4901)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1484  max mem: 15572
Epoch: [34]  [ 160/2809]  eta: 0:27:33  lr: 0.000003  min_lr: 0.000000  loss: 3.8116 (3.6684)  class_acc: 0.2917 (0.3558)  loss_scale: 65536.0000 (49864.3478)  weight_decay: 0.0500 (0.0500)  time: 0.4146  data: 0.0005  max mem: 15572
Epoch: [34]  [ 170/2809]  eta: 0:26:58  lr: 0.000003  min_lr: 0.000000  loss: 3.7955 (3.6783)  class_acc: 0.3333 (0.3548)  loss_scale: 65536.0000 (50780.8187)  weight_decay: 0.0500 (0.0500)  time: 0.4309  data: 0.0071  max mem: 15572
Epoch: [34]  [ 180/2809]  eta: 0:26:49  lr: 0.000003  min_lr: 0.000000  loss: 3.6866 (3.6834)  class_acc: 0.3333 (0.3527)  loss_scale: 65536.0000 (51596.0221)  weight_decay: 0.0500 (0.0500)  time: 0.5159  data: 0.0954  max mem: 15572
Epoch: [34]  [ 190/2809]  eta: 0:26:47  lr: 0.000003  min_lr: 0.000000  loss: 3.8525 (3.6812)  class_acc: 0.3333 (0.3508)  loss_scale: 65536.0000 (52325.8639)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1966  max mem: 15572
Epoch: [34]  [ 200/2809]  eta: 0:26:41  lr: 0.000003  min_lr: 0.000000  loss: 3.8525 (3.6932)  class_acc: 0.2917 (0.3476)  loss_scale: 65536.0000 (52983.0846)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.1958  max mem: 15572
[2025-01-16 06:31:28,586] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:31:28,586] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:31:30,677] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95716
[2025-01-16 06:31:30,678] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:31:30,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 210/2809]  eta: 0:26:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6421 (3.6841)  class_acc: 0.3333 (0.3517)  loss_scale: 65536.0000 (55130.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1081  max mem: 15572
Epoch: [34]  [ 220/2809]  eta: 0:26:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7812 (3.6959)  class_acc: 0.3333 (0.3492)  loss_scale: 65536.0000 (55601.8100)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1113  max mem: 15572
Epoch: [34]  [ 230/2809]  eta: 0:25:56  lr: 0.000003  min_lr: 0.000000  loss: 3.9713 (3.7021)  class_acc: 0.2917 (0.3474)  loss_scale: 65536.0000 (56031.8615)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1312  max mem: 15572
Epoch: [34]  [ 240/2809]  eta: 0:25:40  lr: 0.000003  min_lr: 0.000000  loss: 3.7531 (3.7024)  class_acc: 0.3333 (0.3472)  loss_scale: 65536.0000 (56426.2241)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0740  max mem: 15572
Epoch: [34]  [ 250/2809]  eta: 0:25:31  lr: 0.000003  min_lr: 0.000000  loss: 3.9761 (3.7163)  class_acc: 0.2917 (0.3443)  loss_scale: 65536.0000 (56789.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0998  max mem: 15572
Epoch: [34]  [ 260/2809]  eta: 0:25:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7643 (3.7079)  class_acc: 0.3333 (0.3459)  loss_scale: 65536.0000 (57124.2912)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.1736  max mem: 15572
Epoch: [34]  [ 270/2809]  eta: 0:25:15  lr: 0.000003  min_lr: 0.000000  loss: 3.7055 (3.7131)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (57434.6863)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1450  max mem: 15572
Epoch: [34]  [ 280/2809]  eta: 0:24:55  lr: 0.000003  min_lr: 0.000000  loss: 3.9102 (3.7171)  class_acc: 0.3333 (0.3449)  loss_scale: 65536.0000 (57722.9893)  weight_decay: 0.0500 (0.0500)  time: 0.4793  data: 0.0462  max mem: 15572
Epoch: [34]  [ 290/2809]  eta: 0:25:00  lr: 0.000003  min_lr: 0.000000  loss: 3.8029 (3.7217)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (57991.4777)  weight_decay: 0.0500 (0.0500)  time: 0.5786  data: 0.1407  max mem: 15572
Epoch: [34]  [ 300/2809]  eta: 0:24:50  lr: 0.000003  min_lr: 0.000000  loss: 3.7115 (3.7249)  class_acc: 0.2917 (0.3427)  loss_scale: 65536.0000 (58242.1262)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.1896  max mem: 15572
Epoch: [34]  [ 310/2809]  eta: 0:24:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7623 (3.7290)  class_acc: 0.2917 (0.3412)  loss_scale: 65536.0000 (58476.6559)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0990  max mem: 15572
Epoch: [34]  [ 320/2809]  eta: 0:24:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7755 (3.7291)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (58696.5732)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1132  max mem: 15572
Epoch: [34]  [ 330/2809]  eta: 0:24:29  lr: 0.000003  min_lr: 0.000000  loss: 3.9079 (3.7340)  class_acc: 0.3333 (0.3418)  loss_scale: 65536.0000 (58903.2024)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.1518  max mem: 15572
[2025-01-16 06:32:43,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:32:43,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:32:44,412] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95846
[2025-01-16 06:32:44,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:32:44,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 340/2809]  eta: 0:24:20  lr: 0.000003  min_lr: 0.000000  loss: 3.7987 (3.7348)  class_acc: 0.3333 (0.3415)  loss_scale: 65536.0000 (59289.9003)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1338  max mem: 15572
Epoch: [34]  [ 350/2809]  eta: 0:24:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7523 (3.7370)  class_acc: 0.3333 (0.3402)  loss_scale: 65536.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.1343  max mem: 15572
Epoch: [34]  [ 360/2809]  eta: 0:24:07  lr: 0.000003  min_lr: 0.000000  loss: 3.7523 (3.7304)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (59635.9446)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1245  max mem: 15572
Epoch: [34]  [ 370/2809]  eta: 0:24:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7740 (3.7314)  class_acc: 0.3333 (0.3410)  loss_scale: 65536.0000 (59794.9757)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1216  max mem: 15572
Epoch: [34]  [ 380/2809]  eta: 0:23:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7740 (3.7329)  class_acc: 0.2917 (0.3414)  loss_scale: 65536.0000 (59945.6588)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1296  max mem: 15572
Epoch: [34]  [ 390/2809]  eta: 0:23:46  lr: 0.000003  min_lr: 0.000000  loss: 3.6852 (3.7309)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (60088.6343)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1045  max mem: 15572
Epoch: [34]  [ 400/2809]  eta: 0:23:39  lr: 0.000003  min_lr: 0.000000  loss: 3.8881 (3.7339)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (60224.4788)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1161  max mem: 15572
Epoch: [34]  [ 410/2809]  eta: 0:23:27  lr: 0.000003  min_lr: 0.000000  loss: 3.8881 (3.7320)  class_acc: 0.3333 (0.3413)  loss_scale: 65536.0000 (60353.7129)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0915  max mem: 15572
Epoch: [34]  [ 420/2809]  eta: 0:23:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7911 (3.7361)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (60476.8076)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0951  max mem: 15572
Epoch: [34]  [ 430/2809]  eta: 0:23:10  lr: 0.000003  min_lr: 0.000000  loss: 3.6815 (3.7335)  class_acc: 0.2917 (0.3388)  loss_scale: 65536.0000 (60594.1903)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.1090  max mem: 15572
Epoch: [34]  [ 440/2809]  eta: 0:23:06  lr: 0.000003  min_lr: 0.000000  loss: 3.6799 (3.7338)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (60706.2494)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1216  max mem: 15572
Epoch: [34]  [ 450/2809]  eta: 0:22:58  lr: 0.000003  min_lr: 0.000000  loss: 3.6811 (3.7305)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (60813.3392)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1435  max mem: 15572
Epoch: [34]  [ 460/2809]  eta: 0:22:53  lr: 0.000003  min_lr: 0.000000  loss: 3.6495 (3.7342)  class_acc: 0.2500 (0.3360)  loss_scale: 65536.0000 (60915.7831)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1484  max mem: 15572
[2025-01-16 06:33:57,921] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:33:57,922] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 470/2809]  eta: 0:22:48  lr: 0.000003  min_lr: 0.000000  loss: 4.1112 (3.7368)  class_acc: 0.2083 (0.3352)  loss_scale: 65536.0000 (61292.1614)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1673  max mem: 15572
[2025-01-16 06:33:59,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 95978
[2025-01-16 06:33:59,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:33:59,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 480/2809]  eta: 0:22:41  lr: 0.000003  min_lr: 0.000000  loss: 3.8106 (3.7339)  class_acc: 0.2917 (0.3359)  loss_scale: 65536.0000 (61516.6403)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1463  max mem: 15572
Epoch: [34]  [ 490/2809]  eta: 0:22:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4365 (3.7305)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (61598.5010)  weight_decay: 0.0500 (0.0500)  time: 0.5238  data: 0.0904  max mem: 15572
[2025-01-16 06:34:10,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=96000, skipped=637, lr=[3.075723261633489e-08, 3.075723261633489e-08, 4.393890373762128e-08, 4.393890373762128e-08, 6.276986248231613e-08, 6.276986248231613e-08, 8.967123211759446e-08, 8.967123211759446e-08, 1.281017601679921e-07, 1.281017601679921e-07, 1.83002514525703e-07, 1.83002514525703e-07, 2.6143216360814715e-07, 2.6143216360814715e-07, 3.734745194402103e-07, 3.734745194402103e-07, 5.335350277717289e-07, 5.335350277717289e-07, 7.621928968167557e-07, 7.621928968167557e-07, 1.0888469954525083e-06, 1.0888469954525083e-06, 1.5554957077892975e-06, 1.5554957077892975e-06, 2.222136725413282e-06, 2.222136725413282e-06, 3.1744810363046893e-06, 3.1744810363046893e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:34:10,212] [INFO] [timer.py:260:stop] epoch=0/micro_step=96000/global_step=96000, RunningAvgSamplesPerSec=28.572255539124306, CurrSamplesPerSec=31.09164228670831, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [ 500/2809]  eta: 0:22:26  lr: 0.000003  min_lr: 0.000000  loss: 3.6491 (3.7319)  class_acc: 0.3333 (0.3360)  loss_scale: 65536.0000 (61677.0938)  weight_decay: 0.0500 (0.0500)  time: 0.5404  data: 0.1002  max mem: 15572
Epoch: [34]  [ 510/2809]  eta: 0:22:21  lr: 0.000003  min_lr: 0.000000  loss: 3.5685 (3.7285)  class_acc: 0.3750 (0.3362)  loss_scale: 65536.0000 (61752.6106)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.1600  max mem: 15572
Epoch: [34]  [ 520/2809]  eta: 0:22:13  lr: 0.000003  min_lr: 0.000000  loss: 3.5180 (3.7270)  class_acc: 0.3750 (0.3365)  loss_scale: 65536.0000 (61825.2284)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1299  max mem: 15572
Epoch: [34]  [ 530/2809]  eta: 0:22:02  lr: 0.000003  min_lr: 0.000000  loss: 3.7417 (3.7308)  class_acc: 0.3333 (0.3360)  loss_scale: 65536.0000 (61895.1111)  weight_decay: 0.0500 (0.0500)  time: 0.5020  data: 0.0533  max mem: 15572
Epoch: [34]  [ 540/2809]  eta: 0:21:57  lr: 0.000003  min_lr: 0.000000  loss: 3.9992 (3.7334)  class_acc: 0.2917 (0.3357)  loss_scale: 65536.0000 (61962.4104)  weight_decay: 0.0500 (0.0500)  time: 0.5295  data: 0.0978  max mem: 15572
Epoch: [34]  [ 550/2809]  eta: 0:21:50  lr: 0.000003  min_lr: 0.000000  loss: 3.9728 (3.7363)  class_acc: 0.2917 (0.3347)  loss_scale: 65536.0000 (62027.2668)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1371  max mem: 15572
Epoch: [34]  [ 560/2809]  eta: 0:21:42  lr: 0.000003  min_lr: 0.000000  loss: 3.9239 (3.7380)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (62089.8111)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0908  max mem: 15572
[2025-01-16 06:34:48,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96068
[2025-01-16 06:34:48,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:34:48,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 570/2809]  eta: 0:21:36  lr: 0.000003  min_lr: 0.000000  loss: 3.8473 (3.7358)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (61633.6813)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.1169  max mem: 15572
Epoch: [34]  [ 580/2809]  eta: 0:21:35  lr: 0.000003  min_lr: 0.000000  loss: 3.6832 (3.7375)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (61136.8537)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.2003  max mem: 15572
Epoch: [34]  [ 590/2809]  eta: 0:21:33  lr: 0.000003  min_lr: 0.000000  loss: 3.6910 (3.7362)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (60656.8393)  weight_decay: 0.0500 (0.0500)  time: 0.6890  data: 0.2550  max mem: 15572
Epoch: [34]  [ 600/2809]  eta: 0:21:22  lr: 0.000003  min_lr: 0.000000  loss: 3.6382 (3.7348)  class_acc: 0.2917 (0.3352)  loss_scale: 32768.0000 (60192.7987)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1303  max mem: 15572
Epoch: [34]  [ 610/2809]  eta: 0:21:15  lr: 0.000003  min_lr: 0.000000  loss: 3.6784 (3.7345)  class_acc: 0.3333 (0.3356)  loss_scale: 32768.0000 (59743.9476)  weight_decay: 0.0500 (0.0500)  time: 0.4941  data: 0.0494  max mem: 15572
Epoch: [34]  [ 620/2809]  eta: 0:21:09  lr: 0.000003  min_lr: 0.000000  loss: 3.8719 (3.7387)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (59309.5523)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1224  max mem: 15572
Epoch: [34]  [ 630/2809]  eta: 0:21:03  lr: 0.000003  min_lr: 0.000000  loss: 3.8203 (3.7368)  class_acc: 0.2917 (0.3345)  loss_scale: 32768.0000 (58888.9255)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1388  max mem: 15572
Epoch: [34]  [ 640/2809]  eta: 0:20:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7100 (3.7386)  class_acc: 0.2917 (0.3336)  loss_scale: 32768.0000 (58481.4228)  weight_decay: 0.0500 (0.0500)  time: 0.5437  data: 0.0943  max mem: 15572
Epoch: [34]  [ 650/2809]  eta: 0:20:47  lr: 0.000003  min_lr: 0.000000  loss: 3.9373 (3.7399)  class_acc: 0.2500 (0.3336)  loss_scale: 32768.0000 (58086.4393)  weight_decay: 0.0500 (0.0500)  time: 0.5134  data: 0.0581  max mem: 15572
Epoch: [34]  [ 660/2809]  eta: 0:20:43  lr: 0.000003  min_lr: 0.000000  loss: 3.8035 (3.7410)  class_acc: 0.2917 (0.3332)  loss_scale: 32768.0000 (57703.4070)  weight_decay: 0.0500 (0.0500)  time: 0.5812  data: 0.1227  max mem: 15572
Epoch: [34]  [ 670/2809]  eta: 0:20:33  lr: 0.000003  min_lr: 0.000000  loss: 3.8035 (3.7416)  class_acc: 0.2917 (0.3328)  loss_scale: 32768.0000 (57331.7914)  weight_decay: 0.0500 (0.0500)  time: 0.5427  data: 0.0940  max mem: 15572
Epoch: [34]  [ 680/2809]  eta: 0:20:32  lr: 0.000003  min_lr: 0.000000  loss: 3.7870 (3.7421)  class_acc: 0.2917 (0.3329)  loss_scale: 32768.0000 (56971.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5835  data: 0.1363  max mem: 15572
Epoch: [34]  [ 690/2809]  eta: 0:20:26  lr: 0.000003  min_lr: 0.000000  loss: 3.7870 (3.7429)  class_acc: 0.2917 (0.3322)  loss_scale: 32768.0000 (56620.8278)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1971  max mem: 15572
[2025-01-16 06:36:03,248] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:36:03,249] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 700/2809]  eta: 0:20:19  lr: 0.000003  min_lr: 0.000000  loss: 3.5529 (3.7375)  class_acc: 0.3333 (0.3330)  loss_scale: 32768.0000 (56748.0057)  weight_decay: 0.0500 (0.0500)  time: 0.5471  data: 0.0922  max mem: 15572
Epoch: [34]  [ 710/2809]  eta: 0:20:10  lr: 0.000003  min_lr: 0.000000  loss: 3.5293 (3.7360)  class_acc: 0.4167 (0.3343)  loss_scale: 65536.0000 (56871.6062)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0705  max mem: 15572
Epoch: [34]  [ 720/2809]  eta: 0:20:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6037 (3.7339)  class_acc: 0.4167 (0.3346)  loss_scale: 65536.0000 (56991.7781)  weight_decay: 0.0500 (0.0500)  time: 0.4727  data: 0.0444  max mem: 15572
Epoch: [34]  [ 730/2809]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000000  loss: 3.6832 (3.7348)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (57108.6621)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0841  max mem: 15572
Epoch: [34]  [ 740/2809]  eta: 0:19:50  lr: 0.000003  min_lr: 0.000000  loss: 3.6914 (3.7331)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (57222.3914)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1343  max mem: 15572
Epoch: [34]  [ 750/2809]  eta: 0:19:44  lr: 0.000003  min_lr: 0.000000  loss: 3.5948 (3.7318)  class_acc: 0.3750 (0.3357)  loss_scale: 65536.0000 (57333.0919)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1047  max mem: 15572
Epoch: [34]  [ 760/2809]  eta: 0:19:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6342 (3.7315)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (57440.8830)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1074  max mem: 15572
Epoch: [34]  [ 770/2809]  eta: 0:19:33  lr: 0.000003  min_lr: 0.000000  loss: 3.5779 (3.7312)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (57545.8781)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1482  max mem: 15572
Epoch: [34]  [ 780/2809]  eta: 0:19:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5617 (3.7301)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (57648.1844)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1163  max mem: 15572
Epoch: [34]  [ 790/2809]  eta: 0:19:21  lr: 0.000003  min_lr: 0.000000  loss: 3.4786 (3.7275)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (57747.9039)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1040  max mem: 15572
Epoch: [34]  [ 800/2809]  eta: 0:19:15  lr: 0.000003  min_lr: 0.000000  loss: 3.5604 (3.7295)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (57845.1336)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1361  max mem: 15572
Epoch: [34]  [ 810/2809]  eta: 0:19:09  lr: 0.000003  min_lr: 0.000000  loss: 3.5749 (3.7281)  class_acc: 0.3333 (0.3356)  loss_scale: 65536.0000 (57939.9655)  weight_decay: 0.0500 (0.0500)  time: 0.5657  data: 0.1174  max mem: 15572
[2025-01-16 06:37:14,150] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:37:14,150] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 820/2809]  eta: 0:19:02  lr: 0.000003  min_lr: 0.000000  loss: 3.5649 (3.7256)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (58192.1364)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1101  max mem: 15572
[2025-01-16 06:37:15,827] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96329
[2025-01-16 06:37:15,827] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:37:15,827] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 830/2809]  eta: 0:18:56  lr: 0.000003  min_lr: 0.000000  loss: 3.7548 (3.7266)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (58438.2383)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1127  max mem: 15572
[2025-01-16 06:37:21,538] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96339
[2025-01-16 06:37:21,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:37:21,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 840/2809]  eta: 0:18:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7578 (3.7256)  class_acc: 0.2917 (0.3365)  loss_scale: 65536.0000 (58210.9298)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1366  max mem: 15572
Epoch: [34]  [ 850/2809]  eta: 0:18:45  lr: 0.000003  min_lr: 0.000000  loss: 3.5081 (3.7203)  class_acc: 0.3750 (0.3376)  loss_scale: 32768.0000 (57911.9530)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1332  max mem: 15572
Epoch: [34]  [ 860/2809]  eta: 0:18:39  lr: 0.000003  min_lr: 0.000000  loss: 3.5081 (3.7217)  class_acc: 0.4167 (0.3375)  loss_scale: 32768.0000 (57619.9210)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1110  max mem: 15572
Epoch: [34]  [ 870/2809]  eta: 0:18:34  lr: 0.000003  min_lr: 0.000000  loss: 3.8601 (3.7244)  class_acc: 0.2500 (0.3366)  loss_scale: 32768.0000 (57334.5947)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.1380  max mem: 15572
Epoch: [34]  [ 880/2809]  eta: 0:18:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7920 (3.7232)  class_acc: 0.3333 (0.3372)  loss_scale: 32768.0000 (57055.7457)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1270  max mem: 15572
Epoch: [34]  [ 890/2809]  eta: 0:18:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7447 (3.7242)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (56783.1560)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1049  max mem: 15572
Epoch: [34]  [ 900/2809]  eta: 0:18:15  lr: 0.000003  min_lr: 0.000000  loss: 3.8894 (3.7266)  class_acc: 0.3333 (0.3368)  loss_scale: 32768.0000 (56516.6171)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1163  max mem: 15572
Epoch: [34]  [ 910/2809]  eta: 0:18:09  lr: 0.000003  min_lr: 0.000000  loss: 3.9131 (3.7282)  class_acc: 0.3333 (0.3364)  loss_scale: 32768.0000 (56255.9297)  weight_decay: 0.0500 (0.0500)  time: 0.5501  data: 0.0952  max mem: 15572
Epoch: [34]  [ 920/2809]  eta: 0:18:01  lr: 0.000003  min_lr: 0.000000  loss: 3.8603 (3.7288)  class_acc: 0.2917 (0.3366)  loss_scale: 32768.0000 (56000.9034)  weight_decay: 0.0500 (0.0500)  time: 0.4988  data: 0.0424  max mem: 15572
Epoch: [34]  [ 930/2809]  eta: 0:17:55  lr: 0.000003  min_lr: 0.000000  loss: 3.7424 (3.7284)  class_acc: 0.2917 (0.3361)  loss_scale: 32768.0000 (55751.3555)  weight_decay: 0.0500 (0.0500)  time: 0.5150  data: 0.0636  max mem: 15572
Epoch: [34]  [ 940/2809]  eta: 0:17:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7065 (3.7302)  class_acc: 0.2917 (0.3359)  loss_scale: 32768.0000 (55507.1116)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1613  max mem: 15572
Epoch: [34]  [ 950/2809]  eta: 0:17:44  lr: 0.000003  min_lr: 0.000000  loss: 3.6316 (3.7319)  class_acc: 0.2917 (0.3364)  loss_scale: 32768.0000 (55268.0042)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1415  max mem: 15572
Epoch: [34]  [ 960/2809]  eta: 0:17:37  lr: 0.000003  min_lr: 0.000000  loss: 3.9828 (3.7331)  class_acc: 0.2500 (0.3362)  loss_scale: 32768.0000 (55033.8730)  weight_decay: 0.0500 (0.0500)  time: 0.5156  data: 0.0733  max mem: 15572
[2025-01-16 06:38:33,552] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:38:33,552] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [ 970/2809]  eta: 0:17:31  lr: 0.000003  min_lr: 0.000000  loss: 3.9828 (3.7348)  class_acc: 0.2500 (0.3361)  loss_scale: 32768.0000 (55108.2842)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0819  max mem: 15572
Epoch: [34]  [ 980/2809]  eta: 0:17:27  lr: 0.000003  min_lr: 0.000000  loss: 3.7920 (3.7353)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (55214.5810)  weight_decay: 0.0500 (0.0500)  time: 0.6158  data: 0.1646  max mem: 15572
[2025-01-16 06:38:46,359] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96489
[2025-01-16 06:38:46,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:38:46,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [ 990/2809]  eta: 0:17:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7711 (3.7361)  class_acc: 0.3333 (0.3364)  loss_scale: 65536.0000 (55054.2079)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1902  max mem: 15572
Epoch: [34]  [1000/2809]  eta: 0:17:17  lr: 0.000003  min_lr: 0.000000  loss: 3.8074 (3.7363)  class_acc: 0.3333 (0.3366)  loss_scale: 32768.0000 (54831.5684)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1516  max mem: 15572
Epoch: [34]  [1010/2809]  eta: 0:17:10  lr: 0.000003  min_lr: 0.000000  loss: 3.9743 (3.7362)  class_acc: 0.2917 (0.3363)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1050  max mem: 15572
Epoch: [34]  [1020/2809]  eta: 0:17:03  lr: 0.000003  min_lr: 0.000000  loss: 3.8936 (3.7367)  class_acc: 0.2500 (0.3360)  loss_scale: 32768.0000 (54399.3732)  weight_decay: 0.0500 (0.0500)  time: 0.5161  data: 0.0770  max mem: 15572
Epoch: [34]  [1030/2809]  eta: 0:16:58  lr: 0.000003  min_lr: 0.000000  loss: 3.8849 (3.7380)  class_acc: 0.2917 (0.3358)  loss_scale: 32768.0000 (54189.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1117  max mem: 15572
Epoch: [34]  [1040/2809]  eta: 0:16:52  lr: 0.000003  min_lr: 0.000000  loss: 3.7699 (3.7371)  class_acc: 0.3750 (0.3361)  loss_scale: 32768.0000 (53983.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1174  max mem: 15572
Epoch: [34]  [1050/2809]  eta: 0:16:46  lr: 0.000003  min_lr: 0.000000  loss: 3.5501 (3.7376)  class_acc: 0.3333 (0.3360)  loss_scale: 32768.0000 (53781.9220)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1139  max mem: 15572
Epoch: [34]  [1060/2809]  eta: 0:16:39  lr: 0.000003  min_lr: 0.000000  loss: 3.8503 (3.7408)  class_acc: 0.2917 (0.3353)  loss_scale: 32768.0000 (53583.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1156  max mem: 15572
Epoch: [34]  [1070/2809]  eta: 0:16:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7496 (3.7390)  class_acc: 0.2917 (0.3357)  loss_scale: 32768.0000 (53389.5051)  weight_decay: 0.0500 (0.0500)  time: 0.5426  data: 0.1081  max mem: 15572
Epoch: [34]  [1080/2809]  eta: 0:16:29  lr: 0.000003  min_lr: 0.000000  loss: 3.7406 (3.7400)  class_acc: 0.3333 (0.3353)  loss_scale: 32768.0000 (53198.7419)  weight_decay: 0.0500 (0.0500)  time: 0.5965  data: 0.1437  max mem: 15572
Epoch: [34]  [1090/2809]  eta: 0:16:23  lr: 0.000003  min_lr: 0.000000  loss: 3.9100 (3.7426)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (53011.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1421  max mem: 15572
Epoch: [34]  [1100/2809]  eta: 0:16:18  lr: 0.000003  min_lr: 0.000000  loss: 3.9100 (3.7436)  class_acc: 0.2917 (0.3345)  loss_scale: 32768.0000 (52827.6113)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1186  max mem: 15572
Epoch: [34]  [1110/2809]  eta: 0:16:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6844 (3.7405)  class_acc: 0.3333 (0.3352)  loss_scale: 32768.0000 (52647.0567)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.0875  max mem: 15572
[2025-01-16 06:39:58,961] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:39:58,962] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1120/2809]  eta: 0:16:05  lr: 0.000003  min_lr: 0.000000  loss: 3.6528 (3.7413)  class_acc: 0.3750 (0.3352)  loss_scale: 32768.0000 (52732.8029)  weight_decay: 0.0500 (0.0500)  time: 0.5251  data: 0.0847  max mem: 15572
Epoch: [34]  [1130/2809]  eta: 0:15:59  lr: 0.000003  min_lr: 0.000000  loss: 3.7947 (3.7416)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (52846.0053)  weight_decay: 0.0500 (0.0500)  time: 0.5528  data: 0.1162  max mem: 15572
Epoch: [34]  [1140/2809]  eta: 0:15:54  lr: 0.000003  min_lr: 0.000000  loss: 3.8818 (3.7415)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (52957.2235)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1458  max mem: 15572
Epoch: [34]  [1150/2809]  eta: 0:15:48  lr: 0.000003  min_lr: 0.000000  loss: 3.8818 (3.7429)  class_acc: 0.2917 (0.3345)  loss_scale: 65536.0000 (53066.5091)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1584  max mem: 15572
Epoch: [34]  [1160/2809]  eta: 0:15:40  lr: 0.000003  min_lr: 0.000000  loss: 3.9973 (3.7447)  class_acc: 0.2500 (0.3342)  loss_scale: 65536.0000 (53173.9121)  weight_decay: 0.0500 (0.0500)  time: 0.5044  data: 0.0664  max mem: 15572
Epoch: [34]  [1170/2809]  eta: 0:15:34  lr: 0.000003  min_lr: 0.000000  loss: 3.6710 (3.7434)  class_acc: 0.3333 (0.3340)  loss_scale: 65536.0000 (53279.4808)  weight_decay: 0.0500 (0.0500)  time: 0.4830  data: 0.0486  max mem: 15572
Epoch: [34]  [1180/2809]  eta: 0:15:28  lr: 0.000003  min_lr: 0.000000  loss: 3.6292 (3.7430)  class_acc: 0.3333 (0.3344)  loss_scale: 65536.0000 (53383.2616)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0867  max mem: 15572
Epoch: [34]  [1190/2809]  eta: 0:15:22  lr: 0.000003  min_lr: 0.000000  loss: 3.7076 (3.7424)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (53485.2997)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1106  max mem: 15572
Epoch: [34]  [1200/2809]  eta: 0:15:17  lr: 0.000003  min_lr: 0.000000  loss: 3.7606 (3.7431)  class_acc: 0.3333 (0.3351)  loss_scale: 65536.0000 (53585.6386)  weight_decay: 0.0500 (0.0500)  time: 0.5945  data: 0.1472  max mem: 15572
Epoch: [34]  [1210/2809]  eta: 0:15:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7606 (3.7423)  class_acc: 0.4167 (0.3356)  loss_scale: 65536.0000 (53684.3204)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1196  max mem: 15572
Epoch: [34]  [1220/2809]  eta: 0:15:05  lr: 0.000003  min_lr: 0.000000  loss: 3.6396 (3.7424)  class_acc: 0.3750 (0.3354)  loss_scale: 65536.0000 (53781.3857)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.1084  max mem: 15572
Epoch: [34]  [1230/2809]  eta: 0:15:00  lr: 0.000003  min_lr: 0.000000  loss: 3.8287 (3.7433)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (53876.8741)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.1332  max mem: 15572
[2025-01-16 06:41:10,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:41:10,485] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [1240/2809]  eta: 0:14:54  lr: 0.000003  min_lr: 0.000000  loss: 3.8287 (3.7439)  class_acc: 0.2917 (0.3352)  loss_scale: 65536.0000 (54023.6326)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1148  max mem: 15572
[2025-01-16 06:41:10,877] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96747
[2025-01-16 06:41:10,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:41:10,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1250/2809]  eta: 0:14:48  lr: 0.000003  min_lr: 0.000000  loss: 3.8229 (3.7448)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (54115.6579)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0896  max mem: 15572
[2025-01-16 06:41:17,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 96758
[2025-01-16 06:41:17,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:41:17,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1260/2809]  eta: 0:14:42  lr: 0.000003  min_lr: 0.000000  loss: 3.8517 (3.7452)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (53972.3521)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.0942  max mem: 15572
Epoch: [34]  [1270/2809]  eta: 0:14:37  lr: 0.000003  min_lr: 0.000000  loss: 3.7095 (3.7446)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (53805.5201)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1279  max mem: 15572
Epoch: [34]  [1280/2809]  eta: 0:14:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6508 (3.7450)  class_acc: 0.2917 (0.3346)  loss_scale: 32768.0000 (53641.2927)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1284  max mem: 15572
Epoch: [34]  [1290/2809]  eta: 0:14:26  lr: 0.000003  min_lr: 0.000000  loss: 3.8592 (3.7463)  class_acc: 0.2917 (0.3343)  loss_scale: 32768.0000 (53479.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1644  max mem: 15572
Epoch: [34]  [1300/2809]  eta: 0:14:20  lr: 0.000003  min_lr: 0.000000  loss: 3.8090 (3.7464)  class_acc: 0.2917 (0.3344)  loss_scale: 32768.0000 (53320.4120)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1811  max mem: 15572
Epoch: [34]  [1310/2809]  eta: 0:14:16  lr: 0.000003  min_lr: 0.000000  loss: 3.6814 (3.7462)  class_acc: 0.3333 (0.3345)  loss_scale: 32768.0000 (53163.6430)  weight_decay: 0.0500 (0.0500)  time: 0.6143  data: 0.1592  max mem: 15572
Epoch: [34]  [1320/2809]  eta: 0:14:09  lr: 0.000003  min_lr: 0.000000  loss: 3.9252 (3.7466)  class_acc: 0.3333 (0.3344)  loss_scale: 32768.0000 (53009.2475)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1228  max mem: 15572
Epoch: [34]  [1330/2809]  eta: 0:14:03  lr: 0.000003  min_lr: 0.000000  loss: 3.7154 (3.7467)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (52857.1721)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1044  max mem: 15572
Epoch: [34]  [1340/2809]  eta: 0:13:57  lr: 0.000003  min_lr: 0.000000  loss: 3.6401 (3.7457)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (52707.3647)  weight_decay: 0.0500 (0.0500)  time: 0.5642  data: 0.1297  max mem: 15572
Epoch: [34]  [1350/2809]  eta: 0:13:51  lr: 0.000003  min_lr: 0.000000  loss: 3.6401 (3.7444)  class_acc: 0.2917 (0.3347)  loss_scale: 32768.0000 (52559.7750)  weight_decay: 0.0500 (0.0500)  time: 0.5409  data: 0.1077  max mem: 15572
Epoch: [34]  [1360/2809]  eta: 0:13:45  lr: 0.000003  min_lr: 0.000000  loss: 3.7151 (3.7433)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (52414.3542)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.1013  max mem: 15572
Epoch: [34]  [1370/2809]  eta: 0:13:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7365 (3.7438)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (52271.0547)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0837  max mem: 15572
Epoch: [34]  [1380/2809]  eta: 0:13:33  lr: 0.000003  min_lr: 0.000000  loss: 3.8961 (3.7445)  class_acc: 0.2917 (0.3349)  loss_scale: 32768.0000 (52129.8306)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0736  max mem: 15572
[2025-01-16 06:42:29,663] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:42:29,663] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [1390/2809]  eta: 0:13:27  lr: 0.000003  min_lr: 0.000000  loss: 3.8078 (3.7439)  class_acc: 0.2917 (0.3349)  loss_scale: 32768.0000 (52226.2085)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.0818  max mem: 15572
Epoch: [34]  [1400/2809]  eta: 0:13:21  lr: 0.000003  min_lr: 0.000000  loss: 3.6062 (3.7425)  class_acc: 0.3750 (0.3351)  loss_scale: 65536.0000 (52321.2106)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1080  max mem: 15572
Epoch: [34]  [1410/2809]  eta: 0:13:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5346 (3.7415)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (52414.8661)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1462  max mem: 15572
Epoch: [34]  [1420/2809]  eta: 0:13:11  lr: 0.000003  min_lr: 0.000000  loss: 3.7656 (3.7430)  class_acc: 0.2917 (0.3354)  loss_scale: 65536.0000 (52507.2034)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.1822  max mem: 15572
Epoch: [34]  [1430/2809]  eta: 0:13:05  lr: 0.000003  min_lr: 0.000000  loss: 3.8272 (3.7417)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (52598.2502)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.1627  max mem: 15572
Epoch: [34]  [1440/2809]  eta: 0:13:00  lr: 0.000003  min_lr: 0.000000  loss: 3.5849 (3.7415)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (52688.0333)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1354  max mem: 15572
Epoch: [34]  [1450/2809]  eta: 0:12:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7362 (3.7421)  class_acc: 0.3333 (0.3354)  loss_scale: 65536.0000 (52776.5789)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1236  max mem: 15572
Epoch: [34]  [1460/2809]  eta: 0:12:47  lr: 0.000003  min_lr: 0.000000  loss: 3.5508 (3.7402)  class_acc: 0.3750 (0.3355)  loss_scale: 65536.0000 (52863.9124)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0808  max mem: 15572
Epoch: [34]  [1470/2809]  eta: 0:12:42  lr: 0.000003  min_lr: 0.000000  loss: 3.4296 (3.7395)  class_acc: 0.3750 (0.3357)  loss_scale: 65536.0000 (52950.0585)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0990  max mem: 15572
Epoch: [34]  [1480/2809]  eta: 0:12:37  lr: 0.000003  min_lr: 0.000000  loss: 3.5722 (3.7390)  class_acc: 0.2917 (0.3355)  loss_scale: 65536.0000 (53035.0412)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1499  max mem: 15572
Epoch: [34]  [1490/2809]  eta: 0:12:31  lr: 0.000003  min_lr: 0.000000  loss: 3.7048 (3.7400)  class_acc: 0.2500 (0.3350)  loss_scale: 65536.0000 (53118.8840)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.1614  max mem: 15572
[2025-01-16 06:43:34,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=97000, skipped=643, lr=[2.723864396118874e-08, 2.723864396118874e-08, 3.891234851598392e-08, 3.891234851598392e-08, 5.558906930854846e-08, 5.558906930854846e-08, 7.941295615506924e-08, 7.941295615506924e-08, 1.1344708022152749e-07, 1.1344708022152749e-07, 1.62067257459325e-07, 1.62067257459325e-07, 2.3152465351332144e-07, 2.3152465351332144e-07, 3.3074950501903063e-07, 3.3074950501903063e-07, 4.724992928843295e-07, 4.724992928843295e-07, 6.749989898347565e-07, 6.749989898347565e-07, 9.642842711925092e-07, 9.642842711925092e-07, 1.377548958846442e-06, 1.377548958846442e-06, 1.967927084066346e-06, 1.967927084066346e-06, 2.8113244058090655e-06, 2.8113244058090655e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:43:34,176] [INFO] [timer.py:260:stop] epoch=0/micro_step=97000/global_step=97000, RunningAvgSamplesPerSec=28.572398842804514, CurrSamplesPerSec=27.741864150340216, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [1500/2809]  eta: 0:12:26  lr: 0.000003  min_lr: 0.000000  loss: 3.9431 (3.7399)  class_acc: 0.2500 (0.3352)  loss_scale: 65536.0000 (53201.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.1553  max mem: 15572
[2025-01-16 06:43:43,688] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:43:43,689] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [1510/2809]  eta: 0:12:20  lr: 0.000003  min_lr: 0.000000  loss: 3.9661 (3.7421)  class_acc: 0.2917 (0.3346)  loss_scale: 65536.0000 (53369.9854)  weight_decay: 0.0500 (0.0500)  time: 0.5631  data: 0.1361  max mem: 15572
[2025-01-16 06:43:44,978] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97018
[2025-01-16 06:43:44,979] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:43:44,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1520/2809]  eta: 0:12:14  lr: 0.000003  min_lr: 0.000000  loss: 4.0985 (3.7430)  class_acc: 0.2500 (0.3342)  loss_scale: 65536.0000 (53493.0598)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1256  max mem: 15572
Epoch: [34]  [1530/2809]  eta: 0:12:09  lr: 0.000003  min_lr: 0.000000  loss: 3.8743 (3.7428)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (53571.7204)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1456  max mem: 15572
Epoch: [34]  [1540/2809]  eta: 0:12:03  lr: 0.000003  min_lr: 0.000000  loss: 3.7568 (3.7416)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (53649.3602)  weight_decay: 0.0500 (0.0500)  time: 0.5586  data: 0.1281  max mem: 15572
Epoch: [34]  [1550/2809]  eta: 0:11:57  lr: 0.000003  min_lr: 0.000000  loss: 3.8167 (3.7420)  class_acc: 0.2917 (0.3347)  loss_scale: 65536.0000 (53725.9987)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1206  max mem: 15572
Epoch: [34]  [1560/2809]  eta: 0:11:51  lr: 0.000003  min_lr: 0.000000  loss: 3.9075 (3.7433)  class_acc: 0.2917 (0.3344)  loss_scale: 65536.0000 (53801.6553)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0837  max mem: 15572
Epoch: [34]  [1570/2809]  eta: 0:11:45  lr: 0.000003  min_lr: 0.000000  loss: 3.8686 (3.7435)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (53876.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5037  data: 0.0584  max mem: 15572
Epoch: [34]  [1580/2809]  eta: 0:11:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7349 (3.7434)  class_acc: 0.3333 (0.3346)  loss_scale: 65536.0000 (53950.0974)  weight_decay: 0.0500 (0.0500)  time: 0.5182  data: 0.0777  max mem: 15572
Epoch: [34]  [1590/2809]  eta: 0:11:34  lr: 0.000003  min_lr: 0.000000  loss: 3.7063 (3.7438)  class_acc: 0.2917 (0.3343)  loss_scale: 65536.0000 (54022.9189)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.1712  max mem: 15572
Epoch: [34]  [1600/2809]  eta: 0:11:28  lr: 0.000003  min_lr: 0.000000  loss: 3.9266 (3.7452)  class_acc: 0.2500 (0.3338)  loss_scale: 65536.0000 (54094.8307)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.1971  max mem: 15572
Epoch: [34]  [1610/2809]  eta: 0:11:23  lr: 0.000003  min_lr: 0.000000  loss: 3.8985 (3.7444)  class_acc: 0.2917 (0.3339)  loss_scale: 65536.0000 (54165.8498)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1270  max mem: 15572
Epoch: [34]  [1620/2809]  eta: 0:11:17  lr: 0.000003  min_lr: 0.000000  loss: 3.7474 (3.7438)  class_acc: 0.3333 (0.3343)  loss_scale: 65536.0000 (54235.9926)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.1349  max mem: 15572
Epoch: [34]  [1630/2809]  eta: 0:11:11  lr: 0.000003  min_lr: 0.000000  loss: 3.6296 (3.7431)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (54305.2753)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1042  max mem: 15572
Epoch: [34]  [1640/2809]  eta: 0:11:05  lr: 0.000003  min_lr: 0.000000  loss: 3.7292 (3.7442)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (54373.7136)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1012  max mem: 15572
[2025-01-16 06:44:58,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:44:58,411] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:44:59,229] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97149
[2025-01-16 06:44:59,229] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:44:59,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1650/2809]  eta: 0:11:00  lr: 0.000003  min_lr: 0.000000  loss: 3.6248 (3.7430)  class_acc: 0.3750 (0.3353)  loss_scale: 65536.0000 (54520.7123)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.1315  max mem: 15572
Epoch: [34]  [1660/2809]  eta: 0:10:54  lr: 0.000003  min_lr: 0.000000  loss: 3.6028 (3.7428)  class_acc: 0.4167 (0.3354)  loss_scale: 65536.0000 (54587.0295)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.1288  max mem: 15572
Epoch: [34]  [1670/2809]  eta: 0:10:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7309 (3.7428)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (54652.5530)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1189  max mem: 15572
Epoch: [34]  [1680/2809]  eta: 0:10:42  lr: 0.000003  min_lr: 0.000000  loss: 3.8959 (3.7439)  class_acc: 0.2917 (0.3347)  loss_scale: 65536.0000 (54717.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1030  max mem: 15572
Epoch: [34]  [1690/2809]  eta: 0:10:37  lr: 0.000003  min_lr: 0.000000  loss: 3.8473 (3.7432)  class_acc: 0.2917 (0.3348)  loss_scale: 65536.0000 (54781.2750)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1530  max mem: 15572
Epoch: [34]  [1700/2809]  eta: 0:10:31  lr: 0.000003  min_lr: 0.000000  loss: 3.5178 (3.7426)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (54844.5009)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1328  max mem: 15572
Epoch: [34]  [1710/2809]  eta: 0:10:25  lr: 0.000003  min_lr: 0.000000  loss: 3.7335 (3.7434)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (54906.9877)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0969  max mem: 15572
Epoch: [34]  [1720/2809]  eta: 0:10:19  lr: 0.000003  min_lr: 0.000000  loss: 3.7345 (3.7439)  class_acc: 0.2917 (0.3349)  loss_scale: 65536.0000 (54968.7484)  weight_decay: 0.0500 (0.0500)  time: 0.5418  data: 0.0956  max mem: 15572
Epoch: [34]  [1730/2809]  eta: 0:10:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6831 (3.7438)  class_acc: 0.3333 (0.3347)  loss_scale: 65536.0000 (55029.7955)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0217  max mem: 15572
Epoch: [34]  [1740/2809]  eta: 0:10:07  lr: 0.000003  min_lr: 0.000000  loss: 3.6526 (3.7440)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (55090.1413)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0450  max mem: 15572
Epoch: [34]  [1750/2809]  eta: 0:10:01  lr: 0.000003  min_lr: 0.000000  loss: 3.8288 (3.7433)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (55149.7978)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1134  max mem: 15572
Epoch: [34]  [1760/2809]  eta: 0:09:56  lr: 0.000003  min_lr: 0.000000  loss: 3.5934 (3.7420)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (55208.7768)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1457  max mem: 15572
Epoch: [34]  [1770/2809]  eta: 0:09:49  lr: 0.000003  min_lr: 0.000000  loss: 3.7199 (3.7426)  class_acc: 0.2917 (0.3351)  loss_scale: 65536.0000 (55267.0898)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0771  max mem: 15572
[2025-01-16 06:46:09,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:46:09,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:46:10,268] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97280
[2025-01-16 06:46:10,268] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:46:10,268] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1780/2809]  eta: 0:09:43  lr: 0.000003  min_lr: 0.000000  loss: 3.7702 (3.7423)  class_acc: 0.2917 (0.3352)  loss_scale: 65536.0000 (55398.3425)  weight_decay: 0.0500 (0.0500)  time: 0.4945  data: 0.0617  max mem: 15572
Epoch: [34]  [1790/2809]  eta: 0:09:38  lr: 0.000003  min_lr: 0.000000  loss: 3.8054 (3.7426)  class_acc: 0.3333 (0.3354)  loss_scale: 65536.0000 (55454.9458)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1157  max mem: 15572
Epoch: [34]  [1800/2809]  eta: 0:09:32  lr: 0.000003  min_lr: 0.000000  loss: 4.0041 (3.7431)  class_acc: 0.2917 (0.3356)  loss_scale: 65536.0000 (55510.9206)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1506  max mem: 15572
Epoch: [34]  [1810/2809]  eta: 0:09:27  lr: 0.000003  min_lr: 0.000000  loss: 3.8576 (3.7434)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (55566.2772)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1629  max mem: 15572
Epoch: [34]  [1820/2809]  eta: 0:09:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7106 (3.7427)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (55621.0258)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0669  max mem: 15572
Epoch: [34]  [1830/2809]  eta: 0:09:16  lr: 0.000003  min_lr: 0.000000  loss: 3.5602 (3.7430)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (55675.1764)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1056  max mem: 15572
Epoch: [34]  [1840/2809]  eta: 0:09:09  lr: 0.000003  min_lr: 0.000000  loss: 3.9664 (3.7432)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (55728.7387)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1089  max mem: 15572
Epoch: [34]  [1850/2809]  eta: 0:09:04  lr: 0.000003  min_lr: 0.000000  loss: 3.9664 (3.7445)  class_acc: 0.3333 (0.3359)  loss_scale: 65536.0000 (55781.7223)  weight_decay: 0.0500 (0.0500)  time: 0.5329  data: 0.0818  max mem: 15572
Epoch: [34]  [1860/2809]  eta: 0:08:58  lr: 0.000003  min_lr: 0.000000  loss: 3.8678 (3.7448)  class_acc: 0.2917 (0.3356)  loss_scale: 65536.0000 (55834.1365)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1393  max mem: 15572
Epoch: [34]  [1870/2809]  eta: 0:08:52  lr: 0.000003  min_lr: 0.000000  loss: 3.8391 (3.7446)  class_acc: 0.3333 (0.3356)  loss_scale: 65536.0000 (55885.9904)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1075  max mem: 15572
Epoch: [34]  [1880/2809]  eta: 0:08:47  lr: 0.000003  min_lr: 0.000000  loss: 3.8321 (3.7449)  class_acc: 0.3333 (0.3354)  loss_scale: 65536.0000 (55937.2929)  weight_decay: 0.0500 (0.0500)  time: 0.5415  data: 0.1014  max mem: 15572
Epoch: [34]  [1890/2809]  eta: 0:08:41  lr: 0.000003  min_lr: 0.000000  loss: 3.9054 (3.7461)  class_acc: 0.2917 (0.3354)  loss_scale: 65536.0000 (55988.0529)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.1634  max mem: 15572
Epoch: [34]  [1900/2809]  eta: 0:08:35  lr: 0.000003  min_lr: 0.000000  loss: 3.8211 (3.7451)  class_acc: 0.3333 (0.3357)  loss_scale: 65536.0000 (56038.2788)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.1519  max mem: 15572
[2025-01-16 06:47:23,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:47:23,512] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:47:24,043] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97410
[2025-01-16 06:47:24,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:47:24,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [1910/2809]  eta: 0:08:30  lr: 0.000003  min_lr: 0.000000  loss: 3.7076 (3.7453)  class_acc: 0.3333 (0.3357)  loss_scale: 65536.0000 (56122.2732)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0886  max mem: 15572
Epoch: [34]  [1920/2809]  eta: 0:08:24  lr: 0.000003  min_lr: 0.000000  loss: 3.5891 (3.7434)  class_acc: 0.3750 (0.3362)  loss_scale: 65536.0000 (56171.2775)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0759  max mem: 15572
Epoch: [34]  [1930/2809]  eta: 0:08:18  lr: 0.000003  min_lr: 0.000000  loss: 3.6002 (3.7435)  class_acc: 0.3750 (0.3364)  loss_scale: 65536.0000 (56219.7742)  weight_decay: 0.0500 (0.0500)  time: 0.4808  data: 0.0502  max mem: 15572
Epoch: [34]  [1940/2809]  eta: 0:08:12  lr: 0.000003  min_lr: 0.000000  loss: 3.7873 (3.7440)  class_acc: 0.3333 (0.3363)  loss_scale: 65536.0000 (56267.7713)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.1034  max mem: 15572
Epoch: [34]  [1950/2809]  eta: 0:08:06  lr: 0.000003  min_lr: 0.000000  loss: 3.7327 (3.7436)  class_acc: 0.2917 (0.3362)  loss_scale: 65536.0000 (56315.2763)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1153  max mem: 15572
Epoch: [34]  [1960/2809]  eta: 0:08:01  lr: 0.000003  min_lr: 0.000000  loss: 3.6926 (3.7426)  class_acc: 0.2917 (0.3362)  loss_scale: 65536.0000 (56362.2968)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1045  max mem: 15572
[2025-01-16 06:48:00,391] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97476
[2025-01-16 06:48:00,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:48:00,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [34]  [1970/2809]  eta: 0:07:55  lr: 0.000003  min_lr: 0.000000  loss: 3.5457 (3.7417)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (56392.2151)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1222  max mem: 15572
Epoch: [34]  [1980/2809]  eta: 0:07:49  lr: 0.000003  min_lr: 0.000000  loss: 3.5457 (3.7408)  class_acc: 0.4167 (0.3370)  loss_scale: 32768.0000 (56272.9611)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.0896  max mem: 15572
Epoch: [34]  [1990/2809]  eta: 0:07:43  lr: 0.000003  min_lr: 0.000000  loss: 3.5045 (3.7389)  class_acc: 0.4167 (0.3375)  loss_scale: 32768.0000 (56154.9051)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.1084  max mem: 15572
Epoch: [34]  [2000/2809]  eta: 0:07:38  lr: 0.000003  min_lr: 0.000000  loss: 3.6670 (3.7391)  class_acc: 0.3750 (0.3374)  loss_scale: 32768.0000 (56038.0290)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1342  max mem: 15572
Epoch: [34]  [2010/2809]  eta: 0:07:32  lr: 0.000003  min_lr: 0.000000  loss: 3.7641 (3.7384)  class_acc: 0.3333 (0.3374)  loss_scale: 32768.0000 (55922.3153)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1660  max mem: 15572
Epoch: [34]  [2020/2809]  eta: 0:07:27  lr: 0.000003  min_lr: 0.000000  loss: 3.5142 (3.7373)  class_acc: 0.3333 (0.3378)  loss_scale: 32768.0000 (55807.7467)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1912  max mem: 15572
Epoch: [34]  [2030/2809]  eta: 0:07:21  lr: 0.000003  min_lr: 0.000000  loss: 3.7767 (3.7380)  class_acc: 0.2917 (0.3375)  loss_scale: 32768.0000 (55694.3063)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1235  max mem: 15572
Epoch: [34]  [2040/2809]  eta: 0:07:16  lr: 0.000003  min_lr: 0.000000  loss: 3.8981 (3.7377)  class_acc: 0.2917 (0.3377)  loss_scale: 32768.0000 (55581.9775)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1239  max mem: 15572
Epoch: [34]  [2050/2809]  eta: 0:07:10  lr: 0.000003  min_lr: 0.000000  loss: 3.8460 (3.7384)  class_acc: 0.2917 (0.3376)  loss_scale: 32768.0000 (55470.7440)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1163  max mem: 15572
Epoch: [34]  [2060/2809]  eta: 0:07:04  lr: 0.000003  min_lr: 0.000000  loss: 3.6618 (3.7373)  class_acc: 0.2917 (0.3380)  loss_scale: 32768.0000 (55360.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5087  data: 0.0491  max mem: 15572
Epoch: [34]  [2070/2809]  eta: 0:06:58  lr: 0.000003  min_lr: 0.000000  loss: 3.7589 (3.7381)  class_acc: 0.3333 (0.3379)  loss_scale: 32768.0000 (55251.4998)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1062  max mem: 15572
Epoch: [34]  [2080/2809]  eta: 0:06:53  lr: 0.000003  min_lr: 0.000000  loss: 3.8415 (3.7383)  class_acc: 0.2917 (0.3376)  loss_scale: 32768.0000 (55143.4580)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1302  max mem: 15572
Epoch: [34]  [2090/2809]  eta: 0:06:47  lr: 0.000003  min_lr: 0.000000  loss: 3.9274 (3.7395)  class_acc: 0.2500 (0.3374)  loss_scale: 32768.0000 (55036.4495)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1222  max mem: 15572
[2025-01-16 06:49:13,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:49:13,047] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [2100/2809]  eta: 0:06:41  lr: 0.000003  min_lr: 0.000000  loss: 3.8815 (3.7398)  class_acc: 0.2917 (0.3376)  loss_scale: 32768.0000 (54961.6525)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1680  max mem: 15572
Epoch: [34]  [2110/2809]  eta: 0:06:36  lr: 0.000003  min_lr: 0.000000  loss: 3.7793 (3.7395)  class_acc: 0.4167 (0.3378)  loss_scale: 65536.0000 (55011.7442)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1363  max mem: 15572
Epoch: [34]  [2120/2809]  eta: 0:06:31  lr: 0.000003  min_lr: 0.000000  loss: 3.6957 (3.7394)  class_acc: 0.3750 (0.3379)  loss_scale: 65536.0000 (55061.3635)  weight_decay: 0.0500 (0.0500)  time: 0.6435  data: 0.1667  max mem: 15572
Epoch: [34]  [2130/2809]  eta: 0:06:24  lr: 0.000003  min_lr: 0.000000  loss: 3.6946 (3.7381)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (55110.5171)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1254  max mem: 15572
Epoch: [34]  [2140/2809]  eta: 0:06:19  lr: 0.000003  min_lr: 0.000000  loss: 3.4609 (3.7366)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (55159.2116)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0896  max mem: 15572
Epoch: [34]  [2150/2809]  eta: 0:06:13  lr: 0.000003  min_lr: 0.000000  loss: 3.6993 (3.7367)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (55207.4533)  weight_decay: 0.0500 (0.0500)  time: 0.5687  data: 0.1329  max mem: 15572
Epoch: [34]  [2160/2809]  eta: 0:06:08  lr: 0.000003  min_lr: 0.000000  loss: 3.8562 (3.7371)  class_acc: 0.2917 (0.3380)  loss_scale: 65536.0000 (55255.2485)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1557  max mem: 15572
Epoch: [34]  [2170/2809]  eta: 0:06:02  lr: 0.000003  min_lr: 0.000000  loss: 3.9533 (3.7382)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (55302.6034)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1785  max mem: 15572
Epoch: [34]  [2180/2809]  eta: 0:05:56  lr: 0.000003  min_lr: 0.000000  loss: 3.9591 (3.7380)  class_acc: 0.2917 (0.3378)  loss_scale: 65536.0000 (55349.5241)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.1130  max mem: 15572
Epoch: [34]  [2190/2809]  eta: 0:05:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7682 (3.7374)  class_acc: 0.3750 (0.3380)  loss_scale: 65536.0000 (55396.0164)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0951  max mem: 15572
Epoch: [34]  [2200/2809]  eta: 0:05:45  lr: 0.000003  min_lr: 0.000000  loss: 3.7682 (3.7373)  class_acc: 0.3750 (0.3381)  loss_scale: 65536.0000 (55442.0863)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0763  max mem: 15572
Epoch: [34]  [2210/2809]  eta: 0:05:39  lr: 0.000003  min_lr: 0.000000  loss: 3.7380 (3.7372)  class_acc: 0.3333 (0.3382)  loss_scale: 65536.0000 (55487.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5154  data: 0.0493  max mem: 15572
Epoch: [34]  [2220/2809]  eta: 0:05:33  lr: 0.000003  min_lr: 0.000000  loss: 3.7117 (3.7371)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (55532.9815)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0912  max mem: 15572
[2025-01-16 06:50:26,102] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:50:26,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:50:26,968] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97735
[2025-01-16 06:50:26,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:50:26,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2230/2809]  eta: 0:05:28  lr: 0.000003  min_lr: 0.000000  loss: 3.8571 (3.7374)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (55636.5684)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1341  max mem: 15572
Epoch: [34]  [2240/2809]  eta: 0:05:22  lr: 0.000003  min_lr: 0.000000  loss: 3.8571 (3.7371)  class_acc: 0.2917 (0.3381)  loss_scale: 65536.0000 (55680.7425)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1412  max mem: 15572
Epoch: [34]  [2250/2809]  eta: 0:05:16  lr: 0.000003  min_lr: 0.000000  loss: 3.7158 (3.7368)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (55724.5242)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.1503  max mem: 15572
Epoch: [34]  [2260/2809]  eta: 0:05:11  lr: 0.000003  min_lr: 0.000000  loss: 3.4962 (3.7351)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (55767.9186)  weight_decay: 0.0500 (0.0500)  time: 0.5819  data: 0.1311  max mem: 15572
Epoch: [34]  [2270/2809]  eta: 0:05:05  lr: 0.000003  min_lr: 0.000000  loss: 3.6919 (3.7360)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (55810.9309)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1270  max mem: 15572
Epoch: [34]  [2280/2809]  eta: 0:04:59  lr: 0.000003  min_lr: 0.000000  loss: 3.9088 (3.7362)  class_acc: 0.2500 (0.3381)  loss_scale: 65536.0000 (55853.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1049  max mem: 15572
Epoch: [34]  [2290/2809]  eta: 0:04:54  lr: 0.000003  min_lr: 0.000000  loss: 3.7505 (3.7362)  class_acc: 0.2917 (0.3381)  loss_scale: 65536.0000 (55895.8289)  weight_decay: 0.0500 (0.0500)  time: 0.5260  data: 0.0635  max mem: 15572
Epoch: [34]  [2300/2809]  eta: 0:04:48  lr: 0.000003  min_lr: 0.000000  loss: 3.7512 (3.7366)  class_acc: 0.3333 (0.3381)  loss_scale: 65536.0000 (55937.7245)  weight_decay: 0.0500 (0.0500)  time: 0.5498  data: 0.0780  max mem: 15572
Epoch: [34]  [2310/2809]  eta: 0:04:42  lr: 0.000003  min_lr: 0.000000  loss: 3.6564 (3.7357)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (55979.2575)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0547  max mem: 15572
Epoch: [34]  [2320/2809]  eta: 0:04:36  lr: 0.000003  min_lr: 0.000000  loss: 3.5548 (3.7352)  class_acc: 0.3750 (0.3384)  loss_scale: 65536.0000 (56020.4326)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.1061  max mem: 15572
Epoch: [34]  [2330/2809]  eta: 0:04:31  lr: 0.000003  min_lr: 0.000000  loss: 3.4932 (3.7335)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (56061.2544)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1818  max mem: 15572
Epoch: [34]  [2340/2809]  eta: 0:04:25  lr: 0.000003  min_lr: 0.000000  loss: 3.5985 (3.7337)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (56101.7275)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1419  max mem: 15572
Epoch: [34]  [2350/2809]  eta: 0:04:19  lr: 0.000003  min_lr: 0.000000  loss: 3.8466 (3.7339)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (56141.8562)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1110  max mem: 15572
[2025-01-16 06:51:40,013] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:51:40,013] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [2360/2809]  eta: 0:04:14  lr: 0.000003  min_lr: 0.000000  loss: 3.7981 (3.7343)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (56264.9183)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1377  max mem: 15572
[2025-01-16 06:51:43,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97869
[2025-01-16 06:51:43,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:51:43,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2370/2809]  eta: 0:04:08  lr: 0.000003  min_lr: 0.000000  loss: 3.7981 (3.7346)  class_acc: 0.2917 (0.3384)  loss_scale: 65536.0000 (56359.3016)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.1719  max mem: 15572
Epoch: [34]  [2380/2809]  eta: 0:04:03  lr: 0.000003  min_lr: 0.000000  loss: 3.8098 (3.7343)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56397.8429)  weight_decay: 0.0500 (0.0500)  time: 0.5957  data: 0.1436  max mem: 15572
Epoch: [34]  [2390/2809]  eta: 0:03:57  lr: 0.000003  min_lr: 0.000000  loss: 3.8580 (3.7346)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (56436.0619)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1164  max mem: 15572
Epoch: [34]  [2400/2809]  eta: 0:03:51  lr: 0.000003  min_lr: 0.000000  loss: 3.7888 (3.7336)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56473.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.1254  max mem: 15572
Epoch: [34]  [2410/2809]  eta: 0:03:46  lr: 0.000002  min_lr: 0.000000  loss: 3.5442 (3.7329)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56511.5487)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0827  max mem: 15572
Epoch: [34]  [2420/2809]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000000  loss: 3.7092 (3.7334)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56548.8245)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0995  max mem: 15572
Epoch: [34]  [2430/2809]  eta: 0:03:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6942 (3.7330)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (56585.7935)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1159  max mem: 15572
Epoch: [34]  [2440/2809]  eta: 0:03:29  lr: 0.000002  min_lr: 0.000000  loss: 3.6820 (3.7334)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (56622.4596)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1059  max mem: 15572
Epoch: [34]  [2450/2809]  eta: 0:03:23  lr: 0.000002  min_lr: 0.000000  loss: 3.6709 (3.7332)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (56658.8266)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1692  max mem: 15572
Epoch: [34]  [2460/2809]  eta: 0:03:17  lr: 0.000002  min_lr: 0.000000  loss: 3.6701 (3.7328)  class_acc: 0.3750 (0.3385)  loss_scale: 65536.0000 (56694.8980)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.1817  max mem: 15572
Epoch: [34]  [2470/2809]  eta: 0:03:12  lr: 0.000002  min_lr: 0.000000  loss: 3.6464 (3.7320)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56730.6775)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1157  max mem: 15572
Epoch: [34]  [2480/2809]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6740 (3.7321)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (56766.1685)  weight_decay: 0.0500 (0.0500)  time: 0.5366  data: 0.0685  max mem: 15572
Epoch: [34]  [2490/2809]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8646 (3.7329)  class_acc: 0.3750 (0.3385)  loss_scale: 65536.0000 (56801.3745)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0811  max mem: 15572
[2025-01-16 06:52:57,626] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:52:57,626] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:52:58,028] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 97999
[2025-01-16 06:52:58,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:52:58,029] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 06:52:58,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=98000, skipped=651, lr=[2.3924327692572e-08, 2.3924327692572e-08, 3.417761098938857e-08, 3.417761098938857e-08, 4.88251585562694e-08, 4.88251585562694e-08, 6.975022650895629e-08, 6.975022650895629e-08, 9.964318072708041e-08, 9.964318072708041e-08, 1.4234740103868632e-07, 1.4234740103868632e-07, 2.0335343005526615e-07, 2.0335343005526615e-07, 2.905049000789517e-07, 2.905049000789517e-07, 4.1500700011278813e-07, 4.1500700011278813e-07, 5.928671430182688e-07, 5.928671430182688e-07, 8.469530614546697e-07, 8.469530614546697e-07, 1.2099329449352427e-06, 1.2099329449352427e-06, 1.7284756356217752e-06, 1.7284756356217752e-06, 2.4692509080311077e-06, 2.4692509080311077e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 06:52:58,033] [INFO] [timer.py:260:stop] epoch=0/micro_step=98000/global_step=98000, RunningAvgSamplesPerSec=28.571891009449732, CurrSamplesPerSec=32.08226252540905, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [34]  [2500/2809]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7461 (3.7326)  class_acc: 0.2917 (0.3384)  loss_scale: 65536.0000 (56862.5030)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.0894  max mem: 15572
Epoch: [34]  [2510/2809]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 3.6693 (3.7329)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (56897.0450)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1396  max mem: 15572
Epoch: [34]  [2520/2809]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 3.7716 (3.7330)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (56931.3130)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1400  max mem: 15572
Epoch: [34]  [2530/2809]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 3.5107 (3.7327)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (56965.3102)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1322  max mem: 15572
Epoch: [34]  [2540/2809]  eta: 0:02:32  lr: 0.000002  min_lr: 0.000000  loss: 3.5107 (3.7317)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (56999.0397)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1054  max mem: 15572
Epoch: [34]  [2550/2809]  eta: 0:02:26  lr: 0.000002  min_lr: 0.000000  loss: 3.5990 (3.7314)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (57032.5049)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1187  max mem: 15572
Epoch: [34]  [2560/2809]  eta: 0:02:21  lr: 0.000002  min_lr: 0.000000  loss: 3.7557 (3.7319)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (57065.7087)  weight_decay: 0.0500 (0.0500)  time: 0.5791  data: 0.1072  max mem: 15572
Epoch: [34]  [2570/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 3.9405 (3.7327)  class_acc: 0.2917 (0.3383)  loss_scale: 65536.0000 (57098.6542)  weight_decay: 0.0500 (0.0500)  time: 0.4888  data: 0.0217  max mem: 15572
Epoch: [34]  [2580/2809]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000000  loss: 3.8483 (3.7329)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (57131.3444)  weight_decay: 0.0500 (0.0500)  time: 0.5534  data: 0.1036  max mem: 15572
Epoch: [34]  [2590/2809]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 3.6444 (3.7322)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (57163.7823)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1589  max mem: 15572
Epoch: [34]  [2600/2809]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 3.4806 (3.7315)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (57195.9708)  weight_decay: 0.0500 (0.0500)  time: 0.5736  data: 0.1192  max mem: 15572
Epoch: [34]  [2610/2809]  eta: 0:01:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5449 (3.7313)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (57227.9127)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1026  max mem: 15572
Epoch: [34]  [2620/2809]  eta: 0:01:47  lr: 0.000002  min_lr: 0.000000  loss: 3.5579 (3.7305)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (57259.6108)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0531  max mem: 15572
[2025-01-16 06:54:10,666] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:54:10,666] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:54:11,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98129
[2025-01-16 06:54:11,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:54:11,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2630/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 3.5579 (3.7301)  class_acc: 0.4167 (0.3389)  loss_scale: 65536.0000 (57315.9772)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0426  max mem: 15572
Epoch: [34]  [2640/2809]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 3.6557 (3.7294)  class_acc: 0.4167 (0.3391)  loss_scale: 65536.0000 (57347.1019)  weight_decay: 0.0500 (0.0500)  time: 0.5665  data: 0.1021  max mem: 15572
Epoch: [34]  [2650/2809]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7624 (3.7298)  class_acc: 0.3750 (0.3393)  loss_scale: 65536.0000 (57377.9917)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.1382  max mem: 15572
Epoch: [34]  [2660/2809]  eta: 0:01:24  lr: 0.000002  min_lr: 0.000000  loss: 3.8274 (3.7299)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (57408.6494)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1314  max mem: 15572
Epoch: [34]  [2670/2809]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6572 (3.7288)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (57439.0775)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1121  max mem: 15572
Epoch: [34]  [2680/2809]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 3.6572 (3.7286)  class_acc: 0.3750 (0.3396)  loss_scale: 65536.0000 (57469.2786)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1874  max mem: 15572
Epoch: [34]  [2690/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 3.8050 (3.7288)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (57499.2553)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.2018  max mem: 15572
Epoch: [34]  [2700/2809]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 3.6031 (3.7277)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (57529.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1074  max mem: 15572
Epoch: [34]  [2710/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 3.6031 (3.7278)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (57558.5452)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0702  max mem: 15572
Epoch: [34]  [2720/2809]  eta: 0:00:50  lr: 0.000002  min_lr: 0.000000  loss: 3.7163 (3.7276)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (57587.8633)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.1103  max mem: 15572
Epoch: [34]  [2730/2809]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 3.6766 (3.7265)  class_acc: 0.3750 (0.3400)  loss_scale: 65536.0000 (57616.9667)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1605  max mem: 15572
Epoch: [34]  [2740/2809]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 3.6766 (3.7261)  class_acc: 0.3750 (0.3401)  loss_scale: 65536.0000 (57645.8577)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.1438  max mem: 15572
Epoch: [34]  [2750/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 3.9653 (3.7268)  class_acc: 0.2500 (0.3399)  loss_scale: 65536.0000 (57674.5387)  weight_decay: 0.0500 (0.0500)  time: 0.5635  data: 0.0880  max mem: 15572
[2025-01-16 06:55:26,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:55:26,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:55:28,326] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98262
[2025-01-16 06:55:28,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:55:28,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [2760/2809]  eta: 0:00:27  lr: 0.000002  min_lr: 0.000000  loss: 3.7827 (3.7267)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (57797.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1253  max mem: 15572
Epoch: [34]  [2770/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7373 (3.7269)  class_acc: 0.2917 (0.3398)  loss_scale: 65536.0000 (57825.8824)  weight_decay: 0.0500 (0.0500)  time: 0.6194  data: 0.1291  max mem: 15572
Epoch: [34]  [2780/2809]  eta: 0:00:16  lr: 0.000002  min_lr: 0.000000  loss: 3.8122 (3.7268)  class_acc: 0.2500 (0.3397)  loss_scale: 65536.0000 (57853.6066)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0781  max mem: 15572
Epoch: [34]  [2790/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7783 (3.7271)  class_acc: 0.2500 (0.3396)  loss_scale: 65536.0000 (57881.1322)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1356  max mem: 15572
Epoch: [34]  [2800/2809]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000000  loss: 3.7833 (3.7274)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (57908.4613)  weight_decay: 0.0500 (0.0500)  time: 0.5164  data: 0.0946  max mem: 15572
Epoch: [34]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8516 (3.7278)  class_acc: 0.2917 (0.3394)  loss_scale: 65536.0000 (57930.1844)  weight_decay: 0.0500 (0.0500)  time: 0.4906  data: 0.0945  max mem: 15572
Epoch: [34] Total time: 0:26:33 (0.5674 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 3.8516 (3.7278)  class_acc: 0.2917 (0.3394)  loss_scale: 65536.0000 (57930.1844)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:29:08  loss: 0.3769 (0.3769)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.4294  data: 6.2139  max mem: 15572
Val:  [ 10/272]  eta: 0:03:23  loss: 2.3104 (2.2193)  acc1: 50.0000 (46.4646)  acc5: 77.7778 (73.7374)  time: 0.7761  data: 0.5656  max mem: 15572
Val:  [ 20/272]  eta: 0:02:15  loss: 2.2710 (2.2655)  acc1: 50.0000 (48.6772)  acc5: 72.2222 (74.3386)  time: 0.2417  data: 0.0445  max mem: 15572
Val:  [ 30/272]  eta: 0:01:48  loss: 2.3099 (2.3518)  acc1: 44.4444 (44.8029)  acc5: 72.2222 (74.7312)  time: 0.2689  data: 0.0779  max mem: 15572
Val:  [ 40/272]  eta: 0:01:36  loss: 2.5186 (2.4036)  acc1: 27.7778 (41.7344)  acc5: 72.2222 (74.3902)  time: 0.2890  data: 0.0918  max mem: 15572
Val:  [ 50/272]  eta: 0:01:28  loss: 2.4706 (2.3317)  acc1: 33.3333 (43.8998)  acc5: 77.7778 (75.9259)  time: 0.3178  data: 0.1233  max mem: 15572
Val:  [ 60/272]  eta: 0:01:22  loss: 1.4497 (2.2255)  acc1: 61.1111 (47.0856)  acc5: 88.8889 (76.8670)  time: 0.3337  data: 0.1417  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.4497 (2.1418)  acc1: 66.6667 (49.6870)  acc5: 88.8889 (78.0908)  time: 0.3139  data: 0.1254  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.8345 (2.1582)  acc1: 61.1111 (49.5885)  acc5: 83.3333 (77.5034)  time: 0.2882  data: 0.0937  max mem: 15572
Val:  [ 90/272]  eta: 0:01:06  loss: 2.1326 (2.1599)  acc1: 50.0000 (49.6337)  acc5: 77.7778 (77.8999)  time: 0.3248  data: 0.1325  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.0664 (2.1892)  acc1: 50.0000 (48.7899)  acc5: 77.7778 (77.5028)  time: 0.3304  data: 0.1451  max mem: 15572
Val:  [110/272]  eta: 0:00:57  loss: 2.4046 (2.2648)  acc1: 22.2222 (46.6466)  acc5: 72.2222 (76.4264)  time: 0.3060  data: 0.1133  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 2.8710 (2.3024)  acc1: 22.2222 (45.8219)  acc5: 72.2222 (76.2167)  time: 0.3127  data: 0.1251  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.0876 (2.2693)  acc1: 44.4444 (46.7345)  acc5: 83.3333 (76.8872)  time: 0.3203  data: 0.1331  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.7145 (2.2634)  acc1: 55.5556 (47.0843)  acc5: 88.8889 (76.8322)  time: 0.3073  data: 0.1126  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.2562 (2.2687)  acc1: 44.4444 (46.4312)  acc5: 77.7778 (76.9684)  time: 0.3095  data: 0.1218  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2562 (2.2599)  acc1: 44.4444 (46.8944)  acc5: 77.7778 (77.0876)  time: 0.2882  data: 0.1060  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4168 (2.2802)  acc1: 44.4444 (46.5562)  acc5: 72.2222 (76.6732)  time: 0.2757  data: 0.0842  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.3318 (2.2730)  acc1: 38.8889 (46.2247)  acc5: 77.7778 (76.9184)  time: 0.3401  data: 0.1354  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.3259 (2.3250)  acc1: 33.3333 (44.9098)  acc5: 77.7778 (75.7126)  time: 0.2996  data: 0.1062  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.5444 (2.3324)  acc1: 33.3333 (44.7208)  acc5: 66.6667 (75.5113)  time: 0.2286  data: 0.0432  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.1205 (2.3340)  acc1: 50.0000 (44.9184)  acc5: 77.7778 (75.4871)  time: 0.2979  data: 0.1007  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1907 (2.3242)  acc1: 55.5556 (45.0980)  acc5: 77.7778 (75.5907)  time: 0.3513  data: 0.1407  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7453 (2.2918)  acc1: 61.1111 (46.1279)  acc5: 83.3333 (75.9981)  time: 0.3098  data: 0.0901  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.5768 (2.2766)  acc1: 66.6667 (46.4269)  acc5: 83.3333 (76.2333)  time: 0.3090  data: 0.0966  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2419 (2.2878)  acc1: 44.4444 (45.7725)  acc5: 77.7778 (76.2506)  time: 0.3207  data: 0.1071  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1791 (2.2301)  acc1: 66.6667 (47.3819)  acc5: 88.8889 (76.9264)  time: 0.2854  data: 0.0832  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3450 (2.2246)  acc1: 72.2222 (47.4990)  acc5: 88.8889 (77.0603)  time: 0.2089  data: 0.0439  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3450 (2.2290)  acc1: 72.2222 (47.4708)  acc5: 88.8889 (77.0428)  time: 0.2023  data: 0.0439  max mem: 15572
Val: Total time: 0:01:26 (0.3175 s / it)
* Acc@1 47.471 Acc@5 77.043 loss 2.229
Accuracy of the network on the 4883 val videos: 47.5%
[2025-01-16 06:57:22,800] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 06:57:22,804] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 06:57:22,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 06:57:25,373] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 06:57:25,373] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.47%
Epoch: [35]  [   0/2809]  eta: 3:36:17  lr: 0.000002  min_lr: 0.000000  loss: 3.2752 (3.2752)  class_acc: 0.2917 (0.2917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.6200  data: 4.1929  max mem: 15572
Epoch: [35]  [  10/2809]  eta: 0:35:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7619 (3.7901)  class_acc: 0.2917 (0.2841)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7678  data: 0.3814  max mem: 15572
Epoch: [35]  [  20/2809]  eta: 0:28:40  lr: 0.000002  min_lr: 0.000000  loss: 3.7739 (3.7684)  class_acc: 0.3333 (0.3353)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4167  data: 0.0004  max mem: 15572
Epoch: [35]  [  30/2809]  eta: 0:25:43  lr: 0.000002  min_lr: 0.000000  loss: 3.8052 (3.7388)  class_acc: 0.3333 (0.3360)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4387  data: 0.0007  max mem: 15572
Epoch: [35]  [  40/2809]  eta: 0:25:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7772 (3.7558)  class_acc: 0.2917 (0.3201)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0569  max mem: 15572
Epoch: [35]  [  50/2809]  eta: 0:26:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7772 (3.7879)  class_acc: 0.2917 (0.3178)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1469  max mem: 15572
Epoch: [35]  [  60/2809]  eta: 0:26:51  lr: 0.000002  min_lr: 0.000000  loss: 4.0453 (3.8208)  class_acc: 0.2917 (0.3081)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.1767  max mem: 15572
Epoch: [35]  [  70/2809]  eta: 0:27:35  lr: 0.000002  min_lr: 0.000000  loss: 3.7578 (3.7812)  class_acc: 0.3333 (0.3222)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.2206  max mem: 15572
[2025-01-16 06:58:13,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:58:13,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 06:58:14,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98393
[2025-01-16 06:58:14,444] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 06:58:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [  80/2809]  eta: 0:28:02  lr: 0.000002  min_lr: 0.000000  loss: 3.6866 (3.7676)  class_acc: 0.3750 (0.3230)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.2543  max mem: 15572
Epoch: [35]  [  90/2809]  eta: 0:28:33  lr: 0.000002  min_lr: 0.000000  loss: 3.7091 (3.7328)  class_acc: 0.3750 (0.3320)  loss_scale: 65536.0000 (66976.3516)  weight_decay: 0.0500 (0.0500)  time: 0.7208  data: 0.2545  max mem: 15572
[2025-01-16 06:58:28,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98415
[2025-01-16 06:58:28,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 06:58:28,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 100/2809]  eta: 0:28:24  lr: 0.000002  min_lr: 0.000000  loss: 3.7266 (3.7449)  class_acc: 0.2917 (0.3271)  loss_scale: 65536.0000 (66509.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6803  data: 0.2227  max mem: 15572
Epoch: [35]  [ 110/2809]  eta: 0:28:28  lr: 0.000002  min_lr: 0.000000  loss: 3.5547 (3.7113)  class_acc: 0.2917 (0.3345)  loss_scale: 32768.0000 (63469.5495)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.1814  max mem: 15572
Epoch: [35]  [ 120/2809]  eta: 0:28:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5547 (3.7128)  class_acc: 0.3333 (0.3375)  loss_scale: 32768.0000 (60932.2314)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.1788  max mem: 15572
Epoch: [35]  [ 130/2809]  eta: 0:28:33  lr: 0.000002  min_lr: 0.000000  loss: 3.8055 (3.7247)  class_acc: 0.3333 (0.3375)  loss_scale: 32768.0000 (58782.2901)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.2238  max mem: 15572
Epoch: [35]  [ 140/2809]  eta: 0:28:11  lr: 0.000002  min_lr: 0.000000  loss: 3.9097 (3.7288)  class_acc: 0.3333 (0.3348)  loss_scale: 32768.0000 (56937.3050)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.1948  max mem: 15572
Epoch: [35]  [ 150/2809]  eta: 0:27:50  lr: 0.000002  min_lr: 0.000000  loss: 3.7823 (3.7245)  class_acc: 0.2917 (0.3364)  loss_scale: 32768.0000 (55336.6887)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1202  max mem: 15572
Epoch: [35]  [ 160/2809]  eta: 0:27:05  lr: 0.000002  min_lr: 0.000000  loss: 3.7021 (3.7221)  class_acc: 0.3750 (0.3370)  loss_scale: 32768.0000 (53934.9068)  weight_decay: 0.0500 (0.0500)  time: 0.4715  data: 0.0639  max mem: 15572
Epoch: [35]  [ 170/2809]  eta: 0:26:32  lr: 0.000002  min_lr: 0.000000  loss: 3.7557 (3.7296)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (52697.0760)  weight_decay: 0.0500 (0.0500)  time: 0.4153  data: 0.0005  max mem: 15572
Epoch: [35]  [ 180/2809]  eta: 0:26:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8174 (3.7343)  class_acc: 0.2500 (0.3329)  loss_scale: 32768.0000 (51596.0221)  weight_decay: 0.0500 (0.0500)  time: 0.4311  data: 0.0005  max mem: 15572
Epoch: [35]  [ 190/2809]  eta: 0:25:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7388 (3.7308)  class_acc: 0.3750 (0.3357)  loss_scale: 32768.0000 (50610.2618)  weight_decay: 0.0500 (0.0500)  time: 0.4735  data: 0.0485  max mem: 15572
Epoch: [35]  [ 200/2809]  eta: 0:25:32  lr: 0.000002  min_lr: 0.000000  loss: 3.6856 (3.7282)  class_acc: 0.3750 (0.3352)  loss_scale: 32768.0000 (49722.5871)  weight_decay: 0.0500 (0.0500)  time: 0.5305  data: 0.1065  max mem: 15572
Epoch: [35]  [ 210/2809]  eta: 0:25:20  lr: 0.000002  min_lr: 0.000000  loss: 3.7577 (3.7335)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (48919.0521)  weight_decay: 0.0500 (0.0500)  time: 0.5393  data: 0.0966  max mem: 15572
Epoch: [35]  [ 220/2809]  eta: 0:25:11  lr: 0.000002  min_lr: 0.000000  loss: 3.8107 (3.7282)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (48188.2353)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1031  max mem: 15572
[2025-01-16 06:59:39,378] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 06:59:39,379] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 230/2809]  eta: 0:24:58  lr: 0.000002  min_lr: 0.000000  loss: 3.8107 (3.7409)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (47804.3983)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1181  max mem: 15572
Epoch: [35]  [ 240/2809]  eta: 0:24:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7247 (3.7385)  class_acc: 0.2917 (0.3333)  loss_scale: 65536.0000 (48540.1494)  weight_decay: 0.0500 (0.0500)  time: 0.5357  data: 0.1064  max mem: 15572
Epoch: [35]  [ 250/2809]  eta: 0:24:46  lr: 0.000002  min_lr: 0.000000  loss: 3.6876 (3.7452)  class_acc: 0.2917 (0.3310)  loss_scale: 65536.0000 (49217.2749)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1262  max mem: 15572
Epoch: [35]  [ 260/2809]  eta: 0:24:42  lr: 0.000002  min_lr: 0.000000  loss: 3.7465 (3.7371)  class_acc: 0.3333 (0.3349)  loss_scale: 65536.0000 (49842.5134)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1524  max mem: 15572
Epoch: [35]  [ 270/2809]  eta: 0:24:41  lr: 0.000002  min_lr: 0.000000  loss: 3.5643 (3.7303)  class_acc: 0.4167 (0.3373)  loss_scale: 65536.0000 (50421.6089)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1732  max mem: 15572
Epoch: [35]  [ 280/2809]  eta: 0:24:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6984 (3.7347)  class_acc: 0.3333 (0.3367)  loss_scale: 65536.0000 (50959.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1627  max mem: 15572
Epoch: [35]  [ 290/2809]  eta: 0:24:28  lr: 0.000002  min_lr: 0.000000  loss: 3.7272 (3.7334)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (51460.3986)  weight_decay: 0.0500 (0.0500)  time: 0.5757  data: 0.1386  max mem: 15572
Epoch: [35]  [ 300/2809]  eta: 0:24:17  lr: 0.000002  min_lr: 0.000000  loss: 3.8289 (3.7396)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (51928.0266)  weight_decay: 0.0500 (0.0500)  time: 0.5557  data: 0.1126  max mem: 15572
Epoch: [35]  [ 310/2809]  eta: 0:24:07  lr: 0.000002  min_lr: 0.000000  loss: 3.9889 (3.7461)  class_acc: 0.2500 (0.3340)  loss_scale: 65536.0000 (52365.5820)  weight_decay: 0.0500 (0.0500)  time: 0.5301  data: 0.0927  max mem: 15572
Epoch: [35]  [ 320/2809]  eta: 0:24:01  lr: 0.000002  min_lr: 0.000000  loss: 3.9007 (3.7492)  class_acc: 0.2917 (0.3339)  loss_scale: 65536.0000 (52775.8754)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1144  max mem: 15572
[2025-01-16 07:00:32,682] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98638
[2025-01-16 07:00:32,683] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:00:32,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 330/2809]  eta: 0:24:01  lr: 0.000002  min_lr: 0.000000  loss: 3.8045 (3.7507)  class_acc: 0.3333 (0.3340)  loss_scale: 65536.0000 (52369.4018)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.1726  max mem: 15572
Epoch: [35]  [ 340/2809]  eta: 0:23:53  lr: 0.000002  min_lr: 0.000000  loss: 3.5455 (3.7421)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (51794.5806)  weight_decay: 0.0500 (0.0500)  time: 0.6089  data: 0.1705  max mem: 15572
Epoch: [35]  [ 350/2809]  eta: 0:23:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6219 (3.7493)  class_acc: 0.2917 (0.3319)  loss_scale: 32768.0000 (51252.5128)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1580  max mem: 15572
Epoch: [35]  [ 360/2809]  eta: 0:23:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7700 (3.7510)  class_acc: 0.2500 (0.3308)  loss_scale: 32768.0000 (50740.4765)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.1542  max mem: 15572
Epoch: [35]  [ 370/2809]  eta: 0:23:34  lr: 0.000002  min_lr: 0.000000  loss: 3.8198 (3.7524)  class_acc: 0.2917 (0.3309)  loss_scale: 32768.0000 (50256.0431)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1092  max mem: 15572
Epoch: [35]  [ 380/2809]  eta: 0:23:25  lr: 0.000002  min_lr: 0.000000  loss: 3.9240 (3.7540)  class_acc: 0.2500 (0.3304)  loss_scale: 32768.0000 (49797.0394)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.1007  max mem: 15572
Epoch: [35]  [ 390/2809]  eta: 0:23:18  lr: 0.000002  min_lr: 0.000000  loss: 3.8283 (3.7554)  class_acc: 0.2500 (0.3300)  loss_scale: 32768.0000 (49361.5141)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.0998  max mem: 15572
Epoch: [35]  [ 400/2809]  eta: 0:23:09  lr: 0.000002  min_lr: 0.000000  loss: 3.7361 (3.7513)  class_acc: 0.3333 (0.3304)  loss_scale: 32768.0000 (48947.7107)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0947  max mem: 15572
Epoch: [35]  [ 410/2809]  eta: 0:23:03  lr: 0.000002  min_lr: 0.000000  loss: 3.5946 (3.7500)  class_acc: 0.3333 (0.3312)  loss_scale: 32768.0000 (48554.0438)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1097  max mem: 15572
Epoch: [35]  [ 420/2809]  eta: 0:22:54  lr: 0.000002  min_lr: 0.000000  loss: 3.7103 (3.7487)  class_acc: 0.3333 (0.3318)  loss_scale: 32768.0000 (48179.0784)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0940  max mem: 15572
Epoch: [35]  [ 430/2809]  eta: 0:22:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7936 (3.7509)  class_acc: 0.3333 (0.3321)  loss_scale: 32768.0000 (47821.5128)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0764  max mem: 15572
Epoch: [35]  [ 440/2809]  eta: 0:22:37  lr: 0.000002  min_lr: 0.000000  loss: 3.7906 (3.7504)  class_acc: 0.3333 (0.3323)  loss_scale: 32768.0000 (47480.1633)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0764  max mem: 15572
Epoch: [35]  [ 450/2809]  eta: 0:22:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6437 (3.7495)  class_acc: 0.3750 (0.3334)  loss_scale: 32768.0000 (47153.9512)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0692  max mem: 15572
[2025-01-16 07:01:44,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:01:44,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 460/2809]  eta: 0:22:21  lr: 0.000002  min_lr: 0.000000  loss: 3.6437 (3.7479)  class_acc: 0.3333 (0.3333)  loss_scale: 32768.0000 (47481.6139)  weight_decay: 0.0500 (0.0500)  time: 0.5248  data: 0.0691  max mem: 15572
Epoch: [35]  [ 470/2809]  eta: 0:22:18  lr: 0.000002  min_lr: 0.000000  loss: 3.7586 (3.7489)  class_acc: 0.2500 (0.3329)  loss_scale: 65536.0000 (47864.9342)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1309  max mem: 15572
Epoch: [35]  [ 480/2809]  eta: 0:22:12  lr: 0.000002  min_lr: 0.000000  loss: 3.6600 (3.7467)  class_acc: 0.3333 (0.3330)  loss_scale: 65536.0000 (48232.3160)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.1763  max mem: 15572
Epoch: [35]  [ 490/2809]  eta: 0:22:02  lr: 0.000002  min_lr: 0.000000  loss: 3.6600 (3.7489)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (48584.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0878  max mem: 15572
Epoch: [35]  [ 500/2809]  eta: 0:21:59  lr: 0.000002  min_lr: 0.000000  loss: 3.6515 (3.7460)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (48923.0818)  weight_decay: 0.0500 (0.0500)  time: 0.5508  data: 0.1051  max mem: 15572
Epoch: [35]  [ 510/2809]  eta: 0:21:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5776 (3.7446)  class_acc: 0.3333 (0.3337)  loss_scale: 65536.0000 (49248.1879)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1328  max mem: 15572
Epoch: [35]  [ 520/2809]  eta: 0:21:45  lr: 0.000002  min_lr: 0.000000  loss: 3.7708 (3.7478)  class_acc: 0.3333 (0.3338)  loss_scale: 65536.0000 (49560.8138)  weight_decay: 0.0500 (0.0500)  time: 0.5443  data: 0.0989  max mem: 15572
Epoch: [35]  [ 530/2809]  eta: 0:21:43  lr: 0.000002  min_lr: 0.000000  loss: 3.7708 (3.7474)  class_acc: 0.3333 (0.3342)  loss_scale: 65536.0000 (49861.6648)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.1643  max mem: 15572
Epoch: [35]  [ 540/2809]  eta: 0:21:41  lr: 0.000002  min_lr: 0.000000  loss: 3.7640 (3.7472)  class_acc: 0.2917 (0.3340)  loss_scale: 65536.0000 (50151.3937)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.2017  max mem: 15572
Epoch: [35]  [ 550/2809]  eta: 0:21:34  lr: 0.000002  min_lr: 0.000000  loss: 3.8527 (3.7498)  class_acc: 0.2917 (0.3334)  loss_scale: 65536.0000 (50430.6062)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1398  max mem: 15572
Epoch: [35]  [ 560/2809]  eta: 0:21:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6711 (3.7470)  class_acc: 0.3333 (0.3343)  loss_scale: 65536.0000 (50699.8645)  weight_decay: 0.0500 (0.0500)  time: 0.5585  data: 0.1130  max mem: 15572
Epoch: [35]  [ 570/2809]  eta: 0:21:20  lr: 0.000002  min_lr: 0.000000  loss: 3.6604 (3.7483)  class_acc: 0.3333 (0.3336)  loss_scale: 65536.0000 (50959.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1042  max mem: 15572
[2025-01-16 07:02:57,921] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:02:57,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 580/2809]  eta: 0:21:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6712 (3.7455)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (51323.3735)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0988  max mem: 15572
[2025-01-16 07:02:58,331] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 98896
[2025-01-16 07:02:58,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:02:58,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 590/2809]  eta: 0:21:07  lr: 0.000002  min_lr: 0.000000  loss: 3.6610 (3.7472)  class_acc: 0.3333 (0.3345)  loss_scale: 65536.0000 (51563.8579)  weight_decay: 0.0500 (0.0500)  time: 0.5578  data: 0.1120  max mem: 15572
Epoch: [35]  [ 600/2809]  eta: 0:21:03  lr: 0.000002  min_lr: 0.000000  loss: 3.6999 (3.7447)  class_acc: 0.3333 (0.3355)  loss_scale: 65536.0000 (51796.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5806  data: 0.1408  max mem: 15572
Epoch: [35]  [ 610/2809]  eta: 0:20:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6999 (3.7439)  class_acc: 0.3750 (0.3362)  loss_scale: 65536.0000 (52021.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1492  max mem: 15572
Epoch: [35]  [ 620/2809]  eta: 0:20:49  lr: 0.000002  min_lr: 0.000000  loss: 3.6844 (3.7427)  class_acc: 0.3750 (0.3371)  loss_scale: 65536.0000 (52238.8406)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0962  max mem: 15572
Epoch: [35]  [ 630/2809]  eta: 0:20:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7766 (3.7451)  class_acc: 0.4167 (0.3372)  loss_scale: 65536.0000 (52449.5721)  weight_decay: 0.0500 (0.0500)  time: 0.5483  data: 0.1167  max mem: 15572
Epoch: [35]  [ 640/2809]  eta: 0:20:41  lr: 0.000002  min_lr: 0.000000  loss: 3.9105 (3.7454)  class_acc: 0.3750 (0.3373)  loss_scale: 65536.0000 (52653.7285)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.1703  max mem: 15572
Epoch: [35]  [ 650/2809]  eta: 0:20:34  lr: 0.000002  min_lr: 0.000000  loss: 3.5529 (3.7426)  class_acc: 0.3750 (0.3379)  loss_scale: 65536.0000 (52851.6129)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1495  max mem: 15572
Epoch: [35]  [ 660/2809]  eta: 0:20:31  lr: 0.000002  min_lr: 0.000000  loss: 3.6710 (3.7430)  class_acc: 0.3333 (0.3380)  loss_scale: 65536.0000 (53043.5098)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1396  max mem: 15572
Epoch: [35]  [ 670/2809]  eta: 0:20:23  lr: 0.000002  min_lr: 0.000000  loss: 3.6710 (3.7420)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53229.6870)  weight_decay: 0.0500 (0.0500)  time: 0.5658  data: 0.1128  max mem: 15572
Epoch: [35]  [ 680/2809]  eta: 0:20:18  lr: 0.000002  min_lr: 0.000000  loss: 3.7207 (3.7436)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53410.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1051  max mem: 15572
[2025-01-16 07:03:57,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=99000, skipped=657, lr=[2.0817667707773205e-08, 2.0817667707773205e-08, 2.9739525296818868e-08, 2.9739525296818868e-08, 4.248503613831267e-08, 4.248503613831267e-08, 6.06929087690181e-08, 6.06929087690181e-08, 8.670415538431158e-08, 8.670415538431158e-08, 1.2386307912044512e-07, 1.2386307912044512e-07, 1.7694725588635018e-07, 1.7694725588635018e-07, 2.5278179412335745e-07, 2.5278179412335745e-07, 3.611168487476535e-07, 3.611168487476535e-07, 5.158812124966479e-07, 5.158812124966479e-07, 7.36973160709497e-07, 7.36973160709497e-07, 1.0528188010135672e-06, 1.0528188010135672e-06, 1.5040268585908103e-06, 1.5040268585908103e-06, 2.148609797986872e-06, 2.148609797986872e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:03:57,613] [INFO] [timer.py:260:stop] epoch=0/micro_step=99000/global_step=99000, RunningAvgSamplesPerSec=28.571111491431925, CurrSamplesPerSec=27.728124531727914, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [ 690/2809]  eta: 0:20:12  lr: 0.000002  min_lr: 0.000000  loss: 3.8530 (3.7471)  class_acc: 0.2917 (0.3376)  loss_scale: 65536.0000 (53585.8755)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1331  max mem: 15572
Epoch: [35]  [ 700/2809]  eta: 0:20:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8366 (3.7465)  class_acc: 0.3333 (0.3376)  loss_scale: 65536.0000 (53756.3481)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1044  max mem: 15572
[2025-01-16 07:04:10,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:04:10,809] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 710/2809]  eta: 0:19:55  lr: 0.000002  min_lr: 0.000000  loss: 3.8714 (3.7495)  class_acc: 0.3333 (0.3374)  loss_scale: 65536.0000 (54014.1997)  weight_decay: 0.0500 (0.0500)  time: 0.4825  data: 0.0466  max mem: 15572
[2025-01-16 07:04:11,274] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99026
[2025-01-16 07:04:11,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:04:11,275] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 720/2809]  eta: 0:19:50  lr: 0.000002  min_lr: 0.000000  loss: 3.5820 (3.7470)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (54174.0028)  weight_decay: 0.0500 (0.0500)  time: 0.5098  data: 0.0790  max mem: 15572
Epoch: [35]  [ 730/2809]  eta: 0:19:44  lr: 0.000002  min_lr: 0.000000  loss: 3.5520 (3.7459)  class_acc: 0.3750 (0.3380)  loss_scale: 65536.0000 (54329.4337)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1433  max mem: 15572
Epoch: [35]  [ 740/2809]  eta: 0:19:36  lr: 0.000002  min_lr: 0.000000  loss: 3.6704 (3.7459)  class_acc: 0.2917 (0.3381)  loss_scale: 65536.0000 (54480.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5293  data: 0.0988  max mem: 15572
Epoch: [35]  [ 750/2809]  eta: 0:19:33  lr: 0.000002  min_lr: 0.000000  loss: 3.6704 (3.7444)  class_acc: 0.3750 (0.3390)  loss_scale: 65536.0000 (54627.8775)  weight_decay: 0.0500 (0.0500)  time: 0.5746  data: 0.1348  max mem: 15572
Epoch: [35]  [ 760/2809]  eta: 0:19:29  lr: 0.000002  min_lr: 0.000000  loss: 3.6134 (3.7441)  class_acc: 0.3750 (0.3390)  loss_scale: 65536.0000 (54771.2168)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1887  max mem: 15572
Epoch: [35]  [ 770/2809]  eta: 0:19:21  lr: 0.000002  min_lr: 0.000000  loss: 3.5301 (3.7399)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (54910.8379)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1306  max mem: 15572
Epoch: [35]  [ 780/2809]  eta: 0:19:15  lr: 0.000002  min_lr: 0.000000  loss: 3.4995 (3.7412)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (55046.8835)  weight_decay: 0.0500 (0.0500)  time: 0.5320  data: 0.0913  max mem: 15572
Epoch: [35]  [ 790/2809]  eta: 0:19:10  lr: 0.000002  min_lr: 0.000000  loss: 3.5073 (3.7378)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (55179.4893)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1272  max mem: 15572
Epoch: [35]  [ 800/2809]  eta: 0:19:06  lr: 0.000002  min_lr: 0.000000  loss: 3.4635 (3.7359)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (55308.7840)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1719  max mem: 15572
Epoch: [35]  [ 810/2809]  eta: 0:19:01  lr: 0.000002  min_lr: 0.000000  loss: 3.7452 (3.7380)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (55434.8903)  weight_decay: 0.0500 (0.0500)  time: 0.6267  data: 0.1781  max mem: 15572
Epoch: [35]  [ 820/2809]  eta: 0:18:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7036 (3.7350)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (55557.9245)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1359  max mem: 15572
Epoch: [35]  [ 830/2809]  eta: 0:18:48  lr: 0.000002  min_lr: 0.000000  loss: 3.6144 (3.7333)  class_acc: 0.4167 (0.3406)  loss_scale: 65536.0000 (55677.9976)  weight_decay: 0.0500 (0.0500)  time: 0.5272  data: 0.0773  max mem: 15572
[2025-01-16 07:05:25,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:05:25,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 840/2809]  eta: 0:18:42  lr: 0.000002  min_lr: 0.000000  loss: 3.8369 (3.7332)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (55873.1415)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.0996  max mem: 15572
[2025-01-16 07:05:27,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99160
[2025-01-16 07:05:27,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:05:27,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 850/2809]  eta: 0:18:35  lr: 0.000002  min_lr: 0.000000  loss: 3.8073 (3.7332)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (56294.7309)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1208  max mem: 15572
Epoch: [35]  [ 860/2809]  eta: 0:18:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7855 (3.7330)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (56402.0627)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1032  max mem: 15572
Epoch: [35]  [ 870/2809]  eta: 0:18:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7168 (3.7321)  class_acc: 0.2917 (0.3399)  loss_scale: 65536.0000 (56506.9300)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0935  max mem: 15572
Epoch: [35]  [ 880/2809]  eta: 0:18:16  lr: 0.000002  min_lr: 0.000000  loss: 3.8497 (3.7342)  class_acc: 0.2917 (0.3390)  loss_scale: 65536.0000 (56609.4166)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0630  max mem: 15572
Epoch: [35]  [ 890/2809]  eta: 0:18:10  lr: 0.000002  min_lr: 0.000000  loss: 3.8219 (3.7341)  class_acc: 0.2500 (0.3387)  loss_scale: 65536.0000 (56709.6027)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.0847  max mem: 15572
Epoch: [35]  [ 900/2809]  eta: 0:18:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8163 (3.7358)  class_acc: 0.2500 (0.3381)  loss_scale: 65536.0000 (56807.5649)  weight_decay: 0.0500 (0.0500)  time: 0.5738  data: 0.1361  max mem: 15572
Epoch: [35]  [ 910/2809]  eta: 0:18:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8725 (3.7354)  class_acc: 0.2500 (0.3378)  loss_scale: 65536.0000 (56903.3765)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.1745  max mem: 15572
Epoch: [35]  [ 920/2809]  eta: 0:17:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7833 (3.7352)  class_acc: 0.2917 (0.3376)  loss_scale: 65536.0000 (56997.1075)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.1735  max mem: 15572
Epoch: [35]  [ 930/2809]  eta: 0:17:48  lr: 0.000002  min_lr: 0.000000  loss: 3.8421 (3.7383)  class_acc: 0.2917 (0.3368)  loss_scale: 65536.0000 (57088.8249)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1083  max mem: 15572
Epoch: [35]  [ 940/2809]  eta: 0:17:42  lr: 0.000002  min_lr: 0.000000  loss: 3.9787 (3.7393)  class_acc: 0.3333 (0.3369)  loss_scale: 65536.0000 (57178.5930)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.1026  max mem: 15572
Epoch: [35]  [ 950/2809]  eta: 0:17:38  lr: 0.000002  min_lr: 0.000000  loss: 3.9664 (3.7415)  class_acc: 0.3333 (0.3365)  loss_scale: 65536.0000 (57266.4732)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1733  max mem: 15572
Epoch: [35]  [ 960/2809]  eta: 0:17:31  lr: 0.000002  min_lr: 0.000000  loss: 3.9125 (3.7402)  class_acc: 0.2917 (0.3362)  loss_scale: 65536.0000 (57352.5245)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1506  max mem: 15572
Epoch: [35]  [ 970/2809]  eta: 0:17:28  lr: 0.000002  min_lr: 0.000000  loss: 3.4827 (3.7382)  class_acc: 0.3333 (0.3368)  loss_scale: 65536.0000 (57436.8033)  weight_decay: 0.0500 (0.0500)  time: 0.6095  data: 0.1636  max mem: 15572
[2025-01-16 07:06:41,661] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:06:41,661] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:06:42,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99290
[2025-01-16 07:06:42,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:06:42,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 980/2809]  eta: 0:17:21  lr: 0.000002  min_lr: 0.000000  loss: 3.6943 (3.7386)  class_acc: 0.3333 (0.3371)  loss_scale: 65536.0000 (57586.1692)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1356  max mem: 15572
Epoch: [35]  [ 990/2809]  eta: 0:17:14  lr: 0.000002  min_lr: 0.000000  loss: 3.8678 (3.7402)  class_acc: 0.2917 (0.3365)  loss_scale: 65536.0000 (57666.3895)  weight_decay: 0.0500 (0.0500)  time: 0.4992  data: 0.0561  max mem: 15572
Epoch: [35]  [1000/2809]  eta: 0:17:09  lr: 0.000002  min_lr: 0.000000  loss: 3.8915 (3.7402)  class_acc: 0.2917 (0.3364)  loss_scale: 65536.0000 (57745.0070)  weight_decay: 0.0500 (0.0500)  time: 0.5630  data: 0.1304  max mem: 15572
Epoch: [35]  [1010/2809]  eta: 0:17:03  lr: 0.000002  min_lr: 0.000000  loss: 3.8915 (3.7402)  class_acc: 0.3333 (0.3366)  loss_scale: 65536.0000 (57822.0692)  weight_decay: 0.0500 (0.0500)  time: 0.5950  data: 0.1520  max mem: 15572
Epoch: [35]  [1020/2809]  eta: 0:16:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6166 (3.7368)  class_acc: 0.3750 (0.3377)  loss_scale: 65536.0000 (57897.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0740  max mem: 15572
Epoch: [35]  [1030/2809]  eta: 0:16:51  lr: 0.000002  min_lr: 0.000000  loss: 3.5061 (3.7356)  class_acc: 0.4167 (0.3380)  loss_scale: 65536.0000 (57971.7090)  weight_decay: 0.0500 (0.0500)  time: 0.5473  data: 0.0719  max mem: 15572
Epoch: [35]  [1040/2809]  eta: 0:16:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7139 (3.7357)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (58044.3727)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.1506  max mem: 15572
Epoch: [35]  [1050/2809]  eta: 0:16:39  lr: 0.000002  min_lr: 0.000000  loss: 3.8994 (3.7348)  class_acc: 0.3750 (0.3391)  loss_scale: 65536.0000 (58115.6537)  weight_decay: 0.0500 (0.0500)  time: 0.5523  data: 0.0948  max mem: 15572
Epoch: [35]  [1060/2809]  eta: 0:16:33  lr: 0.000002  min_lr: 0.000000  loss: 3.9377 (3.7374)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (58185.5910)  weight_decay: 0.0500 (0.0500)  time: 0.4785  data: 0.0256  max mem: 15572
Epoch: [35]  [1070/2809]  eta: 0:16:27  lr: 0.000002  min_lr: 0.000000  loss: 3.8802 (3.7383)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (58254.2222)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.0890  max mem: 15572
Epoch: [35]  [1080/2809]  eta: 0:16:20  lr: 0.000002  min_lr: 0.000000  loss: 3.6980 (3.7378)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (58321.5837)  weight_decay: 0.0500 (0.0500)  time: 0.5399  data: 0.1097  max mem: 15572
Epoch: [35]  [1090/2809]  eta: 0:16:14  lr: 0.000002  min_lr: 0.000000  loss: 3.8046 (3.7387)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (58387.7104)  weight_decay: 0.0500 (0.0500)  time: 0.5217  data: 0.0865  max mem: 15572
Epoch: [35]  [1100/2809]  eta: 0:16:09  lr: 0.000002  min_lr: 0.000000  loss: 3.8057 (3.7391)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (58452.6358)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1238  max mem: 15572
[2025-01-16 07:07:53,500] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:07:53,501] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:07:55,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99423
[2025-01-16 07:07:55,336] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:07:55,336] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1110/2809]  eta: 0:16:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9158 (3.7417)  class_acc: 0.3333 (0.3384)  loss_scale: 65536.0000 (58752.3456)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1422  max mem: 15572
Epoch: [35]  [1120/2809]  eta: 0:15:57  lr: 0.000002  min_lr: 0.000000  loss: 3.9158 (3.7402)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (58812.8599)  weight_decay: 0.0500 (0.0500)  time: 0.5629  data: 0.1093  max mem: 15572
Epoch: [35]  [1130/2809]  eta: 0:15:52  lr: 0.000002  min_lr: 0.000000  loss: 3.7101 (3.7398)  class_acc: 0.3750 (0.3394)  loss_scale: 65536.0000 (58872.3042)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1331  max mem: 15572
Epoch: [35]  [1140/2809]  eta: 0:15:47  lr: 0.000002  min_lr: 0.000000  loss: 3.8081 (3.7422)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (58930.7064)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.1457  max mem: 15572
Epoch: [35]  [1150/2809]  eta: 0:15:42  lr: 0.000002  min_lr: 0.000000  loss: 4.0335 (3.7431)  class_acc: 0.2917 (0.3389)  loss_scale: 65536.0000 (58988.0938)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1596  max mem: 15572
Epoch: [35]  [1160/2809]  eta: 0:15:36  lr: 0.000002  min_lr: 0.000000  loss: 3.6983 (3.7414)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (59044.4927)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1408  max mem: 15572
Epoch: [35]  [1170/2809]  eta: 0:15:31  lr: 0.000002  min_lr: 0.000000  loss: 3.6131 (3.7403)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (59099.9283)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1117  max mem: 15572
Epoch: [35]  [1180/2809]  eta: 0:15:24  lr: 0.000002  min_lr: 0.000000  loss: 3.7140 (3.7405)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (59154.4251)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1088  max mem: 15572
Epoch: [35]  [1190/2809]  eta: 0:15:19  lr: 0.000002  min_lr: 0.000000  loss: 3.8170 (3.7419)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (59208.0067)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.1202  max mem: 15572
Epoch: [35]  [1200/2809]  eta: 0:15:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7554 (3.7413)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (59260.6961)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.1382  max mem: 15572
Epoch: [35]  [1210/2809]  eta: 0:15:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7155 (3.7405)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (59312.5153)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.0945  max mem: 15572
Epoch: [35]  [1220/2809]  eta: 0:15:02  lr: 0.000002  min_lr: 0.000000  loss: 3.4673 (3.7395)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (59363.4857)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1205  max mem: 15572
Epoch: [35]  [1230/2809]  eta: 0:14:56  lr: 0.000002  min_lr: 0.000000  loss: 3.7366 (3.7401)  class_acc: 0.3333 (0.3395)  loss_scale: 65536.0000 (59413.6279)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.1601  max mem: 15572
[2025-01-16 07:09:10,277] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:09:10,277] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [1240/2809]  eta: 0:14:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7388 (3.7394)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (59674.1982)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1619  max mem: 15572
[2025-01-16 07:09:11,899] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99556
[2025-01-16 07:09:11,900] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:09:11,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1250/2809]  eta: 0:14:45  lr: 0.000002  min_lr: 0.000000  loss: 3.5738 (3.7385)  class_acc: 0.3333 (0.3397)  loss_scale: 65536.0000 (59721.0552)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1222  max mem: 15572
Epoch: [35]  [1260/2809]  eta: 0:14:41  lr: 0.000002  min_lr: 0.000000  loss: 3.5963 (3.7388)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (59767.1689)  weight_decay: 0.0500 (0.0500)  time: 0.6097  data: 0.1530  max mem: 15572
Epoch: [35]  [1270/2809]  eta: 0:14:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6583 (3.7385)  class_acc: 0.2917 (0.3398)  loss_scale: 65536.0000 (59812.5570)  weight_decay: 0.0500 (0.0500)  time: 0.5648  data: 0.1182  max mem: 15572
Epoch: [35]  [1280/2809]  eta: 0:14:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6214 (3.7377)  class_acc: 0.3750 (0.3402)  loss_scale: 65536.0000 (59857.2365)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0530  max mem: 15572
[2025-01-16 07:09:38,794] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99603
[2025-01-16 07:09:38,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:09:38,795] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1290/2809]  eta: 0:14:23  lr: 0.000002  min_lr: 0.000000  loss: 3.6304 (3.7373)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (59825.0782)  weight_decay: 0.0500 (0.0500)  time: 0.5840  data: 0.1270  max mem: 15572
Epoch: [35]  [1300/2809]  eta: 0:14:17  lr: 0.000002  min_lr: 0.000000  loss: 3.9160 (3.7384)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (59617.1068)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1373  max mem: 15572
Epoch: [35]  [1310/2809]  eta: 0:14:12  lr: 0.000002  min_lr: 0.000000  loss: 3.6661 (3.7367)  class_acc: 0.2917 (0.3406)  loss_scale: 32768.0000 (59412.3082)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1559  max mem: 15572
Epoch: [35]  [1320/2809]  eta: 0:14:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8040 (3.7381)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (59210.6101)  weight_decay: 0.0500 (0.0500)  time: 0.5425  data: 0.0934  max mem: 15572
Epoch: [35]  [1330/2809]  eta: 0:13:59  lr: 0.000002  min_lr: 0.000000  loss: 3.8624 (3.7378)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (59011.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0816  max mem: 15572
Epoch: [35]  [1340/2809]  eta: 0:13:54  lr: 0.000002  min_lr: 0.000000  loss: 3.7731 (3.7372)  class_acc: 0.3333 (0.3404)  loss_scale: 32768.0000 (58816.2386)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1746  max mem: 15572
Epoch: [35]  [1350/2809]  eta: 0:13:48  lr: 0.000002  min_lr: 0.000000  loss: 3.7664 (3.7364)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (58623.4315)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1203  max mem: 15572
Epoch: [35]  [1360/2809]  eta: 0:13:43  lr: 0.000002  min_lr: 0.000000  loss: 3.8755 (3.7380)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (58433.4578)  weight_decay: 0.0500 (0.0500)  time: 0.5407  data: 0.0883  max mem: 15572
Epoch: [35]  [1370/2809]  eta: 0:13:37  lr: 0.000002  min_lr: 0.000000  loss: 3.8013 (3.7369)  class_acc: 0.2917 (0.3404)  loss_scale: 32768.0000 (58246.2553)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1056  max mem: 15572
Epoch: [35]  [1380/2809]  eta: 0:13:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7302 (3.7367)  class_acc: 0.3750 (0.3403)  loss_scale: 32768.0000 (58061.7639)  weight_decay: 0.0500 (0.0500)  time: 0.5110  data: 0.0677  max mem: 15572
Epoch: [35]  [1390/2809]  eta: 0:13:25  lr: 0.000002  min_lr: 0.000000  loss: 3.7858 (3.7382)  class_acc: 0.2500 (0.3398)  loss_scale: 32768.0000 (57879.9252)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1046  max mem: 15572
Epoch: [35]  [1400/2809]  eta: 0:13:20  lr: 0.000002  min_lr: 0.000000  loss: 3.5541 (3.7361)  class_acc: 0.3333 (0.3402)  loss_scale: 32768.0000 (57700.6824)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.1712  max mem: 15572
Epoch: [35]  [1410/2809]  eta: 0:13:14  lr: 0.000002  min_lr: 0.000000  loss: 3.4047 (3.7351)  class_acc: 0.3750 (0.3405)  loss_scale: 32768.0000 (57523.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1156  max mem: 15572
[2025-01-16 07:10:51,866] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:10:51,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1420/2809]  eta: 0:13:08  lr: 0.000002  min_lr: 0.000000  loss: 3.9225 (3.7377)  class_acc: 0.2917 (0.3397)  loss_scale: 32768.0000 (57442.0042)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.0968  max mem: 15572
Epoch: [35]  [1430/2809]  eta: 0:13:02  lr: 0.000002  min_lr: 0.000000  loss: 3.9545 (3.7380)  class_acc: 0.2083 (0.3394)  loss_scale: 65536.0000 (57498.5660)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.0967  max mem: 15572
Epoch: [35]  [1440/2809]  eta: 0:12:56  lr: 0.000002  min_lr: 0.000000  loss: 3.6950 (3.7385)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (57554.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5138  data: 0.0802  max mem: 15572
Epoch: [35]  [1450/2809]  eta: 0:12:51  lr: 0.000002  min_lr: 0.000000  loss: 3.6501 (3.7381)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (57609.3508)  weight_decay: 0.0500 (0.0500)  time: 0.5643  data: 0.1461  max mem: 15572
[2025-01-16 07:11:14,414] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 99774
[2025-01-16 07:11:14,415] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:11:14,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1460/2809]  eta: 0:12:45  lr: 0.000002  min_lr: 0.000000  loss: 3.4428 (3.7364)  class_acc: 0.3750 (0.3394)  loss_scale: 65536.0000 (57618.7488)  weight_decay: 0.0500 (0.0500)  time: 0.5672  data: 0.1223  max mem: 15572
Epoch: [35]  [1470/2809]  eta: 0:12:39  lr: 0.000002  min_lr: 0.000000  loss: 3.6878 (3.7362)  class_acc: 0.3333 (0.3391)  loss_scale: 32768.0000 (57449.8110)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0777  max mem: 15572
Epoch: [35]  [1480/2809]  eta: 0:12:34  lr: 0.000002  min_lr: 0.000000  loss: 3.7085 (3.7364)  class_acc: 0.2917 (0.3390)  loss_scale: 32768.0000 (57283.1546)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1561  max mem: 15572
Epoch: [35]  [1490/2809]  eta: 0:12:27  lr: 0.000002  min_lr: 0.000000  loss: 3.6709 (3.7370)  class_acc: 0.2500 (0.3386)  loss_scale: 32768.0000 (57118.7337)  weight_decay: 0.0500 (0.0500)  time: 0.5543  data: 0.1095  max mem: 15572
Epoch: [35]  [1500/2809]  eta: 0:12:22  lr: 0.000002  min_lr: 0.000000  loss: 3.8785 (3.7382)  class_acc: 0.2500 (0.3381)  loss_scale: 32768.0000 (56956.5037)  weight_decay: 0.0500 (0.0500)  time: 0.4900  data: 0.0413  max mem: 15572
Epoch: [35]  [1510/2809]  eta: 0:12:16  lr: 0.000002  min_lr: 0.000000  loss: 3.9914 (3.7391)  class_acc: 0.2917 (0.3382)  loss_scale: 32768.0000 (56796.4209)  weight_decay: 0.0500 (0.0500)  time: 0.5722  data: 0.1233  max mem: 15572
Epoch: [35]  [1520/2809]  eta: 0:12:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7736 (3.7389)  class_acc: 0.3750 (0.3384)  loss_scale: 32768.0000 (56638.4431)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1528  max mem: 15572
Epoch: [35]  [1530/2809]  eta: 0:12:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8393 (3.7399)  class_acc: 0.3333 (0.3380)  loss_scale: 32768.0000 (56482.5291)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1478  max mem: 15572
Epoch: [35]  [1540/2809]  eta: 0:11:59  lr: 0.000002  min_lr: 0.000000  loss: 3.6962 (3.7376)  class_acc: 0.3333 (0.3385)  loss_scale: 32768.0000 (56328.6385)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1382  max mem: 15572
Epoch: [35]  [1550/2809]  eta: 0:11:53  lr: 0.000002  min_lr: 0.000000  loss: 3.5060 (3.7371)  class_acc: 0.3750 (0.3387)  loss_scale: 32768.0000 (56176.7324)  weight_decay: 0.0500 (0.0500)  time: 0.5209  data: 0.0854  max mem: 15572
Epoch: [35]  [1560/2809]  eta: 0:11:47  lr: 0.000002  min_lr: 0.000000  loss: 3.7234 (3.7383)  class_acc: 0.3750 (0.3382)  loss_scale: 32768.0000 (56026.7726)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0953  max mem: 15572
Epoch: [35]  [1570/2809]  eta: 0:11:41  lr: 0.000002  min_lr: 0.000000  loss: 3.7234 (3.7382)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (55878.7218)  weight_decay: 0.0500 (0.0500)  time: 0.5432  data: 0.0947  max mem: 15572
Epoch: [35]  [1580/2809]  eta: 0:11:36  lr: 0.000002  min_lr: 0.000000  loss: 3.9348 (3.7390)  class_acc: 0.3333 (0.3382)  loss_scale: 32768.0000 (55732.5440)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.1223  max mem: 15572
[2025-01-16 07:12:26,804] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:12:26,804] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1590/2809]  eta: 0:11:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6520 (3.7387)  class_acc: 0.3333 (0.3382)  loss_scale: 32768.0000 (55649.9912)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.1462  max mem: 15572
Epoch: [35]  [1600/2809]  eta: 0:11:24  lr: 0.000002  min_lr: 0.000000  loss: 3.6520 (3.7386)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (55711.7402)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0913  max mem: 15572
Epoch: [35]  [1610/2809]  eta: 0:11:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6871 (3.7379)  class_acc: 0.3750 (0.3386)  loss_scale: 65536.0000 (55772.7225)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0836  max mem: 15572
Epoch: [35]  [1620/2809]  eta: 0:11:13  lr: 0.000002  min_lr: 0.000000  loss: 3.6553 (3.7366)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (55832.9525)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1248  max mem: 15572
Epoch: [35]  [1630/2809]  eta: 0:11:07  lr: 0.000002  min_lr: 0.000000  loss: 3.8209 (3.7376)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (55892.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0979  max mem: 15572
Epoch: [35]  [1640/2809]  eta: 0:11:01  lr: 0.000002  min_lr: 0.000000  loss: 3.8209 (3.7373)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (55951.2102)  weight_decay: 0.0500 (0.0500)  time: 0.5424  data: 0.1022  max mem: 15572
Epoch: [35]  [1650/2809]  eta: 0:10:57  lr: 0.000002  min_lr: 0.000000  loss: 3.7352 (3.7368)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (56009.2647)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.2058  max mem: 15572
Epoch: [35]  [1660/2809]  eta: 0:10:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7341 (3.7372)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (56066.6201)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.1579  max mem: 15572
Epoch: [35]  [1670/2809]  eta: 0:10:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7368 (3.7378)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (56123.2890)  weight_decay: 0.0500 (0.0500)  time: 0.5101  data: 0.0716  max mem: 15572
Epoch: [35]  [1680/2809]  eta: 0:10:39  lr: 0.000002  min_lr: 0.000000  loss: 3.8551 (3.7390)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (56179.2838)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.1028  max mem: 15572
[2025-01-16 07:13:21,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=100000, skipped=664, lr=[1.7921835888244555e-08, 1.7921835888244555e-08, 2.5602622697492225e-08, 2.5602622697492225e-08, 3.6575175282131754e-08, 3.6575175282131754e-08, 5.225025040304537e-08, 5.225025040304537e-08, 7.464321486149339e-08, 7.464321486149339e-08, 1.0663316408784769e-07, 1.0663316408784769e-07, 1.5233309155406816e-07, 1.5233309155406816e-07, 2.1761870222009737e-07, 2.1761870222009737e-07, 3.1088386031442484e-07, 3.1088386031442484e-07, 4.441198004491784e-07, 4.441198004491784e-07, 6.344568577845405e-07, 6.344568577845405e-07, 9.063669396922008e-07, 9.063669396922008e-07, 1.2948099138460012e-06, 1.2948099138460012e-06, 1.8497284483514305e-06, 1.8497284483514305e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:13:21,292] [INFO] [timer.py:260:stop] epoch=0/micro_step=100000/global_step=100000, RunningAvgSamplesPerSec=28.57181451407806, CurrSamplesPerSec=27.743653289000797, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [1690/2809]  eta: 0:10:33  lr: 0.000002  min_lr: 0.000000  loss: 3.7942 (3.7384)  class_acc: 0.2917 (0.3383)  loss_scale: 65536.0000 (56234.6162)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1507  max mem: 15572
Epoch: [35]  [1700/2809]  eta: 0:10:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6112 (3.7370)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (56289.2981)  weight_decay: 0.0500 (0.0500)  time: 0.5781  data: 0.1391  max mem: 15572
Epoch: [35]  [1710/2809]  eta: 0:10:22  lr: 0.000002  min_lr: 0.000000  loss: 3.6948 (3.7367)  class_acc: 0.4167 (0.3392)  loss_scale: 65536.0000 (56343.3407)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.1659  max mem: 15572
[2025-01-16 07:13:40,099] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:13:40,099] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:13:41,542] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100034
[2025-01-16 07:13:41,543] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:13:41,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1720/2809]  eta: 0:10:17  lr: 0.000002  min_lr: 0.000000  loss: 3.7965 (3.7376)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (56510.9959)  weight_decay: 0.0500 (0.0500)  time: 0.6249  data: 0.1802  max mem: 15572
Epoch: [35]  [1730/2809]  eta: 0:10:11  lr: 0.000002  min_lr: 0.000000  loss: 3.7214 (3.7368)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (56563.1334)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.1517  max mem: 15572
Epoch: [35]  [1740/2809]  eta: 0:10:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6295 (3.7359)  class_acc: 0.3750 (0.3393)  loss_scale: 65536.0000 (56614.6720)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1432  max mem: 15572
Epoch: [35]  [1750/2809]  eta: 0:10:00  lr: 0.000002  min_lr: 0.000000  loss: 3.4876 (3.7339)  class_acc: 0.3750 (0.3398)  loss_scale: 65536.0000 (56665.6219)  weight_decay: 0.0500 (0.0500)  time: 0.5269  data: 0.0711  max mem: 15572
Epoch: [35]  [1760/2809]  eta: 0:09:54  lr: 0.000002  min_lr: 0.000000  loss: 3.4876 (3.7336)  class_acc: 0.3750 (0.3395)  loss_scale: 65536.0000 (56715.9932)  weight_decay: 0.0500 (0.0500)  time: 0.4815  data: 0.0419  max mem: 15572
[2025-01-16 07:14:06,945] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100080
[2025-01-16 07:14:06,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:14:06,946] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1770/2809]  eta: 0:09:48  lr: 0.000002  min_lr: 0.000000  loss: 3.7168 (3.7332)  class_acc: 0.3333 (0.3397)  loss_scale: 65536.0000 (56654.7804)  weight_decay: 0.0500 (0.0500)  time: 0.5254  data: 0.0960  max mem: 15572
Epoch: [35]  [1780/2809]  eta: 0:09:42  lr: 0.000002  min_lr: 0.000000  loss: 3.4816 (3.7321)  class_acc: 0.3750 (0.3398)  loss_scale: 32768.0000 (56520.6603)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.1008  max mem: 15572
Epoch: [35]  [1790/2809]  eta: 0:09:36  lr: 0.000002  min_lr: 0.000000  loss: 3.4673 (3.7320)  class_acc: 0.3750 (0.3398)  loss_scale: 32768.0000 (56388.0380)  weight_decay: 0.0500 (0.0500)  time: 0.5360  data: 0.1047  max mem: 15572
Epoch: [35]  [1800/2809]  eta: 0:09:30  lr: 0.000002  min_lr: 0.000000  loss: 3.6184 (3.7317)  class_acc: 0.4167 (0.3401)  loss_scale: 32768.0000 (56256.8884)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.1138  max mem: 15572
Epoch: [35]  [1810/2809]  eta: 0:09:25  lr: 0.000002  min_lr: 0.000000  loss: 3.6184 (3.7312)  class_acc: 0.3333 (0.3401)  loss_scale: 32768.0000 (56127.1872)  weight_decay: 0.0500 (0.0500)  time: 0.5559  data: 0.1207  max mem: 15572
Epoch: [35]  [1820/2809]  eta: 0:09:19  lr: 0.000002  min_lr: 0.000000  loss: 3.6369 (3.7312)  class_acc: 0.3333 (0.3404)  loss_scale: 32768.0000 (55998.9105)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.1494  max mem: 15572
Epoch: [35]  [1830/2809]  eta: 0:09:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6369 (3.7299)  class_acc: 0.3750 (0.3410)  loss_scale: 32768.0000 (55872.0350)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.1822  max mem: 15572
Epoch: [35]  [1840/2809]  eta: 0:09:08  lr: 0.000002  min_lr: 0.000000  loss: 3.8831 (3.7301)  class_acc: 0.3750 (0.3408)  loss_scale: 32768.0000 (55746.5378)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1313  max mem: 15572
Epoch: [35]  [1850/2809]  eta: 0:09:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9242 (3.7306)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (55622.3965)  weight_decay: 0.0500 (0.0500)  time: 0.5423  data: 0.1087  max mem: 15572
Epoch: [35]  [1860/2809]  eta: 0:08:57  lr: 0.000002  min_lr: 0.000000  loss: 4.0082 (3.7320)  class_acc: 0.2500 (0.3405)  loss_scale: 32768.0000 (55499.5895)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1259  max mem: 15572
Epoch: [35]  [1870/2809]  eta: 0:08:51  lr: 0.000002  min_lr: 0.000000  loss: 3.8412 (3.7307)  class_acc: 0.2917 (0.3409)  loss_scale: 32768.0000 (55378.0951)  weight_decay: 0.0500 (0.0500)  time: 0.5194  data: 0.0629  max mem: 15572
Epoch: [35]  [1880/2809]  eta: 0:08:45  lr: 0.000002  min_lr: 0.000000  loss: 3.6203 (3.7307)  class_acc: 0.3333 (0.3407)  loss_scale: 32768.0000 (55257.8926)  weight_decay: 0.0500 (0.0500)  time: 0.5380  data: 0.1015  max mem: 15572
Epoch: [35]  [1890/2809]  eta: 0:08:40  lr: 0.000002  min_lr: 0.000000  loss: 3.8986 (3.7316)  class_acc: 0.2500 (0.3404)  loss_scale: 32768.0000 (55138.9614)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.1759  max mem: 15572
[2025-01-16 07:15:20,359] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:15:20,360] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [1900/2809]  eta: 0:08:34  lr: 0.000002  min_lr: 0.000000  loss: 3.9948 (3.7323)  class_acc: 0.2917 (0.3404)  loss_scale: 32768.0000 (55141.9421)  weight_decay: 0.0500 (0.0500)  time: 0.6122  data: 0.1847  max mem: 15572
Epoch: [35]  [1910/2809]  eta: 0:08:29  lr: 0.000002  min_lr: 0.000000  loss: 3.7530 (3.7323)  class_acc: 0.2917 (0.3406)  loss_scale: 65536.0000 (55196.3328)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.1689  max mem: 15572
Epoch: [35]  [1920/2809]  eta: 0:08:23  lr: 0.000002  min_lr: 0.000000  loss: 3.5781 (3.7306)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (55250.1572)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.1506  max mem: 15572
[2025-01-16 07:15:40,161] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100244
[2025-01-16 07:15:40,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:15:40,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [1930/2809]  eta: 0:08:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6430 (3.7312)  class_acc: 0.3750 (0.3411)  loss_scale: 65536.0000 (55269.4852)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1273  max mem: 15572
Epoch: [35]  [1940/2809]  eta: 0:08:12  lr: 0.000002  min_lr: 0.000000  loss: 3.8141 (3.7316)  class_acc: 0.2917 (0.3409)  loss_scale: 32768.0000 (55153.5580)  weight_decay: 0.0500 (0.0500)  time: 0.5256  data: 0.0920  max mem: 15572
Epoch: [35]  [1950/2809]  eta: 0:08:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6786 (3.7314)  class_acc: 0.2917 (0.3410)  loss_scale: 32768.0000 (55038.8191)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.1089  max mem: 15572
Epoch: [35]  [1960/2809]  eta: 0:08:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8082 (3.7323)  class_acc: 0.2917 (0.3406)  loss_scale: 32768.0000 (54925.2504)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1422  max mem: 15572
Epoch: [35]  [1970/2809]  eta: 0:07:55  lr: 0.000002  min_lr: 0.000000  loss: 4.0232 (3.7334)  class_acc: 0.2500 (0.3403)  loss_scale: 32768.0000 (54812.8341)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1222  max mem: 15572
Epoch: [35]  [1980/2809]  eta: 0:07:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7608 (3.7320)  class_acc: 0.3333 (0.3404)  loss_scale: 32768.0000 (54701.5528)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0746  max mem: 15572
Epoch: [35]  [1990/2809]  eta: 0:07:43  lr: 0.000002  min_lr: 0.000000  loss: 3.5728 (3.7318)  class_acc: 0.3750 (0.3405)  loss_scale: 32768.0000 (54591.3893)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0728  max mem: 15572
Epoch: [35]  [2000/2809]  eta: 0:07:37  lr: 0.000002  min_lr: 0.000000  loss: 3.8106 (3.7326)  class_acc: 0.2917 (0.3402)  loss_scale: 32768.0000 (54482.3268)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.0942  max mem: 15572
Epoch: [35]  [2010/2809]  eta: 0:07:32  lr: 0.000002  min_lr: 0.000000  loss: 3.8106 (3.7334)  class_acc: 0.2917 (0.3401)  loss_scale: 32768.0000 (54374.3491)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1225  max mem: 15572
Epoch: [35]  [2020/2809]  eta: 0:07:26  lr: 0.000002  min_lr: 0.000000  loss: 3.6563 (3.7329)  class_acc: 0.3750 (0.3402)  loss_scale: 32768.0000 (54267.4399)  weight_decay: 0.0500 (0.0500)  time: 0.5714  data: 0.1363  max mem: 15572
Epoch: [35]  [2030/2809]  eta: 0:07:20  lr: 0.000002  min_lr: 0.000000  loss: 3.6297 (3.7325)  class_acc: 0.3333 (0.3404)  loss_scale: 32768.0000 (54161.5835)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1016  max mem: 15572
Epoch: [35]  [2040/2809]  eta: 0:07:15  lr: 0.000002  min_lr: 0.000000  loss: 3.8303 (3.7326)  class_acc: 0.2917 (0.3404)  loss_scale: 32768.0000 (54056.7643)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.1180  max mem: 15572
Epoch: [35]  [2050/2809]  eta: 0:07:09  lr: 0.000002  min_lr: 0.000000  loss: 3.8560 (3.7331)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (53952.9673)  weight_decay: 0.0500 (0.0500)  time: 0.5540  data: 0.1226  max mem: 15572
[2025-01-16 07:16:51,187] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:16:51,187] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2060/2809]  eta: 0:07:03  lr: 0.000002  min_lr: 0.000000  loss: 3.9066 (3.7338)  class_acc: 0.2500 (0.3399)  loss_scale: 32768.0000 (53897.8748)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1160  max mem: 15572
Epoch: [35]  [2070/2809]  eta: 0:06:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6660 (3.7336)  class_acc: 0.2500 (0.3399)  loss_scale: 65536.0000 (53954.0705)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1465  max mem: 15572
Epoch: [35]  [2080/2809]  eta: 0:06:52  lr: 0.000002  min_lr: 0.000000  loss: 3.5315 (3.7330)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (54009.7261)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1364  max mem: 15572
Epoch: [35]  [2090/2809]  eta: 0:06:46  lr: 0.000002  min_lr: 0.000000  loss: 3.6181 (3.7326)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (54064.8494)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1188  max mem: 15572
Epoch: [35]  [2100/2809]  eta: 0:06:41  lr: 0.000002  min_lr: 0.000000  loss: 3.6181 (3.7324)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (54119.4479)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.1942  max mem: 15572
Epoch: [35]  [2110/2809]  eta: 0:06:35  lr: 0.000002  min_lr: 0.000000  loss: 3.4180 (3.7308)  class_acc: 0.4167 (0.3406)  loss_scale: 65536.0000 (54173.5291)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.1510  max mem: 15572
Epoch: [35]  [2120/2809]  eta: 0:06:30  lr: 0.000002  min_lr: 0.000000  loss: 3.5035 (3.7310)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (54227.1004)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.0990  max mem: 15572
Epoch: [35]  [2130/2809]  eta: 0:06:24  lr: 0.000002  min_lr: 0.000000  loss: 3.7538 (3.7305)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (54280.1689)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1219  max mem: 15572
Epoch: [35]  [2140/2809]  eta: 0:06:18  lr: 0.000002  min_lr: 0.000000  loss: 3.6165 (3.7298)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (54332.7417)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.0978  max mem: 15572
Epoch: [35]  [2150/2809]  eta: 0:06:12  lr: 0.000002  min_lr: 0.000000  loss: 3.6165 (3.7298)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (54384.8257)  weight_decay: 0.0500 (0.0500)  time: 0.5105  data: 0.0603  max mem: 15572
Epoch: [35]  [2160/2809]  eta: 0:06:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6611 (3.7295)  class_acc: 0.3333 (0.3411)  loss_scale: 65536.0000 (54436.4276)  weight_decay: 0.0500 (0.0500)  time: 0.4598  data: 0.0115  max mem: 15572
Epoch: [35]  [2170/2809]  eta: 0:06:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6251 (3.7287)  class_acc: 0.3333 (0.3411)  loss_scale: 65536.0000 (54487.5541)  weight_decay: 0.0500 (0.0500)  time: 0.5039  data: 0.0579  max mem: 15572
Epoch: [35]  [2180/2809]  eta: 0:05:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7523 (3.7297)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (54538.2118)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.1491  max mem: 15572
[2025-01-16 07:18:01,987] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100500
[2025-01-16 07:18:01,988] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:18:01,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2190/2809]  eta: 0:05:49  lr: 0.000002  min_lr: 0.000000  loss: 3.9584 (3.7298)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (54498.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1258  max mem: 15572
Epoch: [35]  [2200/2809]  eta: 0:05:43  lr: 0.000002  min_lr: 0.000000  loss: 3.6915 (3.7289)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (54399.9418)  weight_decay: 0.0500 (0.0500)  time: 0.4999  data: 0.0649  max mem: 15572
Epoch: [35]  [2210/2809]  eta: 0:05:38  lr: 0.000002  min_lr: 0.000000  loss: 3.7848 (3.7300)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (54302.1040)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0687  max mem: 15572
Epoch: [35]  [2220/2809]  eta: 0:05:32  lr: 0.000002  min_lr: 0.000000  loss: 3.9760 (3.7304)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (54205.1472)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0943  max mem: 15572
Epoch: [35]  [2230/2809]  eta: 0:05:27  lr: 0.000002  min_lr: 0.000000  loss: 3.6409 (3.7298)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (54109.0596)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1310  max mem: 15572
Epoch: [35]  [2240/2809]  eta: 0:05:21  lr: 0.000002  min_lr: 0.000000  loss: 3.6739 (3.7302)  class_acc: 0.3750 (0.3409)  loss_scale: 32768.0000 (54013.8295)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0955  max mem: 15572
Epoch: [35]  [2250/2809]  eta: 0:05:15  lr: 0.000002  min_lr: 0.000000  loss: 3.7895 (3.7301)  class_acc: 0.2917 (0.3410)  loss_scale: 32768.0000 (53919.4456)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1031  max mem: 15572
Epoch: [35]  [2260/2809]  eta: 0:05:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7895 (3.7308)  class_acc: 0.2917 (0.3409)  loss_scale: 32768.0000 (53825.8965)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1310  max mem: 15572
Epoch: [35]  [2270/2809]  eta: 0:05:04  lr: 0.000002  min_lr: 0.000000  loss: 3.7556 (3.7304)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (53733.1713)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0596  max mem: 15572
Epoch: [35]  [2280/2809]  eta: 0:04:58  lr: 0.000002  min_lr: 0.000000  loss: 3.6610 (3.7302)  class_acc: 0.3750 (0.3412)  loss_scale: 32768.0000 (53641.2591)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.1049  max mem: 15572
Epoch: [35]  [2290/2809]  eta: 0:04:53  lr: 0.000002  min_lr: 0.000000  loss: 3.6610 (3.7300)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (53550.1493)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.1905  max mem: 15572
Epoch: [35]  [2300/2809]  eta: 0:04:47  lr: 0.000002  min_lr: 0.000000  loss: 3.6754 (3.7304)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (53459.8314)  weight_decay: 0.0500 (0.0500)  time: 0.6282  data: 0.1539  max mem: 15572
Epoch: [35]  [2310/2809]  eta: 0:04:42  lr: 0.000002  min_lr: 0.000000  loss: 3.8025 (3.7300)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (53370.2951)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1242  max mem: 15572
[2025-01-16 07:19:15,533] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:19:15,533] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [2320/2809]  eta: 0:04:36  lr: 0.000002  min_lr: 0.000000  loss: 3.5494 (3.7299)  class_acc: 0.3333 (0.3411)  loss_scale: 32768.0000 (53380.3567)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1079  max mem: 15572
Epoch: [35]  [2330/2809]  eta: 0:04:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7385 (3.7307)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (53432.5045)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1449  max mem: 15572
Epoch: [35]  [2340/2809]  eta: 0:04:25  lr: 0.000002  min_lr: 0.000000  loss: 3.8641 (3.7305)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (53484.2067)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1403  max mem: 15572
Epoch: [35]  [2350/2809]  eta: 0:04:19  lr: 0.000002  min_lr: 0.000000  loss: 3.8641 (3.7302)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (53535.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.1110  max mem: 15572
Epoch: [35]  [2360/2809]  eta: 0:04:13  lr: 0.000002  min_lr: 0.000000  loss: 3.8531 (3.7306)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (53586.2973)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.1142  max mem: 15572
Epoch: [35]  [2370/2809]  eta: 0:04:08  lr: 0.000002  min_lr: 0.000000  loss: 3.7843 (3.7299)  class_acc: 0.3750 (0.3414)  loss_scale: 65536.0000 (53636.6968)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.1513  max mem: 15572
Epoch: [35]  [2380/2809]  eta: 0:04:02  lr: 0.000002  min_lr: 0.000000  loss: 3.4070 (3.7292)  class_acc: 0.4167 (0.3417)  loss_scale: 65536.0000 (53686.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5815  data: 0.1574  max mem: 15572
Epoch: [35]  [2390/2809]  eta: 0:03:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6345 (3.7292)  class_acc: 0.3750 (0.3418)  loss_scale: 65536.0000 (53736.2309)  weight_decay: 0.0500 (0.0500)  time: 0.6394  data: 0.2137  max mem: 15572
Epoch: [35]  [2400/2809]  eta: 0:03:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7812 (3.7295)  class_acc: 0.3333 (0.3417)  loss_scale: 65536.0000 (53785.3761)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.2092  max mem: 15572
Epoch: [35]  [2410/2809]  eta: 0:03:45  lr: 0.000002  min_lr: 0.000000  loss: 3.6798 (3.7285)  class_acc: 0.3750 (0.3420)  loss_scale: 65536.0000 (53834.1136)  weight_decay: 0.0500 (0.0500)  time: 0.5452  data: 0.1177  max mem: 15572
Epoch: [35]  [2420/2809]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000000  loss: 3.6189 (3.7281)  class_acc: 0.4167 (0.3424)  loss_scale: 65536.0000 (53882.4486)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1150  max mem: 15572
Epoch: [35]  [2430/2809]  eta: 0:03:34  lr: 0.000002  min_lr: 0.000000  loss: 3.6189 (3.7279)  class_acc: 0.4167 (0.3427)  loss_scale: 65536.0000 (53930.3858)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.1134  max mem: 15572
Epoch: [35]  [2440/2809]  eta: 0:03:28  lr: 0.000002  min_lr: 0.000000  loss: 3.6329 (3.7279)  class_acc: 0.3333 (0.3426)  loss_scale: 65536.0000 (53977.9304)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.1185  max mem: 15572
[2025-01-16 07:20:28,610] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:20:28,610] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:20:30,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100761
[2025-01-16 07:20:30,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:20:30,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2450/2809]  eta: 0:03:23  lr: 0.000002  min_lr: 0.000000  loss: 3.7550 (3.7279)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54132.0408)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1323  max mem: 15572
Epoch: [35]  [2460/2809]  eta: 0:03:17  lr: 0.000002  min_lr: 0.000000  loss: 3.7069 (3.7279)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (54178.3795)  weight_decay: 0.0500 (0.0500)  time: 0.5820  data: 0.1503  max mem: 15572
Epoch: [35]  [2470/2809]  eta: 0:03:11  lr: 0.000002  min_lr: 0.000000  loss: 3.6283 (3.7270)  class_acc: 0.3750 (0.3434)  loss_scale: 65536.0000 (54224.3432)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1420  max mem: 15572
Epoch: [35]  [2480/2809]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 3.6283 (3.7269)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (54269.9363)  weight_decay: 0.0500 (0.0500)  time: 0.5844  data: 0.1398  max mem: 15572
Epoch: [35]  [2490/2809]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000000  loss: 3.8800 (3.7272)  class_acc: 0.3333 (0.3432)  loss_scale: 65536.0000 (54315.1634)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1637  max mem: 15572
Epoch: [35]  [2500/2809]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 3.7713 (3.7273)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (54360.0288)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.1710  max mem: 15572
Epoch: [35]  [2510/2809]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 3.7579 (3.7271)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (54404.5368)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1016  max mem: 15572
Epoch: [35]  [2520/2809]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 3.7654 (3.7270)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (54448.6918)  weight_decay: 0.0500 (0.0500)  time: 0.5284  data: 0.0697  max mem: 15572
Epoch: [35]  [2530/2809]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 3.9923 (3.7282)  class_acc: 0.2500 (0.3428)  loss_scale: 65536.0000 (54492.4978)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0988  max mem: 15572
Epoch: [35]  [2540/2809]  eta: 0:02:32  lr: 0.000002  min_lr: 0.000000  loss: 3.8425 (3.7285)  class_acc: 0.2500 (0.3427)  loss_scale: 65536.0000 (54535.9591)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1072  max mem: 15572
Epoch: [35]  [2550/2809]  eta: 0:02:26  lr: 0.000002  min_lr: 0.000000  loss: 3.8401 (3.7289)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (54579.0796)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1276  max mem: 15572
Epoch: [35]  [2560/2809]  eta: 0:02:20  lr: 0.000002  min_lr: 0.000000  loss: 3.8644 (3.7290)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54621.8633)  weight_decay: 0.0500 (0.0500)  time: 0.5670  data: 0.1351  max mem: 15572
Epoch: [35]  [2570/2809]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 3.8814 (3.7288)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (54664.3143)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1544  max mem: 15572
[2025-01-16 07:21:44,617] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:21:44,618] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:21:47,853] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 100893
[2025-01-16 07:21:47,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:21:47,854] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2580/2809]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000000  loss: 3.5485 (3.7285)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54782.6114)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.1938  max mem: 15572
Epoch: [35]  [2590/2809]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 3.5553 (3.7278)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54824.1142)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1277  max mem: 15572
Epoch: [35]  [2600/2809]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 3.5553 (3.7270)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54865.2980)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0679  max mem: 15572
Epoch: [35]  [2610/2809]  eta: 0:01:52  lr: 0.000002  min_lr: 0.000000  loss: 3.4711 (3.7260)  class_acc: 0.3750 (0.3430)  loss_scale: 65536.0000 (54906.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1019  max mem: 15572
Epoch: [35]  [2620/2809]  eta: 0:01:47  lr: 0.000002  min_lr: 0.000000  loss: 3.6223 (3.7260)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54946.7226)  weight_decay: 0.0500 (0.0500)  time: 0.6258  data: 0.1697  max mem: 15572
Epoch: [35]  [2630/2809]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 3.6864 (3.7260)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54986.9707)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1384  max mem: 15572
Epoch: [35]  [2640/2809]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 3.6643 (3.7256)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (55026.9140)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1044  max mem: 15572
Epoch: [35]  [2650/2809]  eta: 0:01:30  lr: 0.000002  min_lr: 0.000000  loss: 3.7672 (3.7262)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (55066.5560)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1140  max mem: 15572
Epoch: [35]  [2660/2809]  eta: 0:01:24  lr: 0.000002  min_lr: 0.000000  loss: 3.7557 (3.7256)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (55105.9000)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1313  max mem: 15572
Epoch: [35]  [2670/2809]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 3.7557 (3.7259)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (55144.9495)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1685  max mem: 15572
Epoch: [35]  [2680/2809]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 3.8012 (3.7255)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (55183.7076)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0932  max mem: 15572
[2025-01-16 07:22:46,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=101000, skipped=670, lr=[1.523978886112964e-08, 1.523978886112964e-08, 2.1771126944470916e-08, 2.1771126944470916e-08, 3.110160992067274e-08, 3.110160992067274e-08, 4.4430871315246775e-08, 4.4430871315246775e-08, 6.34726733074954e-08, 6.34726733074954e-08, 9.067524758213629e-08, 9.067524758213629e-08, 1.2953606797448042e-07, 1.2953606797448042e-07, 1.8505152567782917e-07, 1.8505152567782917e-07, 2.6435932239689883e-07, 2.6435932239689883e-07, 3.7765617485271267e-07, 3.7765617485271267e-07, 5.39508821218161e-07, 5.39508821218161e-07, 7.707268874545158e-07, 7.707268874545158e-07, 1.1010384106493083e-06, 1.1010384106493083e-06, 1.5729120152132976e-06, 1.5729120152132976e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:22:46,555] [INFO] [timer.py:260:stop] epoch=0/micro_step=101000/global_step=101000, RunningAvgSamplesPerSec=28.57301753528249, CurrSamplesPerSec=31.673531033128686, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [35]  [2690/2809]  eta: 0:01:07  lr: 0.000002  min_lr: 0.000000  loss: 3.5660 (3.7249)  class_acc: 0.3750 (0.3430)  loss_scale: 65536.0000 (55222.1776)  weight_decay: 0.0500 (0.0500)  time: 0.5001  data: 0.0621  max mem: 15572
Epoch: [35]  [2700/2809]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 3.7565 (3.7259)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (55260.3628)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1540  max mem: 15572
[2025-01-16 07:23:00,257] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:23:00,257] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:23:02,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101024
[2025-01-16 07:23:02,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:23:02,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [2710/2809]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 3.8176 (3.7253)  class_acc: 0.3750 (0.3430)  loss_scale: 65536.0000 (55346.6145)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.1582  max mem: 15572
[2025-01-16 07:23:08,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101034
[2025-01-16 07:23:08,719] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:23:08,719] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [2720/2809]  eta: 0:00:50  lr: 0.000002  min_lr: 0.000000  loss: 3.7325 (3.7255)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (55359.9765)  weight_decay: 0.0500 (0.0500)  time: 0.6228  data: 0.1649  max mem: 15572
Epoch: [35]  [2730/2809]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 3.7325 (3.7248)  class_acc: 0.3333 (0.3431)  loss_scale: 32768.0000 (55277.2523)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1160  max mem: 15572
Epoch: [35]  [2740/2809]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 3.6402 (3.7247)  class_acc: 0.3750 (0.3432)  loss_scale: 32768.0000 (55195.1317)  weight_decay: 0.0500 (0.0500)  time: 0.4612  data: 0.0209  max mem: 15572
Epoch: [35]  [2750/2809]  eta: 0:00:33  lr: 0.000002  min_lr: 0.000000  loss: 3.8628 (3.7254)  class_acc: 0.3333 (0.3431)  loss_scale: 32768.0000 (55113.6081)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.0866  max mem: 15572
Epoch: [35]  [2760/2809]  eta: 0:00:27  lr: 0.000002  min_lr: 0.000000  loss: 3.8083 (3.7256)  class_acc: 0.2500 (0.3428)  loss_scale: 32768.0000 (55032.6751)  weight_decay: 0.0500 (0.0500)  time: 0.5205  data: 0.0662  max mem: 15572
Epoch: [35]  [2770/2809]  eta: 0:00:22  lr: 0.000002  min_lr: 0.000000  loss: 3.7348 (3.7252)  class_acc: 0.2917 (0.3429)  loss_scale: 32768.0000 (54952.3262)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0737  max mem: 15572
Epoch: [35]  [2780/2809]  eta: 0:00:16  lr: 0.000002  min_lr: 0.000000  loss: 3.7440 (3.7251)  class_acc: 0.3333 (0.3429)  loss_scale: 32768.0000 (54872.5552)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0737  max mem: 15572
Epoch: [35]  [2790/2809]  eta: 0:00:10  lr: 0.000002  min_lr: 0.000000  loss: 3.7460 (3.7256)  class_acc: 0.3333 (0.3429)  loss_scale: 32768.0000 (54793.3558)  weight_decay: 0.0500 (0.0500)  time: 0.4502  data: 0.0008  max mem: 15572
Epoch: [35]  [2800/2809]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000000  loss: 3.8679 (3.7257)  class_acc: 0.3333 (0.3430)  loss_scale: 32768.0000 (54714.7219)  weight_decay: 0.0500 (0.0500)  time: 0.4816  data: 0.0530  max mem: 15572
Epoch: [35]  [2808/2809]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 3.6121 (3.7248)  class_acc: 0.4167 (0.3433)  loss_scale: 32768.0000 (54652.2179)  weight_decay: 0.0500 (0.0500)  time: 0.4450  data: 0.0528  max mem: 15572
Epoch: [35] Total time: 0:26:26 (0.5649 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 3.6121 (3.7248)  class_acc: 0.4167 (0.3433)  loss_scale: 32768.0000 (54652.2179)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:21:42  loss: 0.3878 (0.3878)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.7875  data: 4.5791  max mem: 15572
Val:  [ 10/272]  eta: 0:03:31  loss: 2.2763 (2.2160)  acc1: 50.0000 (44.4444)  acc5: 77.7778 (75.2525)  time: 0.8091  data: 0.6173  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 2.2763 (2.2610)  acc1: 44.4444 (45.2381)  acc5: 77.7778 (74.6032)  time: 0.3385  data: 0.1449  max mem: 15572
Val:  [ 30/272]  eta: 0:01:47  loss: 2.3054 (2.3441)  acc1: 44.4444 (43.1900)  acc5: 77.7778 (73.8351)  time: 0.2415  data: 0.0457  max mem: 15572
Val:  [ 40/272]  eta: 0:01:37  loss: 2.5272 (2.4130)  acc1: 27.7778 (39.8374)  acc5: 72.2222 (73.8482)  time: 0.2863  data: 0.0925  max mem: 15572
Val:  [ 50/272]  eta: 0:01:31  loss: 2.4465 (2.3352)  acc1: 33.3333 (41.8301)  acc5: 77.7778 (75.4902)  time: 0.3631  data: 0.1692  max mem: 15572
Val:  [ 60/272]  eta: 0:01:24  loss: 1.4749 (2.2340)  acc1: 66.6667 (45.0820)  acc5: 83.3333 (76.2295)  time: 0.3555  data: 0.1268  max mem: 15572
Val:  [ 70/272]  eta: 0:01:19  loss: 1.5625 (2.1615)  acc1: 66.6667 (47.6526)  acc5: 83.3333 (76.9953)  time: 0.3436  data: 0.1032  max mem: 15572
Val:  [ 80/272]  eta: 0:01:14  loss: 1.8541 (2.1711)  acc1: 55.5556 (47.8052)  acc5: 77.7778 (76.7490)  time: 0.3521  data: 0.1241  max mem: 15572
Val:  [ 90/272]  eta: 0:01:10  loss: 2.1920 (2.1757)  acc1: 50.0000 (48.0464)  acc5: 77.7778 (77.2283)  time: 0.3687  data: 0.1271  max mem: 15572
Val:  [100/272]  eta: 0:01:05  loss: 2.1088 (2.2010)  acc1: 44.4444 (47.2497)  acc5: 83.3333 (77.1727)  time: 0.3567  data: 0.1079  max mem: 15572
Val:  [110/272]  eta: 0:01:01  loss: 2.3223 (2.2725)  acc1: 27.7778 (45.3453)  acc5: 83.3333 (76.1261)  time: 0.3368  data: 0.1155  max mem: 15572
Val:  [120/272]  eta: 0:00:58  loss: 2.8238 (2.3077)  acc1: 16.6667 (44.6281)  acc5: 66.6667 (75.7576)  time: 0.3849  data: 0.1946  max mem: 15572
Val:  [130/272]  eta: 0:00:53  loss: 2.1490 (2.2738)  acc1: 50.0000 (45.5471)  acc5: 83.3333 (76.5055)  time: 0.3909  data: 0.1960  max mem: 15572
Val:  [140/272]  eta: 0:00:49  loss: 1.6563 (2.2675)  acc1: 55.5556 (45.9811)  acc5: 88.8889 (76.2805)  time: 0.3172  data: 0.1247  max mem: 15572
Val:  [150/272]  eta: 0:00:44  loss: 2.2413 (2.2715)  acc1: 38.8889 (45.5114)  acc5: 83.3333 (76.4533)  time: 0.2731  data: 0.0762  max mem: 15572
Val:  [160/272]  eta: 0:00:40  loss: 2.2413 (2.2611)  acc1: 44.4444 (45.9282)  acc5: 77.7778 (76.6046)  time: 0.2901  data: 0.0868  max mem: 15572
Val:  [170/272]  eta: 0:00:36  loss: 2.3827 (2.2797)  acc1: 44.4444 (45.4516)  acc5: 72.2222 (76.1209)  time: 0.2880  data: 0.0919  max mem: 15572
Val:  [180/272]  eta: 0:00:32  loss: 2.2673 (2.2694)  acc1: 38.8889 (45.3959)  acc5: 72.2222 (76.5500)  time: 0.3131  data: 0.1208  max mem: 15572
Val:  [190/272]  eta: 0:00:29  loss: 2.2673 (2.3220)  acc1: 33.3333 (44.2408)  acc5: 77.7778 (75.3054)  time: 0.3281  data: 0.1216  max mem: 15572
Val:  [200/272]  eta: 0:00:25  loss: 2.5741 (2.3322)  acc1: 38.8889 (43.9746)  acc5: 66.6667 (75.0415)  time: 0.3033  data: 0.1029  max mem: 15572
Val:  [210/272]  eta: 0:00:21  loss: 2.1589 (2.3352)  acc1: 44.4444 (44.1022)  acc5: 77.7778 (74.9078)  time: 0.3024  data: 0.0997  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1087 (2.3224)  acc1: 44.4444 (44.4193)  acc5: 77.7778 (75.0377)  time: 0.2876  data: 0.0845  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.7120 (2.2937)  acc1: 61.1111 (45.4305)  acc5: 77.7778 (75.4209)  time: 0.2998  data: 0.1200  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 1.5732 (2.2775)  acc1: 61.1111 (45.7815)  acc5: 83.3333 (75.7492)  time: 0.3202  data: 0.1296  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.1863 (2.2887)  acc1: 50.0000 (45.2191)  acc5: 83.3333 (75.7857)  time: 0.3160  data: 0.1112  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.1328 (2.2321)  acc1: 66.6667 (46.8923)  acc5: 88.8889 (76.4794)  time: 0.2772  data: 0.0858  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2673 (2.2254)  acc1: 66.6667 (47.0890)  acc5: 88.8889 (76.6913)  time: 0.1973  data: 0.0381  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2673 (2.2299)  acc1: 66.6667 (47.0612)  acc5: 88.8889 (76.6537)  time: 0.1910  data: 0.0381  max mem: 15572
Val: Total time: 0:01:30 (0.3318 s / it)
* Acc@1 47.061 Acc@5 76.654 loss 2.230
Accuracy of the network on the 4883 val videos: 47.1%
Max accuracy: 47.47%
Epoch: [36]  [   0/2809]  eta: 5:10:07  lr: 0.000002  min_lr: 0.000000  loss: 3.7275 (3.7275)  class_acc: 0.2917 (0.2917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 6.6242  data: 6.0996  max mem: 15572
Epoch: [36]  [  10/2809]  eta: 0:53:13  lr: 0.000002  min_lr: 0.000000  loss: 3.7174 (3.6883)  class_acc: 0.3333 (0.3220)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1409  data: 0.7282  max mem: 15572
Epoch: [36]  [  20/2809]  eta: 0:36:19  lr: 0.000002  min_lr: 0.000000  loss: 3.7337 (3.8356)  class_acc: 0.2917 (0.3036)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4892  data: 0.0997  max mem: 15572
Epoch: [36]  [  30/2809]  eta: 0:31:35  lr: 0.000002  min_lr: 0.000000  loss: 4.0569 (3.8423)  class_acc: 0.2083 (0.3118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4299  data: 0.0046  max mem: 15572
[2025-01-16 07:25:47,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:25:47,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [  40/2809]  eta: 0:28:51  lr: 0.000002  min_lr: 0.000000  loss: 3.7956 (3.8259)  class_acc: 0.3333 (0.3232)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.4617  data: 0.0007  max mem: 15572
Epoch: [36]  [  50/2809]  eta: 0:27:19  lr: 0.000002  min_lr: 0.000000  loss: 3.8735 (3.8288)  class_acc: 0.3333 (0.3235)  loss_scale: 65536.0000 (40478.1176)  weight_decay: 0.0500 (0.0500)  time: 0.4581  data: 0.0006  max mem: 15572
Epoch: [36]  [  60/2809]  eta: 0:26:54  lr: 0.000002  min_lr: 0.000000  loss: 3.8908 (3.8191)  class_acc: 0.2917 (0.3231)  loss_scale: 65536.0000 (44585.9672)  weight_decay: 0.0500 (0.0500)  time: 0.5092  data: 0.0477  max mem: 15572
Epoch: [36]  [  70/2809]  eta: 0:27:54  lr: 0.000002  min_lr: 0.000000  loss: 3.7799 (3.7852)  class_acc: 0.3333 (0.3286)  loss_scale: 65536.0000 (47536.6761)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.1882  max mem: 15572
Epoch: [36]  [  80/2809]  eta: 0:27:57  lr: 0.000002  min_lr: 0.000000  loss: 3.6335 (3.7546)  class_acc: 0.3750 (0.3374)  loss_scale: 65536.0000 (49758.8148)  weight_decay: 0.0500 (0.0500)  time: 0.6975  data: 0.2377  max mem: 15572
Epoch: [36]  [  90/2809]  eta: 0:28:14  lr: 0.000002  min_lr: 0.000000  loss: 3.6751 (3.7592)  class_acc: 0.3333 (0.3352)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.2179  max mem: 15572
Epoch: [36]  [ 100/2809]  eta: 0:28:31  lr: 0.000002  min_lr: 0.000000  loss: 3.7662 (3.7432)  class_acc: 0.2917 (0.3366)  loss_scale: 65536.0000 (52883.0099)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.2407  max mem: 15572
Epoch: [36]  [ 110/2809]  eta: 0:28:33  lr: 0.000002  min_lr: 0.000000  loss: 3.5509 (3.7307)  class_acc: 0.3333 (0.3375)  loss_scale: 65536.0000 (54022.9189)  weight_decay: 0.0500 (0.0500)  time: 0.6881  data: 0.2302  max mem: 15572
Epoch: [36]  [ 120/2809]  eta: 0:28:54  lr: 0.000002  min_lr: 0.000000  loss: 3.6994 (3.7368)  class_acc: 0.2917 (0.3364)  loss_scale: 65536.0000 (54974.4132)  weight_decay: 0.0500 (0.0500)  time: 0.7127  data: 0.2625  max mem: 15572
Epoch: [36]  [ 130/2809]  eta: 0:28:40  lr: 0.000002  min_lr: 0.000000  loss: 3.9381 (3.7510)  class_acc: 0.2917 (0.3327)  loss_scale: 65536.0000 (55780.6412)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.2271  max mem: 15572
Epoch: [36]  [ 140/2809]  eta: 0:28:40  lr: 0.000002  min_lr: 0.000000  loss: 3.7896 (3.7450)  class_acc: 0.3333 (0.3348)  loss_scale: 65536.0000 (56472.5106)  weight_decay: 0.0500 (0.0500)  time: 0.6411  data: 0.1833  max mem: 15572
Epoch: [36]  [ 150/2809]  eta: 0:28:42  lr: 0.000002  min_lr: 0.000000  loss: 3.8434 (3.7449)  class_acc: 0.2917 (0.3333)  loss_scale: 65536.0000 (57072.7417)  weight_decay: 0.0500 (0.0500)  time: 0.6850  data: 0.2256  max mem: 15572
Epoch: [36]  [ 160/2809]  eta: 0:28:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6716 (3.7285)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (57598.4099)  weight_decay: 0.0500 (0.0500)  time: 0.6130  data: 0.1720  max mem: 15572
[2025-01-16 07:27:08,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:27:08,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:27:09,139] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101292
[2025-01-16 07:27:09,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:27:09,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 170/2809]  eta: 0:27:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8722 (3.7449)  class_acc: 0.3750 (0.3348)  loss_scale: 65536.0000 (58445.8480)  weight_decay: 0.0500 (0.0500)  time: 0.4772  data: 0.0548  max mem: 15572
Epoch: [36]  [ 180/2809]  eta: 0:27:09  lr: 0.000001  min_lr: 0.000000  loss: 3.8879 (3.7394)  class_acc: 0.2500 (0.3352)  loss_scale: 65536.0000 (58837.5691)  weight_decay: 0.0500 (0.0500)  time: 0.4527  data: 0.0174  max mem: 15572
Epoch: [36]  [ 190/2809]  eta: 0:27:09  lr: 0.000001  min_lr: 0.000000  loss: 3.8879 (3.7574)  class_acc: 0.2500 (0.3296)  loss_scale: 65536.0000 (59188.2723)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1219  max mem: 15572
Epoch: [36]  [ 200/2809]  eta: 0:27:01  lr: 0.000001  min_lr: 0.000000  loss: 4.0070 (3.7595)  class_acc: 0.2917 (0.3304)  loss_scale: 65536.0000 (59504.0796)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.1804  max mem: 15572
Epoch: [36]  [ 210/2809]  eta: 0:26:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7960 (3.7615)  class_acc: 0.3333 (0.3314)  loss_scale: 65536.0000 (59789.9526)  weight_decay: 0.0500 (0.0500)  time: 0.5910  data: 0.1481  max mem: 15572
[2025-01-16 07:27:35,299] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101339
[2025-01-16 07:27:35,299] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:27:35,300] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 220/2809]  eta: 0:26:33  lr: 0.000001  min_lr: 0.000000  loss: 3.7411 (3.7576)  class_acc: 0.3333 (0.3307)  loss_scale: 65536.0000 (59160.3258)  weight_decay: 0.0500 (0.0500)  time: 0.5555  data: 0.1278  max mem: 15572
Epoch: [36]  [ 230/2809]  eta: 0:26:18  lr: 0.000001  min_lr: 0.000000  loss: 3.6180 (3.7576)  class_acc: 0.3750 (0.3317)  loss_scale: 32768.0000 (58017.8009)  weight_decay: 0.0500 (0.0500)  time: 0.5372  data: 0.0991  max mem: 15572
Epoch: [36]  [ 240/2809]  eta: 0:26:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7190 (3.7593)  class_acc: 0.3333 (0.3311)  loss_scale: 32768.0000 (56970.0913)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1272  max mem: 15572
Epoch: [36]  [ 250/2809]  eta: 0:26:01  lr: 0.000001  min_lr: 0.000000  loss: 3.6428 (3.7512)  class_acc: 0.2917 (0.3307)  loss_scale: 32768.0000 (56005.8645)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.1450  max mem: 15572
Epoch: [36]  [ 260/2809]  eta: 0:25:53  lr: 0.000001  min_lr: 0.000000  loss: 3.8655 (3.7602)  class_acc: 0.3333 (0.3297)  loss_scale: 32768.0000 (55115.5249)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1389  max mem: 15572
Epoch: [36]  [ 270/2809]  eta: 0:25:34  lr: 0.000001  min_lr: 0.000000  loss: 3.8655 (3.7580)  class_acc: 0.3750 (0.3307)  loss_scale: 32768.0000 (54290.8930)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0928  max mem: 15572
Epoch: [36]  [ 280/2809]  eta: 0:25:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7131 (3.7598)  class_acc: 0.3750 (0.3307)  loss_scale: 32768.0000 (53524.9537)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1102  max mem: 15572
Epoch: [36]  [ 290/2809]  eta: 0:25:13  lr: 0.000001  min_lr: 0.000000  loss: 3.8168 (3.7649)  class_acc: 0.3333 (0.3295)  loss_scale: 32768.0000 (52811.6564)  weight_decay: 0.0500 (0.0500)  time: 0.5484  data: 0.1313  max mem: 15572
Epoch: [36]  [ 300/2809]  eta: 0:25:09  lr: 0.000001  min_lr: 0.000000  loss: 3.8732 (3.7653)  class_acc: 0.2917 (0.3289)  loss_scale: 32768.0000 (52145.7542)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.1272  max mem: 15572
Epoch: [36]  [ 310/2809]  eta: 0:24:53  lr: 0.000001  min_lr: 0.000000  loss: 3.5977 (3.7560)  class_acc: 0.3333 (0.3316)  loss_scale: 32768.0000 (51522.6752)  weight_decay: 0.0500 (0.0500)  time: 0.5549  data: 0.1115  max mem: 15572
Epoch: [36]  [ 320/2809]  eta: 0:24:54  lr: 0.000001  min_lr: 0.000000  loss: 3.4358 (3.7527)  class_acc: 0.3333 (0.3305)  loss_scale: 32768.0000 (50938.4174)  weight_decay: 0.0500 (0.0500)  time: 0.5832  data: 0.1555  max mem: 15572
Epoch: [36]  [ 330/2809]  eta: 0:24:40  lr: 0.000001  min_lr: 0.000000  loss: 3.8188 (3.7620)  class_acc: 0.2917 (0.3298)  loss_scale: 32768.0000 (50389.4622)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1642  max mem: 15572
Epoch: [36]  [ 340/2809]  eta: 0:24:32  lr: 0.000001  min_lr: 0.000000  loss: 3.8640 (3.7648)  class_acc: 0.2500 (0.3286)  loss_scale: 32768.0000 (49872.7038)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0965  max mem: 15572
[2025-01-16 07:28:47,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:28:47,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 350/2809]  eta: 0:24:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7941 (3.7634)  class_acc: 0.2917 (0.3285)  loss_scale: 32768.0000 (50038.8832)  weight_decay: 0.0500 (0.0500)  time: 0.5814  data: 0.1429  max mem: 15572
Epoch: [36]  [ 360/2809]  eta: 0:24:26  lr: 0.000001  min_lr: 0.000000  loss: 3.9303 (3.7713)  class_acc: 0.2917 (0.3272)  loss_scale: 65536.0000 (50468.1662)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.1816  max mem: 15572
Epoch: [36]  [ 370/2809]  eta: 0:24:12  lr: 0.000001  min_lr: 0.000000  loss: 3.9303 (3.7727)  class_acc: 0.2500 (0.3273)  loss_scale: 65536.0000 (50874.3073)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1288  max mem: 15572
Epoch: [36]  [ 380/2809]  eta: 0:24:03  lr: 0.000001  min_lr: 0.000000  loss: 3.8675 (3.7789)  class_acc: 0.2500 (0.3260)  loss_scale: 65536.0000 (51259.1286)  weight_decay: 0.0500 (0.0500)  time: 0.5140  data: 0.0687  max mem: 15572
Epoch: [36]  [ 390/2809]  eta: 0:23:56  lr: 0.000001  min_lr: 0.000000  loss: 3.9841 (3.7820)  class_acc: 0.2917 (0.3256)  loss_scale: 65536.0000 (51624.2660)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0953  max mem: 15572
Epoch: [36]  [ 400/2809]  eta: 0:23:46  lr: 0.000001  min_lr: 0.000000  loss: 3.7513 (3.7785)  class_acc: 0.3333 (0.3275)  loss_scale: 65536.0000 (51971.1920)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.0870  max mem: 15572
Epoch: [36]  [ 410/2809]  eta: 0:23:38  lr: 0.000001  min_lr: 0.000000  loss: 3.5910 (3.7776)  class_acc: 0.3750 (0.3270)  loss_scale: 65536.0000 (52301.2360)  weight_decay: 0.0500 (0.0500)  time: 0.5457  data: 0.1088  max mem: 15572
Epoch: [36]  [ 420/2809]  eta: 0:23:32  lr: 0.000001  min_lr: 0.000000  loss: 3.7998 (3.7785)  class_acc: 0.3333 (0.3259)  loss_scale: 65536.0000 (52615.6010)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.1435  max mem: 15572
Epoch: [36]  [ 430/2809]  eta: 0:23:21  lr: 0.000001  min_lr: 0.000000  loss: 3.6373 (3.7766)  class_acc: 0.3750 (0.3265)  loss_scale: 65536.0000 (52915.3782)  weight_decay: 0.0500 (0.0500)  time: 0.5448  data: 0.1047  max mem: 15572
Epoch: [36]  [ 440/2809]  eta: 0:23:14  lr: 0.000001  min_lr: 0.000000  loss: 3.6228 (3.7726)  class_acc: 0.3750 (0.3275)  loss_scale: 65536.0000 (53201.5601)  weight_decay: 0.0500 (0.0500)  time: 0.5356  data: 0.1047  max mem: 15572
Epoch: [36]  [ 450/2809]  eta: 0:23:06  lr: 0.000001  min_lr: 0.000000  loss: 3.6497 (3.7678)  class_acc: 0.3750 (0.3294)  loss_scale: 65536.0000 (53475.0510)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1259  max mem: 15572
Epoch: [36]  [ 460/2809]  eta: 0:22:57  lr: 0.000001  min_lr: 0.000000  loss: 3.7700 (3.7684)  class_acc: 0.3750 (0.3297)  loss_scale: 65536.0000 (53736.6768)  weight_decay: 0.0500 (0.0500)  time: 0.5358  data: 0.1068  max mem: 15572
Epoch: [36]  [ 470/2809]  eta: 0:22:53  lr: 0.000001  min_lr: 0.000000  loss: 4.0052 (3.7697)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (53987.1932)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1419  max mem: 15572
[2025-01-16 07:30:00,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:30:00,051] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:30:03,676] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101603
[2025-01-16 07:30:03,676] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:30:03,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 480/2809]  eta: 0:22:42  lr: 0.000001  min_lr: 0.000000  loss: 3.7932 (3.7674)  class_acc: 0.3333 (0.3300)  loss_scale: 65536.0000 (55181.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5516  data: 0.1138  max mem: 15572
Epoch: [36]  [ 490/2809]  eta: 0:22:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7882 (3.7670)  class_acc: 0.3333 (0.3299)  loss_scale: 65536.0000 (55391.9348)  weight_decay: 0.0500 (0.0500)  time: 0.5130  data: 0.0738  max mem: 15572
Epoch: [36]  [ 500/2809]  eta: 0:22:31  lr: 0.000001  min_lr: 0.000000  loss: 3.6688 (3.7611)  class_acc: 0.3750 (0.3319)  loss_scale: 65536.0000 (55594.4112)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.1553  max mem: 15572
Epoch: [36]  [ 510/2809]  eta: 0:22:25  lr: 0.000001  min_lr: 0.000000  loss: 3.9002 (3.7653)  class_acc: 0.2917 (0.3303)  loss_scale: 65536.0000 (55788.9628)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1953  max mem: 15572
Epoch: [36]  [ 520/2809]  eta: 0:22:16  lr: 0.000001  min_lr: 0.000000  loss: 3.7771 (3.7630)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (55976.0461)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1295  max mem: 15572
Epoch: [36]  [ 530/2809]  eta: 0:22:08  lr: 0.000001  min_lr: 0.000000  loss: 3.7785 (3.7675)  class_acc: 0.3750 (0.3309)  loss_scale: 65536.0000 (56156.0829)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0937  max mem: 15572
Epoch: [36]  [ 540/2809]  eta: 0:22:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7785 (3.7623)  class_acc: 0.3333 (0.3317)  loss_scale: 65536.0000 (56329.4640)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0987  max mem: 15572
Epoch: [36]  [ 550/2809]  eta: 0:21:54  lr: 0.000001  min_lr: 0.000000  loss: 3.4661 (3.7579)  class_acc: 0.3750 (0.3331)  loss_scale: 65536.0000 (56496.5517)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.0981  max mem: 15572
Epoch: [36]  [ 560/2809]  eta: 0:21:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7637 (3.7593)  class_acc: 0.3750 (0.3330)  loss_scale: 65536.0000 (56657.6827)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1199  max mem: 15572
Epoch: [36]  [ 570/2809]  eta: 0:21:41  lr: 0.000001  min_lr: 0.000000  loss: 3.9567 (3.7622)  class_acc: 0.2917 (0.3325)  loss_scale: 65536.0000 (56813.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1128  max mem: 15572
Epoch: [36]  [ 580/2809]  eta: 0:21:37  lr: 0.000001  min_lr: 0.000000  loss: 3.9102 (3.7622)  class_acc: 0.2917 (0.3321)  loss_scale: 65536.0000 (56963.3046)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1272  max mem: 15572
Epoch: [36]  [ 590/2809]  eta: 0:21:26  lr: 0.000001  min_lr: 0.000000  loss: 3.8419 (3.7615)  class_acc: 0.2917 (0.3315)  loss_scale: 65536.0000 (57108.3587)  weight_decay: 0.0500 (0.0500)  time: 0.5290  data: 0.0929  max mem: 15572
Epoch: [36]  [ 600/2809]  eta: 0:21:22  lr: 0.000001  min_lr: 0.000000  loss: 3.8730 (3.7645)  class_acc: 0.2500 (0.3306)  loss_scale: 65536.0000 (57248.5857)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.1151  max mem: 15572
[2025-01-16 07:31:16,296] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:31:16,297] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 610/2809]  eta: 0:21:19  lr: 0.000001  min_lr: 0.000000  loss: 3.9199 (3.7638)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (57706.0033)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.2290  max mem: 15572
[2025-01-16 07:31:19,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101736
[2025-01-16 07:31:19,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:31:19,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 620/2809]  eta: 0:21:13  lr: 0.000001  min_lr: 0.000000  loss: 3.8287 (3.7634)  class_acc: 0.2917 (0.3307)  loss_scale: 65536.0000 (57937.6232)  weight_decay: 0.0500 (0.0500)  time: 0.6199  data: 0.1930  max mem: 15572
Epoch: [36]  [ 630/2809]  eta: 0:21:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7521 (3.7626)  class_acc: 0.3333 (0.3316)  loss_scale: 65536.0000 (58058.0412)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1559  max mem: 15572
Epoch: [36]  [ 640/2809]  eta: 0:20:59  lr: 0.000001  min_lr: 0.000000  loss: 3.6225 (3.7604)  class_acc: 0.3750 (0.3320)  loss_scale: 65536.0000 (58174.7020)  weight_decay: 0.0500 (0.0500)  time: 0.5397  data: 0.1012  max mem: 15572
Epoch: [36]  [ 650/2809]  eta: 0:20:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7059 (3.7626)  class_acc: 0.3333 (0.3313)  loss_scale: 65536.0000 (58287.7788)  weight_decay: 0.0500 (0.0500)  time: 0.4770  data: 0.0523  max mem: 15572
Epoch: [36]  [ 660/2809]  eta: 0:20:48  lr: 0.000001  min_lr: 0.000000  loss: 3.9545 (3.7614)  class_acc: 0.2917 (0.3313)  loss_scale: 65536.0000 (58397.4342)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1786  max mem: 15572
Epoch: [36]  [ 670/2809]  eta: 0:20:42  lr: 0.000001  min_lr: 0.000000  loss: 3.7934 (3.7622)  class_acc: 0.3333 (0.3311)  loss_scale: 65536.0000 (58503.8212)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.2054  max mem: 15572
Epoch: [36]  [ 680/2809]  eta: 0:20:36  lr: 0.000001  min_lr: 0.000000  loss: 3.6874 (3.7591)  class_acc: 0.2917 (0.3316)  loss_scale: 65536.0000 (58607.0837)  weight_decay: 0.0500 (0.0500)  time: 0.5671  data: 0.1208  max mem: 15572
Epoch: [36]  [ 690/2809]  eta: 0:20:29  lr: 0.000001  min_lr: 0.000000  loss: 3.8363 (3.7586)  class_acc: 0.3333 (0.3324)  loss_scale: 65536.0000 (58707.3575)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.1135  max mem: 15572
Epoch: [36]  [ 700/2809]  eta: 0:20:20  lr: 0.000001  min_lr: 0.000000  loss: 3.7773 (3.7558)  class_acc: 0.3333 (0.3328)  loss_scale: 65536.0000 (58804.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0737  max mem: 15572
Epoch: [36]  [ 710/2809]  eta: 0:20:14  lr: 0.000001  min_lr: 0.000000  loss: 3.5240 (3.7519)  class_acc: 0.3333 (0.3332)  loss_scale: 65536.0000 (58899.4430)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0877  max mem: 15572
Epoch: [36]  [ 720/2809]  eta: 0:20:07  lr: 0.000001  min_lr: 0.000000  loss: 3.4654 (3.7493)  class_acc: 0.3750 (0.3334)  loss_scale: 65536.0000 (58991.4896)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1197  max mem: 15572
Epoch: [36]  [ 730/2809]  eta: 0:20:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5163 (3.7487)  class_acc: 0.3333 (0.3329)  loss_scale: 65536.0000 (59081.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1141  max mem: 15572
Epoch: [36]  [ 740/2809]  eta: 0:19:53  lr: 0.000001  min_lr: 0.000000  loss: 3.5553 (3.7466)  class_acc: 0.3750 (0.3338)  loss_scale: 65536.0000 (59168.1296)  weight_decay: 0.0500 (0.0500)  time: 0.5368  data: 0.1015  max mem: 15572
[2025-01-16 07:32:30,651] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:32:30,651] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:32:31,852] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101868
[2025-01-16 07:32:31,853] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:32:31,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 750/2809]  eta: 0:19:48  lr: 0.000001  min_lr: 0.000000  loss: 3.7244 (3.7483)  class_acc: 0.3750 (0.3334)  loss_scale: 65536.0000 (59514.7164)  weight_decay: 0.0500 (0.0500)  time: 0.5569  data: 0.1255  max mem: 15572
Epoch: [36]  [ 760/2809]  eta: 0:19:42  lr: 0.000001  min_lr: 0.000000  loss: 3.8440 (3.7495)  class_acc: 0.2917 (0.3336)  loss_scale: 65536.0000 (59593.8397)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1469  max mem: 15572
Epoch: [36]  [ 770/2809]  eta: 0:19:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8368 (3.7503)  class_acc: 0.4167 (0.3342)  loss_scale: 65536.0000 (59670.9105)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1300  max mem: 15572
Epoch: [36]  [ 780/2809]  eta: 0:19:29  lr: 0.000001  min_lr: 0.000000  loss: 3.8244 (3.7508)  class_acc: 0.3333 (0.3334)  loss_scale: 65536.0000 (59746.0077)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1213  max mem: 15572
Epoch: [36]  [ 790/2809]  eta: 0:19:24  lr: 0.000001  min_lr: 0.000000  loss: 3.8375 (3.7549)  class_acc: 0.2500 (0.3329)  loss_scale: 65536.0000 (59819.2061)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1206  max mem: 15572
Epoch: [36]  [ 800/2809]  eta: 0:19:17  lr: 0.000001  min_lr: 0.000000  loss: 3.7873 (3.7521)  class_acc: 0.3333 (0.3336)  loss_scale: 65536.0000 (59890.5768)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1162  max mem: 15572
Epoch: [36]  [ 810/2809]  eta: 0:19:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7873 (3.7536)  class_acc: 0.2917 (0.3328)  loss_scale: 65536.0000 (59960.1874)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0869  max mem: 15572
Epoch: [36]  [ 820/2809]  eta: 0:19:04  lr: 0.000001  min_lr: 0.000000  loss: 3.8508 (3.7536)  class_acc: 0.2917 (0.3330)  loss_scale: 65536.0000 (60028.1023)  weight_decay: 0.0500 (0.0500)  time: 0.5474  data: 0.1147  max mem: 15572
[2025-01-16 07:33:18,735] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 101950
[2025-01-16 07:33:18,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:33:18,735] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 830/2809]  eta: 0:18:58  lr: 0.000001  min_lr: 0.000000  loss: 3.7706 (3.7515)  class_acc: 0.3333 (0.3333)  loss_scale: 65536.0000 (59897.2226)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1333  max mem: 15572
Epoch: [36]  [ 840/2809]  eta: 0:18:51  lr: 0.000001  min_lr: 0.000000  loss: 3.7925 (3.7525)  class_acc: 0.2917 (0.3330)  loss_scale: 32768.0000 (59574.6397)  weight_decay: 0.0500 (0.0500)  time: 0.5488  data: 0.1019  max mem: 15572
Epoch: [36]  [ 850/2809]  eta: 0:18:43  lr: 0.000001  min_lr: 0.000000  loss: 3.8044 (3.7532)  class_acc: 0.2917 (0.3330)  loss_scale: 32768.0000 (59259.6381)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0540  max mem: 15572
Epoch: [36]  [ 860/2809]  eta: 0:18:38  lr: 0.000001  min_lr: 0.000000  loss: 3.8935 (3.7560)  class_acc: 0.2917 (0.3325)  loss_scale: 32768.0000 (58951.9535)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.0989  max mem: 15572
Epoch: [36]  [ 870/2809]  eta: 0:18:31  lr: 0.000001  min_lr: 0.000000  loss: 3.8690 (3.7564)  class_acc: 0.2917 (0.3324)  loss_scale: 32768.0000 (58651.3341)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.1251  max mem: 15572
[2025-01-16 07:33:44,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=102000, skipped=678, lr=[1.2774264980564707e-08, 1.2774264980564707e-08, 1.82489499722353e-08, 1.82489499722353e-08, 2.6069928531764718e-08, 2.6069928531764718e-08, 3.724275504537817e-08, 3.724275504537817e-08, 5.320393577911167e-08, 5.320393577911167e-08, 7.60056225415881e-08, 7.60056225415881e-08, 1.085794607736973e-07, 1.085794607736973e-07, 1.5511351539099616e-07, 1.5511351539099616e-07, 2.2159073627285164e-07, 2.2159073627285164e-07, 3.165581946755024e-07, 3.165581946755024e-07, 4.5222599239357485e-07, 4.5222599239357485e-07, 6.460371319908213e-07, 6.460371319908213e-07, 9.229101885583161e-07, 9.229101885583161e-07, 1.3184431265118803e-06, 1.3184431265118803e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:33:44,444] [INFO] [timer.py:260:stop] epoch=0/micro_step=102000/global_step=102000, RunningAvgSamplesPerSec=28.57656820302233, CurrSamplesPerSec=31.254535733513663, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [ 880/2809]  eta: 0:18:25  lr: 0.000001  min_lr: 0.000000  loss: 3.7876 (3.7564)  class_acc: 0.2917 (0.3321)  loss_scale: 32768.0000 (58357.5392)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0941  max mem: 15572
Epoch: [36]  [ 890/2809]  eta: 0:18:21  lr: 0.000001  min_lr: 0.000000  loss: 3.7094 (3.7565)  class_acc: 0.2917 (0.3323)  loss_scale: 32768.0000 (58070.3389)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1653  max mem: 15572
Epoch: [36]  [ 900/2809]  eta: 0:18:16  lr: 0.000001  min_lr: 0.000000  loss: 3.6938 (3.7543)  class_acc: 0.3750 (0.3326)  loss_scale: 32768.0000 (57789.5139)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.1928  max mem: 15572
Epoch: [36]  [ 910/2809]  eta: 0:18:08  lr: 0.000001  min_lr: 0.000000  loss: 3.8692 (3.7535)  class_acc: 0.3333 (0.3326)  loss_scale: 32768.0000 (57514.8540)  weight_decay: 0.0500 (0.0500)  time: 0.5385  data: 0.1109  max mem: 15572
Epoch: [36]  [ 920/2809]  eta: 0:18:01  lr: 0.000001  min_lr: 0.000000  loss: 3.6614 (3.7515)  class_acc: 0.3333 (0.3331)  loss_scale: 32768.0000 (57246.1585)  weight_decay: 0.0500 (0.0500)  time: 0.5005  data: 0.0666  max mem: 15572
Epoch: [36]  [ 930/2809]  eta: 0:17:55  lr: 0.000001  min_lr: 0.000000  loss: 3.8596 (3.7540)  class_acc: 0.2917 (0.3324)  loss_scale: 32768.0000 (56983.2352)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.1018  max mem: 15572
Epoch: [36]  [ 940/2809]  eta: 0:17:50  lr: 0.000001  min_lr: 0.000000  loss: 3.8596 (3.7534)  class_acc: 0.3333 (0.3326)  loss_scale: 32768.0000 (56725.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1618  max mem: 15572
Epoch: [36]  [ 950/2809]  eta: 0:17:44  lr: 0.000001  min_lr: 0.000000  loss: 3.5383 (3.7493)  class_acc: 0.3750 (0.3331)  loss_scale: 32768.0000 (56473.9769)  weight_decay: 0.0500 (0.0500)  time: 0.5908  data: 0.1544  max mem: 15572
[2025-01-16 07:34:29,948] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:34:29,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 960/2809]  eta: 0:17:41  lr: 0.000001  min_lr: 0.000000  loss: 3.5069 (3.7498)  class_acc: 0.3333 (0.3330)  loss_scale: 32768.0000 (56431.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1864  max mem: 15572
Epoch: [36]  [ 970/2809]  eta: 0:17:34  lr: 0.000001  min_lr: 0.000000  loss: 3.8072 (3.7505)  class_acc: 0.2917 (0.3329)  loss_scale: 65536.0000 (56525.6437)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1734  max mem: 15572
Epoch: [36]  [ 980/2809]  eta: 0:17:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7273 (3.7472)  class_acc: 0.3333 (0.3341)  loss_scale: 65536.0000 (56617.4924)  weight_decay: 0.0500 (0.0500)  time: 0.5391  data: 0.1107  max mem: 15572
[2025-01-16 07:34:46,611] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102107
[2025-01-16 07:34:46,611] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:34:46,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [ 990/2809]  eta: 0:17:22  lr: 0.000001  min_lr: 0.000000  loss: 3.4133 (3.7450)  class_acc: 0.3750 (0.3341)  loss_scale: 65536.0000 (56442.9627)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1363  max mem: 15572
Epoch: [36]  [1000/2809]  eta: 0:17:18  lr: 0.000001  min_lr: 0.000000  loss: 3.7365 (3.7464)  class_acc: 0.3333 (0.3339)  loss_scale: 32768.0000 (56206.4496)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.1750  max mem: 15572
Epoch: [36]  [1010/2809]  eta: 0:17:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7868 (3.7450)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (55974.6152)  weight_decay: 0.0500 (0.0500)  time: 0.5462  data: 0.1197  max mem: 15572
Epoch: [36]  [1020/2809]  eta: 0:17:04  lr: 0.000001  min_lr: 0.000000  loss: 4.0634 (3.7486)  class_acc: 0.2500 (0.3336)  loss_scale: 32768.0000 (55747.3222)  weight_decay: 0.0500 (0.0500)  time: 0.5231  data: 0.0921  max mem: 15572
Epoch: [36]  [1030/2809]  eta: 0:16:58  lr: 0.000001  min_lr: 0.000000  loss: 3.8557 (3.7481)  class_acc: 0.2500 (0.3338)  loss_scale: 32768.0000 (55524.4384)  weight_decay: 0.0500 (0.0500)  time: 0.5662  data: 0.1218  max mem: 15572
Epoch: [36]  [1040/2809]  eta: 0:16:53  lr: 0.000001  min_lr: 0.000000  loss: 3.7036 (3.7466)  class_acc: 0.3750 (0.3343)  loss_scale: 32768.0000 (55305.8367)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1108  max mem: 15572
Epoch: [36]  [1050/2809]  eta: 0:16:47  lr: 0.000001  min_lr: 0.000000  loss: 3.8759 (3.7485)  class_acc: 0.2917 (0.3338)  loss_scale: 32768.0000 (55091.3949)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1345  max mem: 15572
Epoch: [36]  [1060/2809]  eta: 0:16:41  lr: 0.000001  min_lr: 0.000000  loss: 3.8157 (3.7468)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (54880.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1248  max mem: 15572
Epoch: [36]  [1070/2809]  eta: 0:16:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6157 (3.7463)  class_acc: 0.3333 (0.3344)  loss_scale: 32768.0000 (54674.5247)  weight_decay: 0.0500 (0.0500)  time: 0.5525  data: 0.1152  max mem: 15572
Epoch: [36]  [1080/2809]  eta: 0:16:30  lr: 0.000001  min_lr: 0.000000  loss: 3.7156 (3.7459)  class_acc: 0.3333 (0.3343)  loss_scale: 32768.0000 (54471.8742)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1347  max mem: 15572
Epoch: [36]  [1090/2809]  eta: 0:16:22  lr: 0.000001  min_lr: 0.000000  loss: 3.7025 (3.7440)  class_acc: 0.3333 (0.3351)  loss_scale: 32768.0000 (54272.9386)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0961  max mem: 15572
Epoch: [36]  [1100/2809]  eta: 0:16:16  lr: 0.000001  min_lr: 0.000000  loss: 3.7742 (3.7451)  class_acc: 0.3333 (0.3346)  loss_scale: 32768.0000 (54077.6167)  weight_decay: 0.0500 (0.0500)  time: 0.5040  data: 0.0616  max mem: 15572
Epoch: [36]  [1110/2809]  eta: 0:16:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7150 (3.7427)  class_acc: 0.3333 (0.3354)  loss_scale: 32768.0000 (53885.8110)  weight_decay: 0.0500 (0.0500)  time: 0.5362  data: 0.0979  max mem: 15572
[2025-01-16 07:35:58,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:35:58,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1120/2809]  eta: 0:16:04  lr: 0.000001  min_lr: 0.000000  loss: 3.5488 (3.7433)  class_acc: 0.3333 (0.3352)  loss_scale: 32768.0000 (53960.5067)  weight_decay: 0.0500 (0.0500)  time: 0.5512  data: 0.1229  max mem: 15572
Epoch: [36]  [1130/2809]  eta: 0:15:59  lr: 0.000001  min_lr: 0.000000  loss: 3.6473 (3.7416)  class_acc: 0.3750 (0.3359)  loss_scale: 65536.0000 (54062.8541)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1667  max mem: 15572
Epoch: [36]  [1140/2809]  eta: 0:15:54  lr: 0.000001  min_lr: 0.000000  loss: 3.6473 (3.7428)  class_acc: 0.3750 (0.3362)  loss_scale: 65536.0000 (54163.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.1670  max mem: 15572
Epoch: [36]  [1150/2809]  eta: 0:15:49  lr: 0.000001  min_lr: 0.000000  loss: 3.9220 (3.7434)  class_acc: 0.2917 (0.3358)  loss_scale: 65536.0000 (54262.2137)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1706  max mem: 15572
Epoch: [36]  [1160/2809]  eta: 0:15:43  lr: 0.000001  min_lr: 0.000000  loss: 3.9638 (3.7443)  class_acc: 0.2917 (0.3359)  loss_scale: 65536.0000 (54359.3178)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1506  max mem: 15572
Epoch: [36]  [1170/2809]  eta: 0:15:37  lr: 0.000001  min_lr: 0.000000  loss: 3.7892 (3.7445)  class_acc: 0.3333 (0.3361)  loss_scale: 65536.0000 (54454.7635)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.1123  max mem: 15572
[2025-01-16 07:36:38,085] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102304
[2025-01-16 07:36:38,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:36:38,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1180/2809]  eta: 0:15:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7892 (3.7452)  class_acc: 0.3333 (0.3358)  loss_scale: 65536.0000 (54520.8467)  weight_decay: 0.0500 (0.0500)  time: 0.5526  data: 0.1247  max mem: 15572
Epoch: [36]  [1190/2809]  eta: 0:15:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7448 (3.7440)  class_acc: 0.3750 (0.3366)  loss_scale: 32768.0000 (54338.2032)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1633  max mem: 15572
Epoch: [36]  [1200/2809]  eta: 0:15:20  lr: 0.000001  min_lr: 0.000000  loss: 3.7467 (3.7443)  class_acc: 0.3750 (0.3368)  loss_scale: 32768.0000 (54158.6012)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1284  max mem: 15572
Epoch: [36]  [1210/2809]  eta: 0:15:14  lr: 0.000001  min_lr: 0.000000  loss: 3.7467 (3.7425)  class_acc: 0.3750 (0.3371)  loss_scale: 32768.0000 (53981.9653)  weight_decay: 0.0500 (0.0500)  time: 0.5444  data: 0.0980  max mem: 15572
Epoch: [36]  [1220/2809]  eta: 0:15:08  lr: 0.000001  min_lr: 0.000000  loss: 3.7143 (3.7434)  class_acc: 0.3333 (0.3370)  loss_scale: 32768.0000 (53808.2228)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1326  max mem: 15572
Epoch: [36]  [1230/2809]  eta: 0:15:03  lr: 0.000001  min_lr: 0.000000  loss: 4.0201 (3.7444)  class_acc: 0.2500 (0.3367)  loss_scale: 32768.0000 (53637.3030)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1322  max mem: 15572
Epoch: [36]  [1240/2809]  eta: 0:14:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6646 (3.7428)  class_acc: 0.3750 (0.3374)  loss_scale: 32768.0000 (53469.1378)  weight_decay: 0.0500 (0.0500)  time: 0.5641  data: 0.1225  max mem: 15572
Epoch: [36]  [1250/2809]  eta: 0:14:50  lr: 0.000001  min_lr: 0.000000  loss: 3.6055 (3.7419)  class_acc: 0.4167 (0.3375)  loss_scale: 32768.0000 (53303.6611)  weight_decay: 0.0500 (0.0500)  time: 0.5252  data: 0.0836  max mem: 15572
Epoch: [36]  [1260/2809]  eta: 0:14:45  lr: 0.000001  min_lr: 0.000000  loss: 3.8467 (3.7438)  class_acc: 0.2917 (0.3371)  loss_scale: 32768.0000 (53140.8089)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1193  max mem: 15572
Epoch: [36]  [1270/2809]  eta: 0:14:39  lr: 0.000001  min_lr: 0.000000  loss: 3.8812 (3.7420)  class_acc: 0.3333 (0.3380)  loss_scale: 32768.0000 (52980.5193)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.1569  max mem: 15572
Epoch: [36]  [1280/2809]  eta: 0:14:33  lr: 0.000001  min_lr: 0.000000  loss: 3.4962 (3.7389)  class_acc: 0.3750 (0.3385)  loss_scale: 32768.0000 (52822.7322)  weight_decay: 0.0500 (0.0500)  time: 0.5727  data: 0.1219  max mem: 15572
Epoch: [36]  [1290/2809]  eta: 0:14:28  lr: 0.000001  min_lr: 0.000000  loss: 3.6748 (3.7397)  class_acc: 0.3333 (0.3386)  loss_scale: 32768.0000 (52667.3896)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1077  max mem: 15572
Epoch: [36]  [1300/2809]  eta: 0:14:21  lr: 0.000001  min_lr: 0.000000  loss: 3.8544 (3.7408)  class_acc: 0.2917 (0.3380)  loss_scale: 32768.0000 (52514.4350)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0894  max mem: 15572
[2025-01-16 07:37:51,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:37:51,974] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1310/2809]  eta: 0:14:16  lr: 0.000001  min_lr: 0.000000  loss: 3.7649 (3.7386)  class_acc: 0.2917 (0.3384)  loss_scale: 32768.0000 (52413.8032)  weight_decay: 0.0500 (0.0500)  time: 0.5790  data: 0.1345  max mem: 15572
Epoch: [36]  [1320/2809]  eta: 0:14:11  lr: 0.000001  min_lr: 0.000000  loss: 3.7414 (3.7375)  class_acc: 0.3750 (0.3385)  loss_scale: 65536.0000 (52513.1385)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.1821  max mem: 15572
Epoch: [36]  [1330/2809]  eta: 0:14:06  lr: 0.000001  min_lr: 0.000000  loss: 3.6788 (3.7360)  class_acc: 0.3750 (0.3391)  loss_scale: 65536.0000 (52610.9812)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.1816  max mem: 15572
Epoch: [36]  [1340/2809]  eta: 0:14:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7163 (3.7366)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (52707.3647)  weight_decay: 0.0500 (0.0500)  time: 0.6210  data: 0.1796  max mem: 15572
Epoch: [36]  [1350/2809]  eta: 0:13:54  lr: 0.000001  min_lr: 0.000000  loss: 3.8364 (3.7372)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (52802.3212)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1002  max mem: 15572
Epoch: [36]  [1360/2809]  eta: 0:13:48  lr: 0.000001  min_lr: 0.000000  loss: 3.9734 (3.7380)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (52895.8824)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0734  max mem: 15572
Epoch: [36]  [1370/2809]  eta: 0:13:42  lr: 0.000001  min_lr: 0.000000  loss: 3.9587 (3.7384)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (52988.0788)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0787  max mem: 15572
Epoch: [36]  [1380/2809]  eta: 0:13:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8567 (3.7396)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (53078.9399)  weight_decay: 0.0500 (0.0500)  time: 0.5195  data: 0.0823  max mem: 15572
Epoch: [36]  [1390/2809]  eta: 0:13:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7362 (3.7385)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (53168.4946)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0943  max mem: 15572
Epoch: [36]  [1400/2809]  eta: 0:13:23  lr: 0.000001  min_lr: 0.000000  loss: 3.5687 (3.7384)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53256.7709)  weight_decay: 0.0500 (0.0500)  time: 0.5352  data: 0.0942  max mem: 15572
Epoch: [36]  [1410/2809]  eta: 0:13:17  lr: 0.000001  min_lr: 0.000000  loss: 3.8265 (3.7396)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (53343.7959)  weight_decay: 0.0500 (0.0500)  time: 0.5033  data: 0.0538  max mem: 15572
Epoch: [36]  [1420/2809]  eta: 0:13:10  lr: 0.000001  min_lr: 0.000000  loss: 3.8169 (3.7395)  class_acc: 0.2917 (0.3386)  loss_scale: 65536.0000 (53429.5961)  weight_decay: 0.0500 (0.0500)  time: 0.4863  data: 0.0286  max mem: 15572
Epoch: [36]  [1430/2809]  eta: 0:13:04  lr: 0.000001  min_lr: 0.000000  loss: 3.6937 (3.7396)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (53514.1971)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0777  max mem: 15572
[2025-01-16 07:39:02,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:39:02,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1440/2809]  eta: 0:12:59  lr: 0.000001  min_lr: 0.000000  loss: 3.8971 (3.7403)  class_acc: 0.3333 (0.3383)  loss_scale: 65536.0000 (53779.5420)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.1310  max mem: 15572
[2025-01-16 07:39:04,354] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102565
[2025-01-16 07:39:04,354] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:39:04,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1450/2809]  eta: 0:12:52  lr: 0.000001  min_lr: 0.000000  loss: 3.8971 (3.7399)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53860.5651)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0819  max mem: 15572
Epoch: [36]  [1460/2809]  eta: 0:12:47  lr: 0.000001  min_lr: 0.000000  loss: 3.8817 (3.7400)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (53940.4791)  weight_decay: 0.0500 (0.0500)  time: 0.5257  data: 0.0821  max mem: 15572
Epoch: [36]  [1470/2809]  eta: 0:12:42  lr: 0.000001  min_lr: 0.000000  loss: 3.7497 (3.7401)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (54019.3066)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.1813  max mem: 15572
[2025-01-16 07:39:22,151] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102597
[2025-01-16 07:39:22,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:39:22,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [1480/2809]  eta: 0:12:36  lr: 0.000001  min_lr: 0.000000  loss: 3.6571 (3.7397)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (53920.0648)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.1597  max mem: 15572
Epoch: [36]  [1490/2809]  eta: 0:12:31  lr: 0.000001  min_lr: 0.000000  loss: 3.6270 (3.7380)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (53778.1999)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1468  max mem: 15572
Epoch: [36]  [1500/2809]  eta: 0:12:25  lr: 0.000001  min_lr: 0.000000  loss: 3.9079 (3.7394)  class_acc: 0.3333 (0.3394)  loss_scale: 32768.0000 (53638.2252)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1062  max mem: 15572
Epoch: [36]  [1510/2809]  eta: 0:12:19  lr: 0.000001  min_lr: 0.000000  loss: 3.7469 (3.7390)  class_acc: 0.3333 (0.3396)  loss_scale: 32768.0000 (53500.1032)  weight_decay: 0.0500 (0.0500)  time: 0.5225  data: 0.0950  max mem: 15572
Epoch: [36]  [1520/2809]  eta: 0:12:13  lr: 0.000001  min_lr: 0.000000  loss: 3.6705 (3.7394)  class_acc: 0.3333 (0.3393)  loss_scale: 32768.0000 (53363.7975)  weight_decay: 0.0500 (0.0500)  time: 0.5632  data: 0.1498  max mem: 15572
Epoch: [36]  [1530/2809]  eta: 0:12:07  lr: 0.000001  min_lr: 0.000000  loss: 3.6793 (3.7383)  class_acc: 0.3333 (0.3395)  loss_scale: 32768.0000 (53229.2724)  weight_decay: 0.0500 (0.0500)  time: 0.5288  data: 0.0941  max mem: 15572
Epoch: [36]  [1540/2809]  eta: 0:12:01  lr: 0.000001  min_lr: 0.000000  loss: 3.4828 (3.7370)  class_acc: 0.3750 (0.3396)  loss_scale: 32768.0000 (53096.4932)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0418  max mem: 15572
Epoch: [36]  [1550/2809]  eta: 0:11:56  lr: 0.000001  min_lr: 0.000000  loss: 3.6481 (3.7371)  class_acc: 0.3333 (0.3396)  loss_scale: 32768.0000 (52965.4262)  weight_decay: 0.0500 (0.0500)  time: 0.5626  data: 0.1030  max mem: 15572
Epoch: [36]  [1560/2809]  eta: 0:11:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7631 (3.7381)  class_acc: 0.3333 (0.3393)  loss_scale: 32768.0000 (52836.0384)  weight_decay: 0.0500 (0.0500)  time: 0.5674  data: 0.1406  max mem: 15572
Epoch: [36]  [1570/2809]  eta: 0:11:45  lr: 0.000001  min_lr: 0.000000  loss: 3.8616 (3.7381)  class_acc: 0.3333 (0.3390)  loss_scale: 32768.0000 (52708.2979)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.1773  max mem: 15572
Epoch: [36]  [1580/2809]  eta: 0:11:39  lr: 0.000001  min_lr: 0.000000  loss: 3.7556 (3.7383)  class_acc: 0.3333 (0.3389)  loss_scale: 32768.0000 (52582.1733)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.2006  max mem: 15572
Epoch: [36]  [1590/2809]  eta: 0:11:33  lr: 0.000001  min_lr: 0.000000  loss: 3.7833 (3.7390)  class_acc: 0.3333 (0.3388)  loss_scale: 32768.0000 (52457.6342)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.1330  max mem: 15572
Epoch: [36]  [1600/2809]  eta: 0:11:27  lr: 0.000001  min_lr: 0.000000  loss: 3.8227 (3.7398)  class_acc: 0.2917 (0.3385)  loss_scale: 32768.0000 (52334.6508)  weight_decay: 0.0500 (0.0500)  time: 0.5213  data: 0.0593  max mem: 15572
[2025-01-16 07:40:34,521] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:40:34,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [1610/2809]  eta: 0:11:21  lr: 0.000001  min_lr: 0.000000  loss: 3.8227 (3.7402)  class_acc: 0.2917 (0.3385)  loss_scale: 32768.0000 (52396.2557)  weight_decay: 0.0500 (0.0500)  time: 0.5294  data: 0.0669  max mem: 15572
Epoch: [36]  [1620/2809]  eta: 0:11:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7483 (3.7408)  class_acc: 0.3750 (0.3384)  loss_scale: 65536.0000 (52477.3152)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1283  max mem: 15572
Epoch: [36]  [1630/2809]  eta: 0:11:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7483 (3.7396)  class_acc: 0.3750 (0.3387)  loss_scale: 65536.0000 (52557.3807)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.1614  max mem: 15572
Epoch: [36]  [1640/2809]  eta: 0:11:05  lr: 0.000001  min_lr: 0.000000  loss: 3.8635 (3.7402)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (52636.4704)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1901  max mem: 15572
Epoch: [36]  [1650/2809]  eta: 0:10:59  lr: 0.000001  min_lr: 0.000000  loss: 3.8100 (3.7396)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (52714.6021)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.1759  max mem: 15572
Epoch: [36]  [1660/2809]  eta: 0:10:53  lr: 0.000001  min_lr: 0.000000  loss: 3.7697 (3.7395)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (52791.7929)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1084  max mem: 15572
Epoch: [36]  [1670/2809]  eta: 0:10:47  lr: 0.000001  min_lr: 0.000000  loss: 3.7697 (3.7395)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (52868.0598)  weight_decay: 0.0500 (0.0500)  time: 0.5004  data: 0.0416  max mem: 15572
Epoch: [36]  [1680/2809]  eta: 0:10:41  lr: 0.000001  min_lr: 0.000000  loss: 3.5577 (3.7389)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (52943.4194)  weight_decay: 0.0500 (0.0500)  time: 0.5261  data: 0.0839  max mem: 15572
Epoch: [36]  [1690/2809]  eta: 0:10:36  lr: 0.000001  min_lr: 0.000000  loss: 3.6123 (3.7395)  class_acc: 0.3333 (0.3387)  loss_scale: 65536.0000 (53017.8876)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.1730  max mem: 15572
Epoch: [36]  [1700/2809]  eta: 0:10:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7752 (3.7395)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53091.4803)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.1573  max mem: 15572
Epoch: [36]  [1710/2809]  eta: 0:10:25  lr: 0.000001  min_lr: 0.000000  loss: 3.7752 (3.7403)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (53164.2127)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.1200  max mem: 15572
Epoch: [36]  [1720/2809]  eta: 0:10:20  lr: 0.000001  min_lr: 0.000000  loss: 3.9414 (3.7410)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (53236.0999)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1686  max mem: 15572
[2025-01-16 07:41:49,886] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:41:49,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1730/2809]  eta: 0:10:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8294 (3.7409)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (53345.0168)  weight_decay: 0.0500 (0.0500)  time: 0.6382  data: 0.2040  max mem: 15572
[2025-01-16 07:41:50,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102855
[2025-01-16 07:41:50,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:41:50,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1740/2809]  eta: 0:10:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7399 (3.7406)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53415.0396)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1463  max mem: 15572
Epoch: [36]  [1750/2809]  eta: 0:10:03  lr: 0.000001  min_lr: 0.000000  loss: 3.7719 (3.7406)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (53484.2627)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1288  max mem: 15572
Epoch: [36]  [1760/2809]  eta: 0:09:57  lr: 0.000001  min_lr: 0.000000  loss: 3.7719 (3.7402)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53552.6996)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1486  max mem: 15572
Epoch: [36]  [1770/2809]  eta: 0:09:52  lr: 0.000001  min_lr: 0.000000  loss: 3.7513 (3.7404)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53620.3636)  weight_decay: 0.0500 (0.0500)  time: 0.5656  data: 0.1417  max mem: 15572
Epoch: [36]  [1780/2809]  eta: 0:09:46  lr: 0.000001  min_lr: 0.000000  loss: 3.6476 (3.7394)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (53687.2678)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.1451  max mem: 15572
Epoch: [36]  [1790/2809]  eta: 0:09:40  lr: 0.000001  min_lr: 0.000000  loss: 3.6783 (3.7399)  class_acc: 0.3333 (0.3390)  loss_scale: 65536.0000 (53753.4249)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1350  max mem: 15572
Epoch: [36]  [1800/2809]  eta: 0:09:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7699 (3.7396)  class_acc: 0.3750 (0.3392)  loss_scale: 65536.0000 (53818.8473)  weight_decay: 0.0500 (0.0500)  time: 0.5309  data: 0.0842  max mem: 15572
Epoch: [36]  [1810/2809]  eta: 0:09:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7621 (3.7399)  class_acc: 0.3750 (0.3394)  loss_scale: 65536.0000 (53883.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5054  data: 0.0516  max mem: 15572
Epoch: [36]  [1820/2809]  eta: 0:09:22  lr: 0.000001  min_lr: 0.000000  loss: 3.7621 (3.7401)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (53947.5365)  weight_decay: 0.0500 (0.0500)  time: 0.4921  data: 0.0307  max mem: 15572
Epoch: [36]  [1830/2809]  eta: 0:09:16  lr: 0.000001  min_lr: 0.000000  loss: 3.7946 (3.7405)  class_acc: 0.3333 (0.3392)  loss_scale: 65536.0000 (54010.8269)  weight_decay: 0.0500 (0.0500)  time: 0.5109  data: 0.0625  max mem: 15572
Epoch: [36]  [1840/2809]  eta: 0:09:10  lr: 0.000001  min_lr: 0.000000  loss: 3.5379 (3.7385)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (54073.4297)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1370  max mem: 15572
Epoch: [36]  [1850/2809]  eta: 0:09:05  lr: 0.000001  min_lr: 0.000000  loss: 3.7141 (3.7403)  class_acc: 0.2917 (0.3391)  loss_scale: 65536.0000 (54135.3560)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1325  max mem: 15572
[2025-01-16 07:43:00,699] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:43:00,699] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1860/2809]  eta: 0:08:59  lr: 0.000001  min_lr: 0.000000  loss: 3.8850 (3.7398)  class_acc: 0.2500 (0.3392)  loss_scale: 65536.0000 (54231.8323)  weight_decay: 0.0500 (0.0500)  time: 0.5381  data: 0.1059  max mem: 15572
[2025-01-16 07:43:02,179] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 102985
[2025-01-16 07:43:02,179] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:43:02,179] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1870/2809]  eta: 0:08:53  lr: 0.000001  min_lr: 0.000000  loss: 3.7596 (3.7404)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (54292.2501)  weight_decay: 0.0500 (0.0500)  time: 0.5770  data: 0.1549  max mem: 15572
[2025-01-16 07:43:11,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=103000, skipped=684, lr=[1.0527781531835162e-08, 1.0527781531835162e-08, 1.5039687902621662e-08, 1.5039687902621662e-08, 2.1485268432316662e-08, 2.1485268432316662e-08, 3.069324061759523e-08, 3.069324061759523e-08, 4.384748659656462e-08, 4.384748659656462e-08, 6.26392665665209e-08, 6.26392665665209e-08, 8.948466652360128e-08, 8.948466652360128e-08, 1.2783523789085898e-07, 1.2783523789085898e-07, 1.8262176841551284e-07, 1.8262176841551284e-07, 2.608882405935898e-07, 2.608882405935898e-07, 3.7269748656227115e-07, 3.7269748656227115e-07, 5.324249808032445e-07, 5.324249808032445e-07, 7.606071154332065e-07, 7.606071154332065e-07, 1.0865815934760094e-06, 1.0865815934760094e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:43:11,074] [INFO] [timer.py:260:stop] epoch=0/micro_step=103000/global_step=103000, RunningAvgSamplesPerSec=28.579809317968977, CurrSamplesPerSec=31.539697171170413, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [36]  [1880/2809]  eta: 0:08:48  lr: 0.000001  min_lr: 0.000000  loss: 3.7596 (3.7402)  class_acc: 0.2917 (0.3390)  loss_scale: 65536.0000 (54352.0255)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.2063  max mem: 15572
Epoch: [36]  [1890/2809]  eta: 0:08:42  lr: 0.000001  min_lr: 0.000000  loss: 3.5596 (3.7396)  class_acc: 0.3333 (0.3391)  loss_scale: 65536.0000 (54411.1687)  weight_decay: 0.0500 (0.0500)  time: 0.5860  data: 0.1520  max mem: 15572
Epoch: [36]  [1900/2809]  eta: 0:08:37  lr: 0.000001  min_lr: 0.000000  loss: 3.5342 (3.7385)  class_acc: 0.3333 (0.3394)  loss_scale: 65536.0000 (54469.6896)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1042  max mem: 15572
Epoch: [36]  [1910/2809]  eta: 0:08:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7687 (3.7388)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (54527.5981)  weight_decay: 0.0500 (0.0500)  time: 0.5590  data: 0.1128  max mem: 15572
Epoch: [36]  [1920/2809]  eta: 0:08:25  lr: 0.000001  min_lr: 0.000000  loss: 3.7382 (3.7385)  class_acc: 0.3333 (0.3393)  loss_scale: 65536.0000 (54584.9037)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1180  max mem: 15572
Epoch: [36]  [1930/2809]  eta: 0:08:19  lr: 0.000001  min_lr: 0.000000  loss: 3.6032 (3.7368)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (54641.6157)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1220  max mem: 15572
Epoch: [36]  [1940/2809]  eta: 0:08:14  lr: 0.000001  min_lr: 0.000000  loss: 3.5086 (3.7369)  class_acc: 0.3750 (0.3398)  loss_scale: 65536.0000 (54697.7434)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1373  max mem: 15572
Epoch: [36]  [1950/2809]  eta: 0:08:08  lr: 0.000001  min_lr: 0.000000  loss: 3.6524 (3.7365)  class_acc: 0.3750 (0.3400)  loss_scale: 65536.0000 (54753.2957)  weight_decay: 0.0500 (0.0500)  time: 0.5278  data: 0.0854  max mem: 15572
Epoch: [36]  [1960/2809]  eta: 0:08:03  lr: 0.000001  min_lr: 0.000000  loss: 3.6631 (3.7363)  class_acc: 0.3750 (0.3400)  loss_scale: 65536.0000 (54808.2815)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1398  max mem: 15572
Epoch: [36]  [1970/2809]  eta: 0:07:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6457 (3.7354)  class_acc: 0.3750 (0.3402)  loss_scale: 65536.0000 (54862.7093)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2081  max mem: 15572
Epoch: [36]  [1980/2809]  eta: 0:07:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6527 (3.7351)  class_acc: 0.3750 (0.3401)  loss_scale: 65536.0000 (54916.5876)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1689  max mem: 15572
[2025-01-16 07:44:16,883] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:44:16,883] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1990/2809]  eta: 0:07:46  lr: 0.000001  min_lr: 0.000000  loss: 3.7703 (3.7353)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (55002.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1353  max mem: 15572
[2025-01-16 07:44:18,195] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103115
[2025-01-16 07:44:18,195] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:44:18,195] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2000/2809]  eta: 0:07:40  lr: 0.000001  min_lr: 0.000000  loss: 3.7867 (3.7353)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (55055.4803)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.0909  max mem: 15572
Epoch: [36]  [2010/2809]  eta: 0:07:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7867 (3.7356)  class_acc: 0.3333 (0.3397)  loss_scale: 65536.0000 (55107.5962)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1237  max mem: 15572
Epoch: [36]  [2020/2809]  eta: 0:07:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7089 (3.7352)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (55159.1964)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.1700  max mem: 15572
Epoch: [36]  [2030/2809]  eta: 0:07:23  lr: 0.000001  min_lr: 0.000000  loss: 3.5611 (3.7337)  class_acc: 0.3750 (0.3402)  loss_scale: 65536.0000 (55210.2885)  weight_decay: 0.0500 (0.0500)  time: 0.5246  data: 0.0902  max mem: 15572
Epoch: [36]  [2040/2809]  eta: 0:07:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6459 (3.7331)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (55260.8800)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0822  max mem: 15572
Epoch: [36]  [2050/2809]  eta: 0:07:11  lr: 0.000001  min_lr: 0.000000  loss: 3.7834 (3.7344)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (55310.9781)  weight_decay: 0.0500 (0.0500)  time: 0.5705  data: 0.1121  max mem: 15572
Epoch: [36]  [2060/2809]  eta: 0:07:05  lr: 0.000001  min_lr: 0.000000  loss: 3.8598 (3.7342)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (55360.5900)  weight_decay: 0.0500 (0.0500)  time: 0.5210  data: 0.0741  max mem: 15572
Epoch: [36]  [2070/2809]  eta: 0:07:00  lr: 0.000001  min_lr: 0.000000  loss: 3.8876 (3.7353)  class_acc: 0.2917 (0.3398)  loss_scale: 65536.0000 (55409.7228)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.1433  max mem: 15572
Epoch: [36]  [2080/2809]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 3.7637 (3.7345)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (55458.3835)  weight_decay: 0.0500 (0.0500)  time: 0.6455  data: 0.1950  max mem: 15572
Epoch: [36]  [2090/2809]  eta: 0:06:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7319 (3.7349)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (55506.5787)  weight_decay: 0.0500 (0.0500)  time: 0.5900  data: 0.1428  max mem: 15572
Epoch: [36]  [2100/2809]  eta: 0:06:43  lr: 0.000001  min_lr: 0.000000  loss: 3.8556 (3.7355)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (55554.3151)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.1063  max mem: 15572
Epoch: [36]  [2110/2809]  eta: 0:06:38  lr: 0.000001  min_lr: 0.000000  loss: 3.7659 (3.7352)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (55601.5992)  weight_decay: 0.0500 (0.0500)  time: 0.5817  data: 0.1513  max mem: 15572
[2025-01-16 07:45:30,933] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:45:30,934] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [2120/2809]  eta: 0:06:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6796 (3.7350)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (55679.3362)  weight_decay: 0.0500 (0.0500)  time: 0.5927  data: 0.1347  max mem: 15572
[2025-01-16 07:45:31,758] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103246
[2025-01-16 07:45:31,758] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:45:31,758] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2130/2809]  eta: 0:06:26  lr: 0.000001  min_lr: 0.000000  loss: 3.6513 (3.7343)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (55756.3435)  weight_decay: 0.0500 (0.0500)  time: 0.5212  data: 0.0706  max mem: 15572
Epoch: [36]  [2140/2809]  eta: 0:06:20  lr: 0.000001  min_lr: 0.000000  loss: 3.4939 (3.7337)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (55802.0215)  weight_decay: 0.0500 (0.0500)  time: 0.5639  data: 0.1394  max mem: 15572
Epoch: [36]  [2150/2809]  eta: 0:06:15  lr: 0.000001  min_lr: 0.000000  loss: 3.5702 (3.7342)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (55847.2748)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.2120  max mem: 15572
Epoch: [36]  [2160/2809]  eta: 0:06:09  lr: 0.000001  min_lr: 0.000000  loss: 3.8277 (3.7343)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (55892.1092)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1580  max mem: 15572
Epoch: [36]  [2170/2809]  eta: 0:06:03  lr: 0.000001  min_lr: 0.000000  loss: 3.6403 (3.7333)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (55936.5306)  weight_decay: 0.0500 (0.0500)  time: 0.5139  data: 0.0892  max mem: 15572
Epoch: [36]  [2180/2809]  eta: 0:05:58  lr: 0.000001  min_lr: 0.000000  loss: 3.6230 (3.7332)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (55980.5447)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.1025  max mem: 15572
Epoch: [36]  [2190/2809]  eta: 0:05:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6471 (3.7329)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (56024.1570)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1462  max mem: 15572
Epoch: [36]  [2200/2809]  eta: 0:05:46  lr: 0.000001  min_lr: 0.000000  loss: 3.8086 (3.7334)  class_acc: 0.2917 (0.3395)  loss_scale: 65536.0000 (56067.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5318  data: 0.1000  max mem: 15572
Epoch: [36]  [2210/2809]  eta: 0:05:40  lr: 0.000001  min_lr: 0.000000  loss: 3.8086 (3.7332)  class_acc: 0.2917 (0.3395)  loss_scale: 65536.0000 (56110.1981)  weight_decay: 0.0500 (0.0500)  time: 0.4937  data: 0.0705  max mem: 15572
Epoch: [36]  [2220/2809]  eta: 0:05:34  lr: 0.000001  min_lr: 0.000000  loss: 3.8135 (3.7333)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (56152.6376)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1029  max mem: 15572
Epoch: [36]  [2230/2809]  eta: 0:05:29  lr: 0.000001  min_lr: 0.000000  loss: 3.6844 (3.7321)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (56194.6965)  weight_decay: 0.0500 (0.0500)  time: 0.5491  data: 0.1107  max mem: 15572
Epoch: [36]  [2240/2809]  eta: 0:05:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6737 (3.7324)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (56236.3802)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1307  max mem: 15572
Epoch: [36]  [2250/2809]  eta: 0:05:17  lr: 0.000001  min_lr: 0.000000  loss: 3.8292 (3.7326)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (56277.6935)  weight_decay: 0.0500 (0.0500)  time: 0.5562  data: 0.1242  max mem: 15572
[2025-01-16 07:46:43,132] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:46:43,132] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:46:44,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103378
[2025-01-16 07:46:44,388] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:46:44,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2260/2809]  eta: 0:05:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8473 (3.7327)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (56405.5975)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1290  max mem: 15572
Epoch: [36]  [2270/2809]  eta: 0:05:06  lr: 0.000001  min_lr: 0.000000  loss: 3.8473 (3.7334)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (56445.8018)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1241  max mem: 15572
Epoch: [36]  [2280/2809]  eta: 0:05:00  lr: 0.000001  min_lr: 0.000000  loss: 3.7629 (3.7327)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (56485.6537)  weight_decay: 0.0500 (0.0500)  time: 0.5122  data: 0.0746  max mem: 15572
Epoch: [36]  [2290/2809]  eta: 0:04:54  lr: 0.000001  min_lr: 0.000000  loss: 3.7441 (3.7334)  class_acc: 0.3750 (0.3398)  loss_scale: 65536.0000 (56525.1576)  weight_decay: 0.0500 (0.0500)  time: 0.5285  data: 0.0910  max mem: 15572
Epoch: [36]  [2300/2809]  eta: 0:04:49  lr: 0.000001  min_lr: 0.000000  loss: 3.8918 (3.7342)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (56564.3181)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.1653  max mem: 15572
Epoch: [36]  [2310/2809]  eta: 0:04:43  lr: 0.000001  min_lr: 0.000000  loss: 3.8918 (3.7337)  class_acc: 0.2917 (0.3398)  loss_scale: 65536.0000 (56603.1398)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1683  max mem: 15572
Epoch: [36]  [2320/2809]  eta: 0:04:38  lr: 0.000001  min_lr: 0.000000  loss: 3.7746 (3.7333)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (56641.6269)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1764  max mem: 15572
Epoch: [36]  [2330/2809]  eta: 0:04:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6134 (3.7325)  class_acc: 0.4167 (0.3401)  loss_scale: 65536.0000 (56679.7838)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.1713  max mem: 15572
Epoch: [36]  [2340/2809]  eta: 0:04:26  lr: 0.000001  min_lr: 0.000000  loss: 3.6134 (3.7320)  class_acc: 0.4167 (0.3402)  loss_scale: 65536.0000 (56717.6147)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1031  max mem: 15572
Epoch: [36]  [2350/2809]  eta: 0:04:20  lr: 0.000001  min_lr: 0.000000  loss: 3.7026 (3.7327)  class_acc: 0.2500 (0.3399)  loss_scale: 65536.0000 (56755.1238)  weight_decay: 0.0500 (0.0500)  time: 0.5282  data: 0.0778  max mem: 15572
Epoch: [36]  [2360/2809]  eta: 0:04:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8194 (3.7334)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (56792.3151)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.0950  max mem: 15572
Epoch: [36]  [2370/2809]  eta: 0:04:09  lr: 0.000001  min_lr: 0.000000  loss: 3.9133 (3.7338)  class_acc: 0.2500 (0.3395)  loss_scale: 65536.0000 (56829.1927)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0999  max mem: 15572
Epoch: [36]  [2380/2809]  eta: 0:04:03  lr: 0.000001  min_lr: 0.000000  loss: 3.7476 (3.7338)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (56865.7606)  weight_decay: 0.0500 (0.0500)  time: 0.5420  data: 0.0927  max mem: 15572
[2025-01-16 07:47:57,758] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:47:57,759] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:47:58,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103508
[2025-01-16 07:47:58,171] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:47:58,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2390/2809]  eta: 0:03:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6493 (3.7342)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (56929.4320)  weight_decay: 0.0500 (0.0500)  time: 0.5264  data: 0.0753  max mem: 15572
Epoch: [36]  [2400/2809]  eta: 0:03:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6772 (3.7340)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (56965.2778)  weight_decay: 0.0500 (0.0500)  time: 0.5324  data: 0.0803  max mem: 15572
Epoch: [36]  [2410/2809]  eta: 0:03:46  lr: 0.000001  min_lr: 0.000000  loss: 3.8599 (3.7345)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (57000.8262)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.1557  max mem: 15572
Epoch: [36]  [2420/2809]  eta: 0:03:41  lr: 0.000001  min_lr: 0.000000  loss: 3.9132 (3.7346)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (57036.0810)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.1650  max mem: 15572
Epoch: [36]  [2430/2809]  eta: 0:03:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6906 (3.7332)  class_acc: 0.3333 (0.3399)  loss_scale: 65536.0000 (57071.0457)  weight_decay: 0.0500 (0.0500)  time: 0.5583  data: 0.1137  max mem: 15572
Epoch: [36]  [2440/2809]  eta: 0:03:29  lr: 0.000001  min_lr: 0.000000  loss: 3.5402 (3.7336)  class_acc: 0.3750 (0.3399)  loss_scale: 65536.0000 (57105.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1111  max mem: 15572
Epoch: [36]  [2450/2809]  eta: 0:03:24  lr: 0.000001  min_lr: 0.000000  loss: 3.9508 (3.7345)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (57140.1191)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1483  max mem: 15572
Epoch: [36]  [2460/2809]  eta: 0:03:18  lr: 0.000001  min_lr: 0.000000  loss: 3.8114 (3.7341)  class_acc: 0.2917 (0.3396)  loss_scale: 65536.0000 (57174.2349)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.1565  max mem: 15572
Epoch: [36]  [2470/2809]  eta: 0:03:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7403 (3.7344)  class_acc: 0.3333 (0.3396)  loss_scale: 65536.0000 (57208.0745)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.1661  max mem: 15572
Epoch: [36]  [2480/2809]  eta: 0:03:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7442 (3.7337)  class_acc: 0.3333 (0.3398)  loss_scale: 65536.0000 (57241.6413)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1593  max mem: 15572
Epoch: [36]  [2490/2809]  eta: 0:03:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5773 (3.7326)  class_acc: 0.4167 (0.3403)  loss_scale: 65536.0000 (57274.9386)  weight_decay: 0.0500 (0.0500)  time: 0.5808  data: 0.1478  max mem: 15572
Epoch: [36]  [2500/2809]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 3.7743 (3.7333)  class_acc: 0.3750 (0.3401)  loss_scale: 65536.0000 (57307.9696)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1102  max mem: 15572
Epoch: [36]  [2510/2809]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7415 (3.7324)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (57340.7376)  weight_decay: 0.0500 (0.0500)  time: 0.5313  data: 0.0894  max mem: 15572
[2025-01-16 07:49:12,317] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:49:12,317] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:49:12,819] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103638
[2025-01-16 07:49:12,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:49:12,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2520/2809]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 3.6220 (3.7327)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (57399.2416)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.1089  max mem: 15572
Epoch: [36]  [2530/2809]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 3.9248 (3.7334)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (57431.3900)  weight_decay: 0.0500 (0.0500)  time: 0.5361  data: 0.1233  max mem: 15572
Epoch: [36]  [2540/2809]  eta: 0:02:32  lr: 0.000001  min_lr: 0.000000  loss: 3.8624 (3.7327)  class_acc: 0.2917 (0.3404)  loss_scale: 65536.0000 (57463.2853)  weight_decay: 0.0500 (0.0500)  time: 0.5451  data: 0.1128  max mem: 15572
Epoch: [36]  [2550/2809]  eta: 0:02:27  lr: 0.000001  min_lr: 0.000000  loss: 3.7969 (3.7330)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (57494.9306)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1196  max mem: 15572
Epoch: [36]  [2560/2809]  eta: 0:02:21  lr: 0.000001  min_lr: 0.000000  loss: 3.5554 (3.7324)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (57526.3288)  weight_decay: 0.0500 (0.0500)  time: 0.6110  data: 0.1686  max mem: 15572
Epoch: [36]  [2570/2809]  eta: 0:02:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7621 (3.7328)  class_acc: 0.3750 (0.3404)  loss_scale: 65536.0000 (57557.4827)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1201  max mem: 15572
Epoch: [36]  [2580/2809]  eta: 0:02:10  lr: 0.000001  min_lr: 0.000000  loss: 3.8674 (3.7326)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (57588.3952)  weight_decay: 0.0500 (0.0500)  time: 0.5050  data: 0.0539  max mem: 15572
Epoch: [36]  [2590/2809]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 3.8457 (3.7332)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (57619.0691)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0636  max mem: 15572
Epoch: [36]  [2600/2809]  eta: 0:01:58  lr: 0.000001  min_lr: 0.000000  loss: 3.6198 (3.7332)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (57649.5071)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0929  max mem: 15572
Epoch: [36]  [2610/2809]  eta: 0:01:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6653 (3.7331)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (57679.7120)  weight_decay: 0.0500 (0.0500)  time: 0.5733  data: 0.1378  max mem: 15572
Epoch: [36]  [2620/2809]  eta: 0:01:47  lr: 0.000001  min_lr: 0.000000  loss: 3.5787 (3.7323)  class_acc: 0.3750 (0.3407)  loss_scale: 65536.0000 (57709.6864)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.1943  max mem: 15572
Epoch: [36]  [2630/2809]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 3.5598 (3.7325)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (57739.4329)  weight_decay: 0.0500 (0.0500)  time: 0.6170  data: 0.2027  max mem: 15572
Epoch: [36]  [2640/2809]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 3.5598 (3.7310)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (57768.9542)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.1480  max mem: 15572
[2025-01-16 07:50:25,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:50:25,609] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:50:27,079] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103770
[2025-01-16 07:50:27,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:50:27,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [2650/2809]  eta: 0:01:30  lr: 0.000001  min_lr: 0.000000  loss: 3.6596 (3.7312)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (57872.4164)  weight_decay: 0.0500 (0.0500)  time: 0.5530  data: 0.0813  max mem: 15572
[2025-01-16 07:50:30,280] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 103776
[2025-01-16 07:50:30,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:50:30,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [2660/2809]  eta: 0:01:24  lr: 0.000001  min_lr: 0.000000  loss: 3.8586 (3.7308)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (57790.3886)  weight_decay: 0.0500 (0.0500)  time: 0.5085  data: 0.0338  max mem: 15572
Epoch: [36]  [2670/2809]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 3.6430 (3.7309)  class_acc: 0.3750 (0.3410)  loss_scale: 32768.0000 (57696.7069)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0573  max mem: 15572
Epoch: [36]  [2680/2809]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 3.6430 (3.7307)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (57603.7240)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0792  max mem: 15572
Epoch: [36]  [2690/2809]  eta: 0:01:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7804 (3.7311)  class_acc: 0.3333 (0.3410)  loss_scale: 32768.0000 (57511.4322)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1166  max mem: 15572
Epoch: [36]  [2700/2809]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7804 (3.7312)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (57419.8238)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.1509  max mem: 15572
Epoch: [36]  [2710/2809]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 3.7437 (3.7312)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (57328.8912)  weight_decay: 0.0500 (0.0500)  time: 0.5277  data: 0.0678  max mem: 15572
Epoch: [36]  [2720/2809]  eta: 0:00:50  lr: 0.000001  min_lr: 0.000000  loss: 3.6268 (3.7304)  class_acc: 0.3750 (0.3409)  loss_scale: 32768.0000 (57238.6270)  weight_decay: 0.0500 (0.0500)  time: 0.5519  data: 0.0868  max mem: 15572
Epoch: [36]  [2730/2809]  eta: 0:00:44  lr: 0.000001  min_lr: 0.000000  loss: 3.6894 (3.7307)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (57149.0238)  weight_decay: 0.0500 (0.0500)  time: 0.5805  data: 0.1205  max mem: 15572
Epoch: [36]  [2740/2809]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 3.7792 (3.7310)  class_acc: 0.2917 (0.3408)  loss_scale: 32768.0000 (57060.0744)  weight_decay: 0.0500 (0.0500)  time: 0.5074  data: 0.0382  max mem: 15572
Epoch: [36]  [2750/2809]  eta: 0:00:33  lr: 0.000001  min_lr: 0.000000  loss: 3.5977 (3.7310)  class_acc: 0.2917 (0.3407)  loss_scale: 32768.0000 (56971.7717)  weight_decay: 0.0500 (0.0500)  time: 0.4842  data: 0.0268  max mem: 15572
Epoch: [36]  [2760/2809]  eta: 0:00:27  lr: 0.000001  min_lr: 0.000000  loss: 3.7845 (3.7314)  class_acc: 0.2917 (0.3405)  loss_scale: 32768.0000 (56884.1087)  weight_decay: 0.0500 (0.0500)  time: 0.5383  data: 0.0870  max mem: 15572
Epoch: [36]  [2770/2809]  eta: 0:00:22  lr: 0.000001  min_lr: 0.000000  loss: 3.9963 (3.7326)  class_acc: 0.2917 (0.3403)  loss_scale: 32768.0000 (56797.0783)  weight_decay: 0.0500 (0.0500)  time: 0.6288  data: 0.1581  max mem: 15572
Epoch: [36]  [2780/2809]  eta: 0:00:16  lr: 0.000001  min_lr: 0.000000  loss: 3.8538 (3.7318)  class_acc: 0.2917 (0.3405)  loss_scale: 32768.0000 (56710.6739)  weight_decay: 0.0500 (0.0500)  time: 0.6323  data: 0.1605  max mem: 15572
[2025-01-16 07:51:41,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:51:41,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [2790/2809]  eta: 0:00:10  lr: 0.000001  min_lr: 0.000000  loss: 3.6407 (3.7316)  class_acc: 0.3333 (0.3405)  loss_scale: 32768.0000 (56742.2945)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0796  max mem: 15572
Epoch: [36]  [2800/2809]  eta: 0:00:05  lr: 0.000001  min_lr: 0.000000  loss: 3.7219 (3.7320)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (56773.6894)  weight_decay: 0.0500 (0.0500)  time: 0.4868  data: 0.0169  max mem: 15572
Epoch: [36]  [2808/2809]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 4.0578 (3.7332)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (56798.6444)  weight_decay: 0.0500 (0.0500)  time: 0.4586  data: 0.0167  max mem: 15572
Epoch: [36] Total time: 0:26:31 (0.5667 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 4.0578 (3.7332)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (56798.6444)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:18:44  loss: 0.3893 (0.3893)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.1333  data: 3.9539  max mem: 15572
Val:  [ 10/272]  eta: 0:03:22  loss: 2.3306 (2.2413)  acc1: 50.0000 (44.9495)  acc5: 77.7778 (73.2323)  time: 0.7730  data: 0.5739  max mem: 15572
Val:  [ 20/272]  eta: 0:02:18  loss: 2.2331 (2.2706)  acc1: 50.0000 (45.5026)  acc5: 77.7778 (73.8095)  time: 0.3687  data: 0.1673  max mem: 15572
Val:  [ 30/272]  eta: 0:01:51  loss: 2.2702 (2.3601)  acc1: 50.0000 (42.8315)  acc5: 72.2222 (73.4767)  time: 0.2874  data: 0.0775  max mem: 15572
Val:  [ 40/272]  eta: 0:01:41  loss: 2.5759 (2.4210)  acc1: 27.7778 (39.8374)  acc5: 72.2222 (73.5772)  time: 0.3188  data: 0.0942  max mem: 15572
Val:  [ 50/272]  eta: 0:01:29  loss: 2.3674 (2.3410)  acc1: 33.3333 (41.9390)  acc5: 77.7778 (75.7081)  time: 0.3177  data: 0.1067  max mem: 15572
Val:  [ 60/272]  eta: 0:01:19  loss: 1.4824 (2.2392)  acc1: 66.6667 (45.1730)  acc5: 88.8889 (76.5938)  time: 0.2539  data: 0.0506  max mem: 15572
Val:  [ 70/272]  eta: 0:01:14  loss: 1.5346 (2.1592)  acc1: 66.6667 (47.5743)  acc5: 88.8889 (77.6995)  time: 0.2724  data: 0.0709  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.8155 (2.1655)  acc1: 55.5556 (47.6680)  acc5: 77.7778 (77.6406)  time: 0.3231  data: 0.1251  max mem: 15572
Val:  [ 90/272]  eta: 0:01:04  loss: 2.0911 (2.1679)  acc1: 50.0000 (48.4127)  acc5: 77.7778 (78.2662)  time: 0.3148  data: 0.1210  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.1015 (2.1963)  acc1: 50.0000 (47.6898)  acc5: 83.3333 (77.7778)  time: 0.3212  data: 0.1310  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.4306 (2.2682)  acc1: 22.2222 (45.6957)  acc5: 72.2222 (76.6266)  time: 0.3341  data: 0.1344  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.8221 (2.3045)  acc1: 22.2222 (45.0872)  acc5: 72.2222 (76.0331)  time: 0.3066  data: 0.0995  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.1483 (2.2690)  acc1: 50.0000 (45.9712)  acc5: 77.7778 (76.8024)  time: 0.3159  data: 0.1072  max mem: 15572
Val:  [140/272]  eta: 0:00:46  loss: 1.6353 (2.2634)  acc1: 55.5556 (46.3357)  acc5: 88.8889 (76.5563)  time: 0.3802  data: 0.1833  max mem: 15572
Val:  [150/272]  eta: 0:00:42  loss: 2.2683 (2.2704)  acc1: 44.4444 (45.6954)  acc5: 77.7778 (76.8212)  time: 0.3422  data: 0.1499  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2683 (2.2588)  acc1: 44.4444 (46.2733)  acc5: 77.7778 (77.0876)  time: 0.2350  data: 0.0349  max mem: 15572
Val:  [170/272]  eta: 0:00:33  loss: 2.3709 (2.2801)  acc1: 44.4444 (45.8739)  acc5: 77.7778 (76.5757)  time: 0.2278  data: 0.0218  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.2535 (2.2706)  acc1: 38.8889 (45.7950)  acc5: 72.2222 (76.9797)  time: 0.2821  data: 0.0860  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.2707 (2.3250)  acc1: 33.3333 (44.4444)  acc5: 77.7778 (75.6835)  time: 0.3202  data: 0.1262  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.5692 (2.3334)  acc1: 33.3333 (44.1957)  acc5: 66.6667 (75.3455)  time: 0.3331  data: 0.1393  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.0781 (2.3379)  acc1: 50.0000 (44.3391)  acc5: 77.7778 (75.2238)  time: 0.3626  data: 0.1716  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.0781 (2.3248)  acc1: 50.0000 (44.5701)  acc5: 77.7778 (75.3896)  time: 0.3260  data: 0.1237  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7682 (2.2960)  acc1: 66.6667 (45.5507)  acc5: 83.3333 (75.7816)  time: 0.2903  data: 0.0881  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6402 (2.2820)  acc1: 66.6667 (45.8276)  acc5: 83.3333 (75.9567)  time: 0.3133  data: 0.1265  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2514 (2.2928)  acc1: 38.8889 (45.2191)  acc5: 83.3333 (75.9407)  time: 0.3385  data: 0.1563  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.2164 (2.2356)  acc1: 72.2222 (46.9349)  acc5: 88.8889 (76.6709)  time: 0.3017  data: 0.1221  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3715 (2.2303)  acc1: 72.2222 (47.0480)  acc5: 88.8889 (76.7733)  time: 0.2000  data: 0.0339  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3715 (2.2353)  acc1: 66.6667 (47.0203)  acc5: 88.8889 (76.7356)  time: 0.1897  data: 0.0338  max mem: 15572
Val: Total time: 0:01:27 (0.3215 s / it)
* Acc@1 47.020 Acc@5 76.736 loss 2.235
Accuracy of the network on the 4883 val videos: 47.0%
Max accuracy: 47.47%
Epoch: [37]  [   0/2809]  eta: 5:01:06  lr: 0.000001  min_lr: 0.000000  loss: 3.5908 (3.5908)  class_acc: 0.5000 (0.5000)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.4315  data: 6.0270  max mem: 15572
Epoch: [37]  [  10/2809]  eta: 0:59:24  lr: 0.000001  min_lr: 0.000000  loss: 3.5908 (3.6906)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2736  data: 0.8565  max mem: 15572
Epoch: [37]  [  20/2809]  eta: 0:40:33  lr: 0.000001  min_lr: 0.000000  loss: 3.8849 (3.8003)  class_acc: 0.3333 (0.3254)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1700  max mem: 15572
Epoch: [37]  [  30/2809]  eta: 0:33:59  lr: 0.000001  min_lr: 0.000000  loss: 3.8645 (3.7395)  class_acc: 0.3333 (0.3414)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4373  data: 0.0294  max mem: 15572
Epoch: [37]  [  40/2809]  eta: 0:30:26  lr: 0.000001  min_lr: 0.000000  loss: 3.8352 (3.7890)  class_acc: 0.3333 (0.3343)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4363  data: 0.0294  max mem: 15572
Epoch: [37]  [  50/2809]  eta: 0:28:36  lr: 0.000001  min_lr: 0.000000  loss: 3.8352 (3.7638)  class_acc: 0.2917 (0.3358)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4485  data: 0.0005  max mem: 15572
Epoch: [37]  [  60/2809]  eta: 0:27:28  lr: 0.000001  min_lr: 0.000000  loss: 3.7111 (3.7475)  class_acc: 0.3333 (0.3402)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4766  data: 0.0007  max mem: 15572
[2025-01-16 07:54:02,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=104000, skipped=691, lr=[8.502632161242406e-09, 8.502632161242406e-09, 1.214661737320344e-08, 1.214661737320344e-08, 1.735231053314777e-08, 1.735231053314777e-08, 2.4789015047353962e-08, 2.4789015047353962e-08, 3.541287863907709e-08, 3.541287863907709e-08, 5.0589826627252987e-08, 5.0589826627252987e-08, 7.22711808960757e-08, 7.22711808960757e-08, 1.03244544137251e-07, 1.03244544137251e-07, 1.4749220591035857e-07, 1.4749220591035857e-07, 2.107031513005123e-07, 2.107031513005123e-07, 3.010045018578747e-07, 3.010045018578747e-07, 4.3000643122553533e-07, 4.3000643122553533e-07, 6.142949017507648e-07, 6.142949017507648e-07, 8.775641453582355e-07, 8.775641453582355e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 07:54:02,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=104000/global_step=104000, RunningAvgSamplesPerSec=28.580474539719955, CurrSamplesPerSec=22.40355525168789, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [  70/2809]  eta: 0:27:49  lr: 0.000001  min_lr: 0.000000  loss: 3.8174 (3.7473)  class_acc: 0.3333 (0.3386)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5772  data: 0.1079  max mem: 15572
Epoch: [37]  [  80/2809]  eta: 0:27:56  lr: 0.000001  min_lr: 0.000000  loss: 3.9061 (3.7396)  class_acc: 0.2917 (0.3405)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.2065  max mem: 15572
Epoch: [37]  [  90/2809]  eta: 0:27:57  lr: 0.000001  min_lr: 0.000000  loss: 3.7114 (3.7342)  class_acc: 0.3333 (0.3420)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6441  data: 0.2022  max mem: 15572
[2025-01-16 07:54:24,773] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:54:24,773] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [ 100/2809]  eta: 0:28:08  lr: 0.000001  min_lr: 0.000000  loss: 3.5726 (3.7328)  class_acc: 0.3750 (0.3469)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.1863  max mem: 15572
[2025-01-16 07:54:26,765] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104034
[2025-01-16 07:54:26,765] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:54:26,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 07:54:30,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104042
[2025-01-16 07:54:30,416] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:54:30,417] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 110/2809]  eta: 0:28:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7438 (3.7258)  class_acc: 0.3750 (0.3506)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6623  data: 0.1827  max mem: 15572
Epoch: [37]  [ 120/2809]  eta: 0:28:20  lr: 0.000001  min_lr: 0.000000  loss: 3.8943 (3.7425)  class_acc: 0.3333 (0.3478)  loss_scale: 32768.0000 (62827.9008)  weight_decay: 0.0500 (0.0500)  time: 0.6773  data: 0.2125  max mem: 15572
Epoch: [37]  [ 130/2809]  eta: 0:28:31  lr: 0.000001  min_lr: 0.000000  loss: 3.8800 (3.7492)  class_acc: 0.2917 (0.3445)  loss_scale: 32768.0000 (60533.2519)  weight_decay: 0.0500 (0.0500)  time: 0.7141  data: 0.2437  max mem: 15572
Epoch: [37]  [ 140/2809]  eta: 0:28:30  lr: 0.000001  min_lr: 0.000000  loss: 3.7209 (3.7453)  class_acc: 0.2917 (0.3475)  loss_scale: 32768.0000 (58564.0851)  weight_decay: 0.0500 (0.0500)  time: 0.6928  data: 0.2316  max mem: 15572
Epoch: [37]  [ 150/2809]  eta: 0:28:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7209 (3.7470)  class_acc: 0.3333 (0.3463)  loss_scale: 32768.0000 (56855.7351)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.2059  max mem: 15572
Epoch: [37]  [ 160/2809]  eta: 0:28:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6665 (3.7343)  class_acc: 0.3333 (0.3499)  loss_scale: 32768.0000 (55359.6025)  weight_decay: 0.0500 (0.0500)  time: 0.6964  data: 0.2352  max mem: 15572
Epoch: [37]  [ 170/2809]  eta: 0:28:15  lr: 0.000001  min_lr: 0.000000  loss: 3.5529 (3.7240)  class_acc: 0.3750 (0.3511)  loss_scale: 32768.0000 (54038.4561)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.1942  max mem: 15572
Epoch: [37]  [ 180/2809]  eta: 0:27:35  lr: 0.000001  min_lr: 0.000000  loss: 3.6255 (3.7290)  class_acc: 0.3750 (0.3504)  loss_scale: 32768.0000 (52863.2928)  weight_decay: 0.0500 (0.0500)  time: 0.4851  data: 0.0641  max mem: 15572
Epoch: [37]  [ 190/2809]  eta: 0:27:09  lr: 0.000001  min_lr: 0.000000  loss: 3.7249 (3.7299)  class_acc: 0.3333 (0.3514)  loss_scale: 32768.0000 (51811.1832)  weight_decay: 0.0500 (0.0500)  time: 0.4485  data: 0.0200  max mem: 15572
Epoch: [37]  [ 200/2809]  eta: 0:27:01  lr: 0.000001  min_lr: 0.000000  loss: 3.8099 (3.7350)  class_acc: 0.3333 (0.3503)  loss_scale: 32768.0000 (50863.7612)  weight_decay: 0.0500 (0.0500)  time: 0.5467  data: 0.0979  max mem: 15572
Epoch: [37]  [ 210/2809]  eta: 0:26:41  lr: 0.000001  min_lr: 0.000000  loss: 3.6997 (3.7208)  class_acc: 0.3750 (0.3515)  loss_scale: 32768.0000 (50006.1422)  weight_decay: 0.0500 (0.0500)  time: 0.5574  data: 0.1188  max mem: 15572
Epoch: [37]  [ 220/2809]  eta: 0:26:37  lr: 0.000001  min_lr: 0.000000  loss: 3.6798 (3.7203)  class_acc: 0.3333 (0.3509)  loss_scale: 32768.0000 (49226.1357)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1467  max mem: 15572
Epoch: [37]  [ 230/2809]  eta: 0:26:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7472 (3.7249)  class_acc: 0.3333 (0.3506)  loss_scale: 32768.0000 (48513.6623)  weight_decay: 0.0500 (0.0500)  time: 0.5580  data: 0.1219  max mem: 15572
[2025-01-16 07:55:47,909] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:55:47,910] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 240/2809]  eta: 0:26:06  lr: 0.000001  min_lr: 0.000000  loss: 3.7523 (3.7281)  class_acc: 0.2917 (0.3499)  loss_scale: 32768.0000 (48268.2158)  weight_decay: 0.0500 (0.0500)  time: 0.5297  data: 0.0857  max mem: 15572
Epoch: [37]  [ 250/2809]  eta: 0:25:58  lr: 0.000001  min_lr: 0.000000  loss: 3.9023 (3.7340)  class_acc: 0.2917 (0.3488)  loss_scale: 65536.0000 (48956.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1505  max mem: 15572
Epoch: [37]  [ 260/2809]  eta: 0:25:50  lr: 0.000001  min_lr: 0.000000  loss: 3.7739 (3.7296)  class_acc: 0.3333 (0.3496)  loss_scale: 65536.0000 (49591.4176)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1591  max mem: 15572
Epoch: [37]  [ 270/2809]  eta: 0:25:39  lr: 0.000001  min_lr: 0.000000  loss: 3.6628 (3.7293)  class_acc: 0.4167 (0.3501)  loss_scale: 65536.0000 (50179.7786)  weight_decay: 0.0500 (0.0500)  time: 0.5706  data: 0.1334  max mem: 15572
Epoch: [37]  [ 280/2809]  eta: 0:25:28  lr: 0.000001  min_lr: 0.000000  loss: 3.6062 (3.7243)  class_acc: 0.4167 (0.3508)  loss_scale: 65536.0000 (50726.2633)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1077  max mem: 15572
Epoch: [37]  [ 290/2809]  eta: 0:25:17  lr: 0.000001  min_lr: 0.000000  loss: 3.7269 (3.7220)  class_acc: 0.3333 (0.3511)  loss_scale: 65536.0000 (51235.1890)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1148  max mem: 15572
Epoch: [37]  [ 300/2809]  eta: 0:25:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7398 (3.7241)  class_acc: 0.2917 (0.3515)  loss_scale: 65536.0000 (51710.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.1376  max mem: 15572
Epoch: [37]  [ 310/2809]  eta: 0:24:58  lr: 0.000001  min_lr: 0.000000  loss: 4.0272 (3.7291)  class_acc: 0.3333 (0.3494)  loss_scale: 65536.0000 (52154.8553)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1296  max mem: 15572
Epoch: [37]  [ 320/2809]  eta: 0:25:00  lr: 0.000001  min_lr: 0.000000  loss: 3.8305 (3.7271)  class_acc: 0.3333 (0.3494)  loss_scale: 65536.0000 (52571.7134)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.1520  max mem: 15572
Epoch: [37]  [ 330/2809]  eta: 0:24:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6184 (3.7274)  class_acc: 0.3333 (0.3493)  loss_scale: 65536.0000 (52963.3837)  weight_decay: 0.0500 (0.0500)  time: 0.6348  data: 0.1559  max mem: 15572
Epoch: [37]  [ 340/2809]  eta: 0:24:45  lr: 0.000001  min_lr: 0.000000  loss: 3.9670 (3.7311)  class_acc: 0.2917 (0.3480)  loss_scale: 65536.0000 (53332.0821)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1416  max mem: 15572
Epoch: [37]  [ 350/2809]  eta: 0:24:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7622 (3.7234)  class_acc: 0.2917 (0.3503)  loss_scale: 65536.0000 (53679.7721)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1343  max mem: 15572
Epoch: [37]  [ 360/2809]  eta: 0:24:27  lr: 0.000001  min_lr: 0.000000  loss: 3.4837 (3.7195)  class_acc: 0.3750 (0.3516)  loss_scale: 65536.0000 (54008.1994)  weight_decay: 0.0500 (0.0500)  time: 0.5527  data: 0.1170  max mem: 15572
[2025-01-16 07:57:01,595] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:57:01,595] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 07:57:02,574] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104301
[2025-01-16 07:57:02,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 07:57:02,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [ 370/2809]  eta: 0:24:16  lr: 0.000001  min_lr: 0.000000  loss: 3.5138 (3.7163)  class_acc: 0.3750 (0.3524)  loss_scale: 65536.0000 (54672.2156)  weight_decay: 0.0500 (0.0500)  time: 0.5496  data: 0.1042  max mem: 15572
Epoch: [37]  [ 380/2809]  eta: 0:24:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5526 (3.7118)  class_acc: 0.4167 (0.3543)  loss_scale: 65536.0000 (54957.3543)  weight_decay: 0.0500 (0.0500)  time: 0.4946  data: 0.0382  max mem: 15572
Epoch: [37]  [ 390/2809]  eta: 0:23:55  lr: 0.000001  min_lr: 0.000000  loss: 3.5333 (3.7137)  class_acc: 0.4167 (0.3542)  loss_scale: 65536.0000 (55227.9079)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0715  max mem: 15572
Epoch: [37]  [ 400/2809]  eta: 0:23:48  lr: 0.000001  min_lr: 0.000000  loss: 3.5504 (3.7128)  class_acc: 0.3333 (0.3554)  loss_scale: 65536.0000 (55484.9676)  weight_decay: 0.0500 (0.0500)  time: 0.5792  data: 0.1196  max mem: 15572
Epoch: [37]  [ 410/2809]  eta: 0:23:43  lr: 0.000001  min_lr: 0.000000  loss: 3.8435 (3.7164)  class_acc: 0.3333 (0.3551)  loss_scale: 65536.0000 (55729.5182)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.1379  max mem: 15572
Epoch: [37]  [ 420/2809]  eta: 0:23:32  lr: 0.000001  min_lr: 0.000000  loss: 3.8337 (3.7154)  class_acc: 0.3333 (0.3558)  loss_scale: 65536.0000 (55962.4513)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1257  max mem: 15572
Epoch: [37]  [ 430/2809]  eta: 0:23:23  lr: 0.000001  min_lr: 0.000000  loss: 3.4510 (3.7095)  class_acc: 0.3750 (0.3559)  loss_scale: 65536.0000 (56184.5754)  weight_decay: 0.0500 (0.0500)  time: 0.5147  data: 0.0796  max mem: 15572
Epoch: [37]  [ 440/2809]  eta: 0:23:12  lr: 0.000001  min_lr: 0.000000  loss: 3.6147 (3.7116)  class_acc: 0.3333 (0.3551)  loss_scale: 65536.0000 (56396.6259)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0716  max mem: 15572
Epoch: [37]  [ 450/2809]  eta: 0:23:10  lr: 0.000001  min_lr: 0.000000  loss: 3.9350 (3.7128)  class_acc: 0.2917 (0.3546)  loss_scale: 65536.0000 (56599.2727)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1384  max mem: 15572
Epoch: [37]  [ 460/2809]  eta: 0:23:02  lr: 0.000001  min_lr: 0.000000  loss: 3.9602 (3.7158)  class_acc: 0.2917 (0.3534)  loss_scale: 65536.0000 (56793.1280)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1688  max mem: 15572
Epoch: [37]  [ 470/2809]  eta: 0:22:56  lr: 0.000001  min_lr: 0.000000  loss: 3.9651 (3.7227)  class_acc: 0.2917 (0.3523)  loss_scale: 65536.0000 (56978.7516)  weight_decay: 0.0500 (0.0500)  time: 0.5653  data: 0.1247  max mem: 15572
[2025-01-16 07:58:01,523] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104408
[2025-01-16 07:58:01,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:58:01,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 480/2809]  eta: 0:22:45  lr: 0.000001  min_lr: 0.000000  loss: 3.8211 (3.7204)  class_acc: 0.3333 (0.3521)  loss_scale: 65536.0000 (56747.9085)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.1061  max mem: 15572
Epoch: [37]  [ 490/2809]  eta: 0:22:36  lr: 0.000001  min_lr: 0.000000  loss: 3.6224 (3.7187)  class_acc: 0.3333 (0.3517)  loss_scale: 32768.0000 (56259.5193)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0980  max mem: 15572
Epoch: [37]  [ 500/2809]  eta: 0:22:27  lr: 0.000001  min_lr: 0.000000  loss: 3.7392 (3.7205)  class_acc: 0.2500 (0.3496)  loss_scale: 32768.0000 (55790.6267)  weight_decay: 0.0500 (0.0500)  time: 0.5168  data: 0.0784  max mem: 15572
Epoch: [37]  [ 510/2809]  eta: 0:22:16  lr: 0.000001  min_lr: 0.000000  loss: 4.0659 (3.7236)  class_acc: 0.2500 (0.3489)  loss_scale: 32768.0000 (55340.0861)  weight_decay: 0.0500 (0.0500)  time: 0.4886  data: 0.0348  max mem: 15572
Epoch: [37]  [ 520/2809]  eta: 0:22:14  lr: 0.000001  min_lr: 0.000000  loss: 3.8251 (3.7250)  class_acc: 0.2917 (0.3487)  loss_scale: 32768.0000 (54906.8407)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1157  max mem: 15572
Epoch: [37]  [ 530/2809]  eta: 0:22:05  lr: 0.000001  min_lr: 0.000000  loss: 3.6933 (3.7243)  class_acc: 0.3750 (0.3486)  loss_scale: 32768.0000 (54489.9134)  weight_decay: 0.0500 (0.0500)  time: 0.5906  data: 0.1489  max mem: 15572
Epoch: [37]  [ 540/2809]  eta: 0:22:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7198 (3.7255)  class_acc: 0.3333 (0.3476)  loss_scale: 32768.0000 (54088.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1369  max mem: 15572
Epoch: [37]  [ 550/2809]  eta: 0:21:54  lr: 0.000001  min_lr: 0.000000  loss: 3.8021 (3.7280)  class_acc: 0.2917 (0.3471)  loss_scale: 32768.0000 (53701.4592)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.1472  max mem: 15572
Epoch: [37]  [ 560/2809]  eta: 0:21:48  lr: 0.000001  min_lr: 0.000000  loss: 3.8021 (3.7289)  class_acc: 0.2917 (0.3463)  loss_scale: 32768.0000 (53328.3137)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1405  max mem: 15572
Epoch: [37]  [ 570/2809]  eta: 0:21:41  lr: 0.000001  min_lr: 0.000000  loss: 3.7397 (3.7293)  class_acc: 0.3333 (0.3462)  loss_scale: 32768.0000 (52968.2382)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1342  max mem: 15572
Epoch: [37]  [ 580/2809]  eta: 0:21:34  lr: 0.000001  min_lr: 0.000000  loss: 3.7306 (3.7279)  class_acc: 0.3750 (0.3465)  loss_scale: 32768.0000 (52620.5577)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1108  max mem: 15572
Epoch: [37]  [ 590/2809]  eta: 0:21:27  lr: 0.000001  min_lr: 0.000000  loss: 3.6246 (3.7262)  class_acc: 0.3333 (0.3462)  loss_scale: 32768.0000 (52284.6430)  weight_decay: 0.0500 (0.0500)  time: 0.5390  data: 0.1026  max mem: 15572
Epoch: [37]  [ 600/2809]  eta: 0:21:21  lr: 0.000001  min_lr: 0.000000  loss: 3.7375 (3.7266)  class_acc: 0.2500 (0.3450)  loss_scale: 32768.0000 (51959.9068)  weight_decay: 0.0500 (0.0500)  time: 0.5550  data: 0.1117  max mem: 15572
[2025-01-16 07:59:12,481] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 07:59:12,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 610/2809]  eta: 0:21:11  lr: 0.000001  min_lr: 0.000000  loss: 3.8252 (3.7274)  class_acc: 0.3333 (0.3455)  loss_scale: 32768.0000 (52021.2111)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0822  max mem: 15572
Epoch: [37]  [ 620/2809]  eta: 0:21:04  lr: 0.000001  min_lr: 0.000000  loss: 3.8655 (3.7271)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (52238.8406)  weight_decay: 0.0500 (0.0500)  time: 0.4996  data: 0.0532  max mem: 15572
Epoch: [37]  [ 630/2809]  eta: 0:20:55  lr: 0.000001  min_lr: 0.000000  loss: 3.8363 (3.7265)  class_acc: 0.2500 (0.3446)  loss_scale: 65536.0000 (52449.5721)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0687  max mem: 15572
[2025-01-16 07:59:28,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104567
[2025-01-16 07:59:28,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 07:59:28,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [ 640/2809]  eta: 0:20:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7265 (3.7259)  class_acc: 0.2917 (0.3450)  loss_scale: 65536.0000 (52295.8877)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.1059  max mem: 15572
Epoch: [37]  [ 650/2809]  eta: 0:20:46  lr: 0.000001  min_lr: 0.000000  loss: 3.8198 (3.7261)  class_acc: 0.3750 (0.3458)  loss_scale: 32768.0000 (51995.9201)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1749  max mem: 15572
Epoch: [37]  [ 660/2809]  eta: 0:20:38  lr: 0.000001  min_lr: 0.000000  loss: 3.6801 (3.7236)  class_acc: 0.3750 (0.3462)  loss_scale: 32768.0000 (51705.0287)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1466  max mem: 15572
Epoch: [37]  [ 670/2809]  eta: 0:20:29  lr: 0.000001  min_lr: 0.000000  loss: 3.6416 (3.7218)  class_acc: 0.3750 (0.3459)  loss_scale: 32768.0000 (51422.8077)  weight_decay: 0.0500 (0.0500)  time: 0.5002  data: 0.0596  max mem: 15572
Epoch: [37]  [ 680/2809]  eta: 0:20:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7726 (3.7231)  class_acc: 0.2917 (0.3448)  loss_scale: 32768.0000 (51148.8752)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.1162  max mem: 15572
Epoch: [37]  [ 690/2809]  eta: 0:20:21  lr: 0.000001  min_lr: 0.000000  loss: 3.8324 (3.7241)  class_acc: 0.2917 (0.3452)  loss_scale: 32768.0000 (50882.8712)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.1973  max mem: 15572
Epoch: [37]  [ 700/2809]  eta: 0:20:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8151 (3.7234)  class_acc: 0.3333 (0.3453)  loss_scale: 32768.0000 (50624.4565)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1277  max mem: 15572
Epoch: [37]  [ 710/2809]  eta: 0:20:06  lr: 0.000001  min_lr: 0.000000  loss: 3.5559 (3.7193)  class_acc: 0.3333 (0.3456)  loss_scale: 32768.0000 (50373.3108)  weight_decay: 0.0500 (0.0500)  time: 0.5135  data: 0.0430  max mem: 15572
Epoch: [37]  [ 720/2809]  eta: 0:20:01  lr: 0.000001  min_lr: 0.000000  loss: 3.5829 (3.7196)  class_acc: 0.3333 (0.3452)  loss_scale: 32768.0000 (50129.1318)  weight_decay: 0.0500 (0.0500)  time: 0.5410  data: 0.0940  max mem: 15572
Epoch: [37]  [ 730/2809]  eta: 0:19:58  lr: 0.000001  min_lr: 0.000000  loss: 3.6450 (3.7185)  class_acc: 0.3333 (0.3456)  loss_scale: 32768.0000 (49891.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.2000  max mem: 15572
Epoch: [37]  [ 740/2809]  eta: 0:19:51  lr: 0.000001  min_lr: 0.000000  loss: 3.6053 (3.7170)  class_acc: 0.4167 (0.3465)  loss_scale: 32768.0000 (49660.5452)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.1703  max mem: 15572
Epoch: [37]  [ 750/2809]  eta: 0:19:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7242 (3.7202)  class_acc: 0.3333 (0.3456)  loss_scale: 32768.0000 (49435.6112)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.1916  max mem: 15572
Epoch: [37]  [ 760/2809]  eta: 0:19:40  lr: 0.000001  min_lr: 0.000000  loss: 3.7526 (3.7184)  class_acc: 0.2917 (0.3463)  loss_scale: 32768.0000 (49216.5887)  weight_decay: 0.0500 (0.0500)  time: 0.5795  data: 0.1354  max mem: 15572
[2025-01-16 08:00:41,889] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:00:41,889] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 770/2809]  eta: 0:19:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7130 (3.7190)  class_acc: 0.3333 (0.3462)  loss_scale: 32768.0000 (49343.2529)  weight_decay: 0.0500 (0.0500)  time: 0.4540  data: 0.0053  max mem: 15572
Epoch: [37]  [ 780/2809]  eta: 0:19:22  lr: 0.000001  min_lr: 0.000000  loss: 3.7504 (3.7196)  class_acc: 0.3333 (0.3460)  loss_scale: 65536.0000 (49550.5864)  weight_decay: 0.0500 (0.0500)  time: 0.4574  data: 0.0123  max mem: 15572
Epoch: [37]  [ 790/2809]  eta: 0:19:20  lr: 0.000001  min_lr: 0.000000  loss: 3.9644 (3.7233)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (49752.6776)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1362  max mem: 15572
Epoch: [37]  [ 800/2809]  eta: 0:19:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7677 (3.7211)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (49949.7228)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.2127  max mem: 15572
Epoch: [37]  [ 810/2809]  eta: 0:19:11  lr: 0.000001  min_lr: 0.000000  loss: 3.6168 (3.7206)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (50141.9088)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.1911  max mem: 15572
Epoch: [37]  [ 820/2809]  eta: 0:19:04  lr: 0.000001  min_lr: 0.000000  loss: 3.7160 (3.7203)  class_acc: 0.3333 (0.3452)  loss_scale: 65536.0000 (50329.4129)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1463  max mem: 15572
Epoch: [37]  [ 830/2809]  eta: 0:18:58  lr: 0.000001  min_lr: 0.000000  loss: 3.7465 (3.7220)  class_acc: 0.2917 (0.3448)  loss_scale: 65536.0000 (50512.4043)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1050  max mem: 15572
Epoch: [37]  [ 840/2809]  eta: 0:18:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6713 (3.7227)  class_acc: 0.2917 (0.3445)  loss_scale: 65536.0000 (50691.0440)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1324  max mem: 15572
Epoch: [37]  [ 850/2809]  eta: 0:18:47  lr: 0.000001  min_lr: 0.000000  loss: 3.7997 (3.7249)  class_acc: 0.2917 (0.3439)  loss_scale: 65536.0000 (50865.4853)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1462  max mem: 15572
Epoch: [37]  [ 860/2809]  eta: 0:18:40  lr: 0.000001  min_lr: 0.000000  loss: 3.6986 (3.7240)  class_acc: 0.2917 (0.3441)  loss_scale: 65536.0000 (51035.8746)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.1217  max mem: 15572
Epoch: [37]  [ 870/2809]  eta: 0:18:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6985 (3.7231)  class_acc: 0.3750 (0.3446)  loss_scale: 65536.0000 (51202.3513)  weight_decay: 0.0500 (0.0500)  time: 0.4969  data: 0.0530  max mem: 15572
Epoch: [37]  [ 880/2809]  eta: 0:18:27  lr: 0.000001  min_lr: 0.000000  loss: 3.6576 (3.7218)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (51365.0488)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0964  max mem: 15572
Epoch: [37]  [ 890/2809]  eta: 0:18:20  lr: 0.000001  min_lr: 0.000000  loss: 3.6533 (3.7211)  class_acc: 0.4167 (0.3460)  loss_scale: 65536.0000 (51524.0943)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1478  max mem: 15572
[2025-01-16 08:01:53,816] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:01:53,817] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:01:57,080] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104825
[2025-01-16 08:01:57,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:01:57,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [ 900/2809]  eta: 0:18:18  lr: 0.000001  min_lr: 0.000000  loss: 3.7408 (3.7212)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (51752.3463)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.2060  max mem: 15572
Epoch: [37]  [ 910/2809]  eta: 0:18:12  lr: 0.000001  min_lr: 0.000000  loss: 3.5950 (3.7187)  class_acc: 0.3333 (0.3467)  loss_scale: 65536.0000 (51903.6487)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.1952  max mem: 15572
Epoch: [37]  [ 920/2809]  eta: 0:18:04  lr: 0.000001  min_lr: 0.000000  loss: 3.8044 (3.7217)  class_acc: 0.3333 (0.3462)  loss_scale: 65536.0000 (52051.6656)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0794  max mem: 15572
Epoch: [37]  [ 930/2809]  eta: 0:17:58  lr: 0.000001  min_lr: 0.000000  loss: 3.8497 (3.7208)  class_acc: 0.2917 (0.3461)  loss_scale: 65536.0000 (52196.5027)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0917  max mem: 15572
Epoch: [37]  [ 940/2809]  eta: 0:17:52  lr: 0.000001  min_lr: 0.000000  loss: 3.5640 (3.7200)  class_acc: 0.3333 (0.3462)  loss_scale: 65536.0000 (52338.2614)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1189  max mem: 15572
Epoch: [37]  [ 950/2809]  eta: 0:17:47  lr: 0.000001  min_lr: 0.000000  loss: 3.4752 (3.7185)  class_acc: 0.3333 (0.3457)  loss_scale: 65536.0000 (52477.0389)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1447  max mem: 15572
Epoch: [37]  [ 960/2809]  eta: 0:17:43  lr: 0.000001  min_lr: 0.000000  loss: 3.8871 (3.7197)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (52612.9282)  weight_decay: 0.0500 (0.0500)  time: 0.6330  data: 0.2066  max mem: 15572
Epoch: [37]  [ 970/2809]  eta: 0:17:35  lr: 0.000001  min_lr: 0.000000  loss: 3.7962 (3.7200)  class_acc: 0.3333 (0.3450)  loss_scale: 65536.0000 (52746.0185)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1173  max mem: 15572
Epoch: [37]  [ 980/2809]  eta: 0:17:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7102 (3.7182)  class_acc: 0.2917 (0.3450)  loss_scale: 65536.0000 (52876.3955)  weight_decay: 0.0500 (0.0500)  time: 0.4960  data: 0.0544  max mem: 15572
Epoch: [37]  [ 990/2809]  eta: 0:17:23  lr: 0.000001  min_lr: 0.000000  loss: 3.5865 (3.7165)  class_acc: 0.3750 (0.3459)  loss_scale: 65536.0000 (53004.1413)  weight_decay: 0.0500 (0.0500)  time: 0.5651  data: 0.1248  max mem: 15572
Epoch: [37]  [1000/2809]  eta: 0:17:18  lr: 0.000001  min_lr: 0.000000  loss: 3.7095 (3.7181)  class_acc: 0.3750 (0.3454)  loss_scale: 65536.0000 (53129.3347)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1297  max mem: 15572
Epoch: [37]  [1010/2809]  eta: 0:17:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7717 (3.7183)  class_acc: 0.3333 (0.3455)  loss_scale: 65536.0000 (53252.0514)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1129  max mem: 15572
Epoch: [37]  [1020/2809]  eta: 0:17:05  lr: 0.000001  min_lr: 0.000000  loss: 3.6524 (3.7173)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (53372.3643)  weight_decay: 0.0500 (0.0500)  time: 0.5510  data: 0.1074  max mem: 15572
[2025-01-16 08:03:08,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:03:08,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:03:09,858] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104957
[2025-01-16 08:03:09,859] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:03:09,859] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1030/2809]  eta: 0:17:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7058 (3.7171)  class_acc: 0.3333 (0.3454)  loss_scale: 65536.0000 (53681.0398)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1486  max mem: 15572
[2025-01-16 08:03:20,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 104972
[2025-01-16 08:03:20,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:03:20,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1040/2809]  eta: 0:16:56  lr: 0.000001  min_lr: 0.000000  loss: 3.8031 (3.7172)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (53731.9654)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.1991  max mem: 15572
Epoch: [37]  [1050/2809]  eta: 0:16:51  lr: 0.000001  min_lr: 0.000000  loss: 3.9703 (3.7201)  class_acc: 0.2917 (0.3448)  loss_scale: 32768.0000 (53532.4986)  weight_decay: 0.0500 (0.0500)  time: 0.6218  data: 0.1731  max mem: 15572
Epoch: [37]  [1060/2809]  eta: 0:16:45  lr: 0.000001  min_lr: 0.000000  loss: 3.8353 (3.7199)  class_acc: 0.2917 (0.3448)  loss_scale: 32768.0000 (53336.7917)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1284  max mem: 15572
[2025-01-16 08:03:34,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=105000, skipped=699, lr=[6.70088453430434e-09, 6.70088453430434e-09, 9.572692191863344e-09, 9.572692191863344e-09, 1.367527455980478e-08, 1.367527455980478e-08, 1.9536106514006828e-08, 1.9536106514006828e-08, 2.7908723591438326e-08, 2.7908723591438326e-08, 3.986960513062619e-08, 3.986960513062619e-08, 5.695657875803741e-08, 5.695657875803741e-08, 8.13665410829106e-08, 8.13665410829106e-08, 1.1623791583272941e-07, 1.1623791583272941e-07, 1.6605416547532776e-07, 1.6605416547532776e-07, 2.3722023639332537e-07, 2.3722023639332537e-07, 3.3888605199046486e-07, 3.3888605199046486e-07, 4.841229314149498e-07, 4.841229314149498e-07, 6.916041877356426e-07, 6.916041877356426e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:03:34,789] [INFO] [timer.py:260:stop] epoch=0/micro_step=105000/global_step=105000, RunningAvgSamplesPerSec=28.581665568023443, CurrSamplesPerSec=27.04845023312604, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [1070/2809]  eta: 0:16:39  lr: 0.000001  min_lr: 0.000000  loss: 3.9366 (3.7227)  class_acc: 0.2917 (0.3439)  loss_scale: 32768.0000 (53144.7395)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.1184  max mem: 15572
Epoch: [37]  [1080/2809]  eta: 0:16:32  lr: 0.000001  min_lr: 0.000000  loss: 3.9761 (3.7223)  class_acc: 0.2500 (0.3438)  loss_scale: 32768.0000 (52956.2405)  weight_decay: 0.0500 (0.0500)  time: 0.5317  data: 0.0972  max mem: 15572
Epoch: [37]  [1090/2809]  eta: 0:16:28  lr: 0.000001  min_lr: 0.000000  loss: 3.6607 (3.7230)  class_acc: 0.3750 (0.3439)  loss_scale: 32768.0000 (52771.1971)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.1467  max mem: 15572
Epoch: [37]  [1100/2809]  eta: 0:16:22  lr: 0.000001  min_lr: 0.000000  loss: 3.6455 (3.7209)  class_acc: 0.3750 (0.3450)  loss_scale: 32768.0000 (52589.5150)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.1883  max mem: 15572
Epoch: [37]  [1110/2809]  eta: 0:16:15  lr: 0.000001  min_lr: 0.000000  loss: 3.7369 (3.7215)  class_acc: 0.3333 (0.3447)  loss_scale: 32768.0000 (52411.1035)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0958  max mem: 15572
Epoch: [37]  [1120/2809]  eta: 0:16:10  lr: 0.000001  min_lr: 0.000000  loss: 3.7453 (3.7203)  class_acc: 0.2917 (0.3451)  loss_scale: 32768.0000 (52235.8751)  weight_decay: 0.0500 (0.0500)  time: 0.5582  data: 0.1096  max mem: 15572
Epoch: [37]  [1130/2809]  eta: 0:16:04  lr: 0.000001  min_lr: 0.000000  loss: 3.7111 (3.7198)  class_acc: 0.4167 (0.3458)  loss_scale: 32768.0000 (52063.7454)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.1581  max mem: 15572
Epoch: [37]  [1140/2809]  eta: 0:16:00  lr: 0.000001  min_lr: 0.000000  loss: 3.5931 (3.7197)  class_acc: 0.3750 (0.3461)  loss_scale: 32768.0000 (51894.6328)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.1861  max mem: 15572
Epoch: [37]  [1150/2809]  eta: 0:15:52  lr: 0.000001  min_lr: 0.000000  loss: 3.6121 (3.7193)  class_acc: 0.3333 (0.3462)  loss_scale: 32768.0000 (51728.4587)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1301  max mem: 15572
Epoch: [37]  [1160/2809]  eta: 0:15:47  lr: 0.000001  min_lr: 0.000000  loss: 3.6121 (3.7190)  class_acc: 0.3750 (0.3462)  loss_scale: 32768.0000 (51565.1473)  weight_decay: 0.0500 (0.0500)  time: 0.5355  data: 0.0940  max mem: 15572
[2025-01-16 08:04:33,907] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:04:33,907] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1170/2809]  eta: 0:15:41  lr: 0.000001  min_lr: 0.000000  loss: 3.8066 (3.7197)  class_acc: 0.2917 (0.3456)  loss_scale: 32768.0000 (51488.5739)  weight_decay: 0.0500 (0.0500)  time: 0.5709  data: 0.1339  max mem: 15572
Epoch: [37]  [1180/2809]  eta: 0:15:35  lr: 0.000001  min_lr: 0.000000  loss: 3.7384 (3.7196)  class_acc: 0.2917 (0.3452)  loss_scale: 65536.0000 (51607.5191)  weight_decay: 0.0500 (0.0500)  time: 0.5291  data: 0.0934  max mem: 15572
Epoch: [37]  [1190/2809]  eta: 0:15:28  lr: 0.000001  min_lr: 0.000000  loss: 3.6961 (3.7198)  class_acc: 0.2917 (0.3451)  loss_scale: 65536.0000 (51724.4668)  weight_decay: 0.0500 (0.0500)  time: 0.5396  data: 0.1046  max mem: 15572
Epoch: [37]  [1200/2809]  eta: 0:15:22  lr: 0.000001  min_lr: 0.000000  loss: 3.9673 (3.7220)  class_acc: 0.2500 (0.3445)  loss_scale: 65536.0000 (51839.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.1020  max mem: 15572
Epoch: [37]  [1210/2809]  eta: 0:15:15  lr: 0.000001  min_lr: 0.000000  loss: 4.0094 (3.7230)  class_acc: 0.2500 (0.3441)  loss_scale: 65536.0000 (51952.5681)  weight_decay: 0.0500 (0.0500)  time: 0.4859  data: 0.0426  max mem: 15572
Epoch: [37]  [1220/2809]  eta: 0:15:07  lr: 0.000001  min_lr: 0.000000  loss: 3.6785 (3.7227)  class_acc: 0.3333 (0.3440)  loss_scale: 65536.0000 (52063.8165)  weight_decay: 0.0500 (0.0500)  time: 0.4467  data: 0.0006  max mem: 15572
Epoch: [37]  [1230/2809]  eta: 0:15:02  lr: 0.000001  min_lr: 0.000000  loss: 3.6442 (3.7223)  class_acc: 0.3750 (0.3444)  loss_scale: 65536.0000 (52173.2575)  weight_decay: 0.0500 (0.0500)  time: 0.5199  data: 0.0749  max mem: 15572
Epoch: [37]  [1240/2809]  eta: 0:14:55  lr: 0.000001  min_lr: 0.000000  loss: 3.6218 (3.7214)  class_acc: 0.3750 (0.3447)  loss_scale: 65536.0000 (52280.9347)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1159  max mem: 15572
Epoch: [37]  [1250/2809]  eta: 0:14:48  lr: 0.000001  min_lr: 0.000000  loss: 3.4532 (3.7204)  class_acc: 0.3750 (0.3445)  loss_scale: 65536.0000 (52386.8905)  weight_decay: 0.0500 (0.0500)  time: 0.4844  data: 0.0637  max mem: 15572
Epoch: [37]  [1260/2809]  eta: 0:14:44  lr: 0.000001  min_lr: 0.000000  loss: 3.4682 (3.7190)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (52491.1657)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1599  max mem: 15572
Epoch: [37]  [1270/2809]  eta: 0:14:40  lr: 0.000001  min_lr: 0.000000  loss: 3.5348 (3.7195)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (52593.8002)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.2801  max mem: 15572
Epoch: [37]  [1280/2809]  eta: 0:14:35  lr: 0.000001  min_lr: 0.000000  loss: 3.8868 (3.7207)  class_acc: 0.2500 (0.3441)  loss_scale: 65536.0000 (52694.8322)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.2442  max mem: 15572
Epoch: [37]  [1290/2809]  eta: 0:14:29  lr: 0.000001  min_lr: 0.000000  loss: 3.9322 (3.7209)  class_acc: 0.2500 (0.3437)  loss_scale: 65536.0000 (52794.2990)  weight_decay: 0.0500 (0.0500)  time: 0.5750  data: 0.1463  max mem: 15572
[2025-01-16 08:05:43,916] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:05:43,916] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:05:44,896] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105230
[2025-01-16 08:05:44,896] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:05:44,897] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1300/2809]  eta: 0:14:23  lr: 0.000001  min_lr: 0.000000  loss: 3.7557 (3.7213)  class_acc: 0.2917 (0.3439)  loss_scale: 65536.0000 (52942.6103)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0718  max mem: 15572
Epoch: [37]  [1310/2809]  eta: 0:14:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6845 (3.7202)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (53038.6697)  weight_decay: 0.0500 (0.0500)  time: 0.5439  data: 0.0893  max mem: 15572
Epoch: [37]  [1320/2809]  eta: 0:14:12  lr: 0.000001  min_lr: 0.000000  loss: 3.6845 (3.7203)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (53133.2748)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.1351  max mem: 15572
Epoch: [37]  [1330/2809]  eta: 0:14:07  lr: 0.000001  min_lr: 0.000000  loss: 3.6860 (3.7208)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (53226.4583)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.1663  max mem: 15572
Epoch: [37]  [1340/2809]  eta: 0:14:01  lr: 0.000001  min_lr: 0.000000  loss: 3.6721 (3.7196)  class_acc: 0.3333 (0.3437)  loss_scale: 65536.0000 (53318.2521)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1651  max mem: 15572
Epoch: [37]  [1350/2809]  eta: 0:13:56  lr: 0.000001  min_lr: 0.000000  loss: 3.7891 (3.7208)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (53408.6869)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.1270  max mem: 15572
Epoch: [37]  [1360/2809]  eta: 0:13:49  lr: 0.000001  min_lr: 0.000000  loss: 3.5516 (3.7188)  class_acc: 0.3333 (0.3437)  loss_scale: 65536.0000 (53497.7928)  weight_decay: 0.0500 (0.0500)  time: 0.5454  data: 0.0960  max mem: 15572
Epoch: [37]  [1370/2809]  eta: 0:13:44  lr: 0.000001  min_lr: 0.000000  loss: 3.4340 (3.7187)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (53585.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5913  data: 0.1358  max mem: 15572
Epoch: [37]  [1380/2809]  eta: 0:13:38  lr: 0.000001  min_lr: 0.000000  loss: 3.6100 (3.7173)  class_acc: 0.2917 (0.3436)  loss_scale: 65536.0000 (53672.1332)  weight_decay: 0.0500 (0.0500)  time: 0.5728  data: 0.1142  max mem: 15572
Epoch: [37]  [1390/2809]  eta: 0:13:32  lr: 0.000001  min_lr: 0.000000  loss: 3.6535 (3.7174)  class_acc: 0.2500 (0.3432)  loss_scale: 65536.0000 (53757.4234)  weight_decay: 0.0500 (0.0500)  time: 0.5289  data: 0.0875  max mem: 15572
Epoch: [37]  [1400/2809]  eta: 0:13:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7777 (3.7177)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (53841.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5768  data: 0.1378  max mem: 15572
Epoch: [37]  [1410/2809]  eta: 0:13:21  lr: 0.000001  min_lr: 0.000000  loss: 3.7777 (3.7177)  class_acc: 0.2917 (0.3430)  loss_scale: 65536.0000 (53924.3770)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.1558  max mem: 15572
Epoch: [37]  [1420/2809]  eta: 0:13:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8315 (3.7175)  class_acc: 0.3333 (0.3432)  loss_scale: 65536.0000 (54006.0915)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1305  max mem: 15572
[2025-01-16 08:06:58,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:06:58,969] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:07:01,077] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105361
[2025-01-16 08:07:01,078] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:07:01,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1430/2809]  eta: 0:13:09  lr: 0.000001  min_lr: 0.000000  loss: 3.8065 (3.7175)  class_acc: 0.3750 (0.3433)  loss_scale: 65536.0000 (54178.2586)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1015  max mem: 15572
Epoch: [37]  [1440/2809]  eta: 0:13:03  lr: 0.000001  min_lr: 0.000000  loss: 3.6243 (3.7172)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (54257.0770)  weight_decay: 0.0500 (0.0500)  time: 0.5552  data: 0.1091  max mem: 15572
Epoch: [37]  [1450/2809]  eta: 0:12:57  lr: 0.000001  min_lr: 0.000000  loss: 3.5827 (3.7167)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (54334.8091)  weight_decay: 0.0500 (0.0500)  time: 0.5389  data: 0.1151  max mem: 15572
[2025-01-16 08:07:13,084] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105384
[2025-01-16 08:07:13,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:07:13,085] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [1460/2809]  eta: 0:12:51  lr: 0.000001  min_lr: 0.000000  loss: 3.6410 (3.7170)  class_acc: 0.3750 (0.3433)  loss_scale: 32768.0000 (54187.1923)  weight_decay: 0.0500 (0.0500)  time: 0.5468  data: 0.1189  max mem: 15572
Epoch: [37]  [1470/2809]  eta: 0:12:46  lr: 0.000001  min_lr: 0.000000  loss: 3.9560 (3.7186)  class_acc: 0.2500 (0.3427)  loss_scale: 32768.0000 (54041.5826)  weight_decay: 0.0500 (0.0500)  time: 0.5787  data: 0.1406  max mem: 15572
Epoch: [37]  [1480/2809]  eta: 0:12:40  lr: 0.000001  min_lr: 0.000000  loss: 3.9919 (3.7201)  class_acc: 0.2500 (0.3422)  loss_scale: 32768.0000 (53897.9392)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1680  max mem: 15572
Epoch: [37]  [1490/2809]  eta: 0:12:34  lr: 0.000001  min_lr: 0.000000  loss: 3.9419 (3.7201)  class_acc: 0.2500 (0.3419)  loss_scale: 32768.0000 (53756.2227)  weight_decay: 0.0500 (0.0500)  time: 0.5717  data: 0.1215  max mem: 15572
Epoch: [37]  [1500/2809]  eta: 0:12:28  lr: 0.000001  min_lr: 0.000000  loss: 3.6723 (3.7213)  class_acc: 0.3333 (0.3421)  loss_scale: 32768.0000 (53616.3944)  weight_decay: 0.0500 (0.0500)  time: 0.5175  data: 0.0612  max mem: 15572
Epoch: [37]  [1510/2809]  eta: 0:12:22  lr: 0.000001  min_lr: 0.000000  loss: 3.6590 (3.7213)  class_acc: 0.3750 (0.3422)  loss_scale: 32768.0000 (53478.4169)  weight_decay: 0.0500 (0.0500)  time: 0.5084  data: 0.0508  max mem: 15572
Epoch: [37]  [1520/2809]  eta: 0:12:16  lr: 0.000001  min_lr: 0.000000  loss: 3.7282 (3.7215)  class_acc: 0.3333 (0.3421)  loss_scale: 32768.0000 (53342.2538)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0739  max mem: 15572
Epoch: [37]  [1530/2809]  eta: 0:12:10  lr: 0.000001  min_lr: 0.000000  loss: 3.5609 (3.7206)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (53207.8694)  weight_decay: 0.0500 (0.0500)  time: 0.5350  data: 0.0782  max mem: 15572
Epoch: [37]  [1540/2809]  eta: 0:12:04  lr: 0.000001  min_lr: 0.000000  loss: 3.5173 (3.7202)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (53075.2291)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.1137  max mem: 15572
Epoch: [37]  [1550/2809]  eta: 0:11:59  lr: 0.000001  min_lr: 0.000000  loss: 3.6871 (3.7211)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (52944.2992)  weight_decay: 0.0500 (0.0500)  time: 0.5842  data: 0.1241  max mem: 15572
Epoch: [37]  [1560/2809]  eta: 0:11:53  lr: 0.000001  min_lr: 0.000000  loss: 3.8353 (3.7205)  class_acc: 0.2917 (0.3420)  loss_scale: 32768.0000 (52815.0468)  weight_decay: 0.0500 (0.0500)  time: 0.5826  data: 0.1233  max mem: 15572
Epoch: [37]  [1570/2809]  eta: 0:11:47  lr: 0.000001  min_lr: 0.000000  loss: 3.6826 (3.7201)  class_acc: 0.3750 (0.3423)  loss_scale: 32768.0000 (52687.4398)  weight_decay: 0.0500 (0.0500)  time: 0.5783  data: 0.1419  max mem: 15572
[2025-01-16 08:08:25,515] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:08:25,516] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [1580/2809]  eta: 0:11:42  lr: 0.000001  min_lr: 0.000000  loss: 3.7077 (3.7194)  class_acc: 0.3750 (0.3425)  loss_scale: 32768.0000 (52582.1733)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1183  max mem: 15572
Epoch: [37]  [1590/2809]  eta: 0:11:35  lr: 0.000001  min_lr: 0.000000  loss: 3.8745 (3.7205)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (52663.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5043  data: 0.0703  max mem: 15572
Epoch: [37]  [1600/2809]  eta: 0:11:29  lr: 0.000001  min_lr: 0.000000  loss: 3.9660 (3.7206)  class_acc: 0.3333 (0.3424)  loss_scale: 65536.0000 (52743.9950)  weight_decay: 0.0500 (0.0500)  time: 0.5179  data: 0.0910  max mem: 15572
Epoch: [37]  [1610/2809]  eta: 0:11:23  lr: 0.000001  min_lr: 0.000000  loss: 3.7940 (3.7208)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (52823.3991)  weight_decay: 0.0500 (0.0500)  time: 0.5702  data: 0.1294  max mem: 15572
Epoch: [37]  [1620/2809]  eta: 0:11:18  lr: 0.000001  min_lr: 0.000000  loss: 3.7940 (3.7219)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (52901.8236)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1278  max mem: 15572
Epoch: [37]  [1630/2809]  eta: 0:11:13  lr: 0.000001  min_lr: 0.000000  loss: 3.7616 (3.7223)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (52979.2863)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1625  max mem: 15572
Epoch: [37]  [1640/2809]  eta: 0:11:07  lr: 0.000001  min_lr: 0.000000  loss: 3.7889 (3.7220)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (53055.8050)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1812  max mem: 15572
Epoch: [37]  [1650/2809]  eta: 0:11:01  lr: 0.000001  min_lr: 0.000000  loss: 3.7069 (3.7215)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (53131.3967)  weight_decay: 0.0500 (0.0500)  time: 0.5242  data: 0.0945  max mem: 15572
Epoch: [37]  [1660/2809]  eta: 0:10:55  lr: 0.000001  min_lr: 0.000000  loss: 3.7069 (3.7216)  class_acc: 0.3750 (0.3424)  loss_scale: 65536.0000 (53206.0783)  weight_decay: 0.0500 (0.0500)  time: 0.4949  data: 0.0611  max mem: 15572
Epoch: [37]  [1670/2809]  eta: 0:10:48  lr: 0.000001  min_lr: 0.000000  loss: 3.6722 (3.7213)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (53279.8659)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0740  max mem: 15572
Epoch: [37]  [1680/2809]  eta: 0:10:43  lr: 0.000001  min_lr: 0.000000  loss: 3.5465 (3.7211)  class_acc: 0.4167 (0.3430)  loss_scale: 65536.0000 (53352.7757)  weight_decay: 0.0500 (0.0500)  time: 0.5456  data: 0.1127  max mem: 15572
Epoch: [37]  [1690/2809]  eta: 0:10:37  lr: 0.000001  min_lr: 0.000000  loss: 3.7463 (3.7216)  class_acc: 0.3750 (0.3431)  loss_scale: 65536.0000 (53424.8232)  weight_decay: 0.0500 (0.0500)  time: 0.5803  data: 0.1618  max mem: 15572
Epoch: [37]  [1700/2809]  eta: 0:10:31  lr: 0.000001  min_lr: 0.000000  loss: 3.8272 (3.7210)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (53496.0235)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1205  max mem: 15572
[2025-01-16 08:09:36,057] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:09:36,057] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:09:36,447] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105642
[2025-01-16 08:09:36,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:09:36,447] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1710/2809]  eta: 0:10:25  lr: 0.000001  min_lr: 0.000000  loss: 3.4839 (3.7201)  class_acc: 0.3750 (0.3433)  loss_scale: 65536.0000 (53604.6943)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.1138  max mem: 15572
Epoch: [37]  [1720/2809]  eta: 0:10:20  lr: 0.000001  min_lr: 0.000000  loss: 3.7227 (3.7194)  class_acc: 0.3750 (0.3433)  loss_scale: 65536.0000 (53674.0221)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1321  max mem: 15572
Epoch: [37]  [1730/2809]  eta: 0:10:14  lr: 0.000001  min_lr: 0.000000  loss: 3.7890 (3.7201)  class_acc: 0.3333 (0.3432)  loss_scale: 65536.0000 (53742.5488)  weight_decay: 0.0500 (0.0500)  time: 0.5694  data: 0.1369  max mem: 15572
Epoch: [37]  [1740/2809]  eta: 0:10:09  lr: 0.000001  min_lr: 0.000000  loss: 3.6092 (3.7192)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (53810.2883)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1975  max mem: 15572
Epoch: [37]  [1750/2809]  eta: 0:10:03  lr: 0.000001  min_lr: 0.000000  loss: 3.5963 (3.7191)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (53877.2541)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1451  max mem: 15572
Epoch: [37]  [1760/2809]  eta: 0:09:57  lr: 0.000001  min_lr: 0.000000  loss: 3.6593 (3.7191)  class_acc: 0.2500 (0.3432)  loss_scale: 65536.0000 (53943.4594)  weight_decay: 0.0500 (0.0500)  time: 0.5302  data: 0.0867  max mem: 15572
Epoch: [37]  [1770/2809]  eta: 0:09:51  lr: 0.000001  min_lr: 0.000000  loss: 3.8021 (3.7199)  class_acc: 0.2917 (0.3429)  loss_scale: 65536.0000 (54008.9170)  weight_decay: 0.0500 (0.0500)  time: 0.5798  data: 0.1211  max mem: 15572
Epoch: [37]  [1780/2809]  eta: 0:09:45  lr: 0.000001  min_lr: 0.000000  loss: 3.7313 (3.7197)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54073.6395)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0702  max mem: 15572
Epoch: [37]  [1790/2809]  eta: 0:09:40  lr: 0.000001  min_lr: 0.000000  loss: 3.6535 (3.7198)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (54137.6393)  weight_decay: 0.0500 (0.0500)  time: 0.5693  data: 0.1433  max mem: 15572
Epoch: [37]  [1800/2809]  eta: 0:09:34  lr: 0.000001  min_lr: 0.000000  loss: 3.8015 (3.7194)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (54200.9284)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.1799  max mem: 15572
Epoch: [37]  [1810/2809]  eta: 0:09:28  lr: 0.000001  min_lr: 0.000000  loss: 3.8015 (3.7193)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54263.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5537  data: 0.1127  max mem: 15572
Epoch: [37]  [1820/2809]  eta: 0:09:23  lr: 0.000001  min_lr: 0.000000  loss: 3.8761 (3.7201)  class_acc: 0.3333 (0.3425)  loss_scale: 65536.0000 (54325.4212)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.1014  max mem: 15572
Epoch: [37]  [1830/2809]  eta: 0:09:17  lr: 0.000001  min_lr: 0.000000  loss: 3.9009 (3.7205)  class_acc: 0.3333 (0.3426)  loss_scale: 65536.0000 (54386.6477)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1561  max mem: 15572
[2025-01-16 08:10:50,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:10:50,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:10:51,513] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105773
[2025-01-16 08:10:51,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:10:51,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1840/2809]  eta: 0:09:12  lr: 0.000001  min_lr: 0.000000  loss: 3.9009 (3.7213)  class_acc: 0.3333 (0.3425)  loss_scale: 65536.0000 (54518.4052)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1894  max mem: 15572
Epoch: [37]  [1850/2809]  eta: 0:09:06  lr: 0.000001  min_lr: 0.000000  loss: 3.7911 (3.7206)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (54577.9276)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1514  max mem: 15572
Epoch: [37]  [1860/2809]  eta: 0:09:00  lr: 0.000001  min_lr: 0.000000  loss: 3.7521 (3.7202)  class_acc: 0.3750 (0.3429)  loss_scale: 65536.0000 (54636.8103)  weight_decay: 0.0500 (0.0500)  time: 0.5460  data: 0.1050  max mem: 15572
Epoch: [37]  [1870/2809]  eta: 0:08:54  lr: 0.000001  min_lr: 0.000000  loss: 3.7043 (3.7196)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54695.0636)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1139  max mem: 15572
Epoch: [37]  [1880/2809]  eta: 0:08:48  lr: 0.000001  min_lr: 0.000000  loss: 3.7649 (3.7206)  class_acc: 0.3333 (0.3426)  loss_scale: 65536.0000 (54752.6975)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1225  max mem: 15572
Epoch: [37]  [1890/2809]  eta: 0:08:43  lr: 0.000001  min_lr: 0.000000  loss: 4.0521 (3.7218)  class_acc: 0.2500 (0.3425)  loss_scale: 65536.0000 (54809.7218)  weight_decay: 0.0500 (0.0500)  time: 0.5482  data: 0.1189  max mem: 15572
Epoch: [37]  [1900/2809]  eta: 0:08:37  lr: 0.000001  min_lr: 0.000000  loss: 3.8728 (3.7220)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54866.1462)  weight_decay: 0.0500 (0.0500)  time: 0.5233  data: 0.0706  max mem: 15572
Epoch: [37]  [1910/2809]  eta: 0:08:31  lr: 0.000001  min_lr: 0.000000  loss: 3.7869 (3.7220)  class_acc: 0.3750 (0.3429)  loss_scale: 65536.0000 (54921.9801)  weight_decay: 0.0500 (0.0500)  time: 0.4743  data: 0.0253  max mem: 15572
Epoch: [37]  [1920/2809]  eta: 0:08:25  lr: 0.000001  min_lr: 0.000000  loss: 3.6957 (3.7216)  class_acc: 0.3333 (0.3429)  loss_scale: 65536.0000 (54977.2327)  weight_decay: 0.0500 (0.0500)  time: 0.5349  data: 0.0946  max mem: 15572
Epoch: [37]  [1930/2809]  eta: 0:08:19  lr: 0.000001  min_lr: 0.000000  loss: 3.7378 (3.7220)  class_acc: 0.2917 (0.3425)  loss_scale: 65536.0000 (55031.9130)  weight_decay: 0.0500 (0.0500)  time: 0.5503  data: 0.1122  max mem: 15572
Epoch: [37]  [1940/2809]  eta: 0:08:14  lr: 0.000001  min_lr: 0.000000  loss: 3.7715 (3.7224)  class_acc: 0.2500 (0.3422)  loss_scale: 65536.0000 (55086.0299)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1639  max mem: 15572
Epoch: [37]  [1950/2809]  eta: 0:08:08  lr: 0.000001  min_lr: 0.000000  loss: 3.7690 (3.7223)  class_acc: 0.2917 (0.3423)  loss_scale: 65536.0000 (55139.5920)  weight_decay: 0.0500 (0.0500)  time: 0.6462  data: 0.2158  max mem: 15572
Epoch: [37]  [1960/2809]  eta: 0:08:03  lr: 0.000001  min_lr: 0.000000  loss: 3.8965 (3.7232)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (55192.6079)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.1865  max mem: 15572
[2025-01-16 08:12:04,339] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:12:04,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [1970/2809]  eta: 0:07:57  lr: 0.000001  min_lr: 0.000000  loss: 3.9152 (3.7231)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (55311.5860)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.1458  max mem: 15572
[2025-01-16 08:12:05,296] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 105904
[2025-01-16 08:12:05,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:12:05,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [1980/2809]  eta: 0:07:51  lr: 0.000001  min_lr: 0.000000  loss: 3.8557 (3.7238)  class_acc: 0.2917 (0.3421)  loss_scale: 65536.0000 (55363.1984)  weight_decay: 0.0500 (0.0500)  time: 0.5308  data: 0.0903  max mem: 15572
Epoch: [37]  [1990/2809]  eta: 0:07:46  lr: 0.000001  min_lr: 0.000000  loss: 3.8557 (3.7234)  class_acc: 0.2917 (0.3422)  loss_scale: 65536.0000 (55414.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5414  data: 0.1003  max mem: 15572
Epoch: [37]  [2000/2809]  eta: 0:07:40  lr: 0.000001  min_lr: 0.000000  loss: 3.5802 (3.7227)  class_acc: 0.3750 (0.3424)  loss_scale: 65536.0000 (55464.8756)  weight_decay: 0.0500 (0.0500)  time: 0.5724  data: 0.1355  max mem: 15572
Epoch: [37]  [2010/2809]  eta: 0:07:34  lr: 0.000001  min_lr: 0.000000  loss: 3.5802 (3.7222)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (55514.9557)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1476  max mem: 15572
Epoch: [37]  [2020/2809]  eta: 0:07:29  lr: 0.000001  min_lr: 0.000000  loss: 3.6819 (3.7223)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (55564.5403)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1332  max mem: 15572
Epoch: [37]  [2030/2809]  eta: 0:07:23  lr: 0.000001  min_lr: 0.000000  loss: 3.6819 (3.7221)  class_acc: 0.3750 (0.3429)  loss_scale: 65536.0000 (55613.6366)  weight_decay: 0.0500 (0.0500)  time: 0.5247  data: 0.0817  max mem: 15572
Epoch: [37]  [2040/2809]  eta: 0:07:17  lr: 0.000001  min_lr: 0.000000  loss: 3.6490 (3.7222)  class_acc: 0.2917 (0.3427)  loss_scale: 65536.0000 (55662.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5762  data: 0.1268  max mem: 15572
Epoch: [37]  [2050/2809]  eta: 0:07:12  lr: 0.000001  min_lr: 0.000000  loss: 3.8024 (3.7223)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (55710.3930)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.2163  max mem: 15572
Epoch: [37]  [2060/2809]  eta: 0:07:06  lr: 0.000001  min_lr: 0.000000  loss: 3.8069 (3.7220)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (55758.0670)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1616  max mem: 15572
[2025-01-16 08:12:59,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=106000, skipped=705, lr=[5.12437822468119e-09, 5.12437822468119e-09, 7.320540320973129e-09, 7.320540320973129e-09, 1.045791474424733e-08, 1.045791474424733e-08, 1.4939878206067616e-08, 1.4939878206067616e-08, 2.1342683151525164e-08, 2.1342683151525164e-08, 3.048954735932166e-08, 3.048954735932166e-08, 4.355649622760238e-08, 4.355649622760238e-08, 6.222356603943198e-08, 6.222356603943198e-08, 8.889080862775997e-08, 8.889080862775997e-08, 1.2698686946822854e-07, 1.2698686946822854e-07, 1.8140981352604078e-07, 1.8140981352604078e-07, 2.5915687646577256e-07, 2.5915687646577256e-07, 3.7022410923681794e-07, 3.7022410923681794e-07, 5.288915846240257e-07, 5.288915846240257e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:12:59,386] [INFO] [timer.py:260:stop] epoch=0/micro_step=106000/global_step=106000, RunningAvgSamplesPerSec=28.58430872104032, CurrSamplesPerSec=22.800675707814808, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [37]  [2070/2809]  eta: 0:07:00  lr: 0.000001  min_lr: 0.000000  loss: 3.6410 (3.7217)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (55805.2805)  weight_decay: 0.0500 (0.0500)  time: 0.5430  data: 0.0919  max mem: 15572
Epoch: [37]  [2080/2809]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 3.6168 (3.7217)  class_acc: 0.3750 (0.3429)  loss_scale: 65536.0000 (55852.0404)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1191  max mem: 15572
Epoch: [37]  [2090/2809]  eta: 0:06:49  lr: 0.000001  min_lr: 0.000000  loss: 3.7105 (3.7213)  class_acc: 0.3750 (0.3430)  loss_scale: 65536.0000 (55898.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1071  max mem: 15572
[2025-01-16 08:13:18,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:13:18,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [2100/2809]  eta: 0:06:43  lr: 0.000001  min_lr: 0.000000  loss: 3.6109 (3.7203)  class_acc: 0.3750 (0.3434)  loss_scale: 65536.0000 (55975.4174)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1241  max mem: 15572
[2025-01-16 08:13:19,342] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106034
[2025-01-16 08:13:19,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:13:19,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [2110/2809]  eta: 0:06:38  lr: 0.000001  min_lr: 0.000000  loss: 3.5044 (3.7201)  class_acc: 0.3750 (0.3435)  loss_scale: 65536.0000 (56020.7068)  weight_decay: 0.0500 (0.0500)  time: 0.6220  data: 0.1817  max mem: 15572
Epoch: [37]  [2120/2809]  eta: 0:06:32  lr: 0.000001  min_lr: 0.000000  loss: 3.5294 (3.7198)  class_acc: 0.4167 (0.3437)  loss_scale: 65536.0000 (56065.5691)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1447  max mem: 15572
Epoch: [37]  [2130/2809]  eta: 0:06:26  lr: 0.000001  min_lr: 0.000000  loss: 3.7126 (3.7203)  class_acc: 0.3750 (0.3437)  loss_scale: 65536.0000 (56110.0103)  weight_decay: 0.0500 (0.0500)  time: 0.5378  data: 0.0990  max mem: 15572
Epoch: [37]  [2140/2809]  eta: 0:06:21  lr: 0.000001  min_lr: 0.000000  loss: 3.8268 (3.7208)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (56154.0364)  weight_decay: 0.0500 (0.0500)  time: 0.5789  data: 0.1278  max mem: 15572
Epoch: [37]  [2150/2809]  eta: 0:06:15  lr: 0.000001  min_lr: 0.000000  loss: 3.8280 (3.7206)  class_acc: 0.2917 (0.3435)  loss_scale: 65536.0000 (56197.6532)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1010  max mem: 15572
Epoch: [37]  [2160/2809]  eta: 0:06:09  lr: 0.000001  min_lr: 0.000000  loss: 3.6992 (3.7195)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (56240.8663)  weight_decay: 0.0500 (0.0500)  time: 0.5202  data: 0.0801  max mem: 15572
Epoch: [37]  [2170/2809]  eta: 0:06:03  lr: 0.000001  min_lr: 0.000000  loss: 3.6579 (3.7198)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (56283.6813)  weight_decay: 0.0500 (0.0500)  time: 0.5181  data: 0.0699  max mem: 15572
Epoch: [37]  [2180/2809]  eta: 0:05:57  lr: 0.000001  min_lr: 0.000000  loss: 3.8555 (3.7209)  class_acc: 0.3333 (0.3434)  loss_scale: 65536.0000 (56326.1036)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.0988  max mem: 15572
Epoch: [37]  [2190/2809]  eta: 0:05:52  lr: 0.000001  min_lr: 0.000000  loss: 3.9153 (3.7209)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (56368.1387)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1631  max mem: 15572
Epoch: [37]  [2200/2809]  eta: 0:05:46  lr: 0.000001  min_lr: 0.000000  loss: 3.6792 (3.7204)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (56409.7919)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1036  max mem: 15572
Epoch: [37]  [2210/2809]  eta: 0:05:40  lr: 0.000001  min_lr: 0.000000  loss: 3.5819 (3.7192)  class_acc: 0.3333 (0.3435)  loss_scale: 65536.0000 (56451.0683)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0883  max mem: 15572
Epoch: [37]  [2220/2809]  eta: 0:05:35  lr: 0.000001  min_lr: 0.000000  loss: 3.5819 (3.7196)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (56491.9730)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0932  max mem: 15572
[2025-01-16 08:14:31,717] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:14:31,717] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [2230/2809]  eta: 0:05:29  lr: 0.000001  min_lr: 0.000000  loss: 3.7994 (3.7192)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (56561.8861)  weight_decay: 0.0500 (0.0500)  time: 0.5476  data: 0.0827  max mem: 15572
[2025-01-16 08:14:32,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106164
[2025-01-16 08:14:32,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:14:32,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [2240/2809]  eta: 0:05:23  lr: 0.000001  min_lr: 0.000000  loss: 3.7658 (3.7199)  class_acc: 0.2917 (0.3429)  loss_scale: 65536.0000 (56601.9313)  weight_decay: 0.0500 (0.0500)  time: 0.5568  data: 0.0936  max mem: 15572
[2025-01-16 08:14:39,839] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106179
[2025-01-16 08:14:39,839] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:14:39,839] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2250/2809]  eta: 0:05:17  lr: 0.000001  min_lr: 0.000000  loss: 3.8879 (3.7206)  class_acc: 0.2500 (0.3426)  loss_scale: 65536.0000 (56568.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5743  data: 0.1358  max mem: 15572
Epoch: [37]  [2260/2809]  eta: 0:05:12  lr: 0.000001  min_lr: 0.000000  loss: 3.7210 (3.7200)  class_acc: 0.2917 (0.3426)  loss_scale: 32768.0000 (56463.5683)  weight_decay: 0.0500 (0.0500)  time: 0.6161  data: 0.1787  max mem: 15572
Epoch: [37]  [2270/2809]  eta: 0:05:06  lr: 0.000000  min_lr: 0.000000  loss: 3.7210 (3.7205)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (56359.2285)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1687  max mem: 15572
Epoch: [37]  [2280/2809]  eta: 0:05:01  lr: 0.000000  min_lr: 0.000000  loss: 3.8208 (3.7206)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (56255.8036)  weight_decay: 0.0500 (0.0500)  time: 0.5748  data: 0.1222  max mem: 15572
Epoch: [37]  [2290/2809]  eta: 0:04:55  lr: 0.000000  min_lr: 0.000000  loss: 3.8114 (3.7213)  class_acc: 0.2917 (0.3423)  loss_scale: 32768.0000 (56153.2815)  weight_decay: 0.0500 (0.0500)  time: 0.5649  data: 0.1201  max mem: 15572
Epoch: [37]  [2300/2809]  eta: 0:04:49  lr: 0.000000  min_lr: 0.000000  loss: 3.8158 (3.7212)  class_acc: 0.3333 (0.3423)  loss_scale: 32768.0000 (56051.6506)  weight_decay: 0.0500 (0.0500)  time: 0.5560  data: 0.1224  max mem: 15572
Epoch: [37]  [2310/2809]  eta: 0:04:44  lr: 0.000000  min_lr: 0.000000  loss: 3.8121 (3.7215)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (55950.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.1366  max mem: 15572
Epoch: [37]  [2320/2809]  eta: 0:04:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8393 (3.7230)  class_acc: 0.2917 (0.3419)  loss_scale: 32768.0000 (55851.0159)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1625  max mem: 15572
Epoch: [37]  [2330/2809]  eta: 0:04:32  lr: 0.000000  min_lr: 0.000000  loss: 3.9560 (3.7233)  class_acc: 0.2500 (0.3417)  loss_scale: 32768.0000 (55751.9897)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.1959  max mem: 15572
Epoch: [37]  [2340/2809]  eta: 0:04:27  lr: 0.000000  min_lr: 0.000000  loss: 3.6190 (3.7224)  class_acc: 0.3750 (0.3420)  loss_scale: 32768.0000 (55653.8095)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1417  max mem: 15572
Epoch: [37]  [2350/2809]  eta: 0:04:21  lr: 0.000000  min_lr: 0.000000  loss: 3.5947 (3.7216)  class_acc: 0.3750 (0.3422)  loss_scale: 32768.0000 (55556.4645)  weight_decay: 0.0500 (0.0500)  time: 0.5321  data: 0.1035  max mem: 15572
Epoch: [37]  [2360/2809]  eta: 0:04:15  lr: 0.000000  min_lr: 0.000000  loss: 3.7848 (3.7210)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (55459.9441)  weight_decay: 0.0500 (0.0500)  time: 0.5441  data: 0.0968  max mem: 15572
Epoch: [37]  [2370/2809]  eta: 0:04:09  lr: 0.000000  min_lr: 0.000000  loss: 3.5868 (3.7208)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (55364.2379)  weight_decay: 0.0500 (0.0500)  time: 0.5116  data: 0.0539  max mem: 15572
[2025-01-16 08:15:54,283] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:15:54,283] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [2380/2809]  eta: 0:04:03  lr: 0.000000  min_lr: 0.000000  loss: 3.7116 (3.7215)  class_acc: 0.2917 (0.3418)  loss_scale: 32768.0000 (55351.9093)  weight_decay: 0.0500 (0.0500)  time: 0.5102  data: 0.0708  max mem: 15572
Epoch: [37]  [2390/2809]  eta: 0:03:58  lr: 0.000000  min_lr: 0.000000  loss: 3.9686 (3.7224)  class_acc: 0.2500 (0.3414)  loss_scale: 65536.0000 (55394.5027)  weight_decay: 0.0500 (0.0500)  time: 0.5298  data: 0.0868  max mem: 15572
Epoch: [37]  [2400/2809]  eta: 0:03:52  lr: 0.000000  min_lr: 0.000000  loss: 3.9437 (3.7223)  class_acc: 0.2917 (0.3415)  loss_scale: 65536.0000 (55436.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1513  max mem: 15572
Epoch: [37]  [2410/2809]  eta: 0:03:47  lr: 0.000000  min_lr: 0.000000  loss: 3.8416 (3.7218)  class_acc: 0.3333 (0.3414)  loss_scale: 65536.0000 (55478.6296)  weight_decay: 0.0500 (0.0500)  time: 0.6103  data: 0.1671  max mem: 15572
Epoch: [37]  [2420/2809]  eta: 0:03:41  lr: 0.000000  min_lr: 0.000000  loss: 3.9837 (3.7231)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (55520.1718)  weight_decay: 0.0500 (0.0500)  time: 0.5823  data: 0.1346  max mem: 15572
Epoch: [37]  [2430/2809]  eta: 0:03:35  lr: 0.000000  min_lr: 0.000000  loss: 3.9779 (3.7234)  class_acc: 0.2917 (0.3411)  loss_scale: 65536.0000 (55561.3723)  weight_decay: 0.0500 (0.0500)  time: 0.5296  data: 0.0987  max mem: 15572
Epoch: [37]  [2440/2809]  eta: 0:03:29  lr: 0.000000  min_lr: 0.000000  loss: 3.6170 (3.7225)  class_acc: 0.3750 (0.3414)  loss_scale: 65536.0000 (55602.2351)  weight_decay: 0.0500 (0.0500)  time: 0.5394  data: 0.1194  max mem: 15572
Epoch: [37]  [2450/2809]  eta: 0:03:24  lr: 0.000000  min_lr: 0.000000  loss: 3.6170 (3.7229)  class_acc: 0.3750 (0.3414)  loss_scale: 65536.0000 (55642.7646)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.1957  max mem: 15572
Epoch: [37]  [2460/2809]  eta: 0:03:18  lr: 0.000000  min_lr: 0.000000  loss: 3.7523 (3.7227)  class_acc: 0.3750 (0.3416)  loss_scale: 65536.0000 (55682.9646)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.1710  max mem: 15572
Epoch: [37]  [2470/2809]  eta: 0:03:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7229 (3.7227)  class_acc: 0.2917 (0.3417)  loss_scale: 65536.0000 (55722.8393)  weight_decay: 0.0500 (0.0500)  time: 0.5753  data: 0.1430  max mem: 15572
Epoch: [37]  [2480/2809]  eta: 0:03:07  lr: 0.000000  min_lr: 0.000000  loss: 3.7707 (3.7231)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (55762.3926)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.1515  max mem: 15572
Epoch: [37]  [2490/2809]  eta: 0:03:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7768 (3.7231)  class_acc: 0.3333 (0.3415)  loss_scale: 65536.0000 (55801.6283)  weight_decay: 0.0500 (0.0500)  time: 0.5830  data: 0.1370  max mem: 15572
Epoch: [37]  [2500/2809]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 3.6208 (3.7226)  class_acc: 0.3750 (0.3417)  loss_scale: 65536.0000 (55840.5502)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1575  max mem: 15572
[2025-01-16 08:17:08,346] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:17:08,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:17:10,656] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106441
[2025-01-16 08:17:10,656] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:17:10,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [2510/2809]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7149 (3.7226)  class_acc: 0.3333 (0.3417)  loss_scale: 65536.0000 (56009.6599)  weight_decay: 0.0500 (0.0500)  time: 0.5330  data: 0.0903  max mem: 15572
Epoch: [37]  [2520/2809]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 3.7437 (3.7224)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (56047.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5304  data: 0.0887  max mem: 15572
Epoch: [37]  [2530/2809]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 3.7257 (3.7218)  class_acc: 0.4167 (0.3422)  loss_scale: 65536.0000 (56084.9372)  weight_decay: 0.0500 (0.0500)  time: 0.5673  data: 0.1283  max mem: 15572
Epoch: [37]  [2540/2809]  eta: 0:02:33  lr: 0.000000  min_lr: 0.000000  loss: 3.7257 (3.7215)  class_acc: 0.3750 (0.3422)  loss_scale: 65536.0000 (56122.1314)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0986  max mem: 15572
Epoch: [37]  [2550/2809]  eta: 0:02:27  lr: 0.000000  min_lr: 0.000000  loss: 3.7432 (3.7214)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (56159.0341)  weight_decay: 0.0500 (0.0500)  time: 0.5327  data: 0.0868  max mem: 15572
Epoch: [37]  [2560/2809]  eta: 0:02:21  lr: 0.000000  min_lr: 0.000000  loss: 3.6598 (3.7210)  class_acc: 0.3333 (0.3421)  loss_scale: 65536.0000 (56195.6486)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0930  max mem: 15572
Epoch: [37]  [2570/2809]  eta: 0:02:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7091 (3.7216)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (56231.9782)  weight_decay: 0.0500 (0.0500)  time: 0.6134  data: 0.1478  max mem: 15572
Epoch: [37]  [2580/2809]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 3.9667 (3.7218)  class_acc: 0.2917 (0.3418)  loss_scale: 65536.0000 (56268.0263)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.1436  max mem: 15572
Epoch: [37]  [2590/2809]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 3.5291 (3.7206)  class_acc: 0.3750 (0.3423)  loss_scale: 65536.0000 (56303.7962)  weight_decay: 0.0500 (0.0500)  time: 0.5645  data: 0.1198  max mem: 15572
Epoch: [37]  [2600/2809]  eta: 0:01:58  lr: 0.000000  min_lr: 0.000000  loss: 3.6313 (3.7205)  class_acc: 0.4167 (0.3424)  loss_scale: 65536.0000 (56339.2910)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.1175  max mem: 15572
Epoch: [37]  [2610/2809]  eta: 0:01:53  lr: 0.000000  min_lr: 0.000000  loss: 3.6313 (3.7196)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (56374.5140)  weight_decay: 0.0500 (0.0500)  time: 0.5497  data: 0.0993  max mem: 15572
Epoch: [37]  [2620/2809]  eta: 0:01:47  lr: 0.000000  min_lr: 0.000000  loss: 3.6498 (3.7199)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (56409.4681)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1527  max mem: 15572
Epoch: [37]  [2630/2809]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 3.7162 (3.7193)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (56444.1566)  weight_decay: 0.0500 (0.0500)  time: 0.6274  data: 0.1655  max mem: 15572
[2025-01-16 08:18:25,698] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:18:25,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [2640/2809]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 3.6038 (3.7195)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (56577.8417)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.1459  max mem: 15572
[2025-01-16 08:18:27,335] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106574
[2025-01-16 08:18:27,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:18:27,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [2650/2809]  eta: 0:01:30  lr: 0.000000  min_lr: 0.000000  loss: 3.7069 (3.7191)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (56611.6333)  weight_decay: 0.0500 (0.0500)  time: 0.5570  data: 0.1244  max mem: 15572
Epoch: [37]  [2660/2809]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7239 (3.7191)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (56645.1710)  weight_decay: 0.0500 (0.0500)  time: 0.5477  data: 0.1042  max mem: 15572
Epoch: [37]  [2670/2809]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 3.7239 (3.7187)  class_acc: 0.3333 (0.3426)  loss_scale: 65536.0000 (56678.4575)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1298  max mem: 15572
Epoch: [37]  [2680/2809]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7010 (3.7189)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (56711.4957)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1119  max mem: 15572
Epoch: [37]  [2690/2809]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 3.7311 (3.7191)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (56744.2884)  weight_decay: 0.0500 (0.0500)  time: 0.5712  data: 0.1284  max mem: 15572
[2025-01-16 08:18:57,360] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106628
[2025-01-16 08:18:57,361] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:18:57,362] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [2700/2809]  eta: 0:01:02  lr: 0.000000  min_lr: 0.000000  loss: 3.7311 (3.7195)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (56704.0474)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1121  max mem: 15572
Epoch: [37]  [2710/2809]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 3.7563 (3.7197)  class_acc: 0.3333 (0.3427)  loss_scale: 32768.0000 (56615.7551)  weight_decay: 0.0500 (0.0500)  time: 0.5785  data: 0.1296  max mem: 15572
Epoch: [37]  [2720/2809]  eta: 0:00:50  lr: 0.000000  min_lr: 0.000000  loss: 3.8063 (3.7207)  class_acc: 0.2917 (0.3425)  loss_scale: 32768.0000 (56528.1117)  weight_decay: 0.0500 (0.0500)  time: 0.6384  data: 0.2060  max mem: 15572
Epoch: [37]  [2730/2809]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 3.9397 (3.7215)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (56441.1102)  weight_decay: 0.0500 (0.0500)  time: 0.5734  data: 0.1348  max mem: 15572
Epoch: [37]  [2740/2809]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 3.8423 (3.7216)  class_acc: 0.2917 (0.3423)  loss_scale: 32768.0000 (56354.7435)  weight_decay: 0.0500 (0.0500)  time: 0.5267  data: 0.0754  max mem: 15572
Epoch: [37]  [2750/2809]  eta: 0:00:33  lr: 0.000000  min_lr: 0.000000  loss: 3.6297 (3.7211)  class_acc: 0.3333 (0.3425)  loss_scale: 32768.0000 (56269.0047)  weight_decay: 0.0500 (0.0500)  time: 0.5579  data: 0.0956  max mem: 15572
Epoch: [37]  [2760/2809]  eta: 0:00:27  lr: 0.000000  min_lr: 0.000000  loss: 3.5990 (3.7214)  class_acc: 0.3333 (0.3423)  loss_scale: 32768.0000 (56183.8870)  weight_decay: 0.0500 (0.0500)  time: 0.5507  data: 0.1020  max mem: 15572
Epoch: [37]  [2770/2809]  eta: 0:00:22  lr: 0.000000  min_lr: 0.000000  loss: 3.5630 (3.7204)  class_acc: 0.2917 (0.3425)  loss_scale: 32768.0000 (56099.3836)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0767  max mem: 15572
Epoch: [37]  [2780/2809]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7201 (3.7214)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (56015.4880)  weight_decay: 0.0500 (0.0500)  time: 0.5370  data: 0.0791  max mem: 15572
Epoch: [37]  [2790/2809]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 3.9337 (3.7217)  class_acc: 0.2500 (0.3420)  loss_scale: 32768.0000 (55932.1935)  weight_decay: 0.0500 (0.0500)  time: 0.5859  data: 0.1173  max mem: 15572
Epoch: [37]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7457 (3.7214)  class_acc: 0.2917 (0.3420)  loss_scale: 32768.0000 (55849.4938)  weight_decay: 0.0500 (0.0500)  time: 0.5461  data: 0.1045  max mem: 15572
Epoch: [37]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.6038 (3.7212)  class_acc: 0.3750 (0.3422)  loss_scale: 32768.0000 (55783.7579)  weight_decay: 0.0500 (0.0500)  time: 0.4418  data: 0.0469  max mem: 15572
Epoch: [37] Total time: 0:26:37 (0.5687 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.6038 (3.7212)  class_acc: 0.3750 (0.3422)  loss_scale: 32768.0000 (55783.7579)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:16:27  loss: 0.3748 (0.3748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.6309  data: 3.4567  max mem: 15572
Val:  [ 10/272]  eta: 0:03:03  loss: 2.3742 (2.2459)  acc1: 44.4444 (41.4141)  acc5: 72.2222 (73.7374)  time: 0.7005  data: 0.5011  max mem: 15572
Val:  [ 20/272]  eta: 0:02:05  loss: 2.3501 (2.2701)  acc1: 44.4444 (44.9735)  acc5: 77.7778 (75.3968)  time: 0.3430  data: 0.1416  max mem: 15572
Val:  [ 30/272]  eta: 0:01:41  loss: 2.3501 (2.3567)  acc1: 50.0000 (43.3692)  acc5: 77.7778 (74.1935)  time: 0.2630  data: 0.0741  max mem: 15572
Val:  [ 40/272]  eta: 0:01:33  loss: 2.4604 (2.4099)  acc1: 27.7778 (40.9214)  acc5: 72.2222 (74.3902)  time: 0.2979  data: 0.1075  max mem: 15572
Val:  [ 50/272]  eta: 0:01:24  loss: 2.3852 (2.3371)  acc1: 38.8889 (42.5926)  acc5: 77.7778 (76.0349)  time: 0.3265  data: 0.1217  max mem: 15572
Val:  [ 60/272]  eta: 0:01:17  loss: 1.4477 (2.2329)  acc1: 61.1111 (45.5373)  acc5: 88.8889 (76.7760)  time: 0.2926  data: 0.0959  max mem: 15572
Val:  [ 70/272]  eta: 0:01:12  loss: 1.4882 (2.1512)  acc1: 66.6667 (48.0438)  acc5: 83.3333 (77.8560)  time: 0.2926  data: 0.0980  max mem: 15572
Val:  [ 80/272]  eta: 0:01:08  loss: 1.8291 (2.1600)  acc1: 55.5556 (48.2853)  acc5: 77.7778 (77.4348)  time: 0.3272  data: 0.1298  max mem: 15572
Val:  [ 90/272]  eta: 0:01:03  loss: 2.1158 (2.1659)  acc1: 50.0000 (48.3516)  acc5: 77.7778 (78.0220)  time: 0.3282  data: 0.1360  max mem: 15572
Val:  [100/272]  eta: 0:00:59  loss: 2.1158 (2.1915)  acc1: 50.0000 (47.7998)  acc5: 83.3333 (77.7778)  time: 0.3142  data: 0.1277  max mem: 15572
Val:  [110/272]  eta: 0:00:55  loss: 2.4312 (2.2640)  acc1: 27.7778 (45.5956)  acc5: 77.7778 (76.7768)  time: 0.3203  data: 0.1242  max mem: 15572
Val:  [120/272]  eta: 0:00:52  loss: 2.7642 (2.3016)  acc1: 22.2222 (45.0872)  acc5: 72.2222 (76.2167)  time: 0.3416  data: 0.1222  max mem: 15572
Val:  [130/272]  eta: 0:00:48  loss: 2.1076 (2.2662)  acc1: 50.0000 (46.0984)  acc5: 83.3333 (76.9720)  time: 0.3433  data: 0.1331  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.6001 (2.2595)  acc1: 55.5556 (46.5721)  acc5: 88.8889 (76.7928)  time: 0.3059  data: 0.1025  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.2745 (2.2647)  acc1: 38.8889 (46.1369)  acc5: 77.7778 (76.9316)  time: 0.3018  data: 0.0925  max mem: 15572
Val:  [160/272]  eta: 0:00:37  loss: 2.2745 (2.2540)  acc1: 44.4444 (46.6184)  acc5: 77.7778 (77.1912)  time: 0.3104  data: 0.1032  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.3557 (2.2753)  acc1: 44.4444 (46.0689)  acc5: 72.2222 (76.7706)  time: 0.3251  data: 0.1144  max mem: 15572
Val:  [180/272]  eta: 0:00:30  loss: 2.3194 (2.2676)  acc1: 38.8889 (45.9177)  acc5: 72.2222 (77.1332)  time: 0.3321  data: 0.1205  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.3194 (2.3213)  acc1: 38.8889 (44.7062)  acc5: 77.7778 (75.7708)  time: 0.3094  data: 0.1153  max mem: 15572
Val:  [200/272]  eta: 0:00:23  loss: 2.5368 (2.3303)  acc1: 38.8889 (44.4168)  acc5: 72.2222 (75.5943)  time: 0.2920  data: 0.1043  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.0261 (2.3337)  acc1: 44.4444 (44.5234)  acc5: 83.3333 (75.5661)  time: 0.2947  data: 0.0996  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.2003 (2.3222)  acc1: 44.4444 (44.6958)  acc5: 77.7778 (75.7416)  time: 0.3165  data: 0.1291  max mem: 15572
Val:  [230/272]  eta: 0:00:13  loss: 1.7222 (2.2936)  acc1: 66.6667 (45.6710)  acc5: 83.3333 (76.0702)  time: 0.2957  data: 0.1125  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.6321 (2.2775)  acc1: 66.6667 (45.9659)  acc5: 83.3333 (76.3485)  time: 0.2662  data: 0.0795  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.1498 (2.2871)  acc1: 38.8889 (45.3298)  acc5: 83.3333 (76.2948)  time: 0.3102  data: 0.1220  max mem: 15572
Val:  [260/272]  eta: 0:00:03  loss: 1.1258 (2.2301)  acc1: 72.2222 (46.9562)  acc5: 88.8889 (76.9689)  time: 0.3250  data: 0.1360  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3379 (2.2243)  acc1: 72.2222 (47.0480)  acc5: 88.8889 (77.2448)  time: 0.2707  data: 0.0988  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3379 (2.2294)  acc1: 72.2222 (47.0203)  acc5: 88.8889 (77.2066)  time: 0.2614  data: 0.0987  max mem: 15572
Val: Total time: 0:01:27 (0.3230 s / it)
* Acc@1 47.020 Acc@5 77.207 loss 2.229
Accuracy of the network on the 4883 val videos: 47.0%
Max accuracy: 47.47%
Epoch: [38]  [   0/2809]  eta: 7:54:25  lr: 0.000000  min_lr: 0.000000  loss: 3.4234 (3.4234)  class_acc: 0.4167 (0.4167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 10.1336  data: 9.6950  max mem: 15572
Epoch: [38]  [  10/2809]  eta: 1:00:52  lr: 0.000000  min_lr: 0.000000  loss: 3.8417 (3.7358)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3048  data: 0.8821  max mem: 15572
[2025-01-16 08:21:43,631] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:21:43,632] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [  20/2809]  eta: 0:41:07  lr: 0.000000  min_lr: 0.000000  loss: 3.3895 (3.5709)  class_acc: 0.3750 (0.3730)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  time: 0.4223  data: 0.0007  max mem: 15572
Epoch: [38]  [  30/2809]  eta: 0:34:46  lr: 0.000000  min_lr: 0.000000  loss: 3.2846 (3.5001)  class_acc: 0.4167 (0.3777)  loss_scale: 65536.0000 (49680.5161)  weight_decay: 0.0500 (0.0500)  time: 0.4461  data: 0.0008  max mem: 15572
Epoch: [38]  [  40/2809]  eta: 0:31:30  lr: 0.000000  min_lr: 0.000000  loss: 3.7150 (3.5675)  class_acc: 0.3750 (0.3720)  loss_scale: 65536.0000 (53547.7073)  weight_decay: 0.0500 (0.0500)  time: 0.4707  data: 0.0007  max mem: 15572
Epoch: [38]  [  50/2809]  eta: 0:30:19  lr: 0.000000  min_lr: 0.000000  loss: 3.7301 (3.5802)  class_acc: 0.3333 (0.3668)  loss_scale: 65536.0000 (55898.3529)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0558  max mem: 15572
Epoch: [38]  [  60/2809]  eta: 0:29:51  lr: 0.000000  min_lr: 0.000000  loss: 3.5379 (3.5995)  class_acc: 0.3333 (0.3607)  loss_scale: 65536.0000 (57478.2951)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1420  max mem: 15572
Epoch: [38]  [  70/2809]  eta: 0:30:14  lr: 0.000000  min_lr: 0.000000  loss: 3.4030 (3.5786)  class_acc: 0.3333 (0.3633)  loss_scale: 65536.0000 (58613.1831)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.2094  max mem: 15572
Epoch: [38]  [  80/2809]  eta: 0:29:58  lr: 0.000000  min_lr: 0.000000  loss: 3.4437 (3.5825)  class_acc: 0.2917 (0.3601)  loss_scale: 65536.0000 (59467.8519)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.2116  max mem: 15572
Epoch: [38]  [  90/2809]  eta: 0:30:02  lr: 0.000000  min_lr: 0.000000  loss: 3.7551 (3.6036)  class_acc: 0.2500 (0.3553)  loss_scale: 65536.0000 (60134.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.2106  max mem: 15572
Epoch: [38]  [ 100/2809]  eta: 0:30:19  lr: 0.000000  min_lr: 0.000000  loss: 3.7515 (3.6169)  class_acc: 0.3333 (0.3523)  loss_scale: 65536.0000 (60669.4653)  weight_decay: 0.0500 (0.0500)  time: 0.7222  data: 0.2706  max mem: 15572
Epoch: [38]  [ 110/2809]  eta: 0:30:03  lr: 0.000000  min_lr: 0.000000  loss: 3.7415 (3.6246)  class_acc: 0.3333 (0.3502)  loss_scale: 65536.0000 (61107.8919)  weight_decay: 0.0500 (0.0500)  time: 0.6908  data: 0.2400  max mem: 15572
Epoch: [38]  [ 120/2809]  eta: 0:30:04  lr: 0.000000  min_lr: 0.000000  loss: 3.7415 (3.6371)  class_acc: 0.3333 (0.3519)  loss_scale: 65536.0000 (61473.8512)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.1977  max mem: 15572
Epoch: [38]  [ 130/2809]  eta: 0:29:51  lr: 0.000000  min_lr: 0.000000  loss: 3.8925 (3.6559)  class_acc: 0.3333 (0.3515)  loss_scale: 65536.0000 (61783.9389)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.1884  max mem: 15572
Epoch: [38]  [ 140/2809]  eta: 0:29:58  lr: 0.000000  min_lr: 0.000000  loss: 3.7197 (3.6384)  class_acc: 0.3333 (0.3508)  loss_scale: 65536.0000 (62050.0426)  weight_decay: 0.0500 (0.0500)  time: 0.6904  data: 0.1924  max mem: 15572
[2025-01-16 08:23:03,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:23:03,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:23:04,691] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106886
[2025-01-16 08:23:04,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:23:04,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [ 150/2809]  eta: 0:29:22  lr: 0.000000  min_lr: 0.000000  loss: 3.7197 (3.6478)  class_acc: 0.3333 (0.3504)  loss_scale: 65536.0000 (62714.9139)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1493  max mem: 15572
Epoch: [38]  [ 160/2809]  eta: 0:28:31  lr: 0.000000  min_lr: 0.000000  loss: 3.9692 (3.6682)  class_acc: 0.2917 (0.3455)  loss_scale: 65536.0000 (62890.1366)  weight_decay: 0.0500 (0.0500)  time: 0.4517  data: 0.0392  max mem: 15572
Epoch: [38]  [ 170/2809]  eta: 0:27:51  lr: 0.000000  min_lr: 0.000000  loss: 3.9712 (3.6805)  class_acc: 0.2917 (0.3448)  loss_scale: 65536.0000 (63044.8655)  weight_decay: 0.0500 (0.0500)  time: 0.4101  data: 0.0005  max mem: 15572
Epoch: [38]  [ 180/2809]  eta: 0:27:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8522 (3.6806)  class_acc: 0.2917 (0.3451)  loss_scale: 65536.0000 (63182.4972)  weight_decay: 0.0500 (0.0500)  time: 0.4357  data: 0.0006  max mem: 15572
Epoch: [38]  [ 190/2809]  eta: 0:26:51  lr: 0.000000  min_lr: 0.000000  loss: 3.4677 (3.6692)  class_acc: 0.3750 (0.3466)  loss_scale: 65536.0000 (63305.7173)  weight_decay: 0.0500 (0.0500)  time: 0.4619  data: 0.0234  max mem: 15572
[2025-01-16 08:23:26,844] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 106935
[2025-01-16 08:23:26,844] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:23:26,844] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 200/2809]  eta: 0:26:36  lr: 0.000000  min_lr: 0.000000  loss: 3.4857 (3.6708)  class_acc: 0.3750 (0.3474)  loss_scale: 65536.0000 (62112.4776)  weight_decay: 0.0500 (0.0500)  time: 0.5112  data: 0.0822  max mem: 15572
Epoch: [38]  [ 210/2809]  eta: 0:26:34  lr: 0.000000  min_lr: 0.000000  loss: 3.7670 (3.6805)  class_acc: 0.3333 (0.3456)  loss_scale: 32768.0000 (60721.7441)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.1734  max mem: 15572
Epoch: [38]  [ 220/2809]  eta: 0:26:31  lr: 0.000000  min_lr: 0.000000  loss: 3.8602 (3.6829)  class_acc: 0.2917 (0.3452)  loss_scale: 32768.0000 (59456.8688)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.2155  max mem: 15572
Epoch: [38]  [ 230/2809]  eta: 0:26:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8945 (3.6960)  class_acc: 0.2917 (0.3433)  loss_scale: 32768.0000 (58301.5065)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1407  max mem: 15572
Epoch: [38]  [ 240/2809]  eta: 0:26:07  lr: 0.000000  min_lr: 0.000000  loss: 3.8334 (3.6916)  class_acc: 0.3333 (0.3447)  loss_scale: 32768.0000 (57242.0249)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1096  max mem: 15572
Epoch: [38]  [ 250/2809]  eta: 0:25:59  lr: 0.000000  min_lr: 0.000000  loss: 3.6804 (3.6917)  class_acc: 0.3333 (0.3468)  loss_scale: 32768.0000 (56266.9641)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1525  max mem: 15572
[2025-01-16 08:24:04,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=107000, skipped=713, lr=[3.774722835981766e-09, 3.774722835981766e-09, 5.392461194259667e-09, 5.392461194259667e-09, 7.703515991799525e-09, 7.703515991799525e-09, 1.1005022845427893e-08, 1.1005022845427893e-08, 1.5721461207754134e-08, 1.5721461207754134e-08, 2.245923029679162e-08, 2.245923029679162e-08, 3.208461470970231e-08, 3.208461470970231e-08, 4.5835163871003313e-08, 4.5835163871003313e-08, 6.547880553000474e-08, 6.547880553000474e-08, 9.354115075714963e-08, 9.354115075714963e-08, 1.336302153673566e-07, 1.336302153673566e-07, 1.9090030766765233e-07, 1.9090030766765233e-07, 2.7271472523950333e-07, 2.7271472523950333e-07, 3.8959246462786193e-07, 3.8959246462786193e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:24:04,436] [INFO] [timer.py:260:stop] epoch=0/micro_step=107000/global_step=107000, RunningAvgSamplesPerSec=28.584297117745983, CurrSamplesPerSec=25.60350308346021, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [ 260/2809]  eta: 0:25:46  lr: 0.000000  min_lr: 0.000000  loss: 3.6725 (3.6911)  class_acc: 0.3333 (0.3469)  loss_scale: 32768.0000 (55366.6207)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1297  max mem: 15572
Epoch: [38]  [ 270/2809]  eta: 0:25:26  lr: 0.000000  min_lr: 0.000000  loss: 3.6725 (3.6943)  class_acc: 0.3333 (0.3475)  loss_scale: 32768.0000 (54532.7232)  weight_decay: 0.0500 (0.0500)  time: 0.4998  data: 0.0478  max mem: 15572
Epoch: [38]  [ 280/2809]  eta: 0:25:11  lr: 0.000000  min_lr: 0.000000  loss: 3.8976 (3.7012)  class_acc: 0.3333 (0.3464)  loss_scale: 32768.0000 (53758.1779)  weight_decay: 0.0500 (0.0500)  time: 0.4774  data: 0.0420  max mem: 15572
Epoch: [38]  [ 290/2809]  eta: 0:25:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7616 (3.6972)  class_acc: 0.3333 (0.3477)  loss_scale: 32768.0000 (53036.8660)  weight_decay: 0.0500 (0.0500)  time: 0.5237  data: 0.0976  max mem: 15572
Epoch: [38]  [ 300/2809]  eta: 0:24:47  lr: 0.000000  min_lr: 0.000000  loss: 3.6409 (3.6964)  class_acc: 0.3750 (0.3480)  loss_scale: 32768.0000 (52363.4817)  weight_decay: 0.0500 (0.0500)  time: 0.5283  data: 0.0948  max mem: 15572
Epoch: [38]  [ 310/2809]  eta: 0:24:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6273 (3.6916)  class_acc: 0.3750 (0.3498)  loss_scale: 32768.0000 (51733.4019)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1355  max mem: 15572
Epoch: [38]  [ 320/2809]  eta: 0:24:37  lr: 0.000000  min_lr: 0.000000  loss: 3.7217 (3.6921)  class_acc: 0.3333 (0.3492)  loss_scale: 32768.0000 (51142.5794)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1800  max mem: 15572
[2025-01-16 08:24:39,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:24:39,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 330/2809]  eta: 0:24:24  lr: 0.000000  min_lr: 0.000000  loss: 3.6419 (3.6912)  class_acc: 0.2917 (0.3494)  loss_scale: 32768.0000 (51478.4290)  weight_decay: 0.0500 (0.0500)  time: 0.5446  data: 0.1268  max mem: 15572
Epoch: [38]  [ 340/2809]  eta: 0:24:18  lr: 0.000000  min_lr: 0.000000  loss: 3.5895 (3.6943)  class_acc: 0.3333 (0.3487)  loss_scale: 65536.0000 (51890.6745)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1210  max mem: 15572
Epoch: [38]  [ 350/2809]  eta: 0:24:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8654 (3.6973)  class_acc: 0.3333 (0.3486)  loss_scale: 65536.0000 (52279.4302)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1535  max mem: 15572
[2025-01-16 08:24:57,820] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107098
[2025-01-16 08:24:57,821] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:24:57,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 360/2809]  eta: 0:24:02  lr: 0.000000  min_lr: 0.000000  loss: 3.8184 (3.7015)  class_acc: 0.2917 (0.3470)  loss_scale: 65536.0000 (52192.7978)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1266  max mem: 15572
Epoch: [38]  [ 370/2809]  eta: 0:23:58  lr: 0.000000  min_lr: 0.000000  loss: 3.7142 (3.7019)  class_acc: 0.2917 (0.3461)  loss_scale: 32768.0000 (51669.2183)  weight_decay: 0.0500 (0.0500)  time: 0.5843  data: 0.1318  max mem: 15572
Epoch: [38]  [ 380/2809]  eta: 0:23:52  lr: 0.000000  min_lr: 0.000000  loss: 3.6524 (3.7013)  class_acc: 0.2917 (0.3441)  loss_scale: 32768.0000 (51173.1234)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1484  max mem: 15572
Epoch: [38]  [ 390/2809]  eta: 0:23:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8074 (3.7011)  class_acc: 0.2917 (0.3447)  loss_scale: 32768.0000 (50702.4041)  weight_decay: 0.0500 (0.0500)  time: 0.5920  data: 0.1505  max mem: 15572
Epoch: [38]  [ 400/2809]  eta: 0:23:43  lr: 0.000000  min_lr: 0.000000  loss: 3.7865 (3.6986)  class_acc: 0.3333 (0.3450)  loss_scale: 32768.0000 (50255.1621)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1791  max mem: 15572
Epoch: [38]  [ 410/2809]  eta: 0:23:35  lr: 0.000000  min_lr: 0.000000  loss: 3.6827 (3.7009)  class_acc: 0.3333 (0.3445)  loss_scale: 32768.0000 (49829.6837)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.1504  max mem: 15572
Epoch: [38]  [ 420/2809]  eta: 0:23:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8129 (3.7068)  class_acc: 0.3333 (0.3438)  loss_scale: 32768.0000 (49424.4181)  weight_decay: 0.0500 (0.0500)  time: 0.5710  data: 0.1241  max mem: 15572
Epoch: [38]  [ 430/2809]  eta: 0:23:16  lr: 0.000000  min_lr: 0.000000  loss: 3.8802 (3.7095)  class_acc: 0.3333 (0.3443)  loss_scale: 32768.0000 (49037.9582)  weight_decay: 0.0500 (0.0500)  time: 0.5218  data: 0.0781  max mem: 15572
Epoch: [38]  [ 440/2809]  eta: 0:23:03  lr: 0.000000  min_lr: 0.000000  loss: 3.7630 (3.7030)  class_acc: 0.3750 (0.3457)  loss_scale: 32768.0000 (48669.0249)  weight_decay: 0.0500 (0.0500)  time: 0.4585  data: 0.0090  max mem: 15572
Epoch: [38]  [ 450/2809]  eta: 0:22:57  lr: 0.000000  min_lr: 0.000000  loss: 3.4217 (3.6990)  class_acc: 0.3750 (0.3465)  loss_scale: 32768.0000 (48316.4523)  weight_decay: 0.0500 (0.0500)  time: 0.5234  data: 0.0708  max mem: 15572
Epoch: [38]  [ 460/2809]  eta: 0:22:48  lr: 0.000000  min_lr: 0.000000  loss: 3.6078 (3.6994)  class_acc: 0.3333 (0.3466)  loss_scale: 32768.0000 (47979.1757)  weight_decay: 0.0500 (0.0500)  time: 0.5459  data: 0.1185  max mem: 15572
Epoch: [38]  [ 470/2809]  eta: 0:22:47  lr: 0.000000  min_lr: 0.000000  loss: 3.6078 (3.6959)  class_acc: 0.3333 (0.3487)  loss_scale: 32768.0000 (47656.2208)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.1743  max mem: 15572
Epoch: [38]  [ 480/2809]  eta: 0:22:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6748 (3.6974)  class_acc: 0.3333 (0.3481)  loss_scale: 32768.0000 (47346.6944)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.2301  max mem: 15572
[2025-01-16 08:26:11,465] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:26:11,466] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 490/2809]  eta: 0:22:40  lr: 0.000000  min_lr: 0.000000  loss: 3.7298 (3.7003)  class_acc: 0.2917 (0.3472)  loss_scale: 32768.0000 (47450.1996)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.2010  max mem: 15572
[2025-01-16 08:26:16,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107233
[2025-01-16 08:26:16,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:26:16,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 500/2809]  eta: 0:22:32  lr: 0.000000  min_lr: 0.000000  loss: 3.9317 (3.7024)  class_acc: 0.3333 (0.3473)  loss_scale: 32768.0000 (47157.1417)  weight_decay: 0.0500 (0.0500)  time: 0.5850  data: 0.1447  max mem: 15572
Epoch: [38]  [ 510/2809]  eta: 0:22:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8397 (3.7060)  class_acc: 0.2917 (0.3469)  loss_scale: 32768.0000 (46875.5538)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.1333  max mem: 15572
Epoch: [38]  [ 520/2809]  eta: 0:22:26  lr: 0.000000  min_lr: 0.000000  loss: 3.8299 (3.7093)  class_acc: 0.2500 (0.3452)  loss_scale: 32768.0000 (46604.7754)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.1995  max mem: 15572
Epoch: [38]  [ 530/2809]  eta: 0:22:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7075 (3.7075)  class_acc: 0.2500 (0.3450)  loss_scale: 32768.0000 (46344.1959)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1401  max mem: 15572
Epoch: [38]  [ 540/2809]  eta: 0:22:09  lr: 0.000000  min_lr: 0.000000  loss: 3.6353 (3.7067)  class_acc: 0.3333 (0.3450)  loss_scale: 32768.0000 (46093.2495)  weight_decay: 0.0500 (0.0500)  time: 0.5160  data: 0.0844  max mem: 15572
Epoch: [38]  [ 550/2809]  eta: 0:21:58  lr: 0.000000  min_lr: 0.000000  loss: 3.6083 (3.7040)  class_acc: 0.4167 (0.3467)  loss_scale: 32768.0000 (45851.4120)  weight_decay: 0.0500 (0.0500)  time: 0.5106  data: 0.0701  max mem: 15572
Epoch: [38]  [ 560/2809]  eta: 0:21:51  lr: 0.000000  min_lr: 0.000000  loss: 3.6103 (3.7014)  class_acc: 0.3750 (0.3468)  loss_scale: 32768.0000 (45618.1961)  weight_decay: 0.0500 (0.0500)  time: 0.5155  data: 0.0798  max mem: 15572
Epoch: [38]  [ 570/2809]  eta: 0:21:45  lr: 0.000000  min_lr: 0.000000  loss: 3.8022 (3.7055)  class_acc: 0.2917 (0.3465)  loss_scale: 32768.0000 (45393.1489)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.1281  max mem: 15572
Epoch: [38]  [ 580/2809]  eta: 0:21:35  lr: 0.000000  min_lr: 0.000000  loss: 3.8154 (3.7068)  class_acc: 0.3333 (0.3467)  loss_scale: 32768.0000 (45175.8485)  weight_decay: 0.0500 (0.0500)  time: 0.5183  data: 0.0793  max mem: 15572
Epoch: [38]  [ 590/2809]  eta: 0:21:28  lr: 0.000000  min_lr: 0.000000  loss: 3.7703 (3.7074)  class_acc: 0.2917 (0.3460)  loss_scale: 32768.0000 (44965.9019)  weight_decay: 0.0500 (0.0500)  time: 0.5177  data: 0.0812  max mem: 15572
Epoch: [38]  [ 600/2809]  eta: 0:21:21  lr: 0.000000  min_lr: 0.000000  loss: 3.6880 (3.7058)  class_acc: 0.2917 (0.3456)  loss_scale: 32768.0000 (44762.9418)  weight_decay: 0.0500 (0.0500)  time: 0.5485  data: 0.0947  max mem: 15572
Epoch: [38]  [ 610/2809]  eta: 0:21:18  lr: 0.000000  min_lr: 0.000000  loss: 3.7427 (3.7075)  class_acc: 0.2917 (0.3453)  loss_scale: 32768.0000 (44566.6252)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.1424  max mem: 15572
[2025-01-16 08:27:28,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:27:28,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 620/2809]  eta: 0:21:11  lr: 0.000000  min_lr: 0.000000  loss: 3.7456 (3.7074)  class_acc: 0.3333 (0.3459)  loss_scale: 32768.0000 (44429.3977)  weight_decay: 0.0500 (0.0500)  time: 0.6075  data: 0.1628  max mem: 15572
Epoch: [38]  [ 630/2809]  eta: 0:21:06  lr: 0.000000  min_lr: 0.000000  loss: 3.7456 (3.7062)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (44763.8922)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.1327  max mem: 15572
Epoch: [38]  [ 640/2809]  eta: 0:20:58  lr: 0.000000  min_lr: 0.000000  loss: 3.7440 (3.7048)  class_acc: 0.3750 (0.3469)  loss_scale: 65536.0000 (45087.9501)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.1371  max mem: 15572
Epoch: [38]  [ 650/2809]  eta: 0:20:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7440 (3.7072)  class_acc: 0.3750 (0.3465)  loss_scale: 65536.0000 (45402.0522)  weight_decay: 0.0500 (0.0500)  time: 0.5230  data: 0.0924  max mem: 15572
Epoch: [38]  [ 660/2809]  eta: 0:20:46  lr: 0.000000  min_lr: 0.000000  loss: 3.6798 (3.7046)  class_acc: 0.2917 (0.3461)  loss_scale: 65536.0000 (45706.6505)  weight_decay: 0.0500 (0.0500)  time: 0.5675  data: 0.1255  max mem: 15572
Epoch: [38]  [ 670/2809]  eta: 0:20:39  lr: 0.000000  min_lr: 0.000000  loss: 3.5639 (3.7043)  class_acc: 0.3333 (0.3457)  loss_scale: 65536.0000 (46002.1699)  weight_decay: 0.0500 (0.0500)  time: 0.5867  data: 0.1276  max mem: 15572
Epoch: [38]  [ 680/2809]  eta: 0:20:33  lr: 0.000000  min_lr: 0.000000  loss: 3.5696 (3.7037)  class_acc: 0.4167 (0.3464)  loss_scale: 65536.0000 (46289.0103)  weight_decay: 0.0500 (0.0500)  time: 0.5703  data: 0.1060  max mem: 15572
Epoch: [38]  [ 690/2809]  eta: 0:20:26  lr: 0.000000  min_lr: 0.000000  loss: 3.5694 (3.7018)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (46567.5485)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.1155  max mem: 15572
Epoch: [38]  [ 700/2809]  eta: 0:20:18  lr: 0.000000  min_lr: 0.000000  loss: 3.7903 (3.7038)  class_acc: 0.2917 (0.3458)  loss_scale: 65536.0000 (46838.1398)  weight_decay: 0.0500 (0.0500)  time: 0.5170  data: 0.0664  max mem: 15572
Epoch: [38]  [ 710/2809]  eta: 0:20:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8646 (3.7062)  class_acc: 0.2917 (0.3446)  loss_scale: 65536.0000 (47101.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5071  data: 0.0591  max mem: 15572
Epoch: [38]  [ 720/2809]  eta: 0:20:03  lr: 0.000000  min_lr: 0.000000  loss: 3.7725 (3.7076)  class_acc: 0.2917 (0.3441)  loss_scale: 65536.0000 (47356.8044)  weight_decay: 0.0500 (0.0500)  time: 0.5221  data: 0.0875  max mem: 15572
Epoch: [38]  [ 730/2809]  eta: 0:19:58  lr: 0.000000  min_lr: 0.000000  loss: 3.7275 (3.7089)  class_acc: 0.3750 (0.3445)  loss_scale: 65536.0000 (47605.4938)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1151  max mem: 15572
Epoch: [38]  [ 740/2809]  eta: 0:19:51  lr: 0.000000  min_lr: 0.000000  loss: 3.7931 (3.7106)  class_acc: 0.3750 (0.3440)  loss_scale: 65536.0000 (47847.4710)  weight_decay: 0.0500 (0.0500)  time: 0.5553  data: 0.1202  max mem: 15572
[2025-01-16 08:28:37,568] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:28:37,569] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:28:39,421] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107491
[2025-01-16 08:28:39,422] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:28:39,422] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [ 750/2809]  eta: 0:19:45  lr: 0.000000  min_lr: 0.000000  loss: 3.6447 (3.7088)  class_acc: 0.3750 (0.3445)  loss_scale: 65536.0000 (48170.2690)  weight_decay: 0.0500 (0.0500)  time: 0.5499  data: 0.1233  max mem: 15572
Epoch: [38]  [ 760/2809]  eta: 0:19:36  lr: 0.000000  min_lr: 0.000000  loss: 3.5946 (3.7078)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (48398.4652)  weight_decay: 0.0500 (0.0500)  time: 0.5240  data: 0.0813  max mem: 15572
Epoch: [38]  [ 770/2809]  eta: 0:19:31  lr: 0.000000  min_lr: 0.000000  loss: 3.7476 (3.7078)  class_acc: 0.4167 (0.3454)  loss_scale: 65536.0000 (48620.7419)  weight_decay: 0.0500 (0.0500)  time: 0.5354  data: 0.0922  max mem: 15572
[2025-01-16 08:28:52,044] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107515
[2025-01-16 08:28:52,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:28:52,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 780/2809]  eta: 0:19:28  lr: 0.000000  min_lr: 0.000000  loss: 3.8655 (3.7088)  class_acc: 0.3750 (0.3452)  loss_scale: 65536.0000 (48501.6748)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.1968  max mem: 15572
Epoch: [38]  [ 790/2809]  eta: 0:19:20  lr: 0.000000  min_lr: 0.000000  loss: 3.8337 (3.7083)  class_acc: 0.2917 (0.3457)  loss_scale: 32768.0000 (48302.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5837  data: 0.1302  max mem: 15572
Epoch: [38]  [ 800/2809]  eta: 0:19:13  lr: 0.000000  min_lr: 0.000000  loss: 3.8043 (3.7083)  class_acc: 0.3333 (0.3454)  loss_scale: 32768.0000 (48108.8240)  weight_decay: 0.0500 (0.0500)  time: 0.5031  data: 0.0371  max mem: 15572
Epoch: [38]  [ 810/2809]  eta: 0:19:08  lr: 0.000000  min_lr: 0.000000  loss: 3.7268 (3.7087)  class_acc: 0.3333 (0.3455)  loss_scale: 32768.0000 (47919.6646)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.0924  max mem: 15572
Epoch: [38]  [ 820/2809]  eta: 0:19:02  lr: 0.000000  min_lr: 0.000000  loss: 3.8146 (3.7066)  class_acc: 0.3333 (0.3453)  loss_scale: 32768.0000 (47735.1133)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.1233  max mem: 15572
Epoch: [38]  [ 830/2809]  eta: 0:18:54  lr: 0.000000  min_lr: 0.000000  loss: 3.8610 (3.7093)  class_acc: 0.2500 (0.3446)  loss_scale: 32768.0000 (47555.0036)  weight_decay: 0.0500 (0.0500)  time: 0.5232  data: 0.0738  max mem: 15572
Epoch: [38]  [ 840/2809]  eta: 0:18:47  lr: 0.000000  min_lr: 0.000000  loss: 3.9047 (3.7087)  class_acc: 0.3333 (0.3451)  loss_scale: 32768.0000 (47379.1772)  weight_decay: 0.0500 (0.0500)  time: 0.4961  data: 0.0515  max mem: 15572
Epoch: [38]  [ 850/2809]  eta: 0:18:40  lr: 0.000000  min_lr: 0.000000  loss: 3.8437 (3.7095)  class_acc: 0.3333 (0.3445)  loss_scale: 32768.0000 (47207.4830)  weight_decay: 0.0500 (0.0500)  time: 0.5229  data: 0.0780  max mem: 15572
Epoch: [38]  [ 860/2809]  eta: 0:18:35  lr: 0.000000  min_lr: 0.000000  loss: 3.8437 (3.7098)  class_acc: 0.3333 (0.3448)  loss_scale: 32768.0000 (47039.7770)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1203  max mem: 15572
Epoch: [38]  [ 870/2809]  eta: 0:18:29  lr: 0.000000  min_lr: 0.000000  loss: 3.7768 (3.7123)  class_acc: 0.3750 (0.3444)  loss_scale: 32768.0000 (46875.9219)  weight_decay: 0.0500 (0.0500)  time: 0.5833  data: 0.1447  max mem: 15572
Epoch: [38]  [ 880/2809]  eta: 0:18:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7685 (3.7124)  class_acc: 0.3750 (0.3446)  loss_scale: 32768.0000 (46715.7866)  weight_decay: 0.0500 (0.0500)  time: 0.5732  data: 0.1470  max mem: 15572
Epoch: [38]  [ 890/2809]  eta: 0:18:19  lr: 0.000000  min_lr: 0.000000  loss: 3.6595 (3.7159)  class_acc: 0.2917 (0.3436)  loss_scale: 32768.0000 (46559.2458)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1709  max mem: 15572
Epoch: [38]  [ 900/2809]  eta: 0:18:13  lr: 0.000000  min_lr: 0.000000  loss: 3.9948 (3.7179)  class_acc: 0.2917 (0.3432)  loss_scale: 32768.0000 (46406.1798)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.1573  max mem: 15572
[2025-01-16 08:30:04,639] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:30:04,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-01-16 08:30:09,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107651
[2025-01-16 08:30:09,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:30:09,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 910/2809]  eta: 0:18:08  lr: 0.000000  min_lr: 0.000000  loss: 3.9948 (3.7199)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (46508.2591)  weight_decay: 0.0500 (0.0500)  time: 0.5741  data: 0.1450  max mem: 15572
Epoch: [38]  [ 920/2809]  eta: 0:18:02  lr: 0.000000  min_lr: 0.000000  loss: 3.8735 (3.7200)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (46359.0706)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1533  max mem: 15572
Epoch: [38]  [ 930/2809]  eta: 0:17:55  lr: 0.000000  min_lr: 0.000000  loss: 3.6782 (3.7205)  class_acc: 0.3333 (0.3423)  loss_scale: 32768.0000 (46213.0870)  weight_decay: 0.0500 (0.0500)  time: 0.5472  data: 0.1248  max mem: 15572
Epoch: [38]  [ 940/2809]  eta: 0:17:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7059 (3.7203)  class_acc: 0.3333 (0.3421)  loss_scale: 32768.0000 (46070.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1413  max mem: 15572
Epoch: [38]  [ 950/2809]  eta: 0:17:42  lr: 0.000000  min_lr: 0.000000  loss: 3.7634 (3.7209)  class_acc: 0.3333 (0.3418)  loss_scale: 32768.0000 (45930.3302)  weight_decay: 0.0500 (0.0500)  time: 0.5405  data: 0.0964  max mem: 15572
Epoch: [38]  [ 960/2809]  eta: 0:17:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8098 (3.7215)  class_acc: 0.3333 (0.3422)  loss_scale: 32768.0000 (45793.3652)  weight_decay: 0.0500 (0.0500)  time: 0.5515  data: 0.1101  max mem: 15572
Epoch: [38]  [ 970/2809]  eta: 0:17:30  lr: 0.000000  min_lr: 0.000000  loss: 3.8496 (3.7219)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (45659.2214)  weight_decay: 0.0500 (0.0500)  time: 0.5563  data: 0.1017  max mem: 15572
Epoch: [38]  [ 980/2809]  eta: 0:17:24  lr: 0.000000  min_lr: 0.000000  loss: 3.9104 (3.7240)  class_acc: 0.2917 (0.3415)  loss_scale: 32768.0000 (45527.8124)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0397  max mem: 15572
Epoch: [38]  [ 990/2809]  eta: 0:17:19  lr: 0.000000  min_lr: 0.000000  loss: 3.6773 (3.7217)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (45399.0555)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1127  max mem: 15572
Epoch: [38]  [1000/2809]  eta: 0:17:13  lr: 0.000000  min_lr: 0.000000  loss: 3.5627 (3.7188)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (45272.8711)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.1284  max mem: 15572
Epoch: [38]  [1010/2809]  eta: 0:17:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6142 (3.7187)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (45149.1830)  weight_decay: 0.0500 (0.0500)  time: 0.5533  data: 0.0941  max mem: 15572
Epoch: [38]  [1020/2809]  eta: 0:17:02  lr: 0.000000  min_lr: 0.000000  loss: 3.8043 (3.7190)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (45027.9177)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.1045  max mem: 15572
Epoch: [38]  [1030/2809]  eta: 0:16:56  lr: 0.000000  min_lr: 0.000000  loss: 3.9132 (3.7201)  class_acc: 0.3333 (0.3423)  loss_scale: 32768.0000 (44909.0048)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1315  max mem: 15572
[2025-01-16 08:31:22,364] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:31:22,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1040/2809]  eta: 0:16:51  lr: 0.000000  min_lr: 0.000000  loss: 3.7158 (3.7199)  class_acc: 0.3750 (0.3425)  loss_scale: 32768.0000 (44886.8088)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1655  max mem: 15572
Epoch: [38]  [1050/2809]  eta: 0:16:47  lr: 0.000000  min_lr: 0.000000  loss: 3.6370 (3.7207)  class_acc: 0.3333 (0.3417)  loss_scale: 65536.0000 (45083.2807)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.1974  max mem: 15572
Epoch: [38]  [1060/2809]  eta: 0:16:40  lr: 0.000000  min_lr: 0.000000  loss: 3.7819 (3.7227)  class_acc: 0.2917 (0.3413)  loss_scale: 65536.0000 (45276.0490)  weight_decay: 0.0500 (0.0500)  time: 0.5725  data: 0.1360  max mem: 15572
Epoch: [38]  [1070/2809]  eta: 0:16:35  lr: 0.000000  min_lr: 0.000000  loss: 3.9229 (3.7253)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (45465.2176)  weight_decay: 0.0500 (0.0500)  time: 0.5660  data: 0.1290  max mem: 15572
Epoch: [38]  [1080/2809]  eta: 0:16:29  lr: 0.000000  min_lr: 0.000000  loss: 3.9264 (3.7266)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (45650.8862)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1317  max mem: 15572
Epoch: [38]  [1090/2809]  eta: 0:16:23  lr: 0.000000  min_lr: 0.000000  loss: 3.9234 (3.7277)  class_acc: 0.2500 (0.3403)  loss_scale: 65536.0000 (45833.1512)  weight_decay: 0.0500 (0.0500)  time: 0.5455  data: 0.0633  max mem: 15572
Epoch: [38]  [1100/2809]  eta: 0:16:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8601 (3.7281)  class_acc: 0.2500 (0.3398)  loss_scale: 65536.0000 (46012.1054)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0587  max mem: 15572
Epoch: [38]  [1110/2809]  eta: 0:16:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8484 (3.7299)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (46187.8380)  weight_decay: 0.0500 (0.0500)  time: 0.5363  data: 0.0863  max mem: 15572
Epoch: [38]  [1120/2809]  eta: 0:16:05  lr: 0.000000  min_lr: 0.000000  loss: 3.8979 (3.7306)  class_acc: 0.2917 (0.3397)  loss_scale: 65536.0000 (46360.4353)  weight_decay: 0.0500 (0.0500)  time: 0.5520  data: 0.1093  max mem: 15572
Epoch: [38]  [1130/2809]  eta: 0:15:59  lr: 0.000000  min_lr: 0.000000  loss: 3.8979 (3.7310)  class_acc: 0.2917 (0.3393)  loss_scale: 65536.0000 (46529.9805)  weight_decay: 0.0500 (0.0500)  time: 0.5836  data: 0.1516  max mem: 15572
Epoch: [38]  [1140/2809]  eta: 0:15:53  lr: 0.000000  min_lr: 0.000000  loss: 3.7410 (3.7320)  class_acc: 0.2917 (0.3389)  loss_scale: 65536.0000 (46696.5539)  weight_decay: 0.0500 (0.0500)  time: 0.5807  data: 0.1526  max mem: 15572
Epoch: [38]  [1150/2809]  eta: 0:15:47  lr: 0.000000  min_lr: 0.000000  loss: 3.8234 (3.7322)  class_acc: 0.3333 (0.3388)  loss_scale: 65536.0000 (46860.2328)  weight_decay: 0.0500 (0.0500)  time: 0.5281  data: 0.0988  max mem: 15572
Epoch: [38]  [1160/2809]  eta: 0:15:40  lr: 0.000000  min_lr: 0.000000  loss: 3.7575 (3.7307)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (47021.0922)  weight_decay: 0.0500 (0.0500)  time: 0.5056  data: 0.0806  max mem: 15572
[2025-01-16 08:32:32,847] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:32:32,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:32:33,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 107910
[2025-01-16 08:32:33,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:32:33,647] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [1170/2809]  eta: 0:15:35  lr: 0.000000  min_lr: 0.000000  loss: 3.7575 (3.7313)  class_acc: 0.2917 (0.3387)  loss_scale: 65536.0000 (47291.1358)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.1340  max mem: 15572
Epoch: [38]  [1180/2809]  eta: 0:15:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8395 (3.7324)  class_acc: 0.2917 (0.3382)  loss_scale: 65536.0000 (47445.6224)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.1453  max mem: 15572
Epoch: [38]  [1190/2809]  eta: 0:15:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7919 (3.7317)  class_acc: 0.2917 (0.3384)  loss_scale: 65536.0000 (47597.5147)  weight_decay: 0.0500 (0.0500)  time: 0.5270  data: 0.0893  max mem: 15572
Epoch: [38]  [1200/2809]  eta: 0:15:16  lr: 0.000000  min_lr: 0.000000  loss: 3.6901 (3.7316)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (47746.8776)  weight_decay: 0.0500 (0.0500)  time: 0.5162  data: 0.0810  max mem: 15572
Epoch: [38]  [1210/2809]  eta: 0:15:11  lr: 0.000000  min_lr: 0.000000  loss: 3.6855 (3.7313)  class_acc: 0.3333 (0.3389)  loss_scale: 65536.0000 (47893.7737)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.1158  max mem: 15572
Epoch: [38]  [1220/2809]  eta: 0:15:05  lr: 0.000000  min_lr: 0.000000  loss: 3.6787 (3.7311)  class_acc: 0.3750 (0.3388)  loss_scale: 65536.0000 (48038.2637)  weight_decay: 0.0500 (0.0500)  time: 0.5912  data: 0.1512  max mem: 15572
Epoch: [38]  [1230/2809]  eta: 0:14:59  lr: 0.000000  min_lr: 0.000000  loss: 3.7874 (3.7321)  class_acc: 0.2917 (0.3385)  loss_scale: 65536.0000 (48180.4062)  weight_decay: 0.0500 (0.0500)  time: 0.5659  data: 0.1088  max mem: 15572
Epoch: [38]  [1240/2809]  eta: 0:14:52  lr: 0.000000  min_lr: 0.000000  loss: 3.9347 (3.7327)  class_acc: 0.2917 (0.3383)  loss_scale: 65536.0000 (48320.2579)  weight_decay: 0.0500 (0.0500)  time: 0.5019  data: 0.0495  max mem: 15572
Epoch: [38]  [1250/2809]  eta: 0:14:48  lr: 0.000000  min_lr: 0.000000  loss: 3.7959 (3.7324)  class_acc: 0.3333 (0.3385)  loss_scale: 65536.0000 (48457.8737)  weight_decay: 0.0500 (0.0500)  time: 0.5655  data: 0.1284  max mem: 15572
[2025-01-16 08:33:24,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=108000, skipped=719, lr=[2.6532963583675663e-09, 2.6532963583675663e-09, 3.790423369096524e-09, 3.790423369096524e-09, 5.414890527280749e-09, 5.414890527280749e-09, 7.735557896115355e-09, 7.735557896115355e-09, 1.1050796994450509e-08, 1.1050796994450509e-08, 1.5786852849215013e-08, 1.5786852849215013e-08, 2.255264692745002e-08, 2.255264692745002e-08, 3.221806703921432e-08, 3.221806703921432e-08, 4.602581005602045e-08, 4.602581005602045e-08, 6.575115722288638e-08, 6.575115722288638e-08, 9.393022460412338e-08, 9.393022460412338e-08, 1.341860351487477e-07, 1.341860351487477e-07, 1.9169433592678244e-07, 1.9169433592678244e-07, 2.7384905132397494e-07, 2.7384905132397494e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:33:24,732] [INFO] [timer.py:260:stop] epoch=0/micro_step=108000/global_step=108000, RunningAvgSamplesPerSec=28.58623663787403, CurrSamplesPerSec=31.330762464821742, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [1260/2809]  eta: 0:14:42  lr: 0.000000  min_lr: 0.000000  loss: 3.6330 (3.7318)  class_acc: 0.3750 (0.3389)  loss_scale: 65536.0000 (48593.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.1669  max mem: 15572
Epoch: [38]  [1270/2809]  eta: 0:14:37  lr: 0.000000  min_lr: 0.000000  loss: 3.6115 (3.7309)  class_acc: 0.4167 (0.3395)  loss_scale: 65536.0000 (48726.6090)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.1570  max mem: 15572
Epoch: [38]  [1280/2809]  eta: 0:14:32  lr: 0.000000  min_lr: 0.000000  loss: 3.5821 (3.7302)  class_acc: 0.3750 (0.3397)  loss_scale: 65536.0000 (48857.8298)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1889  max mem: 15572
Epoch: [38]  [1290/2809]  eta: 0:14:25  lr: 0.000000  min_lr: 0.000000  loss: 3.8228 (3.7305)  class_acc: 0.2917 (0.3393)  loss_scale: 65536.0000 (48987.0178)  weight_decay: 0.0500 (0.0500)  time: 0.5259  data: 0.0916  max mem: 15572
[2025-01-16 08:33:46,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:33:46,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:33:46,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108040
[2025-01-16 08:33:46,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:33:46,911] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [1300/2809]  eta: 0:14:18  lr: 0.000000  min_lr: 0.000000  loss: 3.5414 (3.7280)  class_acc: 0.3333 (0.3400)  loss_scale: 65536.0000 (49164.5934)  weight_decay: 0.0500 (0.0500)  time: 0.4539  data: 0.0114  max mem: 15572
Epoch: [38]  [1310/2809]  eta: 0:14:11  lr: 0.000000  min_lr: 0.000000  loss: 3.4510 (3.7262)  class_acc: 0.3750 (0.3404)  loss_scale: 65536.0000 (49289.4706)  weight_decay: 0.0500 (0.0500)  time: 0.4650  data: 0.0112  max mem: 15572
Epoch: [38]  [1320/2809]  eta: 0:14:06  lr: 0.000000  min_lr: 0.000000  loss: 3.4946 (3.7262)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (49412.4572)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.1077  max mem: 15572
Epoch: [38]  [1330/2809]  eta: 0:14:00  lr: 0.000000  min_lr: 0.000000  loss: 3.7955 (3.7261)  class_acc: 0.2917 (0.3402)  loss_scale: 65536.0000 (49533.5958)  weight_decay: 0.0500 (0.0500)  time: 0.5771  data: 0.1360  max mem: 15572
Epoch: [38]  [1340/2809]  eta: 0:13:54  lr: 0.000000  min_lr: 0.000000  loss: 3.7147 (3.7254)  class_acc: 0.3333 (0.3401)  loss_scale: 65536.0000 (49652.9277)  weight_decay: 0.0500 (0.0500)  time: 0.5398  data: 0.0858  max mem: 15572
Epoch: [38]  [1350/2809]  eta: 0:13:49  lr: 0.000000  min_lr: 0.000000  loss: 3.7376 (3.7257)  class_acc: 0.3750 (0.3403)  loss_scale: 65536.0000 (49770.4930)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1449  max mem: 15572
Epoch: [38]  [1360/2809]  eta: 0:13:43  lr: 0.000000  min_lr: 0.000000  loss: 3.9107 (3.7269)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (49886.3306)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1339  max mem: 15572
Epoch: [38]  [1370/2809]  eta: 0:13:36  lr: 0.000000  min_lr: 0.000000  loss: 4.0561 (3.7280)  class_acc: 0.2917 (0.3400)  loss_scale: 65536.0000 (50000.4785)  weight_decay: 0.0500 (0.0500)  time: 0.4982  data: 0.0584  max mem: 15572
Epoch: [38]  [1380/2809]  eta: 0:13:31  lr: 0.000000  min_lr: 0.000000  loss: 3.5424 (3.7261)  class_acc: 0.3750 (0.3404)  loss_scale: 65536.0000 (50112.9732)  weight_decay: 0.0500 (0.0500)  time: 0.5126  data: 0.0834  max mem: 15572
Epoch: [38]  [1390/2809]  eta: 0:13:25  lr: 0.000000  min_lr: 0.000000  loss: 3.5946 (3.7264)  class_acc: 0.3750 (0.3402)  loss_scale: 65536.0000 (50223.8505)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1346  max mem: 15572
Epoch: [38]  [1400/2809]  eta: 0:13:20  lr: 0.000000  min_lr: 0.000000  loss: 3.6878 (3.7260)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (50333.1449)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.1337  max mem: 15572
Epoch: [38]  [1410/2809]  eta: 0:13:14  lr: 0.000000  min_lr: 0.000000  loss: 3.6878 (3.7247)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (50440.8901)  weight_decay: 0.0500 (0.0500)  time: 0.5699  data: 0.1446  max mem: 15572
Epoch: [38]  [1420/2809]  eta: 0:13:09  lr: 0.000000  min_lr: 0.000000  loss: 3.7245 (3.7249)  class_acc: 0.3333 (0.3405)  loss_scale: 65536.0000 (50547.1189)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.1724  max mem: 15572
[2025-01-16 08:34:59,964] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:34:59,964] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:35:01,635] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108172
[2025-01-16 08:35:01,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:35:01,635] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [1430/2809]  eta: 0:13:04  lr: 0.000000  min_lr: 0.000000  loss: 3.8371 (3.7248)  class_acc: 0.3333 (0.3408)  loss_scale: 65536.0000 (50789.2551)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.1706  max mem: 15572
Epoch: [38]  [1440/2809]  eta: 0:12:58  lr: 0.000000  min_lr: 0.000000  loss: 3.8371 (3.7250)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (50891.5920)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1361  max mem: 15572
Epoch: [38]  [1450/2809]  eta: 0:12:52  lr: 0.000000  min_lr: 0.000000  loss: 3.8033 (3.7257)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (50992.5183)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.1210  max mem: 15572
Epoch: [38]  [1460/2809]  eta: 0:12:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8033 (3.7273)  class_acc: 0.2500 (0.3402)  loss_scale: 65536.0000 (51092.0630)  weight_decay: 0.0500 (0.0500)  time: 0.5509  data: 0.0988  max mem: 15572
Epoch: [38]  [1470/2809]  eta: 0:12:41  lr: 0.000000  min_lr: 0.000000  loss: 3.8306 (3.7275)  class_acc: 0.3333 (0.3406)  loss_scale: 65536.0000 (51190.2542)  weight_decay: 0.0500 (0.0500)  time: 0.5489  data: 0.1119  max mem: 15572
Epoch: [38]  [1480/2809]  eta: 0:12:35  lr: 0.000000  min_lr: 0.000000  loss: 3.8306 (3.7276)  class_acc: 0.3750 (0.3407)  loss_scale: 65536.0000 (51287.1195)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1689  max mem: 15572
Epoch: [38]  [1490/2809]  eta: 0:12:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8747 (3.7280)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51382.6854)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1468  max mem: 15572
Epoch: [38]  [1500/2809]  eta: 0:12:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7827 (3.7268)  class_acc: 0.3333 (0.3411)  loss_scale: 65536.0000 (51476.9780)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.1721  max mem: 15572
Epoch: [38]  [1510/2809]  eta: 0:12:18  lr: 0.000000  min_lr: 0.000000  loss: 3.5869 (3.7261)  class_acc: 0.3750 (0.3415)  loss_scale: 65536.0000 (51570.0225)  weight_decay: 0.0500 (0.0500)  time: 0.5811  data: 0.1401  max mem: 15572
Epoch: [38]  [1520/2809]  eta: 0:12:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7058 (3.7261)  class_acc: 0.3750 (0.3413)  loss_scale: 65536.0000 (51661.8435)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.1165  max mem: 15572
Epoch: [38]  [1530/2809]  eta: 0:12:07  lr: 0.000000  min_lr: 0.000000  loss: 3.8174 (3.7266)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (51752.4651)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1562  max mem: 15572
Epoch: [38]  [1540/2809]  eta: 0:12:02  lr: 0.000000  min_lr: 0.000000  loss: 3.7180 (3.7253)  class_acc: 0.2917 (0.3410)  loss_scale: 65536.0000 (51841.9104)  weight_decay: 0.0500 (0.0500)  time: 0.6133  data: 0.1619  max mem: 15572
Epoch: [38]  [1550/2809]  eta: 0:11:56  lr: 0.000000  min_lr: 0.000000  loss: 3.4881 (3.7240)  class_acc: 0.3333 (0.3413)  loss_scale: 65536.0000 (51930.2025)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1234  max mem: 15572
[2025-01-16 08:36:16,347] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:36:16,347] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [38]  [1560/2809]  eta: 0:11:51  lr: 0.000000  min_lr: 0.000000  loss: 3.8489 (3.7256)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52101.3299)  weight_decay: 0.0500 (0.0500)  time: 0.5475  data: 0.0988  max mem: 15572
[2025-01-16 08:36:17,563] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108304
[2025-01-16 08:36:17,563] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:36:17,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [1570/2809]  eta: 0:11:44  lr: 0.000000  min_lr: 0.000000  loss: 3.9036 (3.7254)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52228.5627)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.1087  max mem: 15572
Epoch: [38]  [1580/2809]  eta: 0:11:38  lr: 0.000000  min_lr: 0.000000  loss: 3.6492 (3.7243)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (52312.7337)  weight_decay: 0.0500 (0.0500)  time: 0.4834  data: 0.0627  max mem: 15572
Epoch: [38]  [1590/2809]  eta: 0:11:33  lr: 0.000000  min_lr: 0.000000  loss: 3.6492 (3.7256)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52395.8466)  weight_decay: 0.0500 (0.0500)  time: 0.5581  data: 0.1165  max mem: 15572
Epoch: [38]  [1600/2809]  eta: 0:11:27  lr: 0.000000  min_lr: 0.000000  loss: 3.9124 (3.7258)  class_acc: 0.2917 (0.3409)  loss_scale: 65536.0000 (52477.9213)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1423  max mem: 15572
Epoch: [38]  [1610/2809]  eta: 0:11:21  lr: 0.000000  min_lr: 0.000000  loss: 3.7315 (3.7252)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (52558.9770)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1367  max mem: 15572
Epoch: [38]  [1620/2809]  eta: 0:11:15  lr: 0.000000  min_lr: 0.000000  loss: 3.6616 (3.7246)  class_acc: 0.3750 (0.3417)  loss_scale: 65536.0000 (52639.0327)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1180  max mem: 15572
[2025-01-16 08:36:53,187] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108367
[2025-01-16 08:36:53,187] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:36:53,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [1630/2809]  eta: 0:11:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6616 (3.7245)  class_acc: 0.3750 (0.3417)  loss_scale: 65536.0000 (52597.5622)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1174  max mem: 15572
Epoch: [38]  [1640/2809]  eta: 0:11:04  lr: 0.000000  min_lr: 0.000000  loss: 3.6946 (3.7250)  class_acc: 0.3333 (0.3415)  loss_scale: 32768.0000 (52476.7239)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1253  max mem: 15572
Epoch: [38]  [1650/2809]  eta: 0:10:59  lr: 0.000000  min_lr: 0.000000  loss: 3.9434 (3.7260)  class_acc: 0.2917 (0.3413)  loss_scale: 32768.0000 (52357.3495)  weight_decay: 0.0500 (0.0500)  time: 0.5716  data: 0.1258  max mem: 15572
Epoch: [38]  [1660/2809]  eta: 0:10:53  lr: 0.000000  min_lr: 0.000000  loss: 3.7686 (3.7262)  class_acc: 0.2917 (0.3410)  loss_scale: 32768.0000 (52239.4124)  weight_decay: 0.0500 (0.0500)  time: 0.5637  data: 0.1083  max mem: 15572
Epoch: [38]  [1670/2809]  eta: 0:10:47  lr: 0.000000  min_lr: 0.000000  loss: 3.7206 (3.7275)  class_acc: 0.2500 (0.3407)  loss_scale: 32768.0000 (52122.8869)  weight_decay: 0.0500 (0.0500)  time: 0.5466  data: 0.0874  max mem: 15572
Epoch: [38]  [1680/2809]  eta: 0:10:42  lr: 0.000000  min_lr: 0.000000  loss: 3.7206 (3.7273)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (52007.7478)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.1362  max mem: 15572
Epoch: [38]  [1690/2809]  eta: 0:10:35  lr: 0.000000  min_lr: 0.000000  loss: 3.7985 (3.7275)  class_acc: 0.3333 (0.3407)  loss_scale: 32768.0000 (51893.9704)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.0933  max mem: 15572
Epoch: [38]  [1700/2809]  eta: 0:10:29  lr: 0.000000  min_lr: 0.000000  loss: 3.9326 (3.7281)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (51781.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5171  data: 0.0813  max mem: 15572
Epoch: [38]  [1710/2809]  eta: 0:10:24  lr: 0.000000  min_lr: 0.000000  loss: 3.8854 (3.7277)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (51670.4056)  weight_decay: 0.0500 (0.0500)  time: 0.5700  data: 0.1353  max mem: 15572
Epoch: [38]  [1720/2809]  eta: 0:10:17  lr: 0.000000  min_lr: 0.000000  loss: 3.9012 (3.7283)  class_acc: 0.3333 (0.3408)  loss_scale: 32768.0000 (51560.5718)  weight_decay: 0.0500 (0.0500)  time: 0.5128  data: 0.0748  max mem: 15572
Epoch: [38]  [1730/2809]  eta: 0:10:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7327 (3.7273)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (51452.0069)  weight_decay: 0.0500 (0.0500)  time: 0.5239  data: 0.0754  max mem: 15572
Epoch: [38]  [1740/2809]  eta: 0:10:06  lr: 0.000000  min_lr: 0.000000  loss: 3.7307 (3.7272)  class_acc: 0.3333 (0.3409)  loss_scale: 32768.0000 (51344.6893)  weight_decay: 0.0500 (0.0500)  time: 0.5868  data: 0.1423  max mem: 15572
Epoch: [38]  [1750/2809]  eta: 0:10:00  lr: 0.000000  min_lr: 0.000000  loss: 3.8254 (3.7280)  class_acc: 0.2500 (0.3406)  loss_scale: 32768.0000 (51238.5974)  weight_decay: 0.0500 (0.0500)  time: 0.5286  data: 0.0835  max mem: 15572
[2025-01-16 08:38:02,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:38:02,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [1760/2809]  eta: 0:09:54  lr: 0.000000  min_lr: 0.000000  loss: 3.8991 (3.7285)  class_acc: 0.2500 (0.3406)  loss_scale: 32768.0000 (51263.9637)  weight_decay: 0.0500 (0.0500)  time: 0.5271  data: 0.0780  max mem: 15572
Epoch: [38]  [1770/2809]  eta: 0:09:49  lr: 0.000000  min_lr: 0.000000  loss: 3.8286 (3.7285)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (51344.5511)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1567  max mem: 15572
Epoch: [38]  [1780/2809]  eta: 0:09:43  lr: 0.000000  min_lr: 0.000000  loss: 3.7969 (3.7287)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51424.2336)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1446  max mem: 15572
Epoch: [38]  [1790/2809]  eta: 0:09:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8042 (3.7285)  class_acc: 0.3333 (0.3407)  loss_scale: 65536.0000 (51503.0262)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.1639  max mem: 15572
Epoch: [38]  [1800/2809]  eta: 0:09:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7199 (3.7289)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (51580.9439)  weight_decay: 0.0500 (0.0500)  time: 0.5755  data: 0.1394  max mem: 15572
Epoch: [38]  [1810/2809]  eta: 0:09:26  lr: 0.000000  min_lr: 0.000000  loss: 3.8311 (3.7295)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (51658.0011)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0749  max mem: 15572
Epoch: [38]  [1820/2809]  eta: 0:09:20  lr: 0.000000  min_lr: 0.000000  loss: 3.8311 (3.7288)  class_acc: 0.2917 (0.3401)  loss_scale: 65536.0000 (51734.2120)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0832  max mem: 15572
Epoch: [38]  [1830/2809]  eta: 0:09:15  lr: 0.000000  min_lr: 0.000000  loss: 3.5418 (3.7285)  class_acc: 0.3750 (0.3403)  loss_scale: 65536.0000 (51809.5904)  weight_decay: 0.0500 (0.0500)  time: 0.5486  data: 0.1117  max mem: 15572
Epoch: [38]  [1840/2809]  eta: 0:09:09  lr: 0.000000  min_lr: 0.000000  loss: 3.5315 (3.7274)  class_acc: 0.3333 (0.3403)  loss_scale: 65536.0000 (51884.1499)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1328  max mem: 15572
Epoch: [38]  [1850/2809]  eta: 0:09:03  lr: 0.000000  min_lr: 0.000000  loss: 3.4376 (3.7269)  class_acc: 0.3333 (0.3401)  loss_scale: 65536.0000 (51957.9038)  weight_decay: 0.0500 (0.0500)  time: 0.5532  data: 0.1186  max mem: 15572
Epoch: [38]  [1860/2809]  eta: 0:08:57  lr: 0.000000  min_lr: 0.000000  loss: 3.4736 (3.7262)  class_acc: 0.3750 (0.3405)  loss_scale: 65536.0000 (52030.8651)  weight_decay: 0.0500 (0.0500)  time: 0.5344  data: 0.0916  max mem: 15572
Epoch: [38]  [1870/2809]  eta: 0:08:52  lr: 0.000000  min_lr: 0.000000  loss: 3.6746 (3.7266)  class_acc: 0.3333 (0.3404)  loss_scale: 65536.0000 (52103.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1199  max mem: 15572
Epoch: [38]  [1880/2809]  eta: 0:08:46  lr: 0.000000  min_lr: 0.000000  loss: 4.0111 (3.7276)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (52174.4604)  weight_decay: 0.0500 (0.0500)  time: 0.5487  data: 0.1113  max mem: 15572
[2025-01-16 08:39:14,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:39:14,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:39:14,948] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108625
[2025-01-16 08:39:14,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:39:14,948] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [1890/2809]  eta: 0:08:40  lr: 0.000000  min_lr: 0.000000  loss: 3.9456 (3.7282)  class_acc: 0.2500 (0.3401)  loss_scale: 65536.0000 (52279.7758)  weight_decay: 0.0500 (0.0500)  time: 0.5081  data: 0.0849  max mem: 15572
Epoch: [38]  [1900/2809]  eta: 0:08:35  lr: 0.000000  min_lr: 0.000000  loss: 3.7932 (3.7281)  class_acc: 0.3333 (0.3402)  loss_scale: 65536.0000 (52349.5087)  weight_decay: 0.0500 (0.0500)  time: 0.5715  data: 0.1475  max mem: 15572
Epoch: [38]  [1910/2809]  eta: 0:08:29  lr: 0.000000  min_lr: 0.000000  loss: 3.7121 (3.7288)  class_acc: 0.3750 (0.3404)  loss_scale: 65536.0000 (52418.5118)  weight_decay: 0.0500 (0.0500)  time: 0.5689  data: 0.1184  max mem: 15572
Epoch: [38]  [1920/2809]  eta: 0:08:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7121 (3.7278)  class_acc: 0.3750 (0.3406)  loss_scale: 65536.0000 (52486.7965)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.1086  max mem: 15572
Epoch: [38]  [1930/2809]  eta: 0:08:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6286 (3.7278)  class_acc: 0.3750 (0.3408)  loss_scale: 65536.0000 (52554.3739)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.1494  max mem: 15572
Epoch: [38]  [1940/2809]  eta: 0:08:12  lr: 0.000000  min_lr: 0.000000  loss: 3.6263 (3.7276)  class_acc: 0.3750 (0.3407)  loss_scale: 65536.0000 (52621.2550)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.1315  max mem: 15572
Epoch: [38]  [1950/2809]  eta: 0:08:07  lr: 0.000000  min_lr: 0.000000  loss: 3.7375 (3.7278)  class_acc: 0.2917 (0.3407)  loss_scale: 65536.0000 (52687.4505)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1896  max mem: 15572
Epoch: [38]  [1960/2809]  eta: 0:08:01  lr: 0.000000  min_lr: 0.000000  loss: 3.6801 (3.7277)  class_acc: 0.3750 (0.3410)  loss_scale: 65536.0000 (52752.9709)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1575  max mem: 15572
Epoch: [38]  [1970/2809]  eta: 0:07:55  lr: 0.000000  min_lr: 0.000000  loss: 3.6182 (3.7270)  class_acc: 0.3750 (0.3412)  loss_scale: 65536.0000 (52817.8265)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1039  max mem: 15572
Epoch: [38]  [1980/2809]  eta: 0:07:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7872 (3.7277)  class_acc: 0.3333 (0.3409)  loss_scale: 65536.0000 (52882.0273)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.2155  max mem: 15572
Epoch: [38]  [1990/2809]  eta: 0:07:44  lr: 0.000000  min_lr: 0.000000  loss: 3.9908 (3.7288)  class_acc: 0.2917 (0.3408)  loss_scale: 65536.0000 (52945.5831)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.2033  max mem: 15572
Epoch: [38]  [2000/2809]  eta: 0:07:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8726 (3.7288)  class_acc: 0.3333 (0.3410)  loss_scale: 65536.0000 (53008.5037)  weight_decay: 0.0500 (0.0500)  time: 0.5129  data: 0.0866  max mem: 15572
Epoch: [38]  [2010/2809]  eta: 0:07:33  lr: 0.000000  min_lr: 0.000000  loss: 3.6051 (3.7274)  class_acc: 0.3750 (0.3415)  loss_scale: 65536.0000 (53070.7986)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.1072  max mem: 15572
[2025-01-16 08:40:30,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:40:30,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:40:34,282] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108759
[2025-01-16 08:40:34,282] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:40:34,284] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [2020/2809]  eta: 0:07:28  lr: 0.000000  min_lr: 0.000000  loss: 3.6051 (3.7271)  class_acc: 0.3750 (0.3416)  loss_scale: 65536.0000 (53294.6145)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.1737  max mem: 15572
Epoch: [38]  [2030/2809]  eta: 0:07:21  lr: 0.000000  min_lr: 0.000000  loss: 3.6131 (3.7265)  class_acc: 0.4167 (0.3419)  loss_scale: 65536.0000 (53354.8872)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.0920  max mem: 15572
Epoch: [38]  [2040/2809]  eta: 0:07:16  lr: 0.000000  min_lr: 0.000000  loss: 3.6575 (3.7262)  class_acc: 0.4167 (0.3421)  loss_scale: 65536.0000 (53414.5693)  weight_decay: 0.0500 (0.0500)  time: 0.4924  data: 0.0487  max mem: 15572
Epoch: [38]  [2050/2809]  eta: 0:07:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6578 (3.7257)  class_acc: 0.3750 (0.3421)  loss_scale: 65536.0000 (53473.6694)  weight_decay: 0.0500 (0.0500)  time: 0.5914  data: 0.1541  max mem: 15572
Epoch: [38]  [2060/2809]  eta: 0:07:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7729 (3.7263)  class_acc: 0.2917 (0.3419)  loss_scale: 65536.0000 (53532.1960)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1988  max mem: 15572
Epoch: [38]  [2070/2809]  eta: 0:06:59  lr: 0.000000  min_lr: 0.000000  loss: 3.7852 (3.7264)  class_acc: 0.2917 (0.3419)  loss_scale: 65536.0000 (53590.1574)  weight_decay: 0.0500 (0.0500)  time: 0.5865  data: 0.1546  max mem: 15572
Epoch: [38]  [2080/2809]  eta: 0:06:53  lr: 0.000000  min_lr: 0.000000  loss: 3.7884 (3.7271)  class_acc: 0.3333 (0.3418)  loss_scale: 65536.0000 (53647.5617)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1362  max mem: 15572
Epoch: [38]  [2090/2809]  eta: 0:06:48  lr: 0.000000  min_lr: 0.000000  loss: 3.7353 (3.7272)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (53704.4170)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.1779  max mem: 15572
Epoch: [38]  [2100/2809]  eta: 0:06:42  lr: 0.000000  min_lr: 0.000000  loss: 3.6920 (3.7273)  class_acc: 0.3333 (0.3419)  loss_scale: 65536.0000 (53760.7311)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1082  max mem: 15572
Epoch: [38]  [2110/2809]  eta: 0:06:36  lr: 0.000000  min_lr: 0.000000  loss: 3.7682 (3.7271)  class_acc: 0.3333 (0.3420)  loss_scale: 65536.0000 (53816.5116)  weight_decay: 0.0500 (0.0500)  time: 0.5224  data: 0.0881  max mem: 15572
Epoch: [38]  [2120/2809]  eta: 0:06:31  lr: 0.000000  min_lr: 0.000000  loss: 3.7532 (3.7263)  class_acc: 0.3750 (0.3424)  loss_scale: 65536.0000 (53871.7661)  weight_decay: 0.0500 (0.0500)  time: 0.5493  data: 0.1050  max mem: 15572
Epoch: [38]  [2130/2809]  eta: 0:06:25  lr: 0.000000  min_lr: 0.000000  loss: 3.6485 (3.7256)  class_acc: 0.3750 (0.3426)  loss_scale: 65536.0000 (53926.5021)  weight_decay: 0.0500 (0.0500)  time: 0.5371  data: 0.1117  max mem: 15572
Epoch: [38]  [2140/2809]  eta: 0:06:19  lr: 0.000000  min_lr: 0.000000  loss: 3.6485 (3.7251)  class_acc: 0.3750 (0.3428)  loss_scale: 65536.0000 (53980.7268)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1354  max mem: 15572
[2025-01-16 08:41:46,938] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:41:46,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:41:47,426] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 108889
[2025-01-16 08:41:47,427] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:41:47,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [2150/2809]  eta: 0:06:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7042 (3.7257)  class_acc: 0.3333 (0.3427)  loss_scale: 65536.0000 (54064.9149)  weight_decay: 0.0500 (0.0500)  time: 0.5421  data: 0.1065  max mem: 15572
Epoch: [38]  [2160/2809]  eta: 0:06:08  lr: 0.000000  min_lr: 0.000000  loss: 3.8755 (3.7264)  class_acc: 0.2917 (0.3425)  loss_scale: 65536.0000 (54117.9972)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1224  max mem: 15572
Epoch: [38]  [2170/2809]  eta: 0:06:02  lr: 0.000000  min_lr: 0.000000  loss: 3.8755 (3.7270)  class_acc: 0.2917 (0.3424)  loss_scale: 65536.0000 (54170.5905)  weight_decay: 0.0500 (0.0500)  time: 0.5955  data: 0.1683  max mem: 15572
Epoch: [38]  [2180/2809]  eta: 0:05:57  lr: 0.000000  min_lr: 0.000000  loss: 3.7779 (3.7268)  class_acc: 0.2917 (0.3423)  loss_scale: 65536.0000 (54222.7015)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.2377  max mem: 15572
Epoch: [38]  [2190/2809]  eta: 0:05:51  lr: 0.000000  min_lr: 0.000000  loss: 3.7793 (3.7275)  class_acc: 0.2500 (0.3420)  loss_scale: 65536.0000 (54274.3368)  weight_decay: 0.0500 (0.0500)  time: 0.6230  data: 0.1720  max mem: 15572
Epoch: [38]  [2200/2809]  eta: 0:05:45  lr: 0.000000  min_lr: 0.000000  loss: 3.7786 (3.7272)  class_acc: 0.2917 (0.3421)  loss_scale: 65536.0000 (54325.5030)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0897  max mem: 15572
Epoch: [38]  [2210/2809]  eta: 0:05:40  lr: 0.000000  min_lr: 0.000000  loss: 3.5428 (3.7266)  class_acc: 0.4167 (0.3425)  loss_scale: 65536.0000 (54376.2062)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1504  max mem: 15572
Epoch: [38]  [2220/2809]  eta: 0:05:34  lr: 0.000000  min_lr: 0.000000  loss: 3.5428 (3.7252)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (54426.4529)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.1474  max mem: 15572
Epoch: [38]  [2230/2809]  eta: 0:05:28  lr: 0.000000  min_lr: 0.000000  loss: 3.4403 (3.7237)  class_acc: 0.3333 (0.3428)  loss_scale: 65536.0000 (54476.2492)  weight_decay: 0.0500 (0.0500)  time: 0.5323  data: 0.0944  max mem: 15572
Epoch: [38]  [2240/2809]  eta: 0:05:22  lr: 0.000000  min_lr: 0.000000  loss: 3.5574 (3.7240)  class_acc: 0.3750 (0.3431)  loss_scale: 65536.0000 (54525.6011)  weight_decay: 0.0500 (0.0500)  time: 0.4869  data: 0.0554  max mem: 15572
Epoch: [38]  [2250/2809]  eta: 0:05:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8485 (3.7249)  class_acc: 0.3750 (0.3432)  loss_scale: 65536.0000 (54574.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.1293  max mem: 15572
[2025-01-16 08:42:50,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=109000, skipped=726, lr=[1.7612437616330731e-09, 1.7612437616330731e-09, 2.516062516618676e-09, 2.516062516618676e-09, 3.5943750237409664e-09, 3.5943750237409664e-09, 5.134821462487095e-09, 5.134821462487095e-09, 7.335459232124422e-09, 7.335459232124422e-09, 1.047922747446346e-08, 1.047922747446346e-08, 1.497032496351923e-08, 1.497032496351923e-08, 2.1386178519313188e-08, 2.1386178519313188e-08, 3.055168359901884e-08, 3.055168359901884e-08, 4.364526228431263e-08, 4.364526228431263e-08, 6.235037469187519e-08, 6.235037469187519e-08, 8.907196384553599e-08, 8.907196384553599e-08, 1.2724566263648e-07, 1.2724566263648e-07, 1.817795180521143e-07, 1.817795180521143e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:42:50,473] [INFO] [timer.py:260:stop] epoch=0/micro_step=109000/global_step=109000, RunningAvgSamplesPerSec=28.588749524621612, CurrSamplesPerSec=28.21873880440115, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [38]  [2260/2809]  eta: 0:05:11  lr: 0.000000  min_lr: 0.000000  loss: 3.8316 (3.7252)  class_acc: 0.2917 (0.3430)  loss_scale: 65536.0000 (54622.9951)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.1567  max mem: 15572
Epoch: [38]  [2270/2809]  eta: 0:05:06  lr: 0.000000  min_lr: 0.000000  loss: 3.7499 (3.7244)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (54671.0489)  weight_decay: 0.0500 (0.0500)  time: 0.5514  data: 0.1202  max mem: 15572
[2025-01-16 08:43:01,616] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:43:01,616] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:43:02,482] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109020
[2025-01-16 08:43:02,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:43:02,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [38]  [2280/2809]  eta: 0:05:00  lr: 0.000000  min_lr: 0.000000  loss: 3.7393 (3.7241)  class_acc: 0.3333 (0.3432)  loss_scale: 65536.0000 (54776.1438)  weight_decay: 0.0500 (0.0500)  time: 0.5825  data: 0.1514  max mem: 15572
Epoch: [38]  [2290/2809]  eta: 0:04:54  lr: 0.000000  min_lr: 0.000000  loss: 3.6367 (3.7235)  class_acc: 0.3750 (0.3433)  loss_scale: 65536.0000 (54823.1096)  weight_decay: 0.0500 (0.0500)  time: 0.5494  data: 0.1200  max mem: 15572
Epoch: [38]  [2300/2809]  eta: 0:04:48  lr: 0.000000  min_lr: 0.000000  loss: 3.5588 (3.7226)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (54869.6671)  weight_decay: 0.0500 (0.0500)  time: 0.5575  data: 0.1121  max mem: 15572
Epoch: [38]  [2310/2809]  eta: 0:04:43  lr: 0.000000  min_lr: 0.000000  loss: 3.5588 (3.7225)  class_acc: 0.2917 (0.3431)  loss_scale: 65536.0000 (54915.8217)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1032  max mem: 15572
Epoch: [38]  [2320/2809]  eta: 0:04:37  lr: 0.000000  min_lr: 0.000000  loss: 3.7326 (3.7224)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (54961.5786)  weight_decay: 0.0500 (0.0500)  time: 0.5249  data: 0.0941  max mem: 15572
Epoch: [38]  [2330/2809]  eta: 0:04:31  lr: 0.000000  min_lr: 0.000000  loss: 3.6974 (3.7216)  class_acc: 0.3750 (0.3435)  loss_scale: 65536.0000 (55006.9429)  weight_decay: 0.0500 (0.0500)  time: 0.5148  data: 0.0954  max mem: 15572
[2025-01-16 08:43:36,242] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109082
[2025-01-16 08:43:36,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:43:36,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2340/2809]  eta: 0:04:26  lr: 0.000000  min_lr: 0.000000  loss: 3.7596 (3.7218)  class_acc: 0.3333 (0.3433)  loss_scale: 65536.0000 (55037.9223)  weight_decay: 0.0500 (0.0500)  time: 0.5599  data: 0.1454  max mem: 15572
Epoch: [38]  [2350/2809]  eta: 0:04:20  lr: 0.000000  min_lr: 0.000000  loss: 3.5383 (3.7207)  class_acc: 0.3333 (0.3436)  loss_scale: 32768.0000 (54943.1969)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.1729  max mem: 15572
Epoch: [38]  [2360/2809]  eta: 0:04:14  lr: 0.000000  min_lr: 0.000000  loss: 3.4088 (3.7194)  class_acc: 0.4583 (0.3440)  loss_scale: 32768.0000 (54849.2740)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1067  max mem: 15572
Epoch: [38]  [2370/2809]  eta: 0:04:08  lr: 0.000000  min_lr: 0.000000  loss: 3.5841 (3.7198)  class_acc: 0.3333 (0.3438)  loss_scale: 32768.0000 (54756.1434)  weight_decay: 0.0500 (0.0500)  time: 0.5348  data: 0.0865  max mem: 15572
Epoch: [38]  [2380/2809]  eta: 0:04:03  lr: 0.000000  min_lr: 0.000000  loss: 3.8634 (3.7208)  class_acc: 0.2917 (0.3437)  loss_scale: 32768.0000 (54663.7950)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1516  max mem: 15572
Epoch: [38]  [2390/2809]  eta: 0:03:57  lr: 0.000000  min_lr: 0.000000  loss: 3.7630 (3.7204)  class_acc: 0.3750 (0.3438)  loss_scale: 32768.0000 (54572.2192)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.1709  max mem: 15572
Epoch: [38]  [2400/2809]  eta: 0:03:52  lr: 0.000000  min_lr: 0.000000  loss: 3.7269 (3.7204)  class_acc: 0.3750 (0.3438)  loss_scale: 32768.0000 (54481.4061)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1403  max mem: 15572
Epoch: [38]  [2410/2809]  eta: 0:03:46  lr: 0.000000  min_lr: 0.000000  loss: 3.5862 (3.7188)  class_acc: 0.3750 (0.3443)  loss_scale: 32768.0000 (54391.3463)  weight_decay: 0.0500 (0.0500)  time: 0.5640  data: 0.1209  max mem: 15572
Epoch: [38]  [2420/2809]  eta: 0:03:40  lr: 0.000000  min_lr: 0.000000  loss: 3.5461 (3.7190)  class_acc: 0.4167 (0.3443)  loss_scale: 32768.0000 (54302.0306)  weight_decay: 0.0500 (0.0500)  time: 0.5685  data: 0.1211  max mem: 15572
Epoch: [38]  [2430/2809]  eta: 0:03:35  lr: 0.000000  min_lr: 0.000000  loss: 3.5461 (3.7179)  class_acc: 0.3750 (0.3446)  loss_scale: 32768.0000 (54213.4496)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1630  max mem: 15572
Epoch: [38]  [2440/2809]  eta: 0:03:29  lr: 0.000000  min_lr: 0.000000  loss: 3.5049 (3.7178)  class_acc: 0.3750 (0.3445)  loss_scale: 32768.0000 (54125.5944)  weight_decay: 0.0500 (0.0500)  time: 0.5829  data: 0.1321  max mem: 15572
Epoch: [38]  [2450/2809]  eta: 0:03:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7116 (3.7174)  class_acc: 0.3333 (0.3445)  loss_scale: 32768.0000 (54038.4561)  weight_decay: 0.0500 (0.0500)  time: 0.5505  data: 0.0981  max mem: 15572
Epoch: [38]  [2460/2809]  eta: 0:03:17  lr: 0.000000  min_lr: 0.000000  loss: 3.6858 (3.7180)  class_acc: 0.3333 (0.3444)  loss_scale: 32768.0000 (53952.0260)  weight_decay: 0.0500 (0.0500)  time: 0.5245  data: 0.0624  max mem: 15572
[2025-01-16 08:44:48,850] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:44:48,850] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2470/2809]  eta: 0:03:12  lr: 0.000000  min_lr: 0.000000  loss: 3.8213 (3.7183)  class_acc: 0.3333 (0.3443)  loss_scale: 32768.0000 (53892.8175)  weight_decay: 0.0500 (0.0500)  time: 0.4852  data: 0.0166  max mem: 15572
Epoch: [38]  [2480/2809]  eta: 0:03:06  lr: 0.000000  min_lr: 0.000000  loss: 3.8544 (3.7184)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (53939.7469)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0663  max mem: 15572
Epoch: [38]  [2490/2809]  eta: 0:03:00  lr: 0.000000  min_lr: 0.000000  loss: 3.7379 (3.7178)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (53986.2995)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0656  max mem: 15572
Epoch: [38]  [2500/2809]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 3.5462 (3.7169)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (54032.4798)  weight_decay: 0.0500 (0.0500)  time: 0.5478  data: 0.1044  max mem: 15572
Epoch: [38]  [2510/2809]  eta: 0:02:49  lr: 0.000000  min_lr: 0.000000  loss: 3.6204 (3.7166)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (54078.2923)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1184  max mem: 15572
Epoch: [38]  [2520/2809]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 3.7766 (3.7168)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (54123.7414)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.1208  max mem: 15572
Epoch: [38]  [2530/2809]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 3.7359 (3.7160)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (54168.8313)  weight_decay: 0.0500 (0.0500)  time: 0.6136  data: 0.1598  max mem: 15572
Epoch: [38]  [2540/2809]  eta: 0:02:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7359 (3.7164)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (54213.5663)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.1302  max mem: 15572
Epoch: [38]  [2550/2809]  eta: 0:02:26  lr: 0.000000  min_lr: 0.000000  loss: 3.7432 (3.7165)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (54257.9506)  weight_decay: 0.0500 (0.0500)  time: 0.5167  data: 0.0673  max mem: 15572
[2025-01-16 08:45:36,952] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109295
[2025-01-16 08:45:36,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:45:36,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2560/2809]  eta: 0:02:21  lr: 0.000000  min_lr: 0.000000  loss: 3.6876 (3.7162)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (54199.6283)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.0957  max mem: 15572
Epoch: [38]  [2570/2809]  eta: 0:02:15  lr: 0.000000  min_lr: 0.000000  loss: 3.6277 (3.7170)  class_acc: 0.3750 (0.3445)  loss_scale: 32768.0000 (54116.2692)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.1465  max mem: 15572
Epoch: [38]  [2580/2809]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 3.7012 (3.7169)  class_acc: 0.2917 (0.3445)  loss_scale: 32768.0000 (54033.5560)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1308  max mem: 15572
Epoch: [38]  [2590/2809]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 3.8144 (3.7177)  class_acc: 0.3333 (0.3444)  loss_scale: 32768.0000 (53951.4813)  weight_decay: 0.0500 (0.0500)  time: 0.5745  data: 0.1274  max mem: 15572
Epoch: [38]  [2600/2809]  eta: 0:01:58  lr: 0.000000  min_lr: 0.000000  loss: 3.9253 (3.7178)  class_acc: 0.3333 (0.3444)  loss_scale: 32768.0000 (53870.0377)  weight_decay: 0.0500 (0.0500)  time: 0.4993  data: 0.0535  max mem: 15572
Epoch: [38]  [2610/2809]  eta: 0:01:52  lr: 0.000000  min_lr: 0.000000  loss: 3.9253 (3.7180)  class_acc: 0.2917 (0.3445)  loss_scale: 32768.0000 (53789.2179)  weight_decay: 0.0500 (0.0500)  time: 0.4698  data: 0.0284  max mem: 15572
Epoch: [38]  [2620/2809]  eta: 0:01:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8431 (3.7180)  class_acc: 0.2917 (0.3444)  loss_scale: 32768.0000 (53709.0149)  weight_decay: 0.0500 (0.0500)  time: 0.5180  data: 0.0751  max mem: 15572
Epoch: [38]  [2630/2809]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 3.5932 (3.7176)  class_acc: 0.3333 (0.3445)  loss_scale: 32768.0000 (53629.4215)  weight_decay: 0.0500 (0.0500)  time: 0.5818  data: 0.1254  max mem: 15572
Epoch: [38]  [2640/2809]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 3.6218 (3.7180)  class_acc: 0.3333 (0.3444)  loss_scale: 32768.0000 (53550.4309)  weight_decay: 0.0500 (0.0500)  time: 0.5761  data: 0.1303  max mem: 15572
Epoch: [38]  [2650/2809]  eta: 0:01:30  lr: 0.000000  min_lr: 0.000000  loss: 3.7027 (3.7182)  class_acc: 0.3333 (0.3444)  loss_scale: 32768.0000 (53472.0362)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.1325  max mem: 15572
Epoch: [38]  [2660/2809]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 3.8055 (3.7186)  class_acc: 0.2917 (0.3442)  loss_scale: 32768.0000 (53394.2307)  weight_decay: 0.0500 (0.0500)  time: 0.5779  data: 0.1211  max mem: 15572
Epoch: [38]  [2670/2809]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 3.8372 (3.7189)  class_acc: 0.2917 (0.3441)  loss_scale: 32768.0000 (53317.0079)  weight_decay: 0.0500 (0.0500)  time: 0.5495  data: 0.1134  max mem: 15572
Epoch: [38]  [2680/2809]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7070 (3.7186)  class_acc: 0.3333 (0.3442)  loss_scale: 32768.0000 (53240.3611)  weight_decay: 0.0500 (0.0500)  time: 0.5707  data: 0.1245  max mem: 15572
[2025-01-16 08:46:47,379] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:46:47,379] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [2690/2809]  eta: 0:01:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6909 (3.7183)  class_acc: 0.3333 (0.3443)  loss_scale: 32768.0000 (53273.8759)  weight_decay: 0.0500 (0.0500)  time: 0.6226  data: 0.1805  max mem: 15572
Epoch: [38]  [2700/2809]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7726 (3.7186)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (53319.2743)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.2158  max mem: 15572
Epoch: [38]  [2710/2809]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 3.5774 (3.7181)  class_acc: 0.3333 (0.3443)  loss_scale: 65536.0000 (53364.3379)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.1998  max mem: 15572
Epoch: [38]  [2720/2809]  eta: 0:00:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7302 (3.7181)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (53409.0702)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.2216  max mem: 15572
Epoch: [38]  [2730/2809]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 3.8477 (3.7189)  class_acc: 0.2917 (0.3441)  loss_scale: 65536.0000 (53453.4749)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.1827  max mem: 15572
Epoch: [38]  [2740/2809]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 3.9585 (3.7195)  class_acc: 0.2917 (0.3441)  loss_scale: 65536.0000 (53497.5556)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1424  max mem: 15572
Epoch: [38]  [2750/2809]  eta: 0:00:33  lr: 0.000000  min_lr: 0.000000  loss: 3.8268 (3.7195)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (53541.3159)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.1938  max mem: 15572
Epoch: [38]  [2760/2809]  eta: 0:00:27  lr: 0.000000  min_lr: 0.000000  loss: 3.7581 (3.7188)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (53584.7591)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.1873  max mem: 15572
Epoch: [38]  [2770/2809]  eta: 0:00:22  lr: 0.000000  min_lr: 0.000000  loss: 3.5566 (3.7181)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (53627.8888)  weight_decay: 0.0500 (0.0500)  time: 0.5822  data: 0.1467  max mem: 15572
[2025-01-16 08:47:44,100] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109514
[2025-01-16 08:47:44,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:47:44,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [2780/2809]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 3.8054 (3.7189)  class_acc: 0.2917 (0.3443)  loss_scale: 65536.0000 (53564.6631)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1590  max mem: 15572
Epoch: [38]  [2790/2809]  eta: 0:00:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8399 (3.7190)  class_acc: 0.2917 (0.3442)  loss_scale: 32768.0000 (53490.1498)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.2180  max mem: 15572
Epoch: [38]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 3.8763 (3.7188)  class_acc: 0.3333 (0.3442)  loss_scale: 32768.0000 (53416.1685)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.1953  max mem: 15572
Epoch: [38]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.8879 (3.7192)  class_acc: 0.2917 (0.3439)  loss_scale: 32768.0000 (53357.3628)  weight_decay: 0.0500 (0.0500)  time: 0.4595  data: 0.0519  max mem: 15572
Epoch: [38] Total time: 0:26:37 (0.5688 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.8879 (3.7192)  class_acc: 0.2917 (0.3439)  loss_scale: 32768.0000 (53357.3628)  weight_decay: 0.0500 (0.0500)
Val:  [  0/272]  eta: 0:24:59  loss: 0.3857 (0.3857)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.5113  data: 5.3273  max mem: 15572
Val:  [ 10/272]  eta: 0:04:21  loss: 2.3085 (2.2192)  acc1: 44.4444 (44.9495)  acc5: 77.7778 (74.2424)  time: 0.9962  data: 0.7892  max mem: 15572
Val:  [ 20/272]  eta: 0:02:37  loss: 2.2611 (2.2514)  acc1: 44.4444 (48.1481)  acc5: 72.2222 (74.3386)  time: 0.3809  data: 0.1682  max mem: 15572
Val:  [ 30/272]  eta: 0:02:06  loss: 2.3144 (2.3425)  acc1: 50.0000 (45.1613)  acc5: 72.2222 (73.8351)  time: 0.2642  data: 0.0650  max mem: 15572
Val:  [ 40/272]  eta: 0:01:53  loss: 2.4846 (2.3989)  acc1: 27.7778 (41.7344)  acc5: 72.2222 (73.8482)  time: 0.3492  data: 0.1509  max mem: 15572
Val:  [ 50/272]  eta: 0:01:40  loss: 2.4536 (2.3226)  acc1: 33.3333 (43.4641)  acc5: 77.7778 (75.8170)  time: 0.3421  data: 0.1409  max mem: 15572
Val:  [ 60/272]  eta: 0:01:31  loss: 1.4312 (2.2173)  acc1: 61.1111 (46.3570)  acc5: 83.3333 (76.4117)  time: 0.3153  data: 0.1327  max mem: 15572
Val:  [ 70/272]  eta: 0:01:26  loss: 1.5045 (2.1418)  acc1: 66.6667 (48.5915)  acc5: 83.3333 (77.4648)  time: 0.3701  data: 0.1940  max mem: 15572
Val:  [ 80/272]  eta: 0:01:19  loss: 1.8228 (2.1559)  acc1: 61.1111 (48.6968)  acc5: 77.7778 (77.1605)  time: 0.3494  data: 0.1780  max mem: 15572
Val:  [ 90/272]  eta: 0:01:13  loss: 2.0670 (2.1577)  acc1: 50.0000 (48.5958)  acc5: 83.3333 (77.7167)  time: 0.3103  data: 0.1294  max mem: 15572
Val:  [100/272]  eta: 0:01:08  loss: 2.0860 (2.1877)  acc1: 44.4444 (47.7998)  acc5: 83.3333 (77.2827)  time: 0.3511  data: 0.1461  max mem: 15572
Val:  [110/272]  eta: 0:01:03  loss: 2.4718 (2.2623)  acc1: 22.2222 (45.7958)  acc5: 72.2222 (76.2262)  time: 0.3465  data: 0.1406  max mem: 15572
Val:  [120/272]  eta: 0:00:58  loss: 2.8217 (2.3030)  acc1: 22.2222 (44.9036)  acc5: 72.2222 (75.7117)  time: 0.3135  data: 0.1104  max mem: 15572
Val:  [130/272]  eta: 0:00:54  loss: 2.1347 (2.2666)  acc1: 44.4444 (45.8863)  acc5: 77.7778 (76.4631)  time: 0.3422  data: 0.1364  max mem: 15572
Val:  [140/272]  eta: 0:00:50  loss: 1.6322 (2.2596)  acc1: 55.5556 (46.2175)  acc5: 88.8889 (76.2017)  time: 0.3538  data: 0.1504  max mem: 15572
Val:  [150/272]  eta: 0:00:46  loss: 2.2686 (2.2655)  acc1: 33.3333 (45.7689)  acc5: 77.7778 (76.4533)  time: 0.3258  data: 0.1230  max mem: 15572
Val:  [160/272]  eta: 0:00:41  loss: 2.2686 (2.2553)  acc1: 50.0000 (46.5148)  acc5: 77.7778 (76.6391)  time: 0.3186  data: 0.1298  max mem: 15572
Val:  [170/272]  eta: 0:00:37  loss: 2.3726 (2.2761)  acc1: 44.4444 (46.0039)  acc5: 72.2222 (76.2183)  time: 0.3227  data: 0.1362  max mem: 15572
Val:  [180/272]  eta: 0:00:34  loss: 2.3258 (2.2666)  acc1: 38.8889 (45.8870)  acc5: 72.2222 (76.6728)  time: 0.3630  data: 0.1668  max mem: 15572
Val:  [190/272]  eta: 0:00:30  loss: 2.3258 (2.3198)  acc1: 33.3333 (44.5899)  acc5: 77.7778 (75.3054)  time: 0.4102  data: 0.2183  max mem: 15572
Val:  [200/272]  eta: 0:00:26  loss: 2.5426 (2.3272)  acc1: 33.3333 (44.4168)  acc5: 72.2222 (75.2073)  time: 0.3514  data: 0.1675  max mem: 15572
Val:  [210/272]  eta: 0:00:23  loss: 2.0439 (2.3303)  acc1: 44.4444 (44.5498)  acc5: 77.7778 (75.1448)  time: 0.3323  data: 0.1398  max mem: 15572
Val:  [220/272]  eta: 0:00:19  loss: 2.2719 (2.3201)  acc1: 44.4444 (44.6456)  acc5: 77.7778 (75.2137)  time: 0.3638  data: 0.1725  max mem: 15572
Val:  [230/272]  eta: 0:00:15  loss: 1.7514 (2.2919)  acc1: 61.1111 (45.6229)  acc5: 83.3333 (75.6133)  time: 0.3614  data: 0.1722  max mem: 15572
Val:  [240/272]  eta: 0:00:11  loss: 1.5712 (2.2767)  acc1: 61.1111 (45.9659)  acc5: 83.3333 (75.9336)  time: 0.3476  data: 0.1409  max mem: 15572
Val:  [250/272]  eta: 0:00:08  loss: 2.2777 (2.2883)  acc1: 38.8889 (45.2413)  acc5: 77.7778 (75.9849)  time: 0.3252  data: 0.1228  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.2566 (2.2314)  acc1: 66.6667 (46.8710)  acc5: 88.8889 (76.6709)  time: 0.3174  data: 0.1202  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.3445 (2.2246)  acc1: 72.2222 (47.0480)  acc5: 88.8889 (76.8553)  time: 0.2257  data: 0.0479  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.3445 (2.2293)  acc1: 72.2222 (47.0203)  acc5: 88.8889 (76.8175)  time: 0.2195  data: 0.0479  max mem: 15572
Val: Total time: 0:01:36 (0.3565 s / it)
* Acc@1 47.020 Acc@5 76.818 loss 2.229
Accuracy of the network on the 4883 val videos: 47.0%
Max accuracy: 47.47%
Epoch: [39]  [   0/2809]  eta: 7:42:52  lr: 0.000000  min_lr: 0.000000  loss: 3.7299 (3.7299)  class_acc: 0.4167 (0.4167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 9.8871  data: 9.4627  max mem: 15572
Epoch: [39]  [  10/2809]  eta: 1:07:14  lr: 0.000000  min_lr: 0.000000  loss: 3.8752 (3.8614)  class_acc: 0.3333 (0.3371)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4415  data: 1.0074  max mem: 15572
Epoch: [39]  [  20/2809]  eta: 0:47:08  lr: 0.000000  min_lr: 0.000000  loss: 3.8752 (3.8393)  class_acc: 0.2917 (0.3333)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5704  data: 0.1312  max mem: 15572
Epoch: [39]  [  30/2809]  eta: 0:37:56  lr: 0.000000  min_lr: 0.000000  loss: 3.7306 (3.7640)  class_acc: 0.3750 (0.3454)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4771  data: 0.0506  max mem: 15572
Epoch: [39]  [  40/2809]  eta: 0:32:56  lr: 0.000000  min_lr: 0.000000  loss: 3.5232 (3.7186)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3988  data: 0.0005  max mem: 15572
Epoch: [39]  [  50/2809]  eta: 0:30:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7139 (3.7589)  class_acc: 0.2500 (0.3292)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4164  data: 0.0005  max mem: 15572
Epoch: [39]  [  60/2809]  eta: 0:28:48  lr: 0.000000  min_lr: 0.000000  loss: 3.9555 (3.7769)  class_acc: 0.2917 (0.3361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.4542  data: 0.0008  max mem: 15572
Epoch: [39]  [  70/2809]  eta: 0:28:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7580 (3.7540)  class_acc: 0.3750 (0.3415)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0751  max mem: 15572
Epoch: [39]  [  80/2809]  eta: 0:29:03  lr: 0.000000  min_lr: 0.000000  loss: 3.6650 (3.7321)  class_acc: 0.3750 (0.3441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.2152  max mem: 15572
Epoch: [39]  [  90/2809]  eta: 0:29:05  lr: 0.000000  min_lr: 0.000000  loss: 3.6975 (3.7226)  class_acc: 0.3333 (0.3425)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.2529  max mem: 15572
[2025-01-16 08:50:41,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:50:41,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 100/2809]  eta: 0:29:16  lr: 0.000000  min_lr: 0.000000  loss: 3.6975 (3.7107)  class_acc: 0.2917 (0.3436)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.2484  max mem: 15572
Epoch: [39]  [ 110/2809]  eta: 0:29:10  lr: 0.000000  min_lr: 0.000000  loss: 3.4858 (3.6974)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (38376.9369)  weight_decay: 0.0500 (0.0500)  time: 0.6785  data: 0.2277  max mem: 15572
Epoch: [39]  [ 120/2809]  eta: 0:29:07  lr: 0.000000  min_lr: 0.000000  loss: 3.6378 (3.6950)  class_acc: 0.3333 (0.3450)  loss_scale: 65536.0000 (40621.4876)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.1902  max mem: 15572
Epoch: [39]  [ 130/2809]  eta: 0:29:12  lr: 0.000000  min_lr: 0.000000  loss: 3.6804 (3.6944)  class_acc: 0.3333 (0.3454)  loss_scale: 65536.0000 (42523.3588)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.2268  max mem: 15572
Epoch: [39]  [ 140/2809]  eta: 0:29:31  lr: 0.000000  min_lr: 0.000000  loss: 3.8651 (3.7183)  class_acc: 0.3333 (0.3434)  loss_scale: 65536.0000 (44155.4610)  weight_decay: 0.0500 (0.0500)  time: 0.7482  data: 0.2902  max mem: 15572
Epoch: [39]  [ 150/2809]  eta: 0:29:12  lr: 0.000000  min_lr: 0.000000  loss: 3.8612 (3.7033)  class_acc: 0.3750 (0.3474)  loss_scale: 65536.0000 (45571.3907)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.2339  max mem: 15572
Epoch: [39]  [ 160/2809]  eta: 0:29:21  lr: 0.000000  min_lr: 0.000000  loss: 3.7282 (3.7114)  class_acc: 0.3333 (0.3465)  loss_scale: 65536.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  time: 0.6729  data: 0.2173  max mem: 15572
Epoch: [39]  [ 170/2809]  eta: 0:29:28  lr: 0.000000  min_lr: 0.000000  loss: 3.6260 (3.6907)  class_acc: 0.3333 (0.3494)  loss_scale: 65536.0000 (47906.4327)  weight_decay: 0.0500 (0.0500)  time: 0.7509  data: 0.2987  max mem: 15572
Epoch: [39]  [ 180/2809]  eta: 0:29:08  lr: 0.000000  min_lr: 0.000000  loss: 3.5726 (3.6974)  class_acc: 0.3333 (0.3481)  loss_scale: 65536.0000 (48880.4420)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.2440  max mem: 15572
Epoch: [39]  [ 190/2809]  eta: 0:28:28  lr: 0.000000  min_lr: 0.000000  loss: 3.8938 (3.7009)  class_acc: 0.2917 (0.3482)  loss_scale: 65536.0000 (49752.4607)  weight_decay: 0.0500 (0.0500)  time: 0.5012  data: 0.0910  max mem: 15572
Epoch: [39]  [ 200/2809]  eta: 0:28:09  lr: 0.000000  min_lr: 0.000000  loss: 3.9132 (3.6994)  class_acc: 0.3750 (0.3483)  loss_scale: 65536.0000 (50537.7114)  weight_decay: 0.0500 (0.0500)  time: 0.4898  data: 0.0655  max mem: 15572
Epoch: [39]  [ 210/2809]  eta: 0:27:58  lr: 0.000000  min_lr: 0.000000  loss: 3.5633 (3.6939)  class_acc: 0.3333 (0.3483)  loss_scale: 65536.0000 (51248.5308)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.1400  max mem: 15572
[2025-01-16 08:52:03,592] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:52:03,593] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [39]  [ 220/2809]  eta: 0:27:39  lr: 0.000000  min_lr: 0.000000  loss: 3.6885 (3.6914)  class_acc: 0.3333 (0.3507)  loss_scale: 65536.0000 (52191.5656)  weight_decay: 0.0500 (0.0500)  time: 0.5767  data: 0.1135  max mem: 15572
[2025-01-16 08:52:08,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109777
[2025-01-16 08:52:08,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:52:08,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [ 230/2809]  eta: 0:27:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7406 (3.6965)  class_acc: 0.2917 (0.3483)  loss_scale: 65536.0000 (54187.7749)  weight_decay: 0.0500 (0.0500)  time: 0.5846  data: 0.1337  max mem: 15572
Epoch: [39]  [ 240/2809]  eta: 0:27:37  lr: 0.000000  min_lr: 0.000000  loss: 3.8744 (3.7097)  class_acc: 0.2917 (0.3461)  loss_scale: 65536.0000 (54658.6556)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.2397  max mem: 15572
Epoch: [39]  [ 250/2809]  eta: 0:27:20  lr: 0.000000  min_lr: 0.000000  loss: 3.8744 (3.7114)  class_acc: 0.2917 (0.3448)  loss_scale: 65536.0000 (55092.0159)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1891  max mem: 15572
Epoch: [39]  [ 260/2809]  eta: 0:27:06  lr: 0.000000  min_lr: 0.000000  loss: 3.8174 (3.7150)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (55492.1686)  weight_decay: 0.0500 (0.0500)  time: 0.5541  data: 0.1153  max mem: 15572
Epoch: [39]  [ 270/2809]  eta: 0:27:05  lr: 0.000000  min_lr: 0.000000  loss: 3.8174 (3.7190)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (55862.7897)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.2053  max mem: 15572
Epoch: [39]  [ 280/2809]  eta: 0:26:54  lr: 0.000000  min_lr: 0.000000  loss: 3.7920 (3.7135)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (56207.0320)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.2156  max mem: 15572
Epoch: [39]  [ 290/2809]  eta: 0:26:39  lr: 0.000000  min_lr: 0.000000  loss: 3.8101 (3.7125)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (56527.6151)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1257  max mem: 15572
Epoch: [39]  [ 300/2809]  eta: 0:26:24  lr: 0.000000  min_lr: 0.000000  loss: 3.6906 (3.7145)  class_acc: 0.3750 (0.3450)  loss_scale: 65536.0000 (56826.8970)  weight_decay: 0.0500 (0.0500)  time: 0.5343  data: 0.0989  max mem: 15572
Epoch: [39]  [ 310/2809]  eta: 0:26:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6906 (3.7150)  class_acc: 0.2917 (0.3438)  loss_scale: 65536.0000 (57106.9325)  weight_decay: 0.0500 (0.0500)  time: 0.5831  data: 0.1536  max mem: 15572
Epoch: [39]  [ 320/2809]  eta: 0:26:06  lr: 0.000000  min_lr: 0.000000  loss: 3.6452 (3.7105)  class_acc: 0.3750 (0.3466)  loss_scale: 65536.0000 (57369.5202)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.1641  max mem: 15572
Epoch: [39]  [ 330/2809]  eta: 0:25:57  lr: 0.000000  min_lr: 0.000000  loss: 3.8901 (3.7193)  class_acc: 0.3750 (0.3455)  loss_scale: 65536.0000 (57616.2417)  weight_decay: 0.0500 (0.0500)  time: 0.5747  data: 0.1453  max mem: 15572
Epoch: [39]  [ 340/2809]  eta: 0:25:41  lr: 0.000000  min_lr: 0.000000  loss: 3.9224 (3.7195)  class_acc: 0.3333 (0.3460)  loss_scale: 65536.0000 (57848.4927)  weight_decay: 0.0500 (0.0500)  time: 0.5464  data: 0.1019  max mem: 15572
Epoch: [39]  [ 350/2809]  eta: 0:25:35  lr: 0.000000  min_lr: 0.000000  loss: 3.8370 (3.7196)  class_acc: 0.3333 (0.3459)  loss_scale: 65536.0000 (58067.5100)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.1077  max mem: 15572
[2025-01-16 08:53:24,299] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:53:24,299] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:53:24,756] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 109907
[2025-01-16 08:53:24,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:53:24,756] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [ 360/2809]  eta: 0:25:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7177 (3.7186)  class_acc: 0.2917 (0.3457)  loss_scale: 65536.0000 (58455.9335)  weight_decay: 0.0500 (0.0500)  time: 0.5810  data: 0.1357  max mem: 15572
Epoch: [39]  [ 370/2809]  eta: 0:25:20  lr: 0.000000  min_lr: 0.000000  loss: 3.6514 (3.7132)  class_acc: 0.3333 (0.3464)  loss_scale: 65536.0000 (58646.7709)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.1576  max mem: 15572
Epoch: [39]  [ 380/2809]  eta: 0:25:14  lr: 0.000000  min_lr: 0.000000  loss: 3.6976 (3.7169)  class_acc: 0.3333 (0.3458)  loss_scale: 65536.0000 (58827.5906)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1920  max mem: 15572
Epoch: [39]  [ 390/2809]  eta: 0:25:08  lr: 0.000000  min_lr: 0.000000  loss: 3.7827 (3.7203)  class_acc: 0.3333 (0.3456)  loss_scale: 65536.0000 (58999.1611)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.1852  max mem: 15572
Epoch: [39]  [ 400/2809]  eta: 0:24:59  lr: 0.000000  min_lr: 0.000000  loss: 3.8817 (3.7269)  class_acc: 0.2917 (0.3430)  loss_scale: 65536.0000 (59162.1746)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.1714  max mem: 15572
Epoch: [39]  [ 410/2809]  eta: 0:24:52  lr: 0.000000  min_lr: 0.000000  loss: 3.8653 (3.7267)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (59317.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5924  data: 0.1573  max mem: 15572
Epoch: [39]  [ 420/2809]  eta: 0:24:40  lr: 0.000000  min_lr: 0.000000  loss: 3.8006 (3.7277)  class_acc: 0.3750 (0.3425)  loss_scale: 65536.0000 (59464.9691)  weight_decay: 0.0500 (0.0500)  time: 0.5667  data: 0.1345  max mem: 15572
Epoch: [39]  [ 430/2809]  eta: 0:24:37  lr: 0.000000  min_lr: 0.000000  loss: 3.8304 (3.7283)  class_acc: 0.3333 (0.3423)  loss_scale: 65536.0000 (59605.8283)  weight_decay: 0.0500 (0.0500)  time: 0.6059  data: 0.1666  max mem: 15572
Epoch: [39]  [ 440/2809]  eta: 0:24:31  lr: 0.000000  min_lr: 0.000000  loss: 3.8304 (3.7286)  class_acc: 0.2917 (0.3421)  loss_scale: 65536.0000 (59740.2993)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.2122  max mem: 15572
[2025-01-16 08:54:19,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=110000, skipped=732, lr=[1.0994758261983567e-09, 1.0994758261983567e-09, 1.5706797517119385e-09, 1.5706797517119385e-09, 2.243828216731341e-09, 2.243828216731341e-09, 3.205468881044773e-09, 3.205468881044773e-09, 4.57924125863539e-09, 4.57924125863539e-09, 6.541773226621986e-09, 6.541773226621986e-09, 9.345390323745694e-09, 9.345390323745694e-09, 1.3350557605350994e-08, 1.3350557605350994e-08, 1.907222515050142e-08, 1.907222515050142e-08, 2.7246035929287747e-08, 2.7246035929287747e-08, 3.8922908470411064e-08, 3.8922908470411064e-08, 5.56041549577301e-08, 5.56041549577301e-08, 7.943450708247158e-08, 7.943450708247158e-08, 1.1347786726067369e-07, 1.1347786726067369e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 08:54:19,881] [INFO] [timer.py:260:stop] epoch=0/micro_step=110000/global_step=110000, RunningAvgSamplesPerSec=28.590727116521137, CurrSamplesPerSec=30.38020673316332, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [ 450/2809]  eta: 0:24:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8907 (3.7340)  class_acc: 0.2917 (0.3416)  loss_scale: 65536.0000 (59868.8071)  weight_decay: 0.0500 (0.0500)  time: 0.5463  data: 0.1129  max mem: 15572
Epoch: [39]  [ 460/2809]  eta: 0:24:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7572 (3.7305)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (59991.7397)  weight_decay: 0.0500 (0.0500)  time: 0.5542  data: 0.1265  max mem: 15572
Epoch: [39]  [ 470/2809]  eta: 0:24:00  lr: 0.000000  min_lr: 0.000000  loss: 3.5878 (3.7271)  class_acc: 0.3333 (0.3431)  loss_scale: 65536.0000 (60109.4522)  weight_decay: 0.0500 (0.0500)  time: 0.5751  data: 0.1474  max mem: 15572
Epoch: [39]  [ 480/2809]  eta: 0:23:53  lr: 0.000000  min_lr: 0.000000  loss: 3.4756 (3.7249)  class_acc: 0.4167 (0.3454)  loss_scale: 65536.0000 (60222.2703)  weight_decay: 0.0500 (0.0500)  time: 0.5480  data: 0.1173  max mem: 15572
[2025-01-16 08:54:41,618] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110036
[2025-01-16 08:54:41,618] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:54:41,618] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 490/2809]  eta: 0:23:44  lr: 0.000000  min_lr: 0.000000  loss: 3.7444 (3.7273)  class_acc: 0.3750 (0.3441)  loss_scale: 65536.0000 (59930.0692)  weight_decay: 0.0500 (0.0500)  time: 0.5766  data: 0.1474  max mem: 15572
Epoch: [39]  [ 500/2809]  eta: 0:23:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8837 (3.7308)  class_acc: 0.2917 (0.3436)  loss_scale: 32768.0000 (59387.9122)  weight_decay: 0.0500 (0.0500)  time: 0.5834  data: 0.1549  max mem: 15572
Epoch: [39]  [ 510/2809]  eta: 0:23:27  lr: 0.000000  min_lr: 0.000000  loss: 3.8305 (3.7313)  class_acc: 0.2500 (0.3430)  loss_scale: 32768.0000 (58866.9746)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.1258  max mem: 15572
Epoch: [39]  [ 520/2809]  eta: 0:23:18  lr: 0.000000  min_lr: 0.000000  loss: 3.9341 (3.7350)  class_acc: 0.2917 (0.3427)  loss_scale: 32768.0000 (58366.0345)  weight_decay: 0.0500 (0.0500)  time: 0.5310  data: 0.0973  max mem: 15572
Epoch: [39]  [ 530/2809]  eta: 0:23:10  lr: 0.000000  min_lr: 0.000000  loss: 4.0380 (3.7380)  class_acc: 0.2500 (0.3410)  loss_scale: 32768.0000 (57883.9623)  weight_decay: 0.0500 (0.0500)  time: 0.5548  data: 0.1225  max mem: 15572
Epoch: [39]  [ 540/2809]  eta: 0:23:04  lr: 0.000000  min_lr: 0.000000  loss: 3.7651 (3.7367)  class_acc: 0.2500 (0.3407)  loss_scale: 32768.0000 (57419.7116)  weight_decay: 0.0500 (0.0500)  time: 0.5874  data: 0.1585  max mem: 15572
Epoch: [39]  [ 550/2809]  eta: 0:22:56  lr: 0.000000  min_lr: 0.000000  loss: 3.5110 (3.7338)  class_acc: 0.3333 (0.3417)  loss_scale: 32768.0000 (56972.3122)  weight_decay: 0.0500 (0.0500)  time: 0.5922  data: 0.1605  max mem: 15572
Epoch: [39]  [ 560/2809]  eta: 0:22:51  lr: 0.000000  min_lr: 0.000000  loss: 3.6179 (3.7331)  class_acc: 0.3333 (0.3414)  loss_scale: 32768.0000 (56540.8627)  weight_decay: 0.0500 (0.0500)  time: 0.5960  data: 0.1506  max mem: 15572
Epoch: [39]  [ 570/2809]  eta: 0:22:45  lr: 0.000000  min_lr: 0.000000  loss: 3.7054 (3.7302)  class_acc: 0.3333 (0.3425)  loss_scale: 32768.0000 (56124.5254)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.1654  max mem: 15572
Epoch: [39]  [ 580/2809]  eta: 0:22:38  lr: 0.000000  min_lr: 0.000000  loss: 3.8922 (3.7355)  class_acc: 0.3750 (0.3415)  loss_scale: 32768.0000 (55722.5198)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.1706  max mem: 15572
Epoch: [39]  [ 590/2809]  eta: 0:22:32  lr: 0.000000  min_lr: 0.000000  loss: 3.9696 (3.7346)  class_acc: 0.3333 (0.3426)  loss_scale: 32768.0000 (55334.1184)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.1731  max mem: 15572
Epoch: [39]  [ 600/2809]  eta: 0:22:23  lr: 0.000000  min_lr: 0.000000  loss: 3.8570 (3.7357)  class_acc: 0.3750 (0.3428)  loss_scale: 32768.0000 (54958.6423)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1305  max mem: 15572
Epoch: [39]  [ 610/2809]  eta: 0:22:19  lr: 0.000000  min_lr: 0.000000  loss: 3.6135 (3.7298)  class_acc: 0.4167 (0.3444)  loss_scale: 32768.0000 (54595.4566)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.1340  max mem: 15572
[2025-01-16 08:55:56,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:55:56,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 620/2809]  eta: 0:22:11  lr: 0.000000  min_lr: 0.000000  loss: 3.6135 (3.7295)  class_acc: 0.3750 (0.3447)  loss_scale: 32768.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.1509  max mem: 15572
Epoch: [39]  [ 630/2809]  eta: 0:22:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7771 (3.7317)  class_acc: 0.2917 (0.3442)  loss_scale: 65536.0000 (54786.4342)  weight_decay: 0.0500 (0.0500)  time: 0.5872  data: 0.1423  max mem: 15572
Epoch: [39]  [ 640/2809]  eta: 0:22:00  lr: 0.000000  min_lr: 0.000000  loss: 3.6222 (3.7295)  class_acc: 0.2917 (0.3442)  loss_scale: 65536.0000 (54954.1342)  weight_decay: 0.0500 (0.0500)  time: 0.6212  data: 0.1741  max mem: 15572
Epoch: [39]  [ 650/2809]  eta: 0:21:55  lr: 0.000000  min_lr: 0.000000  loss: 3.5163 (3.7298)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (55116.6820)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.1970  max mem: 15572
[2025-01-16 08:56:19,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110203
[2025-01-16 08:56:19,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:56:19,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 660/2809]  eta: 0:21:49  lr: 0.000000  min_lr: 0.000000  loss: 3.6770 (3.7323)  class_acc: 0.2500 (0.3435)  loss_scale: 65536.0000 (54828.1513)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1936  max mem: 15572
Epoch: [39]  [ 670/2809]  eta: 0:21:42  lr: 0.000000  min_lr: 0.000000  loss: 3.7740 (3.7326)  class_acc: 0.2917 (0.3435)  loss_scale: 32768.0000 (54499.3860)  weight_decay: 0.0500 (0.0500)  time: 0.5969  data: 0.1626  max mem: 15572
Epoch: [39]  [ 680/2809]  eta: 0:21:34  lr: 0.000000  min_lr: 0.000000  loss: 3.7689 (3.7344)  class_acc: 0.2917 (0.3425)  loss_scale: 32768.0000 (54180.2761)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.1176  max mem: 15572
Epoch: [39]  [ 690/2809]  eta: 0:21:26  lr: 0.000000  min_lr: 0.000000  loss: 3.7244 (3.7326)  class_acc: 0.2917 (0.3427)  loss_scale: 32768.0000 (53870.4023)  weight_decay: 0.0500 (0.0500)  time: 0.5408  data: 0.0878  max mem: 15572
Epoch: [39]  [ 700/2809]  eta: 0:21:18  lr: 0.000000  min_lr: 0.000000  loss: 3.6668 (3.7340)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (53569.3695)  weight_decay: 0.0500 (0.0500)  time: 0.5434  data: 0.0794  max mem: 15572
Epoch: [39]  [ 710/2809]  eta: 0:21:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6668 (3.7324)  class_acc: 0.3333 (0.3424)  loss_scale: 32768.0000 (53276.8045)  weight_decay: 0.0500 (0.0500)  time: 0.5428  data: 0.0775  max mem: 15572
Epoch: [39]  [ 720/2809]  eta: 0:21:02  lr: 0.000000  min_lr: 0.000000  loss: 3.6236 (3.7317)  class_acc: 0.3750 (0.3426)  loss_scale: 32768.0000 (52992.3551)  weight_decay: 0.0500 (0.0500)  time: 0.5453  data: 0.0976  max mem: 15572
Epoch: [39]  [ 730/2809]  eta: 0:20:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7332 (3.7330)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (52715.6881)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.1190  max mem: 15572
Epoch: [39]  [ 740/2809]  eta: 0:20:50  lr: 0.000000  min_lr: 0.000000  loss: 3.6896 (3.7316)  class_acc: 0.3333 (0.3429)  loss_scale: 32768.0000 (52446.4885)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1482  max mem: 15572
Epoch: [39]  [ 750/2809]  eta: 0:20:42  lr: 0.000000  min_lr: 0.000000  loss: 3.6896 (3.7338)  class_acc: 0.3333 (0.3420)  loss_scale: 32768.0000 (52184.4581)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1377  max mem: 15572
Epoch: [39]  [ 760/2809]  eta: 0:20:37  lr: 0.000000  min_lr: 0.000000  loss: 3.9137 (3.7327)  class_acc: 0.2917 (0.3424)  loss_scale: 32768.0000 (51929.3141)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.1451  max mem: 15572
Epoch: [39]  [ 770/2809]  eta: 0:20:34  lr: 0.000000  min_lr: 0.000000  loss: 3.7723 (3.7346)  class_acc: 0.3333 (0.3419)  loss_scale: 32768.0000 (51680.7886)  weight_decay: 0.0500 (0.0500)  time: 0.6727  data: 0.2305  max mem: 15572
Epoch: [39]  [ 780/2809]  eta: 0:20:25  lr: 0.000000  min_lr: 0.000000  loss: 3.5321 (3.7299)  class_acc: 0.3333 (0.3434)  loss_scale: 32768.0000 (51438.6274)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.1747  max mem: 15572
[2025-01-16 08:57:34,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:57:34,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 790/2809]  eta: 0:20:18  lr: 0.000000  min_lr: 0.000000  loss: 3.7191 (3.7319)  class_acc: 0.3750 (0.3430)  loss_scale: 32768.0000 (51616.8496)  weight_decay: 0.0500 (0.0500)  time: 0.5314  data: 0.0897  max mem: 15572
Epoch: [39]  [ 800/2809]  eta: 0:20:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7391 (3.7322)  class_acc: 0.2917 (0.3427)  loss_scale: 65536.0000 (51790.6217)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1381  max mem: 15572
Epoch: [39]  [ 810/2809]  eta: 0:20:04  lr: 0.000000  min_lr: 0.000000  loss: 3.8357 (3.7357)  class_acc: 0.2917 (0.3419)  loss_scale: 65536.0000 (51960.1085)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.1245  max mem: 15572
Epoch: [39]  [ 820/2809]  eta: 0:20:00  lr: 0.000000  min_lr: 0.000000  loss: 3.8384 (3.7347)  class_acc: 0.2917 (0.3421)  loss_scale: 65536.0000 (52125.4665)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1567  max mem: 15572
Epoch: [39]  [ 830/2809]  eta: 0:19:54  lr: 0.000000  min_lr: 0.000000  loss: 3.7889 (3.7343)  class_acc: 0.3750 (0.3423)  loss_scale: 65536.0000 (52286.8448)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.1878  max mem: 15572
Epoch: [39]  [ 840/2809]  eta: 0:19:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8512 (3.7339)  class_acc: 0.3750 (0.3422)  loss_scale: 65536.0000 (52444.3853)  weight_decay: 0.0500 (0.0500)  time: 0.5589  data: 0.1226  max mem: 15572
Epoch: [39]  [ 850/2809]  eta: 0:19:40  lr: 0.000000  min_lr: 0.000000  loss: 3.5351 (3.7312)  class_acc: 0.3750 (0.3427)  loss_scale: 65536.0000 (52598.2233)  weight_decay: 0.0500 (0.0500)  time: 0.5765  data: 0.1347  max mem: 15572
Epoch: [39]  [ 860/2809]  eta: 0:19:32  lr: 0.000000  min_lr: 0.000000  loss: 3.4057 (3.7283)  class_acc: 0.4167 (0.3438)  loss_scale: 65536.0000 (52748.4878)  weight_decay: 0.0500 (0.0500)  time: 0.5551  data: 0.1060  max mem: 15572
Epoch: [39]  [ 870/2809]  eta: 0:19:26  lr: 0.000000  min_lr: 0.000000  loss: 3.6352 (3.7282)  class_acc: 0.3750 (0.3437)  loss_scale: 65536.0000 (52895.3020)  weight_decay: 0.0500 (0.0500)  time: 0.5419  data: 0.0975  max mem: 15572
Epoch: [39]  [ 880/2809]  eta: 0:19:19  lr: 0.000000  min_lr: 0.000000  loss: 3.7239 (3.7268)  class_acc: 0.3333 (0.3441)  loss_scale: 65536.0000 (53038.7832)  weight_decay: 0.0500 (0.0500)  time: 0.5729  data: 0.1245  max mem: 15572
Epoch: [39]  [ 890/2809]  eta: 0:19:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7764 (3.7278)  class_acc: 0.3333 (0.3440)  loss_scale: 65536.0000 (53179.0438)  weight_decay: 0.0500 (0.0500)  time: 0.5763  data: 0.1339  max mem: 15572
Epoch: [39]  [ 900/2809]  eta: 0:19:07  lr: 0.000000  min_lr: 0.000000  loss: 3.8215 (3.7277)  class_acc: 0.3333 (0.3442)  loss_scale: 65536.0000 (53316.1909)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.1803  max mem: 15572
[2025-01-16 08:58:48,864] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 08:58:48,865] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 08:58:49,315] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110461
[2025-01-16 08:58:49,315] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 08:58:49,315] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [ 910/2809]  eta: 0:19:00  lr: 0.000000  min_lr: 0.000000  loss: 3.9145 (3.7311)  class_acc: 0.2917 (0.3433)  loss_scale: 65536.0000 (53522.2656)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.1437  max mem: 15572
Epoch: [39]  [ 920/2809]  eta: 0:18:55  lr: 0.000000  min_lr: 0.000000  loss: 3.8013 (3.7294)  class_acc: 0.2917 (0.3438)  loss_scale: 65536.0000 (53652.7079)  weight_decay: 0.0500 (0.0500)  time: 0.5800  data: 0.1465  max mem: 15572
Epoch: [39]  [ 930/2809]  eta: 0:18:48  lr: 0.000000  min_lr: 0.000000  loss: 3.6959 (3.7294)  class_acc: 0.3750 (0.3440)  loss_scale: 65536.0000 (53780.3480)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1567  max mem: 15572
Epoch: [39]  [ 940/2809]  eta: 0:18:41  lr: 0.000000  min_lr: 0.000000  loss: 3.6837 (3.7291)  class_acc: 0.3750 (0.3442)  loss_scale: 65536.0000 (53905.2752)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.1189  max mem: 15572
Epoch: [39]  [ 950/2809]  eta: 0:18:35  lr: 0.000000  min_lr: 0.000000  loss: 3.7827 (3.7307)  class_acc: 0.3333 (0.3439)  loss_scale: 65536.0000 (54027.5752)  weight_decay: 0.0500 (0.0500)  time: 0.5749  data: 0.1282  max mem: 15572
Epoch: [39]  [ 960/2809]  eta: 0:18:31  lr: 0.000000  min_lr: 0.000000  loss: 3.9723 (3.7328)  class_acc: 0.2917 (0.3433)  loss_scale: 65536.0000 (54147.3299)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.1929  max mem: 15572
Epoch: [39]  [ 970/2809]  eta: 0:18:24  lr: 0.000000  min_lr: 0.000000  loss: 3.7448 (3.7339)  class_acc: 0.2917 (0.3432)  loss_scale: 65536.0000 (54264.6179)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.1994  max mem: 15572
Epoch: [39]  [ 980/2809]  eta: 0:18:17  lr: 0.000000  min_lr: 0.000000  loss: 3.8160 (3.7363)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (54379.5148)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.1043  max mem: 15572
Epoch: [39]  [ 990/2809]  eta: 0:18:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8353 (3.7371)  class_acc: 0.2917 (0.3426)  loss_scale: 65536.0000 (54492.0928)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0793  max mem: 15572
Epoch: [39]  [1000/2809]  eta: 0:18:04  lr: 0.000000  min_lr: 0.000000  loss: 3.6278 (3.7360)  class_acc: 0.3333 (0.3425)  loss_scale: 65536.0000 (54602.4216)  weight_decay: 0.0500 (0.0500)  time: 0.5809  data: 0.1463  max mem: 15572
[2025-01-16 08:59:47,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110559
[2025-01-16 08:59:47,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 08:59:47,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1010/2809]  eta: 0:17:59  lr: 0.000000  min_lr: 0.000000  loss: 3.5939 (3.7383)  class_acc: 0.3333 (0.3422)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.1960  max mem: 15572
Epoch: [39]  [1020/2809]  eta: 0:17:53  lr: 0.000000  min_lr: 0.000000  loss: 3.5946 (3.7360)  class_acc: 0.3750 (0.3429)  loss_scale: 32768.0000 (54399.3732)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.1736  max mem: 15572
Epoch: [39]  [1030/2809]  eta: 0:17:45  lr: 0.000000  min_lr: 0.000000  loss: 3.5946 (3.7379)  class_acc: 0.3750 (0.3428)  loss_scale: 32768.0000 (54189.5635)  weight_decay: 0.0500 (0.0500)  time: 0.5435  data: 0.0965  max mem: 15572
Epoch: [39]  [1040/2809]  eta: 0:17:41  lr: 0.000000  min_lr: 0.000000  loss: 3.7329 (3.7359)  class_acc: 0.3333 (0.3432)  loss_scale: 32768.0000 (53983.7848)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1422  max mem: 15572
Epoch: [39]  [1050/2809]  eta: 0:17:35  lr: 0.000000  min_lr: 0.000000  loss: 3.4523 (3.7348)  class_acc: 0.3333 (0.3434)  loss_scale: 32768.0000 (53781.9220)  weight_decay: 0.0500 (0.0500)  time: 0.6453  data: 0.1902  max mem: 15572
Epoch: [39]  [1060/2809]  eta: 0:17:28  lr: 0.000000  min_lr: 0.000000  loss: 3.4997 (3.7328)  class_acc: 0.3750 (0.3437)  loss_scale: 32768.0000 (53583.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5758  data: 0.1403  max mem: 15572
Epoch: [39]  [1070/2809]  eta: 0:17:23  lr: 0.000000  min_lr: 0.000000  loss: 3.6345 (3.7330)  class_acc: 0.3750 (0.3438)  loss_scale: 32768.0000 (53389.5051)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.1860  max mem: 15572
Epoch: [39]  [1080/2809]  eta: 0:17:16  lr: 0.000000  min_lr: 0.000000  loss: 3.7733 (3.7336)  class_acc: 0.2917 (0.3437)  loss_scale: 32768.0000 (53198.7419)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.1631  max mem: 15572
Epoch: [39]  [1090/2809]  eta: 0:17:11  lr: 0.000000  min_lr: 0.000000  loss: 3.7523 (3.7331)  class_acc: 0.3750 (0.3443)  loss_scale: 32768.0000 (53011.4757)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1444  max mem: 15572
Epoch: [39]  [1100/2809]  eta: 0:17:04  lr: 0.000000  min_lr: 0.000000  loss: 3.5883 (3.7311)  class_acc: 0.4167 (0.3450)  loss_scale: 32768.0000 (52827.6113)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.1514  max mem: 15572
Epoch: [39]  [1110/2809]  eta: 0:16:58  lr: 0.000000  min_lr: 0.000000  loss: 3.5806 (3.7317)  class_acc: 0.3750 (0.3447)  loss_scale: 32768.0000 (52647.0567)  weight_decay: 0.0500 (0.0500)  time: 0.5947  data: 0.1172  max mem: 15572
Epoch: [39]  [1120/2809]  eta: 0:16:52  lr: 0.000000  min_lr: 0.000000  loss: 3.7688 (3.7336)  class_acc: 0.2917 (0.3444)  loss_scale: 32768.0000 (52469.7235)  weight_decay: 0.0500 (0.0500)  time: 0.5970  data: 0.1244  max mem: 15572
Epoch: [39]  [1130/2809]  eta: 0:16:48  lr: 0.000000  min_lr: 0.000000  loss: 4.0644 (3.7350)  class_acc: 0.2500 (0.3437)  loss_scale: 32768.0000 (52295.5261)  weight_decay: 0.0500 (0.0500)  time: 0.6352  data: 0.1738  max mem: 15572
[2025-01-16 09:01:05,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:01:05,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1140/2809]  eta: 0:16:40  lr: 0.000000  min_lr: 0.000000  loss: 3.7956 (3.7347)  class_acc: 0.2500 (0.3437)  loss_scale: 32768.0000 (52239.2568)  weight_decay: 0.0500 (0.0500)  time: 0.5966  data: 0.1109  max mem: 15572
Epoch: [39]  [1150/2809]  eta: 0:16:34  lr: 0.000000  min_lr: 0.000000  loss: 3.7186 (3.7336)  class_acc: 0.4167 (0.3441)  loss_scale: 65536.0000 (52354.7802)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0711  max mem: 15572
Epoch: [39]  [1160/2809]  eta: 0:16:28  lr: 0.000000  min_lr: 0.000000  loss: 3.5207 (3.7316)  class_acc: 0.4167 (0.3447)  loss_scale: 65536.0000 (52468.3135)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1380  max mem: 15572
Epoch: [39]  [1170/2809]  eta: 0:16:21  lr: 0.000000  min_lr: 0.000000  loss: 3.5207 (3.7310)  class_acc: 0.4167 (0.3449)  loss_scale: 65536.0000 (52579.9078)  weight_decay: 0.0500 (0.0500)  time: 0.5395  data: 0.1003  max mem: 15572
Epoch: [39]  [1180/2809]  eta: 0:16:15  lr: 0.000000  min_lr: 0.000000  loss: 3.6081 (3.7303)  class_acc: 0.3750 (0.3447)  loss_scale: 65536.0000 (52689.6122)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.1161  max mem: 15572
Epoch: [39]  [1190/2809]  eta: 0:16:08  lr: 0.000000  min_lr: 0.000000  loss: 3.7291 (3.7305)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (52797.4744)  weight_decay: 0.0500 (0.0500)  time: 0.5773  data: 0.1246  max mem: 15572
Epoch: [39]  [1200/2809]  eta: 0:16:02  lr: 0.000000  min_lr: 0.000000  loss: 3.6666 (3.7295)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (52903.5404)  weight_decay: 0.0500 (0.0500)  time: 0.5531  data: 0.1091  max mem: 15572
Epoch: [39]  [1210/2809]  eta: 0:15:57  lr: 0.000000  min_lr: 0.000000  loss: 3.8401 (3.7305)  class_acc: 0.2500 (0.3445)  loss_scale: 65536.0000 (53007.8547)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.1841  max mem: 15572
Epoch: [39]  [1220/2809]  eta: 0:15:51  lr: 0.000000  min_lr: 0.000000  loss: 3.9168 (3.7308)  class_acc: 0.2917 (0.3450)  loss_scale: 65536.0000 (53110.4603)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.1874  max mem: 15572
Epoch: [39]  [1230/2809]  eta: 0:15:45  lr: 0.000000  min_lr: 0.000000  loss: 3.6992 (3.7302)  class_acc: 0.3750 (0.3456)  loss_scale: 65536.0000 (53211.3989)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.1594  max mem: 15572
Epoch: [39]  [1240/2809]  eta: 0:15:38  lr: 0.000000  min_lr: 0.000000  loss: 3.7251 (3.7301)  class_acc: 0.3750 (0.3453)  loss_scale: 65536.0000 (53310.7107)  weight_decay: 0.0500 (0.0500)  time: 0.5816  data: 0.1451  max mem: 15572
Epoch: [39]  [1250/2809]  eta: 0:15:33  lr: 0.000000  min_lr: 0.000000  loss: 3.7499 (3.7294)  class_acc: 0.4167 (0.3460)  loss_scale: 65536.0000 (53408.4349)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1730  max mem: 15572
Epoch: [39]  [1260/2809]  eta: 0:15:28  lr: 0.000000  min_lr: 0.000000  loss: 3.7422 (3.7298)  class_acc: 0.3333 (0.3455)  loss_scale: 65536.0000 (53504.6090)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.2012  max mem: 15572
[2025-01-16 09:02:20,051] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:02:20,052] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:02:20,541] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110817
[2025-01-16 09:02:20,541] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:02:20,541] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [1270/2809]  eta: 0:15:22  lr: 0.000000  min_lr: 0.000000  loss: 3.5885 (3.7293)  class_acc: 0.2917 (0.3458)  loss_scale: 65536.0000 (53650.8324)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.1916  max mem: 15572
Epoch: [39]  [1280/2809]  eta: 0:15:15  lr: 0.000000  min_lr: 0.000000  loss: 3.5860 (3.7285)  class_acc: 0.3750 (0.3460)  loss_scale: 65536.0000 (53743.6128)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.1446  max mem: 15572
Epoch: [39]  [1290/2809]  eta: 0:15:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6398 (3.7284)  class_acc: 0.3333 (0.3458)  loss_scale: 65536.0000 (53834.9558)  weight_decay: 0.0500 (0.0500)  time: 0.5801  data: 0.1299  max mem: 15572
[2025-01-16 09:02:39,376] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 110848
[2025-01-16 09:02:39,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:02:39,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [1300/2809]  eta: 0:15:04  lr: 0.000000  min_lr: 0.000000  loss: 3.7941 (3.7293)  class_acc: 0.3333 (0.3457)  loss_scale: 65536.0000 (53824.1476)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1692  max mem: 15572
Epoch: [39]  [1310/2809]  eta: 0:14:57  lr: 0.000000  min_lr: 0.000000  loss: 3.7613 (3.7298)  class_acc: 0.2917 (0.3452)  loss_scale: 32768.0000 (53663.5362)  weight_decay: 0.0500 (0.0500)  time: 0.5636  data: 0.1127  max mem: 15572
Epoch: [39]  [1320/2809]  eta: 0:14:51  lr: 0.000000  min_lr: 0.000000  loss: 3.8815 (3.7312)  class_acc: 0.2917 (0.3450)  loss_scale: 32768.0000 (53505.3565)  weight_decay: 0.0500 (0.0500)  time: 0.5731  data: 0.1374  max mem: 15572
Epoch: [39]  [1330/2809]  eta: 0:14:45  lr: 0.000000  min_lr: 0.000000  loss: 3.8819 (3.7315)  class_acc: 0.2917 (0.3451)  loss_scale: 32768.0000 (53349.5537)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.1643  max mem: 15572
Epoch: [39]  [1340/2809]  eta: 0:14:39  lr: 0.000000  min_lr: 0.000000  loss: 3.6559 (3.7307)  class_acc: 0.3750 (0.3455)  loss_scale: 32768.0000 (53196.0746)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1671  max mem: 15572
Epoch: [39]  [1350/2809]  eta: 0:14:33  lr: 0.000000  min_lr: 0.000000  loss: 3.6559 (3.7304)  class_acc: 0.3750 (0.3456)  loss_scale: 32768.0000 (53044.8675)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.1603  max mem: 15572
Epoch: [39]  [1360/2809]  eta: 0:14:27  lr: 0.000000  min_lr: 0.000000  loss: 3.7328 (3.7307)  class_acc: 0.3333 (0.3458)  loss_scale: 32768.0000 (52895.8824)  weight_decay: 0.0500 (0.0500)  time: 0.5683  data: 0.1257  max mem: 15572
Epoch: [39]  [1370/2809]  eta: 0:14:21  lr: 0.000000  min_lr: 0.000000  loss: 3.7565 (3.7294)  class_acc: 0.3333 (0.3459)  loss_scale: 32768.0000 (52749.0708)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1626  max mem: 15572
Epoch: [39]  [1380/2809]  eta: 0:14:14  lr: 0.000000  min_lr: 0.000000  loss: 3.8263 (3.7293)  class_acc: 0.3333 (0.3458)  loss_scale: 32768.0000 (52604.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5756  data: 0.1520  max mem: 15572
Epoch: [39]  [1390/2809]  eta: 0:14:08  lr: 0.000000  min_lr: 0.000000  loss: 3.9508 (3.7295)  class_acc: 0.3333 (0.3457)  loss_scale: 32768.0000 (52461.7800)  weight_decay: 0.0500 (0.0500)  time: 0.5413  data: 0.1101  max mem: 15572
Epoch: [39]  [1400/2809]  eta: 0:14:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7713 (3.7297)  class_acc: 0.3333 (0.3457)  loss_scale: 32768.0000 (52321.2106)  weight_decay: 0.0500 (0.0500)  time: 0.5506  data: 0.1162  max mem: 15572
Epoch: [39]  [1410/2809]  eta: 0:13:55  lr: 0.000000  min_lr: 0.000000  loss: 3.6825 (3.7292)  class_acc: 0.3333 (0.3458)  loss_scale: 32768.0000 (52182.6336)  weight_decay: 0.0500 (0.0500)  time: 0.5759  data: 0.1370  max mem: 15572
Epoch: [39]  [1420/2809]  eta: 0:13:50  lr: 0.000000  min_lr: 0.000000  loss: 3.7134 (3.7287)  class_acc: 0.3333 (0.3460)  loss_scale: 32768.0000 (52046.0070)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.1865  max mem: 15572
[2025-01-16 09:03:55,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:03:55,056] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [1430/2809]  eta: 0:13:43  lr: 0.000000  min_lr: 0.000000  loss: 3.7423 (3.7298)  class_acc: 0.2917 (0.3453)  loss_scale: 32768.0000 (52025.7834)  weight_decay: 0.0500 (0.0500)  time: 0.5839  data: 0.1320  max mem: 15572
Epoch: [39]  [1440/2809]  eta: 0:13:37  lr: 0.000000  min_lr: 0.000000  loss: 3.8651 (3.7301)  class_acc: 0.2500 (0.3452)  loss_scale: 65536.0000 (52119.5392)  weight_decay: 0.0500 (0.0500)  time: 0.5136  data: 0.0455  max mem: 15572
[2025-01-16 09:04:05,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=111000, skipped=738, lr=[6.686682132076383e-10, 6.686682132076383e-10, 9.552403045823405e-10, 9.552403045823405e-10, 1.3646290065462008e-09, 1.3646290065462008e-09, 1.9494700093517155e-09, 1.9494700093517155e-09, 2.784957156216737e-09, 2.784957156216737e-09, 3.978510223166767e-09, 3.978510223166767e-09, 5.6835860330953815e-09, 5.6835860330953815e-09, 8.11940861870769e-09, 8.11940861870769e-09, 1.1599155169582413e-08, 1.1599155169582413e-08, 1.6570221670832022e-08, 1.6570221670832022e-08, 2.3671745244045743e-08, 2.3671745244045743e-08, 3.381677892006535e-08, 3.381677892006535e-08, 4.8309684171521934e-08, 4.8309684171521934e-08, 6.901383453074562e-08, 6.901383453074562e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 09:04:05,790] [INFO] [timer.py:260:stop] epoch=0/micro_step=111000/global_step=111000, RunningAvgSamplesPerSec=28.59133445709015, CurrSamplesPerSec=25.139741518174894, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [1450/2809]  eta: 0:13:30  lr: 0.000000  min_lr: 0.000000  loss: 3.8571 (3.7295)  class_acc: 0.2917 (0.3454)  loss_scale: 65536.0000 (52212.0028)  weight_decay: 0.0500 (0.0500)  time: 0.5241  data: 0.0695  max mem: 15572
Epoch: [39]  [1460/2809]  eta: 0:13:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7798 (3.7295)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (52303.2005)  weight_decay: 0.0500 (0.0500)  time: 0.5266  data: 0.1013  max mem: 15572
Epoch: [39]  [1470/2809]  eta: 0:13:18  lr: 0.000000  min_lr: 0.000000  loss: 3.8654 (3.7303)  class_acc: 0.2917 (0.3449)  loss_scale: 65536.0000 (52393.1584)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.1748  max mem: 15572
Epoch: [39]  [1480/2809]  eta: 0:13:12  lr: 0.000000  min_lr: 0.000000  loss: 3.8654 (3.7298)  class_acc: 0.2917 (0.3449)  loss_scale: 65536.0000 (52481.9014)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1778  max mem: 15572
Epoch: [39]  [1490/2809]  eta: 0:13:07  lr: 0.000000  min_lr: 0.000000  loss: 3.8500 (3.7309)  class_acc: 0.3333 (0.3449)  loss_scale: 65536.0000 (52569.4541)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.1996  max mem: 15572
Epoch: [39]  [1500/2809]  eta: 0:13:00  lr: 0.000000  min_lr: 0.000000  loss: 3.9430 (3.7315)  class_acc: 0.2917 (0.3446)  loss_scale: 65536.0000 (52655.8401)  weight_decay: 0.0500 (0.0500)  time: 0.6128  data: 0.1796  max mem: 15572
Epoch: [39]  [1510/2809]  eta: 0:12:54  lr: 0.000000  min_lr: 0.000000  loss: 3.8023 (3.7312)  class_acc: 0.2917 (0.3446)  loss_scale: 65536.0000 (52741.0827)  weight_decay: 0.0500 (0.0500)  time: 0.5521  data: 0.1162  max mem: 15572
Epoch: [39]  [1520/2809]  eta: 0:12:48  lr: 0.000000  min_lr: 0.000000  loss: 3.8697 (3.7326)  class_acc: 0.2500 (0.3443)  loss_scale: 65536.0000 (52825.2045)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.1708  max mem: 15572
Epoch: [39]  [1530/2809]  eta: 0:12:42  lr: 0.000000  min_lr: 0.000000  loss: 3.8095 (3.7327)  class_acc: 0.2500 (0.3441)  loss_scale: 65536.0000 (52908.2273)  weight_decay: 0.0500 (0.0500)  time: 0.5802  data: 0.1458  max mem: 15572
Epoch: [39]  [1540/2809]  eta: 0:12:36  lr: 0.000000  min_lr: 0.000000  loss: 3.7458 (3.7340)  class_acc: 0.2500 (0.3436)  loss_scale: 65536.0000 (52990.1726)  weight_decay: 0.0500 (0.0500)  time: 0.5780  data: 0.1310  max mem: 15572
Epoch: [39]  [1550/2809]  eta: 0:12:30  lr: 0.000000  min_lr: 0.000000  loss: 3.6515 (3.7333)  class_acc: 0.2917 (0.3437)  loss_scale: 65536.0000 (53071.0613)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.1668  max mem: 15572
[2025-01-16 09:05:08,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:05:08,868] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:05:10,373] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111108
[2025-01-16 09:05:10,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:05:10,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [1560/2809]  eta: 0:12:23  lr: 0.000000  min_lr: 0.000000  loss: 3.6359 (3.7337)  class_acc: 0.3333 (0.3436)  loss_scale: 65536.0000 (53276.8635)  weight_decay: 0.0500 (0.0500)  time: 0.5431  data: 0.0899  max mem: 15572
Epoch: [39]  [1570/2809]  eta: 0:12:17  lr: 0.000000  min_lr: 0.000000  loss: 3.7836 (3.7333)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (53354.8975)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0583  max mem: 15572
Epoch: [39]  [1580/2809]  eta: 0:12:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6659 (3.7336)  class_acc: 0.3750 (0.3440)  loss_scale: 65536.0000 (53431.9443)  weight_decay: 0.0500 (0.0500)  time: 0.5319  data: 0.0964  max mem: 15572
Epoch: [39]  [1590/2809]  eta: 0:12:04  lr: 0.000000  min_lr: 0.000000  loss: 3.7191 (3.7342)  class_acc: 0.3333 (0.3438)  loss_scale: 65536.0000 (53508.0226)  weight_decay: 0.0500 (0.0500)  time: 0.5754  data: 0.1410  max mem: 15572
Epoch: [39]  [1600/2809]  eta: 0:11:59  lr: 0.000000  min_lr: 0.000000  loss: 3.8759 (3.7355)  class_acc: 0.2917 (0.3436)  loss_scale: 65536.0000 (53583.1505)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.2019  max mem: 15572
Epoch: [39]  [1610/2809]  eta: 0:11:53  lr: 0.000000  min_lr: 0.000000  loss: 3.6572 (3.7337)  class_acc: 0.3750 (0.3442)  loss_scale: 65536.0000 (53657.3457)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.2253  max mem: 15572
Epoch: [39]  [1620/2809]  eta: 0:11:48  lr: 0.000000  min_lr: 0.000000  loss: 3.5011 (3.7346)  class_acc: 0.3750 (0.3441)  loss_scale: 65536.0000 (53730.6255)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.2196  max mem: 15572
Epoch: [39]  [1630/2809]  eta: 0:11:41  lr: 0.000000  min_lr: 0.000000  loss: 3.9001 (3.7350)  class_acc: 0.2917 (0.3440)  loss_scale: 65536.0000 (53803.0067)  weight_decay: 0.0500 (0.0500)  time: 0.5744  data: 0.1330  max mem: 15572
Epoch: [39]  [1640/2809]  eta: 0:11:35  lr: 0.000000  min_lr: 0.000000  loss: 3.5713 (3.7331)  class_acc: 0.4167 (0.3445)  loss_scale: 65536.0000 (53874.5058)  weight_decay: 0.0500 (0.0500)  time: 0.5492  data: 0.0994  max mem: 15572
Epoch: [39]  [1650/2809]  eta: 0:11:29  lr: 0.000000  min_lr: 0.000000  loss: 3.5330 (3.7329)  class_acc: 0.4167 (0.3444)  loss_scale: 65536.0000 (53945.1387)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.1340  max mem: 15572
Epoch: [39]  [1660/2809]  eta: 0:11:23  lr: 0.000000  min_lr: 0.000000  loss: 3.5330 (3.7327)  class_acc: 0.4167 (0.3445)  loss_scale: 65536.0000 (54014.9211)  weight_decay: 0.0500 (0.0500)  time: 0.5429  data: 0.0953  max mem: 15572
Epoch: [39]  [1670/2809]  eta: 0:11:17  lr: 0.000000  min_lr: 0.000000  loss: 3.6212 (3.7328)  class_acc: 0.4167 (0.3447)  loss_scale: 65536.0000 (54083.8683)  weight_decay: 0.0500 (0.0500)  time: 0.5517  data: 0.1211  max mem: 15572
Epoch: [39]  [1680/2809]  eta: 0:11:11  lr: 0.000000  min_lr: 0.000000  loss: 3.6479 (3.7331)  class_acc: 0.4167 (0.3446)  loss_scale: 65536.0000 (54151.9952)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.1662  max mem: 15572
[2025-01-16 09:06:24,831] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:06:24,832] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:06:28,165] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111241
[2025-01-16 09:06:28,165] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:06:28,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [1690/2809]  eta: 0:11:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7077 (3.7328)  class_acc: 0.2917 (0.3444)  loss_scale: 65536.0000 (54374.3394)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.1491  max mem: 15572
Epoch: [39]  [1700/2809]  eta: 0:10:59  lr: 0.000000  min_lr: 0.000000  loss: 3.7077 (3.7330)  class_acc: 0.2500 (0.3444)  loss_scale: 65536.0000 (54439.9577)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.1851  max mem: 15572
Epoch: [39]  [1710/2809]  eta: 0:10:54  lr: 0.000000  min_lr: 0.000000  loss: 3.7274 (3.7333)  class_acc: 0.2917 (0.3443)  loss_scale: 65536.0000 (54504.8089)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.1885  max mem: 15572
Epoch: [39]  [1720/2809]  eta: 0:10:47  lr: 0.000000  min_lr: 0.000000  loss: 3.6807 (3.7322)  class_acc: 0.4167 (0.3446)  loss_scale: 65536.0000 (54568.9064)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1337  max mem: 15572
Epoch: [39]  [1730/2809]  eta: 0:10:41  lr: 0.000000  min_lr: 0.000000  loss: 3.5446 (3.7316)  class_acc: 0.4167 (0.3447)  loss_scale: 65536.0000 (54632.2634)  weight_decay: 0.0500 (0.0500)  time: 0.5268  data: 0.0747  max mem: 15572
Epoch: [39]  [1740/2809]  eta: 0:10:34  lr: 0.000000  min_lr: 0.000000  loss: 3.5472 (3.7307)  class_acc: 0.3333 (0.3450)  loss_scale: 65536.0000 (54694.8926)  weight_decay: 0.0500 (0.0500)  time: 0.5107  data: 0.0621  max mem: 15572
Epoch: [39]  [1750/2809]  eta: 0:10:28  lr: 0.000000  min_lr: 0.000000  loss: 3.6329 (3.7306)  class_acc: 0.3333 (0.3452)  loss_scale: 65536.0000 (54756.8064)  weight_decay: 0.0500 (0.0500)  time: 0.4936  data: 0.0525  max mem: 15572
Epoch: [39]  [1760/2809]  eta: 0:10:22  lr: 0.000000  min_lr: 0.000000  loss: 3.9580 (3.7323)  class_acc: 0.2500 (0.3448)  loss_scale: 65536.0000 (54818.0170)  weight_decay: 0.0500 (0.0500)  time: 0.5545  data: 0.1107  max mem: 15572
Epoch: [39]  [1770/2809]  eta: 0:10:16  lr: 0.000000  min_lr: 0.000000  loss: 3.6955 (3.7311)  class_acc: 0.2917 (0.3449)  loss_scale: 65536.0000 (54878.5364)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.1976  max mem: 15572
Epoch: [39]  [1780/2809]  eta: 0:10:10  lr: 0.000000  min_lr: 0.000000  loss: 3.6195 (3.7307)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (54938.3762)  weight_decay: 0.0500 (0.0500)  time: 0.5857  data: 0.1515  max mem: 15572
Epoch: [39]  [1790/2809]  eta: 0:10:04  lr: 0.000000  min_lr: 0.000000  loss: 3.6280 (3.7303)  class_acc: 0.3333 (0.3448)  loss_scale: 65536.0000 (54997.5477)  weight_decay: 0.0500 (0.0500)  time: 0.5518  data: 0.1165  max mem: 15572
Epoch: [39]  [1800/2809]  eta: 0:09:58  lr: 0.000000  min_lr: 0.000000  loss: 3.6280 (3.7295)  class_acc: 0.3750 (0.3449)  loss_scale: 65536.0000 (55056.0622)  weight_decay: 0.0500 (0.0500)  time: 0.5369  data: 0.0931  max mem: 15572
Epoch: [39]  [1810/2809]  eta: 0:09:51  lr: 0.000000  min_lr: 0.000000  loss: 3.8248 (3.7309)  class_acc: 0.2917 (0.3445)  loss_scale: 65536.0000 (55113.9304)  weight_decay: 0.0500 (0.0500)  time: 0.5417  data: 0.0891  max mem: 15572
[2025-01-16 09:07:41,794] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:07:41,794] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:07:42,174] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111371
[2025-01-16 09:07:42,175] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:07:42,175] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [1820/2809]  eta: 0:09:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8206 (3.7304)  class_acc: 0.2917 (0.3445)  loss_scale: 65536.0000 (55207.1521)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1662  max mem: 15572
Epoch: [39]  [1830/2809]  eta: 0:09:40  lr: 0.000000  min_lr: 0.000000  loss: 3.6838 (3.7303)  class_acc: 0.2917 (0.3442)  loss_scale: 65536.0000 (55263.5631)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.1920  max mem: 15572
Epoch: [39]  [1840/2809]  eta: 0:09:34  lr: 0.000000  min_lr: 0.000000  loss: 3.7303 (3.7296)  class_acc: 0.2917 (0.3442)  loss_scale: 65536.0000 (55319.3612)  weight_decay: 0.0500 (0.0500)  time: 0.6164  data: 0.1706  max mem: 15572
Epoch: [39]  [1850/2809]  eta: 0:09:28  lr: 0.000000  min_lr: 0.000000  loss: 3.6378 (3.7294)  class_acc: 0.3750 (0.3444)  loss_scale: 65536.0000 (55374.5565)  weight_decay: 0.0500 (0.0500)  time: 0.6203  data: 0.1716  max mem: 15572
Epoch: [39]  [1860/2809]  eta: 0:09:23  lr: 0.000000  min_lr: 0.000000  loss: 3.5180 (3.7282)  class_acc: 0.3750 (0.3448)  loss_scale: 65536.0000 (55429.1585)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.1778  max mem: 15572
Epoch: [39]  [1870/2809]  eta: 0:09:17  lr: 0.000000  min_lr: 0.000000  loss: 3.5010 (3.7269)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (55483.1769)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.1879  max mem: 15572
Epoch: [39]  [1880/2809]  eta: 0:09:12  lr: 0.000000  min_lr: 0.000000  loss: 3.5010 (3.7270)  class_acc: 0.2917 (0.3445)  loss_scale: 65536.0000 (55536.6209)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.2478  max mem: 15572
Epoch: [39]  [1890/2809]  eta: 0:09:05  lr: 0.000000  min_lr: 0.000000  loss: 3.5807 (3.7266)  class_acc: 0.3333 (0.3445)  loss_scale: 65536.0000 (55589.4997)  weight_decay: 0.0500 (0.0500)  time: 0.5774  data: 0.1520  max mem: 15572
Epoch: [39]  [1900/2809]  eta: 0:08:59  lr: 0.000000  min_lr: 0.000000  loss: 3.6060 (3.7267)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (55641.8222)  weight_decay: 0.0500 (0.0500)  time: 0.4492  data: 0.0087  max mem: 15572
Epoch: [39]  [1910/2809]  eta: 0:08:53  lr: 0.000000  min_lr: 0.000000  loss: 3.8021 (3.7261)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (55693.5971)  weight_decay: 0.0500 (0.0500)  time: 0.5401  data: 0.0961  max mem: 15572
Epoch: [39]  [1920/2809]  eta: 0:08:47  lr: 0.000000  min_lr: 0.000000  loss: 3.7781 (3.7266)  class_acc: 0.3750 (0.3447)  loss_scale: 65536.0000 (55744.8329)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.1610  max mem: 15572
Epoch: [39]  [1930/2809]  eta: 0:08:41  lr: 0.000000  min_lr: 0.000000  loss: 3.7775 (3.7257)  class_acc: 0.3750 (0.3446)  loss_scale: 65536.0000 (55795.5381)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.1474  max mem: 15572
Epoch: [39]  [1940/2809]  eta: 0:08:35  lr: 0.000000  min_lr: 0.000000  loss: 3.8066 (3.7259)  class_acc: 0.3750 (0.3446)  loss_scale: 65536.0000 (55845.7208)  weight_decay: 0.0500 (0.0500)  time: 0.5737  data: 0.1378  max mem: 15572
[2025-01-16 09:08:58,421] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:08:58,421] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [39]  [1950/2809]  eta: 0:08:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8542 (3.7265)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (55962.5710)  weight_decay: 0.0500 (0.0500)  time: 0.5384  data: 0.1115  max mem: 15572
[2025-01-16 09:09:00,152] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111504
[2025-01-16 09:09:00,152] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:09:00,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [1960/2809]  eta: 0:08:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7172 (3.7259)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (56078.2295)  weight_decay: 0.0500 (0.0500)  time: 0.5556  data: 0.1087  max mem: 15572
Epoch: [39]  [1970/2809]  eta: 0:08:17  lr: 0.000000  min_lr: 0.000000  loss: 3.6710 (3.7267)  class_acc: 0.3333 (0.3444)  loss_scale: 65536.0000 (56126.2141)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.1389  max mem: 15572
Epoch: [39]  [1980/2809]  eta: 0:08:11  lr: 0.000000  min_lr: 0.000000  loss: 3.7515 (3.7266)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (56173.7143)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.1450  max mem: 15572
Epoch: [39]  [1990/2809]  eta: 0:08:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7515 (3.7266)  class_acc: 0.3333 (0.3446)  loss_scale: 65536.0000 (56220.7373)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.1595  max mem: 15572
Epoch: [39]  [2000/2809]  eta: 0:07:59  lr: 0.000000  min_lr: 0.000000  loss: 3.7851 (3.7266)  class_acc: 0.3333 (0.3448)  loss_scale: 65536.0000 (56267.2904)  weight_decay: 0.0500 (0.0500)  time: 0.5760  data: 0.1292  max mem: 15572
Epoch: [39]  [2010/2809]  eta: 0:07:53  lr: 0.000000  min_lr: 0.000000  loss: 3.8944 (3.7273)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (56313.3804)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.1167  max mem: 15572
Epoch: [39]  [2020/2809]  eta: 0:07:47  lr: 0.000000  min_lr: 0.000000  loss: 3.9515 (3.7277)  class_acc: 0.2917 (0.3446)  loss_scale: 65536.0000 (56359.0143)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.1625  max mem: 15572
Epoch: [39]  [2030/2809]  eta: 0:07:42  lr: 0.000000  min_lr: 0.000000  loss: 3.7541 (3.7269)  class_acc: 0.3333 (0.3449)  loss_scale: 65536.0000 (56404.1989)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.2144  max mem: 15572
Epoch: [39]  [2040/2809]  eta: 0:07:35  lr: 0.000000  min_lr: 0.000000  loss: 3.6548 (3.7270)  class_acc: 0.3333 (0.3448)  loss_scale: 65536.0000 (56448.9407)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.1477  max mem: 15572
Epoch: [39]  [2050/2809]  eta: 0:07:29  lr: 0.000000  min_lr: 0.000000  loss: 3.9205 (3.7280)  class_acc: 0.2500 (0.3445)  loss_scale: 65536.0000 (56493.2462)  weight_decay: 0.0500 (0.0500)  time: 0.4984  data: 0.0393  max mem: 15572
Epoch: [39]  [2060/2809]  eta: 0:07:23  lr: 0.000000  min_lr: 0.000000  loss: 3.7353 (3.7265)  class_acc: 0.3333 (0.3447)  loss_scale: 65536.0000 (56537.1218)  weight_decay: 0.0500 (0.0500)  time: 0.5351  data: 0.1025  max mem: 15572
Epoch: [39]  [2070/2809]  eta: 0:07:17  lr: 0.000000  min_lr: 0.000000  loss: 3.7324 (3.7257)  class_acc: 0.3750 (0.3449)  loss_scale: 65536.0000 (56580.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5312  data: 0.1174  max mem: 15572
Epoch: [39]  [2080/2809]  eta: 0:07:11  lr: 0.000000  min_lr: 0.000000  loss: 3.7712 (3.7257)  class_acc: 0.3333 (0.3450)  loss_scale: 65536.0000 (56623.6079)  weight_decay: 0.0500 (0.0500)  time: 0.5677  data: 0.1448  max mem: 15572
[2025-01-16 09:10:15,589] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:10:15,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:10:16,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111636
[2025-01-16 09:10:16,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:10:16,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [2090/2809]  eta: 0:07:05  lr: 0.000000  min_lr: 0.000000  loss: 3.7808 (3.7267)  class_acc: 0.3333 (0.3449)  loss_scale: 65536.0000 (56760.2563)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.1740  max mem: 15572
Epoch: [39]  [2100/2809]  eta: 0:06:59  lr: 0.000000  min_lr: 0.000000  loss: 3.6550 (3.7264)  class_acc: 0.2917 (0.3450)  loss_scale: 65536.0000 (56802.0257)  weight_decay: 0.0500 (0.0500)  time: 0.5917  data: 0.1481  max mem: 15572
Epoch: [39]  [2110/2809]  eta: 0:06:53  lr: 0.000000  min_lr: 0.000000  loss: 3.6038 (3.7256)  class_acc: 0.3333 (0.3451)  loss_scale: 65536.0000 (56843.3993)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.1202  max mem: 15572
Epoch: [39]  [2120/2809]  eta: 0:06:47  lr: 0.000000  min_lr: 0.000000  loss: 3.5778 (3.7251)  class_acc: 0.4167 (0.3453)  loss_scale: 65536.0000 (56884.3828)  weight_decay: 0.0500 (0.0500)  time: 0.5633  data: 0.0991  max mem: 15572
Epoch: [39]  [2130/2809]  eta: 0:06:41  lr: 0.000000  min_lr: 0.000000  loss: 3.6881 (3.7250)  class_acc: 0.3750 (0.3453)  loss_scale: 65536.0000 (56924.9817)  weight_decay: 0.0500 (0.0500)  time: 0.5782  data: 0.1341  max mem: 15572
Epoch: [39]  [2140/2809]  eta: 0:06:35  lr: 0.000000  min_lr: 0.000000  loss: 3.7318 (3.7255)  class_acc: 0.3333 (0.3453)  loss_scale: 65536.0000 (56965.2013)  weight_decay: 0.0500 (0.0500)  time: 0.5784  data: 0.1457  max mem: 15572
Epoch: [39]  [2150/2809]  eta: 0:06:29  lr: 0.000000  min_lr: 0.000000  loss: 3.7360 (3.7253)  class_acc: 0.3333 (0.3455)  loss_scale: 65536.0000 (57005.0470)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.1467  max mem: 15572
Epoch: [39]  [2160/2809]  eta: 0:06:23  lr: 0.000000  min_lr: 0.000000  loss: 3.5555 (3.7237)  class_acc: 0.3750 (0.3458)  loss_scale: 65536.0000 (57044.5238)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.1275  max mem: 15572
Epoch: [39]  [2170/2809]  eta: 0:06:17  lr: 0.000000  min_lr: 0.000000  loss: 3.5555 (3.7237)  class_acc: 0.3750 (0.3460)  loss_scale: 65536.0000 (57083.6370)  weight_decay: 0.0500 (0.0500)  time: 0.5465  data: 0.1249  max mem: 15572
[2025-01-16 09:11:12,588] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111730
[2025-01-16 09:11:12,588] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-01-16 09:11:12,589] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [2180/2809]  eta: 0:06:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7580 (3.7233)  class_acc: 0.3333 (0.3460)  loss_scale: 65536.0000 (57092.3430)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.1968  max mem: 15572
Epoch: [39]  [2190/2809]  eta: 0:06:06  lr: 0.000000  min_lr: 0.000000  loss: 3.7580 (3.7236)  class_acc: 0.3333 (0.3459)  loss_scale: 32768.0000 (56981.3236)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.2047  max mem: 15572
Epoch: [39]  [2200/2809]  eta: 0:06:00  lr: 0.000000  min_lr: 0.000000  loss: 3.7802 (3.7233)  class_acc: 0.3750 (0.3460)  loss_scale: 32768.0000 (56871.3130)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.1881  max mem: 15572
Epoch: [39]  [2210/2809]  eta: 0:05:54  lr: 0.000000  min_lr: 0.000000  loss: 3.6208 (3.7227)  class_acc: 0.4167 (0.3463)  loss_scale: 32768.0000 (56762.2976)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.1732  max mem: 15572
Epoch: [39]  [2220/2809]  eta: 0:05:48  lr: 0.000000  min_lr: 0.000000  loss: 3.5823 (3.7225)  class_acc: 0.3750 (0.3464)  loss_scale: 32768.0000 (56654.2638)  weight_decay: 0.0500 (0.0500)  time: 0.5544  data: 0.1199  max mem: 15572
Epoch: [39]  [2230/2809]  eta: 0:05:42  lr: 0.000000  min_lr: 0.000000  loss: 3.8133 (3.7225)  class_acc: 0.3333 (0.3465)  loss_scale: 32768.0000 (56547.1986)  weight_decay: 0.0500 (0.0500)  time: 0.5539  data: 0.1153  max mem: 15572
Epoch: [39]  [2240/2809]  eta: 0:05:36  lr: 0.000000  min_lr: 0.000000  loss: 3.7772 (3.7220)  class_acc: 0.3333 (0.3466)  loss_scale: 32768.0000 (56441.0888)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.1775  max mem: 15572
Epoch: [39]  [2250/2809]  eta: 0:05:30  lr: 0.000000  min_lr: 0.000000  loss: 3.8365 (3.7224)  class_acc: 0.2917 (0.3464)  loss_scale: 32768.0000 (56335.9218)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.1864  max mem: 15572
Epoch: [39]  [2260/2809]  eta: 0:05:24  lr: 0.000000  min_lr: 0.000000  loss: 3.8389 (3.7228)  class_acc: 0.2917 (0.3464)  loss_scale: 32768.0000 (56231.6851)  weight_decay: 0.0500 (0.0500)  time: 0.5769  data: 0.1252  max mem: 15572
Epoch: [39]  [2270/2809]  eta: 0:05:18  lr: 0.000000  min_lr: 0.000000  loss: 3.8003 (3.7228)  class_acc: 0.3333 (0.3462)  loss_scale: 32768.0000 (56128.3664)  weight_decay: 0.0500 (0.0500)  time: 0.5669  data: 0.1182  max mem: 15572
Epoch: [39]  [2280/2809]  eta: 0:05:12  lr: 0.000000  min_lr: 0.000000  loss: 3.7089 (3.7228)  class_acc: 0.3333 (0.3461)  loss_scale: 32768.0000 (56025.9535)  weight_decay: 0.0500 (0.0500)  time: 0.5536  data: 0.1185  max mem: 15572
Epoch: [39]  [2290/2809]  eta: 0:05:07  lr: 0.000000  min_lr: 0.000000  loss: 3.7292 (3.7231)  class_acc: 0.2917 (0.3458)  loss_scale: 32768.0000 (55924.4347)  weight_decay: 0.0500 (0.0500)  time: 0.6179  data: 0.1942  max mem: 15572
Epoch: [39]  [2300/2809]  eta: 0:05:01  lr: 0.000000  min_lr: 0.000000  loss: 3.7974 (3.7224)  class_acc: 0.2917 (0.3459)  loss_scale: 32768.0000 (55823.7983)  weight_decay: 0.0500 (0.0500)  time: 0.6614  data: 0.2341  max mem: 15572
[2025-01-16 09:12:28,230] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:12:28,230] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [2310/2809]  eta: 0:04:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7974 (3.7223)  class_acc: 0.3750 (0.3460)  loss_scale: 32768.0000 (55766.5703)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.1975  max mem: 15572
Epoch: [39]  [2320/2809]  eta: 0:04:49  lr: 0.000000  min_lr: 0.000000  loss: 3.7021 (3.7224)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (55808.6618)  weight_decay: 0.0500 (0.0500)  time: 0.5923  data: 0.1637  max mem: 15572
Epoch: [39]  [2330/2809]  eta: 0:04:43  lr: 0.000000  min_lr: 0.000000  loss: 3.6467 (3.7222)  class_acc: 0.3333 (0.3461)  loss_scale: 65536.0000 (55850.3921)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.1249  max mem: 15572
Epoch: [39]  [2340/2809]  eta: 0:04:37  lr: 0.000000  min_lr: 0.000000  loss: 3.5975 (3.7213)  class_acc: 0.3333 (0.3463)  loss_scale: 65536.0000 (55891.7659)  weight_decay: 0.0500 (0.0500)  time: 0.5647  data: 0.1100  max mem: 15572
Epoch: [39]  [2350/2809]  eta: 0:04:31  lr: 0.000000  min_lr: 0.000000  loss: 3.4034 (3.7206)  class_acc: 0.3333 (0.3466)  loss_scale: 65536.0000 (55932.7877)  weight_decay: 0.0500 (0.0500)  time: 0.5841  data: 0.1378  max mem: 15572
Epoch: [39]  [2360/2809]  eta: 0:04:25  lr: 0.000000  min_lr: 0.000000  loss: 3.6026 (3.7200)  class_acc: 0.3333 (0.3467)  loss_scale: 65536.0000 (55973.4621)  weight_decay: 0.0500 (0.0500)  time: 0.6152  data: 0.1579  max mem: 15572
Epoch: [39]  [2370/2809]  eta: 0:04:19  lr: 0.000000  min_lr: 0.000000  loss: 3.5903 (3.7190)  class_acc: 0.3750 (0.3470)  loss_scale: 65536.0000 (56013.7933)  weight_decay: 0.0500 (0.0500)  time: 0.5447  data: 0.0667  max mem: 15572
Epoch: [39]  [2380/2809]  eta: 0:04:13  lr: 0.000000  min_lr: 0.000000  loss: 3.7002 (3.7187)  class_acc: 0.4167 (0.3471)  loss_scale: 65536.0000 (56053.7858)  weight_decay: 0.0500 (0.0500)  time: 0.4871  data: 0.0276  max mem: 15572
Epoch: [39]  [2390/2809]  eta: 0:04:07  lr: 0.000000  min_lr: 0.000000  loss: 3.7035 (3.7186)  class_acc: 0.3333 (0.3470)  loss_scale: 65536.0000 (56093.4437)  weight_decay: 0.0500 (0.0500)  time: 0.5438  data: 0.1004  max mem: 15572
Epoch: [39]  [2400/2809]  eta: 0:04:01  lr: 0.000000  min_lr: 0.000000  loss: 3.8259 (3.7188)  class_acc: 0.3333 (0.3470)  loss_scale: 65536.0000 (56132.7713)  weight_decay: 0.0500 (0.0500)  time: 0.5959  data: 0.1362  max mem: 15572
Epoch: [39]  [2410/2809]  eta: 0:03:55  lr: 0.000000  min_lr: 0.000000  loss: 3.7945 (3.7192)  class_acc: 0.3333 (0.3470)  loss_scale: 65536.0000 (56171.7727)  weight_decay: 0.0500 (0.0500)  time: 0.6336  data: 0.1684  max mem: 15572
Epoch: [39]  [2420/2809]  eta: 0:03:50  lr: 0.000000  min_lr: 0.000000  loss: 3.6384 (3.7187)  class_acc: 0.3750 (0.3472)  loss_scale: 65536.0000 (56210.4519)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.1919  max mem: 15572
Epoch: [39]  [2430/2809]  eta: 0:03:44  lr: 0.000000  min_lr: 0.000000  loss: 3.6289 (3.7181)  class_acc: 0.3750 (0.3474)  loss_scale: 65536.0000 (56248.8128)  weight_decay: 0.0500 (0.0500)  time: 0.6293  data: 0.1786  max mem: 15572
[2025-01-16 09:13:45,411] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:13:45,412] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [39]  [2440/2809]  eta: 0:03:38  lr: 0.000000  min_lr: 0.000000  loss: 3.6392 (3.7176)  class_acc: 0.3333 (0.3475)  loss_scale: 65536.0000 (56421.0995)  weight_decay: 0.0500 (0.0500)  time: 0.6277  data: 0.1790  max mem: 15572
[2025-01-16 09:13:49,201] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 111994
[2025-01-16 09:13:49,201] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:13:49,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-01-16 09:13:52,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=112000, skipped=745, lr=[4.692607746832985e-10, 4.692607746832985e-10, 6.703725352618551e-10, 6.703725352618551e-10, 9.576750503740788e-10, 9.576750503740788e-10, 1.3681072148201127e-09, 1.3681072148201127e-09, 1.9544388783144467e-09, 1.9544388783144467e-09, 2.79205554044921e-09, 2.79205554044921e-09, 3.9886507720703e-09, 3.9886507720703e-09, 5.698072531529001e-09, 5.698072531529001e-09, 8.14010361647e-09, 8.14010361647e-09, 1.1628719452100003e-08, 1.1628719452100003e-08, 1.661245636014286e-08, 1.661245636014286e-08, 2.3732080514489803e-08, 2.3732080514489803e-08, 3.390297216355687e-08, 3.390297216355687e-08, 4.843281737650981e-08, 4.843281737650981e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-01-16 09:13:52,031] [INFO] [timer.py:260:stop] epoch=0/micro_step=112000/global_step=112000, RunningAvgSamplesPerSec=28.593231309747825, CurrSamplesPerSec=31.281555859956693, MemAllocated=0.4GB, MaxMemAllocated=15.21GB
Epoch: [39]  [2450/2809]  eta: 0:03:32  lr: 0.000000  min_lr: 0.000000  loss: 3.7741 (3.7178)  class_acc: 0.2500 (0.3474)  loss_scale: 65536.0000 (56511.7650)  weight_decay: 0.0500 (0.0500)  time: 0.5937  data: 0.1497  max mem: 15572
Epoch: [39]  [2460/2809]  eta: 0:03:26  lr: 0.000000  min_lr: 0.000000  loss: 3.8452 (3.7184)  class_acc: 0.2917 (0.3473)  loss_scale: 65536.0000 (56548.4340)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.1745  max mem: 15572
Epoch: [39]  [2470/2809]  eta: 0:03:20  lr: 0.000000  min_lr: 0.000000  loss: 3.7470 (3.7189)  class_acc: 0.3333 (0.3472)  loss_scale: 65536.0000 (56584.8062)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.1983  max mem: 15572
Epoch: [39]  [2480/2809]  eta: 0:03:14  lr: 0.000000  min_lr: 0.000000  loss: 3.9203 (3.7196)  class_acc: 0.2500 (0.3468)  loss_scale: 65536.0000 (56620.8851)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.1466  max mem: 15572
Epoch: [39]  [2490/2809]  eta: 0:03:08  lr: 0.000000  min_lr: 0.000000  loss: 3.9760 (3.7201)  class_acc: 0.2500 (0.3465)  loss_scale: 65536.0000 (56656.6744)  weight_decay: 0.0500 (0.0500)  time: 0.5778  data: 0.1350  max mem: 15572
Epoch: [39]  [2500/2809]  eta: 0:03:03  lr: 0.000000  min_lr: 0.000000  loss: 3.8662 (3.7205)  class_acc: 0.2917 (0.3464)  loss_scale: 65536.0000 (56692.1775)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.2606  max mem: 15572
Epoch: [39]  [2510/2809]  eta: 0:02:57  lr: 0.000000  min_lr: 0.000000  loss: 3.8693 (3.7208)  class_acc: 0.2917 (0.3462)  loss_scale: 65536.0000 (56727.3978)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.2093  max mem: 15572
Epoch: [39]  [2520/2809]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 3.8693 (3.7213)  class_acc: 0.2917 (0.3460)  loss_scale: 65536.0000 (56762.3388)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0980  max mem: 15572
Epoch: [39]  [2530/2809]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 3.5816 (3.7203)  class_acc: 0.3750 (0.3462)  loss_scale: 65536.0000 (56797.0036)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.1735  max mem: 15572
Epoch: [39]  [2540/2809]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 3.3299 (3.7189)  class_acc: 0.4167 (0.3466)  loss_scale: 65536.0000 (56831.3955)  weight_decay: 0.0500 (0.0500)  time: 0.6466  data: 0.1860  max mem: 15572
Epoch: [39]  [2550/2809]  eta: 0:02:33  lr: 0.000000  min_lr: 0.000000  loss: 3.6244 (3.7194)  class_acc: 0.3750 (0.3466)  loss_scale: 65536.0000 (56865.5178)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.1376  max mem: 15572
Epoch: [39]  [2560/2809]  eta: 0:02:27  lr: 0.000000  min_lr: 0.000000  loss: 3.7864 (3.7196)  class_acc: 0.3333 (0.3466)  loss_scale: 65536.0000 (56899.3737)  weight_decay: 0.0500 (0.0500)  time: 0.5584  data: 0.1262  max mem: 15572
Epoch: [39]  [2570/2809]  eta: 0:02:21  lr: 0.000000  min_lr: 0.000000  loss: 3.6810 (3.7188)  class_acc: 0.3333 (0.3468)  loss_scale: 65536.0000 (56932.9662)  weight_decay: 0.0500 (0.0500)  time: 0.6196  data: 0.1774  max mem: 15572
[2025-01-16 09:15:07,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:15:07,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:15:08,123] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 112124
[2025-01-16 09:15:08,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:15:08,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [2580/2809]  eta: 0:02:15  lr: 0.000000  min_lr: 0.000000  loss: 3.5780 (3.7187)  class_acc: 0.3750 (0.3471)  loss_scale: 65536.0000 (56991.6900)  weight_decay: 0.0500 (0.0500)  time: 0.6641  data: 0.1938  max mem: 15572
Epoch: [39]  [2590/2809]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 3.6437 (3.7188)  class_acc: 0.3750 (0.3472)  loss_scale: 65536.0000 (57024.6669)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.1527  max mem: 15572
Epoch: [39]  [2600/2809]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 3.7564 (3.7192)  class_acc: 0.3333 (0.3470)  loss_scale: 65536.0000 (57057.3902)  weight_decay: 0.0500 (0.0500)  time: 0.6111  data: 0.1734  max mem: 15572
Epoch: [39]  [2610/2809]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 3.7456 (3.7191)  class_acc: 0.3333 (0.3471)  loss_scale: 65536.0000 (57089.8629)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.1715  max mem: 15572
Epoch: [39]  [2620/2809]  eta: 0:01:52  lr: 0.000000  min_lr: 0.000000  loss: 3.7733 (3.7193)  class_acc: 0.3750 (0.3472)  loss_scale: 65536.0000 (57122.0878)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.1751  max mem: 15572
Epoch: [39]  [2630/2809]  eta: 0:01:46  lr: 0.000000  min_lr: 0.000000  loss: 3.8082 (3.7191)  class_acc: 0.3750 (0.3473)  loss_scale: 65536.0000 (57154.0677)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.2056  max mem: 15572
Epoch: [39]  [2640/2809]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 3.8208 (3.7200)  class_acc: 0.3333 (0.3469)  loss_scale: 65536.0000 (57185.8054)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1371  max mem: 15572
Epoch: [39]  [2650/2809]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 3.8208 (3.7197)  class_acc: 0.3333 (0.3471)  loss_scale: 65536.0000 (57217.3037)  weight_decay: 0.0500 (0.0500)  time: 0.5777  data: 0.1349  max mem: 15572
Epoch: [39]  [2660/2809]  eta: 0:01:28  lr: 0.000000  min_lr: 0.000000  loss: 3.7232 (3.7197)  class_acc: 0.3333 (0.3469)  loss_scale: 65536.0000 (57248.5652)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.1943  max mem: 15572
Epoch: [39]  [2670/2809]  eta: 0:01:22  lr: 0.000000  min_lr: 0.000000  loss: 3.7955 (3.7199)  class_acc: 0.3333 (0.3469)  loss_scale: 65536.0000 (57279.5927)  weight_decay: 0.0500 (0.0500)  time: 0.5918  data: 0.1562  max mem: 15572
Epoch: [39]  [2680/2809]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 3.8342 (3.7201)  class_acc: 0.2500 (0.3467)  loss_scale: 65536.0000 (57310.3887)  weight_decay: 0.0500 (0.0500)  time: 0.5628  data: 0.1253  max mem: 15572
Epoch: [39]  [2690/2809]  eta: 0:01:10  lr: 0.000000  min_lr: 0.000000  loss: 3.8801 (3.7211)  class_acc: 0.2500 (0.3466)  loss_scale: 65536.0000 (57340.9558)  weight_decay: 0.0500 (0.0500)  time: 0.5513  data: 0.1171  max mem: 15572
Epoch: [39]  [2700/2809]  eta: 0:01:04  lr: 0.000000  min_lr: 0.000000  loss: 3.7656 (3.7208)  class_acc: 0.3750 (0.3467)  loss_scale: 65536.0000 (57371.2966)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.1818  max mem: 15572
[2025-01-16 09:16:26,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-01-16 09:16:26,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-01-16 09:16:27,218] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 112255
[2025-01-16 09:16:27,219] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-01-16 09:16:27,219] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [39]  [2710/2809]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 3.6945 (3.7209)  class_acc: 0.3750 (0.3468)  loss_scale: 65536.0000 (57449.7617)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.2219  max mem: 15572
Epoch: [39]  [2720/2809]  eta: 0:00:52  lr: 0.000000  min_lr: 0.000000  loss: 3.8723 (3.7217)  class_acc: 0.3333 (0.3465)  loss_scale: 65536.0000 (57479.4796)  weight_decay: 0.0500 (0.0500)  time: 0.5411  data: 0.0812  max mem: 15572
Epoch: [39]  [2730/2809]  eta: 0:00:46  lr: 0.000000  min_lr: 0.000000  loss: 3.9007 (3.7217)  class_acc: 0.2917 (0.3466)  loss_scale: 65536.0000 (57508.9799)  weight_decay: 0.0500 (0.0500)  time: 0.5222  data: 0.0875  max mem: 15572
Epoch: [39]  [2740/2809]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 3.6524 (3.7209)  class_acc: 0.3333 (0.3466)  loss_scale: 65536.0000 (57538.2649)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.2118  max mem: 15572
Epoch: [39]  [2750/2809]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 3.5818 (3.7203)  class_acc: 0.3750 (0.3468)  loss_scale: 65536.0000 (57567.3370)  weight_decay: 0.0500 (0.0500)  time: 0.6682  data: 0.2229  max mem: 15572
Epoch: [39]  [2760/2809]  eta: 0:00:29  lr: 0.000000  min_lr: 0.000000  loss: 3.8261 (3.7205)  class_acc: 0.3333 (0.3467)  loss_scale: 65536.0000 (57596.1985)  weight_decay: 0.0500 (0.0500)  time: 0.5723  data: 0.1509  max mem: 15572
Epoch: [39]  [2770/2809]  eta: 0:00:23  lr: 0.000000  min_lr: 0.000000  loss: 3.8814 (3.7210)  class_acc: 0.2917 (0.3465)  loss_scale: 65536.0000 (57624.8517)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.1672  max mem: 15572
Epoch: [39]  [2780/2809]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 3.7594 (3.7209)  class_acc: 0.3333 (0.3465)  loss_scale: 65536.0000 (57653.2988)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.1618  max mem: 15572
Epoch: [39]  [2790/2809]  eta: 0:00:11  lr: 0.000000  min_lr: 0.000000  loss: 3.6231 (3.7210)  class_acc: 0.3333 (0.3463)  loss_scale: 65536.0000 (57681.5421)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.1315  max mem: 15572
Epoch: [39]  [2800/2809]  eta: 0:00:05  lr: 0.000000  min_lr: 0.000000  loss: 3.6611 (3.7212)  class_acc: 0.2917 (0.3462)  loss_scale: 65536.0000 (57709.5837)  weight_decay: 0.0500 (0.0500)  time: 0.5558  data: 0.1147  max mem: 15572
Epoch: [39]  [2808/2809]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 3.8077 (3.7214)  class_acc: 0.2917 (0.3461)  loss_scale: 65536.0000 (57731.8733)  weight_decay: 0.0500 (0.0500)  time: 0.4305  data: 0.0306  max mem: 15572
Epoch: [39] Total time: 0:27:45 (0.5928 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 3.8077 (3.7214)  class_acc: 0.2917 (0.3461)  loss_scale: 65536.0000 (57731.8733)  weight_decay: 0.0500 (0.0500)
[2025-01-16 09:17:27,087] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2025-01-16 09:17:27,092] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-39/mp_rank_00_model_states.pt
[2025-01-16 09:17:27,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-39/mp_rank_00_model_states.pt...
[2025-01-16 09:17:27,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-39/mp_rank_00_model_states.pt.
[2025-01-16 09:17:27,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [  0/272]  eta: 0:23:20  loss: 0.3745 (0.3745)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.1488  data: 4.9709  max mem: 15572
Val:  [ 10/272]  eta: 0:03:15  loss: 2.2450 (2.2069)  acc1: 44.4444 (45.4545)  acc5: 77.7778 (74.7475)  time: 0.7460  data: 0.5543  max mem: 15572
Val:  [ 20/272]  eta: 0:02:14  loss: 2.2018 (2.2363)  acc1: 44.4444 (47.8836)  acc5: 77.7778 (75.6614)  time: 0.3015  data: 0.1187  max mem: 15572
Val:  [ 30/272]  eta: 0:01:54  loss: 2.2018 (2.3240)  acc1: 50.0000 (44.4444)  acc5: 77.7778 (74.7312)  time: 0.3202  data: 0.1327  max mem: 15572
Val:  [ 40/272]  eta: 0:01:39  loss: 2.5611 (2.3800)  acc1: 33.3333 (42.1409)  acc5: 77.7778 (74.5257)  time: 0.3169  data: 0.1225  max mem: 15572
Val:  [ 50/272]  eta: 0:01:30  loss: 2.4761 (2.3121)  acc1: 38.8889 (43.8998)  acc5: 77.7778 (75.9259)  time: 0.3115  data: 0.1241  max mem: 15572
Val:  [ 60/272]  eta: 0:01:20  loss: 1.4752 (2.2101)  acc1: 61.1111 (46.6302)  acc5: 88.8889 (76.8670)  time: 0.2759  data: 0.0824  max mem: 15572
Val:  [ 70/272]  eta: 0:01:15  loss: 1.5280 (2.1348)  acc1: 66.6667 (49.1393)  acc5: 88.8889 (78.0125)  time: 0.2857  data: 0.0893  max mem: 15572
Val:  [ 80/272]  eta: 0:01:09  loss: 1.8205 (2.1495)  acc1: 61.1111 (49.1084)  acc5: 83.3333 (77.8464)  time: 0.3166  data: 0.1294  max mem: 15572
Val:  [ 90/272]  eta: 0:01:05  loss: 2.0432 (2.1534)  acc1: 50.0000 (49.7558)  acc5: 77.7778 (78.2051)  time: 0.3032  data: 0.1260  max mem: 15572
Val:  [100/272]  eta: 0:01:01  loss: 2.0672 (2.1817)  acc1: 50.0000 (48.8449)  acc5: 83.3333 (77.8878)  time: 0.3273  data: 0.1489  max mem: 15572
Val:  [110/272]  eta: 0:00:56  loss: 2.3786 (2.2556)  acc1: 22.2222 (46.9469)  acc5: 77.7778 (76.8769)  time: 0.3193  data: 0.1309  max mem: 15572
Val:  [120/272]  eta: 0:00:53  loss: 2.8689 (2.2924)  acc1: 22.2222 (46.2810)  acc5: 66.6667 (76.2167)  time: 0.3280  data: 0.1263  max mem: 15572
Val:  [130/272]  eta: 0:00:49  loss: 2.0706 (2.2575)  acc1: 44.4444 (47.0738)  acc5: 77.7778 (76.8872)  time: 0.3385  data: 0.1288  max mem: 15572
Val:  [140/272]  eta: 0:00:45  loss: 1.6402 (2.2530)  acc1: 55.5556 (47.4389)  acc5: 88.8889 (76.6745)  time: 0.3178  data: 0.1170  max mem: 15572
Val:  [150/272]  eta: 0:00:41  loss: 2.2266 (2.2566)  acc1: 33.3333 (46.7991)  acc5: 83.3333 (77.0052)  time: 0.3082  data: 0.1177  max mem: 15572
Val:  [160/272]  eta: 0:00:38  loss: 2.2266 (2.2472)  acc1: 44.4444 (47.3430)  acc5: 77.7778 (77.1567)  time: 0.3121  data: 0.1224  max mem: 15572
Val:  [170/272]  eta: 0:00:34  loss: 2.4041 (2.2677)  acc1: 44.4444 (46.7836)  acc5: 72.2222 (76.7057)  time: 0.3209  data: 0.1318  max mem: 15572
Val:  [180/272]  eta: 0:00:31  loss: 2.2753 (2.2588)  acc1: 38.8889 (46.6544)  acc5: 72.2222 (77.0718)  time: 0.3406  data: 0.1509  max mem: 15572
Val:  [190/272]  eta: 0:00:27  loss: 2.2753 (2.3120)  acc1: 38.8889 (45.3170)  acc5: 77.7778 (75.5963)  time: 0.3240  data: 0.1353  max mem: 15572
Val:  [200/272]  eta: 0:00:24  loss: 2.5135 (2.3203)  acc1: 38.8889 (45.1078)  acc5: 72.2222 (75.4561)  time: 0.3019  data: 0.1055  max mem: 15572
Val:  [210/272]  eta: 0:00:20  loss: 2.1405 (2.3230)  acc1: 44.4444 (45.2080)  acc5: 77.7778 (75.4344)  time: 0.3084  data: 0.1134  max mem: 15572
Val:  [220/272]  eta: 0:00:17  loss: 2.1450 (2.3121)  acc1: 44.4444 (45.3494)  acc5: 77.7778 (75.5656)  time: 0.3142  data: 0.1303  max mem: 15572
Val:  [230/272]  eta: 0:00:14  loss: 1.7191 (2.2844)  acc1: 61.1111 (46.2963)  acc5: 83.3333 (75.9019)  time: 0.3403  data: 0.1500  max mem: 15572
Val:  [240/272]  eta: 0:00:10  loss: 1.5858 (2.2698)  acc1: 66.6667 (46.6113)  acc5: 83.3333 (76.1872)  time: 0.3289  data: 0.1369  max mem: 15572
Val:  [250/272]  eta: 0:00:07  loss: 2.2324 (2.2802)  acc1: 44.4444 (45.9274)  acc5: 83.3333 (76.1842)  time: 0.3053  data: 0.1204  max mem: 15572
Val:  [260/272]  eta: 0:00:04  loss: 1.2130 (2.2243)  acc1: 72.2222 (47.5522)  acc5: 88.8889 (76.8838)  time: 0.3345  data: 0.1530  max mem: 15572
Val:  [270/272]  eta: 0:00:00  loss: 1.2984 (2.2181)  acc1: 72.2222 (47.7040)  acc5: 88.8889 (77.0808)  time: 0.2767  data: 0.1133  max mem: 15572
Val:  [271/272]  eta: 0:00:00  loss: 1.2984 (2.2225)  acc1: 72.2222 (47.6756)  acc5: 88.8889 (77.0633)  time: 0.2680  data: 0.1129  max mem: 15572
Val: Total time: 0:01:29 (0.3307 s / it)
* Acc@1 47.676 Acc@5 77.063 loss 2.222
Accuracy of the network on the 4883 val videos: 47.7%
[2025-01-16 09:18:57,508] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-01-16 09:18:57,512] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt
[2025-01-16 09:18:57,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt...
[2025-01-16 09:19:00,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/maggie/VideoMAE_checkpoints_curriculum/ssv2_small_20data_mask_curriculum_threshold_70/checkpoint-best/mp_rank_00_model_states.pt.
[2025-01-16 09:19:00,575] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 47.68%
Test:  [   0/2442]  eta: 2:34:38  loss: 0.3409 (0.3409)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 3.7994  data: 3.4785  max mem: 15572
Test:  [  10/2442]  eta: 0:22:52  loss: 2.6504 (2.5556)  acc1: 25.0000 (33.3333)  acc5: 75.0000 (68.1818)  time: 0.5643  data: 0.4097  max mem: 15572
Test:  [  20/2442]  eta: 0:17:00  loss: 2.4102 (2.2644)  acc1: 41.6667 (44.8413)  acc5: 75.0000 (72.6190)  time: 0.2523  data: 0.1037  max mem: 15572
Test:  [  30/2442]  eta: 0:14:04  loss: 2.2528 (2.3245)  acc1: 50.0000 (44.3548)  acc5: 75.0000 (73.3871)  time: 0.2324  data: 0.0861  max mem: 15572
Test:  [  40/2442]  eta: 0:12:26  loss: 2.2889 (2.3270)  acc1: 33.3333 (42.8862)  acc5: 75.0000 (74.5935)  time: 0.1946  data: 0.0686  max mem: 15572
Test:  [  50/2442]  eta: 0:11:15  loss: 2.8305 (2.5068)  acc1: 25.0000 (37.5817)  acc5: 66.6667 (71.0784)  time: 0.1771  data: 0.0530  max mem: 15572
Test:  [  60/2442]  eta: 0:10:34  loss: 2.7434 (2.5213)  acc1: 25.0000 (36.8852)  acc5: 66.6667 (71.3115)  time: 0.1752  data: 0.0566  max mem: 15572
Test:  [  70/2442]  eta: 0:09:42  loss: 2.6843 (2.5193)  acc1: 33.3333 (36.7371)  acc5: 75.0000 (71.4789)  time: 0.1526  data: 0.0387  max mem: 15572
Test:  [  80/2442]  eta: 0:09:04  loss: 1.8161 (2.3853)  acc1: 58.3333 (40.8436)  acc5: 83.3333 (73.7654)  time: 0.1211  data: 0.0004  max mem: 15572
Test:  [  90/2442]  eta: 0:08:50  loss: 1.5822 (2.3652)  acc1: 58.3333 (41.4835)  acc5: 91.6667 (73.8095)  time: 0.1538  data: 0.0217  max mem: 15572
Test:  [ 100/2442]  eta: 0:08:47  loss: 2.1847 (2.3523)  acc1: 50.0000 (42.0792)  acc5: 75.0000 (73.7624)  time: 0.2036  data: 0.0610  max mem: 15572
Test:  [ 110/2442]  eta: 0:08:58  loss: 1.7959 (2.2961)  acc1: 50.0000 (43.8438)  acc5: 75.0000 (74.1742)  time: 0.2552  data: 0.1131  max mem: 15572
Test:  [ 120/2442]  eta: 0:09:00  loss: 2.1344 (2.3225)  acc1: 50.0000 (43.1818)  acc5: 75.0000 (73.9669)  time: 0.2718  data: 0.1379  max mem: 15572
Test:  [ 130/2442]  eta: 0:09:07  loss: 2.3220 (2.3134)  acc1: 41.6667 (43.7023)  acc5: 83.3333 (74.5547)  time: 0.2687  data: 0.1386  max mem: 15572
Test:  [ 140/2442]  eta: 0:09:19  loss: 2.2418 (2.3333)  acc1: 41.6667 (43.0851)  acc5: 83.3333 (74.4681)  time: 0.3061  data: 0.1634  max mem: 15572
Test:  [ 150/2442]  eta: 0:09:29  loss: 2.3875 (2.3418)  acc1: 41.6667 (43.3223)  acc5: 75.0000 (74.4481)  time: 0.3264  data: 0.1805  max mem: 15572
Test:  [ 160/2442]  eta: 0:09:36  loss: 2.8122 (2.4157)  acc1: 25.0000 (41.4596)  acc5: 66.6667 (72.6708)  time: 0.3196  data: 0.1703  max mem: 15572
Test:  [ 170/2442]  eta: 0:09:37  loss: 3.0216 (2.4347)  acc1: 16.6667 (40.9844)  acc5: 66.6667 (72.6608)  time: 0.2972  data: 0.1499  max mem: 15572
Test:  [ 180/2442]  eta: 0:09:37  loss: 2.4574 (2.4479)  acc1: 33.3333 (40.8379)  acc5: 75.0000 (72.2836)  time: 0.2744  data: 0.1333  max mem: 15572
Test:  [ 190/2442]  eta: 0:09:38  loss: 1.9810 (2.4274)  acc1: 50.0000 (41.2740)  acc5: 75.0000 (72.6004)  time: 0.2795  data: 0.1374  max mem: 15572
Test:  [ 200/2442]  eta: 0:09:36  loss: 1.4615 (2.3781)  acc1: 66.6667 (42.7446)  acc5: 91.6667 (73.3831)  time: 0.2742  data: 0.1283  max mem: 15572
Test:  [ 210/2442]  eta: 0:09:40  loss: 1.9751 (2.4042)  acc1: 50.0000 (42.1801)  acc5: 83.3333 (72.9068)  time: 0.2903  data: 0.1423  max mem: 15572
Test:  [ 220/2442]  eta: 0:09:40  loss: 2.7149 (2.4129)  acc1: 25.0000 (41.6667)  acc5: 75.0000 (73.0015)  time: 0.3043  data: 0.1594  max mem: 15572
Test:  [ 230/2442]  eta: 0:09:40  loss: 2.5095 (2.4031)  acc1: 41.6667 (41.9192)  acc5: 83.3333 (73.3766)  time: 0.2884  data: 0.1476  max mem: 15572
Test:  [ 240/2442]  eta: 0:09:47  loss: 2.4697 (2.4007)  acc1: 41.6667 (42.0470)  acc5: 75.0000 (73.4094)  time: 0.3283  data: 0.1914  max mem: 15572
Test:  [ 250/2442]  eta: 0:09:45  loss: 2.4697 (2.4018)  acc1: 50.0000 (42.2643)  acc5: 75.0000 (73.4064)  time: 0.3196  data: 0.1699  max mem: 15572
Test:  [ 260/2442]  eta: 0:09:40  loss: 2.3711 (2.4023)  acc1: 41.6667 (42.0498)  acc5: 66.6667 (73.4994)  time: 0.2547  data: 0.0968  max mem: 15572
Test:  [ 270/2442]  eta: 0:09:43  loss: 2.4125 (2.4063)  acc1: 41.6667 (41.9434)  acc5: 75.0000 (73.5855)  time: 0.2905  data: 0.1371  max mem: 15572
Test:  [ 280/2442]  eta: 0:09:38  loss: 2.5396 (2.4387)  acc1: 25.0000 (41.1329)  acc5: 75.0000 (72.8648)  time: 0.2923  data: 0.1517  max mem: 15572
Test:  [ 290/2442]  eta: 0:09:39  loss: 2.9572 (2.4503)  acc1: 25.0000 (40.6071)  acc5: 75.0000 (72.6804)  time: 0.2732  data: 0.1402  max mem: 15572
Test:  [ 300/2442]  eta: 0:09:37  loss: 2.4438 (2.4539)  acc1: 41.6667 (40.7254)  acc5: 75.0000 (72.6190)  time: 0.2960  data: 0.1565  max mem: 15572
Test:  [ 310/2442]  eta: 0:09:37  loss: 2.0580 (2.4559)  acc1: 50.0000 (41.0236)  acc5: 75.0000 (72.6688)  time: 0.2949  data: 0.1498  max mem: 15572
Test:  [ 320/2442]  eta: 0:09:35  loss: 2.1862 (2.4620)  acc1: 41.6667 (41.0436)  acc5: 75.0000 (72.5078)  time: 0.2960  data: 0.1511  max mem: 15572
Test:  [ 330/2442]  eta: 0:09:35  loss: 2.3553 (2.4498)  acc1: 41.6667 (41.3394)  acc5: 75.0000 (72.6838)  time: 0.2985  data: 0.1506  max mem: 15572
Test:  [ 340/2442]  eta: 0:09:35  loss: 2.0278 (2.4405)  acc1: 58.3333 (41.6422)  acc5: 83.3333 (72.9472)  time: 0.3131  data: 0.1715  max mem: 15572
Test:  [ 350/2442]  eta: 0:09:35  loss: 1.2793 (2.4088)  acc1: 66.6667 (42.4264)  acc5: 91.6667 (73.5518)  time: 0.3171  data: 0.1792  max mem: 15572
Test:  [ 360/2442]  eta: 0:09:29  loss: 1.7078 (2.3981)  acc1: 58.3333 (42.6131)  acc5: 91.6667 (73.7996)  time: 0.2725  data: 0.1249  max mem: 15572
Test:  [ 370/2442]  eta: 0:09:26  loss: 2.3856 (2.4074)  acc1: 33.3333 (42.1384)  acc5: 75.0000 (73.6523)  time: 0.2448  data: 0.1019  max mem: 15572
Test:  [ 380/2442]  eta: 0:09:25  loss: 2.7487 (2.4030)  acc1: 25.0000 (42.3010)  acc5: 75.0000 (73.6220)  time: 0.2819  data: 0.1390  max mem: 15572
Test:  [ 390/2442]  eta: 0:09:22  loss: 0.8942 (2.3586)  acc1: 83.3333 (43.5635)  acc5: 91.6667 (74.1688)  time: 0.2875  data: 0.1432  max mem: 15572
Test:  [ 400/2442]  eta: 0:09:20  loss: 1.3222 (2.3521)  acc1: 66.6667 (43.7656)  acc5: 91.6667 (74.3142)  time: 0.2809  data: 0.1420  max mem: 15572
Test:  [ 410/2442]  eta: 0:09:20  loss: 1.9346 (2.3470)  acc1: 50.0000 (43.7753)  acc5: 83.3333 (74.3917)  time: 0.3109  data: 0.1697  max mem: 15572
Test:  [ 420/2442]  eta: 0:09:17  loss: 2.4054 (2.3513)  acc1: 33.3333 (43.4877)  acc5: 83.3333 (74.4260)  time: 0.3042  data: 0.1671  max mem: 15572
Test:  [ 430/2442]  eta: 0:09:15  loss: 2.3652 (2.3388)  acc1: 50.0000 (43.9675)  acc5: 75.0000 (74.5360)  time: 0.2836  data: 0.1586  max mem: 15572
Test:  [ 440/2442]  eta: 0:09:09  loss: 2.0748 (2.3393)  acc1: 58.3333 (43.8209)  acc5: 75.0000 (74.6221)  time: 0.2401  data: 0.1186  max mem: 15572
Test:  [ 450/2442]  eta: 0:09:02  loss: 2.3265 (2.3381)  acc1: 41.6667 (43.9024)  acc5: 75.0000 (74.7044)  time: 0.1893  data: 0.0685  max mem: 15572
Test:  [ 460/2442]  eta: 0:08:52  loss: 2.7275 (2.3516)  acc1: 25.0000 (43.3478)  acc5: 66.6667 (74.4396)  time: 0.1497  data: 0.0357  max mem: 15572
Test:  [ 470/2442]  eta: 0:08:43  loss: 2.5417 (2.3515)  acc1: 25.0000 (43.2236)  acc5: 66.6667 (74.5046)  time: 0.1153  data: 0.0004  max mem: 15572
Test:  [ 480/2442]  eta: 0:08:36  loss: 2.0887 (2.3409)  acc1: 41.6667 (43.4165)  acc5: 83.3333 (74.7748)  time: 0.1308  data: 0.0110  max mem: 15572
Test:  [ 490/2442]  eta: 0:08:31  loss: 1.6727 (2.3355)  acc1: 58.3333 (43.6015)  acc5: 91.6667 (74.8473)  time: 0.1747  data: 0.0425  max mem: 15572
Test:  [ 500/2442]  eta: 0:08:29  loss: 1.4571 (2.3175)  acc1: 75.0000 (44.1118)  acc5: 91.6667 (75.0832)  time: 0.2431  data: 0.1045  max mem: 15572
Test:  [ 510/2442]  eta: 0:08:28  loss: 1.4759 (2.3032)  acc1: 75.0000 (44.5858)  acc5: 91.6667 (75.2935)  time: 0.2975  data: 0.1620  max mem: 15572
Test:  [ 520/2442]  eta: 0:08:27  loss: 1.5042 (2.2921)  acc1: 66.6667 (44.8656)  acc5: 83.3333 (75.3359)  time: 0.3132  data: 0.1651  max mem: 15572
Test:  [ 530/2442]  eta: 0:08:26  loss: 2.2123 (2.2984)  acc1: 41.6667 (44.6485)  acc5: 75.0000 (75.2982)  time: 0.3014  data: 0.1406  max mem: 15572
Test:  [ 540/2442]  eta: 0:08:23  loss: 2.1341 (2.2905)  acc1: 41.6667 (44.9630)  acc5: 83.3333 (75.5237)  time: 0.2825  data: 0.1252  max mem: 15572
Test:  [ 550/2442]  eta: 0:08:22  loss: 1.9278 (2.2936)  acc1: 50.0000 (44.8578)  acc5: 83.3333 (75.5293)  time: 0.2846  data: 0.1340  max mem: 15572
Test:  [ 560/2442]  eta: 0:08:20  loss: 2.1336 (2.2997)  acc1: 33.3333 (44.7267)  acc5: 75.0000 (75.4753)  time: 0.2937  data: 0.1491  max mem: 15572
Test:  [ 570/2442]  eta: 0:08:18  loss: 2.8062 (2.3145)  acc1: 16.6667 (44.3082)  acc5: 66.6667 (75.1897)  time: 0.2946  data: 0.1624  max mem: 15572
Test:  [ 580/2442]  eta: 0:08:18  loss: 2.7115 (2.3125)  acc1: 33.3333 (44.4923)  acc5: 66.6667 (75.2151)  time: 0.3239  data: 0.1904  max mem: 15572
Test:  [ 590/2442]  eta: 0:08:15  loss: 2.2386 (2.3164)  acc1: 50.0000 (44.4162)  acc5: 75.0000 (75.1692)  time: 0.2974  data: 0.1644  max mem: 15572
Test:  [ 600/2442]  eta: 0:08:17  loss: 2.0483 (2.3121)  acc1: 50.0000 (44.5785)  acc5: 83.3333 (75.2496)  time: 0.3260  data: 0.1917  max mem: 15572
Test:  [ 610/2442]  eta: 0:08:15  loss: 1.5973 (2.3062)  acc1: 58.3333 (44.7490)  acc5: 83.3333 (75.2591)  time: 0.3557  data: 0.2080  max mem: 15572
Test:  [ 620/2442]  eta: 0:08:14  loss: 1.8933 (2.3057)  acc1: 50.0000 (44.7531)  acc5: 83.3333 (75.2281)  time: 0.3248  data: 0.1773  max mem: 15572
Test:  [ 630/2442]  eta: 0:08:11  loss: 2.1550 (2.3055)  acc1: 41.6667 (44.6778)  acc5: 83.3333 (75.3302)  time: 0.2893  data: 0.1489  max mem: 15572
Test:  [ 640/2442]  eta: 0:08:10  loss: 2.0825 (2.3006)  acc1: 50.0000 (44.8778)  acc5: 83.3333 (75.4550)  time: 0.2854  data: 0.1473  max mem: 15572
Test:  [ 650/2442]  eta: 0:08:08  loss: 2.0393 (2.2987)  acc1: 50.0000 (45.0077)  acc5: 75.0000 (75.4352)  time: 0.3134  data: 0.1745  max mem: 15572
Test:  [ 660/2442]  eta: 0:08:06  loss: 2.1963 (2.2997)  acc1: 50.0000 (44.9697)  acc5: 75.0000 (75.4160)  time: 0.2944  data: 0.1401  max mem: 15572
Test:  [ 670/2442]  eta: 0:08:05  loss: 2.1736 (2.2971)  acc1: 41.6667 (44.9702)  acc5: 75.0000 (75.4719)  time: 0.3171  data: 0.1569  max mem: 15572
Test:  [ 680/2442]  eta: 0:08:02  loss: 2.1063 (2.2993)  acc1: 33.3333 (44.8850)  acc5: 83.3333 (75.4405)  time: 0.3132  data: 0.1654  max mem: 15572
Test:  [ 690/2442]  eta: 0:08:00  loss: 2.5405 (2.3155)  acc1: 33.3333 (44.5248)  acc5: 66.6667 (75.0482)  time: 0.2850  data: 0.1318  max mem: 15572
Test:  [ 700/2442]  eta: 0:07:57  loss: 2.5405 (2.3152)  acc1: 25.0000 (44.4246)  acc5: 66.6667 (75.1308)  time: 0.2709  data: 0.1204  max mem: 15572
Test:  [ 710/2442]  eta: 0:07:54  loss: 1.9854 (2.3209)  acc1: 50.0000 (44.4210)  acc5: 75.0000 (74.9414)  time: 0.2774  data: 0.1386  max mem: 15572
Test:  [ 720/2442]  eta: 0:07:53  loss: 1.9854 (2.3207)  acc1: 58.3333 (44.5446)  acc5: 83.3333 (74.9538)  time: 0.3123  data: 0.1733  max mem: 15572
Test:  [ 730/2442]  eta: 0:07:51  loss: 2.0821 (2.3216)  acc1: 41.6667 (44.4368)  acc5: 83.3333 (74.9544)  time: 0.3245  data: 0.1798  max mem: 15572
Test:  [ 740/2442]  eta: 0:07:49  loss: 2.1422 (2.3176)  acc1: 41.6667 (44.5569)  acc5: 75.0000 (75.0225)  time: 0.3066  data: 0.1569  max mem: 15572
Test:  [ 750/2442]  eta: 0:07:47  loss: 1.3438 (2.3040)  acc1: 66.6667 (44.9845)  acc5: 83.3333 (75.2330)  time: 0.3125  data: 0.1603  max mem: 15572
Test:  [ 760/2442]  eta: 0:07:44  loss: 1.2520 (2.2938)  acc1: 58.3333 (45.1489)  acc5: 91.6667 (75.4380)  time: 0.2859  data: 0.1368  max mem: 15572
Test:  [ 770/2442]  eta: 0:07:42  loss: 1.9076 (2.2925)  acc1: 50.0000 (45.0605)  acc5: 91.6667 (75.5188)  time: 0.2699  data: 0.1240  max mem: 15572
Test:  [ 780/2442]  eta: 0:07:39  loss: 2.2327 (2.2978)  acc1: 16.6667 (44.8357)  acc5: 75.0000 (75.4375)  time: 0.2918  data: 0.1477  max mem: 15572
Test:  [ 790/2442]  eta: 0:07:36  loss: 1.9156 (2.2858)  acc1: 50.0000 (45.1749)  acc5: 91.6667 (75.6321)  time: 0.2837  data: 0.1426  max mem: 15572
Test:  [ 800/2442]  eta: 0:07:33  loss: 0.8978 (2.2715)  acc1: 83.3333 (45.5576)  acc5: 91.6667 (75.8115)  time: 0.2757  data: 0.1392  max mem: 15572
Test:  [ 810/2442]  eta: 0:07:31  loss: 1.2284 (2.2643)  acc1: 75.0000 (45.7768)  acc5: 91.6667 (75.9556)  time: 0.2805  data: 0.1503  max mem: 15572
Test:  [ 820/2442]  eta: 0:07:29  loss: 1.8062 (2.2673)  acc1: 50.0000 (45.6659)  acc5: 83.3333 (75.8526)  time: 0.2971  data: 0.1603  max mem: 15572
Test:  [ 830/2442]  eta: 0:07:26  loss: 2.4714 (2.2661)  acc1: 41.6667 (45.6980)  acc5: 75.0000 (75.8825)  time: 0.3005  data: 0.1598  max mem: 15572
Test:  [ 840/2442]  eta: 0:07:24  loss: 2.1103 (2.2634)  acc1: 58.3333 (45.8878)  acc5: 83.3333 (75.9215)  time: 0.2932  data: 0.1519  max mem: 15572
Test:  [ 850/2442]  eta: 0:07:21  loss: 2.0332 (2.2660)  acc1: 58.3333 (45.7991)  acc5: 75.0000 (75.8813)  time: 0.2889  data: 0.1428  max mem: 15572
Test:  [ 860/2442]  eta: 0:07:19  loss: 2.7180 (2.2726)  acc1: 25.0000 (45.5672)  acc5: 66.6667 (75.7743)  time: 0.2974  data: 0.1540  max mem: 15572
Test:  [ 870/2442]  eta: 0:07:16  loss: 2.9970 (2.2811)  acc1: 25.0000 (45.3215)  acc5: 66.6667 (75.5932)  time: 0.2898  data: 0.1449  max mem: 15572
Test:  [ 880/2442]  eta: 0:07:13  loss: 2.6604 (2.2835)  acc1: 25.0000 (45.2043)  acc5: 75.0000 (75.6338)  time: 0.2649  data: 0.1151  max mem: 15572
Test:  [ 890/2442]  eta: 0:07:11  loss: 2.1843 (2.2769)  acc1: 41.6667 (45.3610)  acc5: 83.3333 (75.8043)  time: 0.2765  data: 0.1263  max mem: 15572
Test:  [ 900/2442]  eta: 0:07:08  loss: 1.6335 (2.2749)  acc1: 66.6667 (45.4218)  acc5: 83.3333 (75.7584)  time: 0.2814  data: 0.1255  max mem: 15572
Test:  [ 910/2442]  eta: 0:07:05  loss: 1.6700 (2.2696)  acc1: 66.6667 (45.6275)  acc5: 83.3333 (75.8141)  time: 0.2679  data: 0.1095  max mem: 15572
Test:  [ 920/2442]  eta: 0:07:02  loss: 1.9416 (2.2656)  acc1: 58.3333 (45.7293)  acc5: 83.3333 (75.8686)  time: 0.2688  data: 0.1167  max mem: 15572
Test:  [ 930/2442]  eta: 0:06:59  loss: 2.0864 (2.2647)  acc1: 58.3333 (45.7931)  acc5: 75.0000 (75.8861)  time: 0.2766  data: 0.1315  max mem: 15572
Test:  [ 940/2442]  eta: 0:06:56  loss: 2.2669 (2.2668)  acc1: 41.6667 (45.7226)  acc5: 75.0000 (75.8856)  time: 0.2818  data: 0.1409  max mem: 15572
Test:  [ 950/2442]  eta: 0:06:54  loss: 2.2299 (2.2684)  acc1: 41.6667 (45.7326)  acc5: 75.0000 (75.8763)  time: 0.2883  data: 0.1477  max mem: 15572
Test:  [ 960/2442]  eta: 0:06:51  loss: 2.2600 (2.2722)  acc1: 33.3333 (45.6122)  acc5: 75.0000 (75.8585)  time: 0.2900  data: 0.1499  max mem: 15572
Test:  [ 970/2442]  eta: 0:06:49  loss: 2.9131 (2.2807)  acc1: 25.0000 (45.3999)  acc5: 66.6667 (75.6694)  time: 0.3087  data: 0.1664  max mem: 15572
Test:  [ 980/2442]  eta: 0:06:46  loss: 2.9212 (2.2880)  acc1: 16.6667 (45.2005)  acc5: 66.6667 (75.5522)  time: 0.3053  data: 0.1615  max mem: 15572
Test:  [ 990/2442]  eta: 0:06:44  loss: 2.8275 (2.2930)  acc1: 33.3333 (45.1312)  acc5: 66.6667 (75.4205)  time: 0.2894  data: 0.1463  max mem: 15572
Test:  [1000/2442]  eta: 0:06:41  loss: 2.2057 (2.2925)  acc1: 41.6667 (45.1049)  acc5: 75.0000 (75.4579)  time: 0.2907  data: 0.1509  max mem: 15572
Test:  [1010/2442]  eta: 0:06:39  loss: 1.8866 (2.2912)  acc1: 50.0000 (45.1698)  acc5: 83.3333 (75.4616)  time: 0.3044  data: 0.1690  max mem: 15572
Test:  [1020/2442]  eta: 0:06:36  loss: 2.1727 (2.2946)  acc1: 33.3333 (45.1110)  acc5: 75.0000 (75.3836)  time: 0.2931  data: 0.1504  max mem: 15572
Test:  [1030/2442]  eta: 0:06:34  loss: 2.3937 (2.2950)  acc1: 41.6667 (45.0938)  acc5: 66.6667 (75.3718)  time: 0.2965  data: 0.1408  max mem: 15572
Test:  [1040/2442]  eta: 0:06:31  loss: 2.2313 (2.2952)  acc1: 41.6667 (45.0849)  acc5: 83.3333 (75.4403)  time: 0.2866  data: 0.1407  max mem: 15572
Test:  [1050/2442]  eta: 0:06:28  loss: 2.3896 (2.2953)  acc1: 50.0000 (45.1316)  acc5: 75.0000 (75.4440)  time: 0.2641  data: 0.1333  max mem: 15572
Test:  [1060/2442]  eta: 0:06:26  loss: 2.4713 (2.2963)  acc1: 50.0000 (45.1225)  acc5: 75.0000 (75.4320)  time: 0.3048  data: 0.1757  max mem: 15572
Test:  [1070/2442]  eta: 0:06:24  loss: 2.5036 (2.3009)  acc1: 41.6667 (45.0047)  acc5: 66.6667 (75.3035)  time: 0.3230  data: 0.1796  max mem: 15572
Test:  [1080/2442]  eta: 0:06:20  loss: 2.6301 (2.2995)  acc1: 33.3333 (45.0200)  acc5: 66.6667 (75.3623)  time: 0.2806  data: 0.1264  max mem: 15572
Test:  [1090/2442]  eta: 0:06:17  loss: 2.3387 (2.3078)  acc1: 33.3333 (44.8365)  acc5: 75.0000 (75.1757)  time: 0.2539  data: 0.1022  max mem: 15572
Test:  [1100/2442]  eta: 0:06:15  loss: 2.9654 (2.3142)  acc1: 25.0000 (44.6337)  acc5: 66.6667 (75.0454)  time: 0.2982  data: 0.1518  max mem: 15572
Test:  [1110/2442]  eta: 0:06:12  loss: 2.5616 (2.3117)  acc1: 33.3333 (44.6295)  acc5: 75.0000 (75.1725)  time: 0.2926  data: 0.1532  max mem: 15572
Test:  [1120/2442]  eta: 0:06:10  loss: 1.8669 (2.3150)  acc1: 50.0000 (44.6551)  acc5: 91.6667 (75.0743)  time: 0.2847  data: 0.1374  max mem: 15572
Test:  [1130/2442]  eta: 0:06:07  loss: 2.2958 (2.3173)  acc1: 41.6667 (44.6434)  acc5: 75.0000 (75.0368)  time: 0.2971  data: 0.1572  max mem: 15572
Test:  [1140/2442]  eta: 0:06:05  loss: 2.4314 (2.3182)  acc1: 33.3333 (44.5735)  acc5: 75.0000 (75.0292)  time: 0.3113  data: 0.1764  max mem: 15572
Test:  [1150/2442]  eta: 0:06:02  loss: 2.2445 (2.3197)  acc1: 41.6667 (44.5627)  acc5: 75.0000 (74.9638)  time: 0.3298  data: 0.1824  max mem: 15572
Test:  [1160/2442]  eta: 0:06:00  loss: 1.6889 (2.3097)  acc1: 58.3333 (44.8751)  acc5: 75.0000 (75.1148)  time: 0.3212  data: 0.1709  max mem: 15572
Test:  [1170/2442]  eta: 0:05:57  loss: 1.6141 (2.3070)  acc1: 58.3333 (44.8833)  acc5: 91.6667 (75.1993)  time: 0.2952  data: 0.1413  max mem: 15572
Test:  [1180/2442]  eta: 0:05:54  loss: 2.1316 (2.3064)  acc1: 41.6667 (44.8702)  acc5: 83.3333 (75.2540)  time: 0.2767  data: 0.1222  max mem: 15572
Test:  [1190/2442]  eta: 0:05:52  loss: 2.4705 (2.3121)  acc1: 33.3333 (44.6683)  acc5: 75.0000 (75.1399)  time: 0.2899  data: 0.1376  max mem: 15572
Test:  [1200/2442]  eta: 0:05:49  loss: 1.6751 (2.2997)  acc1: 58.3333 (45.0319)  acc5: 83.3333 (75.3192)  time: 0.2886  data: 0.1369  max mem: 15572
Test:  [1210/2442]  eta: 0:05:46  loss: 0.9573 (2.2947)  acc1: 83.3333 (45.1899)  acc5: 100.0000 (75.3854)  time: 0.2894  data: 0.1447  max mem: 15572
Test:  [1220/2442]  eta: 0:05:44  loss: 1.8595 (2.2913)  acc1: 58.3333 (45.2839)  acc5: 91.6667 (75.4709)  time: 0.3115  data: 0.1667  max mem: 15572
Test:  [1230/2442]  eta: 0:05:41  loss: 2.5201 (2.2943)  acc1: 41.6667 (45.1801)  acc5: 75.0000 (75.3588)  time: 0.2948  data: 0.1477  max mem: 15572
Test:  [1240/2442]  eta: 0:05:39  loss: 2.5201 (2.2913)  acc1: 33.3333 (45.2726)  acc5: 66.6667 (75.3626)  time: 0.3042  data: 0.1421  max mem: 15572
Test:  [1250/2442]  eta: 0:05:35  loss: 2.1541 (2.2923)  acc1: 50.0000 (45.2771)  acc5: 75.0000 (75.3864)  time: 0.2799  data: 0.1243  max mem: 15572
Test:  [1260/2442]  eta: 0:05:33  loss: 2.5720 (2.2937)  acc1: 41.6667 (45.2154)  acc5: 75.0000 (75.3767)  time: 0.2710  data: 0.1341  max mem: 15572
Test:  [1270/2442]  eta: 0:05:30  loss: 2.9093 (2.3010)  acc1: 16.6667 (44.9908)  acc5: 66.6667 (75.2098)  time: 0.2993  data: 0.1509  max mem: 15572
Test:  [1280/2442]  eta: 0:05:27  loss: 2.8840 (2.3029)  acc1: 16.6667 (44.9128)  acc5: 58.3333 (75.1496)  time: 0.2774  data: 0.1308  max mem: 15572
Test:  [1290/2442]  eta: 0:05:24  loss: 2.4617 (2.3049)  acc1: 33.3333 (44.7909)  acc5: 66.6667 (75.1162)  time: 0.2889  data: 0.1469  max mem: 15572
Test:  [1300/2442]  eta: 0:05:22  loss: 1.9458 (2.2985)  acc1: 50.0000 (44.9526)  acc5: 75.0000 (75.2498)  time: 0.3091  data: 0.1605  max mem: 15572
Test:  [1310/2442]  eta: 0:05:19  loss: 1.6792 (2.2979)  acc1: 58.3333 (44.9720)  acc5: 91.6667 (75.2415)  time: 0.3084  data: 0.1588  max mem: 15572
Test:  [1320/2442]  eta: 0:05:17  loss: 2.0452 (2.2967)  acc1: 58.3333 (44.9975)  acc5: 75.0000 (75.2523)  time: 0.3022  data: 0.1586  max mem: 15572
Test:  [1330/2442]  eta: 0:05:14  loss: 1.9400 (2.2935)  acc1: 50.0000 (45.0914)  acc5: 75.0000 (75.2504)  time: 0.2996  data: 0.1547  max mem: 15572
Test:  [1340/2442]  eta: 0:05:11  loss: 2.4400 (2.2956)  acc1: 50.0000 (45.0597)  acc5: 75.0000 (75.2113)  time: 0.2900  data: 0.1345  max mem: 15572
Test:  [1350/2442]  eta: 0:05:08  loss: 2.4727 (2.2949)  acc1: 33.3333 (45.0839)  acc5: 75.0000 (75.2467)  time: 0.3022  data: 0.1485  max mem: 15572
Test:  [1360/2442]  eta: 0:05:06  loss: 2.1077 (2.2974)  acc1: 50.0000 (45.0404)  acc5: 83.3333 (75.2204)  time: 0.3330  data: 0.1843  max mem: 15572
Test:  [1370/2442]  eta: 0:05:03  loss: 2.2453 (2.2976)  acc1: 50.0000 (45.0827)  acc5: 75.0000 (75.2371)  time: 0.3011  data: 0.1572  max mem: 15572
Test:  [1380/2442]  eta: 0:05:00  loss: 2.7441 (2.3062)  acc1: 25.0000 (44.8226)  acc5: 66.6667 (75.0845)  time: 0.2796  data: 0.1464  max mem: 15572
Test:  [1390/2442]  eta: 0:04:57  loss: 3.1010 (2.3100)  acc1: 16.6667 (44.7460)  acc5: 66.6667 (75.0359)  time: 0.2871  data: 0.1461  max mem: 15572
Test:  [1400/2442]  eta: 0:04:55  loss: 2.6379 (2.3124)  acc1: 33.3333 (44.7240)  acc5: 66.6667 (74.9881)  time: 0.2904  data: 0.1419  max mem: 15572
Test:  [1410/2442]  eta: 0:04:52  loss: 2.1379 (2.3108)  acc1: 50.0000 (44.7673)  acc5: 75.0000 (75.0177)  time: 0.2845  data: 0.1407  max mem: 15572
Test:  [1420/2442]  eta: 0:04:49  loss: 1.5284 (2.3047)  acc1: 58.3333 (44.9507)  acc5: 91.6667 (75.1290)  time: 0.2619  data: 0.1205  max mem: 15572
Test:  [1430/2442]  eta: 0:04:46  loss: 2.1756 (2.3096)  acc1: 50.0000 (44.8579)  acc5: 83.3333 (75.0000)  time: 0.2933  data: 0.1500  max mem: 15572
Test:  [1440/2442]  eta: 0:04:43  loss: 2.6534 (2.3104)  acc1: 41.6667 (44.8126)  acc5: 75.0000 (75.0289)  time: 0.2904  data: 0.1443  max mem: 15572
Test:  [1450/2442]  eta: 0:04:41  loss: 2.4073 (2.3105)  acc1: 41.6667 (44.7967)  acc5: 75.0000 (75.0517)  time: 0.2931  data: 0.1404  max mem: 15572
Test:  [1460/2442]  eta: 0:04:38  loss: 2.3154 (2.3111)  acc1: 41.6667 (44.8038)  acc5: 75.0000 (75.0456)  time: 0.2864  data: 0.1355  max mem: 15572
Test:  [1470/2442]  eta: 0:04:35  loss: 2.3666 (2.3119)  acc1: 41.6667 (44.8108)  acc5: 75.0000 (75.0453)  time: 0.2807  data: 0.1417  max mem: 15572
Test:  [1480/2442]  eta: 0:04:32  loss: 2.4626 (2.3119)  acc1: 41.6667 (44.8177)  acc5: 75.0000 (75.0563)  time: 0.2780  data: 0.1384  max mem: 15572
Test:  [1490/2442]  eta: 0:04:30  loss: 2.5579 (2.3132)  acc1: 33.3333 (44.8021)  acc5: 75.0000 (75.0335)  time: 0.2916  data: 0.1385  max mem: 15572
Test:  [1500/2442]  eta: 0:04:26  loss: 2.6162 (2.3201)  acc1: 33.3333 (44.6314)  acc5: 75.0000 (74.9056)  time: 0.2840  data: 0.1269  max mem: 15572
Test:  [1510/2442]  eta: 0:04:23  loss: 2.6863 (2.3234)  acc1: 25.0000 (44.5070)  acc5: 75.0000 (74.8511)  time: 0.2433  data: 0.0984  max mem: 15572
Test:  [1520/2442]  eta: 0:04:20  loss: 2.2607 (2.3236)  acc1: 41.6667 (44.5321)  acc5: 75.0000 (74.8740)  time: 0.2677  data: 0.1345  max mem: 15572
Test:  [1530/2442]  eta: 0:04:18  loss: 2.0205 (2.3249)  acc1: 50.0000 (44.5460)  acc5: 83.3333 (74.8476)  time: 0.2838  data: 0.1524  max mem: 15572
Test:  [1540/2442]  eta: 0:04:15  loss: 2.0677 (2.3274)  acc1: 50.0000 (44.5165)  acc5: 83.3333 (74.8107)  time: 0.3208  data: 0.1797  max mem: 15572
Test:  [1550/2442]  eta: 0:04:12  loss: 2.4491 (2.3267)  acc1: 41.6667 (44.5197)  acc5: 75.0000 (74.8281)  time: 0.3205  data: 0.1772  max mem: 15572
Test:  [1560/2442]  eta: 0:04:10  loss: 2.3634 (2.3262)  acc1: 41.6667 (44.5761)  acc5: 75.0000 (74.8345)  time: 0.2969  data: 0.1540  max mem: 15572
Test:  [1570/2442]  eta: 0:04:07  loss: 1.6065 (2.3199)  acc1: 75.0000 (44.7751)  acc5: 91.6667 (74.9629)  time: 0.3005  data: 0.1509  max mem: 15572
Test:  [1580/2442]  eta: 0:04:04  loss: 1.7751 (2.3175)  acc1: 66.6667 (44.7923)  acc5: 91.6667 (75.0211)  time: 0.3096  data: 0.1602  max mem: 15572
Test:  [1590/2442]  eta: 0:04:01  loss: 2.0972 (2.3195)  acc1: 33.3333 (44.6732)  acc5: 75.0000 (74.9948)  time: 0.2885  data: 0.1508  max mem: 15572
Test:  [1600/2442]  eta: 0:03:59  loss: 2.6411 (2.3205)  acc1: 25.0000 (44.6648)  acc5: 66.6667 (74.9427)  time: 0.3175  data: 0.1768  max mem: 15572
Test:  [1610/2442]  eta: 0:03:56  loss: 1.0312 (2.3104)  acc1: 83.3333 (44.9462)  acc5: 91.6667 (75.0776)  time: 0.3237  data: 0.1629  max mem: 15572
Test:  [1620/2442]  eta: 0:03:53  loss: 1.0610 (2.3092)  acc1: 66.6667 (44.9825)  acc5: 91.6667 (75.0874)  time: 0.2697  data: 0.1178  max mem: 15572
Test:  [1630/2442]  eta: 0:03:50  loss: 1.8773 (2.3073)  acc1: 50.0000 (45.0133)  acc5: 83.3333 (75.1277)  time: 0.2650  data: 0.1290  max mem: 15572
Test:  [1640/2442]  eta: 0:03:47  loss: 2.4371 (2.3099)  acc1: 33.3333 (44.9116)  acc5: 75.0000 (75.0813)  time: 0.2741  data: 0.1357  max mem: 15572
Test:  [1650/2442]  eta: 0:03:45  loss: 2.4634 (2.3073)  acc1: 41.6667 (45.0182)  acc5: 75.0000 (75.0909)  time: 0.2895  data: 0.1530  max mem: 15572
Test:  [1660/2442]  eta: 0:03:42  loss: 1.9964 (2.3076)  acc1: 50.0000 (44.9679)  acc5: 83.3333 (75.1154)  time: 0.2929  data: 0.1595  max mem: 15572
Test:  [1670/2442]  eta: 0:03:39  loss: 2.1061 (2.3073)  acc1: 41.6667 (44.9830)  acc5: 83.3333 (75.1596)  time: 0.2895  data: 0.1448  max mem: 15572
Test:  [1680/2442]  eta: 0:03:36  loss: 2.6450 (2.3116)  acc1: 25.0000 (44.8344)  acc5: 75.0000 (75.0793)  time: 0.2877  data: 0.1399  max mem: 15572
Test:  [1690/2442]  eta: 0:03:33  loss: 2.5322 (2.3117)  acc1: 33.3333 (44.8009)  acc5: 75.0000 (75.1232)  time: 0.2908  data: 0.1479  max mem: 15572
Test:  [1700/2442]  eta: 0:03:30  loss: 2.2457 (2.3094)  acc1: 41.6667 (44.8217)  acc5: 83.3333 (75.1764)  time: 0.2878  data: 0.1485  max mem: 15572
Test:  [1710/2442]  eta: 0:03:27  loss: 1.9128 (2.3075)  acc1: 50.0000 (44.8812)  acc5: 83.3333 (75.1997)  time: 0.2697  data: 0.1358  max mem: 15572
Test:  [1720/2442]  eta: 0:03:25  loss: 1.3522 (2.3030)  acc1: 75.0000 (45.0126)  acc5: 83.3333 (75.2615)  time: 0.2919  data: 0.1525  max mem: 15572
Test:  [1730/2442]  eta: 0:03:22  loss: 1.4747 (2.2992)  acc1: 66.6667 (45.1184)  acc5: 91.6667 (75.3033)  time: 0.3088  data: 0.1690  max mem: 15572
Test:  [1740/2442]  eta: 0:03:19  loss: 1.5890 (2.2955)  acc1: 58.3333 (45.2278)  acc5: 75.0000 (75.3255)  time: 0.3007  data: 0.1552  max mem: 15572
Test:  [1750/2442]  eta: 0:03:16  loss: 2.1978 (2.2977)  acc1: 50.0000 (45.1742)  acc5: 66.6667 (75.3093)  time: 0.2906  data: 0.1427  max mem: 15572
Test:  [1760/2442]  eta: 0:03:14  loss: 2.3276 (2.2953)  acc1: 50.0000 (45.2678)  acc5: 83.3333 (75.3880)  time: 0.3004  data: 0.1566  max mem: 15572
Test:  [1770/2442]  eta: 0:03:11  loss: 1.9520 (2.2968)  acc1: 58.3333 (45.2381)  acc5: 83.3333 (75.3529)  time: 0.3017  data: 0.1648  max mem: 15572
Test:  [1780/2442]  eta: 0:03:08  loss: 2.1852 (2.2978)  acc1: 41.6667 (45.2134)  acc5: 75.0000 (75.3556)  time: 0.2964  data: 0.1652  max mem: 15572
Test:  [1790/2442]  eta: 0:03:06  loss: 2.8558 (2.3031)  acc1: 16.6667 (45.0493)  acc5: 66.6667 (75.2513)  time: 0.3379  data: 0.2027  max mem: 15572
Test:  [1800/2442]  eta: 0:03:03  loss: 2.8132 (2.3027)  acc1: 16.6667 (45.0722)  acc5: 66.6667 (75.2684)  time: 0.3139  data: 0.1750  max mem: 15572
Test:  [1810/2442]  eta: 0:03:00  loss: 2.2424 (2.3046)  acc1: 41.6667 (45.0396)  acc5: 75.0000 (75.2531)  time: 0.2870  data: 0.1401  max mem: 15572
Test:  [1820/2442]  eta: 0:02:57  loss: 2.2094 (2.3031)  acc1: 41.6667 (45.0714)  acc5: 75.0000 (75.2837)  time: 0.2615  data: 0.1016  max mem: 15572
Test:  [1830/2442]  eta: 0:02:54  loss: 1.4359 (2.3006)  acc1: 66.6667 (45.1529)  acc5: 91.6667 (75.3004)  time: 0.2466  data: 0.0968  max mem: 15572
Test:  [1840/2442]  eta: 0:02:51  loss: 2.0116 (2.3011)  acc1: 50.0000 (45.1476)  acc5: 75.0000 (75.2897)  time: 0.2962  data: 0.1582  max mem: 15572
Test:  [1850/2442]  eta: 0:02:49  loss: 2.2857 (2.3013)  acc1: 41.6667 (45.0882)  acc5: 75.0000 (75.3287)  time: 0.3494  data: 0.2105  max mem: 15572
Test:  [1860/2442]  eta: 0:02:46  loss: 2.2568 (2.3000)  acc1: 41.6667 (45.1549)  acc5: 83.3333 (75.3627)  time: 0.3032  data: 0.1584  max mem: 15572
Test:  [1870/2442]  eta: 0:02:43  loss: 2.0882 (2.2987)  acc1: 50.0000 (45.2031)  acc5: 83.3333 (75.3875)  time: 0.2746  data: 0.1277  max mem: 15572
Test:  [1880/2442]  eta: 0:02:40  loss: 2.1918 (2.2993)  acc1: 41.6667 (45.1666)  acc5: 83.3333 (75.3987)  time: 0.3011  data: 0.1644  max mem: 15572
Test:  [1890/2442]  eta: 0:02:37  loss: 2.3798 (2.2982)  acc1: 33.3333 (45.1481)  acc5: 83.3333 (75.4363)  time: 0.2822  data: 0.1438  max mem: 15572
Test:  [1900/2442]  eta: 0:02:34  loss: 2.1677 (2.2982)  acc1: 33.3333 (45.1122)  acc5: 75.0000 (75.4296)  time: 0.2964  data: 0.1563  max mem: 15572
Test:  [1910/2442]  eta: 0:02:31  loss: 2.5199 (2.3037)  acc1: 33.3333 (44.9895)  acc5: 66.6667 (75.3096)  time: 0.2962  data: 0.1515  max mem: 15572
Test:  [1920/2442]  eta: 0:02:29  loss: 2.5310 (2.3052)  acc1: 33.3333 (44.8898)  acc5: 66.6667 (75.3037)  time: 0.2910  data: 0.1360  max mem: 15572
Test:  [1930/2442]  eta: 0:02:26  loss: 2.0884 (2.3072)  acc1: 33.3333 (44.8818)  acc5: 83.3333 (75.2330)  time: 0.2933  data: 0.1298  max mem: 15572
Test:  [1940/2442]  eta: 0:02:23  loss: 1.8505 (2.3063)  acc1: 50.0000 (44.9339)  acc5: 83.3333 (75.2705)  time: 0.2927  data: 0.1342  max mem: 15572
Test:  [1950/2442]  eta: 0:02:20  loss: 2.0934 (2.3072)  acc1: 41.6667 (44.8787)  acc5: 83.3333 (75.2776)  time: 0.2972  data: 0.1530  max mem: 15572
Test:  [1960/2442]  eta: 0:02:17  loss: 2.1719 (2.3061)  acc1: 41.6667 (44.9133)  acc5: 75.0000 (75.2890)  time: 0.2698  data: 0.1301  max mem: 15572
Test:  [1970/2442]  eta: 0:02:14  loss: 1.4405 (2.3019)  acc1: 66.6667 (45.0575)  acc5: 91.6667 (75.3594)  time: 0.2695  data: 0.1331  max mem: 15572
Test:  [1980/2442]  eta: 0:02:12  loss: 1.1781 (2.2976)  acc1: 75.0000 (45.1792)  acc5: 91.6667 (75.4459)  time: 0.3234  data: 0.1870  max mem: 15572
Test:  [1990/2442]  eta: 0:02:09  loss: 1.7474 (2.2966)  acc1: 50.0000 (45.1448)  acc5: 83.3333 (75.4855)  time: 0.3247  data: 0.1819  max mem: 15572
Test:  [2000/2442]  eta: 0:02:06  loss: 2.3812 (2.2981)  acc1: 33.3333 (45.0691)  acc5: 83.3333 (75.4664)  time: 0.2919  data: 0.1375  max mem: 15572
Test:  [2010/2442]  eta: 0:02:03  loss: 1.9660 (2.2950)  acc1: 41.6667 (45.1724)  acc5: 83.3333 (75.5056)  time: 0.2800  data: 0.1263  max mem: 15572
Test:  [2020/2442]  eta: 0:02:00  loss: 0.8930 (2.2887)  acc1: 83.3333 (45.3282)  acc5: 91.6667 (75.5938)  time: 0.2905  data: 0.1457  max mem: 15572
Test:  [2030/2442]  eta: 0:01:57  loss: 1.1334 (2.2860)  acc1: 75.0000 (45.4169)  acc5: 91.6667 (75.6442)  time: 0.2970  data: 0.1503  max mem: 15572
Test:  [2040/2442]  eta: 0:01:55  loss: 1.7830 (2.2870)  acc1: 50.0000 (45.3863)  acc5: 83.3333 (75.6206)  time: 0.2913  data: 0.1352  max mem: 15572
Test:  [2050/2442]  eta: 0:01:52  loss: 2.5383 (2.2870)  acc1: 41.6667 (45.3844)  acc5: 75.0000 (75.6176)  time: 0.3211  data: 0.1693  max mem: 15572
Test:  [2060/2442]  eta: 0:01:49  loss: 2.0394 (2.2860)  acc1: 50.0000 (45.4472)  acc5: 75.0000 (75.6348)  time: 0.2933  data: 0.1557  max mem: 15572
Test:  [2070/2442]  eta: 0:01:46  loss: 2.2854 (2.2877)  acc1: 50.0000 (45.3766)  acc5: 75.0000 (75.6197)  time: 0.2952  data: 0.1489  max mem: 15572
Test:  [2080/2442]  eta: 0:01:43  loss: 2.6265 (2.2897)  acc1: 33.3333 (45.3067)  acc5: 75.0000 (75.5807)  time: 0.3031  data: 0.1544  max mem: 15572
Test:  [2090/2442]  eta: 0:01:40  loss: 2.9394 (2.2934)  acc1: 25.0000 (45.1857)  acc5: 58.3333 (75.4822)  time: 0.2890  data: 0.1423  max mem: 15572
Test:  [2100/2442]  eta: 0:01:37  loss: 2.6365 (2.2939)  acc1: 25.0000 (45.1690)  acc5: 66.6667 (75.5037)  time: 0.2889  data: 0.1449  max mem: 15572
Test:  [2110/2442]  eta: 0:01:35  loss: 2.1580 (2.2916)  acc1: 50.0000 (45.2155)  acc5: 83.3333 (75.5487)  time: 0.2869  data: 0.1379  max mem: 15572
Test:  [2120/2442]  eta: 0:01:32  loss: 1.6832 (2.2910)  acc1: 58.3333 (45.2420)  acc5: 83.3333 (75.5343)  time: 0.3055  data: 0.1513  max mem: 15572
Test:  [2130/2442]  eta: 0:01:29  loss: 1.5808 (2.2886)  acc1: 66.6667 (45.3113)  acc5: 83.3333 (75.5592)  time: 0.2782  data: 0.1358  max mem: 15572
Test:  [2140/2442]  eta: 0:01:26  loss: 1.8898 (2.2872)  acc1: 58.3333 (45.3604)  acc5: 75.0000 (75.5722)  time: 0.2745  data: 0.1371  max mem: 15572
Test:  [2150/2442]  eta: 0:01:23  loss: 1.9382 (2.2865)  acc1: 50.0000 (45.3897)  acc5: 75.0000 (75.5773)  time: 0.2920  data: 0.1594  max mem: 15572
Test:  [2160/2442]  eta: 0:01:20  loss: 2.4477 (2.2879)  acc1: 41.6667 (45.3340)  acc5: 75.0000 (75.5862)  time: 0.3059  data: 0.1654  max mem: 15572
Test:  [2170/2442]  eta: 0:01:17  loss: 2.4278 (2.2872)  acc1: 41.6667 (45.3823)  acc5: 83.3333 (75.6103)  time: 0.3062  data: 0.1556  max mem: 15572
Test:  [2180/2442]  eta: 0:01:15  loss: 2.2312 (2.2897)  acc1: 41.6667 (45.3041)  acc5: 75.0000 (75.5502)  time: 0.2826  data: 0.1372  max mem: 15572
Test:  [2190/2442]  eta: 0:01:12  loss: 2.8925 (2.2930)  acc1: 16.6667 (45.2153)  acc5: 58.3333 (75.4906)  time: 0.2889  data: 0.1493  max mem: 15572
Test:  [2200/2442]  eta: 0:01:09  loss: 3.0344 (2.2962)  acc1: 16.6667 (45.1234)  acc5: 58.3333 (75.4316)  time: 0.2964  data: 0.1510  max mem: 15572
Test:  [2210/2442]  eta: 0:01:06  loss: 3.0344 (2.2988)  acc1: 25.0000 (45.0890)  acc5: 58.3333 (75.3769)  time: 0.2731  data: 0.1264  max mem: 15572
Test:  [2220/2442]  eta: 0:01:03  loss: 2.6096 (2.2990)  acc1: 41.6667 (45.0585)  acc5: 66.6667 (75.3790)  time: 0.2802  data: 0.1415  max mem: 15572
Test:  [2230/2442]  eta: 0:01:00  loss: 2.1041 (2.2988)  acc1: 41.6667 (45.0657)  acc5: 83.3333 (75.3847)  time: 0.2815  data: 0.1425  max mem: 15572
Test:  [2240/2442]  eta: 0:00:57  loss: 2.1136 (2.2994)  acc1: 41.6667 (45.0803)  acc5: 83.3333 (75.3607)  time: 0.2636  data: 0.1224  max mem: 15572
Test:  [2250/2442]  eta: 0:00:55  loss: 2.3518 (2.2999)  acc1: 41.6667 (45.0726)  acc5: 66.6667 (75.3517)  time: 0.2932  data: 0.1559  max mem: 15572
Test:  [2260/2442]  eta: 0:00:52  loss: 2.3518 (2.3005)  acc1: 41.6667 (45.0391)  acc5: 75.0000 (75.3538)  time: 0.2943  data: 0.1592  max mem: 15572
Test:  [2270/2442]  eta: 0:00:49  loss: 2.4526 (2.3005)  acc1: 41.6667 (45.0939)  acc5: 75.0000 (75.3486)  time: 0.2886  data: 0.1506  max mem: 15572
Test:  [2280/2442]  eta: 0:00:46  loss: 2.3592 (2.3010)  acc1: 41.6667 (45.1008)  acc5: 75.0000 (75.3471)  time: 0.3133  data: 0.1768  max mem: 15572
Test:  [2290/2442]  eta: 0:00:43  loss: 2.5375 (2.3032)  acc1: 41.6667 (45.0495)  acc5: 75.0000 (75.2983)  time: 0.2760  data: 0.1354  max mem: 15572
Test:  [2300/2442]  eta: 0:00:40  loss: 2.5311 (2.3026)  acc1: 41.6667 (45.0384)  acc5: 75.0000 (75.3477)  time: 0.2732  data: 0.1296  max mem: 15572
Test:  [2310/2442]  eta: 0:00:37  loss: 2.2039 (2.3051)  acc1: 25.0000 (44.9805)  acc5: 83.3333 (75.2849)  time: 0.3121  data: 0.1722  max mem: 15572
Test:  [2320/2442]  eta: 0:00:34  loss: 2.5803 (2.3087)  acc1: 25.0000 (44.8765)  acc5: 75.0000 (75.2154)  time: 0.2762  data: 0.1383  max mem: 15572
Test:  [2330/2442]  eta: 0:00:32  loss: 2.5255 (2.3082)  acc1: 33.3333 (44.8663)  acc5: 75.0000 (75.2538)  time: 0.2601  data: 0.1251  max mem: 15572
Test:  [2340/2442]  eta: 0:00:29  loss: 2.0231 (2.3099)  acc1: 41.6667 (44.8419)  acc5: 83.3333 (75.1958)  time: 0.3029  data: 0.1613  max mem: 15572
Test:  [2350/2442]  eta: 0:00:26  loss: 2.0231 (2.3105)  acc1: 50.0000 (44.8462)  acc5: 75.0000 (75.1879)  time: 0.2934  data: 0.1469  max mem: 15572
Test:  [2360/2442]  eta: 0:00:23  loss: 2.5772 (2.3118)  acc1: 33.3333 (44.7833)  acc5: 75.0000 (75.1694)  time: 0.2954  data: 0.1538  max mem: 15572
Test:  [2370/2442]  eta: 0:00:20  loss: 2.5394 (2.3125)  acc1: 33.3333 (44.7666)  acc5: 66.6667 (75.1441)  time: 0.3037  data: 0.1615  max mem: 15572
Test:  [2380/2442]  eta: 0:00:17  loss: 1.6229 (2.3077)  acc1: 58.3333 (44.9181)  acc5: 83.3333 (75.2170)  time: 0.2903  data: 0.1351  max mem: 15572
Test:  [2390/2442]  eta: 0:00:14  loss: 1.4569 (2.3067)  acc1: 66.6667 (44.9394)  acc5: 91.6667 (75.2509)  time: 0.2853  data: 0.1263  max mem: 15572
Test:  [2400/2442]  eta: 0:00:12  loss: 2.0816 (2.3060)  acc1: 50.0000 (44.9396)  acc5: 83.3333 (75.2811)  time: 0.2985  data: 0.1516  max mem: 15572
Test:  [2410/2442]  eta: 0:00:09  loss: 2.6796 (2.3092)  acc1: 16.6667 (44.8258)  acc5: 75.0000 (75.2178)  time: 0.3121  data: 0.1659  max mem: 15572
Test:  [2420/2442]  eta: 0:00:06  loss: 1.8182 (2.3038)  acc1: 50.0000 (44.9814)  acc5: 83.3333 (75.2926)  time: 0.2823  data: 0.1328  max mem: 15572
Test:  [2430/2442]  eta: 0:00:03  loss: 1.0448 (2.3013)  acc1: 75.0000 (45.0569)  acc5: 91.6667 (75.3188)  time: 0.2588  data: 0.1162  max mem: 15572
Test:  [2440/2442]  eta: 0:00:00  loss: 1.4894 (2.2991)  acc1: 66.6667 (45.1250)  acc5: 91.6667 (75.3858)  time: 0.2047  data: 0.0753  max mem: 15572
Test:  [2441/2442]  eta: 0:00:00  loss: 1.6242 (2.2997)  acc1: 58.3333 (45.1191)  acc5: 91.6667 (75.3772)  time: 0.2024  data: 0.0753  max mem: 15572
Test: Total time: 0:11:39 (0.2864 s / it)
* Acc@1 45.119 Acc@5 75.377 loss 2.300
Start merging results...
Reading individual output files
Computing final results
4883
Accuracy of the network on the 29298 test videos: Top-1: 47.92%, Top-5: 78.31%
Training time 19:08:28
